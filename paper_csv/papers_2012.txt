This is a short story of how we have evolved over the last 40 years, doing semantics. It could partially overlap with a history of PACLIC which is commemorating the 25th year or a quarter of a century of its founding. The story tells how we semanticists of natural language moved from all possible worlds to small worlds, now living in and with a tiny mobile world. 
This paper discusses theoretical and implementational challenges in developing a deep grammar of Indonesian (IndoGram) within the lexical-functional grammar (LFG)-based Parallel Grammar (ParGram) framework, using the Xerox Linguistic Environment (XLE) parser. The ParGram project involves developing and processing computational grammars in parallel to test the LFG’s theoretical claims of language universality, while at the same time testing its robustness to handle typologically quite different languages. Two relevant cases are discussed: voice-related morphosyntactic derivation and crossed-control dependency in Indonesian. It will be demonstrated that parallelism should be taken as a matter of degree, that it cannot always be maintained for good language-specific reasons and that the participation of IndoGram has also contributed to the rethinking and improvement of certain parallelism standards. 
1. Introduction A mark of erudition in verbal communication is the use of idiomatic language employing metaphors and figurative speech. Its study is important not only for linguistic research but also for the study of language, rhetorics, literacy studies and cultural history, and the relationship amongst them. The rhetorical distinction between literal and metaphorical meanings and so semantic and discoursal opacity often associated with idioms is universal. But the format of idioms can stand out, and the means by which the expressions are formed, often drawing on the use of notable objects or events relevant to the native society concerned are often culture bound. These objects and events can often be drawn from relatively closed sets. Idioms are commonly used in metaphors and figurative speech in all languages and in daily communication. They have not only attracted the attention of specialists interested in language, rhetorics and literary studies (Black 1962, Makkai 1972, Xiang 1979), but even visiting national leaders to China from USA and Japan in recent years have cited them in their speeches. In the last few decades, several major areas associated with idioms and metaphors have become noticeable: (a) Syntax and Semantics, e.g. Chafe’s well-known 1968 paper on syntactic decomposability issues of frozen idioms; (Katz and Postal (1963) and Jackendoff (1995)); (b) Cognitive studies, e.g. Gibbs (1980, 1985, 1987), Nippold et al. (1989), Zuo (2006), Zhang (1984); and (c) Cultural studies, e.g. Lakoff (1987) [gender], Tang (2007) [food related items], Nall (2008) [numbers], Fontecha and Catalan (2003) [animals], Liu (1984), Fan (2007) [color terms], Mo (2001) [Chinese culture and idioms]. There are also notable anthologies on the relevant approaches, e.g. Everaert (1989, 1992, and 1995). We note that when some salient linguistic features are found to be shared across two languages, the question often arises as to whether their origin might be due to: (a) shared genetic affinity, or (b) borrowing across language boundaries. Furthermore, they could be also (c) universal features if shared by all other languages, or (d) typological linguistic features if shared by structurally similar natural languages, as well as (e) areal or regional features if they are found only in a particular geographical region. Moreover, they are not mutually exclusive. 
A number of grammar formalisms were proposed in 80’s, such as Lexical Functional Grammars, Generalized Phrase Structure Grammars, and Tree Adjoining Grammars. Those formalisms then started to put a stress on lexicon, and were called as lexicalist (or lexicalized) grammars. Representative examples of lexicalist grammars were Head-driven Phrase Structure Grammars (HPSG) and Lexicalized Tree Adjoining Grammars (LTAG). While grammars and lexicons were two major linguistic resources of syntactic processing of natural languages, lexicons began to play an important role in language processing. Things have changed from early 90’s, when large scale language resources became available and corpus-based research started to dominate almost all aspects of natural language processing (NLP). Partof-speech taggers and syntactic parsers are the most well-studied topics in corpus-based research. Various parsers, based either on phrase structure grammars or on dependency structures, have been developed, applying various machine learning techniques on syntactically annotated corpora. State-ofthe-art parsers developed in this way have achieved very good performance. Those trends are also beneﬁcial to lexicalist grammars since parsing with those grammar formalisms is amenable to phrase structure-based parsing through abstraction of grammatical schemata or a derivation process with those grammar formalism (i.e., a derivation tree) can be considered to correspond to a word dependency tree. Recent trends in NLP have started to target diversely spread areas that require semantic and pragmatic information. Some areas like social media analysis, such as twitter or blog text analysis, have  a more preference to getting semantic or sentiment information than syntactic information. Though this trend is attracting people’s attention and is getting growing importance, still syntactic analysis keeps to play an important role. Simple extension of annotated corpora and lexical statistics will not be able to skyrocket parsers’ performance. Improvement of parsing accuracy especially that of long sentences requires to tackle problems that are not on the current main stream of parser development. 
In this talk, I will outline some of the myriad of challenges and opportunities that social media offer for natural language processing. I will present analysis of how pre-processing can be used to make social media data more amenable to natural language processing, and review a selection of tasks which attempt to harness the considerable potential of different social media services. There is no question that social media are fantastically popular and varied in form — ranging from user forums, to microblogs such as Twitter, to social networking sites such as Facebook — and that much of the content they host is in the form of natural language. This would suggest a myriad of opportunities for natural language processing (NLP), and yet much of the applied research on social media which uses language data is based on superﬁcial analysis, often in the form of simple keyword search. This begs the question: Are NLP methods not suited to social media analysis? Conversely, is social media data too challenging for modern-day NLP? Alternatively, are simple term search-based methods sufﬁcient for social media analysis, i.e. is NLP overkill for social media? In exploring these questions, I attempt to answer the overarching question of whether social media data is the friend or foe of NLP. I approach the question ﬁrst from the perspective of what challenges social media language poses for NLP. The most immediate answer is the infamously free-form nature of language in social media, encompassing spelling inconsistencies, the free-form adoption of new terms, and regular violations of English grammar norms. Unsurprisingly, when NLP  
This paper presents work on the semantic annotation of a multimodal corpus of English television news. The annotation is performed on the second-by-secondaligned transcript layer, adding verb frame categories and semantic roles on top of a morphosyntactic analysis with full dependency information. We use a rulebased method, where Constraint Grammar mapping rules are automatically generated from a syntactically anchored Framenet with about 500 frame types and 50 semantic role types. We discuss design decisions concerning the Framenet, and evaluate the coverage and performance of the pilot system on authentic news data.  The UCLA Communications Studies Archive (UCLA CSA) is a so-called monitor corpus of television news, where newscasts from a large number of channels are recorded daily in high-quality video mode, amounting to ~ 150.000 hours of recorded news, and growing by 100 programs a day (DeLiema, Steen & Turner 2012). To date only English language channels have been targeted, but the author's institution has plans to join the project with matching data for first the Scandinavian languages and German, then further European languages. This paper focuses on the linguistic annotation of the time-stamp-aligned textual layer of the corpus. Optimally, such annotation should address the following issues  
Generative Lexicon theory (GL) establishes three mechanisms at work when a predicate selects an argument, i.e. pure selection, accommodation and type coercion. They are widely used in verbal selection of nouns in the entity domain. However, little attention has been devoted to the compositionality of [N1+event noun] type NN compounds. This paper extends the usage of these mechanisms in two ways: 1) the eventive nominal head selection of a nominal modifier, and 2) their use in the eventive domain, through the case study on [N1+↨䋑b΃sài ‘competition’]. Moreover, it reveals a new compositional mechanism subcomposition. It also discovers the domain contribution in type coercion. This work enriches the study on compositionality and GL. 
Domain adaptation (DA), which involves adapting a classiﬁer developed from source to target data, has been studied intensively in recent years. However, when DA for word sense disambiguation (WSD) was carried out, the optimal DA method varied according to the properties of the source and target data. This paper proposes automatic DA based on comparing the degrees of conﬁdence of multiple classiﬁers for each instance. We compared three classiﬁers for three DA methods, where 1) a classiﬁer was trained with a small amount of target data that was randomly selected and manually labeled but without source data, 2) a classiﬁer was trained with source data and a small amount of target data that was randomly selected and manually labeled, and 3) a classiﬁer was trained with selected source data that were sufﬁciently similar to the target data and a small amount of target data that was randomly selected and manually labeled. We used the method whose degree of conﬁdence was the highest for each instance when Japanese WSD was carried out. The average accuracy of WSD when the DA methods that were determined automatically were used was signiﬁcantly higher than when the original methods were used collectively. 
This study calculates the selectional preference strength between transitive verbs and their co-occurring objects, and thereby investigates how much they are co-related to each other in Korean. The selectional preference strength is automatically measured in a bottom-up way, and the outcomes are evaluated in comparison with a manually constructed resource that indicates which verb takes which class(es) of nouns as its dependents. The measurement offered by this study not only can be used to improve NLP applications, but also has a theoretic signiﬁcance in that it can play a role as distributional evidence in the study of argument structure. 
In this paper, we study how to automatically extract and visualize food (or nutrition) and disease relationships from Chinese publications of Nutritional Genomics. Different from previous approaches that mostly apply handcrafted rules or co-occurrence patterns, we propose an approach using probabilistic models and domain knowledge. In particular, we first utilize encyclopedia to construct a domain knowledge base, and then develop a sentence simplification model to simplify complicated sentences we meet. Afterwards, we treat relation extraction issue as a sequence labeling task and adopt Conditional Random Fields (CRFs) models to extract food and disease relationships. Finally, these relationships are visualized. Experimental results on real-world datasets show that the proposed approach is effective. 
We propose a new method for entity set expansion that achieves highly accurate extraction by suppressing the effect of semantic drift; it requires a small amount of interactive information. We supplement interactive information to re-train the topic models (based on interactive Unigram Mixtures) not only the contextual information. Although the topic information extracted from an unsupervised corpus is effective for reducing the effect of semantic drift, the topic models and target entities sometimes suffer grain mismatch. Interactive Unigram Mixtures can, with very few interactive words, ease the mismatch between topic and target entities. We incorporate the interactive topic information into a two-stage discriminative system for stable set expansion. Experiments conﬁrm that the proposal raises the accuracy of the set expansion system from the baselines examined. 
This paper implements and compares three different strategies to use English as pivot language for Chinese-Japanese patent translation: corpus enrichment, sentence pivot translation and phrase pivot translation. Our results show that both corpus enrichment and phrase pivot translation strategy outperform the baseline system, while the sentence pivot translation strategy failed to improve the system. We apply the strategies on large data set and figure out approaches to improve efficiency. Finally, we perform Minimum Bayes Risk system combination on the different results of direct translation system and pivot translation systems, which significantly outperforms the direct translation system by 4.25 BLEU scores.  Much work has been done to overcome the data bottleneck problem. For example, Lu et al. (2009) exploited the existence of bilingual patent corpora and constructed a Chinese-English patent parallel corpus. Resnik and Smith (2003) took the web as a parallel corpus and mined parallel data from it. Munteanu and Marcu (2005) trained a maximum entropy classifier to extract parallel corpus from large non-parallel newspaper corpora. Our work differs in that we make use of the currently available bilingual corpora, without exploiting extra bilingual data to improve machine translation quality. In other words, we employ pivot translation strategies to improve the performance of SMT systems.  How to apply pivot translation strategies to help scare-resourced language translation?  
 our training data only 10% of words in the discus-  sion forums and tweets appear more than ten times,  With the rapid development of social media and social networks, spontaneously user generated content like tweets and forum posts have become important materials for tracking people’s opinions and sentiments online. In this paper we investigate the limitations of traditional linguistic-based approaches to sentiment analysis when applied to these informal genres. Inspired by various social cognitive  while in movie reviews over 20% of words appear more than ten times); (3) unpredictable shift in topics/issues. The prevalence of debate in both forum posts and tweets leads to the use of more complicated discourse structures involving multiple targets and sentiments, as well as the second-person voice. These difﬁculties are magniﬁed in tweets due to necessarily compressed contexts (tweets are limited to  theories, we combine local linguistic features and global social evidence in a propagation scheme to improve sentiment analysis results. Without using any additional labeled data, this new approach obtains signiﬁcant improvement (up to 12% higher accuracy) for various genres in the domain of presidential election.  140 characters). In this paper, we tackle these challenges from two perspectives. First, we approach the sentiment analysis task by identifying not only a speciﬁc “target” (e.g., presidential candidate) but also its associated “issues” (e.g., foreign policy) before detecting sentiment. This approach is similar to the idea of  modeling “aspect” in product reviews (Titov and M-  
 product, which is often machine translation. While  We introduce and describe ongoing work in our Indonesian dependency treebank. We described characteristics of the source data as well as describe our annotation guidelines for creating the dependency structures. Reported within are the results from the start of the Indonesian dependency treebank.  useful for machine learning as well and linguistic analysis, these treebanks typically only exist for a handful of resource-rich languages. Treebanks tend to come in two linguistic forms, dependency based and constituency based each with their own pros and cons. Dependency treebanks have been made popular by treebanks such as the Prague dependency  We also show ensemble dependency parsing and self training approaches applicable  treebank (Hajic, 1998) and constituency treebanks by the Penn treebank (Marcus et al., 1993). While  to under-resourced languages using our manually annotated dependency structures. We show that for an under-resourced language, the use of tuning data for a meta classiﬁer is more effective than using it as additional training data for individual parsers. This meta-classiﬁer creates an ensemble dependency parser and increases the dependency accuracy by 4.92% on average and 1.99% over the best individual models on average. As the data sizes grow for the the under-resourced language a meta classiﬁer can easily adapt. To the best of our knowledge this is the ﬁrst full implementation of a dependency parser for Indonesian. Using self-training in combination  some linguistic phenomena are better represented in one form instead of another, the two forms are generally able to be transformed into one another. While many of the world’s 6,000+ languages could be considered under-resourced due to a limited number of native speakers and low overall population in their countries, Indonesia is the fourth most populous country in the world with over 23 million native and 215 million non-native Bahasa Indonesia speakers. The development of language resources, treebanks in particular, for Bahasa Indonesia will have an immediate effect for Indonesian NLP. Further development of our Indonesian depen-  with our Ensemble SVM Parser we show aditional improvement. Using this parsing model we plan on expanding the size of the corpus by using a semi-supervised approach by applying the parser and correcting the errors, reducing the amount of annotation time needed.  dency treebank can affect part of speech taggers, named entity recognizers, and machine translation systems. All of these systems have technical beneﬁts to the 238 million native and non-native Indonesian speakers ranging for spell checkers, improved information retrieval, to improved access to more of  
 and Federico, 2009) (Yeniterzi and Oﬂazer, 2010),  In this paper, we study the effect of incorporating morphological information on an Indonesian (id) to English (en) Statistical Machine Translation (SMT) system as part of a preprocessing module. The linguistic phenomenon that is being addressed here is Indonesian cliticized words. The approach is to transform the text by separating the correct clitics from a cliticized word to simplify the word alignment. We also study the effect of applying the preprocessing on different SMT systems trained on different kinds of text, such as spoken language text. The system is built using the state-of-the-art SMT tool, MOSES. The Indonesian morphological information is provided by MorphInd. Overall the preprocessing improves the translation quality, especially for the Indonesian spoken language text, where it gains 1.78 BLEU score points of increase.  and many more. This paper shows an example on how to use Indonesian morphological information on an Indonesian-English SMT system by preprocessing to gain better translation quality. Indonesian has a complex morphology system, including afﬁxation, reduplication, and cliticization. Here we address the problem of cliticized phrase constructions in Indonesian that occur more frequent in spoken language and social media text than in the formal written text. Having more cliticized phrases in a text makes a spoken dialogue text difﬁcult to translate. Here we also evaluate the effect of the preprocessing on other different types of text. 2 Related Work Indonesian or Bahasa Indonesia (“language of Indonesia”), is the ofﬁcial language of the country. Indonesian is the fourth most spoken language in the  
The particle no in Japanese exhibits two types of nominalization: “participant” and “situation” nominalization. Despite several motivations for a uniform account, only a few attempts have been made to address no-nominalization uniformly. In this paper, I shall develop a unified account within the formalism Dynamic Syntax, and show that a number of properties of the phenomenon follow from the analysis.  
This study, based on a variety of data sources, investigates the linguistic and cultural characteristics associated with the black and white expressions among Taiwanese Mandarin, Taiwanese Hakka, and Taiwanese Southern Min. The meaning distributions of the data profile four types: prototypical meanings, metonymic extensions, metaphorical extensions and idiosyncratic examples; and the associated cultural factors are examined. Some meaning extensions are widespread across the three languages, whereas some are language-specific because of cultural roots. Among the three languages, Taiwanese Mndarin develops the most prolific usages and this may be ascribed to the prosperity of cultural, economic or technological developments of the language. 
 jective expressions are boot-strapping algorithms  Identifying and extracting subjective information from News, Blogs and other user generated content has lot of applications. Most of the earlier work concentrated on English data. But, recently subjectivity related research at sentence-level in other languages has increased. In this paper, we achieve sentence-level subjectivity classiﬁcation using language independent feature weighing and selection methods which are consistent across languages. Experiments performed on 5 different languages including English and South Asian language Hindi show that En-  which lacks scalability. But, recently focus shifted to multilingual space (R. Mihalcea.et.al, 2007). Banea (C. Banea.et.al, 2008) worked on sentencelevel subjectivity analysis using machine translation approaches by leveraging resources and tools available for English . Another approach (C. Banea.et.al, 2010) used multilingual space and meta classiﬁers to build high precision classiﬁers for subjectivity classiﬁcation. However, aforementioned work (C. Banea.et.al, 2008) concentrated more on language speciﬁc attributes due to variation in expression of subjec-  tropy based category coverage difference criterion (ECCD) feature selection method with language independent feature weighing methods outperforms other approaches for subjective classiﬁcation.  tivity in different languages. This create a problem of portability of methods to different languages. Other approach (C. Banea.et.al, 2010) which tried achieving language independence created large feature vectors for subjectivity classiﬁcation. Different  
This paper is motivated by the observation that not all adjectives in Chinese have a canonical antonym. For example, most Chinese speakers choose to translate the English word dishonest into a word string bu chengshi ‘not honest’ instead of any antonym candidates of chengshi suggested in antonym dictionaries. Our discourse evidence from corpus data suggests that bu chengshi is evolving into a word in discourse at a faster pace than some other ‘bu + adjective’ strings, and this may result from the lexical gap for a canonical antonym of chengshi and the communicative need for such a word. As a consequence, it is proposed that if the lexicalization process of bu chengshi continues in the future, the string may need to be considered a single word in a segmentation system (i.e., buchengshi ‘dishonest’). For a segmentation system to distinguish between words and phrases, discourse factors should be taken into consideration. 
We describe a method to automatically extract social networks from literary texts. Similar to those in prior research, nodes represent characters found in the texts; edges connect them to other characters with whom they interact, and also display sentences describing their interactions. Furthermore, other nodes encode places and are connected to characters who were active there. Thus, these networks present an overview of the “who”, “what”, and “where” in large text corpora, visualizing associations between people and places. 
Among various important issues pertaining to the so-called right dislocated construction (RDC) in Korean are the basic word order and the grammatical relation the right dislocated (RDed) element assumes to the rest of the structure. In his series of papers, J.-S. Lee (2007a,b, 2008a, 2009a,b, 2010, 2011, 2012) proposes a mono-clausal analysis of Korean RDC, according to which the RDed element is a direct dependent of the preceding predicate and Korean conforms to Kayne's (1994) universal SVO word order hypothesis due to the very existence of the RDC. In contrast, Chung (2008a, 2009b, 2010, 2011) advocates a non-mono-clausal approach, as in Tanaka (2001) and Kato (2007) for Japanese RDC, according to which the RDed element is taken as a fragment of a continuing sentence to which massive ellipsis has applied, while the headfinality is preserved. The current work tries to show that RDed elements cannot be viewed as direct dependents of the preceding predicate due to various asymmetries observed between pre- vs. post-verbal positions, favoring a nonmono-clausal analysis of Korean RDC. 1. Introduction Predicates in Korean are generally fixed at the clause final position, although the dependents are freely ordered, as in (1). It is observed in Nam and Ko (1986: 250-251) and Huh (1988: 263) among others, however, that Korean allows the so-called right dislocated construction (RDC), in which some apparent part of the sentence may show up at the post-predicate position, as in (2).  (1) a. Cheli-ka Yuni-lul manna-ess-ta (SOV) Ch.-Nom Y.-Acc meet-Pst-DE 'Cheli saw Yuni.' b. Yuni-lul Cheli-ka manna-ess-ta (OSV) (2) a. Cheli-ka manna-ess-ta Yuni-lul (SVO) b. Yuni-lul manna-ess-ta Cheli-ka (OVS) c. manna-ess-ta Cheli-ka Yuni-lul (VSO) d. manna-ess-e Yuni-lul Cheli-ka (VOS) The RDC in Korean has recently received a great deal of attention as to the architecture of the structure. (See J.-S. Lee 2007a,b, 2008a, 2009a,b, 2010, 2011, 2012, Chung 2008a, 2009b, 2010, 2011, Lee and Yoon 2009, C.-H. Lee 2009, 2011, among others.) Among various issues around the RDC are the basic word order in Koran and the grammatical relation the RDed element in the post-verbal position assumes with the rest of the construction. Lee (2007a,b, 2008a, 2009a,b, 2010, 2011, 2012) proposes a mono-clausal structure based on Kayne's (1994) universal SVO hypothesis and treats the RDed element as a direct dependent of the preceding predicate. According to this analysis, (2a) is taken as the base word order and all other structures in (1) and (2) are derived from (2a), In contrast, Chung (2008a, 2009b. 2010, 2011), basically following Tanaka's (2001) analysis of Japanese RDC, advocates a non-mono-clausal analysis, according to which the RDC is derived as follows:1 
This study presents the development and evaluation of pattern matching refinements (PMRs) to automatic code switching point (CSP) detection. With all PMRs, evaluation showed an accuracy of 94.51%. This is an improvement to reported accuracy rates of dictionary-based approaches, which are in the range of 75.22%-76.26% (Yeong and Tan, 2010). In our experiments, a 100sentence Tagalog-English corpus was used as test bed. Analyses showed that the dictionary-based approach using part-ofspeech checking yielded an accuracy of 79.76% only, and two notable linguistic phenomena, (1) intra-word code-switching and (2) common words, were shown to have caused the low accuracy. The devised PMRs, namely: (1) common word exclusion, (2) common word identification, and (3) common n-gram pruning address this and showed improved accuracy. The work can be extended using audio files and machine learning with larger language resources. 
Twitter is an online social networking, which has become an important source of information for marketing strategies and online reputation management. In this paper, we probe the problem of organization name disambiguation on twitter messages. This task is challenging due to the fact of lacking sufficient information both from organization and the tweets. We mine organization information from web sources to train a general classifier. Further, we mine tweets information. We train an adaptive classifier for a given organization name with more features derived from twitter messages labeled by the general classifier. The experiments on WePS-3 show mining web sources to enrich organization are effective. The adaptive classifier trained for a given organization is promising. 
 location. The EAT will be used to select the best  In this paper we report our work on a factoid question answering task that avoids namedentity recognition tool in the answer selection process. We use semantic analogical reasoning to ﬁnd the location of the ﬁnal answer from a textual passage.We demonstrate that without employing any linguistic tools during the answer selection process, our approach achieves a better accuracy than a typical factoid question answering architecture.  answer during the answer selection process, usually by utilizing a named-entity recognizer (NER) tool in a factoid QAS (Schlaefer et al., 2006). Different approaches have been used in order to improve the performance of the answer selection component. Ko et al. (2010) employed probabilistic models for answer ranking of NER-based answer selection by utilizing external semantic resources such as WordNet. More advanced techniques utilizing linguistic tools have been proposed in Sun et al. (2005), which uses  
This paper argues against the view that Japanese indirect passives are restricted with respect to the base verb that they take. Specifically, it argues that, despite its initial plausibility, the oft-proposed generalization that unaccusatives cannot appear in indirect passives is too strong and the alleged distribution is a tendency at most, albeit a strong one. After closely examining the restriction along with the counterevidence discussed in the literature, this paper presents novel empirical evidence against the restriction. Moreover, it also argues that no stipulations specific to indirect passives need to be introduced in accounting for the purported evidence for the unaccusative restriction: pragmatic inferences play a crucial role in deriving the observed aversion to unaccusativebased indirect passives. 
 sify the type of referent that is being counted, as in (1) and (2)*.  Numeral classifiers present a challenge to successful machine translation. We investigate two numeral classifier languages: Mandarin Chinese and Japanese. This paper presents a quantitative analysis of classifier translations between these two languages to better understand differences in classifier usage. Keywords – numeral classifier, sortal, translation, Mandarin Chinese, Japanese, contrastive linguistics 
In this paper we conduct a detailed examination of the tough construction in Japanese with the main focus on some types of nominative case particles ga. They are correlated with the difference not only in the nominativegenitive case alternation but also in the semantic or pragmatic interpretation. Based on these data, we discuss the categories of the nominative case particles and derivations for tough predicates within the framework of Combinatory Categorial Grammar. 
Public opinion analysis for micro-blog post is a new trend, and wherein emotional tendency analysis on micro-blog topic is a hot spot in the sentiment analysis. According to the characteristics of contents and the various relations of Chinese micro-blog post, we construct the dictionaries of sentiment words, internet slang and emoticons respectively, and then implement the sentiment analysis algorithms based on phrase path and the multiple characteristics for emotional tendency of micro-blog topics. Using micro-blogs’ forwarding, commentaries, sharing and so on, We take a future step to optimize the algorithm based on the multiple characteristics. According to the experimental results, our approach greatly improves the performance of emotional tendency identification on micro-blog topic. 
Product names with a temporal cue in a product review often refer to several product instances purchased at different times. Previous approaches to product entity recognition and temporal information analysis do not take into account such temporal cues and thus fail to distinguish different product instances. We propose to formulate the resolution of such product names as a classification problem by utilizing time expressions, event features and other temporal cues for a classifier in two stages, detecting the existence of such temporal cues and identifying the purchase time. The empirical results show that term-based features and existing event-based features together enhance the performance of product instance distinction. 
This paper describes the creation of a grammar to automatically detect agreement errors (gender and number) in Spanish texts written by Japanese learners. The grammar has been written using the Constraint Grammar formalism (Karlsson et al., 1995), and uses as input the morphosyntactic analysis provided by the Spanish parser HISPAL (Bick, 2006). For developing and testing the grammar, a learner corpus of 25,000 words has been manually annotated with agreement error tags. Both the grammar and the data from the corpus serve us to draw some conclusions about the characteristics of agreement errors in Japanese learners’ Spanish. 
Employing higher-order subtree structures in graph-based dependency parsing has shown substantial improvement over the accuracy, however suffers from the inefficiency increasing with the order of subtrees. We present a new reranking approach for dependency parsing that can utilize complex subtree representation by applying efficient subtree selection heuristics. We demonstrate the effectiveness of the approach in experiments conducted on the Penn Treebank and the Chinese Treebank. Our system improves the baseline accuracy from 91.88% to 93.37% for English, and in the case of Chinese from 87.39% to 89.16%. 1. Introduction In dependency parsing, graph-based models are prevalent for their state-of-the-art accuracy and efficiency, which are gained from their ability to combine exact inference and discriminative learning methods. The ability to perform efficient exact inference lies on the so-called factorization technique which breaks down a parse tree into smaller substructures to perform an efficient dynamic programming search. This treatment however restricts the representation of features to in a local context which can be, for example, single edges or adjacent edges. Such restriction prohibits the model from exploring large or complex  structures for linguistic evidence, which can be considered as the major drawback of the graphbased approach. Attempts have been made in developing more complex factorization techniques and corresponding decoding methods. Higher-order models that use grand-child, grand-sibling or trisibling factorization were proposed in (Koo and Collins, 2010) to explore more expressive features and have proven significant improvement on parsing accuracy. However, the power of higherorder models comes with the cost of expensive computation and sometimes it requires aggressive pruning in the pre-processing. Another line of research that explores complex feature representations is parse reranking. In its general framework, a K-best list of parse tree candidates is first produced from the base parser; a reranker is then applied to pick up the best parse among these candidates. For constituent parsing, successful results has been reported in (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). For dependency parsing, the efficient algorithms for produce K-best list for graph-based parsers have been proposed in (Huang and Chiang, 2005) for projective parsing and in (Hall, 2007) for nonprojective parsing; Improvements on dependency accuracy has been achieved in (Hall, 2007; Hayashi et al., 2011). However, the feature sets in these studies explored a relatively small context, either by emulating the feature set in the constituent parse reranking, or by factorizing the search space. A desirable approach for the K-best list reranking is to encode features on subtrees extracted from the candidate parse with arbitrary  308 26th Pa2c6CiﬁtohcpPAyarsicigaifhicCCt 2Aoo0nps1fiyae2rirCgebhnoytcn2Mefe0oro1en2SnLchbaeeynnoMg,nuDoaLaSgaiehsn,uegIknnue,faoDgKreama,iwsaIuntaikfohoenarmrKaaaan, wtdaionaCndhoaaSmrnaadpd, uaaCotnaodtKmioSupnrauodpthaaaatogisoehKnsiup3ra0og8he–as3sh13i708-317  orders and structures, as long as the extraction process is tractable. It is an open question how to design this subtree extraction process that is able to selects a set of subtrees which provides reliable and concrete linguistic evidence. Another related challenge is to design a proper back-off strategy for any structures extracted, since large subtree instances are always sparse in the training data. In this paper, we explore a feature set that makes fully use of dependency grammar, can capture global information with less restriction in the structure and the size of the subtrees, and can be encoded efficiently. It exhaustively explores a candidate parse tree for features from the most simple to the most expressive while maintaining the efficiency in the sense that it does not add additional complexities over the K-best parsing. We choose the K-best list reranking framework rather than the forest reranking in (Huang, 2008) because an explicit representation of parse trees is needed in order to compute the features for reranking. We implemented an edge-factored parser and a second-order sibling-factored parser which emulate models in the MSTParser described in (McDonald et al., 2005; McDonald and Pereira, 2006) as our base parsers. In the rest part of this paper, we first give a brief description of the dependency parsing, then we describe the feature set for reranking, which is the major contribution of this paper. Finally, we present a set of experiment for the evaluation of our method. 2. Dependency Parsing The task of dependency parsing is to find a tree structure for a sentence in which edges represent the head-modifier relationship between words: each word is linked to a unique “head” such that the link forms a semantic dependency while the main predicate of the sentence is linked to a dummy “root”. An example of dependency parsing is illustrated in Figure 1. A dependency tree is called projective if the links can be drawn on the linearly ordered words without any crossover. We will focus on projective trees throughout this paper. We formally define the dependency parsing task. Give a sentence , the best parse tree is obtained by searching for the tree with highest score:  ̃  ( )  ( ),  (1)  ROOT saw  man  John  .  the  there in  coat Figure 1. A dependency parse tree of the sentence “the man there in coat saw John.”  where ( ) is the search space of possible parse trees for , and is a parse tree in ( ) . A problem in solving equation (1) is that the number of candidates in the search space grows exponentially with the length of the sentence which makes the searching infeasible. A common remedy for this problem is to factorize a parse tree into small subtrees, called factors, which are scored independently. The score of parse tree under a factorization is the summation of scores of factors:  ( )∑  ( ) , (2)  where is a factor of . The search space can be therefore encoded in a compact form which allows dynamic programming algorithms to perform efficient exact inference. The score function for each factor is assigned as an inner product of a feature vector and a weight vector :  ( )  ( ).  (3)  The feature vector is defined on the factor which means it is only able to capture tree-structure information from a small context. This can be seen as the off-set for performing exact inference. The goal of training a parser is to learn a weight vector that assigns scores to effectively discriminate good parses from bad parses. We use the edge factorization and the sibling factorization models described in (McDonald et al., 2005; McDonald and Pereira, 2006) to construct our base parsers. We learn the weight vector by  309  applying the averaged perceptron algorithm (Collins, 2002) for its efficiency and stable performance. An illustration for generic perceptron algorithm is shown in Pseudocode 1.  Pseudocode 1: Generic perceptron learning  
In this paper, we proposed a Vietnamese named entity question answering (QA) model. This model applies an analytical question method using CRF machine learning algorithm combined with two automatic answering strategies: indexed sentences database-based and Google search engine-based. We gathered a Vietnamese question dataset containing about 2000 popular “Who, Whom, Whose” questions to evaluate our question chunking method and QA model. According to experiments, question chunking phase acquired the average F1 score of 92.99%. Equally significant, in our QA evaluation, experimental results illustrated that our approaches were completely reasonable and realistic with 74.63% precision and 87.9% ability to give the answers. Keywords: Vietnamese question, QA, VPQA, question analysis, answer extraction, question parser 
This paper introduces a new approach to solve the Chinese Pinyin-to-character (PTC) conversion problem. The conversion from Chinese Pinyin to Chinese character can be regarded as a transformation between two different languages (from the Latin writing system of Chinese Pinyin to the character form of Chinese,Hanzi), which can be naturally solved by machine translation framework. PTC problem is usually regarded as a sequence labeling problem, however, it is more difﬁcult than any other general sequence labeling problems, since it requires a large label set of all Chinese characters for the labeling task. The essential difﬁculty of the task lies in the high degree of ambiguities of Chinese characters corresponding to Pinyins. Our approach is novel in that it effectively combines the features of continuous source sequence and target sequence. The experimental results show that the proposed approach is much faster, besides, we got a better result and outperformed the existing sequence labeling approaches. 
Most of Japanese slang words such as Wakamono Kotoba are analyzed as “unknown word” or segmented wrongly by the morphological analysis system. These problems are causing negative effect on sentiment analysis in text. These words generally have many varieties of notations and conjugations, and they lack versatility. As a result, many of them are not registered in the dictionaries, making morphological analysis more difﬁcult. In this paper, we aimed to decrease such negative effects of Wakamono Kotoba for the accuracy of emotion estimation from sentence and proposed a method to increase the accuracy by using a classiﬁcation method based on machine learning. In this method we used emotional expressions which had high relevance with Wakamono Kotoba as feature. As a result, the proposed method obtained 20% higher accuracy than the method only using morpheme Ngram as feature. Emotion Corpus, Japanese Slang, Out of Vocabulary 
Unlike most Western languages, there are no typographic boundaries between words in written Japanese and Chinese. Word segmentation is thus normally adopted as an initial step in most natural language processing tasks for these Asian languages. Although word segmentation techniques have improved greatly both theoretically and practically, there still remains some problems to be tackled. In this paper, we present an effective approach in extracting Chinese and Japanese phrases without conducting word segmentation beforehand, using a sampling-based multilingual alignment method. According to our experiments, it is also feasible to train a statistical machine translation system on a small Japanese-Chinese training corpus without performing word segmentation beforehand.  (CWS) tools applied to the same Chinese sentence may lead to different results depending on their segmentation. For instance, 学生会 (pinyin: xue´ she¯ng hu`ı) in Chinese may be interpreted as 学生 会 ‘student(s) can (do)’ or 学生会 ‘Students’ Union’ respectively. Figure 1 gives an example of pre-segmented text and unsegmented text in both Chinese and Japanese. We applied four CWS tools: Urheen (Wang et al., 2010), ICTCLAS (Zhang et al., 2003) and Standford Chinese word segmenter (Tseng et al., 2005) trained on CTB and PKU. This example clearly shows that word segmentation tools may do harm to cross-lingual tasks, because: (i) there may be inconsistencies of segmentation results across languages such as different sizes of granularity in Japanese and Chinese;  
This paper presents methods for answering, what we call, Cross-passage Evidence Questions. These questions require multiply scattered passages all bearing different and partial evidence for the answers. This poses special challenges to the textual QA systems that employ information retrieval in the “conventional” way because the ensuing Answer Extraction operation assumes that one of the passages retrieved would, by itself, contain sufﬁcient evidence to recognize and extract the answer. One method that may overcome this problem is factoring a Cross-passage Evidence Question into constituent sub-questions and joining the respective answers. The ﬁrst goal of this paper is to develop and put this method into test to see how indeed effective this method could be. Then, we introduce another method, Direct Answer Retrieval, which rely on extensive pre-processing to collect different evidence for a possible answer off-line. We conclude that the latter method is superior both in the correctness of the answers and the overall efﬁciency in dealing with Crosspassage Evidence Questions.  snippet of continuous text, or passage, that supports or justiﬁes an answer to the question posed. More practically, in factoid QA, a piece of evidence is a text span with two properties: (1) An Information Retrieval (IR) procedure can recognise it as relevant to the question and (2) an automated Answer Extraction (AE) procedure can extract from it an answerbearing expression (aka an answer candidate). With respect to a given corpus, we call questions with the following property Single Passage Evidence Questions or SEQs: A question Q is a SEQ if evidence E sufﬁcient to select A as an answer to Q can be found in the same text snippet as A. In contrast, we call a question that requires multiple different pieces of evidence (in multiple text spans with respect to a corpus) a Cross-passage Evidence Question or CEQ: A question Q is a CEQ if the set of evidence E1, ..., En needed to justify A as an answer to Q cannot be found in a single text snippet containing A, but only in a set of such snippets.  
Paraphrase generation in any language has gained much attention and importance in the study of Natural Language Processing. Therefore, the focus of this paper is on Thai language paraphrase generation for the sentence level. Six sentence paraphrasing techniques for Thai are proposed and illustratively explained. In addition, the Thai–sentence Paraphrase Generation (TPG) system is designed using a lexical resource based system subsequently entitled the Thai Lexical Conceptual Structure with Thai Lexicalized Tree Adjoining Grammar (TLCS–TLTAG) Resource. 
 morph, Parse structure etc.) is required in both sta-  tistical as well as rule based anaphora resolution sys-  In this paper, we propose a scheme for anaphora annotation in Hindi Dependency Treebank. The goal is to identify and handle the challenges that arise in the annotation of reference relations in Hindi. We identify some of the issues related to anaphora annotation speciﬁc to Hindi such as distribution of markable span, sequential annotation, representation format, annotation of multiple referents etc. The scheme hence incorporates some characteristics speciﬁc to these issues in order to achieve a consistent annotation. Most signiﬁcant among these characteristics is the head-modiﬁer separation in referent selection. The modiﬁer-modiﬁed dependency relations inside a markable is utilized for this headmodiﬁer distinction. A part of the Hindi De-  tems. Various corpus based studies of anaphoric variation also make use of such a corpus. While a signiﬁcant number of corpora with anaphora annotation for English and other languages like Spanish, Czech etc. are available, for Indian languages, such corpora are scarce. With a view of developing an Anaphora Resolution system in Hindi, our project aims at extending the dependency annotated (Hindi Dependency TreeBank) corpus with anaphoric relations. Hence we propose an anaphora annotation scheme in accordance with the representation format (SSF)(Bharati et al., 2007) of the Treebank, that uses attributevalue pairs to represent linguistic information. In this scheme, we attempt to address some of the  pendency Treebank, of around 2500 sentences has been annotated with anaphoric relations and an inter-annotator study was carried out which shows a signiﬁcant agreement over selection of the head referent using the proposed scheme as compared to MUC annotation format. The current annotation is done for a limited set of pronominal categories.  issues that are commonly faced while annotating anaphora and require efﬁcient handling. Although the scheme is developed while keeping in view the structure of the Dependency Tree-Bank, it is convertible to other formats of annotation as well. In recent years, due to increasing interest in development of statistical systems for anaphora resolution, there have been signiﬁcant attempts for cre-  
 keeping the strengths of phrases, while incorporat-  ing syntax into SMT. Some approaches have been  Reordering is of essential importance for phrase based statistical machine translation (SMT). In this paper, we would like to present a new method of reordering in phrase based SMT. We inspired from (Xia and McCord, 2004) using preprocessing reordering approaches. We used shallow parsing and transformation rules to reorder the source sentence. The experiment results from English-Vietnamese pair showed that our approach achieves signiﬁcant improvements over MOSES which is the state-of-the art phrase based system.  applied at the word-level (Collins et al., 2005). They are particularly useful for language with rich morphology, for reducing data sparseness. Other kinds of syntax reordering methods require parser trees , such as the work in (Quirk et al., 2005; Collins et al., 2005; Huang and Mi, 2010). The parsed tree is more powerful in capturing the sentence structure. However, it is expensive to create tree structure, and building a good quality parser is also a hard task. All the above approaches require much decoding time, which is expensive. The approach we are interested in here is to bal-  ance the quality of translation with decoding time.  
Mainstream word sense disambiguation systems have relied mostly on supervised approaches. Complex interactions have been observed between learning algorithms and knowledge sources, but the factors underlying such phenomena are underexplored. This calls for more qualitative analysis of disambiguation results, possibly from an inter-disciplinary perspective. The current study thus preliminarily explores the relation between sense concreteness and the linguistic means for sense distinction with reference to the context availability model proposed in psycholinguistics and common practice in corpus-based lexicography. It will be shown that to a certain extent the varied usefulness of individual knowledge sources for target words, nouns in particular, may be related to the concreteness of the meanings concerned, which predicts how the sense is distinguished from other senses of the word in the first place. A better understanding of this relation is expected to inform the design of disambiguation systems which could then combine algorithms and knowledge sources in a genuine lexically sensitive way. 
In this paper, we make a distinction between the de se and non-de se interpretations of first person indexicals and Chinese reflexive ziji. Based on the distinction, we discuss the relationship between these expressions in Chinese, and point out the problems with Wechsler's (2010) de se theory of person indexicals as well as the inappropriateness of characterizing Chinese long-distance ziji as a logophor. 
Existing treebanks of Mandarin Chinese such as the Sinica Treebank, the Harbin Institute of Technology Treebank, and the Penn Chinese Treebank, parse Chinese serial verb constructions incorrectly or inconsistently in terms of headedness, i.e. which verb to be assigned with the label of syntactic and/or semantic “head”. Aspectual markers in serial verb constructions can help determine the head of these constructions (Li, 1991; among others). However, the majority of Chinese serial verb constructions do not have overt aspectual markers. Based on large-scale corpus studies, this work investigates the distribution of aspectual markers in Chinese serial verb constructions in order to explore which verb in the serial verbs is more likely to function as the head, and thus provides a reference for parsing serial verb constructions without overt aspectual markers. We find that contrary to previous studies such as Collins (1997), Law and Veenstra (1992) and Sebba (1987) that treat the first verb in a serial verb construction as the head, Chinese serial verb constructions more often have the second verb as the head. The results of this work can not only serve as a reference for automatic parsing of Chinese data, but also shed light on theoretical studies of the structure of serial verb constructions in Chinese and other serial verb languages. 
Japanese dare-mo has been widely acknowledged to be an NPI, furthermore, a “strict” NPI in the sense of Giannakidou (2011) as it seems to be licensed only in an “antiveridical” environment, specifically, with a clausemate negation. However, there is a type of positive sentences in which dare-mo can appear, i.e. non-episodic sentences, which indicates that dare-mo is in fact not an NPI and its NPI-like distribution is an epiphenomenon due to dare-mo’s lexical meaning and the resulting interpretational properties of dare-mo sentences. In the current work, based on novel data we will propose that dare-mo is an “unrestricted” universal quantifier and demonstrate that the proposed meaning of dare-mo and a reasonable assumption about episodic predicates predict that positive episodic dare-mo sentences will be contradictory while negative episodic ones and nonepisodic ones, positive or negative will be contingent, nicely characterizing the grammaticality facts of dare-mo sentences.  
 nominative subject for all kinds of intransitive verbs  In this paper, we introduce a tripartite scheme for the classiﬁcation of intransitive verbs for Hindi and claim it to be a more suitable model of classiﬁcation than the classical binary unac-  whereas Hindi uses ergative case marker ‘ne’ on subject when the verb is unergative and in perfect tense whereas unaccusative doesn’t as exempliﬁed in (1a) and (1b) respectively.  cusative/unergative classiﬁcation. We develop a multi-class SVM classiﬁer based model for automatic classiﬁcation of intransitive verbs into proposed tripartite classes. We rank the unaccusative diagnostic tests for Hindi based on their authenticity in attesting an intransitive verb under unaccussative class. We show that the use of the ranking score in the feature of the classiﬁer improves the efﬁciency of the classiﬁcation model even with a small amount of data. The empirical result illustrates the fact that judicious use of linguistic knowledge builds a better classiﬁcation model than the one that is purely statistical.  (1) a. English: Ram ran a lot.  Hindi: raam-ne khub  dauRaa.  Ram-erg very much run-3 pft  b. English: The glass broke.  Hindi: glaas TuT-aa. Glass break-3 pft  Classifying intransitive verbs of (1a) and (1b) into subclasses can result in producing right case marking on the subject in the target language Hindi. In parsing, identifying the subclass of the intransitive  
There are some cases where the nonJapanese buyers are unable to find products they want through the Japanese shopping Web sites because they require Japanese queries. We propose to transliterate the inputs of the non-Japanese user, i.e., search queries written in English alphabets, into Japanese Katakana to solve this problem. In this research, the pairs of the nonJapanese search query which failed to get the right match obtained from a Japanese shopping website and its transcribed word given by volunteers were used for the training data. Since this corpus includes some noise for transliteration such as the free translation, we used two different filters to filter out the query pairs that are not transliterlated in order to improve the quality of the training data. In addition, we compared three methods, BIGRAM, HMM, and CRF, using these data to investigate which is the best for the query transliteration. The experiment revealed that the HMM was the best. 
 or human social intention detection (Jurafsky et al.,  We consider the task of classifying chat contributions by dialogue act in a multi-party setting. This extends the problem signiﬁcantly over the 1-1 chat scenario due to the semiasynchronous and “entangled” nature of the contributions by chat participants. We experiment with a number of machine learning approaches, using different categories of features: lexical, contextual, structural, keyword and dialogue interaction information. For evaluation, we developed gold-standard data using online forums from the USA Library of Congress. We found that, for multi-party di-  2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has received growing attention since chat services and similar applications have gained popularity as a communication method. However, the majority of previous work on dialogue act classiﬁcation for dialogue has been carried out over spoken dialogue. Although spoken and written dialogue have similarities, they have distinct features which make it difﬁcult to reuse existing methods for live chats. For example, spoken dialogue in-  alogues, features based on 1-gram and keywords produced best performance, while features exploiting structure and interaction did not perform as well as previously reported results over 1-to-1 chats.  troduces difﬁculties due to errors inherent in speech recognition output, but allows acoustic and prosodic features to be leveraged (e.g. Stolcke et al. (2000)). Conversely, live chats introduce other types of complications, including ill-formed data and entangle-  
This paper proposes a syntax-semantics correspondence of locative expressions: This proposal is based on the syntactic hierarchy among three locative structures (PPs, VPs, and verbal affixes) and the semantic hierarchy among four locative arguments (Goal, Source, Symmetric Path, Stative Location). As for the syntactic hierarchy, the verbal affixes are closer to the head verb than the locative/path verbs are, and the locative/path verbs than the locative PPs. As for the semantic hierarchy, the following four arguments form a hierarchy due to their semantic closeness to the motion event: Goal > S-Path > Source > St-Location. (cf. Nam 1995, 2004) We argue for this correspondence claim by identifying some crucial typological implications holding between the syntactic/semantic hierarchies. 
We present a case study on applying common methods for the prediction of lexical properties to a low-resource language, namely Wambaya. Leveraging a small corpus leads to a typical high-precision, low-recall system; using the Web as a corpus has no utility for this language, but a machine learning approach seems to utilise the available resources most effectively. This motivates a semi-supervised approach to lexicon extension. 
 two respects: (1) the lack of socio-cultural (meta-)  This study aims to propose a novel pipeline architecture in building and analyzing largescaled linguistic data on the cloud-based environment, an experimental survey on Chinese Polarity Lexicon will be taken as an example. In this experiment, data are evaluated and tagged by applying crowd sourcing approach using online Google Form. All the data processing and analyzing procedures are completed on-the-ﬂy with free cloud services automatically and dynamically.The paper shows the advantages of using cloud-based environment in collecting and processing linguistic data which can be easily scaled up and efﬁciently computed. In addition, the proposed pipeline architecture also brings out the potentials of merging with mashups from the web for representing and exploring corpus data of various types.  information reﬂected in the data, is incompetent for pragmatic usages and discourse analysis; (2) rather skewed with data in the public domain, heterogeneity of (individualized) language usages and development is not able to be traced. With the advanced technological progress in data availability with storage and computing ability, the issues mentioned can be tackled to a great extent. We take it as the turning point for corpus-based linguistics to transform into a data-intensive and cloudbased linguistics. In light of that, we want to explore the transformation viability in this paper. As a ﬁrst step, we present a novel pipeline architecture to build Chinese Polarity Lexicon on the cloud environment by taking the data from the web as resource. Polarity lexicon contains sentiment-bearing words and phrases, encoded with polarities to each word or phrase, usually either assigned as positive or  
Among various types of recent information explosion, that in news stream is also a kind of serious problems. This paper studies issues regarding topic modeling of information ﬂow in multilingual news streams. If someone wants to ﬁnd differences in the topics of Japanese news and Chinese news, it is usually necessary for him/her to carefully watch every article in Japanese and Chinese news streams at every moment. In such a situation, topic models such as LDA (Latent Dirichlet Allocation) and DTM (dynamic topic model) are quite effective in estimating distribution of topics over a document collection such as articles in a news stream. Especially, as a topic model, this paper employs DTM, but not LDA, since it can consider correspondence between topics of consecutive dates. Based on the results of estimating distribution of topics in Japanese / Chinese news streams, this paper proposes how to analyze cross-lingual alignment of topics in time series Japanese / Chinese news streams. 
This paper presents a conditional random fields based labeling approach to Chinese punctuation prediction. To this end, we first reformulate Chinese punctuation prediction as a multiple-pass labeling task on a sequence of words, and then explore various features from three linguistic levels, namely words, phrase and functional chunks for punctuation prediction under the framework of conditional random fields. Our experimental results on the Tsinghua Chinese Treebank show that using multiple deeper linguistic features and multiple-pass labeling consistently improves performance. 
In our paper we focus on analysing textual information usage (selected politeness factors of speech act) in mother tongue and in foreign language to identify phenomena of a language consciousness transfer from the mother tongue into a foreign language communication – transference phenomena – and their impact on textual structures of politeness in chosen languages. Our aim was to make an analysis of request texts written in English, Spanish and Slovak language, where we examined the occurrence of keywords, in our case the factors of politeness in mother tongue (Slovak) and in foreign languages (English and Spanish). We examined the formulation of requests made by two different groups, requests formulated by linguists - Slovak students studying English as their major subject - on one side, and the requests formulated by non-linguists - Slovak students studying Economy, with the knowledge of Spanish, - on the other side. We used cross-tabulation analysis and association rule analysis as our research methods. The findings are interesting mainly in terms of differences in the use of politeness factors in English and Slovak language, and also the concordance in the use of politeness factors in Slovak and Spanish texts of requests. 
Most set expansion algorithms assume to acquire new instances of different semantic categories independently even when we have seed instances of multiple semantic categories. However, in the setting of set expansion with multiple semantic categories, we might leverage other types of prior knowledge about semantic categories. In this paper, we present a method of set expansion when ontological information related to target semantic categories is available. More speciﬁcally, the proposed method makes use of sibling relations between semantic categories as an additional type of prior knowledge. We demonstrate the effectiveness of sibling relations in set expansion on the dataset in which instances and sibling relations are extracted from Wikipedia in a semi-automatic manner. 
 web pages as the target documents of annotation, we  build a Japanese annotated corpus that consists of  In these days, semantic analysis has been actively studied in natural language processing. For the study of semantic analysis, corpora with semantic annotations are essential. Although there are such corpora annotated on newspaper articles, there are various genres and styles, including linguistic expressions that are not found in newspaper articles. In this paper, we build a diverse document leads corpus annotated with semantic relations. To reduce the workload of annotators and annotate as many various documents as possible, we restrict the annotation target of each document to only the ﬁrst three sentences. We have completed building a corpus of 1,000 documents and report the statistics of this corpus.  various genres. We annotate predicate-argument structures and anaphoric relations as semantic relations. We illustrate these relations and annotations in Example (1)1. “A←rel:B” represents annotating B to A with relation rel. In the following examples, we sometimes omit annotations that are not related to the discussion.  (1) a. ଠ࿠͸ ࣌‫ܭ‬Λ ങͬͨɻ Taro-TOP watch-ACC bought. ‘Taro bought a watch.’ (ങͬͨ ← GA:ଠ࿠, WO:࣌‫)ܭ‬  b. ఋʹ  ͦΕΛ ͋͛ͨɻ  Little brother-DAT it-ACC gave  
There are many languages considered to be low-density languages, either because the population speaking the language is not very large, or because insufﬁcient digitized text material is available in the language even though millions of people speak the language. Bangla is one of the latter ones. Readability classiﬁcation is an important Natural Language Processing (NLP) application that can be used to judge the quality of documents and assist writers to locate possible problems. This paper presents a readability classiﬁer of Bangla textbook documents based on information-theoretic and lexical features. The features proposed in this paper result in an F-score that is 50% higher than that for traditional readability formulas. 
 compounds (Baldwin and Tanaka, 2004). (2) Com-  pounding is a recursive process that can lead to for-  Understanding and interpretation of nominal compounds has been a long-standing area of interest in NLP research for various reasons. (1) Nominal compounds occur frequently in most languages. (2) Compounding is an extremely productive word formation phenomenon. (3) Compounds contain implicit semantic relations between their constituent nouns. Most approaches that have been proposed so far concentrate on building statistical models using machine learning techniques and rely on large-scale, domain-speciﬁc or open- domain knowledge bases. In this paper we present a novel approach that combines the use of lexical hierarchies such as PurposeNet and WordNet, with WordNet-based similarity measures for the interpretation of domainspeciﬁc nominal compounds. We aim at building a robust system that can handle most of the commonly occurring English bigram nominal compounds within the domain.  mation of large and complex compounds, that are difﬁcult for comprehension. (3) Compounds usually carry an implicit meaning that may sometimes differ signiﬁcantly from that of the combining concepts. Consider the example of a garden knife. A garden knife is interpreted as a knife used in the garden. Here the modiﬁer garden modiﬁes the locative information of the head noun knife. Alternatively, consider the example of a gamma knife. A gamma knife is a device used to treat brain tumors by administering gamma radiations in a particular manner. Here, the modiﬁer does not necessarily modify the head, instead they both combine together to denote a different concept. This understanding of the difference in the structure and purpose between gamma knife and other kinds of knife cannot be achieved by any means of statistical predictions or morphological and syntactic analyses of the compound. The most common representations adopted for the  interpretation of nominal compounds involve an in-  
 Bod, 2006; Seginer, 2007; Cohn et al., 2009; Ponvert  The Constituent-Context Model (CCM) achieves promising results for unsupervised grammar induction. However, its performance drops for longer sentences. In this paper, we describe a general feature-based model for CCM, in which linguistic knowledge can be easily integrated as features. Features take the log-linear form with local normalization, so the Expectation-Maximization (EM) algorithm is still applicable to estimate model parameters. The ℓ1-norm is used to control the model complexity, leading to sparse and compact grammar. We also propose to use a separated development to perform model selection and an additional test set to evaluate the performance. Under this framework, we could automatically choose suitable model parameters rather than setting them empirically. Experiments on the English treebank demonstrate that the feature-based model achieves comparable performance on short sentences but signiﬁcant improvement on longer sentences.  et al., 2011), or the dependency grammars (Klein and Manning, 2004; Headden III et al., 2009; Cohen and Smith, 2009; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Among these approaches, the Constituent Context Model (CCM) (Klein and Manning, 2002; Klein, 2005) is a simple but effective generative model for unsupervised constituency grammar induction. Speciﬁcally, the sequences (the contents enclosed by spans) and contexts (the preceding and following words) are directly modelled in CCM. The Expectation-Maximization (EM) algorithm is used to estimate parameters to optimize the data likelihood. Although the CCM achieves promising results on short sentences, its performance drops for longer sentences. There are two possible reasons: (1) CCM models all constituents under only single multinomial distributions, which can not capture the detailed information of span contents; and (2) long sequences only occur a few times in the training corpus, so the probability estimation highly depends on smoothing. Another problem of original CCM and  
We present larger-scale evidence overturning previous results, showing that among the many alternative phrasal lexical similarity measures based on word vectors, the Jaccard coefﬁcient most increases the robustness of MEANT, the recently introduced, fully-automatic, state-of-the-art semantic MT evaluation metric. MEANT critically depends on phrasal lexical similarity scores in order to automatically determine which semantic role ﬁllers should be aligned between reference and machine translations. The robustness experiments were conducted across various data sets following NIST MetricsMaTr protocols, showing higher Kendall correlation with human adequacy judgments against BLEU, METEOR (with and without synsets), WER, PER, TER and CDER. The Jaccard coefﬁcient is shown to be more discriminative and robust than cosine similarity, the Min/Max metric with mutual information, Jensen Shannon divergence, or the Dice’s coefﬁcient. We also show that with Jaccard coefﬁcient as the phrasal lexical similarity metric, individual word token scores are best aggregated into phrasal segment similarity scores using the geometric mean, rather than either the arithmetic mean or competitive linking style word alignments. Furthermore, we show empirically that a context window size of 5 captures the optimal amount of information for training the word vectors. The combined results suggest a new formulation of MEANT with signiﬁcantly improved robustness across data sets. 
Natural and non-natural kinds have significant differences. This paper explores the subclasses of each kind and establishes the type system for event nouns. These nouns are divided into natural types, artifactual types, complex types (including natural complex types and artifactual complex types). This new classification not only enriches the Generative Lexicon theory, but also helps us to capture the properties of different types of event nouns. 
 are underlined while the semantic subjects of resultative phrases are in bold.)  The present paper attempts to formalize the semantic interpretation of resultative phrases in Japanese in the framework of Generative Lexicon, with a focus on the semantic subject of resultative phrases, i.e. the entity which resultative phrases are predicated of. The semantic subject cannot always be identiﬁed with the direct object of transitive verbs or the subject of unaccusative verbs, as generally believed, but also is expressed as an oblique NP or not syntactically expressed at all. It poses a challenge to the interpretation of resultative phrases since it cannot be tied to a speciﬁc syntactic constituent. The interpretation of resultative phrases is encoded in terms of the FORMAL quale and its argument built through the co-composition operation. 
 operation of type coercion, which is defined as follows (Pustejovsky, 1995: 111).  Unlike its English equivalent after, which often takes NP complement, Chinese temporal connective hou tends to take VP complement. In terms of type coercion, while after seems to generally license event coercion, Chinese hou does not (with a few exceptions), as in most cases the presence of a verb is required for the houconstruction (and the sentence) to be correct. Rather than attributing this difference to the different lexicalization of nouns in these two languages, this paper argues that it is due to the difference between hou and after. In particular, hou is weaker in its coercion force than after because of its polysemy. It is either a temporal connective or a locative connective. 
This paper suggests that Generative Lexicon Theory (Pustejovsky, 1995, 2006, 2011) offers a new analysis of numeral classifiers, focusing on Japanese having various kinds of classifiers. It is often said that classifiers agree with quantified nouns, that is, the nouns have to match the semantic requirements of the classifiers. This paper examines their lexical structures and compositional mechanisms. Though Huang and Ahrens (2003) explain the compositional mechanisms between the classifiers and the quantified nouns using “coercion” instead of the agreement, this paper indicates that other mechanisms including Type Matching (Pustejovsky, 2011) also occur in Japanese depending on the type required by the classifier and the source type of the quantified noun, following Mano and Yonezawa’s (to appear) suggestion. 
This paper is concerned with characterizing psych-predicates in Korean and possibly in Japanese in the GL spirit. We focus on the the status of the Experiencer (or ‘judge’) in relation to other arguments and examine the firstperson subjectivity data (constraint). The relevant cause and effect relation and consequent coerced event function is postulated for coherent interpretation. Keywords: psych-predicates, experiencer, first-person (subjectivity) data, causation.  2 Data and Issues Consider (1) (Lee 2010), where description of psych state in the present tense by the firstperson but not by the third or second person is acceptable. Here the ‘judge’ is the speaker. This first-person subjectivity constraint is observed in Korean and Japanese. (1) na/?*ku/?*ne -nun ecirep-ta I/he/you -TOP dizzy-DEC ‘I am/?*he is/?*you are dizzy.’ (2) watashi-wa/?*kare-wa/?*anata-wa sabishi ělonelyĜ desu ‘I am/?*he is/?*you are lonely.’  
This paper investigates the types and the developmental trajectory of noun modifying constructions (NMCs), in the form of [Modifier + de + (Noun)], attested in Mandarin-speaking children’s speech from a semantic perspective based on the generative lexicon framework (Pustejovsky, 1995). Based on 1034 NMCs (including those traditionally defined as relative clauses (RCs)) produced by 135 children aged 3 to 6 from a cross-sectional naturalistic speech corpus “Zhou2” in CHILDES, we analyzed the relation between the modifier and the head noun according to the 4 major roles of qualia structure: formal, constitutive, telic and agentive. Results suggest that (i) NMCs expressing the formal facet of the head noun’s meaning are most frequently produced and acquired earliest, followed by those expressing the constitutive quale, and then those expressing the telic or the agentive quale; (ii) RC-type NMCs emerge either alongside the other nonRC type NMCs at the same time, or emerge later than the other non-RC type NMCs for the constitutive quale; and (iii) the majority of NMCs expressing the agentive and telic quales are those that fall within the traditional domain of RCs (called RC-type NMCs here), while the majority of NMCs expressing the formal and the constitutive quales are non-RC type NMCs. These findings are consistent with: (i) the semantic nature and complexity of the four  qualia relations: formal and constitutive aspects of an object (called natural type concepts in Pustejovsky 2001, 2006) are more basic attributes, while telic and agentive (called artificial type concepts in Pustejovsky 2001, 2006) are derived and often eventive (hence conceptually more complex); and (ii) the properties of their adult input: NMCs expressing the formal quale are also most frequently encountered in the adult input; followed by the constitutive quale, and then the agentive and telic quales. The findings are also consistent with the idea that in Asian languages such as Japanese, Korean and Chinese, RCs develop from attributive constructions specifying a semantic feature of the head noun in acquisition (Diessel 2007, c.f. also Comrie 1996, 1998, 2002). This study is probably the first of using the generative lexicon framework in the field of child language acquisition. 
 (3) cause-effect relation with non-natural phenomenon  [apeci-ka  so-lul phal-un] ton 
In the history of Artificial Intelligence (AI), Turing Test, a question answering imitation game was proposed to determine whether the computer system has intelligence. It becomes the ultimate goal to answer all the natural language questions for generations of AI researchers. In the past century, AI changed tremendously from its theories to its applications, while with this goal unchanged. Especially in the past 20 years, along with the development of the Internet, computers have the ability to acquire, store and process huge volumes of data, which makes the AI-related techniques deeply involve themselves in the domain of intelligent information processing. On one hand, Question Answering develops in theories, models and methods with the combination of the large scale data processing. On the other hand, the next-generation information service engines are expected to integrate Question Answering as an important part to retrieve and display information, where knowledge is important for information accumulation, understanding and serving. This presentation will present the history and development of the Question Answering, its related key technologies and applications in the background of big data and AI. QA: 从图灵测试到智能信息服务 图灵实验（Turing Test）可知,回答自然问题的能力有史以来就是衡量计算机系统是否具有 智能的基本标准。半个世纪以来，人工智能从理念到内容发生了巨大的变化，尤其是近 20 年 来随着互联网产业的发展，大规模数据获取和计算能力的提高使得人工智能的相关技术在智能 信息处理领域中得到了充分体现。 一方面，人工智能和大规模数据处理的结合，对于问答系 统在理论、模型和方法上都有了质的飞跃和发展，另一方面，在下一代信息服务引擎的发展理 念中，问答系统也成为信息获取与展现的重要手段，知识也成为网络信息积累与服务的重要支 撑。本报告将介绍问答系统的发展历史与现状，以及相关的关键技术与应用。 About the Speaker Xiaoyan Zhu, professor, she got bachelor degree at University of Science and Technology Beijing in 1982, master degree at Kobe University in 1987, and Ph. D. degree at Nagoya Institute of Technology, Japan in 1990. She is teaching at Tsinghua University since 1993. She is director of state key lab of intelligent technology and systems, director of Tsinghua-HP Joint research center and the director of Tsinghua-Waterloo Joint research center, Tsinghua University. She is International Research Chair holder of IDRC, Canada, from 2009. She was deputy head of Department of Computer Science and Technology, Tsinghua University from 2004-2007. Her research interests include intelligent information processing, machine learning, natural language processing, query and answering system and bioinformatics. She has authored more than 100 peer-reviewed articles in leading international conferences including SIGKDD, IJCAI, AAAI, ACL, ICDM, CIKM, COLING, and journals including Int. J. Medical Informatics, Bioinformatics, BMC Bioinformatics, Genome Biology and IEEE Trans. on SMC. 
Globalization and multilingualism contribute to code-switching – the phenomenon in which speakers produce utterances containing words or expressions from a second language. Processing code-switched sentences is a significant challenge for multilingual intelligent systems. This study proposes a language modeling approach to the problem of codeswitching language processing, dividing the problem into two subtasks: the detection of code-switched sentences and the identification of code-switched words in sentences. A codeswitched sentence is detected on the basis of whether it contains words or phrases from another language. Once the code-switched sentences are identified, the positions of the code-switched words in the sentences are then identified. Experimental results on MandarinTaiwanese code-switching sentences show that the language modeling approach achieved a 79.52% F-measure and an accuracy of 80.23% for detecting code-switched sentences, and a 51.20% F-measure for the identification of code-switched words. 
Chinese word structure annotation is potentially useful for many NLP tasks, especially for Chinese word segmentation. Li and Zhou (2012) have presented an annotation for word structures in the Penn Chinese Treebank. But they only consider words that have productive affixes, which covers 35% of word types in that corpus. In this paper, we propose a linguistically inspired annotation that covers various morphological derivations of Chinese in a more general way, such that almost all multiple-character words can be structurally analyzed. As manual annotation is expensive, we propose a semi-supervised approach to automatic annotation, which combines the maximum entropy learning and the EM iteration for the Gaussian mixture model. The proposed method has achieved an accuracy of 90% on the testing set. 
In this paper, we present a Chinese lexical taxonomy, a hierarchically organization of Chinese lexical classes of nouns, verbs and adjectives. We first describe the structure of this taxonomy and then present the methods we used to build it. The distinctive characteristics of this lexical taxonomy are: 1) we use definition frame to describe each lexical class, as well as its members, 2) the lexical classes for nouns, verbs and adjectives are inter-connected. We also compare this taxonomy with the Chinese Proposition Bank, to look for possible ways to link these two independently developed language resources. 
The CIPS-SIGHAN CLP 2012 Chinese Word Segmentation on MicroBlog Corpora Bakeoff was held in the autumn of 2012. This bake-off task of Chinese word segmentation is focused on the performance of Chinese word segmentation algorithms on MicroBlog corpora. 17 groups submitted 20 results, among which the best system has all the P, R and F values near 95%, and the average values of the 17 systems are 0.8931, 0.8981 and 0.8953, respectively. 
This paper describes the model we designed for the word segmentation bakeoff on Chinese micro-blog data in the 2nd CIPS-SIGHAN joint conference on Chinese language processing. We presented a linear-time incremental model for word segmentation where rich features including character-based features, word-based features as well as other possible features can be easily employed. We report the performances of our model on four datasets in the SIGHAN bake-off 2005. After adding more features designed for the micro-blog data, the performance of our model is further improved. The F-score of our model for this bake-off is 0.9478 and 44.88% of the sentences are segmented correctly. 
This paper presents a Chinese Word Segmentation system on MicroBlog corpora for the CIPS-SIGHAN Word Segmentation Bakeoff 2012. Our system employs Conditional Random Fields (CRF) as the segmentation model. To make our model more adaptive to MicroBlog, we manually analyze and annotate many MicroBlog messages. After manually checking and analyzing the MicroBlog text, we propose several pre-processing and post-processing rules to improve the performance. As a result, our system obtains a competitive F-score in comparison with other participating systems. 
In this paper, we proposed a Chinese word segmentation model for micro-blog text. Although Conditional Random Fields (CRFs) models have been presented to deal with word segmentation, this is still the first time to apply it for the segmentation in the domain of Chinese micro-blog. Different from the genres of common articles, micro-blog has gradually become a new literary with the development of Internet. However, the unavailable of microblog training data has been the obstacle to develop a good segmenter based on trainable models. Considering the linguistic characteristics of the text, we proposed some methods to make the CRFs models suitable for segmentation in the domain of micro-blog. Several experiments have been conducted with different settings and then an optimal tagging method and feature templates have been designed. The proposed model has been implemented for the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing Bakeoff (Bakeoff-2012) and achieves a very high Fmeasure of 93.38% within the test set of 5,000 micro-blog sentences. One of our main contri-  butions is the online version of toolkit1, which provides segmentation service for Chinese micro-blog text. 
The state-of-the-art Chinese word segmentation systems have achieved high performance on well-formed long document. However, the segmentation for microblog is difficult due to the noise problem and the OOV problem. In this paper, we present a Chinese Micro-Blog Segmentation system for the CIP-SIGHAN Word Segmentation Bakeoff 2012 track. The proposed system adopts a cascaded approach which contains three steps, correspondingly the preprocessing, the word segmentation and the post-processing. In the preprocessing step, the noise which contains the special characters is processed and removed. The remaining sentences are segmented in the second step. Finally, we use the dictionary to detect the OOVs which are not correctly segmented. The results show the competitive performance of our approach. 
We describe two adaptation strategies which are used in our word segmentation system in participating the Microblog word segmentation bake-off: Domain invariant information is extracted from the in-domain unlabelled corpus, and is incorporated as supplementary features to conventional word segmenter based on Conditional Random Field (CRF), we call it statistic-based adaptation. Some heuristic rules are further used to post-process the word segmentation result in order to better handle the characters in emoticons, name entities and special punctuation patterns which extensively exist in micro-blog text, and we call it rule-based adaptation. Experimentally, using both adaptation strategies, our system achieved 92.46 points of F-score, compared with 88.73 points of F-score of the unadapted CRF word segmenter on the pre-released development data. Our system achieved 92.51 points of F-score on the ﬁnal test data. 
With the developments of Web2.0, the process for the data on Internet becomes necessary. This Paper reports our work for Chinese weibo segmentation in the 2012 CIPS-SIGHAN bakeoff. In order to improve the recognition accuracy of out-ofvocabulary words, we propose a cascaded model which ﬁrst segments and disambiguates in-vocabulary words, then recovers out-of-vocabulary words from the fragments. Both the two process are trained by a character-based CRFs model with useredited external vocabulary. The ﬁnal performance on the test data shows that our system achieves a promising result. 
In this evaluation, we have taken part in the task of the Word Segmentation on Chinese MicroBlog. In this task, after analysing the feature of the MicroBlog and the result of our original Chinese word segmentation system, four Optimization Rules are proposed to optimize the segmentation algorithm for Chinese word segmentation on MicroBlog corpora. The optimized segmentation system is based on character-based and word-based Conditional Random Fields (CRFs). Experiments show that the optimized segmentation system can obviously improve the performance of CWS on MicroBlog corpora.  
Chinese word segmentation (CWS) lays the essential foundation for Mandarin Chinese analysis. However, its performance is always limited by the identification of unknown words, especially for short text such as Microblog. While local context are helpless in handling unknown words, global context do manifest enough contextual information, and could be used to guide CWS process. Based on this motivation, in this paper, we report our attempt toward building an integrated model in semi-supervised manner. Considering the complexity of model, we design a strategy to manipulate global and local contextual information asynchronously. Though the coverage of unknown words by such integrated model is still small, official results from CLP2012 present promising result. 
We present a Chinese word segmentation system submitted to the ﬁrst task on CLP 2012 back-offs. Our segmenter is built using a conditional random ﬁeld sequence model. We set the combination of a few annotated micro blogs and People Daily corpus as the training data. We encode special words detected by rules and information extracted from unlabeled data into features. These features are used to improve our model’s performance. We also derive a micro blog speciﬁed lexicon from auto-analyzed data and use lexicon related features to assist the model. When testing on the sample data of this task, these features result in 1.8% improvement over the baseline model. Finally, our model achieves F-score of 94.07% on the bakeoff’s test set. 
This paper proposed a Hidden Markov Model (HMM) based tokenizer for Chinese micro-blog texts. Comparing with normal Chinese texts, micro-blog texts contain more uncertainties. These uncertainties are generally aroused by the irregular use of bloggers (such as network words, dialect words, wrong written characters, mixture of foreign words and symbols, etc.). Besides the lack of the annotated training corpus is also a restriction in solving this task. Hence the segmentation for micro-blogs is much more difficult than that of general text, we present an HMM based segmentation model integrated with a pre and post correction module. The evaluation results show that the proposed approach can achieve an F-measure of 90.98% on test set of 5,000 sentences. 
Microblog is a new and important social media nowadays. Can traditional methods deal well with Chinese microblog word segmentation? We adopt the forward maximum matching (FMM) method and design rules to recognize words with non-Chinese characters. We focus on comparing results between news text and microblog. The lexicon based method allows us to investigate well new words emerging in microblog by comparing with lexicon words. Experimental results show that the performance on microblog outperforms that on news text under the same setup, which may be a signal that microblog word segmentation is not as hard as expected. 
After years of researches, Chinese word segmentation has achieved quite high precisions for formal style text. However, the performance of segmentation is not so satisfying for MicroBlog corpora. In this paper we describe a scheme for Chinese word segmentation for, MicroBlog which integrates the characterbased and word-based information in the directed graph generated by MMSM model. Word-level information is effective for analysis of known words, while character-level information is useful for analysis of unknown words. A multi-chain unequal states CRF model is proposed. The proposed multi-chain unequal states CRF has two state chains with unequal states which can recognize the POS tag simultaneously. The hybrid model was effective and adopted in real-world system. 
Chinese tweets segmentation is a critical problem in natural language processing area. While segmentation of in-vocabulary words is well studied to date, few research findings are yet available concerning the prediction of new words on twitter. In this paper, we attempt to exploit multiple features for segmenting tweets in real text. To this end, we first take morpheme as the basic component units of Chinese words and thus investigate the relationship between Chinese new words and their internal morphological structures. Then, we explore both word internal cues and word external contextual features, and combine them for segmentation of Chinese new words using conditional random field. Our experimental results show that the incorporation of multiple features, especially the word-internal morphological features is of great value to Chinese tweets segmentation. 
The CIPS-SIGHAN 2012 Chinese Named Entity Recognition and Disambiguation (NERD) bake-off was held in the summer of 2012. Named entity recognition and disambiguation is an important task in natural language processing and knowledge base construction. It aims at detecting entity mentions in raw text, followed by pointing the detected mentions to real world entities. Often, real world entities can be found on online encyclopedia like Wikipedia and Baike. This task focuses on NERD in Chinese Language, and presents some challenges unique to Chinese, namely the confusion of named entity with common words, and lack of capital clues as in English. We manually construct query names and a knowledge base from Baike. Evaluation results show promising future of this ﬁeld. 
This paper presents our SIR-NERD system for the Chinese named entity recognition and disambiguation Task in the CIPS-SIGHAN joint conference on Chinese language processing (CLP2012). Our system uses a two-stage method and some key techniques to deal with the named entity recognition and disambiguation (NERD) task. Experimental results on the test data shows that the proposed system, which incorporates classifying and clustering techniques, can achieve competitive performance. 
This paper proposes a template based hybrid model for Chinese Personal Name Disambiguation (CPND). The template makes use of the features of personal role such as discriminating personal name (nickname, stage name), together with the specific context of most frequent words, personal name nearest words named entities, date and time that are effective for this disambiguation task, as well as surrounding context of nominal, verbal and adjectival constituents. The construction of the templates is automatically derived from the articles that maximizes the deviation of different categories of personal names. The extraction algorithm of keyword features based on the distribution of unlabeled data is also proposed in this paper for this challenging task. In addition, an augmented similarity measure for the CPND model has been designed to calculate the similarity between a standard template and an unlabeled text. The final evaluation reveals that the proposed model can achieve the Fmeasure of 75.75% on the test data. 
In this paper, we briefly report our system for Chinese Named Entity Recognition and Disambiguation task in CIPS-SIGHAN joint conference. We first present a method to extract different types of target person attributes from text documents with multiple techniques. Then we use these attributes to disambiguate different entities. Finally a classifier is used to distinguish entities in the knowledge base, and a cluster to recognize entities out of the knowledge base. 
To aim at the evaluation task of CLP2012 named entity recognition and disambiguation in Chinese, a Chinese name disambiguation method based on adaptive clustering with the attribute features is proposed. Firstly, 12-dimensional character attribute features is deﬁned, and tagged attribute feature corpus are used to train to obtain the recognition model of attribute features by Conditional Random Fields algorithm, in order to do the attribute recognition of given texts and knowledge bases. Secondly, the training samples are tagged by utilizing the correspondences of the text attribute and answer, and attribute feature weight model is trained based on the maximum entropy model and the weights are acquired. Finally, the fuzzy clustering matrix is achieved by the correlation of Knowledge Base(KB) ID attributes and text attributes for each KB ID, the clustering threshold is selected adaptively based on the F statistic, and clustering texts corresponding to ID are obtained, thus the texts corresponding to each ID are gained followed. For the texts not belong to KB, Out and Other types are obtained by fuzzy clustering to realize name disambiguation. The evaluation result is: P = 0.7424, R = 0.7428, F = 0.7426. 
This paper presents the HITSZ-PolyU system in the CIPS-SIGHAN bakeoff 2012 Task 3, Chinese Personal Name Disambiguation. This system leveraged the Chinese encyclopedia Baidu Baike (Baike) as the external knowledge to disambiguate the person names. Three kinds of features are extracted from Baike. They are the entities’ texts in Baike, the entities’ work-of-art words and titles in the Baike. With these features, a Decision Tree (DT) based classifier is trained to link test names to nodes in the NameKB. Besides, the contextual information surrounding test names is used to verify whether test names are person name or not. Finally, a simple clustering approach is used to group NIL test names that have no links to the NameKB. Our proposed system attains 64.04% precision, 70.1% recall and 66.95% F-score. 
In this paper we describe an integrated approach for named entity recognition and disambiguation in Chinese. The proposed method relies on named entity recognition (NER), entity linking and document clustering models. Different from other tasks of named entities, both classification and clustering are considered in our models. After segmentation, information extraction and indexing in the preprocessing step, the test names in the documents would be judged to be common words or named entities based on hidden Markov model (HMM). And then each predicted entity should be linked to the category in the given knowledge base (KB) according to the character attributes and keywords. Finally, the named entities which have no reference in KB would be clustered into a new category based on singular value decomposition (SVD). An implementation of our presented models is described, along with experiments and evaluation results on the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing Bakeoff (Bakeoff-2012). Named entity recognition F-measure reaches up to 76.67% and named entity disambiguation F-measure up to 69.47% within the test set of 32 names.  
This paper introduces the task of Chinese personal name disambiguation of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing (CLP) 2012 that Natural Language Processing Laboratory of Zhengzhou University took part in. In this task, we mainly use the Vector Space Model to disambiguate Chinese personal name. We extract different named entity features from diverse names information, and give different weights to various named entity features with the importance. First of all, we classify all the name documents, and then we cluster the documents that cannot be mapped to names that have been defined. Eventually the results of classification and the clustering are combined. In the test corpus experiments, the accuracy rate is 0.6778, the recall rate is 0.7205 and the F value is 0.6985 for all names. 
This paper gives the overview of the third Chinese parsing evaluation: CIPS-SIGHANParsEval-2012, including its parsing sub-tasks, evaluation metrics, training and test data. The detailed evaluation results and simple discussions will be given to show the difficulties in Chinese syntactic parsing. 
We describe our method of traditional Phrase Structure Grammar (PSG) parsing in CIPS-Bakeoff2012 Task3. First, bagging is proposed to enhance the baseline performance of PSG parsing. Then we suggest exploiting another TreeBank (CTB7.0) to improve the performance further. Experimental results on the development data set demonstrate that bagging can boost the baseline F1 score from 81.33% to 84.41%. After exploiting the data of CTB7.0, the F1 score reaches 85.03%. Our ﬁnal results on the ofﬁcial test data set show that the baseline closed system using bagging gets the F1 score of 80.17%. It outperforms the best closed system by nearly 4% which uses a single model. After exploiting the CTB7.0 data, the F1 score reaches 81.16%, demonstrating further increases of about 1%. 
We demonstrate that an unlexicalized PCFG with refined conjunction categories can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar and reflect the Chinese idiosyncratic grammatical property. Indeed, its performance is the best result in the 3nd Chinese Parsing Evaluation of single model. This result has showed that refine the function words to represent Chinese subcat frame is a good method. An unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize. 
We present a challenge to parse simplified Chinese and traditional Chinese with a same rule-based Chinese grammatical resource--Chinese Sentence Structure Grammar: CSSG, which was developed based on a new grammar formalism idea: Sentence Structure Grammar: SSG. We participate in the simplified Chinese parsing task and the traditional Chinese parsing task of CLP 2012 with a same rule-based chart parser implemented the CSSG. The experiments show that the CSSG that was developed for covering simplified Chinese constructions can also analyze most traditional Chinese constructions. 
A key observation is that concept compound constituent labels are detrimental to parsing performance. We use a PCFG parsing algorithm that uses a multilevel coarse-to-fine scheme. Our approach requires a sequence of nested partitions or equivalence classes of the PCFG nonterminals, where the nonterminals of each PCFG are clusters of nonterminals of the finer PCFG. We use the results of parsing at a coarser level to prune the next finer level. The coarse-to-fine method use hierarchical projections for incremental pruning. We present experiments which show that parsing with hierarchical state-splitting is fast and accurate on Tsinghua Chinese Treebank. In addition, we propose a multiple-model method that adds concept compound labels to the output of the simple PCFG model and gains higher bracketing recall from the simple model. This scheme can be implemented by training two models on different labeling styles. 
This paper presents the overview of traditional Chinese parsing task at SIGHAN Bake-offs 2012. On behalf of task organizers, we describe all aspects of the task for traditional Chinese parsing, i.e., task description, data preparation, performance metrics, and evaluation results. We summarize the performance results of all participant teams in this evaluation, in the hope to encourage more future studies on traditional Chinese parsing 
This paper describes the methods used for the parsing the Sinica Treebank for the bakeoff task of SigHan 2012. Based on the statistics of the training data and the experimental results, we show that the major difficulties in parsing the Sinica Treebank comes from both the data sparse problem caused by the fine-grained annotation and the tagging ambiguity. 
Selecting the best structure from several ambiguous structures produced by a syntactic parser is a challenging issue. The quality of the solution depends on the precision of the structure evaluation methods. In this paper, we propose a general model (context-dependent probability re-estimation model, CDM) to enhance the structure probabilities estimation. Compared with using rule probabilities only, the CDM has the advantage of an effective, flexible, and broader range of contexturefeature selection. We conduct experiments on the CDM parsing model by using Sinica Chinese Treebank. The results show that our proposed model significantly outperforms the baseline parser and the open source Berkeley statistical parser. More importantly, we demonstrate that the basic framework of the parsing model does not need to be changed, and the proposed re-estimation functions will adjust the probability estimation for every particular structure, and obtaining the better parsing results. 
In this paper, we propose a new sequential labeling scheme, double sequential labeling, that we apply it on Chinese parsing. The parser is built with conditional random field (CRF) sequential labeling models. One focuses on the beginning of a phrase and the phrase type, while the other focuses on the end of a phrase. Our system, CYUT, attended 2012 the second CIPS-SGHAN conference Bake-off Task4, traditional Chinese parsing task, and got promising result on the sentence parsing task. 
This paper describes our system for the subtask 1 of traditional Chinese Parsing of SIGHAN Bake-off 2012 evaluation. Since this research mainly focuses on speech recognition and synthesis applications, only base phrase chunking was implemented using three Conditional Random Field (CRF) modules, including word segmentation, POS tagging and base phrase chunking sub-systems. The official evaluation results show that the system achieved 0.5038 (0.7210/0.387) micro- and 0.5301 (0.7343/0.4147) macro-averaging F1 (precision/recall) rates on full sentence parsing task. However, if only the performance of base phrase chunking was considered, the Fmeasures may be around 0.70 and is somehow good enough for speech recognition and synthesis applications. 
Chinese parsing has been a highly active research area in recent years. This paper describes a hierarchical maximum pattern matching to integrate rule induction approach for sentence parsing on traditional Chinese parsing task. We have analyzed and extracted statistical POS (part-of-speech) tagging information from training corpus, then used the related information for labeling unknown words in test data. Finally, the rule induction regulation was applied to extract of the structure of short-term syntactic and then performed maximum pattern matching for longterm syntactic structure. On Sentence Parsing task, our system performs at 44% precision, 53% recall, and F1 is 48% in the formal testing evaluation. The proposed method can achieve the significant performance in traditional Chinese sentence parsing. 
We inspect the viability of ﬁnite-state spellchecking and contextless correction of nonword errors in three languages with a large degree of morphological variety. Overviewing previous work, we conduct large-scale tests involving three languages — English, Finnish and Greenlandic — and a variety of error models and algorithms, including proposed improvements of our own. Special reference is made to on-line three-way composition of the input, the error model and the language model. Tests are run on real-world text acquired from freely available sources. We show that the ﬁnite-state approaches discussed are sufﬁciently fast for high-quality correction, even for Greenlandic which, due to its morphological complexity, is a difﬁcult task for non-ﬁnite-state approaches. 
Previous work for encoding Optimality Theory grammars as ﬁnite-state transducers has included two prominent approaches: the socalled ‘counting’ method where constraint violations are counted and ﬁltered out to some set limit of approximability in a ﬁnite-state system, and the ‘matching’ method, where constraint violations in alternative strings are matched through violation alignment in order to remove suboptimal candidates. In this paper we extend the matching approach to show how not only markedness constraints, but also faithfulness constraints and the interaction of the two types of constraints can be captured by the matching method. This often produces exact and small FST representations for OT grammars which we illustrate with two practical example grammars. We also provide a new proof of nonregularity of simple OT grammars. 
A morphological analyser only recognizes words that it already knows in the lexical database. It needs, however, a way of sensing significant changes in the language in the form of newly borrowed or coined words with high frequency. We develop a finite-state morphological guesser in a pipelined methodology for extracting unknown words, lemmatizing them, and giving them a priority weight for inclusion in a lexicon. The processing is performed on a large contemporary corpus of 1,089,111,204 words and passed through a machine-learning-based annotation tool. Our method is tested on a manually-annotated gold standard of 1,310 forms and yields good results despite the complexity of the task. Our work shows the usability of a highly non-deterministic finite state guesser in a practical and complex application. 
This paper introduces a two-way Urdu– Roman transliterator based solely on a nonprobabilistic ﬁnite state transducer that solves the encountered scriptural issues via a particular architectural design in combination with a set of restrictions. In order to deal with the enormous amount of overgenerations caused by inherent properties of the Urdu script, the transliterator depends on a set of phonological and orthographic restrictions and a word list; additionally, a default component is implemented to allow for unknown entities to be transliterated, thus ensuring a large degree of ﬂexibility in addition to robustness. 
The integration of semantic properties into morphological analyzers can significantly enhance the performance of any tool that uses their output as input, e.g., for derivation or for syntactic parsing. In this paper will be presented my approach to the integration of aspectually relevant properties of verbs into a morphological analyzer for English. 
This paper presents a set of tools designed to assist traditional Basque verse writers during the composition process. In this article we are going to focus on the parts that have been created using ﬁnite-state technology: this includes tools such as syllable counters, rhyme checkers and a rhyme search utility. 
 WANT  WANT  This paper presents DAGGER, a toolkit for ﬁnite-state automata that operate on directed acyclic graphs (dags). The work is based on a model introduced by (Kamimura and Slutzki, 1981; Kamimura and Slutzki, 1982), with a few changes to make the automata more applicable to natural language processing. Available algorithms include membership checking in bottom-up dag acceptors, transduction of dags to trees (bottom-up dag-to-tree transducers), k-best generation and basic operations such as union and intersection. 
This paper introduces a new open source, WFST-based toolkit for Grapheme-toPhoneme conversion. The toolkit is efﬁcient, accurate and currently supports a range of features including EM sequence alignment and several decoding techniques novel in the context of G2P. Experimental results show that a combination RNNLM system outperforms all previous reported results on several standard G2P test sets. Preliminary experiments applying Lattice Minimum Bayes-Risk decoding to G2P conversion are also provided. The toolkit is implemented using OpenFst. 
Kleene is a high-level programming language, based on the OpenFst library, for constructing and manipulating ﬁnite-state acceptors and transducers. Users can program using regular expressions, alternation-rule syntax and right-linear phrase-structure grammars; and Kleene provides variables, lists, functions and familiar program-control syntax. Kleene has been approved by SAP AG for release as free, open-source code under the Apache License, Version 2.0, and will be available by August 2012 for downloading from http:// www.kleene-lang.org. The design, implementation, development status and future plans for the language are discussed. 
We explain the implementation of replace rules with the .r-glc. operator and preference relations. Our modular approach combines various preference constraints to form different replace rules. In addition to describing the method, we present illustrative examples. 
This paper presents an application of ﬁnitestate transducers to the domain of medicine. The objective is to assign disease codes to each Diagnostic Term in the medical records generated by the Basque Health Hospital System. As a starting point, a set of manually coded medical records were collected in order to code new medical records on the basis of this set of positive samples. Since the texts are written in natural language by doctors, the same Diagnostic Term might show alternative forms. Hence, trying to code a new medical record by exact matching the samples in the set is not always feasible due to sparsity of data. In an attempt to increase the coverage of the data, our work centered on applying a set of ﬁnite-state transducers that helped the matching process between the positive samples and a set of new entries. That is, these transducers allowed not only exact matching but also approximate matching. While there are related works in languages such as English, this work presents the ﬁrst results on automatic assignment of disease codes to medical records written in Spanish. 
This paper presents the current status of development of a ﬁnite state transducer grammar for the verbal-chain transfer module in Matxin, a Rule Based Machine Translation system between Spanish and Basque. Due to the distance between Spanish and Basque, the verbal-chain transfer is a very complex module in the overall system. The grammar is compiled with foma, an open-source ﬁnitestate toolkit, and yields a translation execution time of 2000 verb chains/second. 
In this paper we describe a conversion of the Buckwalter Morphological Analyzer for Arabic, originally written as a Perl-script, into a pure ﬁnite-state morphological analyzer. Representing a morphological analyzer as a ﬁnite-state transducer (FST) confers many advantages over running a procedural afﬁx-matching algorithm. Apart from application speed, an FST representation immediately offers various possibilities to ﬂexibly modify a grammar. In the case of Arabic, this is illustrated through the addition of the ability to correctly parse partially vocalized forms without overgeneration, something not possible in the original analyzer, as well as to serve both as an analyzer and a generator. 
In this work, we describe a methodology based on the Stochastic Finite State Transducers paradigm for Spoken Language Understanding (SLU) for obtaining concept graphs from word graphs. In the edges of these concept graphs, both semantic and lexical information are represented. This makes these graphs a very useful representation of the information for SLU. The best path in these concept graphs provides the best sequence of concepts. 
A ﬁnite-state approach to temporal ontology for natural language text is described under which intervals (of the real line) paired with event descriptions are encoded as strings. The approach is applied to an interval temporal logic linked to TimeML, a standard mark-up language for time and events, for which various ﬁnite-state mechanisms are proposed. 
This paper presents a ﬁnite-state approach to phrase-based statistical machine translation where a log-linear modelling framework is implemented by means of an on-the-ﬂy composition of weighted ﬁnite-state transducers. Moses, a well-known state-of-the-art system, is used as a machine translation reference in order to validate our results by comparison. Experiments on the TED corpus achieve a similar performance to that yielded by Moses.  
Speech translation can be tackled by means of the so-called decoupled approach: a speech recognition system followed by a text translation system. The major drawback of this two-pass decoding approach lies in the fact that the translation system has to cope with the errors derived from the speech recognition system. There is hardly any cooperation between the acoustic and the translation knowledge sources. There is a line of research focusing on alternatives to implement speech translation efﬁciently: ranging from semi-decoupled to tightly integrated approaches. The goal of integration is to make acoustic and translation models cooperate in the underlying decision problem. That is, the translation is built by virtue of the joint action of both models. As a side-advantage of the integrated approaches, the translation is obtained in a single-pass decoding strategy. The aim of this paper is to assess the quality of the hypotheses explored within different speech translation approaches. Evidence of the performance is given through experimental results on a limited-domain task.  
This work complements a parallel paper of a new ﬁnite-state dependency parser architecture (Yli-Jyra¨, 2012) by a proposal for a linguistically elaborated morphology-syntax interface and its ﬁnite-state implementation. The proposed interface extends Gaifman’s (1965) classical dependency rule formalism by separating lexical word forms and morphological categories from syntactic categories. The separation lets the linguist take advantage of the morphological features in order to reduce the number of dependency rules and to make them lexically selective. In addition, the relative functional speciﬁcity of parse trees gives rise to a measure of parse quality. By ﬁltering worse parses out from the parse forest using ﬁnite-state techniques, the best parses are saved. Finally, we present a synthesis of strict grammar parsing and robust text parsing by connecting fragmental parses into trees with additional linear successor links.  the sentence: an 80-word sentence has potentially 1.1 × 1062 unrooted unlabeled dependency trees that are stored “compactly” into a ﬁnite-state lattice that requires at least 2.4 × 1024 states, see Table 4 in YliJyra¨ (2012). A truly compact representation of the parse forest is provided by an interesting new extended ﬁnitestate parsing architecture (Yli-Jyra¨, 2012) that ﬁrst recognizes the grammatical sentences in quadratic time and space if the nested dependencies are limited by a constant (in cubic time if the length of the sentence limits the nesting). The new system (YliJyra¨, 2012) replaces the additive (Oﬂazer, 2003) and the intersecting (Yli-Jyra¨, 2005) validation of dependency links with reductive validation that gradually contracts the dependencies until the whole tree has been reduced into a trivial one. The idea of the contractions is illustrated in Example 1. In practice, our parser operates on bracketed trees (i.e., strings), but the effect will be similar.  
Minimum Error Rate Training (MERT) is a method for training the parameters of a loglinear model. One advantage of this method of training is that it can use the large number of hypotheses encoded in a translation lattice as training data. We demonstrate that the MERT line optimisation can be modelled as computing the shortest distance in a weighted ﬁnite-state transducer using a tropical polynomial semiring. 
In this paper we present a new biologically inspired approach to the part-of-speech tagging problem, based on particle swarm optimization. As far as we know this is the ﬁrst attempt of solving this problem using swarm intelligence. We divided the part-of-speech problem into two subproblems. The ﬁrst concerns the way of automatically extracting disambiguation rules from an annotated corpus. The second is related with how to apply these rules to perform the automatic tagging. We tackled both problems with particle swarm optimization. We tested our approach using two different corpora of English language and also a Portuguese corpus. The accuracy obtained on both languages is comparable to the best results previously published, including other evolutionary approaches. KEYWORDS: Part-of-speech Tagging, Particle Swarm Optimization, Disambiguation Rules, Natural Language Processing. Proceedings of the First International Workshop on Optimization Techniques for Human Language Technology, pages 1–16, COLING 2012, Mumbai, December 2012. 
Proceedings of the First International Workshop on Optimization Techniques for Human Language Technology, pages 31–48, COLING 2012, Mumbai, December 2012. 31  1. Introduction Historically, India has been a multilingual country and Indian constitution officially recognizes 22 languages with 11 different scripts and 6000 dialects used in different states spread across the country (Mudur 1999). Hindi is the national language of the India and spoken by more than 500 million Indians. Hindi is the world’s fourth most commonly used language after Chinese, English and Spanish. Marathi is one of the widely spoken languages in India especially in the state of Maharashtra. Hindi and Marathi uses the “Devanagari” script for writing and draw their vocabulary mainly from Sanskrit. It is challenging to transliterate out of vocabulary (OOV) words like names and technical terms occurring in the user input across languages with different alphabets and sound inventories. Transliteration is the conversion of a word from one language to another without losing its phonological characteristics. Machine transliteration is usually used to support the machine translation (MT) and cross-language information retrieval (CLIR) to convert the named entities (Padariya 2008). Existing approaches for Named Entity (hence forth denoted as NE) machine transliterations are linguistic-based and statistical-based (Karimi 2011). The linguistic approach uses hand-crafted rules, based on pattern matching which need a linguistic analysis to formulate rules. Statistical approach tries to generate transliterations using statistical methods based on bilingual text corpora. Transliterations are generated on the basis of statistical models, which are derived from the analysis of bilingual text corpora. Hindi/Marathi to English direct NE machine transliteration is quite difficult due to the many factors such as difference in writing script, difference in number of characters/alphabets, concept of capitalization of leading characters, phonetic characteristics, relative character length, presence of a number of exonyms, endonyms and historical variants for many place names, number of valid transliterations and availability of the parallel corpus (Saha 2008). In most of the grapheme based statistical methods parallel corpus is used and trained for the adequate number of entries using one of the learning approaches such as HMM, CRF, SVM etc. As shown below, for the NE / िवजयराघवगढ़ (vijayrāghavgarh)/ which is place name, character alignment is obtained using the available tools (like Moses, GIZA++, SRILM,) and then, aligned corpus is trained using one or more statistical probability approaches. Source Language Target Language िव ज य रा घ व ग ढ़ vi ja y rā gha v ga rh The accuracy of statistical methods depends on how good corpus is prepared and how good learning algorithm is. We have not found any complete named entity bilingual corpus for the Indian languages. 2. Pure and Full Consonant The basic consonant shape in the Indian script always has the implicit vowel /अ(a)/ and hence there is no explicit matra form for the short vowel ‘a’. For example, the NE name /कमल (kamal)/ is linguistically written as कमल = क् +अ+म+् अ+ल+् अ = k+a+m+a+l+a. 32  However, there are equivalent matras for all the other vowels (◌ा –ā/A/aa, ि◌-i, ◌ी-I, ◌,ु u, ◌,ू -U, ◌-े e, ◌ै-ai, ◌ो-0, ◌ौ-au), which get attached to the basic consonant shape whenever the corresponding vowel immediately follows the consonant. The written form of a basic consonant without the implicit ‘a’ vowel either has an explicit shape or it has the graphical sign ‘◌्’, known as the Halant (or Virama in Marathi) attached to its basic consonant shape (e.g. क् ). This is referred to as the ’Halant form’ of the consonant or pure consonant. The Halant is the vowel अ (a) omission sign. It serves to cancel the inherent vowel अ of the consonant to which it is applied (Mudur 1999).  When Devanagari vowel phoneme ‘a’ is added to any generic consonant phoneme then the consonant phoneme is called full consonant. It is necessary to have inherent ‘a’ attached to consonant to add the phone of ‘a’ vowel to it. If inherent ‘a’ is not added to the independent consonant phoneme, it becomes very difficult to utter its transliteration in English as well as to obtain its back transliteration in Devanagari from English. For example NE /कमल (kəməl)/ will be spelled as /kml/ and would be back transliterated as / ल/ which is tri-conjunct (Joshi 2003).  Unicode and ISCII character encoding standards for Indic scripts are based on full form of consonants (Singh 2006). Table 1 shows the pure consonants and full consonants with their English equivalent.  Pure Consonant in Devanagari क् ख् ग्  English Equivalent k kh g  Full Consonant in Devanagari क् + अ = क ख् +अ =ख ग् + अ = ग  English Equivalent ka kha ga  TABLE 1 - Pure and Full Consonant  Following are few examples about how to use the pure and full consonant approach.  Pure Consonant Approach  Full Consonant Approach  स् + अ + ई = सई (s + a + i = sai)  स + ई = सई (sa + i = sai)  प् + अ + क् +ई = पक (p + a + k +i = paki)  प + क + ◌ी = पक (pa + ka + i)  In proposed approach, the input NE for example / िवजयराघवगढ़ /, is segmented into basic syllabic units such as िव ज य रा घ व ग ढ़ and transliterated into English by mapping source language phonetic units into target language phonetic unit using full consonant based phonetic mapping scheme. IAST (International Alphabet of Sanskrit Transliteration) converter can be used to obtain the baseline transliteration. The baseline transliteration for NE / िवजयराघवगढ़ / is shown below.  Source Language Baseline Transliteration  िव ज य रा घ व ग ढ़ vi ja ya rā gha va ga rh  The NE /िवजयराघवगड/ is made up of three NEs िवजय (Vijay), राघव (Rāghav) and गड (garh) respectively. As it is a multi word NE and consists of three segments, there are three breakpoints (less stressed positions) at /ya/, /va/ and /rh/, hence the inherent ‘a’ mapped by full consonant mapping should be deleted to get the correct transliteration as /vijayrāghavgarh/.  33  It is challenging task to find out the number of segments in the given NE, in order to find the boundaries of the segments to remove the inherent ‘a’ attached due to full consonant mapping. The segments in the NE are obtained by assigning two weights based on the number of diacritics used to form a phonetic unit and the length of the source language NE is used as a feature for supervised learning to obtain the multiple classes for the segmentation. Segmentation is used to identify and delete the schwa (less stressed positions) which is a major issue in direct translation without training the corpus. 3. Related Work Existing basic models for NE machine transliterations are Grapheme-based and Phoneme-based. The grapheme based model treats transliteration as an orthographic process and tries to map the source language graphemes directly to the target language graphemes. Phoneme-based model considers transliteration as a phonetic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. The major contributors in the area of machine transliteration in India are C-DAC (Center for Development of Advanced Computing), NCST (National Center for Software Technology) and Indictrans Team. In the early 1980's, the development of GIST (Graphics and Intelligence - Based Script Technology) was a major breakthrough. CDAC developed the hardware based solution GIST based on ISCII (Indian Script Code For Information Interchange). The second development was Unicode based encoding standard UTF-8 for Indic script (BIS 1991). The third development (2003) was a phonemic code based scheme for effective processing of Indian languages and used successfully in turnkey jobs such as telephone directory in Hindi, bilingual certificate for Mumbai University, and collector voters’ list (Joshi 2003). The applications they have localised are Indian Railways Reservation Charts, Mahanagar Telephone Nigams and Bilingual Telephone Directories. One of the early works on transliteration is done by Arbabi. They combined neural networks and expert systems for Arabic-English pair using phoneme model (Arbabi 1994). Knight and Graehl developed a five stage statistical model to do back transliteration, that is, to recover the original English name from its transliteration into Japanese Katakana (Knight 1997). Stalls used this for back transliteration from Arabic to English (Stalls 1998). Al-Onaizan and Knight have produced a simpler Arabic-English transliterator and evaluated how well their system can match a source spelling (AlOnaizan 2002). Their work includes an evaluation of the transliterations in terms of their reasonableness according to human judges. Work in the field of Indian Language CLIR was done by Jaleel and Larkey, which was based on their work in English-Arabic transliteration for CLIR [Jaleel 2003]. Their approach was based on Hidden Markov Model using GIZA++. Phoneme-based models, based on weighted finite state transducers (Knight 1997) and Markov window (Jung 2003) considers transliteration as a phonetic process. OM transliteration scheme provided a script representation which is common for all Indian languages (Ganapathiraju 2005). Punjabi machine transliteration for Punjabi language from Shahmukhi to Gurmukhi used the set of transliteration rules (Malik 2006). Sproat presented a formal computational analysis of Brahmi scripts (Sproat 2002-2004). Kopytonenko focused on computational models that perform grapheme-to-phoneme conversion (Kopytonenko 2006). Ganesh, Harsha, Pingali and 34  Verma have developed a statistical transliteration technique which is language independent. They selected a statistical model for transliteration which is based on Hidden Markov Model alignment and Conditional Random Fields (Ganesh 2008). Sujan Kumar Saha, Partha Sarathi Ghosh, Sudeshna Sarkar, and Pabitra Mitra have proposed a two-phase transliteration methodology (Saha 2008). The transliteration module uses an intermediate alphabet, which is designed by preserving the phonetic properties. Ekbal, Naskar and Bandyopadhyay made significant attempt to develop transliteration systems for Indian languages to English and especially for Bengali-English transliteration (Ekbal 2007-2010). Manoj K. Chinnakotla, Om P. Damani, and Avijit Satoskar have developed a reasonable transliteration system for resource scared languages by judiciously applying statistical techniques to monolingual resources in conjunction with manually created bilingual rule bases (Chinnakotla 2010). The statistical technique used is the Character Sequence Modeling (CSM), called Language Modelling. They have proved that if the word origin is used for the transliteration, then the system performs better than statistical methods. Jong-Hoon Oh approach is based on two transliteration models (Oh 2009). They used three different machine learning algorithms CRF, MIRA and MEM for building multiple machine transliteration engines. Literature survey shows that the maximum word accuracy achieved is 95.5%, for English to Russian (Martin 2009) using grapheme based statistical approach. In India the maximum word accuracy achieved is 91.59% for Hindi to English (Saha 2008) using phonetic model 4. Approach The objective of the work is to transliterate named entities from Hindi and Marathi into English. Following Hindi/Marathi language related terminologies are used. Akshara - It is the minimal articulatory unit of speech in Hindi/Marathi (Pandey 1990). Swara – It is a pure vowel in Devanagari. Swaras is plural of Swara. Vyanjana – It is a consonant in Devanagari. Vyanjanas is the plural form of it. Jodaakshar – It is the conjugates in Devanagari. Syllable - It is made up of phonetic units to establish minimum rhythm. Schwa - The schwa is the vowel sound in many lightly pronounced unaccented syllables in words of more than one syllable. It is represented by /ə/ symbol (Naim 2009). The overall process is carried out as follows. Step 1: Preparation of phonetic map table for Devanagari to English transliteration using full consonant approach with local language context. Step 2: Formation of Devanagari Phonetic Units. Step 3: Generation of Intermediate Phonetic Code (denoted hence forth by IPC) by mapping Devanagari phonetic units to equivalent phonetic unit using phonetic map. Step 4: Pruning of inherent ‘a’ generated by vowel matras due to full consonant approach as well as half consonants ‘a`’ generated by conjuncts (Jodakshar) from intermediate code. (Outcome of this step is denoted hence forth as Modified Intermediate Phonetic Code MIPC). 35  Step 5: Empirical analysis for multiword named entity formation in Hindi and Marathi. Step 6: Statistical Model. Step 7: Segmentation and classification using supervised learning Step 8: Transliteration Example  4.1 Preparation of phonetic map table  The possibility of different scripts between the source and target languages is the problem that transliteration systems need to tackle. Hindi/Marathi uses the Devanagari script whereas English uses the Roman script. Devanagari script used for Hindi have 12 pure vowels, two additional loan vowels taken from the Sanskrit and one loan vowel from English. According to Cambridge Advanced Learner's Dictionary English has only five pure vowels but, the vowel sound is also associated with the consonants w and y (Koul 2008). There are 34 pure consonants, 5 traditional conjuncts, 7 loan consonants and 2 traditional signs in Devanagari script and each consonant have 14 variations through integration of 14 vowels while in Roman script there are only 21 consonants. The 34 pure consonants and 5 traditional conjuncts along with 14 vowels produce 546 different alphabetical characters (Mudur 1999). Table 2 show 15 vowels along with their matra signs and 34 pure consonants in Devanagari script. The consonant /ळ/ is used only in Marathi language.  Vowel Matra Vowel Matra  Pure consonants  अ No matra ॠ  ◌ॄ  क  ख  ग  घ  ङ  आ  ◌ा  ए  ◌े  च  छ  ज  झ  ञ  इ  ि◌  ऎ  ◌ै  ट  ठ  ड  ढ  ण  ई  ◌ी  ओ  ◌ो  त  थ  द  ध  न  उ  ◌ु  औ  ◌ौ  प  फ  ब  भ  म  ऊ  ◌ू  अं  ◌ं  य  र  ल  व  श  ऋ  ◌ृ  अ:  ◌ः  ष  स  ळ  ह  Loan Vowel  ऑॅ  ◌ॅ  TABLE 2 - Vowels and Consonant in Hindi and Marathi  Table 3 shows the 5 traditional conjuncts, 7 loan alphabets, 2 traditional signs and 2 special nasal signs in Devanagari script (Walambe 1990].  Traditional Conjuncts Additional Consonants Loan Consonants Traditional Signs Special nasal  ड़ ढ़ क़ख़ग़ज़ फ़ ॐ ी ◌ं ◌ँঁ  TABLE 3 - Traditional Conjuncts in Hindi and Marathi  Table 4 shows the phonetic based mapping scheme used to transliterate Devanagari consonant and vowel phones into equivalent English Phones using full consonant approach. It includes all alphabets of Devanagari script used in both Hindi/Marathi and  36  fully based on the National Library of Kolkata and ITRANS of IIT Madras, India (Unicode 2007). It is to note that the first vowel /अ/ in Hindi/Marathi is mapped to English letter ‘a’ (short vowel) while the second vowel /आ/ is mapped to ‘ā’ (long vowel as per IPA) in English. The alphabet ‘a’ in English is a short vowel equivalent to /अ/ which is also a short vowel in Devanagari while /आ/ in Devanagari is a long vowel and mapped capital ‘ā’ or ‘A’ in our phonetic scheme to generate the IPC.  ंजनवण GloCONSONANT PHONEME ttal  कोमल क ठन  मूध य द य ओ य  ताल ताल Retroflex Dental Labial  Velar Palatal  अ प ाण  अघोष Unaspirated  Voiceless पश Stops  महा ाण Aspirated अ प ाण  सघोष Unaspirated  Voiced  महा ाण  Aspirated  क क़  च  ट  ka qa cha  Ta  ख ख़  छ  ठ  kha, khha Cha  Tha  ग ग़ज ज़ ड ड़ ga, ghha ja za Da Dha  घ  झढ ढ़  gha jha Dha , rha  त  प  ta pa  थफ फ़  tha pha fa  द  ब  da ba  ध  भ  dha bha  नािस य Nasals Anuswara and Anunasik  ङ nga M (default)  ञ  ण  न  म  nya  Na na ma  ◌ं or ◌ँ  ◌ं or ◌ँ  N (depends on next consonant)  अध वर वण Semivowels  य  र  ल  व  ya  ra  la va/wa  अघोष :  श  संघष Voiceless -h  sha  Fractives  सघोष ह िज हामूलीय Voiced ha िवसजनीय  ष  स  Sha  sa  उप मानीय  वरवण VOWEL  व अ Short a  इ  ऋ  ऌ  उ  i  Ru lRu  u  PHONEMES  दीघ आ Long A/ ā  ई ए I/ee e  ॠ  ॡऊ ओ  RU lRU U oo  संयु Diapthongs  ऎ  औ  ai  au/ou  पारंपा रक जोडा रे  ॐ  ी  Traditional Conjuncts ksha dnya dya shra tra om Shree  वतं वण  ळ  Independent Consonant  La  TABLE 4 - Full Consonant based Phonetic Scheme  4.2 Formation of Devanagari Phonetic Units As Unicode uses full consonant approach it treats Devanagari consonant phoneme and vowel phoneme as a separate units as shown below.  37  िवजयराघवगढ़(Vijayrāghavgarh) -> व + ि◌ + ज + य + र + ◌ा + घ + व + ग +ढ़ This feature of Unicode is very useful in the creation of Devanagari Phonetic Units. From internal representation of Unicode, phonetic units are formed for Devanagari names as shown below. िवजयराघवगढ़ -> िव | ज | य | रा | घ | व | ग | ढ़  4.3 Generation of Intermediate Phonetic Code  The phonetic scheme is based on full consonant approach and dependent on vowel matra ‘अ’. The inherent ‘a’ is added even if any other vowel matra is present with Devanagari phoneme unit. The script generated in English for Devanagari phonetic unit with inherent ‘a’ using the phonetic mapping scheme is denoted as Intermediate Phonetic Code (IPC). Table 5 shows how IPC in English is generated for the Devanagari consonant phoneme ‘क’ when it is combined with different vowel phoneme of Devanagari script.  Devanagari Consonant क क क क क क क  Devanagari Vowel अ आ इ ई उ ऊ ऋ  Devanagari Vowel Matra No Matra ◌ा ि◌ ◌ी ◌ु ◌ू ◌ृ  Devanagari Syllabic Unit क का क क कु कू कृ  IPC in English ka kaA kai kaI kau kaU kaRu  TABLE 5 -IPC for Devanagari Consonant 'क’  The following method is used to generate the IPC in English.  • Devanagari name divided into the syllabic units are called as Source Transliteration  Units (STU). STU is equivalent to phonetic unit of Devanagari. The STU can be  represented using following regular expression.  STU = ((V) | (C) | (CV) | (CCV) | (C…CV)) (G)  where C = Consonant, V = Vowel and G = Nasalization of vowels  • English name divided into the syllabic units called as Target Transliteration Units  (TTU). TTU is equivalent to a phonetic unit of English. The regular expression for  English can be written as TTU = C*V*  • The Devanagari name is represented as a collection of Devanagari phonetic units.  Name in Devanagari = { STU1, STU2, … STUn } • The English name is represented as a collection of English phonetic units.  Name in English = { TTU1, TTU2, … TTUm } • IPC is obtained by using direct mapping of STUs to TTUs on one to one basis.  Table 6 shows the few examples for IPC in English for the Devanagari NE.  NE  STUs  TTUs  IPC in English  नोवरोझाबाद नो|व|रो|झा|बा|द nao|va|rao|jhaā|baā|da naovaraojhaābaāda  TABLE 6 - IPC Examples  38  4.4 Pruning  An IPC is generated by mapping STUs to TTUs. STUs are generated from the Devanagari name which uses Unicode encoding. Due to the full consonant nature of Unicode, there is an inherent ‘a’ followed by consonant phoneme for all Devanagari vowel matras in IPC form. The inherent ‘a’ generated for Devanagari phonetic units having any vowel matra should be removed. One of the problems in Devanagari to English transliteration is the transformation of conjugates. When the half consonant cluster (Virama/Halant ◌्) appears in Devanagari script, it is mapped in English with the letter symbol ‘a`’ which appears in between two consonant phonemes. There is no practice to represent such half consonant in English. All the viramas in Devanagari script mapped as a half consonants ‘a`’ should be eliminated from the IPC. The output after pruning inherent ‘a’ and ‘a`’ is denoted as Modified IPC (MIPC). Table 7 shows the example of removal of inherent ‘a’ if matra or half consonant is present.  STU TTU If matra , remove ‘a’ If conjunct, remove ‘a` Modified IPC  कै kaai  kai  ---  ला laā  lā  ---  श sha  ---  ---  kailāshanātha  ना naā  nā  ---  थ  tha  ----  ---  TABLE 7 - IPC to MIPC for Devanagari NE ‘Kailashnath’  4.5 Empirical analysis for multiword named entity formation An example shown in Table -7, the NE /कै लाशनाथ/ is transliterated as /kailāshanātha/. The NE /कै लाशनाथ/ is multi-word name consists of /कै लाश/ and a suffix /नाथ/ . An ‘a’ followed by ‘sh’ should be deleted as well as the last ‘a’ also should be deleted to obtain the correct transliteration. कै लाशनाथ → [कै | ला | श | ना | थ] → [kaai | laā| sha | naā | tha] → [kai | lā | sha | nā | tha] → [kai | lā| sh | nā | th] →[kailāsh | nāth] →kailāshnāth It has been observed that the minimum length of the NE is 1 akshara (formed using 1 syllabic unit) and maximum length is 8 aksharas. There are very few named entities consisting one syllable. From the number of aksharas in the named entities, 8 categories are made. One akshara is considered equivalent to one phonetic unit in the Devanagari NEs. It is found that nearly 50% NEs used in India are a combination of two or more individual named entities (denoted hence forth NEs). For one akshara, two aksharas and three aksharas NEs, transliteration is quite simple. As the length of a NE increases, the segmentation becomes important to find out the number of words used to form the NE in order to separate the rhythms within it and in turn number of phonetic units in each rhythm. Most of the four aksharas, five aksharas, six aksharas, seven aksharas and eight aksharas NEs are formed with the combination of two or three different rhythmic units. Table 8 shows the observations of possible combinations of phonetic segments from pronunciation point of view.  39  NE  Segmentation  Number of Aksharas =4  ीवधन(Shriwardhan)  ी + वधन(Shrī + wardhan)  बाजीराव(Bājīrāo)  बाजी + राव (Bājī+ rāo)  धवल ी(Dhawalshrī)  धवल + ी (Dhawal + shrī)  Number of Aksharas =5  मनमोहन(Manmohan)  मन + मोहन (Man + mohan)  मािणकराव(Mānikrāo)  मािणक + राव (Mānik + rāo)  ीनारायण(Shrinārāyan)  ी + नारायण (Shrī + nārāyan)  Number of Aksharas =6  करमरकर(Karmarkar)  कर+मर+ कर (Kar + mar + kar)  ेमनारायण(Premnārāyan)  ेम + नारायण (Prem + nārāyan)  भारतभुषण(Bhāratbhushan)  भारत+ भुषण (Bhārat + bhushan)  जनादनराव(Janārdhanrāo)  जनादन + राव (Janārdhan + rāo)  Number of Aksharas =7  राजगु नगर(Rajgurunagar)  राज+गु +नगर(Raj+guru+nagar)  मनमाधवराव(Manmādhavrāo) मन+माधव+राव(Man+mādhav+rāo)  पंढरपुरकर(Pandharpurkar)  पंढर + पुर + कर(Pandharpurkar)  काशनारायण(Prakāshnārāyan)  काश+नारायण(Prakāsh + nārāyan)  िग रराज कशोर(Girirājkishor)  िग रराज + कशोर(Girirāj + kishor)  पु षो मदास(Purushottamdās) पु षो म+ दास(Purushottam + dās)  Number of Aksharas =8  िवजयराघवगढ़(Vijayrāghavgarh) िवजय+राघव+गढ़ (Vijay+rāghav+garh)  नारायणगावकर(Nārāyangāvkar) नारायण+गाव+कर(Nārāyan +gāv+kar)  पु षो मनगर  पु षो म+नगर (Purushottam+nagar)  (Purushottamnagar)  ि भुवननारायण  ि भुवन+ नारायण (Tribhuvan+nārāyan)  (Tribhuvannārāyan)  Segment Lengths 
Semi-supervised learning methods address the problem of building classiﬁers when labeled data is scarce. Text classiﬁcation is often augmented by rich set of labeled features representing a particular class. As tuple level labling is resource consuming, semi-supervised and weakly supervised learning methods are explored recently. Compared to labeling data instances (documents), feature labeling takes much less effort and time. Posterior regularization (PR) is a framework recently proposed for incorporating bias in the form prior knowledge into posterior for the label. Our work focuses on incorporating labeled features into a naive bayes classiﬁer in a semi-supervised setting using PR. Generative learning approaches utilize the unlabeled data more effectively compared to discriminative approaches in a semi-supervised setup. In the current study we formulate a classiﬁcation method which uses the labeled features as constraints for the posterior in a semi-supervised generative learning setting. Our empirical study shows that performance gains are signiﬁcant compared to an approach solely based on Generelized Expectation(GE) or limited amount of labeled data alone. We also show an application of our framework in a transfer learning setup for text classiﬁcation. As we allow labeled data as well as labeled features to be used, our setup allows the presence of limited amount of labeled data on the target side of transfer learning where feature constraints are used for transferring knowledge from source domain to target domain. KEYWORDS: Classiﬁcation,Posterior Regularization. Proceedings of the First International Workshop on Optimization Techniques for Human Language Technology, pages 65–78, COLING 2012, Mumbai, December 2012. 65  
The OS* algorithm is a uniﬁed approach to exact optimization and sampling, based on incremental reﬁnements of a functional upper bound, which combines ideas of adaptive rejection sampling and of A* optimization search. We ﬁrst give a detailed description of OS*. We then explain how it can be applied to several NLP tasks, giving more details on two such applications: (i) decoding and sampling with a high-order HMM, and (ii) decoding and sampling with the intersection of a PCFG and a high-order LM. 
Proceedings of the First International Workshop on Optimization Techniques for Human Language Technology, pages 95–108, COLING 2012, Mumbai, December 2012. 95  
Simple questions require small snippets of text as the answers whereas complex questions require inferencing and synthesizing information from multiple documents to have multiple sentences as the answers. The traditional QA systems can handle simple questions easily but complex questions often need more sophisticated treatment e.g. question decomposition. Therefore, it is necessary to automatically classify an input question as simple or complex to treat them accordingly. We apply two machine learning techniques and a Latent Semantic Analysis (LSA) based method to automatically classify the questions as simple or complex. KEYWORDS: Simple questions, complex questions, support vector machines, k-means clustering, latent semantic analysis. Proceedings of the Workshop on Question Answering for Complex Domains, pages 1–10, COLING 2012, Mumbai, December 2012. 
Proceedings of the Workshop on Question Answering for Complex Domains, pages 11–26, COLING 2012, Mumbai, December 2012. 11  
Proceedings of the Workshop on Question Answering for Complex Domains, pages 47–56, COLING 2012, Mumbai, December 2012. 47  
WikiTalk is an open-domain knowledge access system that talks about topics using Wikipedia articles as its knowledge source. Based on Constructive Dialogue Modelling theory, WikiTalk exploits the concepts of Topic and NewInfo to manage topic-tracking and topic-shifting. As the currently talked-about Topic can be any Wikipedia topic, the system is truly open-domain. NewInfos, the pieces of new information to be conveyed to the partner, are associated with hyperlinks extracted from the Wikipedia texts. Using these hyperlinks the system can change topics smoothly according to the user’s changing interests. As well as user-initiated topics, the system can suggest new topics using for example the daily "Did you know?" items in Wikipedia. WikiTalk can be employed in different environments. It has been demonstrated on Windows, with an open-source robotics simulator, and with the Aldebaran Nao humanoid robot. KEYWORDS: Open-domain knowledge access, spoken dialogue system, Wikipedia, human-robot interaction. Proceedings of the Workshop on Question Answering for Complex Domains, pages 57–70, COLING 2012, Mumbai, December 2012. 57  
For current statistical machine translation system, reordering is still a major problem for language pairs like Chinese-English, where the source and target language have signiﬁcant word order differences. In this paper we propose a novel tagging-style reordering model. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. For the given source sentence, we assign each source token a label which contains the reordering information for that token. We also design an unaligned word tag so that the unaligned word phenomenon is automatically covered in the proposed model. Our reordering model is conditioned on the whole source sentence. Hence it is able to catch long dependencies in the source sentence. The decoder makes use of the tagging information as soft constraints so that in the test phase (during translation) our model is very efﬁcient. The model training on large scale tasks requests notably amounts of computational resources. We carried out experiments on ﬁve Chinese-English NIST tasks trained with BOLT data. Results show that our model improves the baseline system by 0.98 BLEU 1.21 TER on average. KEYWORDS: statistical machine translation, reordering, conditional random ﬁelds. Proceedings of the Workshop on Reordering for Statistical Machine Translation, pages 17–26, COLING 2012, Mumbai, December 2012. 17  
Source side reordering has been shown to improve the performance of phrase based machine translation systems. In this work, we explore the learning of source side reordering given a training corpus of word aligned data. Given the large number of re-orderings this problem is NP-hard. We explore the possibility of representing the problem as a reordering of word sequences, instead of words. To this end, we propose a sequence labelling framework to identify work sequences. We also model the reversal of word sequences as a sequence labelling problem. These transformations reduce the problem to a phrase reordering problem, which has a smaller search space. KEYWORDS: Statistical machine translation; Source reordering; Sequence labelling; Word alignment. Proceedings of the Workshop on Reordering for Statistical Machine Translation, pages 47–54, COLING 2012, Mumbai, December 2012. 47  
KEYWORDS: mentor suggestion, collaboration, information extraction, wikipedia. Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 1–8, COLING 2012, Mumbai, December 2012. 
Evaluation of the content of free texts is a challenging task for humans. Automation of this process is largely useful in order to reduce human related errors. We consider one instance of the “free texts” assessment problems; automatic essay grading where the task is to grade student written essays automatically given course materials and a set of human-graded essays as training data. We use a Latent Semantic Analysis (LSA)-based methodology to accomplish this task. We experiment on a dataset obtained from an occupational therapy course and report the results. We also discuss our ﬁndings, analyze different problem areas and explain the potential solutions. KEYWORDS: Free texts, essay grading, latent semantic analysis, syntactic tree kernel, shallow semantic tree kernel. Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 9–16, COLING 2012, Mumbai, December 2012. 9  
Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 17–24, COLING 2012, Mumbai, December 2012. 17  
Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 25–36, COLING 2012, Mumbai, December 2012. 25  
This paper presents an evaluation of tone recognition systems integrated in a computer-assisted pronunciation training system for German learners of Mandarin. Both the reference tone recognition system as well as a recently redesigned tone recognition system contain monotone, bitone and tritone recognizers for isolated monosyllabic and disyllabic words and sentences, respectively. The performance of the reference system and the redesigned tone recognition systems was compared on data from German learners of Mandarin, while varying the feature vector to contain spectral as well as prosodic features. The redesigned tone recognition system matched or outperformed the reference system. For monosyllabic and disyllabic words it improved when spectral features were added to prosodic features. In contrast, results of tone recognition in sentences yielded better results based on prosodic features only. KEYWORDS: Mandarin Chinese, Tone Recognition, Computer-Assisted Language Learning. Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 37–42, COLING 2012, Mumbai, December 2012. 37  
In this paper we present work done towards populating a domain ontology using a public knowledge base like DBpedia. Using an academic ontology as our target we identify mappings between a subset of its predicates and those in DBpedia and other linked datasets. In the semantic web context, ontology mapping allows linking of independently developed ontologies and inter-operation of heterogeneous resources. Linked open data is an initiative in this direction. We populate our ontology by querying the linked open datasets for extracting instances from these resources. We show how these along with semantic web standards and tools enable us to populate the academic ontology. Resulting instances could then be used as seeds in spirit of the typical bootstrapping paradigm. KEYWORDS: Ontology Mapping, Academic knowledge base, linked open data, DBpedia. Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 51–60, COLING 2012, Mumbai, December 2012. 51  
Feedback on pronunciation is vital for spoken language teaching. Automatic pronunciation evaluation and feedback can help non-native speakers to identify their errors, learn sounds and vocabulary, and improve their pronunciation performance. These evaluations commonly rely on automatic speech recognition, which could be performed using Sphinx trained on a database of native exemplar pronunciation and non-native examples of frequent mistakes. Adaptation techniques using target users' enrollment data would yield much better recognition of non-native speech. Pronunciation scores can be calculated for each phoneme, word, and phrase by means of Hidden Markov Model alignment with the phonemes of the expected text. In addition to the basic acoustic alignment scores, we have also adopted the edit distance based criterion to compare the scores of the spoken phrase with those of models for various mispronunciations and alternative correct pronunciations. These scores may be augmented with factors such as expected duration and relative pitch to achieve more accurate agreement with expert phoneticians' average manual subjective pronunciation scores. Such a system is built and documented using the CMU Sphinx3 system and an Adobe Flash microphone recording, HTML/JavaScript, and rtmplite/Python user interface. Keywords: Pronunciation Evaluation, Text-independent, forced-alignment, edit- distance neighbor phones decoding, CMUSphinx. Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 61–68, COLING 2012, Mumbai, December 2012. 61  
Personalized services are increasingly becoming popular in the Internet. This work proposes a way of generating personalized content and simultaneously recommending users, web pages that, he/she might be interested in, based on his/her personalized content. In this work, we portray a system that not only helps the user in bookmarking the URL and snippets from the web page but also recommends web pages relevant to his/her interest. It applies a content-based ﬁltering approach. We describe the details of the approach and implementation as well as address the challenges associated with it. KEYWORDS: Bookmarking, Recommendation System, PEBL, Clustering. Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 69–78, COLING 2012, Mumbai, December 2012. 69  
In this paper, we study the usefulness of the best path and the complete trellis in dynamic programming based template matching approach for detecting pronunciation mismatch. We show that there exists cues in trellis (a matrix representing all paths), which could be exploited for detecting pronunciation mismatch. Such an approach could be used to build a template based approach for detecting pronunciation mismatch independent of the language. KEYWORDS: Pronunciation mismatch, dynamic programming, template matching. KEYWORDS IN L2: . Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 79–84, COLING 2012, Mumbai, December 2012. 79  
Proceedings of the Workshop on Speech and Language Processing Tools in Education, pages 85–90, COLING 2012, Mumbai, December 2012. 85  
Appropriate Named Entity handling is important for Statistical Machine Translation. In this work we address the challenging issues of generalization and sparsity of NEs in the context of SMT. Our approach uses the source NE Recognition (NER) system to generalize the training data by replacing the recognized Named Entities with place-holders, thus allowing a PhraseBased Statistical Machine Translation (PBMT) system to learn more general patterns. At translation time, the recognized Named Entities are handled through a speciﬁcally adapted translation model, which improves the quality of their translation. We add a post-processing step to a standard NER system in order to make it more suitable for integration with SMT and we also learn a prediction model for deciding between options for translating the Named Entities, based on their context and on their impact on the translation of the entire sentence. We show important improvements in terms of BLEU and TER scores already after integration of NER into SMT, but especially after applying the SMT-adapted post-processing step to the NER component. KEYWORDS: Named Entity Recognition, Statistical Machine Translation. Second ML4HMT Workshop, pages 1–16, COLING 2012, Mumbai, December 2012. 
Semantic Web aims to allow machines to make inferences using the explicit conceptualisations contained in ontologies. By pointing to ontologies, Semantic Web-based applications are able to inter-operate and share common information easily. Nevertheless, multilingual semantic applications are still rare, owing to the fact that most online ontologies are monolingual in English. In order to solve this issue, techniques for ontology localisation and translation are needed. However, traditional machine translation is difﬁcult to apply to ontologies, owing to the fact that ontology labels tend to be quite short in length and linguistically different from the free text paradigm. In this paper, we propose an approach to enhance machine translation of ontologies based on exploiting the well-structured concept descriptions contained in the ontology. In particular, our approach leverages the semantics contained in the ontology by using Cross Lingual Explicit Semantic Analysis (CLESA) for context-based disambiguation in phrase-based Statistical Machine Translation (SMT). The presented work is novel in the sense that application of CLESA in SMT has not been performed earlier to the best of our knowledge. KEYWORDS: Ontology Translation, Word-Sense Disambiguation, Statistical Machine trans- lation, Explicit Semantic Analysis, Ontology Localisation. Second ML4HMT Workshop, pages 25–36, COLING 2012, Mumbai, December 2012. 25  
This paper provides the system description of the IHMM team of Dublin City University for our participation in the system combination task in the Second Workshop on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid MT (ML4HMT-12). Our work is based on a confusion network-based approach to system combination. We propose a new method to build a confusion network for this: (1) incorporate extra alignment information extracted from given meta data, treating them as sure alignments, into the results from IHMM, and (2) decode together with this information. We also heuristically set one of the system outputs as the default backbone. Our results show that this backbone, which is the RBMT system output, achieves an 0.11% improvement in BLEU over the backbone chosen by TER, while the extra information we added in the decoding part does not improve the results. KEYWORDS: system combination, confusion network, indirect HMM alignment, backbone chosen. Second ML4HMT Workshop, pages 37–44, COLING 2012, Mumbai, December 2012. 37  
This paper gives the system description of the domain adaptation team of Dublin City University for our participation in the system combination task in the Second Workshop on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid MT (ML4HMT-12). We used the results of unsupervised document classiﬁcation as meta information to the system combination module. For the Spanish-English data, our strategy achieved 26.33 BLEU points, 0.33 BLEU points absolute improvement over the standard confusion-network-based system combination. This was the best score in terms of BLEU among six participants in ML4HMT-12. KEYWORDS: Statistical Machine Translation, Topic Model, System Combination. Second ML4HMT Workshop, pages 45–54, COLING 2012, Mumbai, December 2012. 45  
This paper provides the system description of the Dublin City University system combination module for our participation in the system combination task in the Second Workshop on Applying Machine Learning Techniques to Optimize the Division of Labour in Hybrid MT (ML4HMT12). We incorporated a sentence-level quality score, obtained by sentence-level Quality Estimation (QE), as meta information guiding system combination. Instead of using BLEU or (minimum average) TER, we select a backbone for the confusion network using the estimated quality score. For the Spanish-English data, our strategy improved 0.89 BLEU points absolute compared to the best single score and 0.20 BLEU points absolute compared to the standard system combination strategy. KEYWORDS: Statistical Machine Translation, System Combination, Quality Estimation. Second ML4HMT Workshop, pages 55–64, COLING 2012, Mumbai, December 2012. 55  
This paper gives the system description of the neural probabilistic language modeling (NPLM) team of Dublin City University for our participation in the system combination task in the Second Workshop on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid MT (ML4HMT-12). We used the information obtained by NPLM as meta information to the system combination module. For the Spanish-English data, our paraphrasing approach achieved 25.81 BLEU points, which lost 0.19 BLEU points absolute compared to the standard confusion network-based system combination. We note that our current usage of NPLM is very limited due to the difﬁculty in combining NPLM and system combination. KEYWORDS: statistical machine translation, neural probabilistic language model, system com- bination. Second ML4HMT Workshop, pages 65–76, COLING 2012, Mumbai, December 2012. 65  
Semantic interpretation and representation of natural language texts is an important task of natural language processing. A number o f semantic representations have been used to represent natural language using conceptual representations. Graph based semantic representations such as semantic networks (Masterman, 1961) is a declarative g raphic representation consisting of definit ional, assertional, imp licational, executable, learning and hybrid networks. Silv io (1961) proposed correlational nets based on the relations such as part whole, case relations, instance, subtype and dependence relations. Yet another graph representation that Hays (1964) proposed dependency graphs based on minimal syntactic units and conceptual graphs (Sowa, 1976) represents relations using inferences of first order logic. Similar semantic graph representation which contains semantic relat ions and attributes is Universal Networking Language (UNL) relations (UNDL, 1997) consists of 46 relat ions include agent, object, place, time, conjunction, disjunction, co -occurrence, content, quantity etc. represent semantics of natural language. Our work focuses on the identificat ion of sub -graphs of semantic graphs for which we use UNL representation. The detailed study on UNL is described in section 3. Identify ing sub-graphs fro m a semantic graph though a difficu lt task is necessary to obtain the boundaries between complex semantic entities and which in turn helps in understanding the complete meaning conveyed by a natural language sentence. In this paper, we focus on identifying sub-graphs and building a nested graph structure for diffe rent types of natural language sentences in morphologically rich and re lative ly free word order languages. In this paper, we des cribe the identification of semantic sub-graphs from Tamil, a morphologically rich and relat ively free word order language. Here, we define two set of rules one based on word associated features and another based on context oriented features to build a hyper-graph or nested UNL graph (NG) structure to represent sub-graphs of different phrases and clauses of sentences. We build the nested graph structure in two stages; one, the identification of sub-graphs while building simple graphs (SG) and second, the identification of sub-graphs after the simp le g raph (SG) construction. The paper is organized as follows. Section 2 discusses the related works carried out using nested graph structure. Section 3 discusses the nested UNL graphs and the rules defined for sub-graph identification and construction. The evaluation and performance of the defined rules are investigated in section 4. Finally, section 5 concluded with future enhancements. 2. Related Work Since we are focusing on the identification and construction of nested UNL graphs, first we discuss UNL in detail and the other similar semantic graph representations in this section. UNL (UNDL, 2011) is an electronic language designed to represent semantic data extracted fro m natural language texts . UNL consists of Universal words (UWs) represents concepts, relations represent the semantic relat ionship between concepts and attributes represents mood, aspect, tense etc. UNL representation is a directed acyclic graph representation in which nodes are concepts and links are relations exist between the concepts. (Blanc, 2000) described the French UNL Deconverter in wh ich the issues in representing the semantic characteristics of predicative concepts have been discussed. Dikonov (2008) d iscussed the representation of UNL graphs by segmenting co mplex graphs into simp le graphs by applying rules based on propositions. Coreferential lin ks are also considered in segmenting the UNL 
 Machine Translation (MT) for low-resource language has low-coverage issues due to Out-OfVocabulary (OOV) Words. In this research we propose a method using sublexical translation to achieve wide-coverage in Example-Based Machine Translation (EBMT) for English to Bangla language. For sublexical translation we divide the OOV words into sublexical units for getting translation candidates. Previous methods without sublexical translation failed to find translation candidate for many joint words. In this research using WordNet and IPA transliteration algorithm we propose to translate OOV words with explanation. The proposed method is better than previous OOV words handling. Our proposal improved translation quality by 20 points in human evaluation. KEYWORDS : Example-Based Machine Translation, Out-Of-Vocabulary Words, WordNet, Word Sense Disambiguation  Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages (MTPIL-2012), pages 39–52, COLING 2012, Mumbai, December 2012. 39  
This paper is organized as follows. Section 2 discusses the work related to discourse parsing. Section 3 illustrates the power of sangatis in representing text and about the proposed sangati based discourse parser; explains the comparison of RST and sangati and describes the RST-Sangati based discourse parser. Section 4 provides the evaluation of the discourse parsers. Conclusions and reference section follow section 4. 2 Related Work Many techniques have been proposed for discourse parsing. Marcu et have come up with an unsupervised approach that identifies five discourse relations namely, Contrast, Explanation, Evidence, Condition and Elaboration al (Marcu et al, 2002). They have used discourse markers and frequently co-occurring word pairs to identify the discourse relations. Hassan et al have designed a rule based discourse parser for Arabic language and they have used cues to identify the discourse relations (Hassan et al, 2008). Using cue phrases, many discourse parsing techniques have been proposed in various languages namely, Mandarin (Songren, 1985), Spanish (Lorraine, 1986), Thai (Sithipoun et al, 2010), and Bengali (Dipankar Das et al, 2010). Recently, Hugo Hernault et al have come up with a discourse parser named HILDA that labels discourse relations using Support Vector Machines (SVM) (Hernault et al, 2010). It has also been claimed that HILDA is computationally more efficient than the earlier techniques on discourse parsing proposed by Reitter (Reitter, 2003) Baldridge et al (Baldridge et al, 2005). HILDA uses lexical and syntactic features trained by the SVM classifier. SVM has been used both as a binary classifier as well as a multi class classifier for discourse parsing. As stated previously, the discourse parser discussed in this paper is the extension of our previous work on language independent discourse structure framework using Universal Networking Language (UNL) (Subalalitha et al, 2011). UNL is a computer language that enables computers to process information and knowledge across the language barriers. Given a sentence, UNL represents it as graph, with nodes as concepts and UNL relations as edges which is known as Enconversion (Enconversion Specifications, 2009). There are 46 UNL relations identified by UNDL. Obj (Object), agt (Agent), plc (Place) are few UNL relations. For instance, for the sentence shown in Example 2, the agent concept, “Ram (iof>person)” and the verb concept, “kill (icl> action)” are connected by the UNL relation, “agt” and the object concept, “Ravana (iof> person)” is connected to the verb concept by the UNL relation, “obj”. Example 2: Ram killed Ravana In the existing RST based discourse parser, the NRS sequences are identified by exploring the similarities that exist between UNL relations and discourse relations relations (Subalalitha et al, 2011). Also by using UNL, the discourse structure formed, becomes language independent which is first of its kind. This paper proposes a similar language independent discourse parser that identifies sangatis using UNL. The details of sangatis and how they are used to build a discourse structure is discussed in the next section. 75  3. RST-Sangati: A comparison: While comparing RST based discourse relations with sangati, both representations are unique in their own way. Table 1 shows the list of sangatis considered along with their English meaning and explanation. Also, discourse relations that are similar and equivalent are listed. Like discourse relations certain sangatis are multi nuclear which is mentioned in the table.  S.No 1 2 3 4  Sangati Upodgh̄ta ( Introduction) apav̄da ( Exception) (multi nuclear) ̄ḳepa ( Objection) (multi nuclear) pr̄sangika ( Related) (multi nuclear)  Equivalent and Similar Discourse Relations  Equivalent Similar  Explanation  Discourse Relation Preparation  Discourse Relation -  -  Contrast  -  Contrast  -  -  upodgh̄ta sangati gives an  introduction of the text.  Preparation discourse relation  prepares the reader to expect and  interpret the text to be presented.  apav̄da sangati indicates an  exceptional Scenario.  Contrast discourse relation may  indicate an exceptional scenario but  not always  ̄ḳepa sangati  indicates an  objectional statement.  Contrast discourse relation may  indicate an objectionable statement but  not always  pr̄sangika is a unique sangati and  does not have an equivalent or similar  discourse relation  5  utt̄na ( Arises)  -  (multi nuclear)  6  sthiri̅ karạa ( Strengthen)  -  7  ̄tideśika ( Transference)  -  (multi nuclear)  8  pr a tya va stha na ( Re-instate)  -  Antithesis Justification -  utt̄na sangati indicates a new issue related to the text. Antithesis discourse relation may indicate a new issue but not always sthiri̅ karạa sangati indicates a strengthening text supporting the topic in focus new issue related to the text. Justification discourse relation may indicate a supporting reasons but the supporting reason may not be a strengthening reasoning always ̄tideśika is a unique sangati and does not have a similar or equivalent discourse relation. pratyavasthana is a unique sangati and does not have a similar or equivalent discourse relation.  76  9  ḍ̣̣anta ( Example)  -  Elaboration  ḍ̣̣anta sangati indicates an example Scenario  Elaboration discourse relation may  indicate an example scenario but not  always.  10  pratyudharạa (Counter  -  Contrast  pratyudharạa sangati indicates an counter example scenario  Example)  Contrast discourse relation may  indicate a counter example scenario  but not always.  11  Ana nta r a (Follows)  Sequence -  anantara sangati links the texts in sequence similar to the Sequence  (multi nuclear)  discourse relation  12  viśẹa (Special Case)  -  Elaboration  viśẹa sangati explains a speciality. Elaboration discourse relation may  describe a specialty but not always.  TABLE 1- A comparison between sangatis and Discourse Relations  It can be observed from the Table 1 that, most of sangatis are unique and specific to a scenario. A discourse relation can be mapped to one more scenarios. For instance, the scenario linked by an Elaboration discourse relation may be an explanation, additional information or it may be a strengthening statement to the scenario. Whereas, distinct sangatis are available to handle the different scenarios handled by the Elaboration discourse relation. For instance, if the additional information of a scenario is just an added information but related to the scenario, it can be linked by pra̅ sȧgika sangathi. If the explanatory text denotes an example or a counter example, it can be linked by ḍ̣̣anta and pratyudharạa sangatis. If the explanatory text describes the speciality of the scenario, it can be linked by viśẹa sangati. On the other side, there are unique RST based discourse relations as well. This paper considers the list of RST based discourse relations considered by Mann et al (1988). It is observed that, there are many discourse relations that handle various scenarios which is not possible with sangatis. Concession, Evaluation, Background are some of the unique discourse relations. Also apart from the list of sangatis discussed here, there are more number of sangatis that need to be explored. For clausal level discourse analysis, discourse relations are more effective than sangatis. Whereas, sangatis go well beyond clause level. So for an efficient and complete discourse parsing, discourse relations and sangatis need to be used in union. The next section discusses how these sangatis are identified. 3. 2 Identifying Sangatis:  The sangati based discourse parser makes use of only UNL to identify the sangatis whereas, the RST-Sangati based discourse parser makes use of both UNL and RST based discourse relations to identify sangatis. This section illustrates about the sangati based discourse parser. 3.2.1 Sangati Based Discourse Parser:  The UNL components such as semantic constraints and UNL relations are used in combination to identify the sangatis. This is similar to the identification of RST based discourse relations done in the existing work (Subalalitha et al, 2011). For example, in the sentences given  77  in the Example 3, the pra̅ sȧgika sangathi is identified from the UNL relations and UNL concept similarity that exists between the UNL graphs constructed for each sentence.  Example 3: I went to Chennai last Friday. Travelling to Chennai during Fridays is terrific.  The rules for identifying sangatis using UNL components are given in Table 2. The rules provide the seed feature set for seven sangatis. The additional features for each sangati are learnt using Naive Bayes Probabilistic classifier whose details is given in the next section.  S.No. 1 2 3 4 5 6 7  Sangati upaj̄vya ̄tideśika anantara viśẹa pra̅ sȧgika upodgh̄ta ḍ̣̣anta  Rules Presence of iof, nam and met UNL relations in Satellite along with UNL concept/ semantic constraint similarity between Nucleus and Satellite UNL graphs UNL graph similarity between the nuclei in terms of concepts, except the main subject. Presence of Seq UNL relation in one of the nuclei UNL graphs. Also used as default relation for tourism documents. Presence of Cue such as “speciality”, “uniqueness” in satellite UNL graph+ UNL relation “pos” in nucleus UNL graph. Presence of identical UNL relations and with UNL concept/ semantic constraint similarity between two nuclei UNL graphs. Presence of UNL concept that is connected to the verb frequently in the document UNL graph which becomes the nucleus and the rest of the text becomes the satellite. Presence of iof UNL relation in the UNL graphs in the satellite UNL graphs along with UNL concept/ semantic constraint similarity between the nucleus and satellite UNL graphs.  TABLE 2- Rules for identifying sangatis using UNL  3.2.2 Learning with Naïve Bayes Probabilistic classification  For a set of text documents, sangati representation of the texts is constructed using the seed features listed in Table 2, which forms the training set. Learning of new features using Naive bayes probabilistic classifier is discussed below. For a sangati Si, the nuclei and satellites connected by it are used as the context. Let Sedu denote the set of sangatis listed in table 2. a) For each sangati Siϵ Sedu, all the nuclei sub graphs connected by Si are extracted from the training set.  b) The nuclei sub graphs extracted at step 1 contains a set of UNL relations in them. These UNL relations are considered as the context window from which the additional features could be learnt. For instance, if m number of nuclei sub graphs is extracted for sangati Si, m number of series of UNL relations is extracted. These series indicate possible additional features that could signal sangati Si apart from the seed feature(s) listed in table 2.  c) Let us denote the additional features as fi, where i ranges from 0 to m. Bayesian  probabilities P (fi/Si ) are comipumted for all features. d) Total Probability, Probtot  P ( fi / Si) and the average probability , i0 Probavg =Probtot /m are calculated.  78  e) The features whose probabilities are more than the Probavg are chosen as additional features that could signal the sangati Si. f) In the testing phase, the classifier tries to match the features present in the nuclei sub graphs with the seed features and the additional features learnt and identify the sangati.  3.3 RST-Sangati Based Discourse Parser  The RST-Sangati based discourse parser identifies both sangatis and RST based discourse relations using UNL and RST. The rules for identifying sangatis using UNL and RST are given in Table 3.  S Sangati No  Rules  
This paper presents the disambiguation of preposition in English to the corresponding post positions in Malayalam while translating text from English to Malayalam. Preposition in English will be replaced with post position in Indian Languages during translation. The polysemous nature of prepositions in English increases the difficulty in determining its exact meaning/function. This makes the task of mapping a preposition in English to an Indian language difficult, particularly for machine translation. Research or works on one to one mapping of English prepositions to Malayalam postpositions are not done for Malayalam. In this paper we illustrate the patterns of preposition mapping from English to Malayalam and present context disambiguation rules for determining the right mapping of different prepositions in English to postposition in Malayalam. Keywords: Postposition, cases, suffixes Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages (MTPIL-2012), pages 93–102, COLING 2012, Mumbai, December 2012. 93  1. INTRODUCTION  It has been observed that almost all postpositions in Malayalam function as case endings.  Postpositions in Malayalam occur after the nominal. They perform similar to inflectional  markers. But unlike inflectional markers, postpositions in Malayalam are free forms. Being  invariants, they can stand alone or alongside another free or bound morpheme. Postpositions (PP)  in Malayalam [1], [2], [3] are certain forms, which occur immediately after nouns and establish  some grammatical relations between the nouns and the verbs of the sentences. In English, the  semantic distribution of a single preposition will be varying in different context due to the  influence of nouns and main verbs that follow. When an English preposition is translated into  Malayalam, the following transformation takes place:  (preposition) (object)  (object) [(inflection)][(postpositional-word)].  I played with Ram.  ŋa:n ra:manre ku:te kaliccu (  )  In the example given above the object is modified with a suffix and postposition after it. It is not necessary that both suffix and the postpositional equivalent word occur simultaneously. They may occur independently or together. It will depend on the type of object and verb.  The factors that determine the generation of appropriate postposition in Malayalam for a preposition in English is discussed in detail in this paper.  The correspondence between English prepositions and Malayalam postpositions (inflections and postpositional words) is not direct. The reference object plays a major role in determining the correct preposition sense. Reference object will decide whether the preposition is used in a spatial sense or other sense. A noun phrase (NP) denoting a place gives rise to a spatial postposition. Similarly, an object referring to a time entity produces a temporal expression.  For instance, a preposition with can have multiple mapping patterns in Malayalam, as shown below.  Example (1)  a. [with = kont(  )]  I wrote with the pen. ŋa:n pe:na kont1 eYuwi.  (I) (pen) (PP) b. [with = kute( )]  (Write PST)  Radha went with Gopi. ra:dha go:piyute kute pOyi.  (Radha) (Gopi)  (PP) (go PAST)  c. [with = pakkal/vaSaM(  / )]  
Various experiments from literature suggest that in statistical machine translation (SMT), applying either pre-processing or post-processing to morphologically rich languages leads to better translation quality. In this work, we focus on the English-Tamil language pair. We implement sufﬁx-separation rules for both of the languages and evaluate the impact of this preprocessing on translation quality of the phrase-based as well as hierarchical model in terms of BLEU score and a small manual evaluation. The results conﬁrm that our simple sufﬁx-based morphological processing helps to obtain better translation performance. A by-product of our efforts is a new parallel corpus of 190k sentence pairs gathered from the web. KEYWORDS: English-Tamil Machine Translation, Parallel Corpora, Sufﬁx Separation. Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages (MTPIL-2012), pages 113–122, COLING 2012, Mumbai, December 2012. 113  
1. Introduction: Telugu, a major Dravidian language, spoken mostly in South India is a morphologically complex language. Various grammatical categories like case, gender, number and person are morphologically encoded and serve as strong cues for identifying the syntactico-semantic relations between the various parts of a sentence. Paninian framework which is based on dependency relations is considered the most suitable for analysing languages like Telugu (Rafiya Begum eT: al., 2008). This paper confines itself to examining the dative suffix -ki/ -ku in Telugu, because this appears to be the most ambiguous of all the case markers exhibiting as many as 16 meaning relations. Dative is the common denominator used while referring to the relations realized by the suffix -ki/ -ku. The various functions performed by the case marker -ki/ku have been discussed extensively in the traditional grammars of Telugu such as Bala Vyaakaranam and Proudha Vyaakaranam and in the modern grammars (Arden 1927; Campbell 1817; Krishnamurti & Gwynn 1985; Ramarao 1975 ) as well. Nouns case marked for dative have also been studied in the generative frame work proposed by Chomsky (Subbarao & Bhaskara Rao 2004). We propose to devise, an algorithm for the purpose of implementing a rule based procedure predicting the suffix -ki/-ku on the basis of the ontological properties of the nouns to which these case markers are attached to as well as those of verbs in the sentence. The road map of the paper is as follows: Section 1 introduces the objectives of the study; Section 2 spells out the approach we have adopted while analyzing the data; Section 3 provides examples from the data under each head classified; Section 4 provides the algorithm while Section 5 summarizes the results in the form of conclusion. Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages (MTPIL-2012), pages 123–132, COLING 2012, Mumbai, December 2012. 123  
The present paper describes a three stage technique to parse Hindi sentences. In the first stage we create a model with the features of head words of each chunk and their dependency relations. Here, the dependency relations are inter-chunk dependency relations. We have experimentally fixed a feature set for learning this model. In the second stage, we extract the intra-chunk dependency relations using a set of rules. The first stage is combined with the second stage to build a two-stage word level Hindi dependency parser. In the third stage, we formulate some rules based on features and used them to post-process the output given by the two-stage parser.  
There have been many studies on ﬁnding what people are interested in at any time through analysing trends in language use in documents as they are published on the web. Few, however have sought to consider material containing subject matter that originates in social media. The work reported here attempts to distinguish such material by ﬁltering out features that trend primarily in news media. Trends in daily occurrences of nouns and named entities are examined using the ICWSM 2009 corpus of blogs and news articles. A signiﬁcant number of trends are found to originate in social media and that named entities are more prevalent in them than nouns. Taking features that trend in later news stories as a indication of a topic of wider interest, named entities are shown to be more likely indicators although the strongest trends are seen in nouns. KEYWORDS: Social media, Trend analysis. Proceedings of the Workshop on Information Extraction and Entity Analytics on Social Media Data, pages 1–16, COLING 2012, Mumbai, December 2012. 
The popular microblogging service Twitter provides a vast amount of short messages that contains interesting information for Information Extraction tasks. This paper presents a rulebased system for the recognition and semantic disambiguation of named entities in tweets. As our experimental results shows, performance of this approach measured through BDM looks promising when using Linked Data as Freebase and syntactical context for disambiguation. KEYWORDS: Ontology-based Information Extraction, Disambiguation, Twitter, Linked Data. 
Economic analysis indicates a relationship between consumer sentiment and stock price movements. In this study we harness features from Twitter messages to capture public mood related to four Tech companies for predicting the daily up and down price movements of these companies’ NASDAQ stocks. We propose a novel model combining features namely positive and negative sentiment, consumer conﬁdence in the product with respect to ‘bullish’ or ‘bearish’ lexicon and three previous stock market movement days. The features are employed in a Decision Tree classiﬁer using cross-fold validation to yield accuracies of 82.93%,80.49%, 75.61% and 75.00% in predicting the daily up and down changes of Apple (AAPL), Google (GOOG), Microsoft (MSFT) and Amazon (AMZN) stocks respectively in a 41 market day sample. KEYWORDS: Stock market prediction, Named entity recognition (NER), Twitter, Sentiment analysis. Proceedings of the Workshop on Information Extraction and Entity Analytics on Social Media Data, pages 23–38, COLING 2012, Mumbai, December 2012. 23  
In this paper, we present an end-to-end pipeline for sentiment analysis of a popular micro-blogging website called Twitter. We acknowledge that much of current research adheres to parts of this pipeline. However, to the best of our knowledge, there is no work that explores the classiﬁer design issues explored in this paper. We build a hierarchal cascaded pipeline of three models to label a tweet as one of Objective, Neutral, Positive, Negative class. We compare the performance of this hierarchal pipeline with that of a 4-way classiﬁcation scheme. In addition, we explore the trade-off between making a prediction on lesser number of tweets versus F1-measure. Overall we show that a cascaded design is better than a 4-way classiﬁer design. Keywords: Sentiment analysis, Twitter, cascaded model design, classiﬁer conﬁdence. 1 Introduction Microblogging websites have evolved to become a source of varied kind of information. This is due to nature of microblogs on which people post real time messages about their opinions on a variety of topics, discuss current issues, complain, and express positive sentiment for products they use in daily life. In fact, companies manufacturing such products have started to poll these microblogs to get a sense of general sentiment for their product. Many times these companies study user reactions and reply to users on microblogs.1 One challenge is to build technology to detect and summarize an overall sentiment. In this paper, we look at one such popular micro-blog called Twitter2 and propose an end-to-end pipeline for classifying tweets into one of four categories: Objective, Neutral, Positive, Negative. Traditionally, Objective category is deﬁned as text segments containing facts and devoid of opinion (Pang and Lee, 2004; Wilson et al., 2005). In the context of micro-blogs, we extend this deﬁnition to include intelligible text, like “SAPSPKSAPKOASKOP SECAFLOZ PSOKASPKOA”. Note, since we are only concerned with sentiment analysis of English language micro-blogs, text in other languages will also fall under the intelligible category and thus under Objective text. One option to classify tweets into one of the four aforementioned categories is to simply implement a 4-way classiﬁer. Another option is to build a cascaded design, stacking 3 classiﬁers on top of each other: Objective versus Subjective, Polar versus Non-polar and Positive versus Negative. In this paper, we explore these possibilities of building a classiﬁer. In addition, we study the trade-off between making predictions on lesser number of examples versus F1-measure. If the conﬁdence of the classiﬁer falls below a threshold, we reserve prediction on that example. In expectation, this will boost the F1-measure, because we are reserving prediction on harder examples. But a-priori the 1http://mashable.com/2010/04/19/sentiment-analysis/ 2www.twitter.com Proceedings of the Workshop on Information Extraction and Entity Analytics on Social Media Data, pages 39–44, COLING 2012, Mumbai, December 2012. 39  relation between the two (threshold and F1-measure) is unclear. Moreover, it is unclear in which of the aforementioned three designs, this trade-off is least. We present this relation graphically and show that one of the cascaded designs is signiﬁcantly better than the other designs. We use manually annotated Twitter data for our experiments. Part of the data, Positive, Negative and Neutral labeled tweets were introduced and made publicly available in (Agarwal et al., 2011). Annotations for tweets belonging to the Objective category are made publicly available through this work.3 In this paper, we do not introduce a new feature space or explore new machine learning paradigms for classiﬁcation. For feature design and exploration of the best machine learning model, we use our previous work (Agarwal et al., 2011). 2 Literature Survey In most of the traditional literature on sentiment analysis, researchers have addressed the binary task of separating text into Positive and Negative categories (Turney, 2002; Hu and Liu, 2004; Kim and Hovy, 2004). However, there is early work on building classiﬁers for ﬁrst detecting if a text is Subjective or Objective followed by separating Subjective text into Positive and Negative classes (Pang and Lee, 2004). The deﬁnition of Subjective class for Pang and Lee (2004) contains only Positive and Negative classes, in contrast to more recent work of Wilson et al. (2005), who additionally consider Neutral class to be part of Subjective class. Yu and Hatzivassiloglou (2003) build classiﬁers for the binary task Subjective versus Objective and the ternary task Neutral, Positive and Negative. However, they do not explore the 4-way design or the cascaded design. One of the earliest work to explore these design issues is by Wilson et al. (2005). They compare a 3-way classiﬁer that separates news snippets into one of three categories: Neutral, Positive and Negative, to a cascaded design of two classiﬁers: Polar versus Non-polar and Positive versus Negative. They deﬁned Polar to contain both Positive and Negative class and Non-polar to contain only Neutral class. We extend on their work to compare a 4-way classiﬁer to a cascaded design of three models: Objective versus Subjective, Polar versus Non-polar and Positive versus Negative. Note, this extension poses a question about training the Polar versus Non-polar model: should Non-polar category only contain Neutral examples or both Neutral and Objective. Of course, the 4-way classiﬁer puts all three categories (Objective, Positive and Negative) together while training a model to detect Neutral. In this paper, we explore these designs. In the context of micro-blogs such as Twitter, to the best of our knowledge, we know of no literature that explores this issue. Barbosa and Feng (2010) build two separate classiﬁers, one for Subjective versus Objective classes and one for Positive versus Negative classes. They present separate evaluation on both models but do not explore combining them or comparing it with a 3-way classiﬁcation scheme. More recently, (Jiang et al., 2011) present results on building a 3-way classiﬁer for Objective, Positive and Negative tweets. However, they do not explore the cascaded design and do not detect Neutral tweets. Moreover, to the best of our knowledge, there is no work in the literature that studies the trade-off between making less predictions and F1-measure. Like human annotations, predictions made by machines have conﬁdence levels. In this paper, we compare the 3 classiﬁer designs in terms of their ability to predict better given a chance to make predictions only on examples they are most conﬁdent on. 3Due to Twitter’s recent policy, we might only be able to provide the tweet identiﬁers and their annotation publicly available: http://www.readwriteweb.com/archives/how_recent_changes_to_twitters_terms_of_service_mi.php 40  3 End-to-end pipeline with cascaded Models The pipeline for end-to-end classiﬁcation of tweets into one of four categories is simple: 1) crawl the tweets from the web, 2) pre-process and normalize the tweets, 3) extract features and ﬁnally 4) build classiﬁers that classify the tweets into one of four categories: Objective, Neutral, Positive, Negative. We use our previous work for pre-processing, feature extraction and selection of suitable classiﬁer (Agarwal et al., 2011). We found Support Vector Machines (SVMs) to perform the best and therefore all our models in this paper are supervised SVM models using Senti-features from our previous work. The main contribution of this work is the exploration of classiﬁer designs. Following is a list of possible classiﬁer designs:  1. Build a 4-way classiﬁer. Note, in a 4-way classiﬁcation scheme, a multi-class one-versus-all SVM builds 4 models, one for identifying each class. Each model is built by treating one class as positive and the remaining three classes as negative. Given an unseen example, the classiﬁer passes this through the four models and predicts the class with highest conﬁdence (as given by the four models). 2. Build a hierarchy of 3 cascaded models: Objective versus Subjective, Polar versus Non-Polar and Positive versus Negative. But there is one design decision to be taken here: while building the Polar versus Non-polar model, do we want to treat both Neutral and Objective examples as Non-polar or only Neutral examples as Non-polar? This decision affects the way we create the Polar versus Non-polar model. Note, a 4-way model, implicitly treats Neutral to be Non-polar and the remaining three classes to be Polar. This scenario is unsatisfying because a-priori there is no reason why Objective examples should be treated as Polar at the time of training. We explore both these options: (a) PNP-neutral: Polar versus Non-polar model, where only Neutral examples are treated as Non-polar whereas Positive and Negative examples combined are treated as Polar. (b) PNP-objective-neutral: Polar versus Non-polar model, where Neutral and Objective examples combined are treated as Non-polar whereas Positive and Negative examples combined are treated as Polar.  In this paper, we present results for each of the aforementioned design decisions in training models.  Moreover, we explore the trade-off between predicting on fewer number of examples and its affect  on the F1-measure. It is not hard to imagine, especially when the output of the classiﬁer is presented  to humans for judgement, that we might want to reserve predictions on examples where the classiﬁer  conﬁdence is low. A recent example scenario is that of Watson (Ferrucci et al., 2010) playing the  popular gameshow Jeopardy! Watson buzzed in to answer a question only if it was conﬁdent over a  certain threshold. We perform classiﬁcation with ﬁltering, i.e. considering classiﬁer conﬁdence  along with its prediction. If the classiﬁer conﬁdence is below a certain threshold, we do not make a  prediction on such examples. If for some value of threshold, θ , we reserve predictions on say x  test examples, and say the total number of test examples is N , then the Rejection rate is given by  x N  ∗  100%.  41  Class Objective Neutral Positive Negative  # instances for training 1859 1029 1042 1020  # instances for testing 629 344 350 327  Table 1: Number of instances of each class used for training and testing  4 Experiments and Results In this section, we present experiments and results for each of the pipelines described in section 3: 4-way, PNP-only-neutral and PNP-objective-neutral. Experimental Setup: For all our experiments, we use support vector machines with linear classiﬁer to create the models. We perform cross-validation to choose the right C value that determines the cost of mis-classifying an example at the time of learning. We report results on an unseen test set whose distribution is given in Table 1.  4.1 Classiﬁer design As explained in section 3, it is not clear a-priori, which of the three design decisions (4-way, PNP-neutral, PNP-objective-neutral) is most appropriate for building and end-to-end pipeline for sentiment analysis of Twitter data. Our results show that that PNP-objective-neutral gives a statistically signiﬁcantly higher F1-measure for Neutral category, while giving same ball-park F1-measure for other three categories as compared to the other two design options.  Category Objective Neutral Negative Positive Average  4-way P R F1 0.70 0.87 0.78 0.51 0.30 0.38 0.56 0.56 0.56 0.59 0.56 0.57 0.59 0.57 0.57  PNP-neutral P R F1 0.77 0.76 0.76 0.48 0.22 0.31 0.49 0.64 0.56 0.51 0.67 0.58 0.56 0.57 0.55  PNP-objective-neutral  PR  F1  0.78 0.76 0.77  0.39 0.46 0.42  0.57 0.57 0.57  0.61 0.53 0.57  0.59 0.58 0.58  Table 2: Results for different classiﬁer designs as mentioned in section 3. Note all numbers are rounded off to 2 signiﬁcant digits. Table 2 presents the result for the three design choices. For predicting the Objective class, all three designs perform in the same ball-park. For predicting the Neutral class, PNP-objective-neutral is signiﬁcantly better than 4-way and PNP-neutral, achieving an F1-measure of 0.42 as compared to 0.38 and 0.31 respectively. For predicting the remaining two classes, Positive and Negative, the performance of the three designs is in the same ball-park.  4.2 Trade-off between Rejection rate versus F1-measure Figure 1 presents a plot of rejection rate (on x-axis) versus mean F1-measure (on y-axis) for 4-way design (dotted green curve) and for PNP-objective-neutral (solid blue curve). The plot for the third design (PNP-neutral) is in the middle of these two curves and is omitted for clarity. First thing to note is that the rejection rate always increases faster than in F1-measure.  42  Figure 1: Rejection rate (on x-axis) versus mean F1-measure (on y-axis) for 4-way design (dotted green curve) and for PNP-objective-neutral (solid blue curve).  P  =  tp tp+ f  p;R  =  tp tp+ f  n; F1  =  2PR P +R  =  tp 2tp + f p +  f  n  where, P is precision, R is recall, F 1 is F1-measure, t p is number of true positive, f n is number of false negative, and f p is number of false positive.  In the best case scenario, reserving predictions will lead to decrease in number of false positives  and false negatives, without affecting true positives. So as x (number of test examples on which  we reserve predictions) increases, rejection rate increases, and f p + f n decreases (all linearly).  Therefore, F 1 ∝  
Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2012), pages 1–2, COLING 2012, Mumbai, December 2012. 
Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2012), pages 17–26, COLING 2012, Mumbai, December 2012. 17  1. Introduction With the rapid growth of web technology, people now express their opinion, experience, attitude, feelings, and emotions on the web. So, it has increased the demand of processing, organizing, and analyzing the web content to know the opinion of the users (Pang B. and Lee L., 2008). An automatic sentiment text classification means to identify the sentiment orientation of the text documents i.e. positive or negative. It is important for users as well as companies to know the opinion of users, for example review for electronic products like laptop, car, movies etc. can be beneficial for users to take decision on which product to purchase and for companies to improve and market their products. Various researchers have applied machine learning algorithms for sentiment analysis (Pang B. and Lee L., 2004; Tan S. and Zhang J., 2008; Pang B. and Lee L., 2008). One of the major problems in sentiment classification is to deal with huge number of features used for describing text documents, which produces hurdles to machine learning methods in determining the sentiment orientation of the document. Thus, it is required to select only prominent features which contribute majorly in the identification of sentiment of the document. The aim of feature selection methods is to produce the reduced feature set which is capable of determining sentiment orientation of the document by eliminating irrelevant and noisy features. Various feature selection methods has been proposed for selecting predominating features for sentiment classification, for example Information Gain (IG), Mutual Information (MI), Chi square (CHI), Gain Ratio (GR), Document Frequency (DF) etc. (Tan S. and Zhang J., 2008; Pang B. and Lee L., 2008). In the proposed approach, feature selection methods are used for improving the performance of the machine learning method. Initially, binary weighting scheme is used to represent the review documents, and then various feature selection methods are applied to reduce the feature set size. Further, machine learning methods are applied to the reduced and prominent feature set. Our contribution: 1. Two new feature selection methods i.e. PPD and CPPD are proposed for sentiment classification. 2. Compared the performance of proposed feature selection methods on two different standard datasets of different domains. The paper is organized as follows: A brief discussion of the related work is given in Section 2. Feature selection methods used for sentiment classification are discussed in Section 3. Dataset, Experimental setup and results are discussed in Section 4. Finally, conclusions and future work is described. 2. Related work Machine learning methods have been widely applied for sentiment classification (Pang B. and Lee L., 2004; Tan S. and Zhang J., 2008; Pang B. and Lee L., 2008). Pang et al. 2002, applied machine learning methods viz. Support Vector Machine (SVM), Naïve Bayes (NB), and Maximum Entropy (ME) for sentiment classification on unigram and bigram features of movie review dataset. Authors found SVM to be performed best among classifiers. Authors also found that binary weighting scheme outperforms Term Frequency (TF) method for representing the text for sentiment classification. Later, a minimum cut method is proposed to eliminate objective 18  sentences from the text (Pang B. and Lee L., 2004), which showed improved performance. Authors (Tan S. and Zhang J., 2008), experimented on five machine learning algorithms i.e. Knearest neighbour (KNN), Centroid classifier, Winnow classifier, NB and SVM with four feature selection methods those are MI, IG, CHI, and DF for sentiment classification on Chinese documents. Authors observed that IG performs best among all the feature selection methods and SVM gives best results among machine learning algorithms. Various feature selection methods have been proposed by various researchers for reducing the feature vector for sentiment classification for improved performance of machine learning methods (Tan S. and Zhang J., 2008; Pang B. and Lee L., 2008). Entropy Weighted Genetic Algorithm (EWGA) is proposed by combining the IG and genetic algorithm, which improved the accuracy of sentiment classification (Abbasi et al. 2008). Sentiment features are highlighted by increasing their weights, further authors used multiple classifiers on various feature vectors to construct the aggregated classifier (Dai et al. 2011). O’ keefe et al. 2009, compared three feature selection methods for sentiment classification, which are based on Categorical Proportional Difference (CPD) and Sentiment Orientation (SO) values. Wang et al. 2009, proposed Fisher's discriminant ratio based feature selection method text review sentiment classification. 3. Feature selection methods Feature selection methods select prominent features from the high dimensional feature vector by eliminating noisy and irrelevant features. Optimal feature vector improves the performance of the machine learning method in terms of both accuracy and execution time. 3.1 Probability Proportion Difference (PPD) Probability Proportion Difference (PPD) measures the degree of belongingness or probability that a term belongs to a particular class. Algorithm 1: Probability Proportion Difference (PPD) Feature Selection Method Input: Document corpus (D) with labels (C) positive or negative, k (number of Optimal features to be selected) Output: OptimalFeatureSet Step 1 Preprocessing t  ExtractUniqueTerms(D) F  TotalUniqueTerms(D) Wp  TotalTermsInPositiveClass(D,C) Wn  TotalTermsInNegativeClass(D,C) Step 2 Main Feature Selection loop for each t  F Ntp =CountPositiveDocumentsInwhichTermAppears(D,t) Ntn =CountNegativeDocumentsInwhichTermAppears(D,t) end for for each t  F end for OptimalFeatureSet  SelectTopTerm(k) 19  If a term has high probability of belongingness to dominantly one category/class (i.e. positive or negative) that indicates the term is important in identifying the category of unknown review. And if a term has almost equal probability of belongingness to both the categories, in that case the term is not useful in discriminating the class. PPD value of a term is calculated by computing the difference of probabilities that a term will belong to positive class or negative class. Thus, if a term has high PPD value, it indicates that the term is important for sentiment classification. Probability of belongingness of a term depends on the number of documents in which a term appears and number of unique terms appeared in that class. Algorithm for calculating PPD value of a term is given in Algorithm 1. Top k features can be selected on the basis of PPD value of the term. 3.2 Categorical Proportion Difference (CPD) Categorical Proportional Different (CPD) value measures the degree to which a term contributes in discriminating the class (Simeon et al. 2008). O’Keefe et al. 2009, have used CPD value for feature selection method. CPD value of a term is computed by finding the ratio of the difference between the number of documents of a category in which it appears and the number of documents in which it appears of another category, to the total number of documents in which that term appears. CPD value for a feature can be calculated by using equation 1. …. (1) Here, posD is the number of positive review document in which a term appears, and negD is the number of negative review documents in which that term appear. Range of CPD value is 0 to1. If any term appears dominantly in positive or negative class, then that feature is useful for the sentiment classification, and if a term is occurring in both the categories equally then that feature is not useful for classification. If CPD value of a feature is close to 1 it means that this feature is occurring dominantly in only one category of documents. For example if “Excellent” word is occurring in 150 positive review documents and in 2 negative review documents, then value of this feature will be (150-2)/(150+2)= 0.97 , its value is near to 1 indicates that this term is useful in identifying the class of unknown document. It indicates that if a new document is having “excellent” word, there is a high chance that this document belongs to positive category. Similarly if a word occurs in same number of positive and negative documents, then CPD value will be 0, which indicates that this term is not useful for classification. 3.3 Categorical Probability Proportion Difference (CPPD) Categorical Probability Proportion Difference (CPPD) based feature selection methods combines the merits and eliminates the demerits of both CPD and PPD methods. Benefit of CPD method is that it measures the degree of class distinguishing property of a term, which is an important attribute of a prominent feature. It can eliminate terms, which are occurring in both the classes equally and are not important for classification. It can easily eliminate the terms with high document frequency but are not important like stop words. However, PPD value of term indicates the belongingness/relatedness of a term to the classes and difference measures the class discriminating ability. It can remove the terms with less document frequency, which is not important for sentiment classification like rare terms. PPD feature selection method also considers the documents length of positive and negative reviews, since generally positive orientation documents are more in length as compared to negative class documents. So, there is a 20  high probability that most of the feature selection method select more positive sentiment words, as compared to negative sentiment words that result in less recall. However, in the proposed CPPD method, length of documents is considered in computing the CPPD value. Demerits of CPD feature selection method is that it can include rare term with less document frequencies but not important, which will be eliminated by PPD method. Similarly, PPD feature selection method may include term with high document frequency but not important, which will be removed by CPD method. So, by combining the merits and removing the demerits of CPD and PPD feature selection, a more reliable feature selection method is proposed for sentiment classification. CPPD feature selection method is described in algorithm2. Algorithm 2: Categorical Probability Proportion Difference (CPPD) Feature Selection Method Input: Document corpus (D) with labels (C) positive or negative Output: ProminentFeatureSet Step 1 Preprocessing t  ExtractUniqueTerms(D) F  TotalUniqueTerms(D) Wp  TotalTermsInPositiveClass(D,C) Wn  TotalTermsInNegativeClass(D,C) Step 2 Main Feature Selection loop for each t  F Ntp =CountPositiveDocumentsInwhichTermAppears(D,t) Ntn =CountNegativeDocumentsInwhichTermAppears(D,t) end for for each t  F  if (cpd >T1 && ppd>T2) ProminentFeatureSet  SelectTerm(t) end for 3.4 Information Gain (IG) Information Gain has been identified as one of the best feature selection method for sentiment classification (Tan S. and Zhang J., 2008). Therefore, we compared proposed feature selection methods with IG. Information gain (IG) is a feature selection method, which computes importance of a feature with respect to class attribute. It is measured by the reduction in the uncertainty in classification when the value of the feature is known (Forman G. 2003). Top ranked features are selected for reducing the feature vector size in turn better classification results. IG of a term can be calculated by using equation 2 (Forman G. 2003). () ∑ ( ) ( )  ( ) ∑ ( | ) ( | ) (⃑⃑⃑ ) ∑ ( |⃑⃑⃑ )  ( ⃑⃑⃑ ) ..(2)  21  Here, P(Cj) is the fraction of number of documents that belongs to class Cj out of total documents and P(w) is fraction of documents in which term w occurs. P(Cj|w) is computed as fraction of documents from class Cj that have term w and P(Cj|⃑⃑ ) is fraction of documents from class Cj that does not contain term w. 4. Experimental Setup and Result Analysis  4.1 Dataset and Experiments One of the most popular publically available standard movie review dataset is used to test the proposed feature selection methods (Pang B., and Lee L., 2004). This standard dataset, known as Cornell Movie Review Dataset is consisting of 2000 reviews that contain 1000 positive and 1000 negative labeled reviews. In addition, product review dataset (book reviews) consisting amazon products reviews has also been used (Blitzer et al. 2007). This dataset contains 1000 positive and 1000 negative labeled book reviews. Documents are initially pre-processed as follows: (i) Negation handling, “NOT_” is added to every words occurring after the negation word (no, not, isn’t, can’t etc.) in the sentence. Since, a negation word inverts the sentiment of the sentence (Pang B. and Lee L., 2002). (ii) Terms which are occurring in less than 2 documents are removed from the feature set. The feature vector generated after pre-processing is further used for the classification. Binary weighting scheme is used for representing text since it has been proved the best method for sentiment classification (Pang B. and Lee L., 2002). Among various machine learning algorithms Support Vector Machine (SVM) and Naïve Bayes (NB) classifiers are mostly used for sentiment classification (Pang B. and Lee L., 2002; O’Keefe et al. 2009; Abbasi et al. 2009; Pang B. and Lee L., 2008). So, in our experiments, SVM and NB are used for classifying review documents into positive or negative class. Evaluation of classification results is done by 10 folds cross validation (Kohavi R., 1995). Linear SVM and Naïve Bayes are used for all the experiments with default setting in weka machine learning tool (WEKA).  4.2 Performance measures To evaluate the performance of sentiment classification with various feature selection methods, F-measure (given in equation 3) is used. It combines precision and recall, which are commonly used measure. Precision for a class C is the fraction of total number of documents that are correctly classified to the total number of documents that classified to the class C (sum of True Positives (TP) and False Positives (FP)). Recall is the fraction of total number of correctly classified documents to the total number of documents that belongs to class C (sum of True Positives and False Negative (FN)).  (  )  ……… (3)  22  4.3 Results and discussions Some cases have been selected from movie review dataset and discussed. CPD and PPD values of some of cases have been shown in Table 1. CPD feature selection method has the drawback that less document frequent term can have very high CPD value, which is not important for classification. For example, if a term is having positive DF of 3 and negative DF of 0, then CPD value will be 1, which is maximum CPD value, even if the feature is not that important (refer case 1 of Table1). Similarly, if a term has positive DF of 1 and negative DF of 6, then CPD value comes out to be 0.714, which is quite high but the feature is not that important for classification (refer case 2 of Table1). This drawback is removed by using PPD feature selection method. Since, these types of terms have very low PPD value, so eliminated by PPD feature selection method. Also, in movie review dataset the term “poor” has low CPD value which is very important term for sentiment classification (refer case 3 of Table 1). This term will be eliminated by CPD method but would be selected by PPD method. Similarly, cases 4, 5, 6, of Table1 for terms “Oscar”, “perfect”, and “bad” respectively are important for sentiment classification, which are eliminated by CPD method but included by PPD method. In contrary, few terms with high DF would have high PPD value, but not important. These terms are eliminated by CPD method. For example, In Table 1 case 7 shows PPD value high for term “because”, it is eliminated by CPD method, but PPD value is high. It is due to the fact that PPD value depends on the DF and total terms in each class of the corpus. In this example, document length of positive reviews is larger as compared to length of negative reviews that is why the PPD value is high. Cases Positive DF Negative DF CPD PPD  
With the growth of web 2.0, people are using it as a medium to express their opinion and thoughts. With the explosion of blogs, journal like user-generated content on the web, companies, celebrities and politicians are concerned about mining and analyzing the discussions about them or their products. In this paper, we present a method to perform opinion mining and summarize opinions at entity level for English blogs. We ﬁrst identify various objects (named entities) which are talked about by the blogger, then we identify the modiﬁers which modify the orientation towards these objects. Finally, we generate object centric opinionated summary from blogs. We perform experiments like named entity identiﬁcation, entity-modiﬁer relationship extraction and modiﬁer orientation estimation. Experiments and Results presented in this paper are cross veriﬁed with the judgment of human annotators. KEYWORDS: Sentiment Analysis, Opinion Mining, English Blog, Object Identiﬁcation, Opinion Summary. Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2012), pages 53–64, COLING 2012, Mumbai, December 2012. 53  
Recently, we can refer to user reviews in the shopping or hotel reservation sites. However, with the exponential growth of information of the Internet, it is becoming increasingly difﬁcult for a user to read and understand all the materials from a large-scale reviews. In this paper, we propose a method for classifying hotel reviews written in Japanese into criteria, e.g., location and facilities. Our system ﬁrstly extracts words which represent criteria from hotel reviews. The extracted words are classiﬁed into 12 criteria classes. Then, for each hotel, each sentence of the guest reviews is classiﬁed into criterion classes by using two different types of Naive Bayes classiﬁers. We performed experiments for estimating accuracy of classifying hotel review into 12 criteria. The results showed the effectiveness of our method and indicated that it can be used for review summarization by guest’s criteria. KEYWORDS: hotel reviews, text segmentation, guest’s criteria. Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2012), pages 65–72, COLING 2012, Mumbai, December 2012. 65  
Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2012), pages 73–80, COLING 2012, Mumbai, December 2012. 73  
Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2012), pages 81–90, COLING 2012, Mumbai, December 2012. 81  
Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2012), pages 91–98, COLING 2012, Mumbai, December 2012. 91  
Traditional sentiment analysis has been focusing on inference of the sentiment polarity using sentiment-bearing words. In this paper, we propose a new way of studying sentiment and capturing ontological changes in a domain speciﬁc context in the perspective of computational linguistics using affect proxies. We used Nexis service to create a domain speciﬁc corpus focusing on banking sectors. We then created an affect dictionary from three kinds of lexica: sentiment lexica as in the General Inquirer dictionary; news ﬂow represented by domain entities such as ﬁnancial regulators and banks; and what we call contested term lexica, which consists of terms whose semantic implication is inconsistent over time. Univariate and multivariate analysis techniques such as factor analysis are used to explore the relationships and underlying patterns among the three types of lexica. Analysis results suggest that citations of regulatory entities show strong correlation with negative sentiments in the banking context. Also, a factor analysis was conducted, which reveals several groups of variables in which the contested terms correlate with positive and negative sentiments. KEYWORDS: sentiment analysis, affect proxy, computational linguistics, factor analysis, con- tested terms, ontological change. Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP 2012), pages 99–114, COLING 2012, Mumbai, December 2012. 99  
The aim of Linked Open Data (LOD) is to improve information management and integration by enhancing accessibility to the existing various forms of open data. The goal of this paper is to make Korean resources linkable entities. By using NLP tools, which are suggested in this paper, Korean texts are converted to RDF resources and they can be connected with other RDF triples. It is worth noticing that to the best of our knowledge there is a few of publicy available Korean NLP tools. For this reason, the Korean NLP platform presented here will be available as open source. And it is shown in this paper that the result of this NLP platform can be used as Linked Data entities. Keywords: Korean Natural Language Processing, NLP2RDF, Linked Open Data. Proceedings of the 10th Workshop on Asian Language Resources, pages 1–10, COLING 2012, Mumbai, December 2012. 
In this paper, we propose an approach to build a large scale text corpus for Tibetan natural language processing. We ﬁnd the distribution of Tibetan web pages on the internet with a crawler which can identify whether or not a web page contains Tibetan text. Three biggest web sites are selected, and topic pages are selected with a rule based method by checking the url. The layout structures of selected pages are analysed, and topic related information are extracted based on web site speciﬁc rules. Consequently, we get a corpus including more than 65 thousands documents, nearly 1.59 million sentences or 35 million syllables in total. Title and Abstract in Chinese 抽取网页文本为藏文自然语言处理构建大规模文本语料库 在本文中，我们提出了一种为藏文自然语言处理构建大规模文本语料库的方法。我们首先 使用网络爬虫技术，并结合藏文网页的编码识别技术判断一个网页中是否包含藏文文本， 并据此考察互联网上藏文网页的分布情况。然后，我们选择了三个最大的藏文网站，根据 网页的 URL，利用预先定义的规则，判断网页是 Hub 页面还是 Topic 页面。之后，我 们分析了每个网站的 Topic 页面的布局结构特点，并利用正则表达式编制了相应的 Topic 相关文本的抽取规则。采用上述方法，我们构建了一个包含 6.5 万文档，共计 159 万句、 3500 万藏文音节字的文本语料库。 Keywords: Tibetan, text corpus, web page, crawler, information extraction . Keywords in Chinese: 藏文, 文本语料, 网页, 爬虫, 信息抽取. Proceedings of the 10th Workshop on Asian Language Resources, pages 11–20, COLING 2012, Mumbai, December 2012. 11  
To study about various naturally occurring phenomenons on natural language text, a well structured text corpus is very much essential. The quality and structure of a corpus can directly influence on performance of various Natural Language Processing applications. Assamese is one of the major Indian languages used by the people of north east India. Language technology development works in Assamese language have been started at various levels, and research and development works started demanding a structured and well covered Assamese Corpus in UNICODE format. Here we present various issues and problems related to building an Assamese text corpus. We review our experience with constructing one such corpus including about 1.5 million words of Assamese language. It will provide a significant effort by serving as an important research tool for language and NLP researchers. KEYWORDS: Assamese, Corpus, linguistics, Natural Language Processing. Proceedings of the 10th Workshop on Asian Language Resources, pages 21–28, COLING 2012, Mumbai, December 2012. 21  
Collection of natural language texts in to a machine readable format for investigating various linguistic phenomenons is call a corpus. A well structured corpus can help to know how people used that language in day-to-day life and to build an intelligent system that can understand natural language texts. Here we review our experience with building a corpus containing 1.5 million words of Bodo language. Bodo is a Sino Tibetan family language mainly spoken in Northern parts of Assam, the North Eastern state of India. We try to improve the quality of Bodo corpora considering various characteristics like representativeness, machine readability, finite size etc. Since Bodo is one of the Indian language which is lesser reach on literary and computationally we face big problem on collecting data and our generated corpus will help the researchers in both field. KEYWORD : Bodo language, Corpus, Linguistics, Natural Language Processing. Proceedings of the 10th Workshop on Asian Language Resources, pages 29–34, COLING 2012, Mumbai, December 2012. 29  
We present two dependency parsers for Persian, MaltParser and MSTParser, trained on the Uppsala PErsian Dependency Treebank. The treebank consists of 1,000 sentences today. Its annotation scheme is based on Stanford Typed Dependencies (STD) extended for Persian with regard to object marking and light verb contructions. The parsers and the treebank are developed simultanously in a bootstrapping scenario. We evaluate the parsers by experimenting with different feature settings. Parser accuracy is also evaluated on automatically generated and gold standard morphological features. Best parser performance is obtained when MaltParser is trained and optimized on 18,000 tokens, achieving 68.68% labeled and 74.81% unlabeled attachment scores, compared to 63.60% and 71.08% for labeled and unlabeled attachment score respectively by optimizing MSTParser. KEYWORDS: Persian dependency treebank, dependency parsing, Farsi, Persian, MaltParser, MSTParser. 
Proceedings of the 10th Workshop on Asian Language Resources, pages 45–54, COLING 2012, Mumbai, December 2012. 45  
In this paper we describe a two-stage dependency parser for Bangla. In the first stage, we build a model using a Bangla dependency Treebank and subsequently this model is used to build a data driven Bangla parser. In the second stage, constraint based parsing has been used to modify the output of the data driven parser. This second stage module implements the Bangla specific constraints with the help of demand frames of Bangla verbs. The features of the words used in both these stages include morphological features like gender, number, person, etc., parts-of-speech tags, chunk tags and named entity tags. The evaluation results show that this two stage parser performs better than one stage parsers.  
The present paper identifies the mistakes made by a data driven Bengali chunker. The analysis of a chunk based machine translation output shows that the major classes of errors are generated from the verb chunk identification mistakes. Therefore, based on the analysis of the types of mistakes in the Bengali verb chunk identification we propose some modules. These modules use tables of manually created entries which are validated using chunk annotated and dependency annotated corpus. These modules are used to repair the Bengali verb chunks and subsequently to improve the quality of Bengali to Hindi transfer based machine translation system.  
We present a k-partite graph learning algorithm for ontology extraction from unstructured text. The algorithm divides the initial set of terms into different partitions based on information content of the terms and then constructs ontology by detecting subsumption relation between terms in different partitions. This approach not only reduces the amount of computation required for ontology construction but also provides an additional level of term ﬁltering. The experiments are conducted for Hindi and English and the performance is evaluated by comparing resulting ontology with manually constructed ontology for Health domain. We observe that our approach signiﬁcantly improves the precision. The proposed approach does not require sophisticated NLP tools such as NER and parser and can be easily adopted for any language. KEYWORDS: Ontology extraction, k-partite graph, wordnet, concept hierarchy.  Proceedings of the 10th Workshop on Asian Language Resources, pages 75–84, COLING 2012, Mumbai, December 2012. 75  
Bilingual terminology dictionaries are resources of much practical importance in many application of bilingual NLP. Because technical terminology can be both very speciﬁc and rapidly evolving, it can however be diﬃcult to obtain dictionaries with good coverage. Mining automatically such terminology from technical documents is therefore an attractive possibility. With this goal in mind, and following some previous works, we devise an algorithm that is eﬃcient at aligning the bilingual keyword list of scientiﬁc papers. Our results show that our approach can extract bilingual terms with very good precision and recall. Keywords: Hidden Markov Model, lexicon extraction, word alignment, alignment model. Proceedings of the 10th Workshop on Asian Language Resources, pages 85–94, COLING 2012, Mumbai, December 2012. 85  
Extraction of named entities (NEs) from the text is an important operation in many natural language processing applications like information extraction, question answering, machine translation etc. Since early 1990s the researchers have taken greater interest in this field and a lot of work has been done regarding Named Entity Recognition (NER) in different languages of the world. Unfortunately Urdu language which is a scarce resourced language has not been taken into account. In this paper we present a statistical Named Entity Recognition (NER) system for Urdu language using two basic n-gram models, namely unigram and bigram. We have also made use of gazetteer lists with both techniques as well as some smoothing techniques with bigram NER tagger. This NER system is capable to recognize 5 classes of NEs using a training data containing 2313 NEs and test data containing 104 NEs. The unigram NER Tagger using gazetteer lists achieves up to 65.21% precision, 88.63% recall and 75.14% f-measure. While the bigram NER Tagger using gazetteer lists and Backoff smoothing achieves up to 66.20% precision, 88.18% recall and 75.83 f-measure. KEYWORDS : Named Entity Recognition, Unigram model, Bigram model, Gazetteer lists, smoothing techniques 
Proceedings of the 10th Workshop on Asian Language Resources, pages 105–114, COLING 2012, Mumbai, December 2012. 105  
Lemmatization is crucial in natural language processing and information retrieval especially for highly inflected languages, such as Finnish and Mongolian. The state-of-the-art method of lemmatization for Mongolian does not need a noun dictionary and is scalable, but errors of this method are mainly caused by problems related to part of speech (POS) information. To resolve this problem, we integrate POS tagging and lemmatization for Mongolian. We evaluate the effectiveness of our method and its contribution to statistical machine translation.  KEYWORDS : Morphological segmentation, Lemmatization, Mongolian language, Statistical Machine Translation.  
In a Hindi to Bengali transfer based machine translation system the baseline lexical transfer module replaces a Hindi word by its most frequent Bengali translation. Some pronouns in Hindi can have multiple translations in Bengali. The choices of actual translations have big impact on the accessibility of the translated sentence. The list of Hindi pronouns is small and their corresponding Bengali translations may be judged using a set of rules. In this paper, we are working on the translations of ambiguous Hindi pronouns to possible Bengali pronouns. We observed the uses of Hindi pronouns in a Hindi corpus and formulated the translation rules based on their translations in parallel Bengali corpus.  
Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon (CogALex-III), pages 1–4, COLING 2012, Mumbai, December 2012. 
Lexical networks can be used with benefit for semantic analysis of texts, word sense disambiguation (WSD) and in general for graph-based Natural Language Processing. Usually strong relations between terms (e.g.: cat --> animal) are sufficient to help for the task, but quite often, weak relations (e.g.: cat --> ball of wool) are necessary. Our purpose here is to acquire such relations by means of online serious games as other classical approaches seems impractical. Indeed, it is difficult to ask the users (non experts) to define a proper weighting for the relations they propose, and then we decided to relate weights with the frequency of their propositions. It allows us to acquire first the strongest relations, but also to populate the long tail of an already existing network. Furthermore, trying to get an estimation of our network by the very users thanks to a tip of the tongue (TOT) software, we realized that they rather tend to favor the relations of the long tail and thus promote their emergence. Developing the long tail of a lexical network with standard and non-standard relations of low weight can be of advantage for tasks such that words retrieval from clues or WSD in texts. KEYWORDS : LEXICAL NETWORK, LONG TAIL, GAME WITH A PURPOSE, TIP OF THE TONGUE SOFTWARE, TYPED RELATIONS, WEIGHTED RELATIONS, WSD Introduction Lexical/semantic networks are very precious resources for NLP applications in general and for Word Sense Disambiguation (WSD) in particular. Their construction is delicate as automated approaches from corpora may have various shortcomings (mainly high noise level and/or low recall) and a manual approach may be long, tedious, costly and of unsatisfactory quality or coverage. A way of handling the building of such resources can be direct crowdsourcing (as contributive approaches) or indirect crowdsourcing through for instance serious games. What is a long tail in a lexical network? A lexical/semantic network (thereafter dubbed JDM) for French is under construction with methods based on popular consensus by means of games with a purpose named JeuxDeMots (Lafourcade 2007). Thus, in 5 years, a high number of players lead to the construction a large scale lexical network for the French language (currently more than 240 000 terms with around 1.4 million semantic relations) representing a common general knowledge but also including word senses referred as word usages (Lafourcade and Joubert, 2010). The relations of the lexical networks created this way are directed and typed, with classical ontological relations (like hypernym, hyponyms, part-of, whole, material/substance, ...), lexical relations (synonyms, antonyms, lexical family, ...); Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon (CogALex-III), pages 5–20, COLING 2012, Mumbai, December 2012. 5  semantic roles (agent, patient, instrument, ...) and less standard relations (typical location and time, cause, consequence, ...). Furthermore, relation occurrences are weighted which constitutes a quite original aspect in the lexical network domain exemplified by (for example) WordNet (Miller, 1990). The interpretation of a weight might by difficult but can be related to the strength of the relation as collectively perceived by speakers/players. The weight computation is done by emergence along with the gaming activity. Obviously by intuition, the relation cat--> animal is stronger than cat --> ball of wool, none withstanding their types. The lexical network has been made available (at http://jeuxdemots.org) and free to use by their authors, giving the research community a resource to play with. The question of the evaluation of its quality, usability in WSD and word recollection (Tip of the Tongue problem), and distributional properties are the main subjects of this article. One specific question is whether low weight but still important relations can be captured by some similar approaches and to which extend they are useful. We observed that many (if not most) relations in JDM are “frontal/direct/obvious” relations (e.g.: chat-->-feline), but some others are more farfetched/indirect. We wish to evaluate but also find practical ways to densify the network increasing the number of “indirect” relations (e.g.: chat --> allergy) belonging to the long tail. To do so, we use a TOT tool in a taboo mode, that is, refraining from using the strongest relations. In a first section, we will briefly remind to the reader the principles of long tail and the link with the network construction. Then, we introduce our TOT (tip of the tongue) tool, named AKI and we will explain the taboo mode, and show how it leads to densifying the JDM network. An evaluation of the long tailed network obtained is done for AKI and for a simplified WSD task. 
How abstract knowledge is organised is a key question in cognitive science, and has clear repercussions for the design of artifical lexical resources, but is poorly understood. We present fMRI results for an experiment where participants imagined situations associated with abstract words, when cued with a visual word stimulus. We use a multivariate-pattern analysis procedure to demonstrate that 7 WordNet style Taxonomic categories (e.g. 'Attribute', 'Event', 'SocialRole'), can be decoded from neural data at a level better than chance. This demonstrates that category distinctions in artificial lexical resources have some explanatory value for neural organisation. Secondly, we tested for similarity in the interrelationship of the taxonomic categories in our fMRI data and the associated interrelations in popular distributed semantic models (LSA,HAL,COALS). Although distributed models have been successfully applied to predict concrete noun fMRI data (e.g. Mitchell et al., 2008), no evidence of association was found for our abstract concepts. This suggests that development of new models/experimental strategies may be necessary to elucidate the organisation of abstract knowledge. KEYWORDS : FMRI, CONCEPT REPRESENTATION, ABSTRACT, MVPA, WORDNET 
We describe here the principles underlying the automatic creation of a semantic map to support navigation in a lexicon, our target group being authors (speakers, writers) rather than readers. While machines can generally access information that it has stored, this does not always hold for people. A speaker may very well know a word, yet still be (occasionally) unable to access it. To help authors to overcome word-finding problems one could add to an existing electronic resource an index based on the (age-old) notion of association. Since ideas or their expressive forms (words) are related, they may evoke each other (lemon-yellow), but the likelihood for doing so varies over time and with the context. For example, the word 'piano' may prime 'instrument' or 'weight', but which of the two gets evoked depends on the context: 'concert' vs. 'house moving'. Given this dynamic aspect of the human brain, we should build the index automatically, computing the relation of terms and their weights on the fly. This dynamic creation of the index could be done via a corpus. This latter representing ideally the dictionary users' world knowledge, and the way how the prominence of words and ideas varies over time. Another important point are link-names, i.e. the type of relationship holding between two associates: [(rose) <--color (red)]. Given the fact that any query (e.g. 'India') may yield many hits, hits whose weights may be misleading, it makes sense to group the output according to some (other) category, for example, link names (color, city_of, instrument, ...). Yet, important as they may be, links or relations are hard to extract and to name. This is why we have decided to start with a very small sub-set, meronymic-, i.e. part-of relations (x is part of y, x has y, etc.). KEYWORDS : Lexical access, navigation, word association, lexical graphs, semantic maps, automatic index creation, dynamic index, link extraction, link-names, part-of relations. 
Dictionaries constructed using distributional models of lexical semantics have a wide range of applications in NLP and in the modeling of linguistic cognition. However when constructing such a model, we are faced with range of corpora to choose from. Often there is a choice between small carefully constructed corpora of well-edited text, and very large heterogeneous collections harvested automatically from the web. There may also be differences in the distribution of genres and registers in such corpora. In this paper we examine these trade-offs by constructing a simple SVD-reduced word-collocate model, using four English corpora: the Google Web 5-gram collection, the Google Book 5-gram collection, the English Wikipedia, and collection of short social messages harvested from Twitter. Since these models need to encode semantics in a way that approximates the mental lexicon, we evaluate the felicity of the resulting semantic representations using a set of behavioral and neural-activity benchmarks that depend on wordsimilarity. We find that the quality of the input text has a very strong effect on the performance of the output model, and that a corpus of high quality at a small size can outperform a corpus of poor quality that is many orders of magnitude larger. We also explore the semantic closeness of the models using their mutual information overlap to interpret the similarity of corpus texts. KEYWORDS : VECTOR SPACE MODELS, DISTRIBUTIONAL SEMANTICS, CORPUS SIZE, CORPUS GENRE, CORPUS QUALITY, NEUROSEMANTICS, WORD SIMILARITY 
It has been known since Ide and Veronis [6] that it is impossible to automatically extract an ontology structure from a dictionary, because that information is simply not present. We attempt to extract structure elements from a dictionary using clues taken from a formal ontology, and use these elements to match dictionary deﬁnitions to ontology synsets; this allows us to enrich the ontology with dictionary deﬁnitions, assign ontological structure to the dictionary, and disambiguate elements of deﬁnitions and synsets. KEYWORDS: Dictionaries, ontologies, WordNet. 
We present the data structure of a lexical resource—the French Lexical Network (FLN)—, that is being hand-crafted using a knowledge-based lexicographic editor. The FLN is formally a lexical graph whose structuring is mainly supported by the system of paradigmatic and syntagmatic lexical functions of the Meaning-Text linguistic approach. Section 1 offers a general characterization of the FLN. Section 2 describes the database and the lexicographic editor in their present state. Section 3 focuses on the SQL data structure used to encode lexical information in the FLN, with special attention paid to the encoding of lexical function relations. Section 4 considers the feasibility of porting the FLN data to known standards such as the Lexical Markup Framework (LMF). Finally, in section 5, we consider the cognitive relevance of the FLN approach to the modeling of lexicons. KEYWORDS: lexical database, lexical graph, virtual dictionary, semantic derivation, collocation, lexical function, Explanatory Combinatorial Lexicology/Lexicography, lexicographic editor, French language. Introduction The French Lexical Network, hereafter FLN,1 is a new hand-crafted lexical resource, currently under development, that possesses many distinguishing features, both in terms of content, structure and building process. In this paper, we focus on the FLN’s data structure and on the graph editor that has been designed to support the lexicographic task of building the FLN. This work is currently performed at the ATILF CNRS laboratory (Nancy, France) in the context of a broader R&D project called RELIEF (Lux-Pogodalla and Polguère, 2011). Though the process of building the FLN is a long-term enterprise and we are at the time of writing only 18 months into this project, the resource’s structure is already sufﬁciently stable, and the resource itself is sufﬁciently well into development, for us to be able to account for our ﬁrst results. We believe that the approach taken in designing the FLN is particularly relevant for the linguistics, NLP and cognitive science communities due to (i) its formal nature, (ii) its strong theoretical linguistic background and (iii) its fundamental semantic orientation. All points that will be made clearer below. 1In French: Réseau Lexical du Français or RLF. Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon (CogALex-III), pages 109–126, COLING 2012, Mumbai, December 2012. 109  
The present paper continues the successful parsing experiments with the method of Segmentation-Cohesion-Dependency (SCD) configurations, a breadth-first, formal grammar-free, and optimal approach to dictionary entry parsing, proposed in the previous CogALex Workshops and applied to the following five very large thesaurus-dictionaries: DLR (The Romanian Thesaurus – new format), DAR (The Romanian Thesaurus – old format), TLF (Le Trésor de la Langue Française), DWB (Deutsches Wörterbuch – GRIMM), and GWB (Göthe-Wörterbuch). In this work we report new results: (a) The lexicographic modeling and parsing experiments of the sixth large DMLRL (Dictionary of Modern Literary Russian Language); (b) Outlining the Enumeration Closing Condition (ECC) for solving the recursive calls between sense marker classes situated on different nodes of a sense dependency hypergraph (SCD-configuration, i.e. parsing level); (c) The central result we report here is the project of a new, procedural DTD (Document Type Description) for dictionaries, based on the formalization of the SCD parsing method, providing parameterized grammars to describe the dependency hypergraphs that correspond to the main parsing levels in a dictionary entry. Here we give two parameterized grammars for DLR, as a small sample from a larger package of combined grammars for the above mentioned dictionaries. This package is constructed as the “least common multiple” of the parameterized grammars written for the parsed dictionaries; it represents the DTD description of a general parser for large dictionary entries, and thoroughly extends the current DTD in the XCES TEI P5 standard. KEYWORDS: SCD dictionary parsing method; procedural DTD for dictionary entry parsing. 
To develop a common language, it is essential to have enough vocabulary to express all the concepts contained in all the world languages. Those vocabularies can only be developed by native speakers and should be defined by formal ways. Considering the situation, at this moment Universal Networking Language (UNL) is the best solution as the common language, and Universal Words (UWs) are the most promising candidates to represent all the world concepts in different languages. However, UWs itself are formal and not always to be understandable for human. To ensure every language speakers can create the correct UWs dictionary entry, we need to provide the explanation of UWs in different natural languages for humans. As there are millions of UWs, it is very expensive to manually build the UWs explanation in all natural languages. To solve this problem, this research proposes the way to auto generate the UWs explanation in UNL, using the property inheritance based on UW System. Using UNL DeConverter from that UNL the system can generate the explanation in more than 40 languages. KEYWORDS : UNL; Ontology; Word Semantics; NLP; 
The growing amount of available information and the growing importance given to the access to technical information enhance the potential role of NLP applications in enabling users to deal with information for a variety of knowledge domains. In this process, lexical resources are crucial. Using and comparing already existent wordnets for common and technical lexica, we set up a basis for integrating these resources without losing their specific information and properties. We demonstrate their compatibility and discuss strategies to overcome the issues arrising in their merging, namely aspects concerning conceptual variation, subnet and synset merging, and the incorporation of technical and non-technical information in definitions. As we are using models of the lexicon that mirror the organization of the mental lexicon, the accomplishment of this goal can provide insights on the type of relations holding between common lexical items and terms. Also, the results of integrating such resources can contribute to the better intercommunication between experts and non-experts, and provide a useful resource for NLP, particularly for tools simultaneously serving specialist and non-specialist publics. KEYWORDS : wordnet, technical lexicon, common lexicon, merging. 
The present paper deals with the design and implementation of multilingual lexical resources of Assamese and Bodo Language with the help of Hindi Wordnet. Here, we present the multilingual dictionaries (for Hindi, Assamese and Bodo), synset based word search for Assamese-Hindi and Bodo-Hindi language. These words, of course, will have to go through some pre-processing before finally being uploaded to a database. The user-interface is being developed for specific language (Assamese, Bodo and Hindi language). KEYWORDS: Lexical Resources, Concept Based Dictionary, Multilingual Dictionary Database, Web-based Interface 
The Mental Lexicon (ML) refers to the organization of lexical entries of a language in the human mind.A clear knowledge of the structure of ML will help us to understand how the human brain processes language. The knowledge of semantic association among the words in ML is essential to many applications. Although, there are works on the representation of lexical entries based on their semantic association in the form of a lexicon in English and other languages, such works of Bangla is in a nascent stage. In this paper, we have proposed a distinct lexical organization based on semantic association between Bangla words which can be accessed efficiently by different applications. We have developed a novel approach of measuring the semantic similarity between words and verified it against user study. Further, a GUI has been designed for easy and efficient access. KEYWORDS : Bangla Lexicon, Synset, Semantic Similarity, Hierarchical Graph 
This paper describes a preliminary classiﬁcation of transitive verbs in terms of the implications of existence (or non-existence) associated with their direct object nominal arguments. The classiﬁcation was built to underlie the lexical marking of verbs in the lexical resources that the automated system BRIDGE developed at Xerox PARC used for textual inference. Similar classiﬁcations are required for other logic-based textual inference systems, but very little is written about the issue. KEYWORDS: textual inference, lexical resources, transitive verbs. 
We present a large-coverage lexical and grammatical resource of Polish economic terminology. It consists of two alternative modules. One is a grammatical lexicon of about 11,000 terminological multi-word units, where inﬂectional and syntactic variation, as well as nesting of terms, are described via graph-based rules. The other one is a fully lexicalized shallow grammar, obtained by an automatic conversion of the lexicon, and partly manually validated. Both resources have a good coverage, evaluated on a manually annotated corpus, and are freely available under the Creative Commons BY-SA license. Keywords: electronic lexicon, shallow grammar, Polish, economic terminology, language resources and tools. 
KEYWORDS: multilingual lexical database, semantic and syntactic model, cross-lingual asymmetry 1. Introduction: Integral Framework for the MLLD Over the past decade, NLP has witnessed a surge in the development of multilingual lexical databases and tools for cross-lingual tasks such as information retrieval, machine translation and foreign language acquisition. Most of the large-scale lexical databases that lately evolved into multilingual frameworks for language-specific lexicons have been initially designed as monolingual databases and developed independently without referring to any particular processor or potential NLP applications. In order to integrate typologically different languages into these frameworks, adjust them to certain processors and guarantee their cross-platform applicability communities of developers have carried out a great amount of work to develop tools for cross-platform integration and universal standards for semantic representations. Still these projects encounter a lot of problems of uniformity and consistency across languages, categories and applications. By contrast, the Comreno semantic model developed by ABBYY was initially designed for multilingual purposes and aimed at machine translation, without being limited to it. The system consists of a language database and includes interrelated modules: morphological, syntactic, semantic and statistical ones. The semantic module is based on a universal semantic hierarchy of thesaurus type which is filled with lexical information. The morphological and the syntactical Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon (CogALex-III), pages 215–230, COLING 2012, Mumbai, December 2012. 215  modules, in turn, are language-specific. This approach proved to be efficient to provide highquality machine translation for English<->Russian pair (refer to Anisimovich et al., 2012). At present, we continue working on German, French and Chinese languages. Currently, we have described more than 96000 English, 85000 Russian, 12 000 German, 11 000 French and 8500 Chinese lexical classes. The choice of the languages is mostly determined by the applied tasks of machine translation within corresponding language pairs, though as we have languages here that are typologically different such a choice allows testing the universality of the Compreno model as well. The format in which the lexical data is implemented has been worked out for this particular system by ABBYY developers. Compreno Parser is available on a fee-for-service basis. In the following, we briefly present existing multilingual lexical databases (2) and linguistic problems they have to encounter (3); give an overview of Compreno semantic framework (4) and, finally, present in more detail how Compreno MLLD deals with cross-lingual asymmetry and serves as a basis for machine translation (5). 2. Snapshot of the Existing Multilingual Lexical Databases In this section we provide an overview of the most representative wide-scale projects aimed at constructing multilingual lexical resources in terms of their theoretical approaches and potential NLP applications leaving aside other less known MLLDs for the reason of space limits. 2.1 EuroWordNet project The mainstream approach to the construction of wide-scale multilingual resources has been demonstrated by the EuroWordNet (Vossen, 2004) and the following Global WordNet Grid initiative. In these projects the goal is to build a worldwide grid of wordnets by means of an interlingual platform. EuroWordNet consists of individual databases for seven European languages (Dutch, English, Italian, Spanish, German, French, Czech and Estonian) and is analogous to the original Princeton WordNet for English. EuroWordNet provides a fine-grained formal concept analysis for nouns. However, it comes with a poor database of illustrating examples and lacks information about the syntactic behavior of verbs and nouns. Besides, in EuroWordNet, each language-specific WordNet is an autonomous language-specific ontology where each language has its own set of concepts and lexical-semantic relations based on the lexicalization patterns of that language. EuroWordNet differentiates between languagespecific and language-independent modules. The language-independent modules consist of a top concept ontology and an unstructured Inter-lingua-Index (ILI) that provides mapping across individual WordNet structures and meanings. 2.2. PAROLE/SIMPLE lexicons The initial goal of the LE PAROLE project conducted by the Council of Europe was to produce a head of the harmonized corpora and lexicons for 12 European languages: Catalan, Danish, Dutch, 216  English, Finnish, French, German, Greek, Italian, Portuguese, Spanish, and Swedish. These efforts resulted in monolingual morphological and syntactic lexicons for these languages, the volume of each lexicon amounting to 20000 entries. The next step towards cross-lingual usage of these resources was the SIMPLE project, when existing morphological and syntactic data were provided with semantic representations. The SIMPLE lexicons were developed in line with the EAGLES (Experts Advisory Group on Language Engineering Standards) requirements on lexical-semantic representations for NLP tasks. Thus developers tried to bear in mind potential NLP applications; still they did not refer to any particular applications that would use this information. The SIMPLE lexicons cover 10000 word meanings for the above mentioned languages; they are built around the same head ontology and the same set of semantic templates. Just as EuroWordNet SIMPLE is constructed not as a property-rich ontology but as a hierarchical net of the lexical items that imposes constrains to its NLP applicability: it lacks disambiguating power and the relations between entities are insufficient (Nirenburg, 2004). To ensure an overlap of lexical senses certain EuroWordNet’s Base concepts were converted into each language providing linking of the lexical stock. The theoretical foundations of the semantic description in SIMPLE are based on the extensions of Generative Lexicon theory (Pustejovsky, 1995) that makes it different from EuroWordNet. A SIMPLE lexical entry includes the following semantic information: 1) semantic type, corresponding with the SemU (semantic unit); 2) domain information 3) lexicographic gloss 4) argument structure for predicate 5) selectional restrictions on the arguments 6) event type to characterize the aspectual properties of verbal predicates 7) link of the arguments to the syntactic subcategorization frames, as represented in PAROLE lexicons 8) Qualia Structure 9) information about regular polysemous alternation in which a word sense may enter 10) cross-part-of-speech relation (derivation) 11) synonymy (McShane et al., 2004). 2.3. FrameNet and FrameNet-like lexicons Another large-scale multilingual project is FrameNet (Baker et al, 1998). FrameNet is based on Fillmore's Frame Semantics (Fillmore, 1976). Frame Semantics models the lexical meaning of predicates in terms of frames; frames describe a conceptual structure or prototypical situation together with a set of semantic roles, or frame elements (FEs) involved in the situation. FrameNet currently contains about 600 frames. FrameNet projects employ the deep syntactic representations provided by large-scale lexical functional grammars as syntactic basis for framebased meaning assignment. As an additional knowledge source FrameNet uses the public SUMO/MILO ontology whose classes are also aligned with WordNet. By employing semantic frames as interlingual representations, FrameNet, as opposed to other MLLDs, focuses on organizational units larger than words. Besides, each FrameNet entry contains exhaustive information about its semantic and syntactic combinatorial potential and semantically annotated example from large parallel corpora. Thus FrameNet’s database deals effectively with paraphrase patterns across languages. Currently, there are several autonomous FrameNet and FrameNet-like lexicons for English, German, Danish, French, Swedish, Spanish, Japanese and Chinese languages, all on different stages of completion. 217  3. Challenges for Multilingual Lexical Databases Construction of MLLDs faces even more complicated problems than those encountered in the creation of monolingual lexical databases (Boas, 2005). Among the main issues developers of the MLLDs have to face are: 1) cross-linguistic polysemy; 2) asymmetry of source and target semantic and syntactic structures; 3) cross-language asymmetry in the delimitation of semantic fields. cross-linguistic polysemy Dictionaries often vary in their organization of word meanings, which makes it difficult to compare definitions across different dictionaries. Besides, most polysemous words are usually the most frequent ones and their meanings are often domain-independent which may make disambiguation impossible. In the case of MLLDs for NLP tasks granularity of sense distinction is a key and controversial both to professional lexicographers and applications (Palmer et. al, 2006). Cross-linguistic polysemy is even more problematic. It may vary from a complete overlapping of word senses through diverging polysemy to the absence of correspondences among senses across languages (Altenberg and Granger, 2002). Thus, consistent criteria for sense distinction and strategies for cross-lingual sense mappings are crucial for the successful implementation of a MLLD. semantic and syntactic asymmetry In addition to providing information about different meanings of a word, any MLLD should accurately describe deep semantic model of each sense and all its possible surface realizations to ensure correct cross-language mapping. cross-language asymmetry in the delimitation of semantic fields As Talmy (2000) points out, languages differ in the kinds of semantic components they lexicalize. This has a number of important implications for the overall architecture of a MLLD. Some languages might make semantic distinctions that are irrelevant in others. For example, English verbs use particles to show the path of motion (“run into”, “go out“, “fall down”), whereas in Russian and German the path is encoded by affixation, in French – usually by the verb itself and in Chinese by directional modifiers. Another challenge is posited by culture-specific vocabulary, lexical gaps and their translation equivalents across languages. In this sense, the conception of MLLD development should stem from the Principle of Practical Effability (Nirenburg and Raskin, 2004), which states that what can be expressed in one language can be somehow expressed in all languages, be it by a word, phrase, etc. It should also take into account fixed multiword expressions (idioms, terms and collocations) and include a description of how to map such multiword expressions across languages. 218  Below, we present in more detail the theoretical approaches that Compreno semantic model employs and demonstrate how it treats the problems mentioned above. 4. Key features of Compreno Semantic Model The Compreno linguistic technology has been originally developed for machine translation, but now it is applied for a wider range of NLP applications aimed at semantic analysis. In the following we will focus on the universal semantic module of the system and show how its mechanisms can be applied to describe a group of typologically different languages (English, Russian, German, French and Chinese). 4.1. Semantic Hierarchy All words in our system are organized in the form of a thesaurus-like hierarchical tree which we call the semantic hierarchy (henceforth SH). The tree consists of language-independent branches called semantic classes (SC), which are filled with lexical items of natural languages – lexical classes (LC). Higher semantic classes denote general notions like entities, characteristics or actions, while their children have more specific meanings, so the deeper the class is the more particular notion it expresses: ENTITY_LIKE_CLASSES > ENTITY > FOOD > SOUP > KHARCHO > kharcho ENTITY_LIKE_CLASSES > ENTITY > FOOD > food Each semantic class can have both semantic and lexical classes as its descendants (fig. 1). FIGURE 1 - Fragment of the Semantic Hierarchy. 219  Lexical classes, in turn, contain lexemes with morphological paradigms. Each lexical class can have several lexemes that are grammatical derivates (GD): typical instances are verbs and verbal nouns (like “translate – translation”) or adjectives and adverbs (like “beautiful – beautifully”) that differ only in their part of speech type. The lexicographic description of the classes includes the following information: 1) a gloss drawn from a dictionary; 2) compatibility examples; 3) semantic and grammatical restrictions for different surface realizations of the actant valencies; 4) examples of voice transformation (for verbs) and additional restrictions imposed by them, if any; 5) relevant grammatical information; 6) examples of nontrivial translations, set expressions and any other relevant information. For Chinese, we also indicate the transcription, the spelling in Traditional characters, variant spellings and give glosses for all examples. It is essential to provide exhaustive information for the core vocabulary as it serves as basis for the syntactic descriptions and parser. Later on, the work becomes more labor-saving as syntactic and semantic models of LCs are inherited from their ancestors and only local mismatches should be marked. All words in the hierarchy are attributed with grammatical and semantic values, called grammemes and semantemes respectively. The usage of grammemes has been minutely examined in Anisimovich et al. 2012, some illustrations will be given below as well. Semantemes help to distinguish different lexical items within one semantic class (for other their functions see Anisimovich et al. 2012): i.e., “beautiful, pretty, handsome” have a <<PolarityPlus>> semanteme while “ugly” takes <<PolarityMinus>>. Semantemes are universal for all languages. We use more than 1100 semantemes in SH. On the contrary, grammatical system is unique for every language. So, the number of grammatical categories varies depending on the language. For example, in Russian we set up about 460 categories and 2500 grammemes, 420 / 2400 in English, 240 / 940 in French, 260 / 1300 in German and 60 / 160 in Chinese. The LC-descendants of one semantic class that have a similar set of semantemes are synonyms. During translation, lexical choice at the synthesis stage usually favors the lexical class with the most similar set of semantemes. Such a choice gets a better evaluation than mismatches between input and output classes. Words with the same root that differ not only morphologically but also semantically are introduced as semantic derivates (SD): SDs are the descendants of one lexical class that differ in semantemes, for example – “handsome – unhandsome”. The possibility to store multiple SDs under one lexical class is especially helpful for words with a big number of SDs. For instance, the verb “go” has about 30 SDs like “go away, go back, go in”, etc., corresponding to such verbs as “leave, return” and “enter”, so we can place all these verbs in one SC, where “go, leave, return” and “enter” will be different LCs while “go away, go back, go in”– the SDs of the LC “go”. Both LCs “leave, return, enter” and the SDs “go away, go back, go in” acquire the semantemes <<From>>, <<Back>> and <<To>>, respectively. This ensures their distinction from the neutral “go”. Semantic derivates are formed by regular morphological models and express semantic relations which are typical for the derivates formed by these models: “go away, fly away, swim away” are all formed with ‘away’ particle and express the semantics of leaving the place, or “go in, come in, fly in” are formed with the help of “in” particle and express the semantics of moving inside. 220  Such derivates can also differ in the semantic valencies they attach: for instance, valency indicating initial point (“come [from school]”) is typical for neutral „come‟ but is rather marginal for the “come in” derivate.  The derivates are marked with derivatemes – fixed combinations of corresponding grammemes and semantemes, which describe both their syntactic and semantic features. For example, the German verb “laufen” (“to run”) has 40 SDs such as “durchlaufen” (“to run through”), “zurücklaufen” (“to run back”) or “fortlaufen” (“to run away”) with the derivatemes <Durch_EnRouteLandmark>, <ZurückRück_Back> and <Fort_Depart> respectively. These derivatemes, in turn, contain semantemes <<En_Route>>, <<Back>> and <<From>>. At the current stage of the project the system numbers about 120 English derivatemes, 150 Russian derivatemes, 120 German derivatemes and 10 French derivatemes.  The following table provides data on language-specific descendents of the SC TO_RUN with a few illustrating examples:  number of LCs  English 9 (run, scatter, jog, lope, etc.)  Russian 3 (бежать, трусить, пробежка)  German  French  2  
Hindi and Urdu share a grammar and a basic vocabulary, but are often mutually unintelligible because they use diﬀerent words in higher registers and sometimes even in quite ordinary situations. We report computational translation evidence of this unusual relationship (it diﬀers from the usual pattern, that related languages share the advanced vocabulary and diﬀer in the basics). We took a GF resource grammar for Urdu and adapted it mechanically for Hindi, changing essentially only the script (Urdu is written in Perso-Arabic, and Hindi in Devanagari) and the lexicon where needed. In evaluation, the Urdu grammar and its Hindi twin either both correctly translated an English sentence, or failed in exactly the same grammatical way, thus conﬁrming computationally that Hindi andUrdu share a grammar. But the evaluation also found that the Hindi and Urdu lexicons diﬀered in 18% of the basic words, in 31% of tourist phrases, and in 92% of school mathematics terms. Keywords: Grammatical Framework, Resource Grammars, Application Grammars. Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 1–14, COLING 2012, Mumbai, December 2012. 
Semantic relation extraction aims to extract relation instances from natural language texts. In this paper, we propose a semantic relation extraction approach based on simple relation templates that determine relation types and their arguments. We attempt to reduce semantic drift of the arguments by using named entity models as semantic constraints. Experimental results indicate that our approach is very promising. We successfully apply our approach to a cultural database and discover more than 18,000 relation instances with expected high accuracy. KEYWORDS: semantic relation extraction, cultural database. Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 15–24, COLING 2012, Mumbai, December 2012. 15  
Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 25–40, COLING 2012, Mumbai, December 2012. 25  
Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 41–52, COLING 2012, Mumbai, December 2012. 41  
Stemming is a procedure that conflates morphologically related terms into a single term without doing complete morphological analysis. Urdu language raises several challenges to Natural Language Processing (NLP) largely due to its rich morphology. The core tool of information retrieval (IR) is a Stemmer which reduces a word to its stem form. Due to the diverse nature of Urdu, developing its stemmer for an IR system is a challenging task. This paper presents a light weight stemmer for Urdu text, which uses rule based approach. Exceptional lists are developed to enhance the accuracy of the stemmer. The result of the stemmer is quite enough and can be effective in IR system. KEYWORDS : Information Retrieval, Light weight stemmer, Exceptional Lists, Suffix and prefix list 
This paper studies the applicability of a set of state-of-the-art unsupervised morphological segmentation algorithms for the problem of morpheme boundary detection in Kannada, a resource-poor language with highly inﬂectional and agglutinative morphology. The choice of the algorithms for the experiment is based in part on their performance with highly inﬂected languages such as Finnish and Bengali (complex morphology similar to that of Kannada). When trained on a corpus of about 990K words, the best performing algorithm had an F-measure of 73% on a test set. The performance was better on a set of inﬂected nouns than on a set of inﬂected verbs. Key advantages of the algorithms conducive to efﬁcient morphological analysis of Kannada were identiﬁed. An important by-product of this study is an empirical analysis of some aspects of vocabulary growth in Kannada based on the word frequency distribution of the words in the reference corpus. KEYWORDS: Unsupervised morphological segmentation, Kannada language. Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 79–94, COLING 2012, Mumbai, December 2012. 79  
yumnamnirmal@gmail.com, sivaji_cse_ju@yahoo.com ABSTRACT The Morphemes of the Manipuri word are the real bottleneck for any of the Manipuri Natural Language Processing (NLP) works. It is one of the Indian Scheduled Language with less advancement so far in terms of NLP applications. This is because the nature of the language is highly agglutinative. Segmentation of a word and identifying the morphemes becomes necessary before proceeding for any of the Manipuri NLP application. A highly inflected word may sometimes consist of ten or more affixes. These affixes are the morphemes which change the semantic and grammatical structure. So the inflexion in a word plays an important role. Words are segmented to the syllables and are examined to extract a morpheme among the syllables. This work is implemented in the Manipuri words written with the Meitei Mayek (script). This is because the syllable formations are distinct comparing to the Manipuri written with Bengali script. The combination of 2-gram or bi-gram and the Standard Deviation technique are used for the identification of the morphemes. This system gives an output with the recall of 59.80%, the precision of 83.02% and the f-score of 69.52%. KEYWORDS : Manipuri, Morpheme, Segmentation, Syllable, Bi-gram, Standard Deviation Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 95–108, COLING 2012, Mumbai, December 2012. 95  
Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 109–122, COLING 2012, Mumbai, December 2012. 109  1. Introduction 
Acoustic Models for Hindi Automatic Speech Recognition  Anik DEY1 Ying Li1 Pascale FUNG1 (1) Human Language Technology Center Department of Engineering and Computer Engineering The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong adey@ust.hk, eewing@ust.hk, pascale@ee.ust.hk ABSTRACT Bilingual speakers of Hindi and English often mix English and Hindi together in their everyday conversations. This motivates us to build a mix language Hindi-English recognizer. For this purpose, we need well-trained English and Hindi recognizers. For training our English recognizer we have at our disposal many hours of annotated English speech data. For Hindi, however, we have very limited resources. Therefore, in this paper we are proposing methods for rapid development of a Hindi speech recognizer using (i) trained English acoustic models to replace Hindi acoustic models; and (ii) adapting Hindi acoustic models from English acoustic models using Maximum Likelihood Linear Regression. We propose using data-driven methods for both substitution and adaptation. Our proposed recognizer has an accuracy of 96% for recognizing isolated Hindi words. KEYWORDS : English, Hindi, Recognizer, Maximum Likelihood Linear Regression, Adaptation, Substituiton, Data-driven 
In this paper, we focus on improving part-of-speech (POS) tagging for Urdu by using existing tools and data for the language. In our experiments, we use Humayoun’s morphological analyzer, the POS tagging module of an Urdu Shallow Parser and our own SVM Tool tagger trained on CRULP manually annotated data. We convert the output of the taggers to a common format and more importantly unify their tagsets. On an independent test set, our tagger outperforms the other tools by far. We gain some further improvement by implementing a voting strategy that allows us to consider not only our tagger but also include suggestions by the other tools. The ﬁnal tagger reaches the accuracy of 87.98%. Keywords: Urdu language, Parts-of-speech tagging, Tagger voting, Tagset uniﬁcation. Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 135–144, COLING 2012, Mumbai, December 2012. 135  
Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 145–152, COLING 2012, Mumbai, December 2012. 145  1. Introduction Annotated text corpora are a basic and a very useful resource for researchers in Natural Language Processing (NLP) for developing various language technologies. The annotation of corpora is done using a set of tags defined for this purpose. The BIS tagset is a set of tags evolved by the POS Tag Standardization committee appointed by the DIT to standardize and streamline the process of POS tagging in Indian languages. A need for standard tagset along with guidelines for using it arose because a lot of researchers were working independently following the tags of their own choice to mark the POS within and across languages. This affected the reusability of the tagged data amongst researchers. Hence, in order to facilitate interoperability an exercise was made to have a consensus on the style and characteristics of POS tagging, so as to arrive at a common standard for tagging in Indian Languages. This led to the standardized BIS tagset for POS tagging for Indian Languages. Several meetings of experts in the field were held to decide on the tagset and all groups engaged in research in NLP were given standard guidelines for annotation. This paper aims at shedding light on the peculiarities of the Konkani language that have posed challenges in tagging corpora using the standard BIS tagset. The paper is organized as follows – section 2 briefly introduces the BIS tagset and the Konkani Language and the challenges encountered while tagging using the BIS POS tagset for Konkani are presented in section 3 which is followed by section on conclusion and future work. 2. BIS tagset and Konkani Language The BIS POS tagset is prepared keeping in view the comments of experts in the area of NLP and Language Technology (LT) for Indian languages. This tagset is an important step taken by DIT to ensure that NLP practitioners involved in tagging follow a common tagset while tagging various corpora. The tagset initially consisted of 38 tags. These tags were then modified after taking inputs from linguists, computer scientists and language experts. More details of the BIS tagset can be found in Chaudhary, 2010. The BIS tagset is a commendable effort. But the process of tagging was, at times, quite challenging as it was different from the conventional style of tagging (for example, the case of marking adverbs of place and location as NSTs (locative nouns) . Konkani is an Indo-European (Indo-Aryan) language evolved from Sanskrit. It is a morphologically rich Indian language (Almeida, 1989). It is one of the twenty two languages included in the Eighth Schedule of the Indian Constitution. It is spoken by the people of the state of Goa, in some parts of Maharashtra, Karnataka and in some pockets of Kerala. It is influenced and enriched by various other languages like Marathi, Kannada, Malayalam, Hindi, Arabic, Persian, Portuguese and English. It is the official language of Goa with Devanagari as the officially recognized script. It is also written in Roman, Kannada, and Malayalam scripts (Walawalikar, et.al. 2010). 2.1 BIS POS tagset for Konkani The BIS tagset was used for tagging the Konkani ILCI corpus consisting of a total of 50000 sentences (730330 tokens). The main objective of the ILCI project was to develop standard quality parallel annotated corpora for 11 Indian languages including English language to promote NLP research for Indian Languages (Chaudhary, 2010). 146  The following is the tagset for Konkani prepared in line with the BIS tagset.  SI Category  Label Annotation Convention  Top Subtype Subtype  Level (level1) (Level2)  
Vers un traitement automatique de la langue Amazighe Depuis l’antiquité, le patrimoine Amazighe est en expansion de génération en génération. Dans l’objectif de sauvegarder, exploiter ce patrimoine et éviter qu’il soit menacé de disparition, il semble opportun d’équiper cette langue de moyens nécessaires pour affronter les enjeux d'accès au domaine des nouvelles technologies de l'information et de la communication (NTIC) qui s’avère primordial pour promouvoir et informatiser cette langue. Dans ce contexte, et dans les perspectives de développer des outils et des ressources linguistiques pour le traitement automatique de cette langue, nous avons entrepris d’utiliser la plateforme d’ingénierie linguistique NooJ afin de créer un module pour la langue Amazighe standard (Ameur et al., 2004a). Notre premier objectif est l'analyse des textes Amazighe. A cet effet, nous commençons par la formalisation du vocabulaire Amazighe (Nom, Verbe et Particules). Dans cet article nous nous intéresserons à la formalisation de deux catégories, nom et de particules, permettant de générer à partir d'une entrée lexicale son genre (masculin, féminin), son nombre (singulier, pluriel) et son état (libre, annexion). Enfin, nous développons un dictionnaire électronique afin de l'utiliser, d'une part, pour tester nos règles de flexions et d'autre part pour l'analyse lexicale des textes Amazighe. KEYWORDS: Amazigh language, NooJ, Natural language processing, Less-resourced language, lexical analysis, inflectional morphology, flexional grammar, dictionary. KEYWORDS IN L2 : La langue Amazighe, NooJ, Traitement automatique des langues naturelles, langue peu dotée, analyse lexicale, morphologie flexionnelle, grammaire flexionnelle, dictionnaire. Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 173–180, COLING 2012, Mumbai, December 2012. 173  Introduction The Amazigh language in Morocco is considered as a prominent constituent of the Moroccan culture and this by its richness and originality. However it has been long discarded otherwise neglected as a source of enrichment cultural. Nevertheless, due to the creation of the Royal Institute of Amazigh Culture (IRCAM)1, this language has been introduced in the public domain including administration, media also in the educational system in collaboration with ministries. It has enjoyed its proper coding in the Unicode Standard (Andries, 2008; Zenkouar, 2008), an official spelling (Ameur et al., 2006a), appropriate standards for keyboard realization and linguistic structures that are being developed with a phased approach (Ameur et al., 2006b; Boukhris et al., 2008). This process was initiated by the standardization, vocabularies construction (Kamel, 2006; Ameur et al., 2009a; Ameur et al., 2006a; Ameur et al., 2009b), Alphabetical Arrangement (Outahajala, 2007), spelling standardization (Ameur et al., 2006a) and development of rules grammar (Boukhris et al., 2008). However, this not sufficient for a less-resourced language (Berment, 2004) as the Amazigh to join the well-resourced language in information and Communication Technologies, mainly due to the lack of already available language processing resources and tools. Therefore, a set of scientific and linguistic research are undertaken to remedy to the current situation. These researches are divided, on the one hand, on researches that are concentrated on optical character recognition (OCR) (Amrouch et al., 2010; Es Saady et al., 2010; Fakir et al., 2009), and in the other hand, on those that are focused on natural language processing (Iazzi and Outahajala, 2008; Ataa Allah and Jaa, 2009; Boulaknadel, 2009; Ataa Allah and Boulaknadel, 2010; Outahajala et al., 2010; Boulaknadel and Ataa Allah, 2011), which constitute the priority components of researches. In this context, the present work deals with ongoing research efforts to build tools and linguistic resources for the Amazigh language. Our first main objective is to develop a morphological analyzer to parse amazigh texts. For this purpose, we begin by building a morphological analyzer for Amazigh nouns, implemented using the Finite State Technology within the linguistic developmental environment Nooj. This paper is structured around five main sections: the first present a description of the Amazigh language particularities. The second expose the automatic Amazigh language processing, which includes an overview of NooJ environment, and the formalization of a set of rules. While the last section is dedicated to the conclusion and perspectives. Amazigh language particularities The Amazigh language also known as Berber or Tamazight (ⵜ ⴰ ⵎ ⴰ ⵣ ⵉ ⵖ ⵜ [tamazight]), is belonged to the African branch of the Afro-Asiatic language family, also referred to as HamitoSemitic in the literature (Greenberg, 1966; Ouakrim, 1995). It is currently presented in a dozen countries ranging from Morocco, with 50% of the overall population2 (Boukous, 1995), to Egypt, passing through Algeria with 25%, the Tunisia, Mauritania, Libya, Niger and the Mali (Chaker, 2003). 
We describe the development of a bidirectional rule-based machine translation system between Indonesian and Malaysian (id-ms), two closely related Austronesian languages natively spoken by approximately 35 million people. The system is based on the re-use of free and publicly available resources, such as the Apertium machine translation platform and Wikipedia articles. We also present our approaches to overcome the data scarcity problems in both languages by exploiting the morphology similarities between the two. KEYWORDS: machine translation, Malay languages, morphology. Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 191–200, COLING 2012, Mumbai, December 2012. 191  
 1. Introduction There are lots of open source frameworks available to build a search engine. These open source frameworks provide multiple features to build an inverted index of web documents used for information retrieval. Some features like Scalability, term storage, document posting list storage etc, are common across these frameworks. These frameworks facilitate customization of building index to make it compatible for the desired application. To retain the structure of a document in an inverted index, field based indexing is used. Instead of viewing a document as a collection of terms, the document is viewed as a collection of fields and the field as a collection of terms. Each document that needs to be indexed is parsed and terms in the document are grouped into fields prior to indexing. The conceptual view of field based inverted index is shown in the figure 1. Figure 1(a) shows two documents as is. Figure 1(b) shows a view of inverted index built for these documents.  
WordNet is a crucial resource that aids in Natural Language Processing (NLP) tasks such as Machine Translation, Information Retrieval, Word Sense Disambiguation, Multi-lingual Dictionary creation, etc. The IndoWordNet is a multilingual WordNet which links WordNets of different Indian languages on a common identiﬁcation number given to each concept. WordNet is designed to capture the vocabulary of a language and can be considered as a dictionary cum thesaurus and much more. WordNets for some Indian Languages are being developed using expansion approach. In this paper we have discussed the details and our experiences during the evolution of this database design while working on the Indradhanush WordNet Project. The Indradhanush WordNet Project is working on the development of WordNets for seven Indian languages. Our database design gives an efﬁcient plan for storage of WordNet data for all languages. In addition it extends the design to hold speciﬁc concepts for a language. KEYWORDS: WordNet, IndoWordNet, synset, database design, expansion approach, semantic relation, lexical relation. Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 229–236, COLING 2012, Mumbai, December 2012. 229  
Work is currently under way to develop WordNet for various Indian languages. The IndoWordNet Consortium consists of member institutions developing their own language WordNet using the expansion approach. Many tools and utilities have been developed by various institutes to help in this process. In this paper, we discuss an object oriented Application Programming Interface (API) that we have implemented to facilitate easy and rapid development of tools and other software resources that require WordNet access and manipulation functionality. The main objective of IndoWordNet Application Programming Interface (IWAPI) is to provide access to the WordNet resource independent of the underlying storage technology. The current implementation manipulates data stored in a relational database. Furthermore the IWAPI also supports parallel access and manipulation of WordNets in multiple languages. In this paper, we discuss functional requirements, design and the implementation of IndoWordNet API and its uses. KEYWORDS: WordNet, Application Programming Interface (API), WordNet CMS, IndoWordNet, IndoWordNet Database, WordNet Website. Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP), pages 237–244, COLING 2012, Mumbai, December 2012. 237  
(2) FEDERAL UNIVERSITY OF OURO PRETO (UFOP), Rua do Seminário, S/N, Mariana/MG, 35.420-000, Brazil fabio-alves@ufmg.br, zeluizvr@ichs.ufop.br,kszpak@ufmg.br ABSTRACT Drawing on the seminal work of Just and Carpenter (1980), eye fixations have been used extensively to analyse instances of processing effort in studies of reading and writing processes. More recently, eye tracking has also been applied to experimental studies in translation process research (Jakobsen and Jensen 2008, Pavlović and Jensen 2009, Alves, Pagano and Silva 2009, Hvelplund 2011, Carl and Kay 2011, Carl and Dragsted 2012, among others). In most of these works, eye-tracking data have provided input for quantitative analyses of fixation count and duration in areas of interest in source and target texts. From a linguistic perspective, however, studies using eye-tracking data are considered rather complex since eye fixations tend to vary considerably among subjects. This paper attempts to tackle this issue by proposing a methodological approach that uses overlapped heat maps of different subjects to select and analyse translation problems. The results yield relevant findings for eye-tracking research in translation. KEYWORDS: translation process research, eye-tracking research, eye-mind assumption, processing effort in translation, micro/macro translation units. Proceedings of the First Workshop on Eye-tracking and Natural Language Processing, pages 5–20, COLING 2012, Mumbai, December 2012. 5  
We present in this paper a robust method for predicting reading times. Robustness ﬁrst comes from the conception of the difﬁculty model, which is based on a morpho-syntactic surprisal index. This metric is not only a good predictor, as shown in the paper, but also intrinsically robust (because relying on POS-tagging instead of parsing). Second, robustness also concerns data analysis: we propose to enlarge the scope of reading processing units by using syntactic chunks instead of words. As a result, words with null reading time do not need any special treatment or ﬁltering. It appears that working at chunks scale smooths out the variability inherent to the different reader’s strategy. The pilot study presented in this paper applies this technique to a new resource we have built, enriching a French treebank with eye-tracking data and difﬁculty prediction measures. KEYWORDS: Linguistic complexity, difﬁculty models, morpho-syntactic surprisal, reading time prediction, chunks. Proceedings of the First Workshop on Eye-tracking and Natural Language Processing, pages 21–36, COLING 2012, Mumbai, December 2012. 21  
Scanpaths, sequences of ﬁxations of the eyes, have historically played an important role in eyetracking research but their use has remained highly limited until recently. Here, we summarize earlier research and argue that scanpaths are a valuable source of information for reading research, speciﬁcally in the study of sentence comprehension. We also discuss a freely available, open source scanpath analysis method that we used to evaluate theoretical claims about human parsing and about how the parser guides the eyes during reading. This scanpath analysis is shown to yield new information that was missed when traditional approaches were used to study theories about eye guidance during garden-pathing. We also show how relatively subtle scanpath effects can be detected when we report the scanpath analysis of a large eyetracking corpus. In sum, we argue that scanpath analyses are likely to serve as an increasingly important tool in reading research, and perhaps also in other areas where eyetracking is used, e.g., in studies using the visual world paradigm. KEYWORDS: scanpaths, reading, eye movements, parsing. Proceedings of the First Workshop on Eye-tracking and Natural Language Processing, pages 37–54, COLING 2012, Mumbai, December 2012. 37  
Human gaze behavior while reading text reﬂects a variety of strategies for precise and efﬁcient reading. Nevertheless, the possibility of extracting and importing these strategies from gaze data into natural language processing technologies has not been explored to any extent. In this research, as a ﬁrst step in this investigation, we examine the possibility of extracting reading strategies through the observation of word-based ﬁxation behavior. Using existing gaze data, we train conditional random ﬁeld models to predict whether each word is ﬁxated by subjects. The experimental results show that, using both lexical and screen position cues, the model has a prediction accuracy of between 73% and 84% for each subject. Moreover, when focusing on the distribution of ﬁxation/skip behavior of subjects on each word, the total similarity between the predicted and observed distributions is 0.9462, which strongly supports the possibility of capturing general reading strategies from gaze data. Title and Abstract in Japanese ਓͷҰൠతͳจষཧղઓུΛଊ͑ΔͨΊͷ CRF ϞσϧΛ༻͍ͨจষதͷ୯‫ޠ‬஫ࢹ༧ଌ ਓ͕ؒจষΛಡΉࡍͷࢹઢߦಈʹ͸ɺਖ਼͔֬ͭޮ཰తʹಡΉͨΊͷ༷ʑͳઓུ͕൓ө͞Εͯ ͍Δɻ͔͠͠ͳ͕ΒɺͦͷઓུΛࢹઢσʔλ͔Βநग़͠ɺࣗવ‫ॲޠݴ‬ཧٕज़ʹऔΓೖΕΔͱ ͍͏Մೳੑʹؔͯ͠͸ɺ͜Ε·Ͱ΄ͱΜͲ‫͞ڀݚ‬Εͯདྷͳ͔ͬͨɻຊ‫Ͱڀݚ‬͸ɺ͜ͷՄೳੑΛ ‫͢ڀݚ‬ΔͨΊͷୈҰาͱͯ͠ɺ୯‫ޠ‬ϕʔεͷ஫ࢹߦಈͷ‫࡯؍‬Λ௨ͯ͠จষཧղઓུͷநग़Մ ೳੑΛௐࠪ͢Δɻզʑ͸‫ط‬ଘͷࢹઢσʔλΛ༻͍ɺ֤୯‫͕ޠ‬ඃ‫ʹऀݧ‬Αͬͯ஫ࢹ͞ΕΔ͔Ͳ ͏͔Λ༧ଌ͢Δ৚݅෇͖֬཰৔ϞσϧΛ‫܇‬࿅͢Δɻ࣮‫Ͱݧ‬͸ɺ‫ޠ‬ኮ৘ใͱը໘Ґஔ৘ใΛख ͕͔Γʹ͢Δ͜ͱͰɺ͜ͷϞσϧ͕֤ඃ‫ʹऀݧ‬ରͯ͠ 73%͔Β 84%ͷ༧ଌਫ਼౓Λ༩͑Δ͜ͱ ͕ࣔ͞ΕΔɻ͞Βʹɺ֤୯‫ʹޠ‬ର͢Δඃ‫ؒऀݧ‬ͷ஫ࢹʗεΩοϓͷ෼෍ʹண໨͢Δͱɺ༧ଌ ͞Εͨ෼෍ͱ࣮ࡍʹ‫͞࡯؍‬Εͨ෼෍ͱͷશମతͳۙࣅ౓͸ 0.9462 Ͱ͋Δ͜ͱ͕ࣔ͞Εɺࢹઢ σʔλ͔ΒҰൠతͳจষཧղઓུΛଊ͑͏ΔՄೳੑΛ‫͘ڧ‬ཪ෇͚Δ࣮‫݁ݧ‬Ռͱͳ͍ͬͯΔɻ Keywords: eye-tracking, gaze data, reading behavior, conditional random ﬁeld (CRF). Keywords in Japanese: ࢹઢ௥੻ɺࢹઢσʔλɺಡղߦಈɺ৚݅෇͖֬཰৔ (CRF). Proceedings of the First Workshop on Eye-tracking and Natural Language Processing, pages 55–70, COLING 2012, Mumbai, December 2012. 55  
Proceedings of the First Workshop on Eye-tracking and Natural Language Processing, pages 71–80, COLING 2012, Mumbai, December 2012. 71  
We propose a method to construct a phrase class n-gram model for Kana-Kanji Conversion by combining phrase and class methods. We use a word-pronunciation pair as the basic prediction unit of the language model. We compared the conversion accuracy and model size of a phrase class bi-gram model constructed by our method to a tri-gram model. The conversion accuracy was measured by F measure and model size was measured by the vocabulary size and the number of non-zero frequency entries. The F measure of our phrase class bi-gram model was 90.41%, while that of a word-pronunciation pair tri-gram model was 90.21%. In addition, the vocabulary size and the number of non-zero frequency entries in the phrase class bi-gram model were 5,550 and 206,978 respectively, while those of the tri-gram model were 22,801 and 645,996 respectively. Thus our method makes a smaller, more accurate language model. KEYWORDS: Kana-Kanji Conversion, n-gram model, phrase-based model, class-based model. Proceedings of the Second Workshop on Advances in Text Input Methods (WTIM 2), pages 1–14, COLING 2012, Mumbai, December 2012. 
Since Japanese and Chinese languages have too many characters to be input directly using a standard keyboard, input methods for these languages that enable users to input the characters are required. Recently, input methods based on statistical models have become popular because of their accuracy and ease of maintenance. Most of them adopt word-based models because they utilize word-segmented corpora to train the models. However, such word-based models suffer from unknown words because they cannot convert words correctly which are not in corpora. To handle this problem, we propose a character-based model that enables input methods to convert unknown words by exploiting character-aligned corpora automatically generated by a monotonic alignment tool. In addition to the character-based model, we propose an ensemble model of both character-based and word-based models to achieve higher accuracy. The ensemble model combines these two models by linear interpolation. All of these models are based on joint source channel model to utilize rich context through higher order joint n-gram. Experiments on Japanese and Chinese datasets showed that the character-based model performs reasonably and the ensemble model outperforms the word-based baseline model. As a future work, the effectiveness of incorporating large raw data should be investigated. KEYWORDS: Input Method, Machine Transliteration, Joint Source Channel Model, Automatic Alignment, Ensemble Method, Japanese, Chinese. Proceedings of the Second Workshop on Advances in Text Input Methods (WTIM 2), pages 15–28, COLING 2012, Mumbai, December 2012. 15  
The development of efﬁcient keyboards is an important element of effective human-computer interaction. This paper explores the use of multi-objective optimization and machine learning to create more effective keyboards. While previous research has focused on simply improving the expected typing speed of keyboards, this research utilizes multiple optimization criteria to create a more robust keyboard conﬁguration. As these criteria are incorporated, they will often conﬂict with each other making this a complex optimization problem. Machine learning techniques were utilized and were proven to be an effective tool in keyboard optimization. The results reported here demonstrate that multi-objective genetic algorithms can be used to efﬁciently generate optimized keyboards. An English keyboard designed after 20000 generations was able to double the efﬁciency of the unoptimized QWERTY keyboard for multiple constraints. Despite having twice the number of characters, an Assamese keyboard was generated that performed better than the QWERTY layout. KEYWORDS: Soft keyboards, user interfaces, Brahmic scripts, optimization, genetic algorithms, Android development.. Proceedings of the Second Workshop on Advances in Text Input Methods (WTIM 2), pages 29–44, COLING 2012, Mumbai, December 2012. 29  
Kana-Kanji conversion is known as one of the representative applications of Natural Language Processing (NLP) for the Japanese language. The N-pos model, presenting the probability of a Kanji candidate sequence by the product of bi-gram Part-of-Speech (POS) probabilities and POS-to-word emission probabilities, has been successfully applied in a number of well-known Japanese Input Method Editor (IME) systems. However, since N-pos model is an approximation of n-gram word-based language model, important word-to-word collocation information are lost during this compression and lead to a drop of the conversion accuracies. In order to overcome this problem, we propose ways to improve current N-pos model. One way is to append the highfrequency collocations and the other way is to sub-categorize the huge POS sets to make them more representative. Experiments on large-scale data veriﬁed our proposals. Keywords: Input Method Editor, K-means clustering, n-gram language model, collocation. 1 Introduction In Japanese IME systems 1, Kana-Kanji conversion is known as one of the representative applications of NLP. Unfortunately, numerous researchers have taken it for granted that current NLP technologies have already given a fully support to this task and there are few things left to be done as research topics. However, as we go deeper to this “trivial” task, we recognize that converting from a romanized Hirakana sequence (i.e., users’ input) into a mixture of Kana and Kanji sequence (i.e., users’ expected output) is more difﬁcult than it looks. Concretely, we are facing a lot of NLP research topics such as Japanese word/chunk segmentation, POS tagging, n-best decoding, etc. Existing algorithms dealing with these topics are challenged by the daily-updating and large-scale Web data. Traditional n-gram word-level language model (short for “word n-gram model”, hereafter) is good at ranking the Kanji candidates. However, by using the large-scale Web data in tera-bytes (TB), even bi-gram word-level language model is too large2 to ﬁt the memories (for loading the model) and computing abilities (for decoding) of users’ personal computers (PCs). Dealing with this limitation, n-pos model (Kudo et al., 2011) was proposed to make a compression3 of the word n-gram model. N-pos model takes POS tags (or word classes) as the latent variable and factorizes the probability of a Kanji candidate sequence into a product of POS-to-POS transition probabilities and POS-to-word 1The Japanese IME mentioned in this paper can be freely downloaded from: http://ime.baidu.jp/type/?source=pstop 2For example, using the 2.5TB data, the word n-gram model has 421 million 1-grams and 2.6 billion 2-grams. 3Indeed, as pointed out by one reviewer, n-pos model has its own beneﬁts by organizing the semantically similar words and dealing with low frequency words. Thus, even the result n-pos model is smaller than word n-gram model, it is considered to be also bring accuracy improvements (Kneser and Ney, 1993; Mori et al.). Proceedings of the Second Workshop on Advances in Text Input Methods (WTIM 2), pages 45–56, COLING 2012, Mumbai, December 2012. 45  emission probabilities. This factorization makes n-pos model alike the well-known Hidden Markov Model (HMM). Since the number of POS tags is far smaller than the number of word types in the training data, n-pos model can signiﬁcantly smaller the ﬁnal model without deteriorating the conversion accuracies too much. Compared with word n-gram model, n-pos model is good for its small size for both storing and decoding. However, the major disadvantage is that important word-level collocation information are not guaranteed to be kept in the model. One direction to remedy the n-pos model is to ﬁnd those lost word-level information and append it. The other direction is to sub-categorize the original POS tags to make the entries under one POS tag contain as less homophonic words as possible. These considerations yielded our proposals, ﬁrst by appending collocations and second by sub-categorizing POS tags. Experiments by making use of large-scale training data veriﬁed the effectiveness of our proposals. This paper is organized as follows. In the next section, we give the formal deﬁnition of n-pos model and explain its disadvantage by real examples. In Section 3 we describe our proposed approaches. Experiments in Section 4 testify our proposals. We ﬁnally conclude this paper in Section 5. 2 N-pos Model and Its Disadvantage 2.1 N-pos model For statistical Kana-Kanji conversion, we use x to express the input Hirakana sequence, y to express the output mixed Kana-Kanji sequence and P(y|x) to express the conditional probability for predicting y given x. We further use yˆ to express the optimal y that maximize P(y|x) given x. Based on the Bayesian theorem, we can derive P(y|x) from the product of the language model P(y) and the Kanji-Kana (pronunciation) model P(x|y). This deﬁnition is also similar with that described in (Mori et al., 1999; Komachi et al., 2008; Kudo et al., 2011). yˆ = argmaxP(y|x) = argmaxP(y)P(x|y)  There are ﬂexibilities in implementing the language model and the pronunciation model. Suppose the output y contains n words, i.e., y = w1...wn. We use 1) product of word class bigram model and word class to word emission model as the language model, and 2) word-pronunciation unigram model as the Kanji-Kana model. That is,  n  P(y) = P(wi|ci)P(ci|ci−1)  (1)  i=1 n  P(x|y) = P(ri|wi)  (2)  i=1  Here, ci is the word class for word wi (frequently corresponds to POS tags or inﬂected forms), P(wi|ci) is the word generation probability from a word class ci to word wi, P(ci|ci−1) is the word class transition probability, ri is the Kana pronunciation candidate for word wi, and P(ri|wi) is the probability that word wi is pronounced as ri. The optimal output yˆ (or even n-best list) can be effectively computed by the Viterbi algorithm (Viterbi, 1967; Huang and Chiang, 2005).  There are many methods for designing the word classes, such as unsupervised clustering, POS tags, etc. Following (Kudo et al., 2011), we designed the word classes by using the POS information  46  generated by an open-source toolkit Mecab4 (Kudo et al., 2004) which was developed for Japanese word segmenting and POS tagging. Since POS bi-gram model plays an essential role in Equation 1, we call it n-pos model. Specially, similar to (Kudo et al., 2011), we also use the following rules to determine a word class: • the deepest POS tag layers (totally six layers) of the IPA POS system5 was used; • for the words with inﬂection forms, their conjugated forms and inﬂections are all appended; • particles, auxiliary verbs, and non-independence content words6 are all taken as independent word classes; and, • high-frequency verbs, nouns except named entities, adjectives, sufﬁxes, preﬁxes are all taken as independent word classes. Since there are many special words that are taken as word classes, we ﬁnally obtained around 2,500 word classes. Probabilities P(wi|ci) and P(ci|ci−1) can be computed from the POS-tagged corpus by using the maximum likelihood estimation method. The Kanji-Kana pronunciation model P(ri|wi) can be computed by ﬁrst mining Kanji-Kana pairs from the Web and then estimate their probabilities in a maximum likelihood way. Since Mecab also assigns Kana pronunciations and POS tags to Japanese words simultaneously during performing word segmentation, we can simply estimate P(ri|wi) using the Web corpus pre-processed by Mecab. That is, P(ri|wi) = f r eq(ri, wi)/ f r eq(wi). Here, function f req() returns the (co-)frequency of a word and/or a Kana sequence in the training data. In our IME system, besides our basic Kana-Kanji conversion dictionary, the Kanji-Kana pairs mined from the Web are individually taken as "cell dictionaries". That is, they are organized by their category such as “idioms", “actor names", and so on. These cell dictionaries are optimal to the users and they can download those dictionaries which ﬁt their interests. We also use a log-style function based on the frequencies of the Kanji candidates to compute the weights of the Kana-Kanji pairs. The weights are used to determine the rank of the Kanji candidates to be shown to the users. 2.2 The disadvantage The basic motivation for factorizing P(y) into Equation 1 is to compress the word n-gram model into the production of a bigram n-pos model P(ci|ci−1) and an emission model P(wi|ci). N-pos model is good for its small size and the usage of syntactic information for predicting the next word. However, compared with word n-gram model, the disadvantage is clear: the co-frequency information of wi−1 and wi is not taken into consideration during predicting. Figure 1 shows an example for intuitive understanding of the disadvantage. Suppose both wi−1 (gennshi/nuclear) and wi (hatsudenn/generate electricity) are low-frequency words in the training corpus, yet wi−1 always appears together with wi (or we say wi−1 and wi form a collocation). Under n-pos model, the total score of wi−1 wi is determined mainly by P(ci|ci−1) and P(wi|ci), but not P(wi|wi−1). Thus, the best candidate “nuclear power” in word n-gram model is possibly not be able to be predicated as the top-1 candidate in n-pos model. 4http://mecab.sourceforge.net/ 5http://sourceforge.jp/projects/ipadic/ 6non-independent content words (such as oru, aru, etc.) are those content words that do not have an independent semantic meaning, but have to be used together with another independent content word to form a complete meaning. 47  Figure 1: An example for the disadvantage of n-pos model.  Figure 2: Changing n-pos model by replacing individual words A and B with collocation AB. 3 The Proposed Method 3.1 Appending “partial” word n-gram model The disadvantage of n-pos model is mainly caused by its compression of word n-gram model. N-pos model can deal with a large part of the Kana-Kanji conversion problem yet short at dealing with collocations. We are wondering if partial of the word n-gram model can be “appended” to the n-pos model to further improve the ﬁnal conversion precision. The challenge is how to balance the usage of the two models for ranking the Kanji candidates. The score of a Kanji candidate sequence AB (with two words) can be computed by both the word n-gram model and the n-pos model. One simple consideration is to trust word n-gram model whenever n-pos model “fails” to make a better ranking. That is, we make use of word n-gram model only if candidate AB was assigned a higher score in word n-gram model than that in n-pos model. We explain this idea through an example shown in Figure 2. In this ﬁgure, we want to replace individual words A and B in the decoding word lattice in the original n-pos model by a collocation AB, knowing that A and B sharing a high co-frequency in the training data.  P1(AB) = P(A|cA)P(cB|cA)P(B|cB)  =  f r eq(A) f r eq(cA)  ×  f  r eq(cAcB) f r eq(cA)  ×  f f  r eq(B) r eq(cB )  ;  P2(AB) =  f f  r eq(AB) r eq(cA)  .  Here, cA and cB stand for the POS tag (or word class) of word A and B; function f req() returns the frequency of words and POS tags in the training data. When appending collocations to the n-pos model, we need to let P1(AB) < P2(AB) to ensure the collocation candidate has a higher rank in the candidate set. That is,  48  f r eq(A) f r eq(cA)  ×  f req(cB) f r eq(cA)  ×  f req(B) f req(cB)  <  f f  r eq(AB) r eq(cA)  ,  i  .e.,  f r eq(A) f r eq(B) f r eq(cA) f r eq(cB)  <  f r eq(AB) f r eq(cAcB)  (3)  We make use of Formula 3 for mining collocations from the bi-grams in the training data. There is one approximation in Formula 3. For collocation AB, its word class sequence is cAcB. When computing P2(AB), we only used f req(cA) instead of f req(cAcB). Note that when computing the right-hand-side of AB in the second line, we still use cB as the right-hand-side POS of AB. Similar strategy (of using cA as the left-hand-side POS tag and cB as the right-hand-side POS tag of AB) has been applied in Mecab and ChaSen7 for Japanese word segmenting and POS tagging.  3.2 K-means clustering The objective for unsupervised K-means clustering is to avoid (as much as possible) assigning entries with identical pronunciations or with large frequency variances into one word class. One big problem that hurts the precision of n-pos model is the existence of Kanji candidates with identical pronunciations in one word class, since the ranking is only determined by p(wi|ci). If we could assign different word classes to the homophonic Kanji candidates, we can further make use of P(ci|ci−1) instead of only P(wi|ci) to yield a better candidate ranking. We deﬁne a kernel function8 F (A1, A2) to describe the difference of pronunciations between two Kanji candidates A1 and A2:  F (A1, A2)  =  ed(pr on(A1),  
Most of the past error correction systems for ESL learners focus on local, lexical errors in a postprocessing manner. However, learners with low English proﬁciency have difﬁculties even constructing basic sentence structure, and many grammatical errors can be prevented by presenting grammatical phrases or patterns while they are writing. To achieve this, we propose an integrated writing environment for ESL learners, called phloat to help such users look up dictionaries to ﬁnd semantically appropriate and grammatical phrases in real-time. Using the system, users can look up phrases by either English or their native language (L1), without being aware of their input method. It subsequently suggests candidates to ﬁll the slots of the phrases. Also, we cluster suggested phrases with semantic groups to help users ﬁnd appropriate phrases. We conduct subject tests using the proposed system, and have found the system is useful to ﬁnd the right phrases and expressions. KEYWORDS: Writing Environment, Input Method, English as a Second Language, Suggestion of Patterns, Clustering of Candidates, Predicate Argument Structure. Proceedings of the Second Workshop on Advances in Text Input Methods (WTIM 2), pages 57–72, COLING 2012, Mumbai, December 2012. 57  
 Current mobile devices do not support Bangla (or Bengali) Input method. Due to this many Bangla language speakers have to write Bangla in mobile phone using English alphabets. During this time they used to write English foreign words using English spelling. This tendency also exists when writing in computer using phonetically input methods, which cause many typing mistakes. In this scenario, computer transliteration input method need to correct the foreign words written using English spelling. In this paper, we proposed a transliteration input method for Bangla language. For English foreign words, the system used International-PhoneticAlphabet(IPA)-based transliteration method for Bangla language. Our proposed approach improved the quality of Bangla transliteration input method by 14 points. KEYWORDS : Foreign Words, Bangla Transliteration, Bangla Input Method  
LuitPad is a stand-alone, fully Unicode compliant software designed for rapid typing of Assamese words and characters. There are two main typing options; one which is based on approximate sound of words and the other based on the sound of characters, both of which are eﬃcient and user-friendly, even for a ﬁrst-time user. In addition, LuitPad comes with an online spell-checker; on “right-clock” over a misspelt word, presents the user with a list of relevant appropriate corrections for replacements. Assamese is an Indic language, spoken throughout North-Eastern parts of India by approximately 30 million people. There is a severe lack of user-friendly software available for typing Assamese text. This is perhaps the underlying reason for the miniscule presence of Assamese based information storage and retrieval systems, both oﬀ-line and on-line. With LuitPad, the user can retrieve Assamese characters and words using an English alphabet based keyboard in an eﬀective and intuitive way. LuitPad is compatible with Windows, Mac and Linux with a GUI. The software can store the contents in LuitPad ﬁle format (“.pad” extension) that can store images and text. In addition, .pad ﬁles can be easily exported to pdf and html ﬁles. Keywords: LuitPad, Assamese language, Unicode, text input. ∗Corresponding author Proceedings of the Second Workshop on Advances in Text Input Methods (WTIM 2), pages 79–88, COLING 2012, Mumbai, December 2012. 79  
In the early 1990’s, online communication was restricted to ASCII (English) only environments. A convention evolved for typing Arabic in Roman characters, this scripting took various names including: Franco Arabic, Romanized Arabic, Arabizi, Arabish, etc… The convention was widely adopted and today, romanized Arabic (RAr) is everywhere: In instant messaging, forums, blog postings, product and movie ads, on mobile phones and on TV! The problem is that the majority of Arab users are more used to the English keyboard layout, and while romanized Arabic is easier to type, Arabic is significantly easier to read, the obvious solution was automatic conversion of romanized Arabic to Arabic script, which would also lead to increasing the amount and quality of authored Arabic online content. The main challenges are that no standard convention of Romanized Arabic (many  1 mappings) is available and there are no parallel data available. We present here a hybrid approach that we devised and implemented to build a romanized Arabic transliteration engine that was later on scaled to cover other scripts. Our approach leverages the work done by Sherif and Kondrak’s (2007b) and Cherry and Suzuki (2009), and is heavily inspired by the basic phrase-based statistical machine translation approach devised by (Och, 2003). KEYWORDS : Transliteration, Romanized Arabic, Franco-Arab, Maren, Arabic IME 
KEYWORDS : transliteration, predictive text input, multilingual text input, Quillpad, decision trees 
The last decade has seen rigorous activities in the ﬁeld of Sanskrit computational linguistics pertaining to word level and sentence level analysis. In this paper we point out the need of special treatment for Sanskrit at discourse level owing to speciﬁc trends in Sanskrit in the production of its literature ranging over two millennia. We present a tagset for inter-sentential analysis followed by a brief account of discourse level relations accounting the sub-topic and topic level analysis, as discussed in the Indian literature illustrating an application of these to a text in the domain of vya¯karan. a (grammar). KEYWORDS: Discourse analysis, Sanskrit, San˙gati. Proceedings of the Workshop on Advances in Discourse Analysis and its Computational Aspects (ADACA), pages 1–16, COLING 2012, Mumbai, December 2012. 
KEYWORDS : discourse relation, rhetorical relation, text clustering, SVMs, cluster validation Proceedings of the Workshop on Advances in Discourse Analysis and its Computational Aspects (ADACA), pages 17–32, COLING 2012, Mumbai, December 2012. 17  
Discourse relations in the recent literature are typically classiﬁed as either explicit (e.g., when a discourse connective like “because” is present) or implicit. This binary treatment of implicitness is advantageous for simplifying the explanation of many phenomena in discourse processing. On the other hand, linguists do not yet agree as to what types of textual particles contribute to revealing the relation between any pair of sentences or clauses in a text. At one extreme, we can claim that every single word in either of the sentences involved can play a role in shaping a discourse relation. In this work, we propose a measure to quantify how good a cue a certain textual element is for a speciﬁc discourse relation, i.e., a measure of the strength of discourse markers. We will illustrate how this measure becomes important both for modeling discourse relation construction as well as developing automatic tools for identifying discourse relations. Keywords: Discourse relations, Discourse markers, Discourse cues, Implicitness, Implicit and explicit relations. Proceedings of the Workshop on Advances in Discourse Analysis and its Computational Aspects (ADACA), pages 33–42, COLING 2012, Mumbai, December 2012. 33  
It is widely acknowledged that current dialogue systems are held back by a lack of ﬂexibility, both in their turn-taking model (typically, allowing only a strict back-and-forth between user and system) and in their interpretation capabilities (typically, restricted to slot ﬁlling). We have developed a component for NLU that attempts to address both of these challenges, by a) constructing robust but deep meaning representations that support a range of further user intention determination techniques from inference / reasoning-based ones to ones based on more basic structures, and b) constructing these representations incrementally and hence providing semantic information on which system reactions can be based concurrently to the ongoing user utterance. The approach is based on an existing semantic representation formalism, Robust Minimal Recursion Semantics, which we have modiﬁed to suit incremental construction. We present the modiﬁcations, our implementation, and discuss applications within a dialogue system context, showing that the approach indeed promises to meet the requirements for more ﬂexibility. KEYWORDS: Incremental Processing, Semantics Construction, Dialogue Systems, Dialogue, Natural Language Understanding, Spoken Language. Proceedings of the Workshop on Advances in Discourse Analysis and its Computational Aspects (ADACA), pages 59–76, COLING 2012, Mumbai, December 2012. 59  
Since Bleam's (2000) initial claim that capturing clitic climbing patterns in Romance requires the descriptive power of set-local MCTAG (Weir, 1988), alternative approaches to relaxing tree-locality restrictions have been developed, including delayed tree-local MCTAG (Chiang and Scheffler, 2008), which, unlike set-local MCTAG, is weakly equivalent to standard TAG. This paper compares 2-delayed treelocal MCTAG with set-local MCTAG in terms of how well the two formalisms can account for the clitic climbing data. We confirm that 2-delay tree-local MCTAG has the formal expressivity to cover the data by proposing an explicit grammar to do so. However, we also find that the constraint on set locality is particularly well-suited for capturing these clitic climbing patterns. I.e., though globally less restrictive, set-local MCTAG appears to be restrictive in just the right way in this specific case. 
 2 Tree Shapes and Type Shifting  In this paper, we introduce a type-shifting operation which provides a principled means of describing the derivational links required in Synchronous TAG accounts of quantiﬁcation. No longer do links appear on root nodes of predicates on an ad hoc basis, rather they are generated as a part of a type-shifting mechanism over arguments of the predicate. By introducing to the system a set of temporal variables, we show how this operation can also be used to account for the scope interactions of clausal embedding. We then move on to consider additional cases of multiple clausal embedding and coordination. 
Even though Minimalist grammars are more powerful than TAG on the string level, the classes of tree languages the two deﬁne are incomparable. I give a constructive proof that if the standard Move operation in Minimalist grammars is replaced by Reset Lowering, every TAG tree language can be generated. The opposite does not hold, so the strong generative capacity of Minimalist grammars with Reset Lowering exceeds that of TAG. 
In this paper we present an analysis of locative alternation phenomena in Russian and English within a frame-based LTAG syntax-semantics interface. The combination of a syntactic theory with an extended domain of locality and frames provides a powerful mechanism for argument linking. Furthermore, the concept of tree families and unanchored trees in LTAG allows for a decomposition of meaning into lexical and constructional components.  . S.  NP . [I= 3 ]  . NP[I.= 1 ]  . V.P V. NP[I.= 2 ]  NP . [I= 4 ] Ma.ry  Joh. n [  ]  .lov.es  [  ]  person  loving  person  3 NAME John EXPERIENCER 1  4 NAME Mary  THEME  2  Figure 1: Derivation for John loves Mary  
Context-free tree grammars, originally introduced by Rounds ((Rounds, 1970)), are powerful grammar devices for the deﬁnition of tree languages. In the present paper, we consider a subclass of the class of context-free tree languages, namely the class of monadic simple context-free tree languages. For this class of context-free tree languages, a faithful rendering of extended TAGs, we show that it can be given a simple logical characterization in terms of monadic second-order transductions. 
This paper presents a novel grammar formalism, Synchronous Tree Uniﬁcation Grammar (STUG), that borrows ideas from two rather distinct exemplars of tree-based grammar formalisms, namely Synchronous Tree Adjoining Grammar and Tree Uniﬁcation Grammar. At the same time STUG differs considerably from those in that it allows for a clean separation of syntax and valency. Exploiting this potential in the modelling of natural language grammar has a number of interesting consequences that we will sketch in the course of this paper. 
We consider pairs of context-free tree grammars combined through synchronous rewriting. The resulting formalism is at least as powerful as synchronous tree adjoining grammars and linear, nondeleting macro tree transducers, while the parsing complexity remains polynomial. Its power is subsumed by context-free hypergraph grammars. The new formalism has an alternative characterization in terms of bimorphisms. An advantage over synchronous variants of linear context-free rewriting systems is the ability to specify tree-to-tree transductions. 
We propose a Neo-Davidsonian semantics approach as a framework for constructing a semantic interpretation simultaneously with a strictly incremental syntactic derivation using the PLTAG formalism, which supports full connectedness of all words under a single node at each point in time. This paper explains why Neo-Davidsonian semantics is particularly suitable for incremental semantic construction and outlines how the semantic construction process works. We focus also on quantiﬁer scope, which turns out to be a particularly interesting question in the context of incremental TAG. 
The paper proposes an LTAG semantic analysis to derive semantic representations for different focus constructions in a uniform way. The proposal is shown via examples of different narrow focus constructions, multiple foci and focus in questions. 
In this paper, we show how the interactions between the tense, aspect and mood preverbal markers in Sa˜o Tomense can be formally and concisely described at an abstract level, using the concept of projection. More precisely, we show how to encode the different valid orders of preverbal markers in an abstract description of a Tree-Adjoining Grammar of Sa˜o Tomense. This description is written using the XMG meta-grammar language (Crabbe´ and Duchier, 2004). 
Solid techniques based on distributional learning have been developed targeting rich subclasses of CFGs and their extensions including linear context-free tree grammars. Along this line we propose a learning algorithm for some subclasses of IO contextfree tree grammars. 
While the derived trees yielded by TAG derivations are uncontroversially taken to correspond to phrase structure, the status of TAG derivation structures as more than a record of TAG operations is less certain. An attractive possibility is to interpret the derivation structure as some representation of semantic meaning, such as a dependency analysis. However, the literature has identified cases where doing so is problematic (Rambow et al., 1995, Candito and Kahane, 1998, Frank and van Genabith, 2001, Gardent and Kallmeyer, 2003, Kallmeyer and Romero 2008), including what has been referred to as the Missing Link Problem: predicates which should have a dependency link are unconnected in the derivation structure. This paper shows that delayed tree-local MC-TAG (Chiang and Scheffler, 2008) provides a solution for certain types of missing links. Further, we observe that the regular form 2-level TAG solutions to the Missing Link Problem given in (Dras et al., 2004) can be reinterpreted using delayed tree-local MC-TAG: the object level derivations of the 2-level TAG derivations can be converted into legal 1delayed tree-local MCTAG derivations. Thus, delayed tree-locality maintains the possibility that TAG derivation structures can be more meaning-laden than solely a record of the combination of trees. 
Several authors have pointed out that the correspondence between LTAG derivation trees and dependency structures is not as direct as it may seem at ﬁrst glance, and various proposals have been made to overcome this divergence. In this paper we propose to view the correspondence between derivation trees and dependency structures as a tree transformation during which the direction of some of the original edges is reversed. We show that, under this transformation, LTAG is able to induce both ill-nested dependency trees and dependency trees with gap-degree greater than 1, which is not possible under the direct reading of derivation trees as dependency trees. 
Using a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG), we present an approach for generating pairs of sentences that are related by a syntactic transformation and we apply this approach to create language learning exercises. We argue that the derivation trees of an FB-LTAG provide a good level of representation for capturing syntactic transformations. We relate our approach to previous work on sentence reformulation, question generation and grammar exercise generation. We evaluate precision and linguistic coverage. And we demonstrate the genericity of the proposal by applying it to a range of transformations including the Passive/Active transformation, the pronominalisation of an NP, the assertion / yes-no question relation and the assertion / wh-question transformation. 
Linear Context-Free Rewriting System (LCFRS) is an extension of Context-Free Grammar (CFG) in which a non-terminal can dominate more than a single continuous span of terminals. Probabilistic LCFRS have recently successfully been used for the direct data-driven parsing of discontinuous structures. In this paper we present a parser for binary PLCFRS of fan-out two, together with a novel monotonous estimate for A∗ parsing, with which we conduct experiments on modiﬁed versions of the German NeGra treebank and the Discontinuous Penn Treebank in which all trees have block degree two. The experiments show that compared to previous work, our approach provides an enormous speed-up while delivering an output of comparable richness. 
We review a number of different ‘algebraic’ perspectives on TAG and STAG in the framework of interpreted regular tree grammars (IRTGs). We then use this framework to derive a new parsing algorithm for TAGs, based on two algebras that describe strings and derived trees. Our algorithm is extremely modular, and can easily be adapted to the synchronous case. 
We discuss four previously published parsing algorithms for parallell multiple context-free grammar (PMCFG), and argue that they are similar to each other, and implement an Earley-style top-down algorithm. Starting from one of these algorithms, we derive three modiﬁcations – one bottom-up and two variants using a left corner ﬁlter. An evaluation shows that substantial improvements can be made by using the algorithm that performs best on a given grammar. The algorithms are implemented in Python and released under an open-source licence.  S → f (A) A → g(A) A → h()  f ( x, y ) = x y g( x, y ) = a x b, c y d h() = a b, c d  Figure 1: A grammar that recognizes the langauge {anbncndn | n > 0}.  S → f (A) A → g(A) A → h()  f.1 = 1.1 1.2  g.1 = a 1.1 b g.2 = c 1.2 d  h.1 = a b  h.2 = c d  Figure 2: The same grammar in variable-free form.  normally written like this:  We start by introducing the necessary concepts. Then we discuss four previously published PMCFG algorithms, and argue that they are similar. We take Angelov (2009) as a starting point for introducing three new parsing strategies. Finally we discuss various optimizations of the parsing strategies and give a small evaluation. 
There is a tension between the idea that idioms can be both listed in the lexicon, and the idea that they are themselves composed of the lexical items which seem to inhabit them in the standard way. In other words, in order to maintain the insight that idioms actually contain the words they look like they contain, we need to derive them syntactically from these words. However, the entity that should be assigned a special meaning is then a derivation, which is not the kind of object that can occur in a lexicon (which is, by deﬁnition, the atoms of which derivations are built), and thus not the kind of thing that we are able to assign meanings directly to. Here I will show how to resolve this tension in an elegant way, one which bears striking similarities to those proposed by psychologists and psycholinguists working on idioms. 
We propose a method for the extraction of a Tree Adjoining Grammar (TAG) from a dependency treebank which has some representative examples annotated with phrase structures. We show that the resulting TAG along with corresponding dependency structure can be used to convert a dependency treebank to a TAG-based phrase structure treebank.  our basic algorithm in Section 4. We then discuss two issues which require extensions to our agorithm: empty categories (Section 5) and longdistance word order variations (Section 6). These extensions are sketched, but we do not present them in full detail due to lack of space. Finally, we discuss remaining issues in Section 7 and then conclude. 2 The Hindi Treebank  
We discuss a class of constraint-based grammars, Lexicalized Well-Founded Grammars (LWFGs) and present the theoretical underpinnings for learning these grammars from a representative set of positive examples. Given several assumptions, we deﬁne the search space as a complete grammar lattice. In order to prove a learnability theorem, we give a general algorithm through which the top and bottom elements of the complete grammar lattice can be built. 
In this work, we present a generalization of the state-split method to probabilistic hypergraphs. We show how to represent the derivational stucture of probabilistic tree-adjoining grammars by hypergraphs and detail how the generalized state-split procedure can be applied to such representations, yielding a state-split procedure for tree-adjoining grammars. 
Recent results show that both TAG and Minimalist grammars can be enriched with rational constraints without increasing their strong generative capacity, where a constraint is rational iff it can be computed by a bottom-up tree automaton. This raises the question which aspects of syntax can be adequately formalized using only such constraints. One of hardest phenomena commonly studied by syntacticians is binding theory. In this paper, we give a high-level implementation of (the syntactic parts of) binding theory in terms of rational constraints, and we argue that this implementation is sufﬁciently powerful for natural language. This conclusion is backed up by data drawn from English, German, and American Sign Language. 
This paper presents a research note on the degree to which strictly incremental derivations (that is derivations which are fully connected at each point in time) are possible in Combinatory Categorial Grammar (CCG). There has been a recent surge of interest in incremental parsing both from the psycholinguistic community in a bid to build psycholinguistically plausible models of language comprehension, and from the NLP community for building systems that process language greedily in order to achieve shorter response times in spoken dialogue systems, for speech recognition and machine translation. CCG allows for a variety of different derivations, including derivations that are almost fully incremental. This paper explores the syntactic constructions for which full incrementality is not possible in standard CCG, a point that recent work on incremental CCG parsing has glossed over. 
Adding cosubstitution to the classical TAG operations of substitution and adjunction, coTAGs have been proposed as an “alternative conceptualization” to resolve the tension between the TAG mantra of locality of syntactic dependencies and the seeming non-locality of quantiﬁer scope. CoTAGs follow the tradition of synchronous TAGs (STAGs) in that they derive syntactic and semantic representations simultaneously as pairs. We demonstrate that the mappings deﬁnable by coTAGs go beyond those of “simple” STAGs. While with regard to the ﬁrst component, coTAGs are weakly and strongly equivalent to classical TAGs, the second projection of the synchronously derived representations, can in particular be— up to a homomorphism—the non-tree adjoining language MIX(k), for any k ≥ 3. 
We propose a new model for transforming dependency trees into target graphs, relying on two distinct stages. During the ﬁrst stage, standard local tree transformation rules based on patterns are applied to collect a ﬁrst set of constrained edges to be added to the target graph. In the second stage, motivated by linguistic considerations, the constraints on edges may be used to displace them or their neighbour edges upwards, or to build new mirror edges. The main advantages of this model is to simplify the design of a transformation scheme, with a smaller set of simpler local rules for the ﬁrst stage, and good properties of termination and conﬂuence for the second level. 
This paper gives an analysis explaining various cases where the scope of two logical operators is non-permutable in a sentence. The explanation depends on a theory of derivational economy couched in the synchronous tree adjoining grammar framework. Locality restrictions made using the STAG formalism allow us to limit the computational complexity of using transderivational constraints and allows us to make interesting empirical predictions. 
Work on the syntax-semantics interface in the TAG framework has grappled with the problem of identifying a system with sufﬁcient power to capture semantic dependencies which also imposes formally and linguistically interesting constraint on the kinds of dependencies that can be expressed. The consensus in recent years appears to have shifted to the use of a system that is substantially more expressive than TAG. In this paper, we revisit some of the arguments in favor of more formal power, particularly those from Nesson and Shieber (2008). We show that these arguments can be defused once we adopt a different perspective on predicate-headed semantic elementary trees, namely that they are divided into scope and variable components like their quantiﬁcational counterparts. We demonstrate as well that this proposal provides an new perspective on scope rigidity. 
 In this paper, we present our system description for the CoNLL-2012 coreference resolution task on English, Chinese and Arabic. We investigate a projection-based model in which we ﬁrst translate Chinese and Arabic into English, run a publicly available coreference system, and then use a new projection algorithm to map the coreferring entities back from English into mention candidates detected in the Chinese and Arabic source. We compare to a baseline that just runs the English coreference system on the supplied parses for Chinese and Arabic. Because our method does not beat the baseline system on the development set, we submit outputs generated by the baseline system as our ﬁnal submission. 
We present a new approach to named-entity recognition that jointly learns to identify named-entities in parallel text. The system generates seed candidates through local, cross-language edit likelihood and then bootstraps to make broad predictions across both languages, optimizing combined contextual, word-shape and alignment models. It is completely unsupervised, with no manually labeled items, no external resources, only using parallel text that does not need to be easily alignable. The results are strong, with F > 0.85 for purely unsupervised namedentity recognition across languages, compared to just F = 0.35 on the same data for supervised cross-domain named-entity recognition within a language. A combination of unsupervised and supervised methods increases the accuracy to F = 0.88. We conclude that we have found a viable new strategy for unsupervised named-entity recognition across lowresource languages and for domain-adaptation within high-resource languages. 
Transliteration has been usually recognized by spelling-based supervised models. However, a single model cannot deal with mixture of words with diﬀerent origins, such as “get” in “piaget” and “target”. Li et al. (2007) propose a class transliteration method, which explicitly models the source language origins and switches them to address this issue. In contrast to their model which requires an explicitly tagged training corpus with language origins, Hagiwara and Sekine (2011) have proposed the latent class transliteration model, which models language origins as latent classes and train the transliteration table via the EM algorithm. However, this model, which can be formulated as unigram mixture, is prone to overﬁtting since it is based on maximum likelihood estimation. We propose a novel latent semantic transliteration model based on Dirichlet mixture, where a Dirichlet mixture prior is introduced to mitigate the overﬁtting problem. We have shown that the proposed method considerably outperform the conventional transliteration models. 
Supervised Named Entity Recognizers require large amounts of annotated text. Since manual annotation is a highly costly procedure, reducing the annotation cost is essential. We present a fully automatic method to build NE annotated corpora from Wikipedia. In contrast to recent work, we apply a new method, which maps the DBpedia classes into CoNLL NE types. Since our method is mainly languageindependent, we used it to generate corpora for English and Hungarian. The corpora are freely available. 
In this paper, we describe our approach to English-to-Korean transliteration task in NEWS 2012. Our system mainly consists of two components: an letter-to-phoneme alignment with m2m-aligner,and transliteration training model DirecTL-p. We construct different parameter settings to train several transliteration models. Then, we use two reranking methods to select the best transliteration among the prediction results from the different models. One re-ranking method is based on the co-occurrence of the transliteration pair in the web corpora. The other one is the JLIS-Reranking method which is based on the features from the alignment results. Our standard and non-standard runs achieves 0.398 and 0.458 in top-1 accuracy in the generation task. 
We developed a machine transliteration system combining mpaligner (an improvement of m2m-aligner), DirecTL+, and some Japanesespeciﬁc heuristics for the purpose of NEWS 2012. Our results show that mpaligner is greatly better than m2m-aligner, and the Japanese-speciﬁc heuristics are effective for JnJk and EnJa tasks. While m2m-aligner is not good at long alignment, mpaligner performs well at longer alignment without any length limit. In JnJk and EnJa tasks, it is crucial to handle long alignment. An experimental result revealed that de-romanization, which is reverse operation of romanization, is crucial for JnJk task. In EnJa task, it is shown that mora is the best alignment unit for Japanese language. 
We consider the task of generating transliterated word forms. To allow for a wide range of interacting features, we use a conditional random ﬁeld (CRF) sequence labeling model. We then present two innovations: a training objective that optimizes toward any of a set of possible correct labels (since more than one transliteration is often possible for a particular input), and a k-best reranking stage to incorporate nonlocal features. This paper presents results on the Arabic-English transliteration task of the NEWS 2012 workshop. 
We report the results of our transliteration experiments with language-speciﬁc adaptations in the context of two language pairs: English to Chinese, and Arabic to English. In particular, we investigate a syllable-based Pinyin intermediate representation for Chinese, and a letter mapping for Arabic.  provide details about the system parameters used in M2M-ALIGNER and DIRECTL+. Section 3 provides details of our strategies adopted in the EnCh task, which incorporate Chinese-speciﬁc knowledge and system combination algorithm. In Section 4 we elaborate on the difﬁculty of Arabic name transliteration and propose a letter mapping scheme. In Section 5 we present the ofﬁcial test results.  
This work presents an English-to-Chinese (E2C) machine transliteration system based on two-stage conditional random fields (CRF) models with accessor variety (AV) as an additional feature to approximate local context of the source language. Experiment results show that two-stage CRF method outperforms the one-stage opponent since the former costs less to encode more features and finer grained labels than the latter. 
Comparisons play a critical role in scientific communication by allowing an author to situate their work in the context of earlier research problems, experimental approaches, and results. Our goal is to identify comparison claims automatically from full-text scientific articles. In this paper, we introduce a set of semantic and syntactic features that characterize a sentence and then demonstrate how those features can be used in three different classifiers: Naïve Bayes (NB), a Support Vector Machine (SVM) and a Bayesian network (BN). Experiments were conducted on 122 full-text toxicology articles containing 14,157 sentences, of which 1,735 (12.25%) were comparisons. Experiments show an F1 score of 0.71, 0.69, and 0.74 on the development set and 0.76, 0.65, and 0.74 on a validation set for the NB, SVM and BN, respectively. 
Key knowledge components of biological research papers are conveyed by structurally and rhetorically salient sentences that summarize the main findings of a particular experiment. In this article we define such sentences as Claimed Knowledge Updates (CKUs), and propose using them in text mining tasks. We provide evidence that CKUs convey the most important new factual information, and thus demonstrate that rhetorical salience is a systematic discourse structure indicator in biology articles along with structural salience. We assume that CKUs can be detected automatically with state-ofthe-art text analysis tools, and suggest some applications for presenting CKUs in knowledge bases and scientific browsing interfaces. 
Sentiment analysis of citations in scientiﬁc papers is a new and interesting problem which can open up many exciting new applications in bibliometrics. Current research assumes that using just the citation sentence is enough for detecting sentiment. In this paper, we show that this approach misses much of the existing sentiment. We present a new corpus in which all mentions of a cited paper have been annotated. We explore methods to automatically identify these mentions and show that the inclusion of implicit citations in citation sentiment analysis improves the quality of the overall sentiment assignment.  (O’Connor, 1982), and its connection with coreference has been duely noted (Kim et al., 2006; Kaplan et al., 2009). Consider Figure 1, which illustrates a typical case.  
Anatomical entities such as kidney, muscle and blood are central to much of biomedical scientiﬁc discourse, and the detection of mentions of anatomical entities is thus necessary for the automatic analysis of the structure of domain texts. Although a number of resources and methods addressing aspects of the task have been introduced, there have so far been no annotated corpora for training and evaluating systems for broad-coverage, open-domain anatomical entity mention detection. We introduce the AnEM corpus, a domain- and species-independent resource manually annotated for anatomical entity mentions using a ﬁne-grained classiﬁcation system. The corpus texts are selected randomly from citation abstracts and full-text papers with the aim of making the corpus representative of the entire available biomedical scientiﬁc literature. We demonstrate the use of the corpus through an evaluation of the broad-coverage MetaMap tagger and a CRF-based system trained on the corpus data, considering also a combination of these two methods. The combined system demonstrates a promising level of performance, approaching 80% F-score for mention detection for a relaxed matching criterion. The corpus and other introduced resources are available under open licences from http:// www.nactem.ac.uk/anatomy/. 
This paper presents a three-way perspective on the annotation of discourse in scientific literature. We use three different schemes, each of which focusses on different aspects of discourse in scientific articles, to annotate a corpus of three full-text papers, and compare the results. One scheme seeks to identify the core components of scientific investigations at the sentence level, a second annotates metaknowledge pertaining to bio-events and a third considers how epistemic knowledge is conveyed at the clause level. We present our analysis of the comparison, and a discussion of the contributions of each scheme. 
We propose a model for knowledge attribution and epistemic evaluation in scientific discourse, consisting of three dimensions with different values: source (author, other, unknown); value (unknown, possible, probable, presumed true) and basis (reasoning, data, other). Based on a literature review, we investigate four linguistic features that mark different types epistemic evaluation (modal auxiliary verbs, adverbs/adjectives, reporting verbs and references). A corpus study on two biology papers indicates the usefulness of this model, and suggest some typical trends. In particular, we find that matrix clauses with a reporting verb of the form ‘These results suggest’, are the predominant feature indicating knowledge attribution in scientific text. 
We integrate semantic information at two stages of the translation process of a state-ofthe-art SMT system. A Word Sense Disambiguation (WSD) classiﬁer produces a probability distribution over the translation candidates of source words which is exploited in two ways. First, the probabilities serve to rerank a list of n-best translations produced by the system. Second, the WSD predictions are used to build a supplementary language model for each sentence, aimed to favor translations that seem more adequate in this speciﬁc sentential context. Both approaches lead to signiﬁcant improvements in translation performance, highlighting the usefulness of source side disambiguation for SMT. 
In this paper, we present our linguisticallyenriched Bulgarian-to-English statistical machine translation model, which takes a statistical machine translation (SMT) system as backbone various linguistic features as factors. The motivation is to take advantages of both the robustness of the SMT system and the rich linguistic knowledge from morphological analysis as well as the hand-crafted grammar resources. The automatic evaluation has shown promising results and our extensive manual analysis conﬁrms the high quality of the translation the system delivers. The whole framework is also extensible for incorporating information provided by different sources. 
This paper presents an approach to improving performance of statistical machine translation by automatically creating new training data for difﬁcult to translate phenomena. In particular this contribution is targeted towards tackling the poor performance of a state-of-the-art system on negated sentences. The corpus expansion is achieved by high quality rephrasing of existing sentences to their negated counterparts making use of semantic transfer. The method is designed to work on both sides of the parallel corpus while preserving the alignment. Our results show an overall improvement of 0.16 BLEU points, with a statistically signiﬁcant increase of 1.63 BLEU points when tested on only negated test data. 
HMEANT (Lo and Wu, 2011a) is a manual MT evaluation technique that focuses on predicate-argument structure of the sentence. We relate HMEANT to an established linguistic theory, highlighting the possibilities of reusing existing knowledge and resources for interpreting and automating HMEANT. We apply HMEANT to a new language, Czech in particular, by evaluating a set of Englishto-Czech MT systems. HMEANT proves to correlate with manual rankings at the sentence level better than a range of automatic metrics. However, the main contribution of this paper is the identiﬁcation of several issues of HMEANT annotation and our proposal on how to resolve them. 
In this paper, we present two dependency parser training methods appropriate for parsing outputs of statistical machine translation (SMT), which pose problems to standard parsers due to their frequent ungrammaticality. We adapt the MST parser by exploiting additional features from the source language, and by introducing artiﬁcial grammatical errors in the parser training data, so that the training sentences resemble SMT output. We evaluate the modiﬁed parser on DEPFIX, a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input. Both parser modiﬁcations led to improvements in BLEU score; their combination was evaluated manually, showing a statistically signiﬁcant improvement of the translation quality. 
We present an unsupervised approach to estimate the appropriate degree of contribution of each semantic role type for semantic translation evaluation, yielding a semantic MT evaluation metric whose correlation with human adequacy judgments is comparable to that of recent supervised approaches but without the high cost of a human-ranked training corpus. Our new unsupervised estimation approach is motivated by an analysis showing that the weights learned from supervised training are distributed in a similar fashion to the relative frequencies of the semantic roles. Empirical results show that even without a training corpus of human adequacy rankings against which to optimize correlation, using instead our relative frequency weighting scheme to approximate the importance of each semantic role type leads to a semantic MT evaluation metric that correlates comparable with human adequacy judgments to previous metrics that require far more expensive human rankings of adequacy over a training corpus. As a result, the cost of semantic MT evaluation is greatly reduced. 
In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chineseto-English (Wang et al., 2007) and Englishto-Japanese (Isozaki et al., 2010b) translation with signiﬁcant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the ﬁrst reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-ﬁnalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more reﬁned rules. 
This paper presents two procedures for extracting transfer rules from parallel corpora for use in a rule-based Japanese-English MT system. First a “shallow” method where the parallel corpus is lemmatized before it is aligned by a phrase aligner, and then a “deep” method where the parallel corpus is parsed by deep parsers before the resulting predicates are aligned by phrase aligners. In both procedures, the phrase tables produced by the phrase aligners are used to extract semantic transfer rules. The procedures were employed on a 10 million word Japanese English parallel corpus and 190,000 semantic transfer rules were extracted. 
Weighted ﬁnite-state acceptors and transducers (Pereira and Riley, 1997) are a critical technology for NLP and speech systems. They ﬂexibly capture many kinds of stateful left-toright substitution, simple transducers can be composed into more complex ones, and they are EM- trainable. They are unable to handle long-range syntactic movement, but tree acceptors and transducers address this weakness (Knight and Graehl, 2005). Tree automata have been proﬁtably used in syntaxbased MT systems. Still, strings and trees are both weak at representing linguistic structure involving semantics and reference (“who did what to whom”). Feature structures provide an attractive, well-studied, standard format (Shieber, 1986; Rounds and Kasper, 1986), which we can view computationally as directed acyclic graphs. In this paper, we develop probabilistic acceptors and transducers for feature structures, demonstrate them on linguistic problems, and lay down a foundation for semantics-based MT. 
In this article we investigate the translation of terms from English into German and vice versa in the isolation of an ontology vocabulary. For this study we built new domainspeciﬁc resources from the translation search engine Linguee and from the online encyclopedia Wikipedia. We learned that a domainspeciﬁc resource produces better results than a bigger, but more general one. The ﬁrst ﬁnding of our research is that the vocabulary and the structure of the parallel corpus are important. By integrating the multilingual knowledge base Wikipedia, we further improved the translation wrt. the domain-speciﬁc resources, whereby some translation evaluation metrics outperformed the results of Google Translate. This ﬁnding leads us to the conclusion that a hybrid translation system, a combination of bilingual terminological resources and statistical machine translation can help to improve translation of domain-speciﬁc terms. 
Verb plays a crucial role of specifying the action or function performed in a sentence. In translating English to morphologically richer language like Hindi, the organization and the order of verbal constructs contributes to the fluency of the language. Mere statistical methods of machine translation are not sufficient enough to consider this aspect. Identification of verb parts in a sentence is essential for its understanding and they constitute as if they are a single entity. Considering them as a single entity improves the translation of the verbal construct and thus the overall quality of the translation. The paper describes a strategy for pre-processing and for identification of verb parts in source and target language corpora. The steps taken towards reducing sparsity further helped in improving the translation results. 
The paper presents a new resource light ﬂexible method for clause alignment which combines the Gale-Church algorithm with internally collected textual information. The method does not resort to any pre-developed linguistic resources which makes it very appropriate for resource light clause alignment. We experiment with a combination of the method with the original Gale-Church algorithm (1993) applied for clause alignment. The performance of this ﬂexible method, as it will be referred to hereafter, is measured over a specially designed test corpus. The clause alignment is explored as means to provide improved training data for the purposes of Statistical Machine Translation (SMT). A series of experiments with Moses demonstrate ways to modify the parallel resource and effects on translation quality: (1) baseline training with a Bulgarian-English parallel corpus aligned at sentence level; (2) training based on parallel clause pairs; (3) training with clause reordering, where clauses in each source language (SL) sentence are reordered according to order of the clauses in the target language (TL) sentence. Evaluation is based on BLEU score and shows small improvement when using the clause aligned corpus. 
In Japanese, particularly, spoken Japanese, subjective, objective and possessive cases are very often omitted. Such Japanese sentences are often translated by Japanese-English statistical machine translation to the English sentence whose subjective, objective and possessive cases are omitted, and it causes to decrease the quality of translation. We performed experiments of J-E phrase based translation using Japanese sentence, whose omitted pronouns are complemented by human. We introduced ‘antecedent F-measure’ as a score for measuring quality of the translated English. As a result, we found that it improves the scores of antecedent F-measure while the BLEU scores were almost unchanged. Every effectiveness of the zero pronoun resolution differs depending on the type and case of each zero pronoun. 
Relational clustering has received much attention from researchers in the last decade. In this paper we present a parametric method that employs a combination of both hard and soft clustering. Based on the corresponding Markov chain of an affinity matrix, we simulate a probability distribution on the states by defining a conditional probability for each subpopulation of states. This probabilistic model would enable us to use expectation maximization for parameter estimation. The effectiveness of the proposed approach is demonstrated on several real datasets against spectral clustering methods. 
Most of the research on social networks has almost exclusively focused on positive links between entities. There are much more insights that we may gain by generalizing social networks to the signed case where both positive and negative edges are considered. One of the reasons why signed social networks have received less attention that networks based on positive links only is the lack of an explicit notion of negative relations in most social network applications. However, most such applications have text embedded in the social network. Applying linguistic analysis techniques to this text enables us to identify both positive and negative interactions. In this work, we propose a new method to automatically construct a signed social network from text. The resulting networks have a polarity associated with every edge. Edge polarity is a means for indicating a positive or negative afﬁnity between two individuals. We apply the proposed method to a larger amount of online discussion posts. Experiments show that the proposed method is capable of constructing networks from text with high accuracy. We also connect out analysis to social psychology theories of signed network, namely the structural balance theory. 
Twitter, a popular social networking service, enables its users to not only send messages but re-broadcast or retweet a message from another Twitter user to their own followers. Considering the number of times that a message is retweeted across Twitter is a straightforward way to estimate how interesting it is. However, a considerable number of messages in Twitter with high retweet counts are actually mundane posts by celebrities that are of interest to themselves and possibly their followers. In this paper, we leverage retweets as implicit relationships between Twitter users and messages and address the problem of automatically ﬁnding messages in Twitter that may be of potential interest to a wide audience by using link analysis methods that look at more than just the sheer number of retweets. Experimental results on real world data demonstrate that the proposed method can achieve better performance than several baseline methods. 
 We learn graph-based similarity measures for the task of extracting word synonyms from a corpus of parsed text. A constrained graph walk variant that has been successfully applied in the past in similar settings is shown to outperform a state-of-the-art syntactic vectorbased approach on this task. Further, we show that learning specialized similarity measures for different word types is advantageous.  
This paper presents a graph-based method for all-word word sense disambiguation of biomedical texts using semantic relatedness as edge weight. Semantic relatedness is derived from a term-topic co-occurrence matrix. The sense inventory is generated by the MetaMap program. Word sense disambiguation is performed on a disambiguation graph via a vertex centrality measure. The proposed method achieves competitive performance on a benchmark dataset. 
In this paper we present the SDOIrmi text graph-based semi-supervised algorithm for the task for relation mention identification when the underlying concept mentions have already been identified and linked to an ontology. To overcome the lack of annotated data, we propose a labelling heuristic based on information extracted from the ontology. We evaluated the algorithm on the kdd09cma1 dataset using a leave-one-document-out framework and demonstrated an increase in F1 in performance over a co-occurrence based AllTrue baseline algorithm. An extrinsic evaluation of the predictions suggests a worthwhile precision on the more confidently predicted additions to the ontology. 
Social tagging systems, which allow users to freely annotate online resources with tags, become popular in the Web 2.0 era. In order to ease the annotation process, research on social tag recommendation has drawn much attention in recent years. Modeling the social tagging behavior could better reflect the nature of this issue and improve the result of recommendation. In this paper, we proposed a novel approach for bringing the associative ability to model the social tagging behavior and then to enhance the performance of automatic tag recommendation. To simulate human tagging process, our approach ranks the candidate tags on a weighted digraph built by the semantic relationships among meaningful words in the summary and the corresponding tags for a given resource. The semantic relationships are learnt via a word alignment model in statistical machine translation on large datasets. Experiments on real world datasets demonstrate that our method is effective, robust and language-independent compared with the stateof-the-art methods.  still find the inner relationship between the tags and the resource that they describe. Figure 1 shows a snapshot of a social tagging example, where the famous artist, Michael Jackson was annotated with multiple social tags by users in Last.fm2. Actually, Figure 1 can be divided into three parts, which are the title, the summary and the tags respectively.  
In this paper, we propose a novel human computation game for sentiment analysis. Our game aims at annotating sentiments of a collection of text documents and simultaneously constructing a highly discriminative lexicon of positive and negative phrases. Human computation games have been widely used in recent years to acquire human knowledge and use it to solve problems which are infeasible to solve by machine intelligence. We package the problems of lexicon construction and sentiment detection as a single human computation game. We compare the results obtained by the game with that of other well-known sentiment detection approaches. Obtained results are promising and show improvements over traditional approaches. 
This paper presents a game with a purpose for the construction of a Portuguese lexicalsemantic network. The network creation is implicit, as players collaboratively create links between words while they have fun. We describe the principles and implementation of the platform. As this is an ongoing project, we discuss challenges and long-term goals.We present the current network in terms a quantitative and qualitative analysis, comparing it to other resources. Finally, we describe our target applications. 
In this paper, we propose the collaborative construction of language resources (translation memories) using a novel browser extension-based client-server architecture that allows translation (or ‘localisation’) of web content capturing and aligning source and target content produced by the ‘power of the crowd’. The architectural approach chosen enables collaborative, in-context, and realtime localisation of web content supported by the crowd and high-quality language resources. To the best of our knowledge, this is the only practical web content localisation methodology currently being proposed that incorporates the collaborative construction and use of TMs. The approach also supports the building of resources such as parallel corpora – resources that are still not available for many, and especially not for underserved languages. 
Taxonomies, such as Library of Congress Subject Headings and Open Directory Project, are widely used to support browsing-style information access in document collections. We call them browsing taxonomies. Most existing browsing taxonomies are manually constructed thus they could not easily adapt to arbitrary document collections. In this paper, we investigate both automatic and interactive techniques to derive taxonomies from scratch for arbitrary document collections. Particular, we focus on encoding user feedback in taxonomy construction process to handle task-speciﬁcation rising from a given document collection. We also addresses the problem of path inconsistency due to local relation recognition in existing taxonomy construction algorithms. The user studies strongly suggest that the proposed approach successfully resolve task speciﬁcation and path inconsistency in taxonomy construction. 
 To summarize, EAGER’s main contributions are  Key to named entity recognition, the manual gazetteering of entity lists is a costly, errorprone process that often yields results that are incomplete and suffer from sampling bias. Exploiting current sources of structured information, we propose a novel method for extending minimal seed lists into complete gazetteers. Like previous approaches, we value WIKIPEDIA as a huge, well-curated, and relatively unbiased source of entities. However, in contrast to previous work, we exploit not only its content, but also its structure, as exposed in DBPEDIA. We extend gazetteers through Wikipedia categories, carefully limiting the impact of noisy categorizations. The resulting gazetteers easily outperform previous approaches on named entity recognition. 
 H: Dr. Bond created a medical institution for sick people.  Recent work on Textual Entailment has shown a crucial role of knowledge to support entailment inferences. However, it has also been demonstrated that currently available entailment rules are still far from being optimal. We propose a methodology for the automatic acquisition of large scale context-rich entailment rules from Wikipedia revisions, taking advantage of the syntactic structure of entailment pairs to deﬁne the more appropriate linguistic constraints for the rule to be successfully applicable. We report on rule acquisition experiments on Wikipedia, showing that it enables the creation of an innovative (i.e. acquired rules are not present in other available resources) and good quality rule repository.  a (directional) lexical rule like: 1) LHS: hospital ⇒ RHS: medical institution probability: 0.8 brings to a TE system (aimed at recognizing that a particular target meaning can be inferred from different text variants in several NLP application, e.g. Question Answering or Information Extraction) the knowledge that the word hospital in Text can be aligned, or transformed, into the word medical institution in the Hypothesis, with a probability 0.8 that this operation preserves the entailment relation among T and H. Similar considerations apply for more complex rules involving verbs, as:  
The current paper presents a languageindependent methodology, which facilitates the creation of machine translation (MT) systems for various language pairs. This methodology is implemented in the PRESEMT hybrid MT system. PRESEMT has the lowest possible requirements on specialised resources and tools, given that for many languages (especially less widely used ones) only limited linguistic resources are available. In PRESEMT, the main translation process comprises two phases. The first one, Structure selection, determines the overall structure of a target language (TL) sentence, drawing on syntactic information from a small bilingual corpus. The second phase, Translation equivalent selection, relies on models extracted solely from monolingual corpora to implement translation disambiguation, determine intra-phrase word order and handle functional words. This paper proposes extracting information for disambiguation from the monolingual corpus. Experimental results indicate that such information substantially contributes in improving translation quality. 
Recognition of Named Entities (NEs) is a difﬁcult process in Indian languages like Hindi, Telugu, etc., where sufﬁcient gazetteers and annotated corpora are not available compared to English language. This paper details a novel clustering and co-occurrence based approach to map English NEs with their equivalent representations from different languages recognized in a language-independent way. We have substituted the required language speciﬁc resources by the richly structured multilingual content of Wikipedia. The approach includes clustering of highly similar Wikipedia articles. Then the NEs in an English article are mapped with other language terms in interlinked articles based on co-occurrence frequencies. The cluster information and the term co-occurrences are considered in extracting the NEs from non-English languages. Hence, the English Wikipedia is used to bootstrap the NEs for other languages. Through this approach, we have availed the structured, semi-structured and multilingual content of the Wikipedia to a massive extent. Experimental results suggest that the proposed approach yields promising results in rates of precision and recall. 
Morph length is one of the indicative feature that helps learning the morphology of languages, in particular agglutinative languages. In this paper, we introduce a simple unsupervised model for morphological segmentation and study how the knowledge of morph length affect the performance of the segmentation task under the Bayesian framework. The model is based on (Goldwater et al., 2006) unigram word segmentation model and assumes a simple prior distribution over morph length. We experiment this model on two highly related and agglutinative languages namely Tamil and Telugu, and compare our results with the state of the art Morfessor system. We show that, knowledge of morph length has a positive impact and provides competitive results in terms of overall performance. 
In this paper we present a methodology for building comparable corpus, using multilingual ontologies of a scpeciﬁc domain. This resource can be exploited to foster research on multilingual corpus-based ontology learning, population and matching. The building resource process is exempliﬁed by the construction of annotated comparable corpora in English, Portuguese, and French. The corpora, from the conference organization domain, are built using the multilingual ontology concept labels as seeds for crawling relevant documents from the web through a search engine. Using ontologies allows a better coverage of the domain. The main goal of this paper is to describe the design methodology followed by the creation of the corpora. We present a preliminary evaluation and discuss their characteristics and potential applications. 
We explore ﬁlled pause usage in spontaneous medical narration. Expert physicians viewed images of dermatological conditions and provided a description while working toward a diagnosis. The narratives were analyzed for differences in ﬁlled pauses used by attending (experienced) and resident (in-training) physicians and by male and female physicians. Attending physicians described more and used more ﬁlled pauses than residents. No difference was found by speaker gender. Acoustic speech features were examined for two types of ﬁlled pauses: nasal (e.g. um) and non-nasal (e.g. uh). Nasal ﬁlled pauses were more often followed by longer silent pauses. Scores capturing diagnostic correctness and diagnostic thoroughness for each narrative were compared against ﬁlled pauses. The number of ﬁlled and silent pauses trends upward as correctness scores increase, indicating a tentative relationship between ﬁlled pause usage and expertise. Also, we report on a computational model for predicting types of ﬁlled pause. 
In this paper, we propose to study the effects of negation and modality on opinion expressions. Based on linguistic experiments informed by native speakers, we distill these effects according to the type of modality and negation. We show that each type has a speciﬁc effect on the opinion expression in its scope: both on the polarity and the strength for negation, and on the strength and/or the degree of certainty for modality. The empirical results reported in this paper provide a basis for future opinion analysis systems that have to compute the sentiment orientation at the sentence or at the clause level. The methodology we used for deriving this basis was applied for French but it can be easily instantiated for other languages like English. 
In the medical domain, misdiagnoses and diagnostic uncertainty put lives at risk and incur substantial ﬁnancial costs. Clearly, medical reasoning and decision-making need to be better understood. We explore a possible link between linguistic expression and diagnostic correctness. We report on an unusual data set of spoken diagnostic narratives used to computationally model and predict diagnostic correctness based on automatically extracted and linguistically motivated features that capture physicians’ uncertainty. A multimodal data set was collected as dermatologists viewed images of skin conditions and explained their diagnostic process and observations aloud. We discuss experimentation and analysis in initial and secondary pilot studies. In both cases, we experimented with computational modeling using features from the acoustic-prosodic and lexical-structural linguistic modalities. 
In this paper we present an iterative methodology to improve classiﬁer performance by incorporating linguistic knowledge, and propose a way to incorporate domain rules into the learning process. We applied the methodology to the tasks of hedge cue recognition and scope detection and obtained competitive results on a publicly available corpus. 
We explore training an automatic modality tagger. Modality is the attitude that a speaker might have toward an event or state. One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. We investigate an approach to automatically training a modality tagger where we ﬁrst gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation. We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance. 
Blanco & Moldovan (Blanco and Moldovan, 2011) have empirically demonstrated that negated sentences often convey implicit positive inferences, or focus, and that these inferences are both human annotatable and machine learnable. Concentrating on their annotation process, this paper argues that the focusbased implicit positivity should be separated from concepts of scalar implicature and negraising, as well as the placement of stress. We show that a model making these distinctions clear and which incorporates the pragmatic notion of question under discussion yields κ rates above .80, but that it substantially deﬂates the rates of focus of negation in text. 
Understanding the ways in which participants in public discussions frame their arguments is important in understanding how public opinion is formed. In this paper, we adopt the position that it is time for more computationallyoriented research on problems involving framing. In the interests of furthering that goal, we propose the following speciﬁc, interesting and, we believe, relatively accessible question: In the controversy regarding the use of genetically-modiﬁed organisms (GMOs) in agriculture, do pro- and anti-GMO articles differ in whether they choose to adopt a more “scientiﬁc” tone? Prior work on the rhetoric and sociology of science suggests that hedging may distinguish popular-science text from text written by professional scientists for their colleagues. We propose a detailed approach to studying whether hedge detection can be used to understanding scientiﬁc framing in the GMO debates, and provide corpora to facilitate this study. Some of our preliminary analyses suggest that hedges occur less frequently in scientiﬁc discourse than in popular text, a ﬁnding that contradicts prior assertions in the literature. We hope that our initial work and data will encourage others to pursue this promising line of inquiry. 
In this paper we investigate two distinct tasks. The ﬁrst task involves detecting arguing subjectivity, a type of linguistic subjectivity on which relatively little work has yet to be done. The second task involves labeling instances of arguing subjectivity with argument tags reﬂecting the conceptual argument being made. We refer to these two tasks collectively as “recognizing arguments”. We develop a new annotation scheme and assemble a new annotated corpus to support our learning efforts. Through our machine learning experiments, we investigate the utility of a sentiment lexicon, discourse parser, and semantic similarity measures with respect to recognizing arguments. By incorporating information gained from these resources, we outperform a unigram baseline by a signiﬁcant margin. In addition, we explore a two-phase approach to recognizing arguments, with promising results.  in the domain of smartphone reviews, aspects could include product features such as the keyboard, screen quality, and battery life. Although sentiment analysis is interesting and important in its own right, this paradigm does not seem to be the best match for ﬁnegrained analysis of ideological domains. While sentiment is also present in documents from this domain, previous work (Somasundaran and Wiebe, 2010) has found that arguing subjectivity, a less-studied form of subjectivity, is more frequently employed and more relevant for a robust assessment of ideological positions. Whereas sentiment conveys the polarity of a writer’s aﬀect towards a topic, arguing subjectivity is a type of linguistic subjectivity in which a person expresses a controversial belief about what is true or what action ought to be taken regarding a central contentious issue (Somasundaran, 2010). For example, consider this sentence about health care reform:  
With more than 10,000 new videos posted online every day on social websites such as YouTube and Facebook, the internet is becoming an almost inﬁnite source of information. One important challenge for the coming decade is to be able to harvest relevant information from this constant ﬂow of multimodal data. In this talk, I will introduce the task of multimodal sentiment analysis, and present a method that integrates linguistic, audio, and visual features for the purpose of identifying sentiment in online videos. I will ﬁrst describe a novel dataset consisting of videos collected from the social media website YouTube, which were annotated for sentiment polarity. I will then show, through comparative experiments, that the joint use of visual, audio, and textual features greatly improves over the use of only one modality at a time. Finally, by running evaluations on datasets in English and Spanish, I will show that the method is portable and works equally well when applied to different languages. This is joint work with Veronica Perez-Rosas and Louis-Philippe Morency. 
Many approaches to opinion and sentiment analysis rely on lexicons of words that may be used to express subjectivity. These are compiled as lists of keywords, rather than word meanings (senses). However, many keywords have both subjective and objective senses. False hits – subjectivity clues used with objective senses – are a signiﬁcant source of error in subjectivity and sentiment analysis. This talk will focus on sense-level opinion and sentiment analysis. First, I will give the results of a study showing that even words judged in previous work to be reliable opinion clues have signiﬁcant degrees of subjectivity sense ambiguity. Then, we will consider the task of distinguishing between the subjective and objective senses of words in a dictionary, and the related task of creating “usage inventories” of opinion clues. Given such distinctions, the next step is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses (we call this task “SWSD”). We will see evidence that SWSD is more feasible than full word sense disambiguation, because it is more coarse grained – often, the exact sense need not be pinpointed, and that SWSD can be exploited to improve the performance of opinion and sentiment analysis systems via sense-aware classiﬁcation. Finally, I will discuss experiments in acquiring SWSD data, via token-based context discrimination where the context vector representation is adapted to distinguish between subjective and objective contexts, and the clustering process is enriched by pair-wise constraints, making it semi-supervised. 
This paper presents a novel approach in Sentiment Polarity Detection on Twitter posts, by extracting a vector of weighted nodes from the graph of WordNet. These weights are used on SentiWordNet to compute a ﬁnal estimation of the polarity. Therefore, the method proposes a non-supervised solution that is domain-independent. The evaluation over a generated corpus of tweets shows that this technique is promising. 
Twitter is a micro blogging website, where users can post messages in very short text called Tweets. Tweets contain user opinion and sentiment towards an object or person. This sentiment information is very useful in various aspects for business and governments. In this paper, we present a method which performs the task of tweet sentiment identiﬁcation using a corpus of pre-annotated tweets. We present a sentiment scoring function which uses prior information to classify (binary classiﬁcation ) and weight various sentiment bearing words/phrases in tweets. Using this scoring function we achieve classiﬁcation accuracy of 87% on Stanford Dataset and 88% on Mejaj dataset. Using supervised machine learning approach, we achieve classiﬁcation accuracy of 88% on Stanford dataset.  
In this work, we present SAMAR, a system for Subjectivity and Sentiment Analysis (SSA) for Arabic social media genres. We investigate: how to best represent lexical information; whether standard features are useful; how to treat Arabic dialects; and, whether genre speciﬁc features have a measurable impact on performance. Our results suggest that we need individualized solutions for each domain and task, but that lemmatization is a feature in all the best approaches. 
The classiﬁcation of opinion texts in positive and negative can be tackled by evaluating separate key words but this is a very limited approach. We propose an approach based on the order of the words without using any syntactic and semantic information. It consists of building one probabilistic model for the positive and another one for the negative opinions. Then the test opinions are compared to both models and a decision and conﬁdence measure are calculated. In order to reduce the complexity of the training corpus we ﬁrst lemmatize the texts and we replace most namedentities with wildcards. We present an accuracy above 81% for Spanish opinions in the ﬁnancial products domain. 
Current sentiment analysis systems rely on static (context independent) sentiment lexica with proximity based fixed-point prior polarities. However, sentimentorientation changes with context and these lexical resources give no indication of which value to pick at what context. The general trend is to pick the highest one, but which that is may vary at context. To overcome the problems of the present proximity-based static sentiment lexicon techniques, the paper proposes a new way to represent sentiment knowledge in a Vector Space Model. This model can store dynamic prior polarity with varying contextual information. The representation of the sentiment knowledge in the Conceptual Spaces of distributional Semantics is termed Sentimantics. 
In the NLP field, there have been a lot of works which focus on the reviewer’s point of view conducted on sentiment analyses, which ranges from trying to estimate the reviewer’s score. However the reviews are used by the readers. The reviews that give a big influence to the readers should have the highest value, rather than the reviews to which was assigned the highest score by the writer. In this paper, we conducted the analyses using the reader’s point of view. We asked 20 subjects to read 500 sentences in the reviews of Rakuten travel and extracted the sentences that gave a big influence to the subjects. We analyze the influential sentences from the following two points of view, 1) targets and evaluations and 2) personal tastes. We found that “room”, “service”, “meal” and “scenery” are important targets which are items included in the reviews, and that “features” and “human senses” are important evaluations which express sentiment or explain targets. Also we showed personal tastes appeared on “meal” and “service”. 
The past years have shown a steady growth in interest in the Natural Language Processing task of sentiment analysis. The research community in this ﬁeld has actively proposed and improved methods to detect and classify the opinions and sentiments expressed in different types of text - from traditional press articles, to blogs, reviews, fora or tweets. A less explored aspect has remained, however, the issue of dealing with sentiment expressed in texts in languages other than English. To this aim, the present article deals with the problem of sentiment detection in three different languages - French, German and Spanish - using three distinct Machine Translation (MT) systems - Bing, Google and Moses. Our extensive evaluation scenarios show that SMT systems are mature enough to be reliably employed to obtain training data for languages other than English and that sentiment analysis systems can obtain comparable performances to the one obtained for English. 
Online debate forums provide a powerful communication platform for individual users to share information, exchange ideas and express opinions on a variety of topics. Understanding people’s opinions in such forums is an important task as its results can be used in many ways. It is, however, a challenging task because of the informal language use and the dynamic nature of online conversations. In this paper, we propose a new method for identifying participants’ agreement or disagreement on an issue by exploiting information contained in each of the posts. Our proposed method ﬁrst regards each post in its local context, then aggregates posts to estimate a participant’s overall position. We have explored the use of sentiment, emotional and durational features to improve the accuracy of automatic agreement and disagreement classiﬁcation. Our experimental results have shown that aggregating local positions over posts yields better performance than nonaggregation baselines when identifying users’ global positions on an issue. 
A set of words labelled with their prior emotion is an obvious place to start on the automatic discovery of the emotion of a sentence, but it is clear that context must also be considered. No simple function of the labels on the individual words may capture the overall emotion of the sentence; words are interrelated and they mutually inﬂuence their affectrelated interpretation. We present a method which enables us to take the contextual emotion of a word and the syntactic structure of the sentence into account to classify sentences by emotion classes. We show that this promising method outperforms both a method based on a Bag-of-Words representation and a system based only on the prior emotions of words. The goal of this work is to distinguish automatically between prior and contextual emotion, with a focus on exploring features important for this task. 
Current approaches to sentiment analysis assume that the sole discourse function of sentiment-bearing texts is expressivity. However, the persuasive discourse function also utilises expressive language. In this work, we present the results of training supervised classiﬁers on a new corpus of clinical texts that contain documents with an expressive discourse function, and we test the learned models on a subset of the same corpus containing persuasive texts. The results of this indicate that despite the difference in discourse function, the learned models perform favourably. 
This paper presents a corpus targeting evaluative meaning as it pertains to descriptions of events. The corpus, POLITICAL-ADS is drawn from 141 television ads from the 2008 U.S. presidential race and contains 3945 NPs and 1549 VPs annotated for scalar sentiment from three different perspectives: the narrator, the annotator, and general society. We show that annotators can distinguish these perspectives reliably and that correlation between the annotator’s own perspective and that of a generic individual is higher than those with the narrator. Finally, as a sample application, we demonstrate that a simple compositional model built off of lexical resources outperforms a lexical baseline. 
This paper presents our research on automatic annotation of a ﬁve-billion-word corpus of Japanese blogs with information on affect and sentiment. We ﬁrst perform a study in emotion blog corpora to discover that there has been no large scale emotion corpus available for the Japanese language. We choose the largest blog corpus for the language and annotate it with the use of two systems for affect analysis: ML-Ask for word- and sentence-level affect analysis and CAO for detailed analysis of emoticons. The annotated information includes affective features like sentence subjectivity (emotive/non-emotive) or emotion classes (joy, sadness, etc.), useful in affect analysis. The annotations are also generalized on a 2-dimensional model of affect to obtain information on sentence valence/polarity (positive/negative) useful in sentiment analysis. The annotations are evaluated in several ways. Firstly, on a test set of a thousand sentences extracted randomly and evaluated by over forty respondents. Secondly, the statistics of annotations are compared to other existing emotion blog corpora. Finally, the corpus is applied in several tasks, such as generation of emotion object ontology or retrieval of emotional and moral consequences of actions. 
 2 Related Work  Evaluation often denotes a key issue in semantics- or subjectivity-related tasks. Here we discuss the difﬁculties of evaluating opinionated keyphrase extraction. We present our method to reduce the subjectivity of the task and to alleviate the evaluation process and we also compare the results of human and machine-based evaluation. 
 tiFrameNet, an extension of FrameNet (Baker et al.,  Current work on sentiment analysis is characterized by approaches with a pragmatic focus, which use shallow techniques in the interest of robustness but often rely on ad-hoc creation of data sets and methods. We argue that progress towards deep analysis depends on a) enriching shallow representations with linguistically motivated, rich information, and b) focussing different branches of research and combining ressources to create synergies with related work in NLP. In the paper, we propose SentiFrameNet, an extension to FrameNet, as a novel representation for sentiment analysis that is tailored to these aims. 
According to previous work on pedophile psychology and cyberpedophilia, sentiments and emotions in texts could be a good clue to detect online sexual predation. In this paper, we have suggested a list of high-level features, including sentiment and emotion based ones, for detection of online sexual predation. In particular, since pedophiles are known to be emotionally unstable, we were interested in investigating if emotion-based features could help in their detection. We have used a corpus of predators’ chats with pseudo-victims downloaded from www.perverted-justice.com and two negative datasets of different nature: cybersex logs available online and the NPS chat corpus. Naive Bayes classiﬁcation based on the proposed features achieves accuracies of up to 94% while baseline systems of word and character n-grams can only reach up to 72%. 
We investigate aspects of interoperability between a broad range of common annotation schemes for syntacto-semantic dependencies. With the practical goal of making the LinGO Redwoods Treebank accessible to broader usage, we contrast seven distinct annotation schemes of functor–argument structure, both in terms of syntactic and semantic relations. Drawing examples from a multi-annotated gold standard, we show how abstractly similar information can take quite different forms across frameworks. We further seek to shed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 
In this paper we describe the Prague Markup Language (PML), a generic and open XMLbased format intended to deﬁne format of linguistic resources, mainly annotated corpora. We also provide an overview of existing tools supporting PML, including annotation editors, a corpus query system, software libraries, etc. 
This paper brings a contribution to the ﬁeld of discourse annotation of corpora. Using ANNODIS, a french corpus annotated with discourse relations by naive and expert annotators, we focus on two of them, Elaboration and Entity-Elaboration. These two very frequent relations are (a) often confused by naive annotators (b) difﬁcult to detect automatically as their signalling is poorly studied. We propose to use lexical cohesion to differentiate between them, and show that Elaboration is more cohesive than Entity-Elaboration. We then integrate lexical cohesion cues in a classiﬁcation experiment, obtaining highly satisfying results. 
This paper will introduce a procedure that we call pair annotation after pair programming. We describe initial annotation procedure of the TDB, followed by the inception of the pair annotation idea and how it came to be used in the Turkish Discourse Bank. We discuss the observed benefits and issues encountered during the process, and conclude by discussing the major benefit of pair annotation, namely higher inter-annotator agreement values. 
This paper compares the reference annotation of structured named entities in two corpora with different origins and properties. It addresses two questions linked to such a comparison. On the one hand, what speciﬁc issues were raised by reusing the same annotation scheme on a corpus that differs from the ﬁrst in terms of media and that predates it by more than a century? On the other hand, what contrasts were observed in the resulting annotations across the two corpora?  After a presentation of related work (Section 2), including the deﬁnition of structured named entities, this paper presents the construction of a new annotated corpus of old newspapers (Section 3). The main goal of the paper is to report the comparison of structured named entity annotation in two contrasting press corpora: the pre-existing broadcast news corpus and this new corpus of old newspapers. This comparison is performed at two levels: the annotation process itself (Section 4.1) and the annotation results (Section 4.2). 2 Related Work  
We present two approaches (rule-based and statistical) for automatically annotating intra-chunk dependencies in Hindi. The intra-chunk dependencies are added to the dependency trees for Hindi which are already annotated with inter-chunk dependencies. Thus, the intra-chunk annotator finally provides a fully parsed dependency tree for a Hindi sentence. In this paper, we first describe the guidelines for marking intra-chunk dependency relations. Although the guidelines are for Hindi, they can easily be extended to other Indian languages. These guidelines are used for framing the rules in the rule-based approach. For the statistical approach, we use MaltParser, a data driven parser. A part of the ICON 2010 tools contest data for Hindi is used for training and testing the MaltParser. The same set is used for testing the rule-based approach. 
This paper describes a comprehensive standard for resource description developed within ISO TC37 SC4). The standard is instantiated in a system of XML headers that accompany data and annotation documents represented using the the Linguistic Annotation Framework’s Graph Annotation Format (GrAF) (Ide and Suderman, 2007; Ide and Suderman, Submitted). It provides mechanisms for describing the organization of the resource, documenting the conventions used in the resource, associating data and annotation documents, and deﬁning and selecting deﬁned portions of the resource and its annotations. It has been designed to accommodate the use of XML technologies for processing, including XPath, XSLT, and, by virtue of the system’s linkage strategy, RDF/OWL, and to accommodate linkage to web-based ontologies and data category registries such as the OLiA ontologies (Chiarcos, 2012) and ISOCat (Marc KempsSnijders and Wright, 2008). 
This paper describes the development of an Indonesian speech recognition web service which complies with two standards: it operates on the Language Grid, ensuring process interoperability, and its output uses the LAF/GrAF format, ensuring data interoperability. It is part of a larger system, currently in development, that aims to collect speech transcriptions via crowdsourcing methods. Its utility is twofold: it exposes a functional speech recognizer to the web, and allows the incremental construction of a large speech corpus.  This paper presents our initial efforts in developing a speech recognition system that utilizes the Language Grid platform (Ishida, 2005) to provide Indonesian speech recognition services accessible through the web and mobile devices in an efficient and practical manner, and support crowdsourcing of speech annotations through an interactive web application. Section 2 will provide an overview of the system, Sections 3 and 4 will discuss related standards, i.e. the Language Grid and the Linguistic Annotation Framework respectively, and Section 5 will present the developed speech recognition service. In Section 6 we briefly discuss the speech transcription crowdsourcing application. 2 System Overview  
This paper explores how and why the Linguistic Annotation Framework might be adapted for compatibility with recent more general proposals for the representation of annotations in the Semantic Web, referred to here as the Open Annotation models. We argue that the adapted model, in addition to being interoperable with other annotations and annotation tools, also resolves some representational limitations and semantic ambiguity of the original data model. 
In this work, we present the data structures that were developed for the Rhapsodie project, an intonosyntactic annotation project of spoken French. Phoneticians and syntacticians work on different base units: a time aligned sound file for the former, and a partially ordered list of tokens for the latter. The alignment between the sound-file and the tokens is partial and nontrivial. We propose to encode this data with a small set of interconnected structures: lists, constituent trees, and directed acyclic graphs (DAGs). Our query language remains simple, similar to the Annis Query language, as the precedence and including relations are handled in accordance with the requested objects and their type of alignment: The order between prosodic units is timebased, whereas the order between syntactic units is lexeme-based. 
The broad goal of this study is to further the understanding of doctors’ diagnostic styles and reasoning processes. We analyze and validate methods for annotating verbal diagnostic narratives collected together with eyemovement data. The long-term goal is to understand the cognitive reasoning and decisionmaking processes of medical experts, which could be useful for clinical information systems. The linguistic data set consists of transcribed recordings. Dermatologists were shown images of cutaneous conditions and asked to explain their observations aloud as they proceeded towards a diagnosis. We report on two linked annotation studies. In the ﬁrst study, a subset of narratives were annotated by experts using a unique annotation scheme developed speciﬁcally for capturing decision-making components in the diagnostic process of dermatologists. We analyze annotator agreement as well as compare this annotation scheme to semantic types of the Uniﬁed Medical Language System as validation. In the second study, we explore the annotation of diagnostic correctness in the narratives at three relevant diagnostic steps, and we also explore the relationship between the two annotation schemes. 
In this paper we present the results of a heuristic usability evaluation of three annotation tools (GATE, MMAX2 and UAM CorpusTool). We describe typical usability problems from two categories: (1) general problems, which arise from a disregard of established best practices and guidelines for user interface (UI) design, and (2) more speciﬁc problems, which are closely related to the domain of linguistic annotation. By discussing the domainspeciﬁc problems we hope to raise tool developers’ awareness for potential problem areas. A set of 28 design recommendations, which describe generic solutions for the identiﬁed problems, points toward a structured and systematic collection of usability patterns for linguistic annotation tools. 
 used in conjunction with the Sketch Engine (Kil-  We show how the lexicographic task of ﬁnding informative and diverse example sentences can be cast as a search result diversiﬁcation problem, where an objective based on relevance and diversity is maximized. This problem has been studied intensively in the information retrieval community during recent  garriff et al., 2004) in several lexicographic tasks. GDEX uses a set of rules of thumb designed to address the relevance issue for lexicographers: example sentences should be medium-short (but not too short) and avoid rare words and syntactic constructions, and the search term should preferably be in the main clause.  years, and efﬁcient algorithms have been devised. We ﬁnally show how the approach has been implemented in a lexicographic project, and describe the relevance and diversity functions used in that context.  In this paper, we argue that the two goals of representativeness and diversity can be cast as a search result diversiﬁcation problem. The task of diversiﬁcation has seen much recent interest in the information retrieval community (Gollapudi and Sharma,  
 concentrate on ﬁnding inconsistencies in linguistic  The paper describes a method for measuring compatibility between two levels of manual corpus annotation: shallow and deep. The proposed measures translate into a procedure for ﬁnding annotation errors at either level.  annotations: if similar (in some well-deﬁned way) inputs receive different annotations, the less frequent of these annotations is suspected of being erroneous. Experiments (reported elsewhere) performed on a Polish treebank show that such methods reach reasonable precision but lack in recall.  
This paper proposes schematic changes to the TempEval framework that target the temporal vagueness problem. Speciﬁcally, two elements of vagueness are singled out for special treatment: vague time expressions, and explicit/implicit temporal modiﬁcation of events. As proof of concept, an annotation experiment on explicit/implicit modiﬁcation is conducted on Amazon’s Mechanical Turk. Results show that the quality of a considerable segment of the annotation is comparable to annotation obtained in the traditional doubleblind setting, only with higher coverage. This approach offers additional ﬂexibility in how the temporal annotation data can be used. 
 essays; is divided according to student level (be-  We aim to sufﬁciently deﬁne annotation for post-positional particle errors in L2 Korean writing, so that future work on automatic particle error detection can make progress. To achieve this goal, we outline the linguistic properties of Korean particles in learner data. Given the agglutinative nature of Korean and the range of functions of particles, this annotation effort involves issues such as deﬁning the tokens and target forms.  ginner, intermediate) and student background (heritage, non-heritage);1 and is hand-annotated for particle errors. This corpus, however, does not contain gold standard segmentation, requiring users to semiautomatically determine particle boundaries. In addition to segmentation, to make particle error detection a widespread task where real systems are developed, we need to outline the scope of particle errors (e.g., error types, inﬂuence of other errors) and incorporate insights into an annotation scheme.  Selecting the correct particle in Korean is compli-  
Developing content extraction methods for Humanities domains raises a number of challenges, from the abundance of non-standard entity types to their complexity to the scarcity of data. Close collaboration with Humanities scholars is essential to address these challenges. We discuss an annotation schema for Archaeological texts developed in collaboration with domain experts. Its development required a number of iterations to make sure all the most important entity types were included, as well as addressing challenges including a domain-speciﬁc handling of temporal expressions, and the existence of many systematic types of ambiguity. 
This paper describes an annotation scheme for expressions of preferences in on-line chats concerning bargaining negotiations in the online version of the competitive game Settlers of Catan. 
Morphological segmentation data for the METU-Sabancı Turkish Treebank is provided in this paper. The generalized lexical forms of the morphemes which the treebank previously lacked are added to the treebank. This data maybe used to train POS-taggers that use stemmer outputs to map these lexical forms to morphological tags.  bank data in a more complete form for further studies. The sentence in (1) is taken from the treebank and is shown with the intended representation given in Figure 1. The LEM ﬁeld contains the lemma information whereas the MORPH ﬁeld contains the lexical representations of the morphemes involved in forming the word. For the explanations of the rest of the ﬁelds the reader is referred to Atalay et al. (2003) and Oﬂazer et al. (2003).  
AlvisAE is a text annotation editor aimed at knowledge acquisition projects. An expressive annotation data model allows AlvisAE to support various knowledge acquisition tasks like construction gold standard corpus, ontology population and assisted reading. Collaboration is achieved through a workﬂow of tasks that emulates common practices (e.g. automatic pre-annotation, adjudication). It is implemented as a Web application requiring no installation by the end-user, thus facilitating the participation of domain experts. AlvisAE is used in several knowledge acquisition projects in the domains of biology and crop science. 
 tematic incorporation of contributions from a com-  munity rather than from a small group.  This paper presents a community-sourcing annotation framework, which is designed to implement a “marketplace model” of annotation tasks and annotators, with an emphasis on efﬁcient management of community of potential annotators. As a position paper, it explains the motivation and the design concept of the framework, with a prototype implementation.  In this work, we propose a community-sourcing annotation framework (CSAF, hereafter) which deﬁnes the components and protocol of a computer system to enable community-sourcing annotation. It is similar to MTurk to some extent in its concept, but it is more speciﬁcally designed for corpus annotation tasks, particularly for those which require special expertise from annotators, e.g., domain knowl-  
In this paper we describe a currently underway treebanking effort for Urdu-a South Asian language. The treebank is built from a newspaper corpus and uses a Karaka based grammatical framework inspired by Paninian grammatical theory. Thus far 3366 sentences (0.1M words) have been annotated with the linguistic information at morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) levels. This work also aims to evaluate the correctness or reliability of this manual annotated dependency treebank. Evaluation is done by measuring the inter-annotator agreement on a manually annotated data set of 196 sentences (5600 words) annotated by two annotators. We present the qualitative analysis of the agreement statistics and identify the possible reasons for the disagreement between the annotators. We also show the syntactic annotation of some constructions speciﬁc to Urdu like Ezaf e and discuss the problem of word segmentation (tokenization). 
 And consequently, downstream NLP applications,  such as question answering or machine translation,  Finding coordinations provides useful information for many NLP endeavors. However, the task has not received much attention in the literature. A major reason for that is that the annotation of major treebanks does not reliably annotate coordination. This makes it virtually impossible to detect coordinations in  would beneﬁt as well. However, since linguistic frameworks in general are challenged by the diverse phenomena of coordination, a consistent annotation of coordinate structures, clearly marking the phenomenon as such as well as its scope, is a difﬁcult enterprise. Conse-  which two conjuncts are separated by punctuation rather than by a coordinating conjunction. In this paper, we present an annotation scheme for the Penn Treebank which introduces a distinction between coordinating from non-coordinating punctuation. We discuss the  quently, this makes the detection of conjuncts and their boundaries a highly non-trivial task. Nevertheless, an exact detection of coordination scopes is necessary for improving parsing approaches to this phenomenon.  general annotation guidelines as well as problematic cases. Eventually, we show that this additional annotation allows the retrieval of a considerable number of coordinate structures beyond the ones having a coordinating conjunction.  A ﬁrst step in the detection of the single conjuncts of a coordinate structure is a reliable detection of the presence of a coordinate structure as such and of the boundaries between its conjuncts. One highly predictive marker for the detection of coordinate structures is the presence of a coordinating  conjunction such as and, or, neither...nor,  
 In example (1), the particle ka indicates subjecthood and ul refers to objecthood.1  We present a novel scheme for annotating the realization and ellipsis of Korean particles. Annotated data include 100,128 Ecel (a spacebased word unit) in spoken and written corpora composed of four different genres in order to evaluate how register variation contributes to Korean particle ellipsis. Identifying the grammatical functions of particles and zero particles is critical for deriving a valid linguistic analysis of argument realization, semantic and discourse analysis, and computational processes of parsing. The primary challenge is to design a reliable scheme for classifying particles while making a clear distinction between ellipsis and non-occurrences. We determine in detail issues involving particle annotation and present solutions. In addition to providing a statistical analysis and outcomes, we briefly discuss linguistic factors involving particle ellipsis.  
We posit that determining the social goals and intentions of dialogue participants is crucial for understanding discourse taking place on social media. In particular, we examine the social goals of being collegial and being adversarial. Through our early experimentation, we found that speech and dialogue acts are not able to capture the complexities and nuances of the social intentions of discourse participants. Therefore, we introduce a set of 9 social acts speciﬁcally designed to capture intentions related to being collegial and being adversarial. Social acts are pragmatic speech acts that signal a dialogue participant’s social intentions. We annotate social acts in discourses communicated in English and Chinese taken from Wikipedia talk pages, public forums, and chat transcripts. Our results show that social acts can be reliably understood by annotators with a good level of inter-rater agreement. 
This contribution focuses on multimodal interaction techniques for a mobile communication and assistance system on a robot platform. The system comprises of acoustic, visual and haptic input modalities. Feedback is given to the user by a graphical user interface and a speech synthesis system. By this, multimodal and natural communication with the robot system is possible. 
This paper discusses the significance of the multimodal interaction in virtual environments (VE) and the criticalities involved in integration and coordination between modes during interaction. Also, we present an architecture and design of the integration mechanism with respect to information access in second language learning. In this connection, we have conducted an experiential study on speech inputs to understand how far users’ experience of information can be considered to be supportive to this architecture. 
The VASA project develops a multimodal assistive system mediated by a virtual agent that is intended to foster autonomy of communication and activity management in older people and people with disabilities. Assistive systems intended for these user groups have to take their individual vulnerabilities into account. A variety of psychic, emotional as well as behavioral conditions can manifest at the same time. Systems that fail to take them into account might not only fail at joint tasks, but also risk damage to their interlocutors. We identify important conditions and disorders and analyze their immediate consequences for the design of careful assistive systems. 
In this paper we describe our recent and future research on multimodal interaction in an Ambient Assisted Living Lab. Our work combines two interaction modes, speech and gesture, for multiple device control in Ambient Assisted Living environments. We conducted a user study concerning multimodal interaction between participants and an intelligent wheelchair in a smart home environment. Important empirical data were collected through the user study, which encouraged further developments on our multimodal interactive system for Ambient Assisted Living environments. 
Due to the demographic changes, support by means of assistive systems will become inevitable for home care and in nursing homes. Robot systems are promising solutions but their value has to be acknowledged by the patients and the care personnel. Natural and intuitive human-machine interfaces are an essential feature to achieve acceptance of the users. Therefore, automatic speech recognition (ASR) is a promising modality for such assistive devices. However, noises produced during movement of robots can degrade the ASR performances. This work focuses on noise reduction by a non-negative matrix factorization (NMF) approach to efﬁciently suppress non stationary noise produced by the sensors of an assisting robot system.  usually picked up by microphones mounted on the robot. In real-world scenarios not only the desired signal part is picked up by these microphones as presented in Figure 1.  frequency  spescpterucmtrusmpeoeucthput  4  0  -5 3 -10 2 -15 
This paper introduces research within the ALADIN project, which aims to develop an assistive vocal interface for people with a physical impairment. In contrast to existing approaches, the vocal interface is self-learning, which means it can be used with any language, dialect, vocabulary and grammar. This paper describes the overall learning framework, and the two components that will provide vocabulary learning and grammar induction. In addition, the paper describes encouraging results of early implementations of these vocabulary and grammar learning components, applied to recorded sessions of a vocally guided card game, Patience. 
Different Bengali TTS systems are already available on a resourceful platform such as a personal computer. However, porting these systems to a resource limited device such as a mobile phone is not an easy task. Practical aspects including application size and processing time have to be concerned. This paper describes the implementation of a Bengali speech synthesizer on a mobile device. For speech generation we used Epoch Synchronous Non Overlap Add (ESNOLA) based concatenative speech synthesis technique which uses the partnemes as the smallest signal units for concatenations. 
This paper investigates the impact on French dependency parsing of lexical generalization methods beyond lemmatization and morphological analysis. A distributional thesaurus is created from a large text corpus and used for distributional clustering and WordNet automatic sense ranking. The standard approach for lexical generalization in parsing is to map a word to a single generalized class, either replacing the word with the class or adding a new feature for the class. We use a richer framework that allows for probabilistic generalization, with a word represented as a probability distribution over a space of generalized classes: lemmas, clusters, or synsets. Probabilistic lexical information is introduced into parser feature vectors by modifying the weights of lexical features. We obtain improvements in parsing accuracy with some lexical generalization conﬁgurations in experiments run on the French Treebank and two out-of-domain treebanks, with slightly better performance for the probabilistic lexical generalization approach compared to the standard single-mapping approach. 
In the last decade, substantial progress has been made in the induction of semantic relations from raw text, especially of hypernymy and meronymy in the English language and in the classiﬁcation of noun-noun relations in compounds or other contexts. We investigate the question of learning qualia-like semantic relations that cross part-of-speech boundaries for German, by ﬁrst introducing a hand-tagged dataset of associated noun-verb pairs for this task, and then provide classiﬁcation results using a general framework for supervised classiﬁcation of lexical relations. 
We introduce a list of Arabic multiword expressions (MWE) collected from various dictionaries. The MWEs are grouped based on their syntactic type. Every constituent word in the expressions is manually annotated with its full context-sensitive morphological analysis. Some of the expressions contain semantic variables as place holders for words that play the same semantic role. In addition, we have automatically annotated a large corpus of Arabic text using a pattern-matching algorithm that considers some morphosyntactic features as expressed by a highly inflected language, such as Arabic. A sample part of the corpus is manually evaluated and the results are reported in this paper. 
This paper introduces a novel unsupervised approach to semantic role induction that uses a generative Bayesian model. To the best of our knowledge, it is the ﬁrst model that jointly clusters syntactic verbs arguments into semantic roles, and also creates verbs classes according to the syntactic frames accepted by the verbs. The model is evaluated on French and English, outperforming, in both cases, a strong baseline. On English, it achieves results comparable to state-of-the-art unsupervised approaches to semantic role induction. 
In many morphologically rich languages, conceptually independent morphemes are glued together to form a new word (a compound) with a meaning that is often at least in part predictable from the meanings of the contributing morphemes. Assuming that most compounds express a subconcept of exactly one sense of its nominal head, we use compounds as a higher-quality alternative to simply using general second-order collocate terms in the task of word sense discrimination. We evaluate our approach using lexical entries from the German wordnet GermaNet (Henrich and Hinrichs, 2010). 
The paper presents a novel approach to extracting dependency information in morphologically rich languages using co-occurrence statistics based not only on lexical forms (as in previously described collocation-based methods), but also on morphosyntactic and wordnet-derived semantic properties of words. Statistics generated from a corpus annotated only at the morphosyntactic level are used as features in a Machine Learning classiﬁer which is able to detect which heads of groups found by a shallow parser are likely to be connected by an edge in the complete parse tree. The approach reaches the precision of 89% and the recall of 65%, with an extra 6% recall, if only words present in the wordnet are considered. 
This paper presents the results of a set of preliminary experiments combining two knowledge-based partial dependency analyzers with two statistical parsers, applied to the Basque Dependency Treebank. The general idea will be to apply a stacked scheme where the output of the rule-based partial parsers will be given as input to MaltParser and MST, two state of the art statistical parsers. The results show a modest improvement over the baseline, although they also present interesting lines for further research. 1. Introduction In this paper we present a set of preliminary experiments on the combination of two knowledge-based partial syntactic analyzers with two state of the art data-driven statistical parsers. The experiments have been performed on the Basque Dependency Treebank (Aduriz et al., 2003). In the last years, many attempts have been performed trying to combine different parsers (Surdeanu and Manning, 2010), with significant improvements over the best individual parser’s baseline. The two most successful approaches have been stacking (Martins et al., 2008) and voting (Sagae and Lavie, 2006, Nivre and McDonald, 2008, McDonald and Nivre, 2011). In this paper we will experiment the use of the stacking technique, giving the tags obtained by the rule-  based syntactic partial parsers as input to the statistical parsers. Morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like English does not reach similar performance levels in languages like Basque, Greek or Turkish (Nivre et al., 2007a). As it was successfully done on part of speech (POS) tagging, where the use of rule-based POS taggers (Tapanainen and Voutilainen, 1994) or a combination of a rulebased POS tagger with a statistical one (Aduriz et al., 1997, Ezeiza et al., 1998) outperformed purely statistical taggers, we think that exploring the combination of knowledge-based and data-driven systems in syntactic processing can be an interesting line of research. Most of the experiments on combined parsers have relied on different types of statistical parsers (Sagae and Lavie, 2006, Martins et al., 2008, McDonald and Nivre, 2011), trained on an automatically annotated treebank. Yeh (2000) used the output of several baseline diverse parsers to increase the performance of a second transformation-based parser. In our work we will study the use of two partial rule-based syntactic analyzers together with two data-driven parsers: • A rule-based chunker (Aduriz et al., 2004) that marks the beginning and end of noun phrases, postpositional phrases and verb chains, in the IOB (Inside/ Outside/Beginning of a chunk) style. • A shallow dependency relation annotator (Aranzabe et al., 2004), which tries to detect dependency relations by assigning a  48  Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 48–54, Jeju, Republic of Korea, 12 July 2012. c 2012 Association for Computational Linguistics  ncsubj ncsubj ncmod  ccomp_obj auxmod  auxmod  Gizonak The-man N-ERG-S B-NP &NCSUBJ>  mutil boy N B-NP &NCSUBJ>  handia tall-the ADJ-ABS-S I-NP $<NCMOD  etorri come V B-VP $CCOMP_OBJ>  dela has+he+that AUXV+S+COMPL I-NP &<AUXMOD  esan tell V B-VP &MAINV  du . he+did+it AUXV I-VP &<AUXMOD  Figure 1. Dependency tree for the sentence Gizonak mutil handia etorri dela esan du (the man told that the tall boy has come). The two last lines show the tags assigned by the rule-based chunker and the rule-based dependency analyzer, respectively. (V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG = ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP, &MAINV = main verb, &<AUXMOD = verbal auxiliary modifier).  set of predefined tags to each word, where each tag gives both the name of a dependency relation (e.g. subject) together with the direction of its head (left or right). • We will use two statistical dependency parsers, MaltParser (Nivre et al., 2007b) and MST (McDonald et al, 2005). In the rest of this paper, section 2 will first present the corpus and the different parsers we will combine, followed by the experimental results in section 3, and the main conclusions of the work. 2. Resources This section will describe the main resources that have been used in the experiments. First, subsection 2.1 will describe the Basque Dependency Treebank, and then subsection 2.2 will explain the main details of the analyzers that have been employed. The analyzers are a rulebased chunker, a rule-based shallow dependency parser and two state of the art data-driven dependency parsers, MaltParser and MST. 2.1 Corpora Our work will make use the second version of the Basque dependency Treebank (BDT II, Aduriz et al., 2003), containing 150,000 tokens (11,225 sentences). Figure 1 presents an example of a syntactically annotated sentence. Each word contains its form, lemma, category or coarse part of speech (CPOS), POS, morphosyntactic features such as case, number of subordinate relations, and the dependency relation (headword + dependency). The information in figure 1 has been simplified due to space reasons, as typically each word  contains many morphosyntactic features (case, number, type of subordinated sentence, ...), which are relevant for parsing. The last two lines of the sentence in figure 1 do not properly correspond to the treebank, but are the result of the rule-based partial syntactic analyzers (see subsection 2.2). For evaluation, we divided the treebank in three sets, corresponding to training, development, and test (80%, 10%, and 10%, respectively). The experiments were performed on the development set, leaving the best system for the final test. 2.2 Analyzers This subsection will present the four types of analyzers that have been used. The rule-based analyzers are based on the Contraint Grammar (CG) formalism (Karlsson et al., 1995), based on the assignment of morphosyntactic tags to words using a formalism that has the capabilities of finite state automata or regular expressions, by means of a set of rules that examine mainly local contexts of words to determine the correct tag assignment. The rule-based chunker (RBC henceforth, Aranzabe et al., 2009) uses 560 rules, where 479 of the rules deal with noun phrases and the rest with verb phrases. The chunker delimits the chunks with three tags, using a standard IOB marking style (see figure 1). The first one is to mark the beginning of the phrase (B-VP if it is a verb phrase and B-NP whether it's a noun phrase) and the other one to mark the continuation of the phrase (I-NP or I-VP, meaning that the word is inside an NP or VP). The last tag marks words that are outside a chunk. The evaluation of the chunker on the BDT gave a result of 87% precision and 85% recall over all chunks. We must take into account that this evaluation was  49  performed on the gold POS tags, rather than on automatically assigned POS tasks, as in the present experiment. For that reason, the results can serve as an upper bound on the real results. The rule-based dependency analyzer (RBDA, Aranzabe et al., 2004) uses a set of 505 CG rules that try to assign dependency relations to wordforms. As the CG formalism only allows the assignment of tags, the rules only aim at marking the name of the dependency relation together with the direction of the head (left or right). For example, this analyzer assigns tags of the form &NCSUBJ> (see figure 1), meaning that the corresponding wordform is a non-clausal syntactic subject and that its head is situated to its right (the “>” or “<” symbols mark the direction of the head). This means that the result of this analysis is on the one hand a partial analysis and, on the other hand, it does not define a dependency tree, and can also be seen as a set of constraints on the shape of the tree. The system was evaluated on the BDT, obtaining f-scores between 90% for the auxmod dependency relation between the auxiliary and the main verb and 52% for the subject dependency relation, giving a (macro) average of 65%. Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al., 2007b) and MST Parser (McDonald et al., 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and treebanks (McDonald and Nivre, 2007). MaltParser (Nivre, 2006) is a representative of local, greedy, transition-based dependency parsing models, where the parser obtains deterministically a dependency tree in a single pass over the input using two data structures: a stack of partially analyzed items and the remaining input sequence. To determine the best action at each step, the parser uses history-based feature models and discriminative machine learning. The learning configuration can include any kind of information (such as word-form, lemma, category, subcategory or morphological features). Several variants of the parser have been implemented, and we will use one of its standard versions (MaltParser version 1.4). In our experiments, we will use the StackLazy algorithm with the liblinear classifier. The MST Parser can be considered a representative of global, exhaustive graph-based 50  parsing (McDonald et al., 2005, 2006). This algorithm finds the highest scoring directed spanning tree in a dependency graph forming a valid dependency tree. To learn arc scores, it uses large-margin structured learning algorithms, which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, and not just over single arc attachments. This is in contrast to the local but richer contexts used by transition-based parsers. We use the freely available version of MSTParser1. In the following experiments we will make use of the second order non-projective algorithm.  3. Experiments  We will experiment the effect of using the output  of the knowledge-based analyzers as input to the  data-driven parsers in a stacked learning scheme.  Figure 1 shows how the two last lines of the  example sentence contain the tags assigned by the  rule-based chunker (B-NP, I-NP, B-VP and I-VP)  and the rule-based partial dependency analyzer  (&NCSUBJ, &<NCMOD, &<AUXMOD,  &CCOMP_OBJ and &MAINV) .  The first step consisted in applying the complete  set of text processing tools for Basque, including:  • Morphological analysis. In Basque, each word can receive multiple affixes, as each  lemma can generate thousands of word-  forms by means of morphological  properties, such as case, number, tense, or  different types of subordination for verbs.  Consequently, the morphological analyzer  for Basque (Aduriz et al. 2000) gives a  high ambiguity. If only categorial (POS)  ambiguity is taken into account, there is an  average of 1.55 interpretations per word-  form, which rises to 2.65 when the full  morphosyntactic information is taken into  account, giving an overall 64% of  ambiguous word-forms.  • Morphological  disambiguation.  Disambiguating the output of  morphological analysis, in order to obtain  a single interpretation for each word-form,  
Although parsing performances have greatly improved in the last years, grammar inference from treebanks for morphologically rich languages, especially from small treebanks, is still a challenging task. In this paper we investigate how state-of-the-art parsing performances can be achieved on Spanish, a language with a rich verbal morphology, with a non-lexicalized parser trained on a treebank containing only around 2,800 trees. We rely on accurate part-of-speech tagging and datadriven lemmatization to provide parsing models able to cope lexical data sparseness. Providing state-of-the-art results on Spanish, our methodology is applicable to other languages with high level of inﬂection. 
Deep linguistic grammars are able to provide rich and highly complex grammatical representations of sentences, capturing, for instance, long-distance dependencies and returning a semantic representation. These grammars lack robustness in the sense that they do not gracefully handle words missing from their lexicon. Several approaches have been explored to handle this problem, many of which consist in pre-annotating the input to the grammar with shallow processing machine-learning tools. Most of these tools, however, use features based on a ﬁxed window of context, such as n-grams. We investigate whether the use of features that encode discrete structures, namely grammatical dependencies, can improve the performance of a machine learning classiﬁer that assigns deep lexical types. In this paper we report on the design and evaluation of this classiﬁer. 
Dependency parsing has been shown to improve NLP systems in certain languages and in many cases helps achieve state of the art results in NLP applications, in particular applications for free word order languages. Morphologically rich languages are often short on training data or require much higher amounts of training data due to the increased size of their lexicon. This paper examines a new approach for addressing morphologically rich languages with little training data to start. Using Tamil as our test language, we create 9 dependency parse models with a limited amount of training data. Using these models we train an SVM classiﬁer using only the model agreements as features. We use this SVM classiﬁer on an edge by edge decision to form an ensemble parse tree. Using only model agreements as features allows this method to remain language independent and applicable to a wide range of morphologically rich languages. We show a statistically signiﬁcant 5.44% improvement over the average dependency model and a statistically signiﬁcant 0.52% improvement over the best individual system. 
Korean is a morphologically rich language in which grammatical functions are marked by inﬂections and afﬁxes, and they can indicate grammatical relations such as subject, object, predicate, etc. A Korean sentence could be thought as a sequence of eojeols. An eojeol is a word or its variant word form agglutinated with grammatical afﬁxes, and eojeols are separated by white space as in English written texts. Korean treebanks (Choi et al., 1994; Han et al., 2002; Korean Language Institute, 2012) use eojeol as their fundamental unit of analysis, thus representing an eojeol as a prepreterminal phrase inside the constituent tree. This eojeol-based annotating schema introduces various complexity to train the parser, for example an entity represented by a sequence of nouns will be annotated as two or more different noun phrases, depending on the number of spaces used. In this paper, we propose methods to transform eojeol-based Korean treebanks into entity-based Korean treebanks. The methods are applied to Sejong treebank, which is the largest constituent treebank in Korean, and the transformed treebank is used to train and test various probabilistic CFG parsers. The experimental result shows that the proposed transformation methods reduce ambiguity in the training corpus, increasing the overall F1 score up to about 9 %.  tion (Finkel and Manning, 2009), semantic role labeling (Gildea and Jurafsky, 2002), or sentimental analysis (Nasukawa and Yi, 2003). Currently most of the state-of-the-art constituent parsers take statistical parsing approach (Klein and Manning, 2003; Bikel, 2004; Petrov and Klein, 2007), which use manually annotated syntactic trees to train the probabilistic models of each consituents. Even though there exist manually annotated Korean treebank corpora such as Sejong Treebank (Korean Language Institute, 2012), very few research projects about the Korean parser, especially using phrase structure grammars have been conducted. In this paper, we aim to transform the treebank so that it could be better used as training data for the alreadyexisting English constituent parsers. Most of Korean treebank corpora use eojeols as their fundamental unit of analysis. An eojeol is a word or its variant word form agglutinated with grammatical afﬁxes, and eojeols are separated by white space as in English written texts (Choi et al., 2011). Figure 1 is one of the example constituent tree from the Sejong Treebank. As can be observed, an eojeol is always determined as a prepreterminal phrase 1. But this kind of bracketing guideline could cause ambiguities to the existing algorithms for parsing English, because: (1) English does not have the concept of “eojeol”, and (2) an eojeol can contain two or more morphemes with different grammatical roles. For example, Korean case par-  
We present an architecture for parsing in two steps. A phrase-structure parser builds for each sentence an n-best list of analyses which are converted to dependency trees. These dependency structures are then rescored by a discriminative reranker. Our method is language agnostic and enables the incorporation of additional information which are useful for the choice of the best parse candidate. We test our approach on the the Penn Treebank and the French Treebank. Evaluation shows a signiﬁcative improvement on different parse metrics. 
Several approaches have been proposed for the automatic acquisition of multiword expressions from corpora. However, there is no agreement about which of them presents the best cost-beneﬁt ratio, as they have been evaluated on distinct datasets and/or languages. To address this issue, we investigate these techniques analysing the following dimensions: expression type (compound nouns, phrasal verbs), language (English, French) and corpus size. Results show that these techniques tend to extract similar candidate lists with high recall (∼ 80%) for nominals and high precision (∼ 70%) for verbals. The use of association measures for candidate ﬁltering is useful but some of them are more onerous and not signiﬁcantly better than raw counts. We ﬁnish with an evaluation of ﬂexibility and an indication of which technique is recommended for each languagetype-size context. 
In my thesis I propose a data-oriented study on how social power relations between participants manifest in the language and structure of online written dialogs. I propose that there are different types of power relations and they are different in the ways they are expressed and revealed in dialog and across different languages, genres and domains. So far, I have deﬁned four types of power and annotated them in corporate email threads in English and found support that they in fact manifest differently in the threads. Using dialog and language features, I have built a system to predict participants possessing these types of power within email threads. I intend to extend this system to other languages, genres and domains and to improve it’s performance using deeper linguistic analysis. 
In sentiment classification, unlabeled user reviews are often free to collect for new products, while sentiment labels are rare. In this case, active learning is often applied to build a high-quality classifier with as small amount of labeled instances as possible. However, when the labeled instances are insufficient, the performance of active learning is limited. In this paper, we aim at enhancing active learning by employing the labeled reviews from a different but related (source) domain. We propose a framework Active Vector Rotation (AVR), which adaptively utilizes the source domain data in the active learning procedure. Thus, AVR gets benefits from source domain when it is helpful, and avoids the negative affects when it is harmful. Extensive experiments on toy data and review texts show our success, compared with other state-of-theart active learning approaches, as well as approaches with domain adaptation. 
This paper describes a query classiﬁcation system for a specialized domain. We take as a case study queries asked to a search engine of an art, cultural and history library and classify them against the library cataloguing categories. We show how click-through links, i.e., the links that a user clicks after submitting a query, can be exploited for extracting information useful to enrich the query as well as for creating the training set for a machine learning based classiﬁer. Moreover, we show how Topic Model can be exploited to further enrich the query with hidden topics induced from the library meta-data. The experimental evaluations show that this system considerably outperforms a matching and ranking classiﬁcation approach, where queries (and categories) were also enriched with similar information. 
This work presents a Text Segmentation algorithm called TopicTiling. This algorithm is based on the well-known TextTiling algorithm, and segments documents using the Latent Dirichlet Allocation (LDA) topic model. We show that using the mode topic ID assigned during the inference method of LDA, used to annotate unseen documents, improves performance by stabilizing the obtained topics. We show signiﬁcant improvements over state of the art segmentation algorithms on two standard datasets. As an additional beneﬁt, TopicTiling performs the segmentation in linear time and thus is computationally less expensive than other LDA-based segmentation methods. 
When porting parsers to a new domain, many of the errors are related to wrong attachment of out-of-vocabulary words. Since there is no available annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domainspecific word classes. Our method uses Latent Dirichlet Allocations (LDA) to learn a domain-specific Selectional Preference model in the target domain using un-annotated data. The model provides features that model the affinities among pairs of words in the domain. To incorporate these new features in the parsing model, we adopt the co-training approach and retrain the parser with the selectional preferences features. We apply this method for adapting Easy First, a fast nondirectional parser trained on WSJ, to the biomedical domain (Genia Treebank). The Selectional Preference features reduce error by 4.5% over the co-training baseline. 
This paper presents recent work on a new method to automatically extract finegrained duration information for common verbs using a large corpus of Twitter tweets. Regular expressions were used to extract verbs and durations from each tweet in a corpus of more than 14 million tweets with 90.38% precision covering 486 verb lemmas. Descriptive statistics for each verb lemma were found as well as the most typical fine-grained duration measure. Mean durations were compared with previous work by Gusev et al. (2011) and it was found that there is a small positive correlation. 
The current debate regarding the data structure necessary to represent discourse structure, specifically whether tree-structure is sufficient to represent discourse structure or not, is mainly focused on written text. This paper reviews some of the major claims about the structure in discourse and proposes an investigation of discourse structure for simultaneous spoken Turkish by focusing on tree-violations and exploring ways to explain them away by non-structural means. 
This paper presents an open and ﬂexible methodological framework for the automatic acquisition of multiword expressions (MWEs) from monolingual textual corpora. This research is motivated by the importance of MWEs for NLP applications. After brieﬂy presenting the modules of the framework, the paper reports extrinsic evaluation results considering two applications: computer-aided lexicography and statistical machine translation. Both applications can beneﬁt from automatic MWE acquisition and the expressions acquired automatically from corpora can both speed up and improve their quality. The promising results of previous and ongoing experiments encourage further investigation about the optimal way to integrate MWE treatment into these and many other applications.  SRC I paid my poor parents a visit MT J’ai payé mes pauvres parents une visite REF J’ai rendu visite à mes pauvres parents SRC Students pay an arm and a leg to park on campus MT Les étudiants paient un bras et une jambe pour se garer sur le campus REF Les étudiants paient les yeux de la tête pour se garer sur le campus SRC It shares the translation-invariance and homo- geneity properties with the central moment MT Il partage la traduction-invariance et propriétés d’homogénéité avec le moment central REF Il partage les propriétés d’invariance par trans- lation et d’homogénéité avec le moment central Table 1: Examples of SMT errors due to MWEs.  
Automatically constructing knowledge bases from online resources has become a crucial task in many research areas. Most existing knowledge bases are built from English resources, while few efforts have been made for other languages. Building knowledge bases for Chinese is of great importance on its own right. However, simply adapting existing tools from English to Chinese yields inferior results.In this paper, we propose to create Chinese knowledge bases from online resources with less human involvement.This project will be formulated in a self-supervised framework which requires little manual work to extract knowledge facts from online encyclopedia resources in a probabilistic view.In addition, this framework will be able to update the constructed knowledge base with knowledge facts extracted from up-to-date newswire.Currently, we have obtained encouraging results in our pilot experiments that extracting knowledge facts from infoboxes can achieve a high accuracy of around 95%, which will be then used as training data for the extraction of plain webpages. 
The ACL Anthology Network (AAN)1 is a comprehensive manually curated networked database of citations and collaborations in the ﬁeld of Computational Linguistics. Each citation edge in AAN is associated with one or more citing sentences. A citing sentence is one that appears in a scientiﬁc article and contains an explicit reference to another article. In this paper, we shed the light on the usefulness of AAN citing sentences for understanding research trends and summarizing previous discoveries and contributions. We also propose and motivate several different uses and applications of citing sentences. 
We develop a people-centered computational history of science that tracks authors over topics and apply it to the history of computational linguistics. We present four ﬁndings in this paper. First, we identify the topical subﬁelds authors work on by assigning automatically generated topics to each paper in the ACL Anthology from 1980 to 2008. Next, we identify four distinct research epochs where the pattern of topical overlaps are stable and different from other eras: an early NLP period from 1980 to 1988, the period of US government-sponsored MUC and ATIS evaluations from 1989 to 1994, a transitory period until 2001, and a modern integration period from 2002 onwards. Third, we analyze the ﬂow of authors across topics to discern how some subﬁelds ﬂow into the next, forming different stages of ACL research. We ﬁnd that the government-sponsored bakeoffs brought new researchers to the ﬁeld, and bridged early topics to modern probabilistic approaches. Last, we identify steep increases in author retention during the bakeoff era and the modern era, suggesting two points at which the ﬁeld became more integrated. 
We present a joint probabilistic model of who cites whom in computational linguistics, and also of the words they use to do the citing. The model reveals latent factions, or groups of individuals whom we expect to collaborate more closely within their faction, cite within the faction using language distinct from citation outside the faction, and be largely understandable through the language used when cited from without. We conduct an exploratory data analysis on the ACL Anthology. We extend the model to reveal changes in some authors’ faction memberships over time. 
Studies of gender balance in academic computer science are typically based on statistics on enrollment and graduation. Going beyond these coarse measures of gender participation, we conduct a ﬁne-grained study of gender in the ﬁeld of Natural Language Processing. We use topic models (Latent Dirichlet Allocation) to explore the research topics of men and women in the ACL Anthology Network. We ﬁnd that women publish more on dialog, discourse, and sentiment, while men publish more than women in parsing, formal semantics, and ﬁnite state models. To conduct our study we labeled the gender of authors in the ACL Anthology mostly manually, creating a useful resource for other gender studies. Finally, our study of historical patterns in female participation shows that the proportion of women authors in computational linguistics has been continuously increasing, with approximately a 50% increase in the three decades since 1980. 
The discourse properties of text have long been recognized as critical to language technology, and over the past 40 years, our understanding of and ability to exploit the discourse properties of text has grown in many ways. This essay brieﬂy recounts these developments, the technology they employ, the applications they support, and the new challenges that each subsequent development has raised. We conclude with the challenges faced by our current understanding of discourse, and the applications that meeting these challenges will promote. 
The paper reports on a comparative study of two approaches to extracting deﬁnitional sentences from a corpus of scholarly discourse: one based on bootstrapping lexico-syntactic patterns and another based on deep analysis. Computational Linguistics was used as the target domain and the ACL Anthology as the corpus. Deﬁnitional sentences extracted for a set of well-deﬁned concepts were rated by domain experts. Results show that both methods extract high-quality deﬁnition sentences intended for automated glossary construction. 
Collocation is a well-known linguistic phenomenon which has a long history of research and use. In this study I employ collocation segmentation to extract terms from the large and complex ACL Anthology Reference Corpus, and also brieﬂy research and describe the history of the ACL. The results of the study show that until 1986, the most signiﬁcant terms were related to formal/rule based methods. Starting in 1987, terms related to statistical methods became more important. For instance, language model, similarity measure, text classiﬁcation. In 1990, the terms Penn Treebank, Mutual Information , statistical parsing, bilingual corpus, and dependency tree became the most important, showing that newly released language resources appeared together with many new research areas in computational linguistics. Although Penn Treebank was a signiﬁcant term only temporarily in the early nineties, the corpus is still used by researchers today. The most recent signiﬁcant terms are Bleu score and semantic role labeling. While machine translation as a term is signiﬁcant throughout the ACL ARC corpus, it is not signiﬁcant for any particular time period. This shows that some terms can be signiﬁcant globally while remaining insigniﬁcant at a local level. 
 with plagiarism. For example, IEEE1 and ACM2 both consider the reuse as plagiarism in case of:  With rapidly increasing community, a plethora of conferences related to Natural Language Processing and easy access to their proceedings make it essential to check the integrity and novelty of the new submissions. This study aims to investigate the trends of text reuse in the ACL submissions, if any. We carried a set of analyses on two spans of ﬁve years papers (the past and the present) of ACL using a publicly available text reuse detection application to notice the behaviour. In our study, we found some strong reuse cases which can be an indicator to establish a clear policy to handle text reuse for the upcoming editions of ACL. The results are anonymised. 
The ACL Anthology was revamped in 2012 to its second major version, encompassing faceted navigation, social media use, as well as author- and reader-generated content and comments on published work as part of the revised frontend user interface. At the backend, the Anthology was updated to incorporate its publication records into a database. We describe the ACL Anthology’s previous legacy, redesign and revamp process and technologies, and its resulting functionality. 
The ACL 2012 Contributed Task is a community effort aiming to provide the full ACL Anthology as a high-quality corpus with rich markup, following the TEI P5 guidelines— a new resource dubbed the ACL Anthology Corpus (AAC). The goal of the task is threefold: (a) to provide a shared resource for experimentation on scientiﬁc text; (b) to serve as a basis for advanced search over the ACL Anthology, based on textual content and citations; and, by combining the aforementioned goals, (c) to present a showcase of the beneﬁts of natural language processing to a broader audience. The Contributed Task extends the current Anthology Reference Corpus (ARC) both in size, quality, and by aiming to provide tools that allow the corpus to be automatically extended with new content—be they scanned or born-digital. 
Extracting textual content and document structure from PDF presents a surprisingly (depressingly, to some, in fact) difﬁcult challenge, owing to the purely display-oriented design of the PDF document standard. While a variety of lower-level PDF extraction toolkits exist, none fully support the recovery of original text (in reading order) and relevant structural elements, even for so-called borndigital PDFs, i.e. those prepared electronically using typesetting systems like LATEX, OpenOfﬁce, and the like. This short paper summarizes a new tool for high-quality extraction of text and structure from PDFs, combining state-ofthe-art PDF parsing, font interpretation, layout analysis, and TEI-compliant output of text and logical document markup.† 
We describe how paperXML, a logical document structure markup for scholarly articles, is generated on the basis of OCR tool outputs. PaperXML has been initially developed for the ACL Anthology Searchbench. The main purpose was to robustly provide uniform access to sentences in ACL Anthology papers from the past 46 years, ranging from scanned, typewriter-written conference and workshop proceedings papers, up to recent high-quality typeset, born-digital journal articles, with varying layouts. PaperXML markup includes information on page and paragraph breaks, section headings, footnotes, tables, captions, boldface and italics character styles as well as bibliographic and publication metadata. The role of paperXML in the ACL Contributed Task Rediscovering 50 Years of Discoveries is to serve as fall-back source (1) for older, scanned papers (mostly published before the year 2000), for which born-digital PDF sources are not available, (2) for borndigital PDF papers on which the PDFExtract method failed, (3) for document parts where PDFExtract does not output useful markup such as currently for tables. We sketch transformation of paperXML into the ACL Contributed Task’s TEI P5 XML. 
In this paper we describe our participation in the contributed task at ACL Special workshop 2012. We contribute to the goal of enriching the textual content of ACL Anthology by identifying the citation contexts in a paper and linking them to their corresponding references in the bibliography section. We use Parscit, to process the Bibliography of each paper. Pattern matching heuristics are then used to connect the citations with their references. Furthermore, we prepared a small evaluation dataset, to test the efﬁciency of our method. We achieved 95% precision and 80% recall on this dataset. 
Human assessment is often considered the gold standard in evaluation of translation systems. But in order for the evaluation to be meaningful, the rankings obtained from human assessment must be consistent and repeatable. Recent analysis by Bojar et al. (2011) raised several concerns about the rankings derived from human assessments of English-Czech translation systems in the 2010 Workshop on Machine Translation. We extend their analysis to all of the ranking tasks from 2010 and 2011, and show through an extension of their reasoning that the ranking is naturally cast as an instance of ﬁnding the minimum feedback arc set in a tournament, a wellknown NP-complete problem. All instances of this problem in the workshop data are efﬁciently solvable, but in some cases the rankings it produces are surprisingly different from the ones previously published. This leads to strong caveats and recommendations for both producers and consumers of these rankings. 
A recent paper described a new machine translation evaluation metric, AMBER. This paper describes two changes to AMBER. The first one is incorporation of a new ordering penalty; the second one is the use of the downhill simplex algorithm to tune the weights for the components of AMBER. We tested the impact of the two changes, using data from the WMT metrics task. Each of the changes by itself improved the performance of AMBER, and the two together yielded even greater improvement, which in some cases was more than additive. The new version of AMBER clearly outperforms BLEU in terms of correlation with human judgment. 
This paper describes Stanford University’s submission to the Shared Evaluation Task of WMT 2012. Our proposed metric (SPEDE) computes probabilistic edit distance as predictions of translation quality. We learn weighted edit distance in a probabilistic ﬁnite state machine (pFSM) model, where state transitions correspond to edit operations. While standard edit distance models cannot capture long-distance word swapping or cross alignments, we rectify these shortcomings using a novel pushdown automaton extension of the pFSM model. Our models are trained in a regression framework, and can easily incorporate a rich set of linguistic features. Evaluated on two different prediction tasks across a diverse set of datasets, our methods achieve state-of-the-art correlation with human judgments. 
We describe a submission to the WMT12 Quality Estimation task, including an extensive Machine Learning experimentation. Data were augmented with features from linguistic analysis and statistical features from the SMT search graph. Several Feature Selection algorithms were employed. The Quality Estimation problem was addressed both as a regression task and as a discretised classiﬁcation task, but the latter did not generalise well on the unseen testset. The most successful regression methods had an RMSE of 0.86 and were trained with a feature set given by Correlation-based Feature Selection. Indications that RMSE is not always sufﬁcient for measuring performance were observed.  such as language modelling (Raybaud et al., 2009), language ﬂuency checking (Parton et al., 2011), parsing (Sa´nchez-Martinez, 2011; Avramidis et al., 2011) and decoding statistics (Specia et al., 2009; Avramidis, 2011). The current submission combines such previous observations in a combinatory experimentation on feature sets, feature selection methods and Machine Learning (ML) algorithms. The structure of the submission is as follows: The approach is deﬁned and the methods are described in section 2, including features acquisition, feature selection and learning. Section 3 includes information about the experiment setup whereas the results are discussed in Section 4. 2 Methods  
This paper describes a study on the contribution of linguistically-informed features to the task of quality estimation for machine translation at sentence level. A standard regression algorithm is used to build models using a combination of linguistic and non-linguistic features extracted from the input text and its machine translation. Experiments with EnglishSpanish translations show that linguistic features, although informative on their own, are not yet able to outperform shallower features based on statistics from the input text, its translation and additional corpora. However, further analysis suggests that linguistic information is actually useful but needs to be carefully combined with other features in order to produce better results. 
This is a description of the submissions made by the pattern recognition and human language technology group (PRHLT) of the Universitat Polite`cnica de Vale`ncia to the quality estimation task of the seventh workshop on statistical machine translation (WMT12). We focus on two different issues: how to effectively combine subsequence-level features into sentence-level features, and how to select the most adequate subset of features. Results showed that an adequate selection of a subset of highly discriminative features can improve efﬁciency and performance of the quality estimation system. 
This paper describes the features and the machine learning methods used by Dublin City University (DCU) and SYMANTEC for the WMT 2012 quality estimation task. Two sets of features are proposed: one constrained, i.e. respecting the data limitation suggested by the workshop organisers, and one unconstrained, i.e. using data or tools trained on data that was not provided by the workshop organisers. In total, more than 300 features were extracted and used to train classiﬁers in order to predict the translation quality of unseen data. In this paper, we focus on a subset of our feature set that we consider to be relatively novel: features based on a topic model built using the Latent Dirichlet Allocation approach, and features based on source and target language syntax extracted using part-of-speech (POS) taggers and parsers. We evaluate nine feature combinations using four classiﬁcation-based and four regression-based machine learning techniques. 
We present in this paper the system submissions of the SDL Language Weaver team in the WMT 2012 Quality Estimation shared-task. Our MT quality-prediction systems use machine learning techniques (M5P regression-tree and SVM-regression models) and a feature-selection algorithm that has been designed to directly optimize towards the ofﬁcial metrics used in this shared-task. The resulting submissions placed 1st (the M5P model) and 2nd (the SVM model), respectively, on both the Ranking task and the Scoring task, out of 11 participating teams. 
We in this paper describe the regression system for our participation in the quality estimation task of WMT12. This paper focuses on exploiting special phrases, or word sequences, to estimate translation quality. Several feature templates on this topic are put forward subsequently. We train a SVM regression model for predicting the scores and numerical results show the effectiveness of our phrase indicators and method in both ranking and scoring tasks. 
This paper presents techniques for referencefree, automatic prediction of Machine Translation output quality at both sentence- and document-level. In addition to helping with document-level quality estimation, sentencelevel predictions are used for system selection, improving the quality of the output translations. We present three system selection techniques and perform evaluations that quantify the gains across multiple domains and language pairs. 
We address two challenges for automatic machine translation evaluation: a) avoiding the use of reference translations, and b) focusing on adequacy estimation. From an economic perspective, getting rid of costly hand-crafted reference translations (a) permits to alleviate the main bottleneck in MT evaluation. From a system evaluation perspective, pushing semantics into MT (b) is a necessity in order to complement the shallow methods currently used overcoming their limitations. Casting the problem as a cross-lingual textual entailment application, we experiment with different benchmarks and evaluation settings. Our method shows high correlation with human judgements and good results on all datasets without relying on reference translations. 
Post-editing performed by translators is an increasingly common use of machine translated texts. While high quality MT may increase productivity, post-editing poor translations can be a frustrating task which requires more effort than translating from scratch. For this reason, estimating whether machine translations are of sufﬁcient quality to be used for post-editing and ﬁnding means to reduce post-editing effort are an important ﬁeld of study. Post-editing effort consists of different aspects, of which temporal effort, or the time spent on post-editing, is the most visible and involves not only the technical effort needed to perform the editing, but also the cognitive effort required to detect and plan necessary corrections. Cognitive effort is difﬁcult to examine directly, but ways to reduce the cognitive effort in particular may prove valuable in reducing the frustration associated with postediting work. In this paper, we describe an experiment aimed at studying the relationship between technical post-editing effort and cognitive post-editing effort by comparing cases where the edit distance and a manual score reﬂecting perceived effort differ. We present results of an error analysis performed on such sentences and discuss the clues they may provide about edits requiring great cognitive effort compared to the technical effort, on one hand, or little cognitive effort, on the other. 
The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system. Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Feng et al., 2010). We present a number of theoretical results regarding the use of permutation parsers in PBSMT. In particular, we show that an existing ITG constraint (Zens et al., 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. Experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 
Statistical phrase-based machine translation requires no linguistic information beyond word-aligned parallel corpora (Zens et al., 2002; Koehn et al., 2003). Unfortunately, this linguistic agnosticism often produces ungrammatical translations. Syntax, or sentence structure, could provide guidance to phrasebased systems, but the “non-constituent” word strings that phrase-based decoders manipulate complicate the use of most recursive syntactic tools. We address these issues by using Combinatory Categorial Grammar, or CCG, (Steedman, 2000), which has a much more ﬂexible notion of constituency, thereby providing more labels for putative nonconstituent multiword translation phrases. Using CCG parse charts, we train a syntactic analogue of a lexicalized reordering model by labelling phrase table entries with multiword labels and demonstrate signiﬁcant improvements in translating between Urdu and English, two language pairs with divergent sentence structure. 
Adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in GHKM (Galley et al., 2004), excludes potentially useful translation rules. SAMT (Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. We show that our labeling scheme performs comparably to SAMT on an Urdu–English translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast.  
Chiang’s hierarchical phrase-based (HPB) translation model advances the state-of-the-art in statistical machine translation by expanding conventional phrases to hierarchical phrases – phrases that contain sub-phrases. However, the original HPB model is prone to overgeneration due to lack of linguistic knowledge: the grammar may suggest more derivations than appropriate, many of which may lead to ungrammatical translations. On the other hand, limitations of glue grammar rules in the original HPB model may actually prevent systems from considering some reasonable derivations. This paper presents a simple but effective translation model, called the Head-Driven HPB (HD-HPB) model, which incorporates head information in translation rules to better capture syntax-driven information in a derivation. In addition, unlike the original glue rules, the HD-HPB model allows improved reordering between any two neighboring non-terminals to explore a larger reordering search space. An extensive set of experiments on Chinese-English translation on four NIST MT test sets, using both a small and a large training set, show that our HDHPB model consistently and statistically signiﬁcantly outperforms Chiang’s model as well as a source side SAMT-style model. 
We introduce the ﬁrst fully automatic, fully semantic frame based MT evaluation metric, MEANT, that outperforms all other commonly used automatic metrics in correlating with human judgment on translation adequacy. Recent work on HMEANT, which is a human metric, indicates that machine translation can be better evaluated via semantic frames than other evaluation paradigms, requiring only minimal effort from monolingual humans to annotate and align semantic frames in the reference and machine translations. We propose a surprisingly effective Occam’s razor automation of HMEANT that combines standard shallow semantic parsing with a simple maximum weighted bipartite matching algorithm for aligning semantic frames. The matching criterion is based on lexical similarity scoring of the semantic role ﬁllers through a simple context vector model which can readily be trained using any publicly available large monolingual corpus. Sentence level correlation analysis, following standard NIST MetricsMATR protocol, shows that this fully automated version of HMEANT achieves signiﬁcantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 
We introduce a taxonomy of factored phrasebased translation scenarios and conduct a range of experiments in this taxonomy. We point out several common pitfalls when designing factored setups. The paper also describes our WMT12 submissions CU-BOJAR and CU-POOR-COMB.  Number of Number of Translation Independent Structure Steps Searches of Searches  One Several  One One Several  – – Serial Complex  Nickname Direct Single-Step Two-Step Complex  Figure 1: A taxonomy of factored phrase-based models.  
This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12). We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building. 
One of the most notable recent improvements of the TectoMT English-to-Czech translation is a systematic and theoretically supported revision of formemes—the annotation of morpho-syntactic features of content words in deep dependency syntactic structures based on the Prague tectogrammatics theory. Our modiﬁcations aim at reducing data sparsity, increasing consistency across languages and widening the usage area of this markup. Formemes can be used not only in MT, but in various other NLP tasks. 
This paper describes the UPC participation in the WMT 12 evaluation campaign. All systems presented are based on standard phrasebased Moses systems. Variations adopted several improvement techniques such as morphology simpliﬁcation and generation and domain adaptation. The morphology simpliﬁcation overcomes the data sparsity problem when translating into morphologicallyrich languages such as Spanish by translating ﬁrst to a morphology-simpliﬁed language and secondly leave the morphology generation to an independent classiﬁcation task. The domain adaptation approach improves the SMT system by adding new translation units learned from MT-output and reference alignment. Results depict an improvement on TER, METEOR, NIST and BLEU scores compared to our baseline system, obtaining on the ofﬁcial test set more beneﬁts from the domain adaptation approach than from the morphological generalization method. 
We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. The main contributions in this release are the introduction of a compact grammar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 
We present a variant of phrase-based SMT that uses source-side parsing and a constituent reordering model based on word alignments in the word-aligned training corpus to predict hierarchical block-wise reordering of the input. Multiple possible translation orders are represented compactly in a source order lattice. This source order lattice is then annotated with phrase-level translations to form a lattice of tokens in the target language. Various feature functions are combined in a log-linear fashion to evaluate paths through that lattice. 
 2 System Description  We describe the systems developed by the team of the Qatar Computing Research Institute for the WMT12 Shared Translation Task. We used a phrase-based statistical machine translation model with several non-standard settings, most notably tuning data selection and phrase table combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 
This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation (WMT 2012). We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions. Both hierarchical and phrase-based SMT systems are applied. A number of different techniques are evaluated, including an insertion model, different lexical smoothing methods, a discriminative reordering extension for the hierarchical system, reverse translation, and system combination. By application of these methods we achieve considerable improvements over the respective baseline systems. 
We describe a substitution-based system for hybrid machine translation (MT) that has been extended with machine learning components controlling its phrase selection. The approach is based on a rule-based MT (RBMT) system which creates template translations. Based on the rule-based generation parse tree and target-to-target alignments, we identify the set of “interesting” translation candidates from one or more translation engines which could be substituted into our translation templates. The substitution process is either controlled by the output from a binary classiﬁer trained on feature vectors from the different MT engines, or it is depending on weights for the decision factors, which have been tuned using MERT. We are able to observe improvements in terms of BLEU scores over a baseline version of the hybrid system. 
We report on ﬁndings of exploiting large data sets for translation modeling, language modeling and tuning for the development of competitive machine translation systems for eight language pairs. 
This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT German→English task. Each group translated the data sets with their own systems and ﬁnally the RWTH system combination combined these translations in our ﬁnal submission. Experimental results show improvements of up to 1.7 points in BLEU and 3.4 points in TER compared to the best single system. 
This paper describes LIMSI’s submissions to the shared translation task. We report results for French-English and German-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams. In this approach, both the translation and target language models are estimated as conventional smoothed n-gram models; an approach we extend here by estimating the translation probabilities in a continuous space using neural networks. Experimental results show a signiﬁcant and consistent BLEU improvement of approximately 1 point for all conditions. We also report preliminary experiments using an “on-the-ﬂy” translation model. 
This paper describes the UPM system for the Spanish-English translation task at the NAACL 2012 workshop on statistical machine translation. This system is based on Moses. We have used all available free corpora, cleaning and deleting some repetitions. In this paper, we also propose a technique for selecting the sentences for tuning the system. This technique is based on the similarity with the sentences to translate. With our approach, we improve the BLEU score from 28.37% to 28.57%. And as a result of the WMT12 challenge we have obtained a 31.80% BLEU with the 2012 test set. Finally, we explain different experiments that we have carried out after the competition. 
This paper describes the PROMT submission for the WMT12 shared translation task. We participated in two language pairs: EnglishFrench and English-Spanish. The translations were made using the PROMT DeepHybrid engine, which is the first hybrid version of the PROMT system. We report on improvements over our baseline RBMT output both in terms of automatic evaluation metrics and linguistic analysis. 
This paper describes the phrase-based SMT systems developed for our participation in the WMT12 Shared Translation Task. Translations for English↔German and English↔French were generated using a phrase-based translation system which is extended by additional models such as bilingual, ﬁne-grained part-of-speech (POS) and automatic cluster language models and discriminative word lexica. In addition, we explicitly handle out-of-vocabulary (OOV) words in German, if we have translations for other morphological forms of the same stem. Furthermore, we extended the POS-based reordering approach to also use information from syntactic trees.  overestimation of the probabilities of the huge, but noisy Giga corpus. In the German-English system, we tried to learn translations for OOV words by exploring different morphological forms of the OOVs with the same lemma. Furthermore, we combined different language models in the log-linear model. We used wordbased language models trained on different parts of the training corpus as well as POS-based language models using ﬁne-grained POS information and language models trained on automatic word clusters. The paper is organized as follows: The next section gives a detailed description of our systems including all the models. The translation results for all directions are presented afterwards and we close with a conclusion.  
This paper describes our submissions for the WMT-12 translation task using Kriya - our hierarchical phrase-based system. We submitted systems in French-English and English-Czech language pairs. In addition to the baseline system following the standard MT pipeline, we tried ensemble decoding for French-English. The ensemble decoding method improved the BLEU score by 0.4 points over the baseline in newstest-2011. For English-Czech, we segmented the Czech side of the corpora and trained two different segmented models in addition to our baseline system. 
We present an improved version of DEPFIX (Marecˇek et al., 2011), a system for automatic rule-based post-processing of Englishto-Czech MT outputs designed to increase their ﬂuency. We enhanced the rule set used by the original DEPFIX system and measured the performance of the individual rules. We also modiﬁed the dependency parser of McDonald et al. (2005) in two ways to adjust it for the parsing of MT outputs. We show that our system is able to improve the quality of the state-of-the-art MT systems. 
We provide a few insights on data selection for machine translation. We evaluate the quality of the new CzEng 1.0, a parallel data source used in WMT12. We describe a simple technique for reducing out-of-vocabulary rate after phrase extraction. We discuss the beneﬁts of tuning towards multiple reference translations for English-Czech language pair. We introduce a novel approach to data selection by full-text indexing and search: we select sentences similar to the test set from a large monolingual corpus and explore several options of incorporating them in a machine translation system. We show that this method can improve translation quality. Finally, we describe our submitted system CU-TAMCH-BOJ.  Another topic we explore is to use multiple references for tuning to make the procedure more robust as suggested by Dyer et al. (2011). We evaluate this approach for translating from English into Czech. The main focus of our paper however lies in presenting a method for data selection using full-text search. We index a large monolingual corpus and then extract sentences from it that are similar to the input sentences. We use these sentences in several ways: to create a new language model, a new phrase table and a tuning set. The method can be seen as a kind of domain adaptation. We show that it contributes positively to translation quality and we provide a thorough evaluation. 2 Data and Tools  
We describe DFKI’s statistical based submission to the 2012 WMT evaluation. The submission is based on the freely available machine translation toolkit Jane, which supports phrase-based and hierarchical phrase-based translation models. Different setups have been tested and combined using a sentence selection method. 
We developed a string-to-tree system for English–German, achieving competitive results against a hierarchical model baseline. We provide details of our implementation of GHKM rule extraction and scope-3 parsing in the Moses toolkit. We compare systems trained on the same data using different grammar extraction methods. 
Recent work has established the efficacy of Amazon’s Mechanical Turk for constructing parallel corpora for machine translation research. We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent: Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu. These languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation. We conduct a variety of baseline experiments and analysis, and release the data to the community. 
Microblogging services such as Twitter have become popular media for real-time usercreated news reporting. Such communication often happens in parallel in different languages, e.g., microblog posts related to the same events of the Arab spring were written in Arabic and in English. The goal of this paper is to exploit this parallelism in order to eliminate the main bottleneck in automatic Twitter translation, namely the lack of bilingual sentence pairs for training SMT systems. We show that translation-based cross-lingual information retrieval can retrieve microblog messages across languages that are similar enough to be used to train a standard phrasebased SMT pipeline. Our method outperforms other approaches to domain adaptation for SMT such as language model adaptation, meta-parameter tuning, or self-translation. 
In statistical machine translation (SMT), it is known that performance declines when the training data is in a different domain from the test data. Nevertheless, it is frequently necessary to supplement scarce in-domain training data with out-of-domain data. In this paper, we ﬁrst try to relate the effect of the outof-domain data on translation performance to measures of corpus similarity, then we separately analyse the effect of adding the outof-domain data at different parts of the training pipeline (alignment, phrase extraction, and phrase scoring). Through experiments in 2 domains and 8 language pairs it is shown that the out-of-domain data improves coverage and translation of rare words, but may degrade the translation quality for more common words. 
The new frontier of computer assisted translation technology is the effective integration of statistical MT within the translation workﬂow. In this respect, the SMT ability of incrementally learning from the translations produced by users plays a central role. A still open problem is the evaluation of SMT systems that evolve over time. In this paper, we propose a new metric for assessing the quality of an adaptive MT component that is derived from the theory of learning curves: the percentage slope. 
SMT typically models translation at the sentence level, ignoring wider document context. Does this hurt the consistency of translated documents? Using a phrase-based SMT system in various data conditions, we show that SMT translates documents remarkably consistently, even without document knowledge. Nevertheless, translation inconsistencies often indicate translation errors. However, unlike in human translation, these errors are rarely due to terminology inconsistency. They are more often symptoms of deeper issues with SMT models instead. 
In statistical machine translation, word lattices are used to represent the ambiguities in the preprocessing of the source sentence, such as word segmentation for Chinese or morphological analysis for German. Several approaches have been proposed to deﬁne the probability of different paths through the lattice with external tools like word segmenters, or by applying indicator features. We introduce a novel lattice design, which explicitly distinguishes between different preprocessing alternatives for the source sentence. It allows us to make use of speciﬁc features for each preprocessing type and to lexicalize the choice of lattice path directly in the phrase translation model. We argue that forced alignment training can be used to learn lattice path and phrase translation model simultaneously. On the newscommentary portion of the German→English WMT 2011 task we can show moderate improvements of up to 0.6% BLEU over a stateof-the-art baseline system. 
Training the phrase table by force-aligning (FA) the training data with the reference translation has been shown to improve the phrasal translation quality while signiﬁcantly reducing the phrase table size on medium sized tasks. We apply this procedure to several large-scale tasks, with the primary goal of reducing model sizes without sacriﬁcing translation quality. To deal with the noise in the automatically crawled parallel training data, we introduce on-demand word deletions, insertions, and backoffs to achieve over 99% successful alignment rate. We also add heuristics to avoid any increase in OOV rates. We are able to reduce already heavily pruned baseline phrase tables by more than 50% with little to no degradation in quality and occasionally slight improvement, without any increase in OOVs. We further introduce two global scaling factors for re-estimation of the phrase table via posterior phrase alignment probabilities and a modiﬁed absolute discounting method that can be applied to fractional counts. Index Terms: phrasal machine translation, phrase training, phrase table pruning 
Minimum error rate training is often the preferred method for optimizing parameters of statistical machine translation systems. MERT minimizes error rate by using a surrogate representation of the search space, such as N best lists or hypergraphs, which only offer an incomplete view of the search space. In our work, we instead minimize error rate directly by integrating the decoder into the minimizer. This approach yields two beneﬁts. First, the function being optimized is the true error rate. Second, it lets us optimize parameters of translations systems other than standard linear model features, such as distortion limit. Since integrating the decoder into the minimizer is often too slow to be practical, we also exploit statistical signiﬁcance tests to accelerate the search by quickly discarding unpromising models. Experiments with a phrasebased system show that our approach is scalable, and that optimizing the parameters that MERT cannot handle brings improvements to translation results. 
The introduction of large-margin based discriminative methods for optimizing statistical machine translation systems in recent years has allowed exploration into many new types of features for the translation process. By removing the limitation on the number of parameters which can be optimized, these methods have allowed integrating millions of sparse features. However, these methods have not yet met with wide-spread adoption. This may be partly due to the perceived complexity of implementation, and partly due to the lack of standard methodology for applying these methods to MT. This papers aims to shed light on large-margin learning for MT, explicitly presenting the simple passive-aggressive algorithm which underlies many previous approaches, with direct application to MT, and empirically comparing several widespread optimization strategies. 
Probabilistic knowledge bases are commonly used in areas such as large-scale information extraction, data integration, and knowledge capture, to name but a few. Inference in probabilistic knowledge bases is a computationally challenging problem. With this contribution, we present our vision of a distributed inference algorithm based on conﬂict graph construction and hypergraph sampling. Early empirical results show that the approach efﬁciently and accurately computes a-posteriori probabilities of a knowledge base derived from a well-known information extraction system. 
In this paper, we propose a single lowdimensional representation of a large collection of table and hyponym data, and show that with a small number of primitive operations, this representation can be used effectively for many purposes. Speciﬁcally we consider queries like set expansion, class prediction etc. We evaluate our methods on publicly available semi-structured datasets from the Web. 
Domain adaptation is a time consuming and costly procedure calling for the development of algorithms and tools to facilitate its automation. This paper presents an unsupervised algorithm able to learn the main concepts in event summaries. The method takes as input a set of domain summaries annotated with shallow linguistic information and produces a domain template. We demonstrate the viability of the method by applying it to three different domains and two languages. We have evaluated the generated templates against human templates obtaining encouraging results. 
The extraction of relations between named entities from natural language text is a longstanding challenge in information extraction, especially in large-scale. A major challenge for the advancement of this research ﬁeld has been the lack of meaningful evaluation frameworks based on realistic-sized corpora. In this paper we propose a framework for large-scale evaluation of relation extraction systems based on an automatic annotator that uses a public online database and a large web corpus. 
We enhance a temporal knowledge base population system to improve the quality of distantly supervised training data and identify a minimal feature set for classiﬁcation. The approach uses multi-class logistic regression to eliminate individual features based on the strength of their association with a temporal label followed by semi-supervised relabeling using a subset of human annotations and lasso regression. As implemented in this work, our technique improves performance and results in notably less computational cost than a parallel system trained on the full feature set. 
 the obtained results with a traditional bilingual dictionary.  Cognitive properties of words are very useful in figurative language understanding, language acquisition and translation. To overcome the subjectivity and low efficiency in manual construction of such database, we propose a web-based method for automatic collection and analysis of cognitive properties. The method employs simile templates to query the search engines. With the help of a bilingual dictionary, the method is able to collect tens of thousands of “vehicleadjective” items of high quality. Frequencies are then used to obtain the common and independent cognitive properties automatically. The method can be extended conveniently to other languages to construct multi-lingual cognitive property knowledgebase. 
We present a practical use case of knowledge base (KB) population at the French news agency AFP. The target KB instances are entities relevant for news production and content enrichment. In order to acquire uniquely identiﬁed entities over news wires, i.e. textual data, and integrate the resulting KB in the Linked Data framework, a series of data models need to be aligned: Web data resources are harvested for creating a wide coverage entity database, which is in turn used to link entities to their mentions in French news wires. Finally, the extracted entities are selected for instantiation in the target KB. We describe our methodology along with the resources created and used for the target KB population. 
Dynamic content is a frequently accessed part of the Web. However, most information extraction approaches are batch-oriented, thus not effective for gathering rapidly changing data. This paper proposes a model for fact extraction in real-time. Our model addresses the difﬁcult challenges that timely fact extraction on frequently updated data entails. We point out a naive solution to the main research question and justify the choices we make in the model we propose. 
Web-scale knowledge bases typically consist entirely of predicates over entities. However, the distributional properties of how those entities appear in text are equally important aspects of knowledge. If noun phrases mapped unambiguously to knowledge base entities, adding this knowledge would simply require counting. The many-to-many relationship between noun phrase mentions and knowledge base entities makes adding distributional knowledge about entities difﬁcult. In this paper, we argue that this information should be explicitly included in web-scale knowledge bases. We propose a generative model that learns these distributional semantics by performing entity linking on the web, and we give some preliminary results that point to its usefulness. 
Current techniques for Open Information Extraction (OIE) focus on the extraction of binary facts and suffer signiﬁcant quality loss for the task of extracting higher order N-ary facts. This quality loss may not only affect the correctness, but also the completeness of an extracted fact. We present KRAKEN, an OIE system speciﬁcally designed to capture N-ary facts, as well as the results of an experimental study on extracting facts from Web text in which we examine the issue of fact completeness. Our preliminary experiments indicate that KRAKEN is a high precision OIE approach that captures more facts per sentence at greater completeness than existing OIE approaches, but is vulnerable to noisy and ungrammatical text. 
A precondition for extracting information from large text corpora is discovering the information structures underlying the text. Progress in this direction is being made in the form of unsupervised information extraction (IE). We describe recent work in unsupervised relation extraction and compare its goals to those of grammar discovery for science sublanguages. We consider what this work on grammar discovery suggests for future directions in unsupervised IE. 
Entity linking refers to the task of assigning mentions in documents to their corresponding knowledge base entities. Entity linking is a central step in knowledge base population. Current entity linking systems do not explicitly model the discourse context in which the communication occurs. Nevertheless, the notion of shared context is central to the linguistic theory of pragmatics and plays a crucial role in Grice’s cooperative communication principle. Furthermore, modeling context facilitates joint resolution of entities, an important problem in entity linking yet to be addressed satisfactorily. This paper describes an approach to context-aware entity linking. 
The steady progress of information extraction systems has been helped by sound methodologies for evaluating their performance in controlled experiments. Annual events like MUC, ACE and TAC have developed evaluation approaches enabling researchers to score and rank their systems relative to reference results. Yet these evaluations have only assessed component technologies needed by a knowledge base population system; none has required the construction of a knowledge base that is then evaluated directly. We describe an approach to the direct evaluation of a knowledge base and an instantiation that will be used in a 2012 TAC Knowledge Base Population track. 
As part of our work on building a "knowledgeable textbook" about biology, we are developing a textual question-answering (QA) system that can answer certain classes of biology questions posed by users. In support of that, we are building a "textual KB" - an assembled set of semi-structured assertions based on the book - that can be used to answer users’ queries, can be improved using global consistency constraints, and can be potentially validated and corrected by domain experts. Our approach is to view the KB as systematically caching answers from a QA system, and the QA system as assembling answers from the KB, the whole process kickstarted with an initial set of textual extractions from the book text itself. Although this research is only in a preliminary stage, we summarize our progress and lessons learned to date. 
The development of knowledge base creation systems has mainly focused on information extraction without considering how to effectively reason over their databases of facts. One reason for this is that the inference required to learn a probabilistic knowledge base from text at any realistic scale is intractable. In this paper, we propose formulating the joint problem of fact extraction and probabilistic model learning in terms of Tractable Markov Logic (TML), a subset of Markov logic in which inference is low-order polynomial in the size of the knowledge base. Using TML, we can tractably extract new information from text while simultaneously learning a probabilistic knowledge base. We will also describe a testbed for our proposal: creating a biomedical knowledge base and making it available for querying on the Web. 
 This paper investigates entity linking over millions of high-precision extractions from a corpus of 500 million Web documents, toward the goal of creating a useful knowledge base of general facts. This paper is the ﬁrst to report on entity linking over this many extractions, and describes new opportunities (such as corpus-level features) and challenges we found when entity linking at Web scale. We present several techniques that we developed and also lessons that we learned. We envision a future where information extraction and entity linking are paired to automatically generate knowledge bases with billions of assertions over millions of linked entities.  Figure 1: Entity Linking elevates textual argument strings to meaningful entities that hold properties, semantic types, and relationships with each other.  
Knowledge bases (KB) provide support for real-world decision making by exposing data in a structured format. However, constructing knowledge bases requires gathering data from many heterogeneous sources. Manual efforts for this task are accurate, but lack scalability, and automated approaches provide good coverage, but are not reliable enough for realworld decision makers to trust. These two approaches to KB construction have complementary strengths: in this paper we propose a novel framework for supporting humanproposed edits to knowledge bases. 
We have created layers of annotation on the English Gigaword v.5 corpus to render it useful as a standardized corpus for knowledge extraction and distributional semantics. Most existing large-scale work is based on inconsistent corpora which often have needed to be re-annotated by research teams independently, each time introducing biases that manifest as results that are only comparable at a high level. We provide to the community a public reference set based on current state-of-the-art syntactic analysis and coreference resolution, along with an interface for programmatic access. Our goal is to enable broader involvement in large-scale knowledge-acquisition efforts by researchers that otherwise may not have had the ability to produce such a resource on their own. 
 (bomb; explode near; ?)  (?; claim; responsibility)  We introduce the Rel-grams language model, which is analogous to an n-grams model, but is computed over relations rather than over words. The model encodes the conditional probability of observing a relational tuple R, given that R was observed in a window of prior relational tuples. We build a database of Rel-grams co-occurence statistics from ReVerb extractions over 1.8M news wire documents and show that a graphical model based on these statistics is useful for automatically discovering event templates. We make this database freely available and hope it will prove a useful resource for a wide variety of NLP tasks.  (bomb; explode at; ?)  0.38 0.40 0.58 0.54  (bomb; explode in; ?)  0.53  0.72  (bomb; kill; ?)  0.48  0.51  0.60  (?; kill; people)  0.43  0.57  0.54  (bomb; wound; ?)  (?; detonate; bomb) 0.40  (bomb; destroy; ?)  Figure 1: Part of a sub-graph that Rel-grams discovers showing relational tuples strongly associated with (bomb; kill; ?)  
We envision an automatic knowledge base construction system consisting of three interrelated components. MADDEN is a knowledge extraction system applying statistical text analysis methods over database systems (DBMS) and massive parallel processing (MPP) frameworks; PROBKB performs probabilistic reasoning over the extracted knowledge to derive additional facts not existing in the original text corpus; CAMEL leverages human intelligence to reduce the uncertainty resulting from both the information extraction and probabilistic reasoning processes. 
Conditional random ﬁelds and other graphical models have achieved state of the art results in a variety of NLP and IE tasks including coreference and relation extraction. Increasingly, practitioners are using models with more complex structure—higher tree-width, larger fanout, more features, and more data—rendering even approximate inference methods such as MCMC inefﬁcient. In this paper we propose an alternative MCMC sampling scheme in which transition probabilities are approximated by sampling from the set of relevant factors. We demonstrate that our method converges more quickly than a traditional MCMC sampler for both marginal and MAP inference. In an author coreference task with over 5 million mentions, we achieve a 13 times speedup over regular MCMC inference. 
In data integration we transform information from a source into a target schema. A general problem in this task is loss of ﬁdelity and coverage: the source expresses more knowledge than can ﬁt into the target schema, or knowledge that is hard to ﬁt into any schema at all. This problem is taken to an extreme in information extraction (IE) where the source is natural language. To address this issue, one can either automatically learn a latent schema emergent in text (a brittle and ill-deﬁned task), or manually extend schemas. We propose instead to store data in a probabilistic database of universal schema. This schema is simply the union of all source schemas, and the probabilistic database learns how to predict the cells of each source relation in this union. For example, the database could store Freebase relations and relations that correspond to natural language surface patterns. The database would learn to predict what freebase relations hold true based on what surface patterns appear, and vice versa. We describe an analogy between such databases and collaborative ﬁltering models, and use it to implement our paradigm with probabilistic PCA, a scalable and effective collaborative ﬁltering method. 
Commonsense reasoning requires knowledge about the frequency with which ordinary events and activities occur: How often do people eat a sandwich, go to sleep, write a book, or get married? This paper introduces work to acquire a knowledge base pairing factoids about such events with frequency categories learned from simple textual patterns. We are releasing a collection of the resulting event frequencies, which are evaluated for accuracy, and we demonstrate an initial application of the results to the problem of knowledge reﬁnement. 
We present a simple tool that enables the computer to read subtitles of movies and TV shows aloud. The tool extracts information from subtitle ﬁles, which can be freely downloaded from the Internet, and reads the text aloud through a speech synthesizer. There are three versions of the tool, one for Windows and Linux, another for Mac OS X, and the third is a browser-based HTML5 prototype. The tools are freely available and open-source. The target audience is people who have trouble reading subtitles while watching a movie, including elderly, people with visual impairments, people with reading difﬁculties and people who wants to learn a second language. The application is currently being evaluated together with user from these groups. 
This paper describes a demonstration of the WinkTalk system, which is a speech synthesis platform using expressive synthetic voices. With the help of a webcamera and facial expression analysis, the system allows the user to control the expressive features of the synthetic speech for a particular utterance with their facial expressions. Based on a personalised mapping between three expressive synthetic voices and the users facial expressions, the system selects a voice that matches their face at the moment of sending a message. The WinkTalk system is an early research prototype that aims to demonstrate that facial expressions can be used as a more intuitive control over expressive speech synthesis than manual selection of voice types, thereby contributing to an improved communication experience for users of speech generating devices. 
This paper presents a method for an AAC system to predict a whole response given features of the previous utterance from the interlocutor. It uses a large corpus of scripted dialogs, computes a variety of lexical, syntactic and whole phrase features for the previous utterance, and predicts features that the response should have, using an entropy-based measure. We evaluate the system on a held-out portion of the corpus. We ﬁnd that for about 3.5% of cases in the held-out corpus, we are able to predict a response, and among those, over half are either exact or at least reasonable substitutes for the actual response. We also present some results on keystroke savings. Finally we compare our approach to a state-of-the-art chatbot, and show (not surprisingly) that a system like ours, tuned for a particular style of conversation, outperforms one that is not. Predicting possible responses automatically by mining a corpus of dialogues is a novel contribution to the literature on whole utterance-based methods in AAC. Also useful, we believe, is our estimate that about 3.5-4.0% of utterances in dialogs are in principle predictable given previous context. 
It is well documented that people with severe speech and physical impairments (SSPI) often experience literacy difficulties, which hinder them from effectively using orthographicbased AAC systems for communication. To address this problem, phoneme-based AAC systems have been proposed, which enable users to access a set of spoken phonemes and combine phonemes into speech output. In this paper we investigate how prediction techniques can be applied to improve user performance of such systems. We have developed a phoneme-based prediction system, which supports single phoneme prediction and phoneme-based word prediction using statistical language models generated using a crowdsourced AAC-like corpus. We incorporated our prediction system into a hypothetical 12-key reduced phoneme keyboard. A computational experiment showed that our prediction system led to 56.3% average keystroke savings. 
Most icon-based augmentative and alternative communication (AAC) devices require users to formulate messages in syntactic order in order to produce syntactic utterances. Reliance on syntactic ordering, however, may not be appropriate for individuals with limited or emerging literacy skills. Some of these users may beneﬁt from unordered message formulation accompanied by automatic message expansion to generate syntactically correct messages. Facilitating communication via unordered message formulation, however, requires new methods of prediction. This paper describes a novel approach to word prediction using semantic grams, or “sem-grams,” which provide relational information about message components regardless of word order. Performance of four word-level prediction algorithms, two based on sem-grams and two based on n-grams, were compared on a corpus of informal blogs. Results showed that sem-grams yield accurate word prediction, but lack prediction coverage. Hybrid methods that combine n-gram and sem-gram approaches may be viable for unordered prediction in AAC. 
The number of people with dementia of the Alzheimer's type (DAT) continues to grow. One of the significant impacts of this disease is a decline in the ability to communicate using natural language. This decline in language facility often results in decreased social interaction and life satisfaction for persons with DAT and their caregivers. One possible strategy to lessen the effects of this loss of language facility is for the unaffected conversational partner (Facilitator) to "co-construct" short autobiographical stories from the life of the DATaffected conversational partner (Storyteller). It has been observed that a skilled conversational partner can facilitate co-constructed narrative with individuals who have mild to moderate DAT. Developing a computational model of this type of co-constructed narrative would enable assistive technology to be developed that can monitor a conversation between a Storyteller and Facilitator. This technology could provide context-sensitive suggestions to an unskilled Facilitator to help maintain the flow of conversation. This paper describes a framework in which the necessary computational model of co-constructed narrative can be developed. An analysis of the fundamental elements of such a model will be presented. 
Currently, health care costs associated with aging at home can be prohibitive if individuals require continual or periodic supervision or assistance because of Alzheimer’s disease. These costs, normally associated with human caregivers, can be mitigated to some extent given automated systems that mimic some of their functions. In this paper, we present inaugural work towards producing a generic automated system that assists individuals with Alzheimer’s to complete daily tasks using verbal communication. Here, we show how to improve rates of correct speech recognition by preprocessing acoustic noise and by modifying the vocabulary according to the task. We conclude by outlining current directions of research including specialized grammars and automatic detection of confusion. 
Tactile maps are important substitutes for visual maps for blind and visually impaired people and the efﬁciency of tactile-map reading can largely be improved by giving assisting utterances that make use of spatial language. In this paper, we elaborate earlier ideas for a system that generates such utterances and present a prototype implementation based on a semantic conceptualization of the movements that the map user performs. A worked example shows the plausibility of the solution and the output that the prototype generates given input derived from experimental data. 
American Sign Language (ASL) synthesis software can improve the accessibility of information and services for deaf individuals with low English literacy. The synthesis component of current ASL animation generation and scripting systems have limited handling of the many ASL verb signs whose movement path is inflected to indicate 3D locations in the signing space associated with discourse referents. Using motion-capture data recorded from human signers, we model how the motion-paths of verb signs vary based on the location of their subject and object. This model yields a lexicon for ASL verb signs that is parameterized on the 3D locations of the verb’s arguments; such a lexicon enables more realistic and understandable ASL animations. A new model presented in this paper, based on identifying the principal movement vector of the hands, shows improvement in modeling ASL verb signs, including when trained on movement data from a different human signer. 
This paper addresses the problem of automatic text simpliﬁcation. Automatic text simpliﬁcations aims at reducing the reading difﬁculty for people with cognitive disability, among other target groups. We describe an automatic text simpliﬁcation system for Spanish which combines a rule based core module with a statistical support module that controls the application of rules in the wrong contexts. Our system is integrated in a service architecture which includes a web service and mobile applications. 
This paper addresses the problem of training an artiﬁcial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. From this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments. 
In order for robots to effectively understand natural language commands, they must be able to acquire a large vocabulary of meaning representations that can be mapped to perceptual features in the external world. Previous approaches to learning these grounded meaning representations require detailed annotations at training time. In this paper, we present an approach which is capable of jointly learning a policy for following natural language commands such as “Pick up the tire pallet,” as well as a mapping between speciﬁc phrases in the language and aspects of the external world; for example the mapping between the words “the tire pallet” and a speciﬁc object in the environment. We assume the action policy takes a parametric form that factors based on the structure of the language, based on the G3 framework and use stochastic gradient ascent to optimize policy parameters. Our preliminary evaluation demonstrates the effectiveness of the model on a corpus of “pick up” commands given to a robotic forklift by untrained users. 
In spite of their well known limitations, most notably their use of very local contexts, n-gram language models remain an essential component of many Natural Language Processing applications, such as Automatic Speech Recognition or Statistical Machine Translation. This paper investigates the potential of language models using larger context windows comprising up to the 9 previous words. This study is made possible by the development of several novel Neural Network Language Model architectures, which can easily fare with such large context windows. We experimentally observed that extending the context size yields clear gains in terms of perplexity and that the n-gram assumption is statistically reasonable as long as n is sufﬁciently high, and that efforts should be focused on improving the estimation procedures for such large models. 
Language models play an important role in large vocabulary speech recognition and statistical machine translation systems. The dominant approach since several decades are back-off language models. Some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words. Lately, this tendency has changed and recent works concentrate on data selection. Continuous space methods are a very competitive approach, but they have a high computational complexity and are not yet in widespread use. This paper presents an experimental comparison of all these approaches on a large statistical machine translation task. We also describe an open-source implementation to train and use continuous space language models (CSLM) for such large tasks. We describe an efﬁcient implementation of the CSLM using graphical processing units from Nvidia. By these means, we are able to train an CSLM on more than 500 million words in 20 hours. This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back-off language model that we were able to build. 
In recent years, neural network language models (NNLMs) have shown success in both peplexity and word error rate (WER) compared to conventional n-gram language models. Most NNLMs are trained with one hidden layer. Deep neural networks (DNNs) with more hidden layers have been shown to capture higher-level discriminative information about input features, and thus produce better networks. Motivated by the success of DNNs in acoustic modeling, we explore deep neural network language models (DNN LMs) in this paper. Results on a Wall Street Journal (WSJ) task demonstrate that DNN LMs offer improvements over a single hidden layer NNLM. Furthermore, our preliminary results are competitive with a model M language model, considered to be one of the current state-of-the-art techniques for language modeling. 
In this paper, we describe a new, publicly available corpus intended to stimulate research into language modeling techniques which are sensitive to overall sentence coherence. The task uses the Scholastic Aptitude Test’s sentence completion format. The test set consists of 1040 sentences, each of which is missing a content word. The goal is to select the correct replacement from amongst ﬁve alternates. In general, all of the options are syntactically valid, and reasonable with respect to local N-gram statistics. The set was generated by using an N-gram language model to generate a long list of likely words, given the immediate context. These options were then hand-groomed, to identify four decoys which are globally incoherent, yet syntactically correct. To ensure the right to public distribution, all the data is derived from out-of-copyright materials from Project Gutenberg. The test sentences were derived from ﬁve of Conan Doyle’s Sherlock Holmes novels, and we provide a large set of Nineteenth and early Twentieth Century texts as training material. 
Modeling of foreign entity names is an important unsolved problem in morpheme-based modeling that is common in morphologically rich languages. In this paper we present an unsupervised vocabulary adaptation method for morph-based speech recognition. Foreign word candidates are detected automatically from in-domain text through the use of letter n-gram perplexity. Over-segmented foreign entity names are restored to their base forms in the morph-segmented in-domain text for easier and more reliable modeling and recognition. The adapted pronunciation rules are ﬁnally generated with a trainable grapheme-tophoneme converter. In ASR performance the unsupervised method almost matches the ability of supervised adaptation in correctly recognizing foreign entity names. 
We present a distributed framework for largescale discriminative language models that can be integrated within a large vocabulary continuous speech recognition (LVCSR) system using lattice rescoring. We intentionally use a weakened acoustic model in a baseline LVCSR system to generate candidate hypotheses for voice-search data; this allows us to utilize large amounts of unsupervised data to train our models. We propose an efﬁcient and scalable MapReduce framework that uses a perceptron-style distributed training strategy to handle these large amounts of data. We report small but signiﬁcant improvements in recognition accuracies on a standard voice-search data set using our discriminative reranking model. We also provide an analysis of the various parameters of our models including model size, types of features, size of partitions in the MapReduce framework with the help of supporting experiments. 
Statistical language models used in deployed systems for speech recognition, machine translation and other human language technologies are almost exclusively n-gram models. They are regarded as linguistically na¨ıve, but estimating them from any amount of text, large or small, is straightforward. Furthermore, they have doggedly matched or outperformed numerous competing proposals for syntactically well-motivated models. This unusual resilience of n-grams, as well as their weaknesses, are examined here. It is demonstrated that n-grams are good word-predictors, even linguistically speaking, in a large majority of word-positions, and it is suggested that to improve over n-grams, one must explore syntax-aware (or other) language models that focus on positions where n-grams are weak. 
Automatic evaluation has greatly facilitated system development in summarization. At the same time, the use of automatic evaluation has been viewed with mistrust by many, as its accuracy and correct application are not well understood. In this paper we provide an assessment of the automatic evaluations used for multi-document summarization of news. We outline our recommendations about how any evaluation, manual or automatic, should be used to ﬁnd statistically signiﬁcant differences between summarization systems. We identify the reference automatic evaluation metrics— ROUGE 1 and 2—that appear to best emulate human pyramid and responsiveness scores on four years of NIST evaluations. We then demonstrate the accuracy of these metrics in reproducing human judgements about the relative content quality of pairs of systems and present an empirical assessment of the relationship between statistically signiﬁcant differences between systems according to manual evaluations, and the difference according to automatic evaluations. Finally, we present a case study of how new metrics should be compared to the reference evaluation, as we search for even more accurate automatic measures. 
Numerous NLP tasks rely on clustering or community detection algorithms. For many of these tasks, the solutions are disjoint, and the relevant evaluation metrics assume nonoverlapping clusters. In contrast, the relatively recent task of abstractive community detection (ACD) results in overlapping clusters of sentences. ACD is a sub-task of an abstractive summarization system and represents a twostep process. In the ﬁrst step, we classify sentence pairs according to whether the sentences should be realized by a common abstractive sentence. This results in an undirected graph with sentences as nodes and predicted abstractive links as edges. The second step is to identify communities within the graph, where each community corresponds to an abstractive sentence to be generated. In this paper, we describe how the Omega Index, a metric for comparing non-disjoint clustering solutions, can be used as a summarization evaluation metric for this task. We use the Omega Index to compare and contrast several community detection algorithms. 
The multilingual summarization pilot task at TAC’11 opened a lot of problems we are facing when we try to evaluate summary quality in different languages. The additional language dimension greatly increases annotation costs. For the TAC pilot task English articles were ﬁrst translated to other 6 languages, model summaries were written and submitted system summaries were evaluated. We start with the discussion whether ROUGE can produce system rankings similar to those received from manual summary scoring by measuring their correlation. We study then three ways of projecting summaries to a different language: projection through sentence alignment in the case of parallel corpora, simple summary translation and summarizing machine translated articles. Building such summaries gives opportunity to run additional experiments and reinforce the evaluation. Later, we investigate whether an evaluation based on machine translated models can perform close to an evaluation based on original models. 
There is little evidence of widespread adoption of speech summarization systems. This may be due in part to the fact that the natural language heuristics used to generate summaries are often optimized with respect to a class of evaluation measures that, while computationally and experimentally inexpensive, rely on subjectively selected gold standards against which automatically generated summaries are scored. This evaluation protocol does not take into account the usefulness of a summary in assisting the listener in achieving his or her goal. In this paper we study how current measures and methods for evaluating summarization systems compare to human-centric evaluation criteria. For this, we have designed and conducted an ecologically valid evaluation that determines the value of a summary when embedded in a task, rather than how closely a summary resembles a gold standard. The results of our evaluation demonstrate that in the domain of lecture summarization, the wellknown baseline of maximal marginal relevance (Carbonell and Goldstein, 1998) is statistically significantly worse than human-generated extractive summaries, and even worse than having no summary at all in a simple quiz-taking task. Priming seems to have no statistically significant effect on the usefulness of the human summaries. In addition, ROUGE scores and, in particular, the contextfree annotations that are often supplied to ROUGE  as references, may not always be reliable as inexpensive proxies for ecologically valid evaluations. In fact, under some conditions, relying exclusively on ROUGE may even lead to scoring human-generated summaries that are inconsistent in their usefulness relative to using no summaries very favourably. 
The development of summarization systems requires reliable similarity (evaluation) measures that compare system outputs with human references. A reliable measure should have correspondence with human judgements. However, the reliability of measures depends on the test collection in which the measure is meta-evaluated; for this reason, it has not yet been possible to reliably establish which are the best evaluation measures for automatic summarization. In this paper, we propose an unsupervised method called HeterogeneityBased Ranking (HBR) that combines summarization evaluation measures without requiring human assessments. Our empirical results indicate that HBR achieves a similar correspondence with human assessments than the best single measure for every observed corpus. In addition, HBR results are more robust across topics than single measures. 
Today, automatic evaluation metrics such as ROUGE have become the de-facto mode of evaluating an automatic summarization system. However, based on the DUC and the TAC evaluation results, (Conroy and Schlesinger, 2008; Dang and Owczarzak, 2008) showed that the performance gap between humangenerated summaries and system-generated summaries is clearly visible in manual evaluations but is often not reﬂected in automated evaluations using ROUGE scores. In this paper, we present our own experiments in comparing the results of manual evaluations versus automatic evaluations using our own text summarizer: BlogSum. We have evaluated BlogSum-generated summary content using ROUGE and compared the results with the original candidate list (OList). The t-test results showed that there is no signiﬁcant difference between BlogSum-generated summaries and OList summaries. However, two manual evaluations for content using two different datasets show that BlogSum performed significantly better than OList. A manual evaluation of summary coherence also shows that BlogSum performs signiﬁcantly better than OList. These results agree with previous work and show the need for a better automated summary evaluation metric rather than the standard ROUGE metric. 
This paper discusses successes and failures of computational linguistics techniques in the study of how inter-event time intervals in a story affect the narrator’s use of different types of referring expressions. The success story shows that a conditional frequency distribution analysis of proper nouns and pronouns yields results that are consistent with our previous results – based on manual coding – that the narrator’s choice of referring expression depends on the amount of time that elapsed between events in a story. Unfortunately, the less successful story indicates that state-of-the-art coreference resolution systems fail to achieve high accuracy for this genre of discourse. Fine-grained analyses of these failures provide insight into the limitations of current coreference resolution systems, and ways of improving them. 
What makes a poem beautiful? We use computational methods to compare the stylistic and content features employed by awardwinning poets and amateur poets. Building upon existing techniques designed to quantitatively analyze style and affect in texts, we examined elements of poetic craft such as diction, sound devices, emotive language, and imagery. Results showed that the most important indicator of high-quality poetry we could detect was the frequency of references to concrete objects. This result highlights the inﬂuence of Imagism in contemporary professional poetry, and suggests that concreteness may be one of the most appealing features of poetry to the modern aesthetic. We also report on other features that characterize high-quality poetry and argue that methods from computational linguistics may provide important insights into the analysis of beauty in verbal art.  sensations of poetic beauty. We built a poetry corpus consisting of poems by award-winning professional poets and amateur poets, and compared poems in the two categories using various quantitative features. Although there are many reasons why some poems are included in prestigious anthologies and others are never read, such as a poet’s fame, we assume that the main distinction between poems in well-known anthologies and poems submitted by amateurs to online forums is that expert editors perceive poems in the former category as more aesthetically pleasing. Given this assumption, we believe that the kind of comparison we propose should be the ﬁrst step towards understanding how certain textual features might evoke aesthetic sensations more than others. The next sections review previous computational work on poetry and motivate the features we use; we then introduce our corpus, our analyses, and results. 2 Computational aesthetics  
The identiﬁcation of stylistic inconsistency is a challenging task relevant to a number of genres, including literature. In this work, we carry out stylistic segmentation of a well-known poem, The Waste Land by T.S. Eliot, which is traditionally analyzed in terms of numerous voices which appear throughout the text. Our method, adapted from work in topic segmentation and plagiarism detection, predicts breaks based on a curve of stylistic change which combines information from a diverse set of features, most notably co-occurrence in larger corpora via reduced-dimensionality vectors. We show that this extrinsic information is more useful than (within-text) distributional features. We achieve well above baseline performance on both artiﬁcial mixed-style texts and The Waste Land itself. 
Electronic versions of literary works abound on the Internet and the rapid dissemination of electronic readers will make electronic books more and more common. It is often the case that literary works exist in more than one language, suggesting that, if properly aligned, they could be turned into useful resources for many practical applications, such as writing and language learning aids, translation studies, or data-based machine translation. To be of any use, these bilingual works need to be aligned as precisely as possible, a notoriously difﬁcult task. In this paper, we revisit the problem of sentence alignment for literary works and explore the performance of a new, multi-pass, approach based on a combination of systems. Experiments conducted on excerpts of ten masterpieces of the French and English literature show that our approach signiﬁcantly outperforms two open source tools. 
This study explores the use of function words for authorship attribution in modern Chinese (C-FWAA). This study consists of three tasks: (1) examine the C-FWAA effectiveness in three genres: novel, essay, and blog; (2) compare the strength of function words as both genre and authorship indicators, and explore the genre interference on C-FWAA; (3) examine whether C-FWAA is sensitive to the time periods when the texts were written. 
Simple text classiﬁcation algorithms perform remarkably well when used for detecting famous quotes in literary or philosophical text, with f-scores approaching 95%. We compare the task to topic classiﬁcation, polarity classiﬁcation and authorship attribution. 
We present a method of authorship attribution and stylometry that exploits hierarchical information in phrase-structures. Contrary to much previous work in stylometry, we focus on content words rather than function words. Texts are parsed to obtain phrase-structures, and compared with texts to be analyzed. An efﬁcient tree kernel method identiﬁes common tree fragments among data of known authors and unknown texts. These fragments are then used to identify authors and characterize their styles. Our experiments show that the structural information from fragments provides complementary information to the baseline trigram model. 
We compare four methods for transcribing early printed texts. Our comparison is through a case-study of digitizing an eighteenthcentury French novel for a new critical edition: the 1784 Lettres ta¨ıtiennes by Jose´phine de Monbart. We provide a detailed error analysis of transcription by optical character recognition (OCR), non-expert humans, and expert humans and weigh each technique based on accuracy, speed, cost and the need for scholarly overhead. Our ﬁndings are relevant to 18th-century French scholars as well as the entire community of scholars working to preserve, present, and revitalize interest in literature published before the digital age. 
Readers suffering from information overload have often turned to collections of pithy and famous quotations. While research on largescale analysis of text reuse has found effective methods for detecting widely disseminated and famous quotations, this paper explores the complementary problem of detecting, from internal evidence alone, which phrases are quotable. These quotable phrases are memorable and succinct statements that people are likely to ﬁnd useful outside of their original context. We evaluate quotable phrase extraction using a large digital library and demonstrate that an integration of lexical and shallow syntactic features results in a reliable extraction process. A study using a reddit community of quote enthusiasts as well as a simple corpus analysis further demonstrate the practical applications of our work. 
The Quran is a significant religious text written in a unique literary style, close to very poetic language in nature. Accordingly it is significantly richer and more complex than the newswire style used in the previously released Arabic PropBank (Zaghouani et al., 2010; Diab et al., 2008). We present preliminary work on the creation of a unique Arabic proposition repository for Quranic Arabic. We annotate the semantic roles for the 50 most frequent verbs in the Quranic Arabic Dependency Treebank (QATB) (Dukes and Buckwalter 2010). The Quranic Arabic PropBank (QAPB) will be a unique new resource of its kind for the Arabic NLP research community as it will allow for interesting insights into the semantic use of classical Arabic, poetic literary Arabic, as well as significant religious texts. Moreover, on a pragmatic level QAPB will add approximately 810 new verbs to the existing Arabic PropBank (APB). In this pilot experiment, we leverage our knowledge and experience from our involvement in the APB project. All the QAPB annotations will be made freely available for research purposes. 
Topic modeling of fashion trends were analyzed using the MALLET toolkit. Harper’s Bazaar magazines from 1860-1899 were used (freely available online). This resulted in 20 topics with 4 characterizing words each. Trends over time were analyzed in several different ways using 100-topics and 20-topics. 
We consider several types of literary-theoretic approaches to literary text analysis; we describe several concepts from Computational Linguistics and Artificial Intelligence that could be used to model and support them. 
Narrative recall tasks are widely used in neuropsychological evaluation protocols in order to detect symptoms of disorders such as autism, language impairment, and dementia. In this paper, we propose a graph-based method commonly used in information retrieval to improve word-level alignments in order to align a source narrative to narrative retellings elicited in a clinical setting. From these alignments, we automatically extract narrative recall scores which can then be used for diagnostic screening. The signiﬁcant reduction in alignment error rate (AER) afforded by the graph-based method results in improved automatic scoring and diagnostic classiﬁcation. The approach described here is general enough to be applied to almost any narrative recall scenario, and the reductions in AER achieved in this work attest to the potential utility of this graph-based method for enhancing multilingual word alignment and alignment of comparable corpora for more standard NLP tasks. 
We describe an open information extraction system for biomedical text based on NELL (the Never-Ending Language Learner) (Carlson et al., 2010), a system designed for extraction from Web text. NELL uses a coupled semi-supervised bootstrapping approach to learn new facts from text, given an initial ontology and a small number of “seeds” for each ontology category. In contrast to previous applications of NELL, in our task the initial ontology and seeds are automatically derived from existing resources. We show that NELL’s bootstrapping algorithm is susceptible to ambiguous seeds, which are frequent in the biomedical domain. Using NELL to extract facts from biomedical text quickly leads to semantic drift. To address this problem, we introduce a method for assessing seed quality, based on a larger corpus of data derived from the Web. In our method, seed quality is assessed at each iteration of the bootstrapping process. Experimental results show signiﬁcant improvements over NELL’s original bootstrapping algorithm on two types of tasks: learning terms from biomedical categories, and named-entity recognition for biomedical entities using a learned lexicon. 
The identiﬁcation of semantically similar linguistic expressions despite their formal difference is an important task within NLP applications (information retrieval and extraction, terminology structuring...) We propose to detect the semantic relatedness between biomedical terms from the pharmacovigilance area. Two approaches are exploited: semantic distance within structured resources and terminology structuring methods applied to a raw list of terms. We compare these methods and study their complementarity. The results are evaluated against the reference pharmacovigilance data and manually by an expert. 
We investigate the task of assigning medical events in clinical narratives to discrete time-bins. The time-bins are deﬁned to capture when a medical event occurs relative to the hospital admission date in each clinical narrative. We model the problem as a sequence tagging task using Conditional Random Fields. We extract a combination of lexical, section-based and temporal features from medical events in each clinical narrative. The sequence tagging system outperforms a system that does not utilize any sequence information modeled using a Maximum Entropy classiﬁer. We present results with both handtagged as well as automatically extracted features. We observe over 8% improvement in overall tagging accuracy with the inclusion of sequence information. 
The growth of digital clinical data has raised questions as to how best to leverage this data to aid the world of healthcare. Promising application areas include Information Retrieval and Question-Answering systems. Such systems require an in-depth understanding of the texts that are processed. One aspect of this understanding is knowing if a medical condition outlined in a patient record is recent, or if it occurred in the past. As well as this, patient records often discuss other individuals such as family members. This presents a second problem - determining if a medical condition is experienced by the patient described in the report or some other individual. In this paper, we investigate the suitability of a machine learning (ML) based system for resolving these tasks on a previously unexplored collection of Patient History and Physical Examination reports. Our results show that our novel Score-based feature approach outperforms the standard Linguistic and Contextual features described in the related literature. Speciﬁcally, near-perfect performance is achieved in resolving if a patient experienced a condition. While for the task of establishing when a patient experienced a condition, our ML system signiﬁcantly outperforms the ConText system (87% versus 69% f-score, respectively). 
We present an algorithm for extracting abbreviation deﬁnitions from biomedical text. Our approach is based on an alignment HMM, matching abbreviations and their deﬁnitions. We report 98% precision and 93% recall on a standard data set, and 95% precision and 91% recall on an additional test set. Our results show an improvement over previously reported methods and our model has several advantages. Our model: (1) is simpler and faster than a comparable alignment-based abbreviation extractor; (2) is naturally generalizable to speciﬁc types of abbreviations, e.g., abbreviations of chemical formulas; (3) is trained on a set of unlabeled examples; and (4) associates a probability with each predicted deﬁnition. Using the abbreviation alignment model we were able to extract over 1.4 million abbreviations from a corpus of 200K full-text PubMed papers, including 455,844 unique deﬁnitions. 
In the English clinical and biomedical text domains, negation and certainty usage are two well-studied phenomena. However, few studies have made an in-depth characterization of uncertainties expressed in a clinical setting, and compared this between different annotation efforts. This preliminary, qualitative study attempts to 1) create a clinical uncertainty and negation taxonomy, 2) develop a translation map to convert annotation labels from an English schema into a Swedish schema, and 3) characterize and compare two data sets using this taxonomy. We deﬁne a clinical uncertainty and negation taxonomy and a translation map for converting annotation labels between two schemas and report observed similarities and differences between the two data sets. 
Active learning can lower the cost of annotation for some natural language processing tasks by using a classiﬁer to select informative instances to send to human annotators. It has worked well in cases where the training instances are selected one at a time and require minimal context for annotation. However, coreference annotations often require some context and the traditional active learning approach may not be feasible. In this work we explore various active learning methods for coreference resolution that ﬁt more realistically into coreference annotation workﬂows. 
The latest discoveries on diseases and their diagnosis/treatment are mostly disseminated in the form of scientific publications. However, with the rapid growth of the biomedical literature and a high level of variation and ambiguity in disease names, the task of retrieving disease-related articles becomes increasingly challenging using the traditional keywordbased approach. An important first step for any disease-related information extraction task in the biomedical literature is the disease mention recognition task. However, despite the strong interest, there has not been enough work done on disease name identification, perhaps because of the difficulty in obtaining adequate corpora. Towards this aim, we created a large-scale disease corpus consisting of 6900 disease mentions in 793 PubMed citations, derived from an earlier corpus. Our corpus contains rich annotations, was developed by a team of 12 annotators (two people per annotation) and covers all sentences in a PubMed abstract. Disease mentions are categorized into Specific Disease, Disease Class, Composite Mention and Modifier categories. When used as the gold standard data for a state-of-the-art machine-learning approach, significantly higher performance can be found on our corpus than the previous one. Such characteristics make this disease name corpus a valuable resource for mining disease-related information from biomedical text. The NCBI corpus is available for download at http://www.ncbi.nlm.nih.gov/CBBresearch/Fe llows/Dogan/disease.html.  
The acquisition of semantic resources and relations is an important task for several applications, such as query expansion, information retrieval and extraction, machine translation. However, their validity should also be computed and indicated, especially for automatic systems and applications. We exploit the compositionality based methods for the acquisition of synonymy relations and of indicators of these synonyms. We then apply pagerank-derived algorithm to the obtained semantic graph in order to ﬁlter out the acquired synonyms. Evaluation performed with two independent experts indicates that the quality of synonyms is systematically improved by 10 to 15% after their ﬁltering. 
In this paper we explore the applicability of existing coreference resolution systems to a biomedical genre: radiology reports. Analysis revealed that, due to the idiosyncrasies of the domain, both the formulation of the problem of coreference resolution and its solution need signiﬁcant domain adaptation work. We reformulated the task and developed an unsupervised algorithm based on heuristics for coreference resolution in radiology reports. The algorithm is shown to perform well on a test dataset of 150 manually annotated radiology reports. 
The goal of this work is to apply NLP techniques to the ﬁeld of BioNLP in order to gain a better insight into the ﬁeld and show connections and trends that might not otherwise be apparent. The data we analyzed was the proceedings from last decade of BioNLP workshops. Our ﬁndings reveal the prominent research problems and techniques in the ﬁeld, their progression over time, the approaches that researchers are using to solve those problems, insightful ways to categorize works in the ﬁeld, and the prominent researchers and groups whose works are inﬂuencing the ﬁeld. 
Manually annotating clinical document corpora to generate reference standards for Natural Language Processing (NLP) systems or Machine Learning (ML) is a timeconsuming and labor-intensive endeavor. Although a variety of open source annotation tools currently exist, there is a clear opportunity to develop new tools and assess functionalities that introduce efficiencies into the process of generating reference standards. These features include: management of document corpora and batch assignment, integration of machine-assisted verification functions, semi-automated curation of annotated information, and support of machine-assisted pre-annotation. The goals of reducing annotator workload and improving the quality of reference standards are important considerations for development of new tools. An infrastructure is also needed that will support largescale but secure annotation of sensitive clinical data as well as crowdsourcing which has proven successful for a variety of annotation tasks. We introduce the Extensible Human Oracle Suite of Tools (eHOST) http://code.google.com/p/ehost that provides such functionalities that when coupled with server integration offer an end-to-end solution to carry out small or large scale as well as crowd sourced annotation projects. 
The application of natural language processing (NLP) in the biology and medical domain crosses many fields from Healthcare Information to Bioinformatics to NLP itself. In order to make sense of how these fields relate and intersect, we have created “MedLingMap” (www.medlingmap.org) which is a compilation of references with a multi-faceted index. The initial focus has been creating the infrastructure and populating it with references annotated with facets such as topic, resources used (ontologies, tools, corpora), and organizations. Simultaneously we are applying NLP techniques to the text to find clusters, key terms and other relationships. The goal for this paper is to introduce MedLingMap to the community and show how it can be a powerful tool for research and exploration in the field. 
Many genetic epidemiological studies of human diseases have multiple variables related to any given phenotype, resulting from different deﬁnitions and multiple measurements or subsets of data. Manually mapping and harmonizing these phenotypes is a timeconsuming process that may still miss the most appropriate variables. Previously, a supervised learning algorithm was proposed for this problem. That algorithm learns to determine whether a pair of phenotypes is in the same class. Though that algorithm accomplished satisfying F-scores, the need to manually label training examples becomes a bottleneck to improve its coverage. Herein we present a novel active learning solution to solve this challenging phenotype-mapping problem. Active learning will make phenotype mapping more efﬁcient and improve its accuracy. 
Block-LDA is a topic modeling approach to perform data fusion between entity-annotated text documents and graphs with entity-entity links. We evaluate Block-LDA in the yeast biology domain by jointly modeling PubMed R articles and yeast protein-protein interaction networks. The topic coherence of the emergent topics and the ability of the model to retrieve relevant scientiﬁc articles and proteins related to the topic are compared to that of a text-only approach that does not make use of the protein-protein interaction matrix. Evaluation of the results by biologists show that the joint modeling results in better topic coherence and improves retrieval performance in the task of identifying top related papers and proteins. 
This paper presents a machine learning approach that selects and, more generally, ranks sentences containing clear relations between genes and terms that are related to them. This is treated as a binary classiﬁcation task, where preference judgments are used to learn how to choose a sentence from a pair of sentences. Features to capture how the relationship is described textually, as well as how central the relationship is in the sentence, are used in the learning process. Simpliﬁcation of complex sentences into simple structures is also applied for the extraction of the features. We show that such simpliﬁcation improves the results by up to 13%. We conducted three different evaluations and we found that the system signiﬁcantly outperforms the baselines. 
The relationship between small molecules and proteins has attracted attention from the biomedical research community. In this paper a text mining method of extracting smallmolecule and protein pairs from natural text is presented, based on a semi-supervised machine learning approach. The technique has been applied to the complete collection of MEDLINE abstracts and pairs were extracted and evaluated. The results show the feasibility of the bootstrapping system, which will subsequently be further investigated and improved. 
Gene name identification is a fundamental step to solve more complicated text mining problems such as gene normalization and protein-protein interactions. However, state-ofthe-art name identification methods are not yet sufficient for use in a fully automated system. In this regard, a relaxed task, gene/protein sentence identification, may serve more effectively for manually searching and browsing biomedical literature. In this paper, we set up a new task, gene/protein sentence classification and propose an ensemble approach for addressing this problem. Wellknown named entity tools use similar goldstandard sets for training and testing, which results in relatively poor performance for unknown sets. We here explore how to combine diverse high-precision gene identifiers for more robust performance. The experimental results show that the proposed approach outperforms BANNER as a stand-alone classifier for newly annotated sets as well as previous gold-standard sets. 
Datasets that answer difﬁcult clinical questions are expensive in part due to the need for medical expertise and patient informed consent. We investigate the effect of small sample size on the performance of a text categorization algorithm. We show how to determine whether the dataset is large enough to train support vector machines. Since it is not possible to cover all aspects of sample size calculation in one manuscript, we focus on how certain types of data relate to certain properties of support vector machines. We show that normal vectors of decision hyperplanes can be used for assessing reliability and internal cross-validation can be used for assessing stability of small sample data. 
There has been an active development of corpora and annotations in the BioNLP community. As those resources accumulate, a new issue arises about the reusability. As a solution to improve the reusability of corpora and annotations, we present PubAnnotation, a persistent and sharable repository, where various corpora and annotations can be stored together in a stable and comparable way. As a position paper, it explains the motivation and the core concepts of the repository and presents a prototype repository as a proof-of-concept. 
The package insert (aka drug product label) is the only publicly-available source of information on drug-drug interactions (DDIs) for some drugs, especially newer ones. Thus, an automated method for identifying DDIs in drug package inserts would be a potentially important complement to methods for identifying DDIs from other sources such as the scientific literature. To develop such an algorithm, we created a corpus of Federal Drug Administration approved drug package insert statements that have been manually annotated for pharmacokinetic DDIs by a pharmacist and a drug information expert. We then evaluated three different machine learning algorithms for their ability to 1) identify pharmacokinetic DDIs in the package insert corpus and 2) classify pharmacokinetic DDI statements by their modality (i.e., whether they report a DDI or no interaction between drug pairs). Experiments found that a support vector machine algorithm performed best on both tasks with an F-measure of 0.859 for pharmacokinetic DDI identification and 0.949 for modality assignment. We also found that the use of syntactic information is very helpful for addressing the problem of sentences containing both interacting and non-interacting pairs of drugs.  point (Marroum & Gobburu 2002). Among the information provided by PIs are drug-drug interactions (DDIs): known and predicted drug combinations that could lead to a clinically meaningful alteration in the effect of one of the drugs. The United States Federal Drug Administration (FDA) mandates that PIs for FDA-approved drugs include both observed and predicted clinically significant DDIs, as well as the results of pharmacokinetic studies that establish the absence of effect (FDA. 2010). Moreover, the PI is the only publicallyavailable source of information on DDIs for some drugs, especially newer ones (Dal-Ré et al. 2010). Hence, an automated method for identifying DDIs from drug PIs would be an important complement to methods for identifying DDIs from other sources such as the scientific literature. In this paper we describe the creation of a new corpus of FDA-approved drug package insert statements that have been manually annotated for pharmacokinetic DDIs. We then discuss how three different machine learning algorithms were evaluated for their ability to 1) identify pharmacokinetic DDIs in drug package inserts and 2) classify pharmacokinetic DDI statements by their modality (i.e., whether they report a DDI or that a drug pair does not interact). 2 Materials and Methods  2.1 The DDI Corpus and Schema  
Publications that report genotype-drug interaction findings, as well as manually curated databases such as DrugBank and PharmGKB are essential to advancing pharmacogenomics, a relatively new area merging pharmacology and genomic research. Natural language processing (NLP) methods can be very useful for automatically extracting knowledge such as gene-drug interactions, offering researchers immediate access to published findings, and allowing curators a shortcut for their work. We present a corpus of gene-drug interactions for evaluating and training systems to extract those interactions. The corpus includes 551 sentences that have a mention of a drug and a gene from about 600 journals found to be relevant to pharmacogenomics through an analysis of gene-drug relationships in the PharmGKB knowledgebase. We evaluated basic approaches to automatic extraction, including gene and drug cooccurrence, co-occurrence plus interaction terms, and a linguistic pattern-based method. The linguistic pattern method had the highest precision (96.61%) but lowest recall (7.30%), for an f-score of 13.57%. Basic co-occurrence yields 68.99% precision, with the addition of an interaction term precision increases slightly (69.60%), though not as much as could be expected. Co-occurrence is a reasonable baseline method, with pattern-based being a promising approach if enough patterns can be generated to address recall. The corpus is available at http://diego.asu.edu/index.php/projects 
A preliminary work on symptom name recognition from free-text clinical records (FCRs) of traditional Chinese medicine (TCM) is depicted in this paper. This problem is viewed as labeling each character in FCRs of TCM with a pre-defined tag (“B-SYC”, “I-SYC” or “OSYC”) to indicate the character’s role (a beginning, inside or outside part of a symptom name). The task is handled by Conditional Random Fields (CRFs) based on two types of features. The symptom name recognition FMeasure can reach up to 62.829% with recognition rate 93.403% and recognition error rate 52.665% under our experiment settings. The feasibility and effectiveness of the methods and reasonable features are verified, and several interesting and helpful results are shown. A detailed analysis for recognizing symptom names from FCRs of TCM is presented through analyzing labeling results of CRFs. 
The most accurate approaches to Word Sense Disambiguation (WSD) for biomedical documents are based on supervised learning. However, these require manually labeled training examples which are expensive to create and consequently supervised WSD systems are normally limited to disambiguating a small set of ambiguous terms. An alternative approach is to create labeled training examples automatically and use them as a substitute for manually labeled ones. This paper describes a large scale WSD system based on automatically labeled examples generated using information from the UMLS Metathesaurus. The labeled examples are generated without any use of labeled training data whatsoever and is therefore completely unsupervised (unlike some previous approaches). The system is evaluated on two widely used data sets and found to outperform a state-of-the-art unsupervised approach which also uses information from the UMLS Metathesaurus. 
When only a small amount of manually annotated data is available, application of a bootstrapping method is often considered to compensate for the lack of sufcient training material for a machine-learning method. The paper reports a series of experimental results of bootstrapping for protein name recognition. The results show that the performance changes signicantly according to the choice of text collection where the training samples to bootstrap, and that an improvement can be obtained only with a well chosen text collection. 
Most tools and resources developed for natural language processing of Arabic are designed for Modern Standard Arabic (MSA) and perform terribly on Arabic dialects, such as Egyptian Arabic. Egyptian Arabic differs from MSA phonologically, morphologically and lexically and has no standardized orthography. We present a linguistically accurate, large-scale morphological analyzer for Egyptian Arabic. The analyzer extends an existing resource, the Egyptian Colloquial Arabic Lexicon, and follows the part-of-speech guidelines used by the Linguistic Data Consortium for Egyptian Arabic. It accepts multiple orthographic variants and normalizes them to a conventional orthography. 
Hindi is an Indian language which is relatively rich in morphology. A few morphological analyzers of this language have been developed. However, they give only inﬂectional analysis of the language. In this paper, we present our Hindi derivational morphological analyzer. Our algorithm upgrades an existing inﬂectional analyzer to a derivational analyzer and primarily achieves two goals. First, it successfully incorporates derivational analysis in the inﬂectional analyzer. Second, it also increases the coverage of the inﬂectional analysis of the existing inﬂectional analyzer. 
Fast re-training of word segmentation models is required for adapting to new resources or domains in NLP of many Asian languages without word delimiters. The traditional tokenization model is efficient but inaccurate. This paper proposes a phrase-based model that factors sentence tokenization into phrase tokenizations, the dependencies of which are also taken into account. The model has a good OOV recognition ability, which improves the overall performance significantly. The training is a linear time phrase extraction and MLE procedure, while the decoding is via dynamic programming based algorithms. 
Languages are constantly evolving through their users due to the need to communicate more efﬁciently. Under this hypothesis, we formulate unsupervised word segmentation as a regularized compression process. We reduce this process to an optimization problem, and propose a greedy inclusion solution. Preliminary test results on the Bernstein-Ratner corpus and Bakeoff-2005 show that the our method is comparable to the state-of-the-art in terms of effectiveness and efﬁciency.  is realized as an optimization problem called regularized compression, which gets this name from its analogy to text compression. The rest of the paper is organized as follows. We brieﬂy summarize related work on unsupervised word segmentation in Section 2. In Section 3, we introduce the proposed formulation. The iterative algorithm and other technical details for solving the optimization problem are covered in Section 4. In Section 5, we describe the evaluation procedure and discuss the experimental results. Finally, we present concluding remarks in Section 6.  
This paper describes a small experiment to test a rule-based approach to unknown word recognition in Arabic. The morphological complexity of Arabic presents its challenges to a variety of NLP applications, but it can also be viewed as an advantage, if we can tap into the complex linguistic knowledge associated with these complex forms. In particular, the derived forms of verbs can be analysed and an educated guess at the likely meaning of a derived form can be predicted, based on the meaning of a known form and the relationship between the known form and the unknown one. The performance of the approach is tested on the NEMLAR Written Arabic Corpus. 
This paper ﬁrst deﬁnes the conditions under which copying and deletion processes are subsequential: speciﬁcally this is the case when the process is bounded in the right ways. Then, if we analyze metathesis as the composition of copying and deletion, it can be shown that the set of attested metathesis patterns fall into the subsequential or reverse subsequential classes. The implications of bounded copying are extended to partial reduplication, which is also shown to be either subsequential or reverse subsequential. 
The problem of the acquisition of phonotactics in Optimality Theory is intractable. This paper offers a way to cope with this hardness result: the problem is reformulated as a well known integer program (the Assignment problem with linear side constraints) paving the way for the application to phonotactics of approximation algorithms recently developed for integer programming. Knowledge of the phonotactics of a language is knowledge of its distinction between licit and illicit forms. The acquisition of phonotactics represents a distinguished and important stage of language acquisition. In fact, in carefully controlled experimental conditions, nine-month-old infants already react differently to licit and illicit sound combinations (Jusczyk et al., 1993). They thus display knowledge of phonotactics already at an early stage of language development. Usually, the problem of the acquisition of the phonotactics of a language given a ﬁnite set of linguistic data is formalized as the problem of ﬁnding a smallest language in the typology that is consistent with the data (Berwick, 1985; Manzini and Wexler, 1987; Prince and Tesar, 2004; Hayes, 2004; Fodor and Sakas, 2005). Section 1 formulates the problem of the acquisition of phonotactics along these lines within the mainstream phonological framework of Optimality Theory (Prince and Smolensky, 2004; Kager, 1999). Unfortunately, (such a formulation of) the problem of the acquisition of phonotactics in OT turns out to be intractable (NP-complete): for any attempted efﬁcient solution algorithm, there are some instances of the problem where the algorithm fails (Magri, 2010; Magri, 2012b). This hardness result holds for the universal formulation of the problem, in the sense of Heinz et al. (2009):  there are no restrictions on the constraint set that deﬁnes the OT typology and indeed the OT typology itself ﬁgures as an input to the problem. There are two strategies to cope with this hardness result. One approach weakens the formulation of the problem through proper restrictions on the constraint set: certain constraint sets are implausible from a phonological perspective, and should therefore be ignored in the proper formulation of the problem (Magri, 2011; Magri, 2012c). This approach raises interesting challenges, as it requires a through investigation of the algorithmic implications of various generalizations developed by phonologists on what counts as a “plausible” OT constraint set. Another approach is to bypass this difﬁculty, and weaken the formulation of the problem by lowering the standard for success: we settle on an approximate solution, namely a “small” language rather than a smallest language. This paper paves the way for the latter approach. I focus on the speciﬁc formulation of the problem of the acquisition of OT phonotactics developed in Prince and Tesar (2004). In Sections 2 and 3, I show that this formulation of the problem can be restated as a classical integer program, namely the Assignment problem with liner side constraints (AssignLSCsPbm). The theory of approximation algorithms for integer programing is a blooming ﬁeld of Computer Science (Bertsimas and Weismantel, 2005). In particular, powerful approximation algorithms have been recently developed for the AssignLSCsPbm. A state-of-the-art algorithm is due to Arora et al. (2002). The integer programming formulation developed in this paper thus paves the way for a new approximation approach to the problem of modeling the acquisition of phonotactics within OT. In Magri (2012a), I report simulation results with Arora’s et. al. (2002) algorithm on various instances of the problem of the acquisition of phonotactics.  52  Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 52–61, Montre´al, Canada, June 7, 2012. c 2012 Association for Computational Linguistics  
We show that a class of cases that has been previously studied in terms of learning of abstract phonological underlying representations (URs) can be handled by a learner that chooses URs from a contextually conditioned distribution over observed surface representations. We implement such a learner in a Maximum Entropy version of Optimality Theory, in which UR learning is an instance of semisupervised learning. Our objective function incorporates a term aimed to ensure generalization, independently required for phonotactic learning in Optimality Theory, and does not have a bias for single URs for morphemes. This learner is successful on a test language provided by Tesar (2006) as a challenge for UR learning. We also provide successful results on learning of a toy case modeled on French vowel alternations, which have also been previously analyzed in terms of abstract URs. This case includes lexically conditioned variation, an aspect of the data that cannot be handled by abstract URs, showing that in this respect our approach is more general. 
This paper presents a memoryless categorization learner that predicts differences in category complexity found in several psycholinguistic and psychological experiments. In particular, this learner predicts the order of difﬁculty of learning simple Boolean categories, including the advantage of conjunctive categories over the disjunctive ones (an advantage that is not typically modeled by the statistical approaches). It also models the effect of labeling (positive and negative labels vs. positive labels of two different kinds) on category complexity. This effect has implications for the differences between learning a single category (e.g., a phonological class of segments) vs. a set of non-overlapping categories (e.g., afﬁxes in a morphological paradigm). 
Economic globalization and the needs of the intelligence community have brought machine translation into the forefront. There are not enough skilled human translators to meet the growing demand for high quality translations or “good enough” translations that suffice only to enable understanding. Much research has been done in creating translation systems to aid human translators and to evaluate the output of these systems. Metrics for the latter have primarily focused on improving the overall quality of entire test sets but not on gauging the understanding of individual sentences or paragraphs. Therefore, we have focused on developing a theory of translation effectiveness by isolating a set of translation variables and measuring their effects on the comprehension of translations. In the following study, we focus on investigating how certain linguistic permutations, omissions, and insertions affect the understanding of translated texts. 1. Introduction There are numerous methods for measuring translation quality and ongoing research to improve relevant and informative metrics (see http://www.itl.nist.gov/iad/mig/tests/metricsmatr) (Przybocki et al., 2008). Many of these automated metrics, including BLEU and NIST, were created to be used only for aggregate counts over an entire test-set. The effectiveness of these methods on translations of short segments remains unclear (Kulesza and Shieber, 2004). Moreover, most of these tools are useful for comparing different sys-  tems, but do not attempt to identify the most dominant cause of errors. All errors are not equal and as such should be evaluated depending on their consequences (Schiaffino and Zearo, 2005). Recently, researchers have begun looking at the frequencies of errors in translations of specific language pairs. Vilar et al. (2006) presented a typology for annotating errors and used it to classify errors between Spanish and English and from Chinese into English. Popovic and Ney (2011) used methods for computing Word Error Rate (WER) and Position-independent word Error Rate (PER) to outline a procedure for automatic error analysis and classification. They evaluated their methodology by looking at translations into English from Arabic, Chinese and German and two-way English-Spanish data (Popovic and Ney, 2007). Condon et al. (2010) used the US National Institute of Standards and Technology’s NIST post-editing tool to annotate errors in English-Arabic translations These methods have all focused on finding frequencies of individual error categories, not on determining their effect on comprehension. In machine translation environments where postediting is used to produce the same linguistic quality as would be achieved by standard human translation, such a focus is justified. A greater reduction in the time needed to correct a translation would be achieved by eliminating errors that frequently occur. However, there are situations in which any translation is an acceptable alternative to no translation, and the direct (not post-edited) content is given to the user. Friends chatting via in-  
In this paper we present the results of the analysis of a parallel corpus of original and simpliﬁed texts in Spanish, gathered for the purpose of developing an automatic simpliﬁcation system for this language. The system is intended for individuals with cognitive disabilities who experience difﬁculties reading and interpreting informative texts. We here concentrate on lexical simpliﬁcation operations applied by human editors on the basis of which we derive a set of rules to be implemented automatically. We have so far addressed the issue of lexical units substitution, with special attention to reporting verbs and adjectives of nationality; insertion of deﬁnitions; simpliﬁcation of numerical expressions; and simpliﬁcation of named entities. 
While there has been much work on computational models to predict readability based on the lexical, syntactic and discourse properties of a text, there are also interesting open questions about how computer generated text should be evaluated with target populations. In this paper, we compare two ofﬂine methods for evaluating sentence quality, magnitude estimation of acceptability judgements and sentence recall. These methods differ in the extent to which they can differentiate between surface level ﬂuency and deeper comprehension issues. We ﬁnd, most importantly, that the two correlate. Magnitude estimation can be run on the web without supervision, and the results can be analysed automatically. The sentence recall methodology is more resource intensive, but allows us to tease apart the ﬂuency and comprehension issues that arise. 
Generally, people with dyslexia are poor readers but strong visual thinkers. The use of graphical schemes for helping text comprehension is recommended in education manuals. This study explores the relation between text readability and the visual conceptual schemes which aim to make the text more clear for these speciﬁc target readers. Our results are based on a user study for Spanish native speakers through a group of twenty three dyslexic users and a control group of similar size. The data collected from our study combines qualitative data from questionnaires and quantitative data from tests carried out using eye tracking. The ﬁndings suggest that graphical schemes may help to improve readability for dyslexics but are, unexpectedly, counterproductive for understandability. 
Lexicons of word difﬁculty are useful for various educational applications, including readability classiﬁcation and text simpliﬁcation. In this work, we explore automatic creation of these lexicons using methods which go beyond simple term frequency, but without relying on age-graded texts. In particular, we derive information for each word type from the readability of the web documents they appear in and the words they co-occur with, linearly combining these various features. We show the efﬁcacy of this approach by comparing our lexicon with an existing coarse-grained, low-coverage resource and a new crowdsourced annotation. 
Although many approaches have been presented to compute and predict readability of documents in different languages, the information provided by readability systems often fail to show in a clear and understandable way how difﬁcult a document is and which aspects contribute to content readability. We address this issue by presenting a system that, for a given document in Italian, provides not only a list of readability indices inspired by CohMetrix, but also a graphical representation of the difﬁculty of the text compared to the three levels of Italian compulsory education, namely elementary, middle and high-school level. We believe that this kind of representation makes readability assessment more intuitive, especially for educators who may not be familiar with readability predictions via supervised classiﬁcation. In addition, we present the ﬁrst available system for readability assessment of Italian inspired by Coh-Metrix. 
Readability formulas are methods used to match texts with the readers’ reading level. Several methodological paradigms have previously been investigated in the ﬁeld. The most popular paradigm dates several decades back and gave rise to well known readability formulas such as the Flesch formula (among several others). This paper compares this approach (henceforth ”classic”) with an emerging paradigm which uses sophisticated NLPenabled features and machine learning techniques. Our experiments, carried on a corpus of texts for French as a foreign language, yield four main results: (1) the new readability formula performed better than the “classic” formula; (2) “non-classic” features were slightly more informative than “classic” features; (3) modern machine learning algorithms did not improve the explanatory power of our readability model, but allowed to better classify new observations; and (4) combining “classic” and “non-classic” features resulted in a significant gain in performance. 
Early primary children’s literature poses some interesting challenges for automated readability assessment: for example, teachers often use ﬁne-grained reading leveling systems for determining appropriate books for children to read (many current systems approach readability assessment at a coarser whole grade level). In previous work (Ma et al., 2012), we suggested that the ﬁne-grained assessment task can be approached using a ranking methodology, and incorporating features that correspond to the visual layout of the page improves performance. However, the previous methodology for using “found” text (e.g., scanning in a book from the library) requires human annotation of the text regions and correction of the OCR text. In this work, we ask whether the annotation process can be automated, and also experiment with richer syntactic features found in the literature that can be automatically derived from either the humancorrected or raw OCR text. We ﬁnd that automated visual and text feature extraction work reasonably well and can allow for scaling to larger datasets, but that in our particular experiments the use of syntactic features adds little to the performance of the system, contrary to previous ﬁndings. 
The main aim of this work is to perform sentiment analysis on Urdu blog data. We use the method of structural correspondence learning (SCL) to transfer sentiment analysis learning from Urdu newswire data to Urdu blog data. The pivots needed to transfer learning from newswire domain to blog domain is not trivial as Urdu blog data, unlike newswire data is written in Latin script and exhibits codemixing and code-switching behavior. We consider two oracles to generate the pivots. 1. Transliteration oracle, to accommodate script variation and spelling variation and 2. Translation oracle, to accommodate code-switching and code-mixing behavior. In order to identify strong candidates for translation, we propose a novel part-of-speech tagging method that helps select words based on POS categories that strongly reflect code-mixing behavior. We validate our approach against a supervised learning method and show that the performance of our proposed approach is comparable. 
Improving mental wellness with preventive measures can help people at risk of experiencing mental health conditions such as depression or post-traumatic stress disorder. We describe an encouraging study on how automatic analysis of short written texts based on relevant linguistic text features can be used to identify whether the authors of such texts are experiencing distress. Such a computational model can be useful in developing an early warning system able to analyze writing samples for signs of mental distress. This could serve as a red flag, signaling when someone might need a professional assessment by a clinician. This paper reports on classification of distressed and non-distressed short, written excerpts from relevant web forums, using features automatically extracted from input text. Varying the value of k in k-fold crossvalidation shows that both coarse-grained and fine-grained automatic classification of affect states are generally 20% more accurate in detecting affect state than randomly assigning a distress label to a text. The study also compares the importance of bundled linguistic superfactors with a 2k factorial model. Analyzing the importance of different linguistic features for this task indicates main effects of affect word list matches, pronouns, and parts of speech in the predictive model. Excerpt length contributed to interaction effects. 
We present an approach to detecting hate speech in online text, where hate speech is deﬁned as abusive speech targeting speciﬁc group characteristics, such as ethnic origin, religion, gender, or sexual orientation. While hate speech against any group may exhibit some common characteristics, we have observed that hatred against each different group is typically characterized by the use of a small set of high frequency stereotypical words; however, such words may be used in either a positive or a negative sense, making our task similar to that of words sense disambiguation. In this paper we describe our deﬁnition of hate speech, the collection and annotation of our hate speech corpus, and a mechanism for detecting some commonly used methods of evading common “dirty word” ﬁlters. We describe pilot classiﬁcation experiments in which we classify anti-semitic speech reaching an accuracy 94%, precision of 68% and recall at 60%, for an F1 measure of .6375. 
We examine the response to the recent natural disaster Hurricane Irene on Twitter.com. We collect over 65,000 Twitter messages relating to Hurricane Irene from August 18th to August 31st, 2011, and group them by location and gender. We train a sentiment classiﬁer to categorize messages based on level of concern, and then use this classiﬁer to investigate demographic differences. We report three principal ﬁndings: (1) the number of Twitter messages related to Hurricane Irene in directly affected regions peaks around the time the hurricane hits that region; (2) the level of concern in the days leading up to the hurricane’s arrival is dependent on region; and (3) the level of concern is dependent on gender, with females being more likely to express concern than males. Qualitative linguistic variations further support these differences. We conclude that social media analysis provides a viable, real-time complement to traditional survey methods for understanding public perception towards an impending disaster. Introduction 
 Jacob Andreas1**  Kathleen McKeown1*  Owen Rambow2†  
What makes a tweet worth sharing? We study the content of tweets to uncover linguistic tendencies of shared microblog posts (retweets), by examining surface linguistic features, deeper parse-based features and Twitterspeciﬁc conventions in tweet content. We show how these features correlate with a functional classiﬁcation of tweets, thereby categorizing people’s writing styles based on their different intentions on Twitter. We ﬁnd that both linguistic features and functional classiﬁcation contribute to re-tweeting. Our work shows that opinion tweets favor originality and pithiness and that update tweets favor direct statements of a tweeter’s current activity. Judicious use of #hashtags also helps to encourage retweeting. 
occur broadly in many languages, hence our  This class of emoticon is far more varied and pro-  approach is language agnostic. Rather than relying on regular expressions over a predeﬁned set of likely tokens, we build weighted context-free grammars that reward graphical afﬁnity and symmetry within whatever symbols are used to construct the emoticon.  ductive than the sideways European style emoticons, and even lists of on the order of ten thousand emoticons will fail to cover all instances in even a modest sized sample of text. This relative productivity is due to several factors, including the horizontal orientation, which allows for more ﬂexibility in  
Social media services such as Twitter offer an immense volume of real-world linguistic data. We explore the use of Twitter to obtain authentic user-generated text in low-resource languages such as Nepali, Urdu, and Ukrainian. Automatic language identiﬁcation (LID) can be used to extract language-speciﬁc data from Twitter, but it is unclear how well LID performs on short, informal texts in low-resource languages. We address this question by annotating and releasing a large collection of tweets in nine languages, focusing on confusable languages using the Cyrillic, Arabic, and Devanagari scripts. This is the ﬁrst publiclyavailable collection of LID-annotated tweets in non-Latin scripts, and should become a standard evaluation set for LID systems. We also advance the state-of-the-art by evaluating new, highly-accurate LID systems, trained both on our new corpus and on standard materials only. Both types of systems achieve a huge performance improvement over the existing state-of-the-art, correctly classifying around 98% of our gold standard tweets. We provide a detailed analysis showing how the accuracy of our systems vary along certain dimensions, such as the tweet-length and the amount of in- and out-of-domain training data. 
Regardless of language, the standard character set for text messages (SMS) and many other social media platforms is the Roman alphabet. There are romanization conventions for some character sets, but they are used inconsistently in informal text, such as SMS. In this work, we convert informal, romanized Urdu messages into the native Arabic script and normalize non-standard SMS language. Doing so prepares the messages for existing downstream processing tools, such as machine translation, which are typically trained on well-formed, native script text. Our model combines information at the word and character levels, allowing it to handle out-of-vocabulary items. Compared with a baseline deterministic approach, our system reduces both word and character error rate by over 50%. 
A key challenge for dialogue-based intelligent tutoring systems lies in selecting follow-up questions that are not only context relevant but also encourage self-expression and stimulate learning. This paper presents an approach to ranking candidate questions for a given dialogue context and introduces an evaluation framework for this task. We learn to rank using judgments collected from expert human tutors, and we show that adding features derived from a rich, multi-layer dialogue act representation improves system performance over baseline lexical and syntactic features to a level in agreement with the judges. The experimental results highlight the important factors in modeling the questioning process. This work provides a framework for future work in automatic question generation and it represents a step toward the larger goal of directly learning tutorial dialogue policies directly from human examples. 
We present initial steps towards an interactive essay writing tutor that improves science knowledge by analyzing student essays for misconceptions and recommending science webpages that help correct those misconceptions. We describe the ﬁve components in this system: identifying core science concepts, determining appropriate pedagogical sequences for the science concepts, identifying student misconceptions in essays, aligning student misconceptions to science concepts, and recommending webpages to address misconceptions. We provide initial models and evaluations of the models for each component. 
The SAVE Science project is an attempt to address the shortcomings of current assessments of science. The project has developed two virtual worlds that each have a mystery or natural phenomenon requiring scientiﬁc explanation; by recording students’ behavior as they investigate the mystery, these worlds can be used to assess their understanding of the scientiﬁc method. Currently, however, the scoring of the assessment depends either on manual grading of students’ written responses, or, on multiple choice questions. This paper presents an automated grader that can combine with SAVE Science’s virtual worlds to provide a cheap mechanism for assessments of the ability to apply scientiﬁc methodology. In experiments on over 300 middle school students, our best automated grader improves by over 50% relative to the closest system from previous work in predicting grades supplied by human judges. 
To date, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence in the noisy domain of learner texts. We present the ﬁrst systematic analysis of several methods for assessing coherence under the framework of automated assessment (AA) of learner free-text responses. We examine the predictive power of different coherence models by measuring the effect on performance when combined with an AA system that achieves competitive results, but does not use discourse coherence features, which are also strong indicators of a learner’s level of attainment. Additionally, we identify new techniques that outperform previously developed ones and improve on the best published result for AA on a publically-available dataset of English learner free-text examination scripts. 
To date, most work in grammatical error correction has focused on targeting speciﬁc error types. We present a probe study into whether we can use round-trip translations obtained from Google Translate via 8 different pivot languages for whole-sentence grammatical error correction. We develop a novel alignment algorithm for combining multiple round-trip translations into a lattice using the TERp machine translation metric. We further implement six different methods for extracting whole-sentence corrections from the lattice. Our preliminary experiments yield fairly satisfactory results but leave signiﬁcant room for improvement. Most importantly, though, they make it clear the methods we propose have strong potential and require further study. 
We describe a study aimed at measuring the use of factual information in test-taker essays and assessing its effectiveness for predicting essay scores. We found medium correlations with the proposed measures, that remained signiﬁcant after the effect of essay length was factored out. The correlations did not differ substantionally between a simple, relatively robust measure vs a more sophisticated measure with better construct validity. Implications for development of automated essay scoring systems are discussed. 
We report two new approaches for building scoring models used by automated speech scoring systems. First, we introduce the Cumulative Logit Model (CLM), which has been widely used in modeling categorical outcomes in statistics. On a large set of responses to an English proﬁciency test, we systematically compare the CLM with two other scoring models that have been widely used, i.e., linear regression and decision trees. Our experiments suggest that the CLM has advantages in its scoring performance and its robustness to limited-sized training data. Second, we propose a novel way to utilize human rating processes in automated speech scoring. Applying accurate human ratings on a small set of responses can improve the whole scoring system’s performance while meeting cost and score-reporting time requirements. We ﬁnd that the scoring difﬁculty of each speech response, which could be modeled by the degree to which it challenged human raters, could provide a way to select an optimal set of responses for the application of human scoring. In a simulation, we show that focusing on challenging responses can achieve a larger scoring performance improvement than simply applying human scoring on the same number of randomly selected responses. 
Paraphrasing is an important aspect of language competence; however, EFL learners have long had difficulty paraphrasing in their writing owing to their limited language proficiency. Therefore, automatic paraphrase suggestion systems can be useful for writers. In this paper, we present PREFER1, a paraphrase reference tool for helping language learners improve their writing skills. In this paper, we attempt to transform the paraphrase generation problem into a graphical problem in which the phrases are treated as nodes and translation similarities as edges. We adopt the PageRank algorithm to rank and filter the paraphrases generated by the pivot-based paraphrase generation method. We manually evaluate the performance of our method and assess the effectiveness of PREFER in language learning. The results show that our method successfully preserves both the semantic meaning and syntactic structure of the query phrase. Moreover, the students’ writing performance improve most with the assistance of PREFER. 1. Introduction Paraphrasing, or restating information using different words, is an essential part of productive language competence (Fuchs, 1980; Mel’čuk, 1992; Martinot, 2003). However, EFL learners have difficulty paraphrasing in their writing partly 
This paper presents an exploration into automated content scoring of non-native spontaneous speech using ontology-based information to enhance a vector space approach. We use content vector analysis as a baseline and evaluate the correlations between human rater proficiency scores and two cosine-similarity-based features, previously used in the context of automated essay scoring. We use two ontology-facilitated approaches to improve feature correlations by exploiting the semantic knowledge encoded in WordNet: (1) extending word vectors with semantic concepts from the WordNet ontology (synsets); and (2) using a reasoning approach for estimating the concept weights of concepts not present in the set of training responses by exploiting the hierarchical structure of WordNet. Furthermore, we compare features computed from human transcriptions of spoken responses with features based on output from an automatic speech recognizer. We find that (1) for one of the two features, both ontologically based approaches improve average feature correlations with human scores, and that (2) the correlations for both features decrease only marginally when moving from human speech transcriptions to speech recognizer output. 
We develop a system for predicting the level of language learners, using only a small amount of targeted language data. In particular, we focus on learners of Hebrew and predict level based on restricted placement exam exercises. As with many language teaching situations, a major problem is data sparsity, which we account for in our feature selection, learning algorithm, and in the setup. Speciﬁcally, we deﬁne a two-phase classiﬁcation process, isolating individual errors and linguistic constructions which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features. 
In this paper we present a new spell-checking system that utilizes contextual information for automatic correction of non-word misspellings. The system is evaluated with a large corpus of essays written by native and nonnative speakers of English to the writing prompts of high-stakes standardized tests (TOEFL® and GRE®). We also present comparative evaluations with Aspell and the speller from Microsoft Office 2007. Using context-informed re-ranking of candidate suggestions, our system exhibits superior errorcorrection results overall and also corrects errors generated by non-native English writers with almost same rate of success as it does for writers who are native English speakers. 
Prior work has shown the utility of syntactic tree fragments as features in judging the grammaticality of text. To date such fragments have been extracted from derivations of Bayesianinduced Tree Substitution Grammars (TSGs). Evaluating on discriminative coarse and ﬁne grammaticality classiﬁcation tasks, we show that a simple, deterministic, count-based approach to fragment identiﬁcation performs on par with the more complicated grammars of Post (2011). This represents a signiﬁcant reduction in complexity for those interested in the use of such fragments in the development of systems for the educational domain. 
Accuracy of content have not been fully utilized in the previous studies on automated speaking assessment. Compared to writing tests, responses in speaking tests are noisy (due to recognition errors), full of incomplete sentences, and short. To handle these challenges for doing content-scoring in speaking tests, we propose two new methods based on information extraction (IE) and machine learning. Compared to using an ordinary content-scoring method based on vector analysis, which is widely used for scoring written essays, our proposed methods provided content features with higher correlations to human holistic scores. 
This paper describes and evaluates DQGen, which automatically generates multiple choice cloze questions to test a child’s comprehension while reading a given text. Unlike previous methods, it generates different types of distracters designed to diagnose different types of comprehension failure, and tests comprehension not only of an individual sentence but of the context that precedes it. We evaluate the quality of the overall questions and the individual distracters, according to 8 human judges blind to the correct answers and intended distracter types. The results, errors, and judges’ comments reveal limitations and suggest how to address some of them. 
Grammar exercises for language learning fall into two distinct classes: those that are based on “real life sentences” extracted from existing documents or from the web; and those that seek to facilitate language acquisition by presenting the learner with exercises whose syntax is as simple as possible and whose vocabulary is restricted to that contained in the textbook being used. In this paper, we introduce a framework (called GramEx) which permits generating the second type of grammar exercises. Using generation techniques, we show that a grammar can be used to semi-automatically generate grammar exercises which target a speciﬁc learning goal; are made of short, simple sentences; and whose vocabulary is restricted to that used in a given textbook. 
We present in this paper a novel, optimal semantic similarity approach based on word-to-word similarity metrics to solve the important task of assessing natural language student input in dialogue-based intelligent tutoring systems. The optimal matching is guaranteed using the sailor assignment problem, also known as the job assignment problem, a well-known combinatorial optimization problem. We compare the optimal matching method with a greedy method as well as with a baseline method on data sets from two intelligent tutoring systems, AutoTutor and iSTART. Introduction We address in this paper the important task of assessing natural language student input in dialogue-based tutoring systems where the primary form of interaction is natural language. Students provide their responses to tutor’s requests by typing or speaking their responses. Therefore, in dialogue-based tutoring systems understanding students’ natural language input becomes a crucial step towards building an accurate student model, i.e. assessing the student’s level of understanding, which in turn is important for optimum feedback and scaffolding and ultimately impacts the tutoring’s effectiveness at inducing learning gains on the student user.  We adopt a semantic similarity approach to assess students’ natural language input in intelligent tutoring systems. The semantic similarity approach to language understanding derives the meaning of a target text, e.g. a student sentence, by comparing it with another text whose meaning is known. If the target text is semantically similar to the known-meaning text then we know the target’s meaning as well. Semantic similarity is one of the two major approaches to language understanding, a central topic in Artificial Intelligence. The alternative approach is full understanding. The full understanding approach is not scalable due to prohibitive costs to encode world and domain knowledge which are needed for full understanding of natural language. 
We investigate the problem of readability assessment using a range of lexical and syntactic features and study their impact on predicting the grade level of texts. As empirical basis, we combined two web-based text sources, Weekly Reader and BBC Bitesize, targeting different age groups, to cover a broad range of school grades. On the conceptual side, we explore the use of lexical and syntactic measures originally designed to measure language development in the production of second language learners. We show that the developmental measures from Second Language Acquisition (SLA) research when combined with traditional readability features such as word length and sentence length provide a good indication of text readability across different grades. The resulting classiﬁers signiﬁcantly outperform the previous approaches on readability classiﬁcation, reaching a classiﬁcation accuracy of 93.3%. 
 2 Design Goals  This paper presents an interactive analytic tool for educational peer-review analysis. It employs data visualization at multiple levels of granularity, and provides automated analytic support using clustering and natural language processing. This tool helps instructors discover interesting patterns in writing performance that are reﬂected through peer reviews. 
This study presents a method that assesses ESL learners’ vocabulary usage to improve an automated scoring system of spontaneous speech responses by non-native English speakers. Focusing on vocabulary sophistication, we estimate the difﬁculty of each word in the vocabulary based on its frequency in a reference corpus and assess the mean difﬁculty level of the vocabulary usage across the responses (vocabulary proﬁle). Three different classes of features were generated based on the words in a spoken response: coverage-related, average word rank and the average word frequency and the extent to which they inﬂuence human-assigned language proﬁciency scores was studied. Among these three types of features, the average word frequency showed the most predictive power. We then explored the impact of vocabulary proﬁle features in an automated speech scoring context, with particular focus on the impact of two factors: genre of reference corpora and the characteristics of item-types. The contribution of the current study lies in the use of vocabulary proﬁle as a measure of lexical sophistication for spoken language assessment, an aspect heretofore unexplored in the context of automated speech scoring. 
A number of different research subﬁelds are concerned with the automatic assessment of student answers to comprehension questions, from language learning contexts to computer science exams. They share the need to evaluate free-text answers but differ in task setting and grading/evaluation criteria, among others. This paper has the intention of fostering synergy between the different research strands. It discusses the different research strands, details the crucial differences, and explores under which circumstances systems can be compared given publicly available data. To that end, we present results with the CoMiC-EN Content Assessment system (Meurers et al., 2011a) on the dataset published by Mohler et al. (2011) and outline what was necessary to perform this comparison. We conclude with a general discussion on comparability and evaluation of short answer assessment systems. 
Previous work on automated error recognition and correction of texts written by learners of English as a Second Language has demonstrated experimentally that training classiﬁers on error-annotated ESL text generally outperforms training on native text alone and that adaptation of error correction models to the native language (L1) of the writer improves performance. Nevertheless, most extant models have poor precision, particularly when attempting error correction, and this limits their usefulness in practical applications requiring feedback. We experiment with various feature types, varying quantities of error-corrected data, and generic versus L1-speciﬁc adaptation to typical errors using Na¨ıve Bayes (NB) classiﬁers and develop one model which maximizes precision. We report and discuss the results for 8 models, 5 trained on the HOO data and 3 (partly) on the full error-coded Cambridge Learner Corpus, from which the HOO data is drawn. 
The growth of open-access technical publications and other open-domain textual information sources means that there is an increasing amount of online technical material that is in principle available to all, but in practice, incomprehensible to most. We propose to address the task of helping readers comprehend complex technical material, by using statistical methods to model the “prerequisite structure” of a corpus — i.e., the semantic impact of documents on an individual reader’s state of knowledge. Experimental results using Wikipedia as the corpus suggest that this task can be approached by crowdsourcing the production of ground-truth labels regarding prerequisite structure, and then generalizing these labels using a learned classiﬁer which combines signals of various sorts. The features that we consider relate pairs of pages by analyzing not only textual features of the pages, but also how the containing corpora is connected and created. 
To support vocabulary acquisition and reading comprehension in a second language, we have developed a system to display senseappropriate examples to learners for difﬁcult words. We describe the construction of the system, incorporating word sense disambiguation, and an experiment we conducted testing it on a group of 60 learners of English as a second language (ESL). We show that sensespeciﬁc information in an intelligent reading system helps learners in their vocabulary acquisition, even if the sense information contains some noise from automatic processing. We also show that it helps learners, to some extent, with their reading comprehension. 
There is a rise in interest in the evaluation of meaning in real-life applications, e.g., for assessing the content of short answers. The approaches typically use a combination of shallow and deep representations, but little use is made of the semantic formalisms created by theoretical linguists to represent meaning. In this paper, we explore the use of the underspeciﬁed semantic formalism LRS, which combines the capability of precisely representing semantic distinctions with the robustness and modularity needed to represent meaning in real-life applications. We show that a content-assessment approach built on LRS outperforms a previous approach on the CREG data set, a freely available corpus of answers to reading comprehension exercises by learners of German. The use of such a formalism also readily supports the integration of notions building on semantic distinctions, such as the information structuring in discourse, which we show to be useful for content assessment. 
The frame-semantic parsing task is challenging for supervised techniques, even for those few languages where relatively large amounts of labeled data are available. In this preliminary work, we consider unsupervised induction of frame-semantic representations. An existing state-of-the-art Bayesian model for PropBank-style unsupervised semantic role induction (Titov and Klementiev, 2012) is extended to jointly induce semantic frames and their roles. We evaluate the model performance both quantitatively and qualitatively by comparing the induced representation against FrameNet annotations. 
In our experiment, we evaluate the transferability of frames from Swedish to Finnish in parallel corpora. We evaluate both the theoretical possibility of transferring frames and the possibility of performing it using available lexical resources. We add the frame information to an extract of the Swedish side of the Kotus and JRC-Acquis corpora using an automatic frame labeler and copy it to the Finnish side. We focus on evaluating the results to get an estimation on how often the parallel sentences can be said to express the same frame. This sheds light on the questions: Are the same situations in the two languages expressed using different frames, i.e. are the frames transferable even in theory? How well can the frame information of running text be transferred from one language to another? 
We show that orthographic cues can be helpful for unsupervised parsing. In the Penn Treebank, transitions between upper- and lowercase tokens tend to align with the boundaries of base (English) noun phrases. Such signals can be used as partial bracketing constraints to train a grammar inducer: in our experiments, directed dependency accuracy increased by 2.2% (average over 14 languages having case information). Combining capitalization with punctuation-induced constraints in inference further improved parsing performance, attaining state-of-the-art levels for many languages. 
We provide a model that extends the splitmerge framework of Petrov et al. (2006) to jointly learn latent annotations and Tree Substitution Grammars (TSGs). We then conduct a variety of experiments with this model, first inducing grammars on a portion of the Penn Treebank and the Korean Treebank 2.0, and next experimenting with grammar refinement from a single nonterminal and from the Universal Part of Speech tagset. We present qualitative analysis showing promising signs across all experiments that our combined approach successfully provides for greater flexibility in grammar induction within the structured guidance provided by the treebank, leveraging the complementary natures of these two approaches. 
For many NLP tasks, EM-trained HMMs are the common models. However, in order to escape local maxima and ﬁnd the best model, we need to start with a good initial model. Researchers suggested repeated random restarts or constraints that guide the model evolution. Neither approach is ideal. Restarts are time-intensive, and most constraint-based approaches require serious re-engineering or external solvers. In this paper we measure the effectiveness of very limited initial constraints: speciﬁcally, annotations of a small number of words in the training data. We vary the amount and distribution of initial partial annotations, and compare the results to unsupervised and supervised approaches. We ﬁnd that partial annotations improve accuracy and can reduce the need for random restarts, which speeds up training time considerably. 
Some of the most used models for statistical word alignment are the IBM models. Although these models generate acceptable alignments, they do not exploit the rich information found in lexical resources, and as such have no reasonable means to choose better translations for speciﬁc senses. We try to address this issue by extending the IBM HMM model with an extra hidden layer which represents the senses a word can take, allowing similar words to share similar output distributions. We test a preliminary version of this model on English-French data. We compare different ways of generating senses and assess the quality of the alignments relative to the IBM HMM model, as well as the generated sense probabilities, in order to gauge the usefulness in Word Sense Disambiguation. 
As linguistic models incorporate more subtle nuances of language and its structure, standard inference techniques can fall behind. Often, such models are tightly coupled such that they defy clever dynamic programming tricks. However, Sequential Monte Carlo (SMC) approaches, i.e. particle ﬁlters, are well suited to approximating such models, resolving their multi-modal nature at the cost of generating additional samples. We implement two particle ﬁlters, which jointly sample either sentences or word types, and incorporate them into a Gibbs sampler for part-of-speech (PoS) inference. We analyze the behavior of the particle ﬁlters, and compare them to a block sentence sampler, a local token sampler, and a heuristic sampler, which constrains inference to a single PoS per word type. Our ﬁndings show that particle ﬁlters can closely approximate a difﬁcult or even intractable sampler quickly. However, we found that high posterior likelihood do not necessarily correspond to better Many-to-One accuracy. The results suggest that the approach has potential and more advanced particle ﬁlters are likely to lead to stronger performance. 
In this paper, we study direct transfer methods for multilingual named entity recognition. Speciﬁcally, we extend the method recently proposed by Ta¨ckstro¨m et al. (2012), which is based on cross-lingual word cluster features. First, we show that by using multiple source languages, combined with self-training for target language adaptation, we can achieve signiﬁcant improvements compared to using only single source direct transfer. Second, we investigate how the direct transfer system fares against a supervised target language system and conclude that between 8,000 and 16,000 word tokens need to be annotated in each target language to match the best direct transfer system. Finally, we show that we can significantly improve target language performance, even after annotating up to 64,000 tokens in the target language, by simply concatenating source and target language annotations. 
This paper presents the results of the PASCAL Challenge on Grammar Induction, a competition in which competitors sought to predict part-of-speech and dependency syntax from text. Although many previous competitions have featured dependency grammars or partsof-speech, these were invariably framed as supervised learning and/or domain adaption. This is the ﬁrst challenge to evaluate unsupervised induction systems, a sub-ﬁeld of syntax which is rapidly becoming very popular. Our challenge made use of a 10 different treebanks annotated in a range of different linguistic formalisms and covering 9 languages. We provide an overview of the approaches taken by the participants, and evaluate their results on each dataset using a range of different evaluation metrics. 
Results in unsupervised dependency parsing are typically compared to branching baselines and the DMV-EM parser of Klein and Manning (2004). State-of-the-art results are now well beyond these baselines. This paper describes two simple, heuristic baselines that are much harder to beat: a simple, heuristic algorithm recently presented in Søgaard (2012) and a heuristic application of the universal rules presented in Naseem et al. (2010). Our ﬁrst baseline (RANK) outperforms existing baselines, including PR-DVM (Gillenwater et al., 2010), while relying only on raw text, but all submitted systems in the Pascal Grammar Induction Challenge score better. Our second baseline (RULES), however, outperforms several submitted systems. 
This paper describes a system for unsupervised dependency parsing based on Gibbs sampling algorithm. The novel approach introduces a fertility model and reducibility model, which assumes that dependent words can be removed from a sentence without violating its syntactic correctness. 
Our system consists of a simple, EM-based induction algorithm (Bisk and Hockenmaier, 2012), which induces a language-speciﬁc Combinatory Categorial grammar (CCG) and lexicon based on a small number of linguistic principles, e.g. that verbs may be the roots of sentences and can take nouns as arguments. 
 where the induced categories provide an interme-  We propose an unsupervised approach to POS tagging where ﬁrst we associate each word type with a probability distribution over word classes using Latent Dirichlet Allocation. Then we create a hierarchical clustering of the word types: we use an agglomerative clustering algorithm where the distance  diate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, Ta¨ckstro¨m et al. 2012). The main difference from the part-of-speech setting is that the focus is on evaluating the performance of the learned categories in  between clusters is deﬁned as the JensenShannon divergence between the probability distributions over classes associated with each word-type. When assigning POS tags, we ﬁnd the tree leaf most similar to the current word and use the preﬁx of the path leading to this leaf as the tag. This simple labeler outperforms a baseline based on Brown clusters on 9 out of 10 datasets.  real tasks rather than on measuring how closely they match gold part-of-speech tags. Some researchers have used both approaches to evaluation. This difference in evaluation methodology also naturally leads to differing constraints on the nature of the induced representations. For part-of-speech tagging what is needed is a mapping from word tokens to a small set of discrete, atomic labels. For  feature learning, there are is no such limitation, and  
In this paper we describe our participating system for the dependency induction track of the PASCAL Challenge on Grammar Induction. Our system incorporates two types of inductive biases: the sparsity bias and the unambiguity bias. The sparsity bias favors a grammar with fewer grammar rules. The unambiguity bias favors a grammar that leads to unambiguous parses, which is motivated by the observation that natural language is remarkably unambiguous in the sense that the number of plausible parses of a natural language sentence is very small. We introduce our approach to combining these two types of biases and discuss the system implementation. Our experiments show that both types of inductive biases are beneﬁcial to grammar induction. 
In the last two decades, information-seeking spoken dialog systems (SDS) have moved from research prototypes to real-life commercial applications. Still, dialog systems are limited by the scale, complexity of the task and coverage of knowledge required by problemsolving machines or mobile personal assistants. Future spoken interaction are required to be multilingual, understand and act on large scale knowledge bases in all its forms (from structured to unstructured). The Web research community have striven to build large scale and open multilingual resources (e.g. Wikipedia) and knowledge bases (e.g. Yago). We argue that a) it is crucial to leverage this massive amount of Web lightly structured knowledge and b) the scale issue can be addressed collaboratively and design open standards to make tools and resources available to the whole speech and language community. 
To increase impact and accelerate progress, the spoken dialog systems research community should work on four shareable things that will also engage and support sister ﬁelds of science and engineering. 
We argue that standardized metrics and automatic evaluation tools are necessary for speeding up knowledge generation and development processes for dialog systems. 
We discuss a change of perspective for training dialogue systems, which requires a shift from traditional empirical methods to online learning methods. We motivate the application of online learning, which provides the beneﬁt of improving the system’s behaviour continuously often after each turn or dialogue rather than after hundreds of dialogues. We describe the requirements and advances for dialogue systems with online learning, and speculate on the future of these kinds of systems.  Training activity  Online learning Oﬄine learning Collected dialogues Figure 1: Learning approaches for dialogue systems. Whilst ofﬂine learning aims for discontinuous learning, online learning aims for continuous learning while interacting with users in a real environment.  
There has been a lot of interest for user simulation in the ﬁeld of spoken dialogue systems during the last decades. User simulation was ﬁrst proposed to assess the performance of SDS before a public release. Since the late 90’s, user simulation is also used for dialogue management optimisation. In this position paper, we focus on statistical methods for user simulation, their main advantages and drawbacks. We initiate a reﬂection about the utility of such methods and give some insights of what their future should be. 
A sketch of dialogue systems as long-term adaptive, conversational agents. 
We outline a set of key challenges for dialog management in physically situated interactive systems, and propose a core shift in perspective that places spoken dialog in the context of the larger collaborative challenge of managing parallel, coordinated actions in the open world. 
Strict-turn taking models of dialogue do not accurately model human incremental processing, where users can process partial input and plan partial utterances in parallel. We discuss the current state of the art in incremental systems and propose tools and data required for further advances in the ﬁeld of Incremental Spoken Dialogue Systems. 
These new dialog systems are different from traditional ones in several ways; they are multi-task, asynchronous, can involve rich context modeling, and have side effects in the “real world”: Multi-task – The system interacts with the user to accomplish a series of (possibly related) tasks. For example, a user might use the system to order a book and then say schedule it for book club - a different task (e.g. requiring different backend DB lookups) but related to the previous one by the book informa- 
Spoken dialog systems frameworks ﬁll a crucial role in the spoken dialog systems community by providing resources to lower barriers to entry. However, different user groups have different requirements and expectations for such systems. Here, we consider the particular needs for spoken dialog systems toolkits within an instructional setting. We discuss the challenges for existing systems in meeting these needs and propose strategies to overcome them. 
Belief tracking is a promising technique for adding robustness to spoken dialog systems, but current research is fractured across different teams, techniques, and domains. This paper ampliﬁes past informal discussions (Raux, 2011) to call for a belief tracking challenge task, based on the Spoken dialog challenge corpus (Black et al., 2011). Beneﬁts, limitations, evaluation design issues, and next steps are presented. 
We herein introduce our project of realizing a framework for the development of a spoken dialogue system based on collaboratively constructed semantic resources. We demonstrate that a semantic Web-oriented approach based on collaboratively constructed semantic resources significantly reduces troublesome rule descriptions and complex configurations, which are caused by the previous relational database-based approach, in the development process of spoken dialogue systems. In addition, we show that the proposed framework enables multilingual spoken dialogue system development due to clear separation of model, view and controller components. 
We describe the 2012 release of our “Incremental Processing Toolkit” (INPROTK)1, which combines a powerful and extensible architecture for incremental processing with components for incremental speech recognition and, new to this release, incremental speech synthesis. These components work fairly domainindependently; we also provide example implementations of higher-level components such as natural language understanding and dialogue management that are somewhat more tied to a particular domain. We offer this release of the toolkit to foster research in this new and exciting area, which promises to help increase the naturalness of behaviours that can be modelled in such systems. 
This paper introduces a simulation-based framework for performing action selection and understanding for interactive agents. By simulating the objects and actions relevant to an interaction, an agent can semantically ground natural language and interact considerately and on its own initiative in situated environments. The framework proposed in this paper leverages models of the environment, user and system to predict possible future world states via simulation. It leverages understanding of spoken language and multimodal input to estimate the state of the ongoing interaction and select actions based on the utility of future outcomes in the simulated world. In this paper we introduce this framework and demonstrate its effectiveness for incar navigation. 
In a spoken dialog system that can handle natural conversation between a human and a machine, spoken language understanding (SLU) is a crucial component aiming at capturing the key semantic components of utterances. Building a robust SLU system is a challenging task due to variability in the usage of language, need for labeled data, and requirements to expand to new domains (movies, travel, ﬁnance, etc.). In this paper, we survey recent research on bootstrapping or improving SLU systems by using information mined or extracted from web search query logs, which include (natural language) queries entered by users as well as the links (web sites) they click on. We focus on learning methods that help unveiling hidden information in search query logs via implicit crowd-sourcing. 
A lot. Since inception of Contender, a machine learning method tailored for computerassisted decision making in industrial spoken dialog systems, it was rolled out in over 200 instances throughout our applications processing nearly 40 million calls. The net effect of this data-driven method is a signiﬁcantly increased system performance gaining about 100,000 additional automated calls every month. 
Information about the quality of a Spoken Dialogue System (SDS) is usually used only for comparing SDSs with each other or manually improving the dialogue strategy. This information, however, provides a means for inherently improving the dialogue performance by adapting the Dialogue Manager during the interaction accordingly. For a quality metric to be suitable, it must sufﬁce certain conditions. Therefore, we address requirements for the quality metric and, additionally, present approaches for quality-adaptive dialogue management. 
Children acquire mental state verbs (MSVs) much later than other, lower-frequency, words. One factor proposed to contribute to this delay is that children must learn various semantic and syntactic cues that draw attention to the difﬁcult-to-observe mental content of a scene. We develop a novel computational approach that enables us to explore the role of such cues, and show that our model can replicate aspects of the developmental trajectory of MSV acquisition. 
For a given concrete noun concept, humans are usually able to cite properties (e.g., elephant is animal, car has wheels) of that concept; cognitive psychologists have theorised that such properties are fundamental to understanding the abstract mental representation of concepts in the brain. Consequently, the ability to automatically extract such properties would be of enormous beneﬁt to the ﬁeld of experimental psychology. This paper investigates the use of semi-supervised learning and support vector machines to automatically extract concept-relation-feature triples from two large corpora (Wikipedia and UKWAC) for concrete noun concepts. Previous approaches have relied on manually-generated rules and hand-crafted resources such as WordNet; our method requires neither yet achieves better performance than these prior approaches, measured both by comparison with a property norm-derived gold standard as well as direct human evaluation. Our technique performs particularly well on extracting features relevant to a given concept, and suggests a number of promising areas for future focus. 
Some of the most robust effects of linguistic variables on eye movements in reading are those of word length. Their leading explanation states that they are caused by visual acuity limitations on word recognition. However, Bicknell (2011) presented data showing that a model of eye movement control in reading that includes visual acuity limitations and models the process of word identiﬁcation from visual input (Bicknell & Levy, 2010) does not produce humanlike word length effects, providing evidence against the visual acuity account. Here, we argue that uncertainty about word length in early word identiﬁcation can drive word length effects. We present an extension of Bicknell and Levy’s model that incorporates word length uncertainty, and show that it produces more humanlike word length effects. 
We describe a computational framework for language learning and parsing in which dynamical systems navigate on fractal sets. We explore the predictions of the framework in an artiﬁcial grammar task in which humans and recurrent neural networks are trained on a language with recursive structure. The results provide evidence for the claim of the dynamical systems models that grammatical systems continuously metamorphose during learning. The present perspective permits structural comparison between the recursive representations in symbolic and neural network models. 
Probabilistic context-free grammars (PCFGs) are a popular cognitive model of syntax (Jurafsky, 1996). These can be formulated to be sensitive to human working memory constraints by application of a right-corner transform (Schuler, 2009). One side-effect of the transform is that it guarantees at most a single expansion (push) and at most a single reduction (pop) during a syntactic parse. The primary ﬁnding of this paper is that this property of right-corner parsing can be exploited to obtain a dramatic reduction in the number of random variables in a probabilistic sequence model parser. This yields a simpler structure that more closely resembles existing simple recurrent network models of sentence comprehension. 
Experimental evidence demonstrates that syntactic structure inﬂuences human online sentence processing behavior. Despite this evidence, open questions remain: which type of syntactic structure best explains observed behavior–hierarchical or sequential, and lexicalized or unlexicalized? Recently, Frank and Bod (2011) ﬁnd that unlexicalized sequential models predict reading times better than unlexicalized hierarchical models, relative to a baseline prediction model that takes wordlevel factors into account. They conclude that the human parser is insensitive to hierarchical syntactic structure. We investigate these claims and ﬁnd a picture more complicated than the one they present. First, we show that incorporating additional lexical n-gram probabilities estimated from several different corpora into the baseline model of Frank and Bod (2011) eliminates all differences in accuracy between those unlexicalized sequential and hierarchical models. Second, we show that lexicalizing the hierarchical models used in Frank and Bod (2011) signiﬁcantly improves prediction accuracy relative to the unlexicalized versions. Third, we show that using stateof-the-art lexicalized hierarchical models further improves prediction accuracy. Our results demonstrate that the claim of Frank and Bod (2011) that sequential models predict reading times better than hierarchical models is premature, and also that lexicalization matters for prediction accuracy.  
Logical metonymies (The student ﬁnished the beer) represent a challenge to compositionality since they involve semantic content not overtly realized in the sentence (covert events → drinking the beer). We present a contrastive study of two classes of computational models for logical metonymy in German, namely a probabilistic and a distributional, similarity-based model. These are built using the SDEWAC corpus and evaluated against a dataset from a self-paced reading and a probe recognition study for their sensitivity to thematic ﬁt effects via their accuracy in predicting the correct covert event in a metonymical context. The similarity-based models allow for better coverage while maintaining the accuracy of the probabilistic models. 
There is considerable evidence that people generally learn items better when the presentation of items is distributed over a period of time (the spacing effect). We hypothesize that both forgetting and attention to novelty play a role in the spacing effect in word learning. We build an incremental probabilistic computational model of word learning that incorporates a forgetting and attentional mechanism. Our model accounts for experimental results on children as well as several patterns observed in adults. 
Conversations in poster sessions in academic events, referred to as poster conversations, pose interesting and challenging topics on multi-modal analysis of multi-party dialogue. This article gives an overview of our project on multi-modal sensing, analysis and “understanding” of poster conversations. We focus on the audience’s feedback behaviors such as non-lexical backchannels (reactive tokens) and noddings as well as joint eye-gaze events by the presenter and the audience. We investigate whether we can predict when and who will ask what kind of questions, and also interest level of the audience. Based on these analyses, we design a smart posterboard which can sense human behaviors and annotate interactions and interest level during poster sessions. 
We present and evaluate two state-of-the art dialogue systems developed to support dialog with French speaking virtual characters in the context of a serious game: one hybrid statistical/symbolic and one purely statistical. We conducted a quantitative evaluation where we compare the accuracy of the interpreter and of the dialog manager used by each system; a user based evaluation based on 22 subjects using both the statistical and the hybrid system; and a corpus based evaluation where we examine such criteria as dialog coherence, dialog success, interpretation and generation errors in the corpus of Human-System interactions collected during the user-based evaluation. We show that although the statistical approach is slightly more robust, the hybrid strategy seems to be better at guiding the player through the game. 
One challenge of implementing spoken dialogue systems for long-term interaction is how to adapt the dialogue as user and system become more familiar. We believe this challenge includes evoking and signaling aspects of long-term relationships such as rapport. For tutoring systems, this may additionally require knowing how relationships are signaled among non-adult users. We therefore investigate conversational strategies used by teenagers in peer tutoring dialogues, and how these strategies function differently among friends or strangers. In particular, we use annotated and automatically extracted linguistic devices to predict impoliteness and positivity in the next turn. To take into account the sparse nature of these features in real data we use models including Lasso, ridge estimator, and elastic net. We evaluate the predictive power of our models under various settings, and compare our sparse models with standard non-sparse solutions. Our experiments demonstrate that our models are more accurate than non-sparse models quantitatively, and that teens use unexpected kinds of language to do relationship work such as signaling rapport, but friends and strangers, tutors and tutees, carry out this work in quite different ways from one another. 
The ability to monitor the communicative success of its utterances and, if necessary, provide feedback and repair is useful for a dialog system. We show that in situated communication, eyetracking can be used to reliably and efﬁciently monitor the hearer’s reference resolution process. An interactive system that draws on hearer gaze to provide positive or negative feedback after referring to objects outperforms baseline systems on metrics of referential success and user confusion. 
We present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify “summaryworthy” words. Concretely, a series of unsupervised topic models is explored and experimental results show that ﬁne-grained topic models, which discover topics at the utterance-level rather than the document-level, can better identify the gist of the decisionmaking process. Moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. 
This paper proposes an unsupervised approach to user simulation in order to automatically furnish updates and assessments of a deployed spoken dialog system. The proposed method adopts a dynamic Bayesian network to infer the unobservable true user action from which the parameters of other components are naturally derived. To verify the quality of the simulation, the proposed method was applied to the Let’s Go domain (Raux et al., 2005) and a set of measures was used to analyze the simulated data at several levels. The results showed a very close correspondence between the real and simulated data, implying that it is possible to create a realistic user simulator that does not necessitate human intervention. 
Conversational practices do not occur at a single unit of analysis. To understand the interplay between social positioning, information sharing, and rhetorical strategy in language, various granularities are necessary. In this work we present a machine learning model for multi-party chat which predicts conversation structure across differing units of analysis. First, we mark sentence-level behavior using an information sharing annotation scheme. By taking advantage of Integer Linear Programming and a sociolinguistic framework, we enforce structural relationships between sentence-level annotations and sequences of interaction. Then, we show that clustering these sequences can effectively disentangle the threads of conversation. This model is highly accurate, performing near human accuracy, and performs analysis on-line, opening the door to real-time analysis of the discourse of conversation. 
We herein propose a method for the rapid development of a spoken dialogue system based on collaboratively constructed semantic resources and compare the proposed method with a conventional method that is based on a relational database. Previous development frameworks of spoken dialogue systems, which presuppose a relational database management system as a background application, require complex data definition, such as making entries in a task-dependent language dictionary, templates of semantic frames, and conversion rules from user utterances to the query language of the database. We demonstrate that a semantic web oriented approach based on collaboratively constructed semantic resources significantly reduces troublesome rule descriptions and complex configurations in the rapid development process of spoken dialogue systems. 
In recent years statistical dialogue systems have gained signiﬁcant attention due to their potential to be more robust to speech recognition errors. However, these systems must also be robust to changes in user behaviour caused by cognitive loading. In this paper, a statistical dialogue system providing restaurant information is evaluated in a set-up where the subjects used a driving simulator whilst talking to the system. The inﬂuences of cognitive loading were investigated and some clear differences in behaviour were discovered. In particular, it was found that users chose to respond to different system questions and use different speaking styles, which indicate the need for an incremental dialogue approach. 
Recent work on consultations between outpatients with schizophrenia and psychiatrists has shown that adherence to treatment can be predicted by patterns of repair – speciﬁcally, the pro-activity of the patient in checking their understanding, i.e. patient clariﬁcation. Using machine learning techniques, we investigate whether this tendency can be predicted from high-level dialogue features, such as backchannels, overlap and each participant’s proportion of talk. The results indicate that these features are not predictive of a patient’s adherence to treatment or satisfaction with the communication, although they do have some association with symptoms. However, all these can be predicted if we allow features at the word level. These preliminary experiments indicate that patient adherence is predictable from dialogue transcripts, but further work is necessary to develop a meaningful, general and reliable feature set. 
We use Reinforcement Learning (RL) to learn question-answering dialogue policies for a real-world application. We analyze a corpus of interactions of museum visitors with two virtual characters that serve as guides at the Museum of Science in Boston, in order to build a realistic model of user behavior when interacting with these characters. A simulated user is built based on this model and used for learning the dialogue policy of the virtual characters using RL. Our learned policy outperforms two baselines (including the original dialogue policy that was used for collecting the corpus) in a simulation setting. 
Convergence is thought to be an important phenomenon in dialogue through which interlocutors adapt to each other. Yet, its mechanisms and relationship to dialogue outcomes are not fully understood. This paper explores convergence in textual task-oriented dialogue during a longitudinal study. The results suggest that over time, convergence between interlocutors increases with successive dialogues. Additionally, for the tutorial dialogue domain at hand, convergence metrics were found to be significant predictors of dialogue outcomes such as learning, mental effort, and emotional states including frustration, boredom, and confusion. The results suggest ways in which dialogue systems may leverage convergence to enhance their interactions with users. 
A robust system that understands route instructions should be able to process instructions generated naturally by humans. Also desirable would be the ability to handle repairs and other modiﬁcations to existing instructions. To this end, we collected a corpus of spoken instructions (and modiﬁed instructions) produced by subjects provided with an origin and a destination. We found that instructions could be classiﬁed into four categories, depending on their intent such as imperative, feedback, or meta comment. We asked a different set of subjects to follow these instructions to determine the usefulness and comprehensibility of individual instructions. Finally, we constructed a semantic grammar and evaluated its coverage. To determine whether instructiongiving forms a predictable sub-language, we tested the grammar on three corpora collected by others and determined that this was largely the case. Our work suggests that predictable sub-languages may exist for well-deﬁned tasks. Index Terms: Robot Navigation, Spoken Instructions 
We provide a systematic study of previously proposed features for implicit discourse relation identiﬁcation, identifying new feature combinations that optimize F1-score. The resulting classiﬁers achieve the best F1-scores to date for the four top-level discourse relation classes of the Penn Discourse Tree Bank: COMPARISON, CONTINGENCY, EXPANSION, and TEMPORAL. We further identify factors for feature extraction that can have a major impact on performance and determine that some features originally proposed for the task no longer provide performance gains in light of more powerful, recently discovered features. Our results constitute a new set of baselines for future studies of implicit discourse relation identiﬁcation. 
Developing sophisticated turn-taking behavior is necessary for next-generation dialogue systems. However, incorporating real users into the development cycle is expensive and current simulation techniques are inadequate. As a foundation for advancing turn-taking behavior, we present a temporal simulator that models the interaction between the user and the system, including speech, voice activity detection, and incremental speech recognition. We describe the details of the simulator and demonstrate it on a sample domain. 
In this work we study the effectiveness of speaker adaptation for dialogue act recognition in multiparty meetings. First, we analyze idiosyncracy in dialogue verbal acts by qualitatively studying the differences and conﬂicts among speakers and by quantitively comparing speaker-speciﬁc models. Based on these observations, we propose a new approach for dialogue act recognition based on reweighted domain adaptation which effectively balance the inﬂuence of speaker speciﬁc and other speakers’ data. Our experiments on a realworld meeting dataset show that with even only 200 speaker-speciﬁc annotated dialogue acts, the performances on dialogue act recognition are signiﬁcantly improved when compared to several baseline algorithms. To our knowledge, this work is the ﬁrst 1 to tackle this promising research direction of speaker adaptation for dialogue act recogntion. 
This work investigates to what degree speakers with different verbal intelligence may adapt to each other. The work is based on a corpus consisting of 100 descriptions of a short ﬁlm (monologues), 56 discussions about the same topic (dialogues), and verbal intelligence scores of the test participants. Adaptation between two dialogue partners was measured using cross-referencing, proportion of “I”, “You” and “We” words, between-subject correlation and similarity of texts. It was shown that lower verbal intelligence speakers repeated more nouns and adjectives from the other and used the same linguistic categories more often than higher verbal intelligence speakers. In dialogues between strangers, participants with higher verbal intelligence showed a greater level of adaptation. 
We demonstrate a spoken dialogue-based information system for pedestrians. The system is novel in combining geographic information system (GIS) modules such as a visibility engine with a question-answering (QA) system, integrated within a dialogue system architecture. Users of the demonstration system can use a web-based version (simulating pedestrian movement using StreetView) to engage in a variety of interleaved conversations such as navigating from A to B, using the QA functionality to learn more about points of interest (PoI) nearby, and searching for amenities and tourist attractions. This system explores a variety of research questions involving the integration of multiple information sources within conversational interaction. 
 We present a mixed initiative conversational dialogue system designed to address primarily mental health care concerns related to military deployment. It is supported by a new information-state based dialogue manager, FLoReS (Forward-Looking, Reward Seeking dialogue manager), that allows both advanced, ﬂexible, mixed initiative interaction, and efﬁcient policy creation by domain experts. To easily reach its target population this dialogue system is accessible as a web application. 
To enable effective referential grounding in situated human robot dialogue, we have conducted an empirical study to investigate how conversation partners collaborate and mediate shared basis when they have mismatched visual perceptual capabilities. In particular, we have developed a graph-based representation to capture linguistic discourse and visual discourse, and applied inexact graph matching to ground references. Our empirical results have shown that, even when computer vision algorithms produce many errors (e.g. 84.7% of the objects in the environment are mis-recognized), our approach can still achieve 66% accuracy in referential grounding. These results demonstrate that, due to its error-tolerance nature, inexact graph matching provides a potential solution to mediate shared perceptual basis for referential grounding in situated interaction. 
A coherently related group of sentences may be referred to as a discourse. In this paper we address the problem of parsing coherence relations as deﬁned in the Penn Discourse Tree Bank (PDTB). A good model for discourse structure analysis needs to account both for local dependencies at the token-level and for global dependencies and statistics. We present techniques on using inter-sentential or sentence-level (global), data-driven, nongrammatical features in the task of parsing discourse. The parser model follows up previous approach based on using tokenlevel (local) features with conditional random ﬁelds for shallow discourse parsing, which is lacking in structural knowledge of discourse. The parser adopts a twostage approach where ﬁrst the local constraints are applied and then global constraints are used on a reduced weighted search space (n-best). In the latter stage we experiment with different rerankers trained on the ﬁrst stage n-best parses, which are generated using lexico-syntactic local features. The two-stage parser yields signiﬁcant improvements over the best performing model of discourse parser on the PDTB corpus. 
 This paper presents a discriminative reranking model for the discourse segmentation task, the ﬁrst step in a discourse parsing system. Our model exploits subtree features to rerank Nbest outputs of a base segmenter, which uses syntactic and lexical features in a CRF framework. Experimental results on the RST Discourse Treebank corpus show that our model outperforms existing discourse segmenters in both settings that use gold standard Penn Treebank parse trees and Stanford parse trees. 
Many modern spoken dialog systems use probabilistic graphical models to update their belief over the concepts under discussion, increasing robustness in the face of noisy input. However, such models are ill-suited to probabilistic reasoning about spatial relationships between entities. In particular, a car navigation system that infers users’ intended destination using nearby landmarks as descriptions must be able to use distance measures as a factor in inference. In this paper, we describe a belief tracking system for a location identiﬁcation task that combines a semantic belief tracker for categorical concepts based on the DPOT framework (Raux and Ma, 2011) with a kernel density estimator that incorporates landmark evidence from multiple turns and landmark hypotheses, into a posterior probability over candidate locations. We evaluate our approach on a corpus of destination setting dialogs and show that it signiﬁcantly outperforms a deterministic baseline. 
Probabilistic models such as Bayesian Networks are now in widespread use in spoken dialogue systems, but their scalability to complex interaction domains remains a challenge. One central limitation is that the state space of such models grows exponentially with the problem size, which makes parameter estimation increasingly difﬁcult, especially for domains where only limited training data is available. In this paper, we show how to capture the underlying structure of a dialogue domain in terms of probabilistic rules operating on the dialogue state. The probabilistic rules are associated with a small, compact set of parameters that can be directly estimated from data. We argue that the introduction of this abstraction mechanism yields probabilistic models that are easier to learn and generalise better than their unstructured counterparts. We empirically demonstrate the beneﬁts of such an approach learning a dialogue policy for a human-robot interaction domain based on a Wizard-of-Oz data set. 
This paper proposes the use of unsupervised approaches to improve components of partition-based belief tracking systems. The proposed method adopts a dynamic Bayesian network to learn the user action model directly from a machine-transcribed dialog corpus. It also addresses conﬁdence score calibration to improve the observation model in a unsupervised manner using dialog-level grounding information. To verify the effectiveness of the proposed method, we applied it to the Let’s Go domain (Raux et al., 2005). Overall system performance for several comparative models were measured. The results show that the proposed method can learn an effective user action model without human intervention. In addition, the calibrated conﬁdence score was veriﬁed by demonstrating the positive inﬂuence on the user action model learning process and on overall system performance. 
Models of dialog state are important, both scientiﬁcally and practically, but today’s best build strongly on tradition. This paper presents a new way to identify the important dimensions of dialog state, more bottomup and empirical than previous approaches. Speciﬁcally, we applied Principal Component Analysis to a large number of low-level prosodic features to ﬁnd the most important dimensions of variation. The top 20 out of 76 dimensions accounted for 81% of the variance, and each of these dimensions clearly related to dialog states and activities, including turn taking, topic structure, grounding, empathy, cognitive processes, attitude and rhetorical structure. 
Addressee identiﬁcation is an element of all language-based interactions, and is critical for turn-taking. We examine the particular problem of identifying when each child playing an interactive game in a small group is speaking to an animated character. After analyzing child and adult behavior, we explore a family of machine learning models to integrate audio and visual features with temporal group interactions and limited, task-independent language. The best model performs identiﬁcation about 20% better than the model that uses the audio-visual features of the child alone. 
We evaluate a wizard-of-oz spoken dialogue system that adapts to multiple user affective states in real-time: user disengagement and uncertainty. We compare this version with the prior version of our system, which only adapts to user uncertainty. Our analysis investigates how iteratively adding new affect adaptation to an existing affect-adaptive system impacts global and local performance. We ﬁnd a signiﬁcant increase in motivation for users who most frequently received the disengagement adaptation. Moreover, responding to disengagement breaks its negative correlations with task success and user satisfaction, reduces uncertainty levels, and reduces the likelihood of continued disengagement. 
We propose a dialog system that creates responses based on a large-scale dialog corpus retrieved from Twitter and real-time crowdsourcing. Instead of using complex dialog management, our system replies with the utterance from the database that is most similar to the user input. We also propose a realtime crowdsourcing framework for handling the case in which there is no adequate response in the database. 
We present a model for automatically predicting information status labels for German referring expressions. We train a CRF on manually annotated phrases, and predict a ﬁne-grained set of labels. We achieve an accuracy score of 69.56% on our most detailed label set, 76.62% when gold standard coreference is available. 
This paper proposes a probabilistic approach to the resolution of referring expressions for task-oriented dialogue systems. The approach resolves descriptions, anaphora, and deixis in a uniﬁed manner. In this approach, the notion of reference domains serves an important role to handle context-dependent attributes of entities and references to sets. The evaluation with the REX-J corpus shows promising results. 
Dialogue act modeling in task-oriented dialogue poses significant challenges. It is particularly challenging for corpora consisting of two interleaved communication streams: a dialogue stream and a task stream. In such corpora, information can be conveyed implicitly by the task stream, yielding a dialogue stream with seemingly missing information. A promising approach leverages rich resources from both the dialog and the task streams, combining verbal and non-verbal features. This paper presents work on dialogue act modeling that leverages body posture, which may be indicative of particular dialogue acts. Combining three information sources (dialogue exchanges, task context, and users’ posture), three types of machine learning frameworks were compared. The results indicate that some models better preserve the structure of task-oriented dialogue than others, and that automatically recognized postural features may help to disambiguate user dialogue moves. 
Ambiguous or open-ended requests to a dialogue system result in more complex dialogues. We present a semantic-speciﬁcity metric to gauge this complexity for dialogue systems that access a relational database. An experiment where a simulated user makes requests to a dialogue system shows that semantic speciﬁcity correlates with dialogue length. 
Unlike in English, the sentence boundaries in Chinese are fuzzy and not well-defined. As a result, Chinese sentences tend to be long and consist of complex discourse relations. In this paper, we focus on two important relations, Contingency and Comparison, which occur often inside a sentence. We construct a moderate-sized corpus for the investigation of intrasentential relations and propose models to label the relation structure. A learning based model is evaluated with various features. Experimental results show our model achieves accuracies of 81.63% in the task of relation labeling and 74.8% in the task of relation structure prediction. 
This paper presents an analysis of how the level of performance achievable by an NLU module can affect the optimal modular design of a dialogue system. We present an evaluation that shows how NLU accuracy levels impact the overall performance of a system that includes an NLU module and a rule-based dialogue policy. We contrast these performance levels with the performance of a direct classiﬁcation design that omits a separate NLU module. We conclude with a discussion of the potential for a hybrid architecture incorporating the strengths of both approaches. 
The goal of this paper is to present a ﬁrst step toward integrating Incremental Speech Recognition (ISR) and Partially-Observable Markov Decision Process (POMDP) based dialogue systems. The former provides support for advanced turn-taking behavior while the other increases the semantic accuracy of speech recognition results. We present an Incremental Interaction Manager that supports the use of ISR with strictly turn-based dialogue managers. We then show that using a POMDP-based dialogue manager with ISR substantially improves the semantic accuracy of the incremental results. 
During conversations, addressees produce conversational acts—verbal and nonverbal backchannels—that facilitate turn-taking, acknowledge speakership, and communicate common ground without disrupting the speaker’s speech. These acts play a key role in achieving ﬂuent conversations. Therefore, gaining a deeper understanding of how these acts interact with speaker behaviors in shaping conversations might offer key insights into the design of technologies such as computer-mediated communication systems and embodied conversational agents. In this paper, we explore how a regression-based approach might offer such insights into modeling predictive relationships between speaker behaviors and addressee backchannels in a storytelling scenario. Our results reveal speaker eye contact as a signiﬁcant predictor of verbal, nonverbal, and bimodal backchannels and utterance boundaries as predictors of nonverbal and bimodal backchannels. 
With the aim of investigating how humans understand each other through language and gestures, this paper focuses on how people understand incomplete sentences. We trained a system based on interrupted but resumed sentences, in order to ﬁnd plausible completions for incomplete sentences. Our promising results are based on multi-modal features. 
Participants in a conversation are normally receptive to their surroundings and their interlocutors, even while they are speaking and can, if necessary, adapt their ongoing utterance. Typical dialogue systems are not receptive and cannot adapt while uttering. We present combinable components for incremental natural language generation and incremental speech synthesis and demonstrate the ﬂexibility they can achieve with an example system that adapts to a listener’s acoustic understanding problems by pausing, repeating and possibly rephrasing problematic parts of an utterance. In an evaluation, this system was rated as signiﬁcantly more natural than two systems representing the current state of the art that either ignore the interrupting event or just pause; it also has a lower response time. 
We present a novel unsupervised framework for focused meeting summarization that views the problem as an instance of relation extraction. We adapt an existing in-domain relation learner (Chen et al., 2011) by exploiting a set of task-speciﬁc constraints and features. We evaluate the approach on a decision summarization task and show that it outperforms unsupervised utterance-level extractive summarization baselines as well as an existing generic relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. 
We present work on understanding natural language in a situated domain, that is, language that possibly refers to visually present entities, in an incremental, word-by-word fashion. Such type of understanding is required in conversational systems that need to act immediately on language input, such as multi-modal systems or dialogue systems for robots. We explore a set of models speciﬁed as Markov Logic Networks, and show that a model that has access to information about the visual context of an utterance, its discourse context, as well as the linguistic structure of the utterance performs best. We explore its incremental properties, and also its use in a joint parsing and understanding module. We conclude that MLNs offer a promising framework for specifying such models in a general, possibly domain-independent way. 
Some people with disabilities find it difficult to access some forms of language. Assistive Technology is a term used to describe a class of technologies/interventions designed to enable people with disabilities to do things that their disabilitie currently make difficult. A large amount of work on Assistive Technology has focused on enabling access to language and communication; this class of interventions could greatly benefit from Natural Language Generation technologies. This talk will briefly survey some Assistive Technology applications that have employed Natural Language Generation technologies – highlighting some of the needs in this application area along with the opportunities that it provides for investigating hard problems in Natural Language Generation. It will then highlight a project, called the SIGHT System, intended to provide access to information graphics (e.g., bar charts, line graphs) found in popular media to people who have visual impairments. This system employs Natural Language Generation technologies to generate appropriate textual summaries of the information graphics. As such, it makes contributions to several areas within the field of Natural Language Generation while also enabling access to the information in these graphics to people who cannot access it with visual means. Biography Dr. Kathleen F. McCoy is a professor in the Department of Computer and Information Sciences at the University of Delaware. She received her PhD from the University of Pennsylvania in 1985 with a dissertation in the area of Natural Language Generation, and has been at the University of Delaware ever since then. Shortly after joining Delaware, she began working in applying Natural Language Processing to Assistive technologies at the Center for Applied Science and Engineering in Rehabilitation at the University of Delaware and the DuPont Hospital for Children. She served as the Center’s director from 2000-2009. She received a University of Delaware Excellence in Teaching Award in 1997, a University of Delaware Excellence in Advising Award in 2001, and a College of Arts and Science Outstanding Advisor Award in 2003. From 1995-2008 she served on the ACL Executive committee in various capacities including 10 years as Treasurer. She is the founding President of the ACL Special Interest Group on Speech and Language Processing for Assistive Technologies (2011). She has been an organizer of several workshops on that area associated with various ACL conferences. She was program co-chair of the User Modeling Conference in 2007, the ACM SIGACCESS Conference on Computers and Accessibility in 2009, and the General Chair of that same conference in 2011. She is a Senior Member of the ACM. 
Recent years have seen the appearance of adaptive learning technologies that offer significant potential for bringing about fundamental improvements in education. A promising development in this arena is the emergence of narrative-centered learning environments, which integrate the inferential capabilities of intelligent tutoring systems with the rich gameplay supported by commercial game engines. While narrative-centered learning environments have demonstrated effectiveness in both student learning and engagement, their capabilities will increase dramatically with expressive NLG. In this talk we will introduce the principles motivating the design of narrative-centered learning environments, discuss the role of NLG in narrative-centered learning, consider the interaction of NLG, affect, and learning, and explore how next-generation learning environments will push the envelope in expressive NLG. Biography Dr. James Lester is a professor Department of Computer Science North Carolina State University. He received the B.A. (Highest Honors), M.S.C.S., and Ph.D. Degrees in Computer Science from the University of Texas at Austin and the B.A in History from Baylor University. A member of Phi Beta Kappa, he has served as Program Chair for the ACM conference on Intelligent User Interfaces (2001), Program Chair for the International Conference on Intelligent Tutoring Systems (2004), Conference CoChair for the International Conference on Intelligent Virtual Agents (2008), and on the editorial board of Autonomous Agents and Multi-Agent Systems (1997-2007). His research focuses on intelligent tutoring systems, computational linguistics, and intelligent user interfaces. It has been recognized by several Best Paper awards. His research interests include intelligent game-based learning environments, computational models of narrative, affective computing, creativity-enhancing technologies, and tutorial dialogue. He is Editor-In-Chief of the International Journal of Artificial Intelligence in Education. 2 
One important subtask of Referring Expression Generation (REG) algorithms is to select the attributes in a deﬁnite description for a given object. In this paper, we study how much training data is required for algorithms to do this properly. We compare two REG algorithms in terms of their performance: the classic Incremental Algorithm and the more recent Graph algorithm. Both rely on a notion of preferred attributes that can be learned from human descriptions. In our experiments, preferences are learned from training sets that vary in size, in two domains and languages. The results show that depending on the algorithm and the complexity of the domain, training on a handful of descriptions can already lead to a performance that is not signiﬁcantly different from training on a much larger data set. 
Commonly, the result of referring expression generation algorithms is a single noun phrase. In interactive settings with a shared workspace, however, human dialog partners often split referring expressions into installments that adapt to changes in the context and to actions of their partners. We present a corpus of human–human interactions in the GIVE-2 setting in which instructions are spoken. A ﬁrst study of object descriptions in this corpus shows that references in installments are quite common in this scenario and suggests that contextual factors partly determine their use. We discuss what new challenges this creates for NLG systems. 
We describe preliminary work on generating contextualized text for nature conservation volunteers. This Natural Language Generation (NLG) differs from other ways of describing spatio-temporal data, in that it deals with abstractions on data across large geographical spaces (total projected area 20,600 km2), as well as temporal trends across longer time frames (ranging from one week up to a year). We identify challenges at all stages of the classical NLG pipeline. 
While in Computer Science, grammar engineering has led to the development of various tools for checking grammar coherence, completion, under- and over-generation, in Natural Langage Processing, most approaches developed to improve a grammar have focused on detecting under-generation and to a much lesser extent, over-generation. We argue that generation can be exploited to address other issues that are relevant to grammar engineering such as in particular, detecting grammar incompleteness, identifying sources of overgeneration and analysing the linguistic coverage of the grammar. We present an algorithm that implements these functionalities and we report on experiments using this algorithm to analyse a Feature-Based Lexicalised Tree Adjoining Grammar consisting of roughly 1500 elementary trees. 
Variation in language style can lead to different perceptions of the interaction, and different behaviour outcomes. Using the CRAG 2 language generation system we examine how accurately judges can perceive character personality from short, automatically generated dialogues, and how alignment (similarity between speakers) alters judge perceptions of the characters’ relationship. Whilst personality perception of our dialogues is consistent with perceptions of human behaviour, we ﬁnd that the introduction of alignment leads to negative perceptions of the dialogues and the interlocutors’ relationship. A follow up evaluation study of the perceptions of different forms of alignment in the dialogues reveals that while similarity at polarity, topic and construction levels is viewed positively, similarity at the word level is regarded negatively. We discuss our ﬁndings in relation to the literature and in the context of dialogue systems. 
Recent studies have shown that incremental systems are perceived as more reactive, natural, and easier to use than non-incremental systems. However, previous work on incremental NLG has not employed recent advances in statistical optimisation using machine learning. This paper combines the two approaches, showing how the update, revoke and purge operations typically used in incremental approaches can be implemented as state transitions in a Markov Decision Process. We design a model of incremental NLG that generates output based on micro-turn interpretations of the user’s utterances and is able to optimise its decisions using statistical machine learning. We present a proof-of-concept study in the domain of Information Presentation (IP), where a learning agent faces the trade-off of whether to present information as soon as it is available (for high reactiveness) or else to wait until input ASR hypotheses are more reliable. Results show that the agent learns to avoid long waiting times, ﬁllers and self-corrections, by re-ordering content based on its conﬁdence. 
Linguist’s Assistant (LA) is a large scale semantic analyzer and multi-lingual natural language generator designed and developed entirely from a linguist’s perspective. The system incorporates extensive typological, semantic, syntactic, and discourse research into its semantic representational system and its transfer and synthesizing grammars. LA has been tested with English, Korean, Kewa (Papua New Guinea), Jula (Cote d’Ivoure), and North Tanna (Vanuatu), and proof-of-concept lexicons and grammars have been developed for Spanish, Urdu, Tagalog, Chinantec (Mexico), and Angas (Nigeria). This paper will summarize the major components of the NLG system, and then present the results of experiments that were performed to determine the quality of the generated texts. The experiments indicate that when experienced mothertongue translators use the drafts generated by LA, their productivity is typically quadrupled without any loss of quality.  duced by LA are always easily understandable, grammatically correct, semantically equivalent to the source documents, and at approximately a sixth grade reading level. Because the system is based on linguistic research, LA is expected to work well for typologically diverse languages; it works equally well for languages that are coranking or clause chaining, highly isolating or highly polysynthetic, fusional or agglutinative, etc. A natural language generator of this type is practical only when translating large quantities of texts into many different languages. Therefore semantic representations for a large variety of texts are being developed for LA. This system is a tool which enables linguists to document a language and simultaneously generate numerous texts for the speakers of that language. A model of LA is shown in Figure 1.  
Despite their ﬂat, semantics-free structure, ontology identiﬁers are often given names or labels corresponding to natural language words or phrases which are very dense with information as to their intended referents. We argue that by taking advantage of this information density, NLG systems applied to ontologies can guide the choice and construction of sentences to express useful ontological information, solely through the verbalisations of identiﬁer names, and that by doing so, they can replace the extremely fussy and repetitive texts produced by ontology verbalisers with shorter and simpler texts which are clearer and easier for human readers to understand. We specify which axioms in an ontology are “deﬁning axioms” for linguistically-complex identiﬁers and analyse a large corpus of OWL ontologies to identify common patterns among all deﬁning axioms. By generating texts from ontologies, and selectively including or omitting these deﬁning axioms, we show by surveys that human readers are typically capable of inferring information implicitly encoded in identiﬁer phrases, and that texts which do not make such “obvious” information explicit are preferred by readers and yet communicate the same information as the longer texts in which such information is spelled out explicitly. 
During the last decade, there has been a shift from developing natural language generation systems to developing generic systems that are capable of producing natural language descriptions directly from Web ontologies. To make these descriptions coherent and accessible in different languages, a methodology is needed for identifying the general principles that would determine the distribution of referential forms. Previous work has proved through crosslinguistic investigations that strategies for building coreference are language dependent. However, to our knowledge, there is no language generation methodology that makes a distinction between languages about the generation of referential chains. To determine the principles governing referential chains, we gathered data from three languages: English, Swedish and Hebrew, and studied how coreference is expressed in a discourse. As a result of the study, a set of language speciﬁc coreference strategies were identiﬁed. Using these strategies, an ontology-based multilingual grammar for generating written natural language descriptions about paintings was implemented in the Grammatical Framework. A preliminary evaluation of our method shows languagedependent coreference strategies lead to better generation results.  createdBy (Guernica, PabloPicasso) currentLocation (Guernica, MuseoReinaSofía) hasColor (Guernica, White) hasColor (Guernica, Gray) hasColor (Guernica, Black) Guernica is created by Pablo Picasso. Guernica has as current location the Museo Reina Sofía. Guernica has as color White, Gray and Black. Figure 1: A natural language description generated from a set of ontology statements. 
Human-written, good quality extractive summaries pay great attention to the text intermixing the extracts. In this work, we focused on the lexical choice for verbs introducing quoted text. We analyzed 4000+ high quality summaries for a high trafﬁc mailing list and manually assembled 39 quotation-introducing verb classes that cover the majority of the verb occurrences. A signiﬁcant amount of the data is covered by on-going work on e-mail “speech acts.” However, we found that one third of the “tail” is composed by “risky” verbs that most likely will be beyond the state of the art for longer time. We used this fact to highlight the trade-offs of risk taking in NLG, where interesting prose might come at the cost of unsettling some of the readers. 
 2 Related Work  We present an approach for generation of morphologically rich languages using statistical machine translation. Given a sequence of lemmas and any subset of morphological features, we produce the inﬂected word forms. Testing on Arabic, a morphologically rich language, our models can reach 92.1% accuracy starting only with lemmas, and 98.9% accuracy if all the gold features are provided. 
While some recent work in tutorial dialogue has touched upon tutor reformulations of student contributions, there has not yet been an attempt to characterize the intentions of reformulations in this educational context nor an attempt to determine which types of reformulation actually contribute to student learning. In this paper we take an initial look at tutor reformulations of student contributions in naturalistic tutorial dialogue in order to characterize the range of pedagogical intentions that may be associated with these reformulations. We further outline our plans for implementing reformulation in our tutorial dialogue system, Rimac, which engages high school physics students in post problem solving reﬂective discussions. By implementing reformulations in a tutorial dialogue system we can begin to test their impact on student learning in a more controlled way in addition to testing whether our approximation of reformulation is adequate. 
NLG developers must work closely with domain experts in order to build good NLG systems, but relatively little has been published about this process. In this paper, we describe how NLG developers worked with clinicians (nurses) to improve an NLG system which generates information for parents of babies in a neonatal intensive care unit, using a structured revision process. We believe that such a process can signiﬁcantly enhance the quality of many NLG systems, in medicine and elsewhere. 
 !"#$ !"#$ %!&  %!& %!&  This paper concerns the architecture of a generator for Italian Sign Language. In particu-  42&)*# ,# '-(3-*2'%*2 .%3-*#)*# 2&&2 (-,#2  lar we describe a microplanner based on an expert-system and a combinatory categorial grammar used in realization.  Figure 1: The (simpliﬁed) syntactic structure of the sentence “Valori di temperatura superiori alla media” (Temperature values exceed the average) produced by the TUP  parser. 
A useful enhancement of an NLG system for verbalising ontologies would be a module capable of explaining undesired entailments of the axioms encoded by the developer. This task raises interesting issues of content planning. One approach, useful as a baseline, is simply to list the subset of axioms relevant to inferring the entailment; however, in many cases it will still not be obvious, even to OWL experts, why the entailment follows. We suggest an approach in which further statements are added in order to construct a proof tree, with every step based on a relatively simple deduction rule of known difﬁculty; we also describe an empirical study through which the difﬁculty of these simple deduction patterns has been measured. 
Question answering is an age old AI challenge. How we approach this challenge is determined by decisions regarding the linguistic and domain knowledge our system will need, the technical and business acumen of our users, the interface used to input questions, and the form in which we should present answers to a user’s questions. Our approach to question answering involves the interactive construction of natural language queries. We describe and evaluate a question answering system that provides a point-and-click, webbased interface in conjunction with a semantic grammar to support user-controlled natural language question generation. A preliminary evaluation is performed using a selection of 12 questions based on the Adventure Works sample database. 
This paper proposes the use of NLG to enhance public engagement during the course of species reintroductions. We examine whether ecological insights can be effectively communicated through blogs about satellite-tagged individuals, and whether such blogs can help create a positive perception of the species in readers’ minds, a requirement for successful reintroduction. We then discuss the implications for NLG systems that generate blogs from satellite-tag data. 
In this paper we introduce an automatic system that generates textual summaries of Internet-style video clips by first identifying suitable high-level descriptive features that have been detected in the video (e.g. visual concepts, recognized speech, actions, objects, persons, etc.). Then a natural language generator is constructed using SimpleNLG to compile the high-level features into a textual form. The generated summary contains information from both visual and acoustic sources, intending to give a general review and summary of the video. To reduce the complexity of the task, we restrict ourselves to work with videos that show a limited number of “events”. In this demo paper, we describe the design of the system and present example outputs generated by the video summarization system.  In this paper, we introduce an automatic video summary generation system that uses a natural language realization engine (Gatt and Reiter, 2009) to create sentences based on state-of-the-art video classification features. These features are computed on a large corpus from the TrecVID evaluation (Bao, et al. 2011). In a recent user study (Ding, et al. 2012), we compared automatically generated and manually generated summaries with respect to several tasks. The study shows, for example, that more specific information (e.g. “food” instead of “some object”) and temporal information (something happened first and then…) is helpful in improving the quality of machine-generated summaries. This is a first step to implement an automatic system which is not only able to describe videos using natural language, but accomplishes more sophisticated tasks such as differentiating videos, finding supporting evidence for video classification and other tasks.  
We demonstrate a novel, robust vision-tolanguage generation system called Midge. Midge is a prototype system that connects computer vision to syntactic structures with semantic constraints, allowing for the automatic generation of detailed image descriptions. We explain how to connect vision detections to trees in Penn Treebank syntax, which provides the scaffolding necessary to further reﬁne data-driven statistical generation approaches for a variety of end goals. 
So far, there has been little success in Natural Language Generation in coming up with general models of the content selection process. Nonetheless, there has been some work on content selection that employ Machine learning or heuristic search. On the other side, there is a clear tendency in NLG towards the use of resources encoded in standard Semantic Web representation formats. For these reasons, we believe that time has come to propose an initial challenge on content selection from Semantic Web data. In this paper, we brieﬂy outline the idea and plan for the execution of this task. 
Quick tour of NLP and speech technologies for ambient assisted living To address the challenges imposed by an ageing population, developed countries are massively supporting the development Information and Communication Technologies (ICT). ICT represents a great opportunity to improve the daily life of the elderly so that they always keep control over their life and use technology to continue to live independently, to learn and to stay involved in social life. Technologies of natural language and speech processing that lie at the heart of human communication, have a major role to play. In this paper, we present a survey of the NLP and speech technologies currently developed as well as the current technical or ethical challenges or pitfalls that may limit their impact. MOTS-CLÉS : habitat intelligent, assistance à la vie autonome, reconnaissance automatique de la parole, traitement automatique du langage naturel. KEYWORDS: smart home, ambiant assisted living, speech processing, natural language proces- sing. 
In this paper, we present a multisource ASR system to detect home automation orders in various everyday listening conditions in a realistic home. The system is based on a state of the art noise cancellation stage that feeds recently introduced ASR techniques. The evaluation was conducted on a realistic noisy dataset acquired in a smart home where a microphone was placed near the noise source and several other microphones were set in the ceiling of the different rooms. This distant speech French corpus was recorded with 23 speakers uttering colloquial or distress sentences as well as home automation orders. Techniques acting at the decoding stage and using a priori knowledge gave the best results in noisy condition compared to the baseline reaching good enough performance for a real usage. If broadcast news is easily handled by the noise canceller, improvements still need to be made when music is used as background noise. MOTS-CLÉS : Domotique, habitat intelligent, parole distante, SRAP multisource, détection de mots clefs. KEYWORDS: Home automation, smart home, distant speech, multisource ASRs, keyword detec- tion. JEP-TALN-RECITAL 2012, Atelier ILADI 2012: Interactions Langagières pour personnes Agées Dans les habitats Intelligents, pages 31–39, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 31  
HD Voice : a new issue for voice processing in elderly Use of automatic speech analysis is an interest and a great comer for home care of elderly. At the moment, it needs the use of microphone for recording a quality sound signal that is necessary for the complex processing of acoustic specificities of elderly voice. However, this methodology prevents experiments spreading. We think that the use of phone would be able to help this scaling up: more tested persons, less costs, possible automation of recordings and analysis… Phone seems to be interesting, but decreases audio signal quality. We present here the HD Voice technology, newly supported by the main telcos, that allows to remove this technological bottle-neck. Ensuring a phone transmission of high quality audio signal, this seal of approval could constitute an efficient and suitable tool for speech processing in elderly people. MOTS-CLES : téléphone, Voix HD, traitement de la parole, personnes âgées, maintien à domicile, gérontechnologie. KEYWORDS: telephone, HD Voice, automatic speech processing, elderly people, home care, gerontechnology. 
Contribution to the study of elderly people’s voice variability in automatic speech recognition Using speech recognition to support ambient assisted living is impeded by the difﬁculty of using ASR systems that are not provided for the elderly voice. To characterize these differences in speech recognition performance, we studied phoneme categories which lead to the lowest recognition rate in the elderly speakers with respect to the younger ones based on the AD80 corpus that we recorded. The results showed that some phonemes (such as plosives) are more speciﬁcally affected by age than others. Moreover, we collected the speciﬁc ERES38 corpus to adapt the ASR acoustic model to the elderly population which resulted in a 15% decrease of the word error rate. Despite a great variability of performances, we characterized how lower performance of ASR systems can be correlated to the autonomy degradation of elderly people. MOTS-CLÉS : reconnaissance automatique de parole, voix des personnes âgées, adaptation acoustique, assistance à la vie autonome. KEYWORDS: automatic speech recognition, ageing voice, acoustic adaptation, ambient assisted living. JEP-TALN-RECITAL 2012, Atelier ILADI 2012: Interactions Langagières pour personnes Agées Dans les habitats Intelligents, pages 49–59, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 49  
Mbochi: oral corpus, automatic processing & phonological mining This contribution describes ongoing research on Mbochi, a Bantu C language spoken by more than 100000 native speakers in Congo-Brazzaville. A first oral corpus has been collected as read speech corresponding to 3 folktales. It has been transcribed by one of the co-authors and it will be extended to radio broadcasts. The corpus is aligned automatically into words and phonemic segments, allowing acoustic-phonetic and phonological studies on a large scale. It is providing the first step towards an automatic transcription system for this under-resourced language. Currently, these resources allow us to improve the description of the language and to improve our knowledge of the nature and conditions of phonological processes such as vowel elision with or without compensatory lengthening at word junctions. The corpus which will contribute to the documentation of Mbochi and its visibility on the web, will be made available to other researchers. MOTS-CLES : mbochi, alignement automatique, élision vocalique, dissimilation consonantique. KEYWORDS : Mbochi, automatic alignment, vowel elision, consonantal dissimilation JEP-TALN-RECITAL 2012, Atelier TALAf 2012: Traitement Automatique des Langues Africaines, pages 1–12, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 
ABSTRACT__________________________________________________________________________________________________________ Construction of the Kanuri-French bilingual dictionary This paper presents the structure of the Kanuri-French dictionary of 6,000 entries prepared during the SOUTÉBA project and then computerized during the DiLAF project. It also presents the Kanuri language, its speakers and the position of the language in several genetic classifcations. A description of its type and its verbal system follows. The article concludes with a description of the Kanuri spelling system. MOTS-CLÉS : dictionnaire bilingue, kanouri, français, kanouri-français, langue nationale. KEYWORDS : bilingual dictionary, Kanuri, French, Kanuri-French, national language. 
ABSTRACT__________________________________________________________________________________________________________ The DILAF project aims to establish a methodology to convert of editorial dictionaries into XML files expressed according with rhe LMF (Lexical Markup Framework) format and to apply tis mothodology on five dictionaries. We present the motivation of this project, then the concerned dictionaries and the alphabets of the languages of these dictionaries. These are bilingual dictionaries Africanlanguage-French: Hausa-French, Kanuri-French, Soŋay Zarma-French, Tamajaq-French and Bambara-French. The jibiki platform is presented, then we detail the adavances of the project thanks to the collaboration of linguists, computer scientists, and lexicographers. The fifth part establishes a balance concerning the Unicode representation of the characters of the different languages and details the particular case of the tifinagh characters. MOTS-CLÉS : LMF, TALN, dictionnaires, langues africaines, Unicode KEYWORDS : LMF, NLP, dictionnaries, African languages, Unicode 
Performance analysis of sub-word language modeling for under-resourced languages with rich morphology : case study on Swahili and Amharic This paper investigates the impact on ASR performance of sub-word units for two underresourced african languages with rich morphology (Amharic and Swahili). Two subword units are considered : syllable and morpheme, the latter being obtained in a supervised or unsupervised way. The important issue of word reconstruction from the syllable (or morpheme) ASR output is also discussed. For both languages, best results are reached with morphemes got from unsupervised approach. It leads to very signiﬁcant WER reduction for Amharic ASR for which LM training data is very small (2.3M words) and it also slightly reduces WER over a Word-LM baseline for Swahili ASR (28M words for LM training). A detailed analysis of the OOV word reconstruction is also presented ; it is shown that a high percentage (up to 75% for Amharic) of OOV words can be recovered with morph-based language model and appropriate reconstruction method. MOTS-CLÉS : Modèle de langage, Morphème, Hors vocabulaire, Langues peu-dotées. KEYWORDS: Language model, Morpheme, Out-of-Vocabulary , Under-resourced languages. JEP-TALN-RECITAL 2012, Atelier TALAf 2012: Traitement Automatique des Langues Africaines, pages 53–62, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 53  
ABSTRACT__________________________________________________________________________________________________________ Formation rules of names in Hausa In the perspective of african language processing, we have described some characteristics of hausa lexical functions. We were interested on the rules of word formation from roots where a root stands for a word to which another thing can be added to form a noun. Diferent words are grouped in diferent categories. Each category has its own way to form a word be it male or female. Plural names are also derived according to some rules. The analysis of hausa structure allowed us to formulate several rules of fexion and derivation of nouns. From the diferent theories existing in this area, the one of Scalise seems to be the most appropriate in the description of hausa words. MOTS-CLÉS : langue africaine, traitement automatique, haoussa, règle de dérivation, règle de fexion. KEYWORDS : african language, automatic processing, hausa, derivation rule, fexion rule. 
Formalization of the standard Amazigh with NooJ Since antiquity, the Amazigh patrimony is expanding from generation to generation. However, the access to the domain of new Information and Communication Technologies (NICT) proves to be primordial to safeguard and exploit this patrimony and to prevent that it will be threatened of disappearance. In this perspective, and in the context of developing tools and linguistic resources, we undertook to build a module NooJ for the standard Amazigh language. This paper proposes a formalization of the category name allowing to generate from a lexical entrance its gender (male, female), its number (singular, plural), and its status (free, annexation). MOTS-CLÉS : La langue amazighe, NooJ, Morphologie flexionnelle. Keywords : Amazigh language, NooJ, Inflectional morphology. 
Describing the Morphology of Verbs in Ikota using a Metagrammar In this paper, we show how the concept of metagrammar originally introduced by Candito (1996) to design large Tree-Adjoining Grammars describing the syntax of French and Italian, can be used to describe the morphology of Ikota, a Bantu language spoken in Gabon. Here, we make use of the expressivity of the XMG (eXtensible MetaGrammar) formalism to describe the morphological variations of verbs in Ikota. This XMG speciﬁcation captures generalizations over these morphological variations. In order to produce the inﬂected forms, one can compile the XMG speciﬁcation, and save the resulting electronic lexicon in an XML ﬁle, thus favorising its reuse in dedicated applications. MOTS-CLÉS : Métagrammaire, morphologie, ikota. KEYWORDS: Metagrammar, Morphology, Ikota. 
Gesture has been arousing a growing interest in linguists who are attracted by the multimodality of information structuring, organized into the verbal, vocal and visual modes. Yet, in order for visual information to be taken into account, one has to consider gesture units. What is a gesture? How can the constant flood of movement be subdivided into discrete units? What degree of fineness is necessary in the segmentation of gesture units to put them into relationship with information in other modes? Drawing on the DEGELS corpus provided by the organizers of the workshop, we describe the criteria adopted in our practice for the annotation of co-speech gesture as well as gaze direction. MOTS-CLÉS : Annotation, gestualité, segmentation, unités. KEYWORDS : Annotation, gesture, segmentation, units. 
ABSTRACT _________________________________________________________________________________________________________ Where do you switch to get the band? Multimodality is a challenge to natural language processing, especially if one is interested in finding out how one should segment a dialogic discourse generated through vocal and gestural channels simultaneously. This paper focuses on the manual pointing gestures a hearing speaker uses while performing a map task. We will successively address the following issues: which formal criteria are relevant in segmenting and identifying terminals for gestural units? Likewise, what criteria can we suggest as appropriate to the prosodic vocal flow? Which degree of granularity is the more relevant to account for the potential interaction between vocal and manual ‘gestures’? We not only provide a description and an annotation of these pointing gestures with the ELAN tool; we also claim for a bottom-up design for segmenting and categorizing the relevant chunks, while previous studies have usually mixed formal and functional criteria in a top-down strategy. MOTS-CLÉS : segmentation, multimodalité, alignement, modèle d’annotations KEYWORDS : segmentation, multimodality, alignment, template for annotations JEP-TALN-RECITAL 2012, Atelier DEGELS 2012: Déﬁ GEste Langue des Signes, pages 23–39, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 23  
Gesture segmentation and coding: a methodology for team working In this paper we intend to describe our methodology for segmenting and coding cospeech gestures. Working as a team of 3 gesture researchers required to find a method to coordinate our work, another to evaluate the inter-rater agreement, and a last one to code and segment the gestures that would bring about consensus. This paper aims at making our approach clear so as to share it with the community of co-speech gesture researchers. MOTS-CLÉS : segmentation, annotation, geste coverbal, méthode KEYWORDS: segmentation, coding, co-speech gesture, methodology 
Segment and annotate a discourse of a Sign French Language speaker : formal continuity and fonctional variation of units This contribution propose to approach the question of the "down levels" units by integrating in the reﬂections the iconic and corporal dynamics which are implied in differents levels of the Sign French Language (Millet, 2002). Our transcription perspective integrates more broadly a multimodal comprehension of the LSF speaker’s discourse phenomenon. Therefore the transcription/annotation grid at which our reﬂections have led us propose, beyond the actors imposed for the DEGELS, to create some actors. The aims of the actors are to detail all the productions of articulators that speaker of LSF has at their disposal in one part, and, in other part, the values and the functions of these productions in the discourse elaboration. So this grid proposes a necessarily linear description of language phenomenons without ignoring the dynamics which co-construct the partition of language. MOTS-CLÉS : LSF, dynamiques iconiques et corporelles, multimodalité, variabilité fonctionnelle. KEYWORDS: LSF, iconics and corporals dynamics, multimodality, fonctional variations. 
Inﬂuence of the temporal segmentation on the sign characterization Temporal segmentation of meaningful units in sign language utterances is a difﬁcult problem because it requires a combination many informations. However, it is often necessary to ﬁnd the beginning and the end of signs before making their characterization. In this article, we show how the characterization of the signs may be inﬂuenced by a variation of their segmentation. Taking the example of the movement characterization, we indicate how the deﬁnition of segmentation criteria based on movement or manual conﬁgurations can inﬂuence the robustness of the characterization to variations of the segment temporal boundaries. We also show how the nature of the measurement made on the segment (maximum, average, values on temporal boundaries) affects the sensitivity to temporal segmentation. MOTS-CLÉS : Langue des Signes Française, caractérisation, segmentation. KEYWORDS: French Sign Language, characterization, segmentation. 
SPPAS : a tool to perform text/speech alignement This paper presents SPPAS, a new tool dedicated to phonetic alignments, from the LPL laboratory. SPPAS produces automatically or semi-automatically annotations which include utterance, word, syllabic and phonemic segmentations from a recorded speech sound and its transcription. SPPAS is currently implemented for French, English, Italian and Chinese. There is a very simple procedure to add other languages in SPPAS : it is just needed to add related resources in the appropriate directories. SPPAS can be used by a large community of users : accessibility and portability are important aspects in its development. The tools and resources will all be distributed with a GPL license. MOTS-CLÉS : segmentation, phonétisation, alignement, syllabation. KEYWORDS: segmentation, phonetization, alignement, syllabiﬁcation. 
An automatic gesture segmentation system applied to Sign Language Many researches focus on the study of automatic sign language recognition. Many of them need a large amount of data to train the recognition systems. Our work address the segmentation of gestures in sign language video corpus in order to identify the beginning and the end of signs. We propose an approach to segment gestures using motion and hand shape features. MOTS-CLÉS : Segmentation de gestes, langue des signes, segmentation de signes. KEYWORDS: Gesture segmentation, sign language, sign segmentation. 
Controlled and free indexing of scientiﬁc papers Presentation and results of the DEFT2012 text-mining challenge In this paper, we present the 2012 edition of the DEFT text-mining challenge. This edition addresses the automatic, keyword-based indexing of scientiﬁc papers through two tracks. The ﬁrst gives to the participants the terminology of keywords used to index the documents, while the second does not provide this terminology. The corpus is composed of scientiﬁc papers published in humanities journals, indexed by their authors. This indexing is used as a reference for the evaluation. The results have been evaluated in terms of micro-measures on the recall, precision and F-measure computed after keyword lemmatization. In the track giving the terminology of used keywords, the mean F-measure is 0.3575, the median is 0.3321 and the standard deviation is 0.2985 ; in the second track, the mean F-measure is 0.2055, the median is 0.1901 and the standard deviation is 0.1516. MOTS-CLÉS : Campagne d’évaluation, fouille de textes, indexation libre, indexation contrôlée, mots-clés, thésaurus. KEYWORDS: Evaluation campaign, Text-Mining, Free Indexing, Controlled Indexing, Keywords, Thesaurus. JEP-TALN-RECITAL 2012, Atelier DEFT 2012: DÉﬁ Fouille de Textes, pages 1–13, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 
We present an adaptation for the French text mining challenge (DEFT 2012) of the KX system for multilingual unsupervised key-concept extraction. KX carries out the selection of a list of weighted keywords from a document by combining basic linguistic annotations with simple statistical measures. In order to adapt it to the French language, a French morphological analyzer (PoS-Tagger) has been added into the extraction pipeline, to derive lexical patterns. Moreover, parameters such as frequency thresholds for collocation extraction and indicators for key-concepts relevance have been calculated and set on the training documents. In the DEFT 2012 tasks, KX achieved good results (i.e. 0.27 F1 for Task 1 - with terminological list, and 0.19 F1 for Task 2) with a limited additional effort for domain and language adaptation. MOTS-CLÉS : Extraction de mots-clés, patrons linguistiques, terminologie. KEYWORDS: Key-concept extraction, linguistic patterns, terminology. 
Terminological acquisition for identifying keywords of scientiﬁc articles The challenge DEFT2012 aims at automatically identifying the keywords chosen by the authors of scientiﬁc articles in the Humanities. A keyword list is provided within the track 1. We propose to exploit terminological acquisition approaches. The extracted terms are also sorted and ﬁltered according to their position in the documents, weighting measures and linguistic criteria. We deﬁned several conﬁgurations of our system. Our best F-measure for the track 1 is 0.3985 while for the track 2, the best F-measure is 0.1921. MOTS-CLÉS : Mots clés, extraction de termes, mesure de pondération, ﬁltrage de termes. KEYWORDS: Keywords, Term Recognition, Weighting Measure, Term Filtering. 
This paper presents the URPAH team’s participation in DEFT 2012.Our approach uses noun phrases in the automatic identiﬁcation of keywords indexing the content of scientiﬁc papers published in a review of Human and Social Sciences, with assistance from the terminology of keywords (piste1) and without terminology (piste2 ) MOTS-CLÉS : syntagmes nominaux, patrons syntaxiques, recherche d’information. KEYWORDS: noun phrases, syntactic patterns, information retrieval. JEP-TALN-RECITAL 2012, Atelier DEFT 2012: DÉﬁ Fouille de Textes, pages 33–39, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 33  
Keywords extraction by repeated string analysis We present here the HULTECH(Human Language Technology) team approach for the Deft 2012 (french text mining challenge). The aim of the challenge is to retrieve the keywords given by the authors of scientiﬁc articles. Our method relies on a text algorithmics technic : detection of maximal repeated strings. This technic is applied at character level and word level. We achieved the third rank (over 10) of the ﬁrst track. MOTS-CLÉS : Recherche d’information, extraction de mots-clés, algorithmique du texte. KEYWORDS: Information retrieval, keywords extraction, string algorithmics. 
IRISA participation to DeFT 2012 : information retrieval and machine learning for keyword generation This paper describes the IRISA participation to the DeFT 2012 text-mining challenge. It consisted in the automatic attribution or generation of keywords to scientiﬁc journal articles. Two tasks were proposed which led us to test two different strategies. For the ﬁrst task, a list of keywords was provided. Based on that, our ﬁrst strategy is to consider that as an Information Retrieval problem in wich the keyword are the queries, which are attributed to the best ranked documents. This approach yielded very good results. For the second task, only the articles were known; for this task, our approach is chieﬂy based on a term extraction system whose results are reordered by machine learning. MOTS-CLÉS : Génération de mots-clés, Extraction de termes, Recherche d’information, Boosting, arbres de décision, TermoStat. KEYWORDS: Keyword generation, Term extraction, Information Retrieval, Boosting, Deci- sion tree, TermoStat. JEP-TALN-RECITAL 2012, Atelier DEFT 2012: DÉﬁ Fouille de Textes, pages 49–60, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 49  
LINA at DEFT 2012 This article presents the participation of the TALN group at LINA to the déﬁ fouille de textes (DEFT) 2012. Developed speciﬁcally for the second task, our system combines the outputs of three different keyword extraction methods. Our system ranked 2nd out of 9 systems with a f-measure of 21,3%. MOTS-CLÉS : extraction de mots clés, deft 2012, combinaison de méthodes. KEYWORDS: keyword extraction, deft 2012, combining methods.  
Automatic unsupervised algorithm for Deft 2012 We describe our approach in Deft 2012 for track 1, which consist in identifying a corresponding list of key word, for a given scientiﬁc paper and summary, from a set of possible key words. The approach is based on the one hand, semantic space for the representation of semantic knowledge, and, on the other hand, graphs for the decision on the allocation of a key word to a document. The proposed method is fully automatic, without any particular tuning, unsupervised and requires no external resources. MOTS-CLÉS : Espace sémantique, Graphe, Random Indexing. KEYWORDS: Semantic Space, Graph, Random Indexing. JEP-TALN-RECITAL 2012, Atelier DEFT 2012: DÉﬁ Fouille de Textes, pages 69–75, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 69  
Enriching and reasoning on semantic spaces for keyword extraction This article presents a multi-modular hybrid system for extraction of keywords from corpus of scientiﬁc articles. System is multi-modular because it integrates components executing transformations on 1) morphosyntactic level (lemmatization and chunking) 2) semantic level (Reﬂected Random Indexing), as well as upon more 3) « pragmatic » aspects of processed documents, modeled by production rules. The system is hybrid because it was able to address both tracks of DEFT2012 competition – a «reduced search-space» scenario of Track 1, whose objective was to map the content of a scientiﬁc article upon one among the members of a « terminological list » ; as well as more « real-life » scenario of Track2 within which no list was associated to documents contained in the corpus. In both Tracks, the system hereby presented has obtained the an F-score of 0.9488 for the Track1, and 0.5874 for the Track2. MOTS-CLÉS : Extraction de mots-clés, Espaces sémantiques, RRI, Réseau bayésien, Règles de production, Chunking. KEYWORDS: Keyword extraction, Semantic spaces, RRI, Bayesian Network, Production Rules, Chunking. 
We present a new transcription mode for the annotation tool ELAN. This mode is designed to speed up the process of creating transcriptions of primary linguistic data (video and/or audio recordings of linguistic behaviour). We survey the basic transcription workﬂow of some commonly used tools (Transcriber, BlitzScribe, and ELAN) and describe how the new transcription interface improves on these existing implementations. We describe the design of the transcription interface and explore some further possibilities for improvement in the areas of segmentation and computational enrichment of annotations. 
 2 Relationship to earlier work  We present work on a verse-composition assistant for composing, checking correctness of, and singing traditional Basque bertsoak—impromptu verses on particular themes. A performing bertsolari—a verse singer in the Basque Country—must adhere to strict rules that dictate the format and content of the verses sung. To help the aspiring bertsolari, we provide a tool that includes a web interface that is able to analyze, correct, provide suggestions and synonyms, and tentatively also sing (using text-to-speech synthesis) verses composed by the user.  There exist some prior works dealing with Basque verse-making and computer technologies, such as BertsolariXa (Arrieta et al., 2001), which is a rhyme search tool implemented as ﬁnite-state automata using the two-level morphology formalism. The tool also contains other features, including semantic categorization of words, narrowing word-searches to certain themes, etc. While BertsolariXa focuses mostly on the word-level, the current work also includes constraints on overall verse structure in its implementation as well as a synonym search tool, a melody suggestion system, and possibilities for plugging in text-tospeech synthesis of verses.  
Today museums and other cultural heritage institutions are increasingly storing object descriptions using semantic web domain ontologies. To make this content accessible in a multilingual world, it will need to be conveyed in many languages, a language generation task which is domain speciﬁc and language dependent. This paper describes how semantic and syntactic information such as that provided in a framenet can contribute to solving this task. It is argued that the kind of information offered by such lexical resources enhances the output quality of a multilingual language generation application, in particular when generating domain speciﬁc content. 
We describe ongoing work aiming at deriving a multilingual controlled vocabulary (German, French, Italian) from the combined subject indices from 22 volumes of a large-scale critical edition of historical documents. The controlled vocabulary is intended to support editors in assigning descriptors to new documents and to support users in retrieving documents of interest regardless of the spelling or language variety used in the documents. 
We present on-going work on the automated ontology-based detection and recognition of characters in folktales, restricting ourselves for the time being to the analysis of referential nominal phrases occurring in such texts. Focus of the presently reported work was to investigate the interaction between an ontology and linguistic analysis of indeﬁnite and indeﬁnite nominal phrase for both the incremental annotation of characters in folktales text, including some inference based co-reference resolution, and the incremental population of the ontology. This in depth study was done at this early stage using only a very small textual base, but the demonstrated feasibility and the promising results of our smallscale experiment are encouraging us to deploy the strategy on a larger text base, covering more linguistic phenomena in a multilingual fashion. 
The volumes of digitized literary collections in various languages increase at a rapid pace, which results also in a growing demand for computational support to analyze such linguistic data. This paper combines robust text analysis with advanced visual analytics and brings a new set of tools to literature analysis. Visual analytics techniques can offer new and unexpected insights and knowledge to the literary scholar. We analyzed a small subset of a large literary collection, the Swedish Literature Bank, by focusing on the extraction of persons’ names, their gender and their normalized, linked form, including mentions of theistic beings (e.g., Gods’ names and mythological ﬁgures), and examined their appearance over the course of the novel. A case study based on 13 novels, from the aforementioned collection, shows a number of interesting applications of visual analytics methods to literature problems, where named entities can play a prominent role, demonstrating the advantage of visual literature analysis. Our work is inspired by the notion of distant reading or macroanalysis for the analyses of large literature collections. 
This paper illustrates the use of distributional techniques, as investigated in computational semantics, for supplying data from large-scale corpora to areas of the humanities which focus on the analysis of concepts. We suggest that the distributional notion of ‘characteristic context’ can be seen as evidence for some representative tendencies of general discourse. We present a case study where distributional data is used by philosophers working in the areas of gender studies and intersectionality as conﬁrmation of certain trends described in previous work. Further, we highlight that different models of phrasal distributions can be compared to support the claim of intersectionality theory that ‘there is more to a phrase than the intersection of its parts’. 
Even though NLP tools are widely used for contemporary text today, there is a lack of tools that can handle historical documents. Such tools could greatly facilitate the work of researchers dealing with large volumes of historical texts. In this paper we propose a method for extracting verbs and their complements from historical Swedish text, using NLP tools and dictionaries developed for contemporary Swedish and a set of normalisation rules that are applied before tagging and parsing the text. When evaluated on a sample of texts from the period 1550– 1880, this method identiﬁes verbs with an F-score of 77.2% and ﬁnds a partially or completely correct set of complements for 55.6% of the verbs. Although these results are in general lower than for contemporary Swedish, they are strong enough to make the approach useful for information extraction in historical research. Moreover, the exact match rate for complete verb constructions is in fact higher for historical texts than for contemporary texts (38.7% vs. 30.8%). 
We introduce a corpus of classical Chinese poems that has been word segmented and tagged with parts-ofspeech (POS). Due to the ill-defined concept of a ‘word’ in Chinese, previous Chinese corpora suffer from a lack of standardization in word segmentation, resulting in inconsistencies in POS tags, therefore hindering interoperability among corpora. We address this problem with nested POS tags, which accommodates different theories of wordhood and facilitates research objectives requiring annotations of the ‘word’ at different levels of granularity. 
A signiﬁcant amount of information about Cultural Heritage artefacts is now available in digital format and has been made available in digital libraries. Being able to identify items that are similar would be useful for search and navigation through these data sets. Information about items in these repositories is often multimodal, such as pictures of the artefact and an accompanying textual description. This paper explores the use of information from these various media for computing similarity between Cultural Heritage artefacts. Results show that combining information from images and text produces better estimates of similarity than when only a single medium is considered.  tion. This makes it difﬁcult to identify information of interest in sites that aggregate information from multiple sources, such as Europeana, or to compare information across multiple collections (such as the Louvre and British Museum). These problems form a signiﬁcant barrier to accessing the information available in these online collections. A ﬁrst step towards improving access would be to identify similar items in collections. This could assist with several applications that are of interest to those working in CH including recommendation of interesting items (Pechenizkzy and Calders, 2007; Wang et al., 2008), generation of virtual tours (Joachims et al., 1997; Wang et al., 2009), visualisation of collections (Kauppinen et al., 2009; Hornbaek and Hertzum, 2011) and exploratory search (Marchionini, 2006; Amin et al., 2008).  
Over the past years large digital cultural heritage collections have become increasingly available. While these provide adequate search functionality for the expert user, this may not offer the best support for non-expert or novice users. In this paper we propose a novel mechanism for introducing new users to the items in a collection by allowing them to browse Wikipedia articles, which are augmented with items from the cultural heritage collection. Using Europeana as a case-study we demonstrate the effectiveness of our approach for encouraging users to spend longer exploring items in Europeana compared with the existing search provision. 
Large numbers of cultural heritage items are now archived digitally along with accompanying metadata and are available to anyone with internet access. This information could be enriched by adding links to resources that provide background information about the items. Techniques have been developed for automatically adding links to Wikipedia to text but the methods are general and not designed for use with cultural heritage data. This paper explores a range of methods for adapting a system for adding links to Wikipedia to cultural heritage items. The approaches make use of the structure of Wikipedia, including the category hierarchy. It is found that an approach that makes use of Wikipedia’s link structure can be used to improve the quality of the Wikipedia links that are added. 
Document layout analysis is an important task needed for handwritten text recognition among other applications. Text layout commonly found in handwritten legacy documents is in the form of one or more paragraphs composed of parallel text lines. An approach for handwritten text line detection is presented which uses machinelearning techniques and methods widely used in natural language processing. It is shown that text line detection can be accurately solved using a formal methodology, as opposed to most of the proposed heuristic approaches found in the literature. Experimental results show the impact of using increasingly constrained ”vertical layout language models” in text line detection accuracy. 
Language classification is a preliminary step for most natural-language related processes. The significant quantity of multilingual documents poses a problem for traditional language-classification schemes and requires segmentation of the document to monolingual sections. This phenomenon is characteristic of classical and medieval Jewish literature, which frequently mixes Hebrew, Aramaic, Judeo-Arabic and other Hebrew-script languages. We propose a method for classification and segmentation of multi-lingual texts in the Hebrew character set, using bigram statistics. For texts, such as the manuscripts found in the Cairo Genizah, we are also forced to deal with a significant level of noise in OCR-processed text. 1. Introduction The identification of the language in which a given test is written is a basic problem in naturallanguage processing and one of the more studied ones. For some tasks, such as automatic cataloguing, it may be used stand-alone, but, more often than not, it is just a preprocessing step for some other language-related task. In some cases, even English and French, the identification of the language is trivial, due to non-identical character sets. But this is not always the case. When looking at Jewish religious documents, we often find a mixture of several languages, all with the same Hebrew character set. Besides Hebrew, these include Aramaic, which was once the lingua franca in the Middle East, and Judeo-Arabic, which was used by Jews living all over the Arab world in medieval times. Language classification has well-established methods with high success rates. In particular, character n-grams, which we dub n-chars, work  well. However, when we looked at recently digitized documents from the Cairo Genizah, we found that a large fraction contains segments in different languages, so a single language class is rather useless. Instead, we need to identify monolingual segments and classify them. Moreover, all that is available is the output of mediocre OCR of handwritten manuscripts that are themselves of poor quality and often seriously degraded. This raises the additional challenge of dealing with significant noise in the text to be segmented and classified. We describe a method for segmenting documents into monolingual sections using statistical analysis of the distribution of n-grams for each language. In particular, we use cosine distance between character unigram and bigram distributions to classify each section and perform smoothing operations to increase accuracy. The algorithms were tested on artificially produced multilingual documents. We also artificially introduced noise to simulate mistakes made in OCR. These test documents are similar in length and language shifts to real Genizah texts, so similar results are expected for actual manuscripts. 2 Related Work Language classification is well-studied, and is usually approached by character-distribution methods (Hakkinen and Tian, 2001) or dictionary-based ones. Due to the lack of appropriate dictionaries for the languages in question and their complex morphology, the dictionary-based approach is not feasible. The poor quality of the results of OCR also precludes using word lists. Most work on text segmentation is in the area of topic segmentation, which involves semantic features of the text. The problem is a simple case of structured prediction (Bakir, 2007). Text tiling (Hearst, 1993) uses a sliding-window approach.  112 Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 112–117, Avignon, France, 24 April 2012. c 2012 Association for Computational Linguistics  Similarities between adjacent blocks within the text are computed using vocabularies, counting new words introduced in each segment. These are smoothed and used to identify topic boundaries via a cutoff function. This method is not suitable for language segmentation, since each topic is assumed to appear once, while languages in documents tend to switch repeatedly. Choi (2000) uses clustering methods for boundary identification. 3. Language Classification Obviously, different languages, even when sharing the same character set, have different distribution of character occurrences. Therefore, gathering statistics on the typical distribution of letters may enable us to uncover the language of a manuscript by comparing its distribution to the known ones. A simple distribution of letters may not suffice, so it is common to employ n-chars (Hakkinen and Tian, 2001). Classification entails the following steps: (1) Collect n-char statistics for relevant languages. (2) Determine n-char distribution for the input manuscript. (3) Compute the distance between the manuscript and each language using some distance measure. (4) Classify the manuscript as being in the language with the minimal distance. The characters we work with all belong to the Hebrew alphabet, including its final variants (at the end of words). The only punctuation we take into account is inter-word space, because different languages can have different average word lengths (shorter words mean more frequent spaces), and different languages tend to have different letters at the beginnings and ends of words. For instance, a human might look for a prevalence of words ending in alef to determine that the language is Aramaic. After testing, bigrams were found to be significantly superior to unigrams and usually superior to trigrams, so bigrams were used throughout the classification process. Moreover, in the segmentation phase, we deal with very short texts on which trigram probabilities will be too sparse. We represent the distribution function as a vector of probabilities. The language with smallest cosine distance between vectors is chosen, as this measure works well in practice. 4. Language Segmentation For the splitting task, we use only n-char statistics, not presuming the availability of useful wordlists. We want the algorithm to work even if  the languages shift frequently, so we do not assume anything about the minimal or maximal length of segments. We do not, of course, consider a few words in another language to constitute a language shift. The algorithm comprises four major steps: (1) Split text into arbitrary segments. (2) Calculate characteristics of each segment. (3) Classify each. (4) Refine classifications and output final results. 4.1 Splitting the Text Documents are not always punctuated into sentences or paragraphs. So, splitting is done in the naïve way of breaking the text into fixed-size segments. As language does not shift mid-word (except for certain prefixes), we break the text between words. If sentences are delineated and one ignores possible transitions mid-sentence, then the breaks should be between sentences. The selection of segment size should depend on the language shift frequency. Nonetheless, each segment is classified using statistical properties, so it has to be long enough to have some statistical significance. But if it is too long, the language transitions will be less accurate, and if a segment contains two shifts, it will miss the inner one. Because the post-processing phase is computationally more expensive, and grows proportionally with segment length, we opt for relatively short initial segments. 4.2 Feature Extraction The core of the algorithm is the initial classification of segments. Textual classification is usually reduced to vector classification, so there each segment is represented as a vector of features. Naturally, the selection of features is critical for successful classification, regardless of classification algorithm. Several other features were tried such as hierarchical clustering of segments and classification of the clusters (Choi, 2000) but did not yield significant improvement. N-char distance – The first and most obvious feature is the classification of the segment using the methods described in Section 3. However, the segments are significantly smaller than the usual documents, so we expect lower accuracy than usual for language classification. The features are the cosine distance from each language model. This is rather natural, since we want to preserve the distances from each language model in order to combine it with other features later on. For each segment f and language l, we com-  113  pute 𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒! = 𝐷𝑖𝑠𝑡 𝑙, 𝑓 , the cosine distance of their bigram distributions. Neighboring segments language – We expect that languages in a document do not shift too frequently, since paragraphs tend to be monolingual and at least several sentences in a row will be in the same language to convey some idea. Therefore, if we are sure about one segment, there is a high chance that the next segment will be in the same language. One way to express such dependency is by post-processing the results to reduce noise. Another way is by combining the classification results of neighboring segments as features in the classification of the segment. Of course, not only neighboring segments can be considered, but all segments within some distance can help. Some parameter should be estimated to be the threshold for the distance between segments under which they will be considered neighbors. We denote by (negative or positive) Neighbor(f,i) the i-th segment before/after f. If i=0, Neighbor(f,i) = f. For each segment f and language l, we compute 𝑁𝐷𝑖𝑠𝑡!,! 𝑖 = 𝐷𝑖𝑠𝑡 𝑙, 𝑁𝑒𝑖𝑔ℎ𝑏𝑜𝑟 𝑓, 𝑖 . Whole document language - Another feature is the cosine distance of the whole document from each language model. This tends to smooth and reduce noise from classification, especially when the proportion of languages is uneven. For a monolingual document, the algorithm is expected to output the whole document as one correctly-classified segment. 4.3 Post-processing We refine the segmentation procedure as follows: We look at the results of the splitting procedure and recognize all language shifts. For each shift, we try to find the place where the shift takes place (at word granularity). We unify the two segments and then try to split the segment at N different points. For every point, we look at the cosine distance of the text before the point from the class of the first segment, and at the distance of the text after the point to the language of the second segment. For example, suppose a segment A1…An was classified as Hebrew and segment B1…Bm, which appeared immediately after was classified Aramaic. We try to split A1…An,B1…Bm at any of N points, N=2, say. First, we try F1=A1…A(n+m)/3 and F2=A(n+m)/3+1…Bm (supposing (n+m)/3<n). We look at cosine distance of F1 to Hebrew and of F2 to Aramaic. Then, we look at F1 = A1…A2(n+m)/3 and F2 = A2(n+m)/3+1…Bm. We choose the split  with the best product of the two cosine distances. The value of N is a tradeoff between accuracy and efficiency. When N is larger, we check more transition points, but for large segments it can be computationally expensive. 4.4 Noise Reduction OCR-processed documents display a significant error rate and classification precision should be expected to drop. Relying only on n-char statistics, we propose probabilistic approaches to the reduction of noise. Several methods (Kukich, 1992) have been proposed for error correction using n-chars, using letter-transition probabilities. Here, we are not interested in error correction, but rather in adjusting the segmentation to handle noisy texts. To account for noise, we introduce a $ sign, meaning “unknown” character, imagining a conservative OCR system that only outputs characters with high probability. There is also no guarantee that word boundaries will not be missed, so $ can occur instead of a space. Ignoring unrecognized n-chars – We simply ignore n-chars containing $ in the similarity measures. We assume there are enough bigrams left in each segment to successfully identify its language. Error correction – Given an unknown character, we could try correcting it using trigrams, looking for the most common trigram of the form a$b. This seems reasonable and enhances the statistical power of the n-char distribution, but does not scale well for high noise levels, since there is no solution for consecutive unknowns. Averaging n-char probabilities – When encountering the $, we can use averaging to estimate the probability of the n-char containing it. For instance, the probability of the bigram $x will be the average probability of all bigrams starting with x in a certain language. This can of course scale to longer n-chars and integrates the noisy into the computation. Top n-chars – When looking at noisy text, we can place more weight on corpus statistics, since they are error free. Therefore, we can look only at the N most common n-chars in the corpus for edit distance computing. Higher n-char space – So far we used bigrams, which showed superior performance. But when the error rate rises, trigrams may show a higher success rate.  114  5. Experimental Results We want to test the algorithm with well-defined parameters and evaluation factors. So, we created artificially mixed documents, containing segments from pairs of different languages (Hebrew/Aramaic, which is hard, Hebrew/JudeoArabic, where classification is easy and segmentation is the main challenge, or a mix of all three). The segments are produced using two parameters: The desired document length d and the average monolingual segment length k. Obviously, 𝑘 < 𝑑. We iteratively take a random number in the range [k–20:k+20] and take a substring of that length from a corpus, rounded to whole words. We cycle through the languages until the text is of size d. The smaller k, the harder to segment. 5.2 Evaluation Measures Obviously splitting will not be perfect and we cannot expect to precisely split a document. Given that, we want to establish some measures for the quality of the splitting result. We would like the measure to produce some kind of score to the algorithm output, using which we can indicate whether a certain feature or parameter in the algorithm improves it or not. However, the result quality is not well defined since it is not clear what is more important: detecting the segment's boundaries accurately, classifying each segment correctly or even split the document to the exact number of segments. For example, given a long document in Hebrew with a small segment in Aramaic, is it better to return that it actually is a long document in Hebrew with Aramaic segment but misidentify the segment's location or rather recognize the Aramaic segment perfectly but classify it as Judeo-Arabic. There are several measures for evaluating text segmentation (Lamprier et al., 2007). Correct word percentage – The most intuitive measure is simply measuring the percentage of the words classified correctly. Since the “atomic” block of the text is words (or sentences in some cases described further), which are certainly monolingual, this measure will resemble the algorithm accuracy pretty good for most cases. It is however not enough, since in some cases it does not reflect the quality of the splitting. Assume a long Hebrew document with several short sentences in Aramaic. If the Hebrew is 95% of the text, a result that classifies the whole text as Hebrew will get 95% but is pretty  useless and we may prefer a result that identifies the Aramaic segments but errs on more words. Segmentation error (SE) estimates the algorithm’s sensitivity to language shifts. It is the difference between the correct number and that returned by the algorithm, divided by correct number. Obviously, SE is in the range [–1:1]. It will indeed resemble the problem previously described, since, if the entire document is classified as Hebrew, the SE score will be very low, as the actual number is much greater than 1.  5.3 Experiments  Neighboring segments – The first thing we  tested is the way a segment’s classification is  affected by neighboring segments. We begin by  checking if adding the distance of the closest  segments enhances performance. Define  𝑆𝑐𝑜𝑟𝑒!,! = 𝐷𝑖𝑠𝑡 𝑙, 𝑓 + 𝑎 𝑁𝐷𝑖𝑠𝑡!,! 1 + 𝑁𝐷𝑖𝑠𝑡!,! −1 . For  the test we set a=0.4.  From Figures 1 and 2, one can see that neigh-  boring segments improve classification of short  segments, while on shorter ones classification  without the neighbors was superior. It is not  surprising that when using neighbors the splitting  procedure tends to split the text to longer seg-  ments, which has good effect only if segments  actually are longer. We can also see from Figure  3 that the SE measure is now positive with  k=100, which means the algorithm underesti-  mates the number of segments even when each  segment is 100 characters long. By further exper-  iments, we can see that the a parameter is insig-  nificant, and fix it at 0.3.  As expected, looking at neighboring segments  can often improve results. The next question is if  farther neighbors also do. Let: 𝑆𝑐𝑜𝑟𝑒!,! = 𝐷𝑖𝑠𝑡 𝑙, 𝑓 +  !! !!! !  𝑁𝐷𝑖𝑠𝑡!,! 𝑘1 + 𝑁𝐷𝑖𝑠𝑡!,! −𝑘  . Parameter N  stands for the longest distance of neighbors to  consider in the score. Parameter a is set to 0.3.  We see that increasing N does not have a  significant impact on algorithm performance, and  on shorter segment lengths performance drops  with N. We conclude that there is no advantage  at looking at distant neighbors.  Post-processing – Another thing we test is the  post-processing of the splitting results to refine  the initial segment choice. We try to move the  transition point from the original position to a  more accurate position using the technique  described above. We note is cannot affect the SE  measure, since we only move the transition  points without changing the classification. As  115  shown in Figure 4, it does improve the performance for all values of l. Noise reduction – To test noise reduction, we artificially added noise, randomly replacing some letters with $. Let P denote the desired noise rate and replace each letter independently with $ with probability P. Since the replacements of character is mutually independent, we can expect a normal distribution of error positions, and the correction phase described above does not assume anything about the error creation process. Error creation does not assign different probabilities for different characters in the text unlike natural OCR systems or other noisy processing. Not surprisingly, Figure 5 illustrates that the accuracy reduces as the error rate rises. However, it does not significantly drop even for a very high error rate, and obviously we cannot expect that the error reducing process will perform better then the algorithm performs on errorless text. Figure 6 illustrates the performance of each method. It looks like looking at most common nchars does not help, nor trying to correct the unrecognized character. Ignoring the unrecognized character, using either bigrams or trigrams, or estimating the missing unrecognized bigram probability show the best and pretty similar results. 6. Conclusion We have described methods for classifying texts, all using the same character set, into several languages. Furthermore, we considered segmented multilingual texts into monolingual components. In both cases, we made allowance for corrupted texts, such as that obtained by OCR from handwritten manuscripts. The results are encouraging and will be used in the Friedberg Genizah digitization project (www.genizah.org).  No Neighbours  With Neighbours   
Languages evolve, undergoing repeated small changes, some with permanent effect and some not. Changes affecting a language may be independent or contactinduced. Independent changes arise internally or, if externally, from non-linguistic causes. En masse, such changes cause isolated languages to drift apart in lexical form and grammatical structure. Contactinduced changes can happen when languages share speakers, or when their speakers are in contact. Frequently, languages in contact are related, having a common ancestor from which they still retain visible structure. This relatedness makes it difﬁcult to distinguish contact-induced change from inherited similarities. In this paper, we present a simulation of contact-induced change. We show that it is possible to distinguish contact-induced change from independent change given (a) enough data, and (b) that the contactinduced change is strong enough. For a particular model, we determine how much data is enough to distinguish these two cases at p < 0.05. 
While the debate between nativism and empiricism exists since several decades, surprisingly few common learning problems have been proposed for assessing the two opposing views. Most empiricist researchers have focused on a relatively small number of linguistic problems, such as Auxiliary Fronting or Anaphoric One. In the current paper we extend the number of common test cases to a much larger series of problems related to wh-questions, relative clause formation, topicalization, extraposition from NP and left dislocation. We show that these hard cases can be empirically solved by an unsupervised tree-substitution grammar inferred from child-directed input in the Adam corpus (Childes database). 
Michael Tomasello. 2001. Perceiving intentions and learning words in the second year of life. In Melissa Bowerman and Stephen Levinson, editors, Language Acquisition and Conceptual Development, pages 132– 158. Cambridge University Press, Cambridge. 
In this demonstration we present our web services to perform Bayesian learning for classification tasks. 
English phonotactic learning is modeled by means of the PHACTS algorithm, a topological neuronal receptive ﬁeld implementing a phonotactic activation function aimed at capturing both local (i.e., phonemic) and global (i.e., word-level) similarities among strings. Limits and merits of the model are presented. 
In this paper we present a proﬁle of verb usage across ages in child-produced sentences in English and Portuguese. We examine in particular lexical and syntactic characteristics of verbs and ﬁnd common trends in these languages as children’s ages increase, such as the prominence of general and polysemic verbs, as well as divergences such as the proportion of subject dropping. We also ﬁnd a correlation between the age of acquisition and the number of complements of a verb for English. 
Much has been discussed about the challenges posed by Multiword Expressions (MWEs) given their idiosyncratic, ﬂexible and heterogeneous nature. Nonetheless, children successfully learn to use them and eventually acquire a number of Multiword Expressions comparable to that of simplex words. In this paper we report a wide-coverage investigation of a particular type of MWE: verb-particle constructions (VPCs) in English and their usage in child-produced and child-directed sentences. Given their potentially higher complexity in relation to simplex verbs, we examine whether they appear less prominently in child-produced than in childdirected speech, and whether the VPCs that children produce are more conservative than adults, displaying proportionally reduced lexical repertoire of VPCs or of verbs in these combinations. The results obtained indicate that regardless of any additional complexity VPCs feature widely in children data following closely adult usage. Studies like these can inform the development of computational models for language acquisition. 
This paper presents Brazilian Portuguese phoneme patterns of distribution, according to an automatic grammar rulesbased grapheme to phoneme converter. The software Nhenhém (Vasilévski, 2008) was used for treating data: written texts which were decoded into phonologic symbols, forming a corpus, and subjected to a statistical analysis. Results support the high level of predictability of Brazilian Portuguese phonemes distribution, the consonantvowel syllabic pattern as the most common, as well as the stress pattern distribution 'CV.CV#. The efficiency of a phoneme-grapheme converter based entirely on rules is also proven. These results are displayed and discussed, as well as some aspects of Nhe-nhém building. 
 Input → Parser → TM → LM → Output  An open question in [FU¨ LO¨ P, MALETTI, VOGLER: Weighted extended tree transducers. Fundamenta Informaticae 111(2), 2011] asks whether weighted linear extended tree transducers preserve recognizability in countably complete commutative semirings. In this contribution, the question is answered positively, which is achieved with a construction that utilizes inside weights. Due to the completeness of the semiring, the inside weights always exist, but the construction is only effective if they can be effectively determined. It is demonstrated how to achieve this in a number of important cases. 
It has remained an open question whether the twins property for weighted tree automata is decidable. This property is crucial for determinizing such an automaton, and it has been argued that determinization improves the output of parsers and translation systems. We show that the twins property for weighted tree automata over extremal semiﬁelds is decidable. 
In this paper we present the tree to tree transduction language, TTT. We motivate the overall ”template-to-template” approach to the design of the language, and outline its constructs, also providing some examples. We then show that TTT allows transparent formalization of rules for parse tree reﬁnement and correction, logical form reﬁnement and predicate disambiguation, inference, and verbalization of logical forms. 
The simultaneously phonological and syntactic grammar of second position clitics is an instance of the broader problem of applying constraints across multiple levels of linguistic analysis. Syntax frameworks extended with simple tree transductions can make efﬁcient use of these necessary additional forms of structure. An analysis of Sahidic Coptic second position clitics in a context-free grammar extended by a monadic second-order transduction exempliﬁes this approach. 
Unsupervised dependency parsing is one of the most challenging tasks in natural languages processing. The task involves ﬁnding the best possible dependency trees from raw sentences without getting any aid from annotated data. In this paper, we illustrate that by applying a supervised incremental parsing model to unsupervised parsing; parsing with a linear time complexity will be faster than the other methods. With only 15 training iterations with linear time complexity, we gain results comparable to those of other state of the art methods. By employing two simple universal linguistic rules inspired from the classical dependency grammar, we improve the results in some languages and get the state of the art results. We also test our model on a part of the ongoing Persian dependency treebank. This work is the ﬁrst work done on the Persian language. 
Building shallow semantic representations from text corpora is the ﬁrst step to perform more complex tasks such as text entailment, enrichment of knowledge bases, or question answering. Open Information Extraction (OIE) is a recent unsupervised strategy to extract billions of basic assertions from massive corpora, which can be considered as being a shallow semantic representation of those corpora. In this paper, we propose a new multilingual OIE system based on robust and fast rule-based dependency parsing. It permits to extract more precise assertions (verb-based triples) from text than state of the art OIE systems, keeping a crucial property of those systems: scaling to Web-size document collections. 
Topic Models (TM) such as Latent Dirichlet Allocation (LDA) are increasingly used in Natural Language Processing applications. At this, the model parameters and the inﬂuence of randomized sampling and inference are rarely examined — usually, the recommendations from the original papers are adopted. In this paper, we examine the parameter space of LDA topic models with respect to the application of Text Segmentation (TS), speciﬁcally targeting error rates and their variance across different runs. We ﬁnd that the recommended settings result in error rates far from optimal for our application. We show substantial variance in the results for different runs of model estimation and inference, and give recommendations for increasing the robustness and stability of topic models. Running the inference step several times and selecting the last topic ID assigned per token, shows considerable improvements. Similar improvements are achieved with the mode method: We store all assigned topic IDs during each inference iteration step and select the most frequent topic ID assigned to each word. These recommendations do not only apply to TS, but are generic enough to transfer to other applications. 
Clustered word classes have been used in connection with statistical machine translation, for instance for improving word alignments. In this work we investigate if clustered word classes can be used in a preordering strategy, where the source language is reordered prior to training and translation. Part-of-speech tagging has previously been successfully used for learning reordering rules that can be applied before training and translation. We show that we can use word clusters for learning rules, and signiﬁcantly improve on a baseline with only slightly worse performance than for standard POS-tags on an English–German translation task. We also show the usefulness of the approach for the less-resourced language Haitian Creole, for translation into English, where the suggested approach is signiﬁcantly better than the baseline. 
Relation extraction is frequently and successfully addressed by machine learning methods. The downside of this approach is the need for annotated training data, typically generated in tedious manual, cost intensive work. Distantly supervised approaches make use of weakly annotated data, like automatically annotated corpora. Recent work in the biomedical domain has applied distant supervision for proteinprotein interaction (PPI) with reasonable results making use of the IntAct database. Such data is typically noisy and heuristics to ﬁlter the data are commonly applied. We propose a constraint to increase the quality of data used for training based on the assumption that no self-interaction of realworld objects are described in sentences. In addition, we make use of the University of Kansas Proteomics Service (KUPS) database. These two steps show an increase of 7 percentage points (pp) for the PPI corpus AIMed. We demonstrate the broad applicability of our approach by using the same workﬂow for the analysis of drug-drug interactions, utilizing relationships available from the drug database DrugBank. We achieve 37.31 % in F1 measure without manually annotated training data on an independent test set. 
We introduce Conflict-Driven Co-Clustering, a novel algorithm for data co-clustering, and apply it to the problem of inducing parts-ofspeech in a corpus of child-directed spoken English. Co-clustering is preferable to unidimensional clustering as it takes into account both item and context ambiguity. We show that the categorization performance of the algorithm is comparable with the coclustering algorithm of Leibbrandt and Powers (2008), but out-performs that algorithm in robustly pruning less-useful clusters and merging them into categories strongly corresponding to the three main open classes of English. 
Dependency Parsing domain adaptation involves adapting a dependency parser, trained on an annotated corpus from a given domain (e.g., newspaper articles), to work on a different target domain (e.g., legal documents), given only an unannotated corpus from the target domain. We present a shift/reduce dependency parser that can handle unlabeled sentences in its training set using a transductive SVM as its action selection classiﬁer. We illustrate the the experiments we performed with this parser on a domain adaptation task for the Italian language.  when applied on text from domains not covered by their training corpus. Several techniques have been proposed to adapt a parser to a new domain, even when only unannotated samples from it are available (Attardi et al., 2007a; Sagae and Tsujii, 2007). In this work we present a domain adaptation based on the semi-supervised training of the classiﬁer of a shift-reduce parser. We implement the classiﬁer as a multi-class SVM and train it with a transductive SVM algorithm that handles both labeled examples (generated from the source-domain annotated corpus) and unlabeled examples (generated from the the target-domain unannotated corpus).  
Unsupervised part-of-speech (POS) tagging has recently been shown to greatly beneﬁt from Bayesian approaches where HMM parameters are integrated out, leading to signiﬁcant increases in tagging accuracy. These improvements in unsupervised methods are important especially in specialized social media domains such as Twitter where little training data is available. Here, we take the Bayesian approach one step further by integrating semantic information from an LDA-like topic model with an HMM. Speciﬁcally, we present Part-of-Speech LDA (POSLDA), a syntactically and semantically consistent generative probabilistic model. This model discovers POS speciﬁc topics from an unlabelled corpus. We show that this model consistently achieves improvements in unsupervised POS tagging and language modeling over the Bayesian HMM approach with varying amounts of side information in the noisy and esoteric domain of Twitter. 
In this paper, we address the issue of how different personalities interact in Twitter. In particular we study users’ interactions using one trait of the standard model known as the “Big Five”: emotional stability. We collected a corpus of about 200000 Twitter posts and we annotated it with an unsupervised personality recognition system. This system exploits linguistic features, such as punctuation and emoticons, and statistical features, such as followers count and retweeted posts. We tested the system on a dataset annotated with personality models produced from human judgements. Network analysis shows that neurotic users post more than secure ones and have the tendency to build longer chains of interacting users. Secure users instead have more mutual connections and simpler networks. 
Recognizing speech act types in Twitter is of much theoretical interest and practical use. Our previous research did not adequately address the deficiency of training data for this multi-class learning task. In this work, we set out by assuming only a small seed training set and experiment with two semi-supervised learning schemes, transductive SVM and graph-based label propagation, which can leverage the knowledge about unlabeled data. The efficacy of semi-supervised learning is established by our extensive experiments, which also show that transductive SVM is more suitable than graph-based label propagation for our task. The empirical findings and detailed evidences can contribute to scalable speech act recognition in Twitter.  linguists, ranging from a few to over a hundred types. In this work, we adopt the 5 types of speech act used in our previous work (Zhang et al. 2011), which are in turn inherited from (Searle 1975): statement, question, suggestion, comment, and miscellaneous. Our choice is based on the fact that unlike face-to-face communication, twittering is more in a “broadcasting” style than on a personal basis. Statement and comment, which are usually intended to make one’s knowledge, thought, and sentiment known, thus befit Twitter’s communicative style. Question and suggestion on Twitter are usually targeted at other tweeters in general or one’s followers. More interpersonal speech acts such as “threat” or “thank” as well as rare speech acts in Twitter (Searle’s (1975) “commissives” and “declaratives”) are relegated to “miscellaneous”. Some examples from our experimental datasets are provided in Table 1.  1. Introduction  The social media platform of Twitter makes available a plethora of data to probe the communicative act of people in a social network woven by interesting events, people, topics, etc. Communicative acts such as disseminating information, asking questions, or expressing feelings all fall in the purview of “speech act”, a long established area in pragmatics (Austin 1962). The automatic recognition of speech act in tons of tweets has both theoretical and practical appeal. Practically, it helps tweeters to find topics to read or tweet about based on speech act compositions. Theoretically, it introduces a new dimension to study social media content as well as providing real-life data to validate or falsify claims in the speech act theory. Different taxonomies of speech act have been proposed by linguists and computational  18 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 18–27, Avignon, France, April 23 - 27 2012. c 2012 Association for Computational Linguistics  Tweet  Speech Act  Libya Releases 4 Times Statement  Journalists  -  http://www.photozz.com/?104k  #sincewebeinghonest why u so obsessed with what me n her do?? Don't u got ya own man???? Oh wait.....  Question  RT @NaonkaMixon: I will donate 10 $ to the Red Cross Japan Earthquake fund for every person that retweets this! #PRAYFORJAPAN  Suggestion  is enjoying this new season of Comment #CelebrityApprentice.... Nikki Taylor = Yum!!  65. I want to get married to Miscellaneous someone i meet in highschool. #100factsaboutme  Table 1. Example Tweets with Speech acts  Assuming one tweet demonstrates only one speech act, the automatic recognition of those speech act types in Twitter is a multi-class classification task. We concede that this assumption may not always hold in real situations. But given the short length of tweets, multi-speech act tweets are rare and we find this simplifying assumption effective in reducing the complexity of our problem. A major problem with this task is the deficiency of training data. Tweeters as well as face-to-face interlocutors do not often identify their speech acts; human annotation is costly and time-consuming. Although our previous research (Zhang et al. 2011) sheds light on the preparation of training data, it did not adequately address this problem. Our contribution in this work is to directly address the problem of training data deficiency by using two well-known semi-supervised learning techniques that leverage the relationship between a small seed of training data and a large body of unlabeled data: transductive SVM and graph-based label propagation. The empirical results show that the knowledge about unlabeled data provides promising solutions to the data deficiency problem, and that transductive SVM is more competent for our task. Our exploration with different training/unlabeled data ratios for three major Twitter categories and a mixed-type category provides solid evidential support for future research.  The rest of the paper is organized as follows. Section 2 reviews works related to speech act recognition and semi-supervised learning; Section 3 briefly discusses supervised learning of speech act types developed in our earlier work and complementing the previous findings with learning curves. The technical details of semisupervised learning are presented in Section 4. Then we report and discuss the results of our experiments in Section 5. Finally, Section 6 concludes the paper and outlines future directions. 2. Related Work The automatic recognition of speech act, also known as “dialogue act”, has attracted sustained interest in computational linguistics and speech technology for over a decade (Searle 1975; Stolcke et al. 2000). A few annotated corpora such as Switchboard-DAMSL (Jurafsky et al. 1997) and Meeting Recorder Dialog Act (Dhillon et al. 2004) are widely used, with data transcribed from telephone or face-to-face conversation. Prior to the flourish of microblogging services such as Twitter, speech act recognition has been extended to electronic media such as email and discussion forum (Cohen et al. 2004; Feng et al. 2006) in order to study the behavior of email or message senders. The annotated corpora for ordinary verbal communications and the methods developed for email, or discussion forum cannot be directly used for our task because Twitter text has a distinctive Netspeak style that is situated between speech and text but resembles neither (Crystal 2006, 2011). Compared with email or forum post, it is rife with linguistic noises such as spelling mistakes, random coinages, mixed use of letters and symbols. Speech act recognition in Twitter is a fairly new task. In our pioneering work (Zhang et al. 2011), we show that Twitter text normalization is unnecessary and even counterproductive for this task. More importantly, we propose a set of useful features and draw empirical conclusion about the scope of this task, such as recognizing speech act on the coarse-grade category level works as well as on the fine-grade topic level. In this work, we continue to adopt this framework including other learning details (speech act types and feature selection for tweets), but the new quest starts where the old one left: tackling insufficient training data.  19  As in many practical applications, sufficient annotated data are hard to obtain. Therefore, unsupervised and semi-supervised learning methods are actively pursued. While unsupervised sentence classification is rule-based and domain-dependent (Deshpande et al. 2010), semi-supervised methods that both alleviate the data deficiency problem and leverage the power of state-of-the-art classifiers hold more promises for different domains (Medlock and Briscoe 2007; Erkan et al. 2007). In the machine learning literature, a classic semi-supervised learning scheme is proposed by Yarowsky (1995), which is a classical selfteaching process that makes no use of labeled data before they are classified. More theoretical analyses are made by (Culp and Michailidis 2007) and (Haffari and Sarkar 2007). Transductive SVM (Joachims 1999) extends the state-of-the-art inductive SVM by explicitly considering the relationship between labeled and unlabeled data. The graph-based label propagation model (Zhu et al. 2003; Zhou et al. 2004) using a harmonic function also accommodates the knowledge about unlabeled data. We will adapt both of them to our multiclass classification task. Jeong et al. (2009) report a semi-supervised approach to classifying speech acts in emails and online forums. But their subtree-based method is not applicable to our task because Twitter’s noisy textual quality cannot be found in the much cleaner email or forum texts. 3. Supervised Learning of Speech Act Types Supervised learning of speech act types in Twitter relies heavily on a good set of features that capture the textual characteristics of both Twitter and speech act utterances. As in our previous work, we use speech act-specific cues, special words (abbreviations and acronyms, opinion words, vulgar words, and emoticons), and special characters (Twitter-specific characters and a few punctuations). Tweetexternal features such as tweeter profile may also help, but that is beyond the focus of this paper. Although it has been empirically shown that speech act recognition in Twitter can be done without using training data specific to topics or even categories, it is not clear how much training data is needed to achieve desirable performance. In order to answer this question, we adopt the same experimental setup and datasets as reported  in (Zhang et al. 2011) and plot the learning curves shown in Figure 1. Figure 1. Learning Curves of Each Category and All Tweets For all individual experiments, the test data are a randomly sampled 10% set of all annotated data. When training data reach 90%, we actually duplicate the reported results. However, Figure 1 shows that it is unnecessary to use so much training data to achieve good classification performance. For News and Entity, the classification makes little noticeable improvement after the training data ratio reaches 40% (training : test = 4 : 1). For Mixed (the aggregate of the News, Entity, LST datasets) and LST, performance peaks even earlier at 20% training data (training : test = 2 : 1) and 10% (training : test = 1 : 1). It is delightful to see that only a moderate number of annotated data are needed for speech act recognition. But even that number (for the Mixed dataset, 10% training data are over 800 annotated tweets) may not be available and in many situations, test data may be much more than training data. Taking this challenge is the next important step we make. 4. Semi-Supervised Learning of Speech Act Types The problem setting of a small seed training (labeled) set and a much larger test (labeled) set fits the semi-supervised learning scheme. Classic semi-supervised learning approaches such as self-teaching methods (e.g., Yarowsky 1995) are mainly concerned with incrementing highconfidence labeled data in each round of training. They do not, however, directly take into account the knowledge about unlabeled data. The recent research emphasis is on leveraging knowledge about unlabeled data during training. In this section, we discuss two such approaches.  20  4.1 Transductive SVM The standard SVM classifier popularly used in text classification is also known as inductive SVM as a model is induced from training data. The model is solely dependent on the training data and agnostic about the test data. In contrast, transductive SVM (Vapnik 1998; Joachims 1999) predicts test labels by using the knowledge about test data. In the case of test (unlabeled) data far outnumbering training (labeled) data, transductive SVM provides a feasible scheme of semi-supervised learning. For a single-class classification problem {xi, yi} that focuses on only one speech act type, where xi is the ith tweet and yi is the corresponding label and yi {1, 1} denotes whether xi contains the speech act or not, inductive SVM is formulated to find an optimal hyperplane sign(w∙xi – b) to maximize the soft margin between positive and negative objects, or to minimize:  object similarities. This is the idea underlying Zhu et al.’s (2003) graph-based label propagation model using Gaussian random fields. We again focus on a single-class classification problem. Formally, {x1, … xN} are N tweets, having their actual speech act labels y = {y1, … yL, … yN} (yi ∊{1, 0} denoting whether xi contains the speech act or not) with the first L of them known, and f = {f1, … fL, … fN} are their predicted labels. Let L = {x1, … xL} and U = {xL+1, … xN} and the task is to determine {fL+1, … fN} for U. We further define a graph G = (V, E), where V = L∪U and E is weighted by W = [wij]N×N with wij denoting the similarity between xi and xj. Preferring label smoothness on G and preserving the given labels, we want to minimize the loss function:   E(f ) 1/ 2  wij ( fi  f j )2  f T Δf  i, jLU  s.t. fi = yi (i = 1, …, L)   1 / 2 w 2  C i i s.t. yi (xi  w  b) 1i , i  0  where i is a slack variable. Adopting the same formulation, transductive SVM further considers test data xi* during training by finding a labeling yj* and a hyperplane to maximize the soft margin between both training and test data, or to minimize:    1 / 2 w 2  C1 i  C2 i  i  i  s.t. yi (xi  w  b) 1i , i  0  yi*(xi*  w  b) 1i , i  0  where i is a slack variable for the test data. In fact, labeling test data is done during training. As the maximal margin approach proves very effective for text classification, its transductive variant that effectively uses the knowledge about test data holds promises of handling the deficiency of labeled data. 4.2 Graph-based Label Propagation An alternative way of using unlabeled data in semi-supervised learning is based on the intuition that similar objects should belong to the same class, which can be translated into label smoothness on a graph with weights indicating  where Δ = D − W is the combinatorial graph Laplacian with D being a diagonal matrix [dij]N×N  and dii  wij . j  This can be expressed as a harmonic function,  h = argmin fL = yLE(f), which satisfies the smoothness property on the graph:  h(i) 1 / dii wik (h(k)) . If we define k  pij  wij / wik and collect pij and h(i) into k matrix P and column vector h, solving Δh = 0 s.t. hL = yL is equivalent to solving h = Ph. To find the solution, we can use L and U to partition h and P:  h    hL hU      ,  P    PLL PUL  , ,  PLU PUU      and it can be shown that hU  (I  PUU )1PULyL . To get the final classification result, those elements in hU that are greater than a threshold (0.5) become 1 and the others become 0. This approach propagates labels from labeled data to unlabeled data on the principle of label smoothness. If the assumption about similar tweets having same speech acts holds, it should work well for our problem.  4.3 Multi-class Classification In the previous formulations, we emphasized “single-class classification” because both  21  transductive SVM and graph-based label propagation are inherently one class-oriented. Since our problem is a multi-class one, we transform the problem to single-class classifications by using the one-vs-all scheme. Specifically, for each class (speech act type) ci, we label all training instances belonging to ci as +1 and all those belonging to other classes as −1 and then do binary classification. For our problem with 5 speech act types, we make 5 such transformations. The final prediction is made by choosing the class with the highest classification score from the 5 binary classifiers. Both transductive SVM and graph-based label propagation produce real-valued classification scores and are amenable to this scheme.  Figure 3. Speech Act Distribution (Entity)  5. Experiments  Our experiments are designed to answer two questions: 1) How useful is semi-supervised speech act learning in comparison with supervised learning? 2) Which semi-supervised learning approach is more appropriate for our problem?  Figure 4. Speech Act Distribution (LST)  5.1 Experimental Setup We use the 6 datasets in our previous study1, which fall into 3 categories: News, Entity, Longstanding Topic (LST). Each of the total 8613 tweets is labeled with one of the following speech act types: sta (statement), que (question), sug (suggestion), com (comment), mis (miscellaneous). In addition, we randomly select 1000 tweets from each of the categories to create a Mixed category of 3000 tweets. Figures 2 to 5 illustrate the distributions of the speech act types in the 3 original categories and the Mixed category.  Figure 5. Speech Act Distribution (Mixed) For each category, we use two labeled/unlabeled data settings, with labeled data accounting for 5% and 10% of the total so that the labeled/unlabeled ratios are set at approximately 1:19 and 1:9. The labeled data in each category are randomly selected in a stratified way: using the same percentage to select labeled data with each speech act type. The stratified selection is intended to keep the speech act distributions in both labeled and unlabeled data. Table 2 and Table 3 list the details of data splitting using the two settings.  Figure 2. Speech Act Distribution (News)  Category # Labeled # Unlabeled Total  News  155  2995  3150  Entity  72  1391  1463  LST  198  3802  4000  Mixed  147  2853  3000  Table 2. Stratified Data Splitting with 5% as  Labeled  
Classifying blog posts by topics is useful for applications such as search and marketing. However, topic classiﬁcation is time consuming and error prone, especially in an open domain such as the blogosphere. The state-of-the-art relies on supervised methods, requiring considerable training effort, that use the whole corpus vocabulary as features, demanding considerable memory to process. We show an effective alternative whereby distant supervision is used to obtain training data: we use Wikipedia articles labelled with Freebase domains. We address the memory requirements by using only named entities as features. We test our classiﬁer on a sample of blog posts, and report up to 0.69 accuracy for multi-class labelling and 0.9 for binary classiﬁcation. 
Originating from a multidisciplinary research project that gathers, around the Semantic Web standards and principles, Social Networking and Natural Language Processing along with some Bioinformatics notions, this paper sheds the light on some of the most critical aspects of the correspondingly adopted framework and realtime knowledge architecture and modeling platform. It recognizes the considerable profits of an appropriate fusion between the aforementioned disciplines, especially via the proper exploitation of OWL 2 (Web Ontology Language) features and novelties, typically OWL 2 language profiles. Accordingly, it proposes a distinctive workflow with well-defined strategies for an ontology-aware user and NLP-assisted flexible and multidimensional approach for the management of the abundantly available Social data. Application scenarios related to awareness and orientation recommender systems based on biomedical domain ontologies for childhood obesity prevention and surveillance are explored as typical proof of concept application areas. 
Text mining of massive Social Media postings presents interesting challenges for NLP applications due to sparse interpretation contexts, grammatical and orthographical variability as well as its very fragmentary nature. No single methodological approach can be expected to work across such diverse typologies as twitter micro-blogging, customer reviews, carefully edited blogs, etc. In this paper we present a modular and scalable framework to Social Media Opinion Mining that combines stochastic and symbolic techniques to structure a semantic space to exploit and interpret efficiently. We describe the use of this framework for the discovery and clustering of opinion targets and topics in user-generated comments for the Telecom and Automotive domains. 
To what extend can one use Twitter in opinion polls for political elections? Merely counting Twitter messages mentioning political party names is no guarantee for obtaining good election predictions. By improving the quality of the document collection and by performing sentiment analysis, predictions based on entity counts in tweets can be considerably improved, and become nearly as good as traditionally obtained opinion polls. 
In this paper, we propose the use of ﬁnegrained information such as opinions and suggestions extracted from users’ reviews about products, in order to improve a recommendation system. While typical recommender systems compare a user proﬁle with some reference characteristics to rate unseen items, they rarely make use of the content of reviews users have done on a given product. In this paper, we show how we applied an opinion extraction system to extract opinions but also suggestions from the content of the reviews, use the results to compare other products with the reviewed one, and eventually recommend a better product to the user. 
Numerous sentiment analysis applications make usage of a sentiment lexicon. In this paper we present experiments on hybrid sentiment lexicon acquisition. The approach is corpus-based and thus suitable for languages lacking general dictionarybased resources. The approach is a hybrid two-step process that combines semisupervised graph-based algorithms and supervised models. We evaluate the performance on three tasks that capture different aspects of a sentiment lexicon: polarity ranking task, polarity regression task, and sentiment classiﬁcation task. Extensive evaluation shows that the results are comparable to those of a well-known sentiment lexicon SentiWordNet on the polarity ranking task. On the sentiment classiﬁcation task, the results are also comparable to SentiWordNet when restricted to monosentimous (all senses carry the same sentiment) words. This is satisfactory, given the absence of explicit semantic relations between words in the corpus. 
This paper describes several novel hybrid semantic similarity measures. We study various combinations of 16 baseline measures based on WordNet, Web as a corpus, corpora, dictionaries, and encyclopedia. The hybrid measures rely on 8 combination methods and 3 measure selection techniques and are evaluated on (a) the task of predicting semantic similarity scores and (b) the task of predicting semantic relation between two terms. Our results show that hybrid measures outperform single measures by a wide margin, achieving a correlation up to 0.890 and MAP(20) up to 0.995. 
Dependency parsing has made many advancements in recent years, in particular for English. There are a few dependency parsers that achieve comparable accuracy scores with each other but with very different types of errors. This paper examines creating a new dependency structure through ensemble learning using a hybrid of the outputs of various parsers. We combine all tree outputs into a weighted edge graph, using 4 weighting mechanisms. The weighted edge graph is the input into our ensemble system and is a hybrid of very different parsing techniques (constituent parsers, transitionbased dependency parsers, and a graphbased parser). From this graph we take a maximum spanning tree. We examine the new dependency structure in terms of accuracy and errors on individual part-of-speech values. The results indicate that using a greater number of more varied parsers will improve accuracy results. The combined ensemble system, using 5 parsers based on 3 different parsing techniques, achieves an accuracy score of 92.58%, beating all single parsers on the Wall Street Journal section 23 test set. Additionally, the ensemble system reduces the average relative error on selected POS tags by 9.82%. 
This contribution addresses generation of natural language descriptions for human actions, behaviour and their relations with other objects observed in video streams. The work starts with implementation of conventional image processing techniques to extract high level features from video. These features are converted into natural language descriptions using context free grammar. Although feature extraction processes are erroneous at various levels, we explore approaches to putting them together to produce a coherent description. Evaluation is made by calculating ROUGE scores between human annotated and machine generated descriptions. Further we introduce a task based evaluation by human subjects which provides qualitative evaluation of generated descriptions. 
OCR (Optical Character Recognition) scanners do not always produce 100% accuracy in recognizing text documents, leading to spelling errors that make the texts hard to process further. This paper presents an investigation for the task of spell checking for OCR-scanned text documents. First, we conduct a detailed analysis on characteristics of spelling errors given by an OCR scanner. Then, we propose a fully automatic approach combining both error detection and correction phases within a unique scheme. The scheme is designed in an unsupervised & data-driven manner, suitable for resource-poor languages. Based on the evaluation on real dataset in Vietnamese language, our approach gives an acceptable performance (detection accuracy 86%, correction accuracy 71%). In addition, we also give a result analysis to show how accurate our approach can achieve. 
This paper contrasts the content and form of objective versus subjective texts. A collection of on-line newspaper news items serve as objective texts, while parliamentary speeches (debates) and blog posts form the basis of our subjective texts, all in Portuguese. The aim is to provide general linguistic patterns as used in objective written media and subjective speeches and blog posts, to help construct domainindependent templates for information extraction and opinion mining. Our hybrid approach combines statistical data along with linguistic knowledge to ﬁlter out irrelevant patterns. As resources for subjective classiﬁcation are still limited for Portuguese, we use a parallel corpus and tools developed for English to build our subjective spoken corpus, through annotations produced for English projected onto a parallel corpus in Portuguese. A measure for the saliency of n-grams is used to extract relevant linguistic patterns deemed “objective” and “subjective”. Perhaps unsurprisingly, our contrastive approach shows that, in Portuguese at least, subjective texts are characterized by markers such as descriptive, reactive and opinionated terms, while objective texts are characterized mainly by the absence of subjective markers. 
We present a joint system for named entity recognition (NER) and entity linking (EL), allowing for named entities mentions extracted from textual data to be matched to uniquely identiﬁable entities. Our approach relies on combined NER modules which transfer the disambiguation step to the EL component, where referential knowledge about entities can be used to select a correct entity reading. Hybridation is a main feature of our system, as we have performed experiments combining two types of NER, based respectively on symbolic and statistical techniques. Furthermore, the statistical EL module relies on entity knowledge acquired over a large news corpus using a simple rule-base disambiguation tool. An implementation of our system is described, along with experiments and evaluation results on French news wires. Linking accuracy reaches up to 87%, and the NER Fscore up to 83%. 
This article reports some initial results from the collaborative work on converting SWBD-DAMSL annotation scheme used in the Switchboard Dialogue Act Corpus to ISO DA annotation framework, as part of our on-going research on the interoperability of standardized linguistic annotations. A qualitative assessment of the conversion between the two annotation schemes was performed to verify the applicability of the new ISO standard using authentic transcribed speech. The results show that in addition to a major part of the SWBD-DAMSL tag set that can be converted to the ISO DA scheme automatically, some problematic SWBD-DAMSL tags still need to be handled manually. We shall report the evaluation of such an application based on the preliminary results from automatic mapping via machine learning techniques. The paper will also describe a user-friendly graphical interface that was designed for manual manipulation. The paper concludes with discussions and suggestions for future work.  1. Introduction This article describes the collaborative work on applying the newly proposed ISO standard for dialogue act annotation to the Switchboard Dialogue Act (SWBD-DA) Corpus, as part of our on-going effort to promote interoperability of standardized linguistic annotations with the ultimate goal of developing shared and open language resources. Dialogue acts (DA) play a key role in the interpretation of the communicative behaviour of dialogue participants and offer valuable insight into the design of human-machine dialogue systems (Bunt et al., 2010). More recently, the emerging ISO DIS 24617-2 (2010) standard for dialogue act annotation defines dialogue acts as the ‘communicative activity of a participant in dialogue interpreted as having a certain communicative function and semantic content, and possibly also having certain functional dependence relations, rhetorical relations and feedback dependence relations’ (p. 3). The semantic content specifies the objects, relations, events, etc. that the dialogue act is about; the communicative function can be viewed as a specification of the way an addressee uses the semantic content to update his or her information state when he or she understands the corresponding stretch of dialogue. Continuing efforts have been made to identify and classify the dialogue acts expressed in dialogue utterances taking into account the empirically proven multifunctionality of utterances, i.e., the fact that utterances often express more than one dialogue act (see Bunt, 2009 and 2011). In other words, an utterance in dialogue typically serves several functions. See Example (1) taken from the SWBD-DA Corpus (sw_0097_3798.utt).  (1) A: B:  Well, Michael, what do you think about, uh, funding for AIDS research? Do you… Well, uh, uh, that’s something I’ve thought a lot about.  With the first utterance, Speaker A performs two dialogue acts: he (a) assigns the next turn to the participant Michael, and (b) formulates an open question. Speaker B, in his response, (a) accepts the turn, (b) stalls for time, and (c) answers the question by making a statement. Our concern in this paper is to explore the applicability of the new ISO Standard to the existing Switchboard corpus with joint efforts of automatic and manual mapping. In the rest of the paper, we shall first describe the Switchboard Dialogue Act (SWBD-DA) Corpus and its annotation scheme (i.e. SWBD-DAMSL). We shall then describe the new ISO Standard and explain our mapping of SWBD-DAMSL to the ISO DIS 24617-2 DA tag set. In addition, machine learning techniques are employed for automatic DA classification on the basis of lexical features to evaluate the application of the new ISO DA scheme using authentic transcribed speech. We shall then introduce the user interface designed for manual mapping and explain the annotation guidelines. Finally, the paper will conclude with discussions and suggestions for future work.  2. Corpus Resource This study uses the Switchboard Dialog Act (SWBD-DA) Corpus as the corpus resource, which is available online from the Linguistic Data Consortium 1 . The corpus  
Within Information Extraction tasks, Named Entity Recognition has received much attention over latest decades. From symbolic / knowledge-based to data-driven / machine-learning systems, many approaches have been experimented. Our work may be viewed as an attempt to bridge the gap from the data-driven perspective back to the knowledge-based one. We use a knowledge-based system, based on manually implemented transducers, that reaches satisfactory performances. It has the undisputable advantage of being modular. However, such a hand-crafted system requires substantial efforts to cope with dedicated tasks. In this context, we implemented a pattern extractor that extracts symbolic knowledge, using hierarchical sequential pattern mining over annotated corpora. To assess the accuracy of mined patterns, we designed a module that recognizes Named Entities in texts by determining their most probable boundaries. Instead of considering Named Entity Recognition as a labeling task, it relies on complex context-aware features provided by lower-level systems and considers the tagging task as a markovian process. Using thos systems, coupling knowledge-based system with extracted patterns is straightforward and leads to a competitive hybrid NE-tagger. We report experiments using this system and compare it to other hybridization strategies along with a baseline CRF model. 
 When digitizing a print bilingual dictionary, whether via optical character recognition or manual entry, it is inevitable that errors are introduced into the electronic version that is created. We investigate automating the process of detecting errors in an XML representation of a digitized print dictionary using a hybrid approach that combines rulebased, feature-based, and language modelbased methods. We investigate combining methods and show that using random forests is a promising approach. We ﬁnd that in isolation, unsupervised methods rival the performance of supervised methods. Random forests typically require training data so we investigate how we can apply random forests to combine individual base methods that are themselves unsupervised without requiring large amounts of training data. Experiments reveal empirically that a relatively small amount of data is sufﬁcient and can potentially be further reduced through speciﬁc selection criteria. 
Question answering systems answer correctly to different questions because they are based on different strategies. In order to increase the number of questions which can be answered by a single process, we propose solutions to combine two question answering systems, QAVAL and RITEL. QAVAL proceeds by selecting short passages, annotates them by question terms, and then extracts from them answers which are ordered by a machine learning validation process. RITEL develops a multi-level analysis of questions and documents. Answers are extracted and ordered according to two strategies: by exploiting the redundancy of candidates and a Bayesian model. In order to merge the system results, we developed different methods either by merging passages before answer ordering, or by merging end-results. The fusion of endresults is realized by voting, merging, and by a machine learning process on answer characteristics, which lead to an improvement of the best system results of 19 %. 
Many tasks in natural language processing require that sentences be classiﬁed from a set of discrete interpretations. In these cases, there appear to be great beneﬁts in using hybrid systems which apply multiple analyses to the test cases. In this paper, we examine a general principle for building hybrid systems, based on combining the results of several, high precision heuristics. By generalising the results of systems for sentiment analysis and ambiguity recognition, we argue that if correctly combined, multiple techniques classify better than single techniques. More importantly, the combined techniques can be used in tasks where no single classiﬁcation is appropriate. 
Prepositions are hard to translate, because their meaning is often vague, and the choice of the correct preposition is often arbitrary. At the same time, making the correct choice is often critical to the coherence of the output text. In the context of statistical machine translation, this difﬁculty is enhanced due to the possible long distance between the preposition and the head it modiﬁes, as opposed to the local nature of standard language models. In this work we use monolingual language resources to determine the set of prepositions that are most likely to occur with each verb. We use this information in a transfer-based Arabic-to-Hebrew statistical machine translation system. We show that incorporating linguistic knowledge on the distribution of prepositions signiﬁcantly improves the translation quality. 
Summarization, like other natural language processing tasks, is tackled with a range of different techniques - particularly machine learning approaches, where human intuition goes into attribute selection and the choice and tuning of the learning algorithm. Such techniques tend to apply differently in different contexts, so in this paper we describe a hybrid approach in which a number of different summarization techniques are combined in a rule-based system using manual knowledge acquisition, where human intuition, supported by data, speciﬁes not only attributes and algorithms, but the contexts where these are best used. We apply this approach to automatic summarization of legal case reports. We show how a preliminary knowledge base, composed of only 23 rules, already outperforms competitive baselines. 
Research syntheses suggest that verbal cues are more diagnostic of deception than other cues. Recently, to avoid human judgmental biases, researchers have sought to find faster and more reliable methods to perform automatic content analyses of statements. However, diversity of methods and inconsistent findings do not present a clear picture of effectiveness. We integrate and statistically synthesize this literature. Our meta-analyses revealed small, but significant effect-sizes on some linguistic categories. Liars use fewer exclusive words, self- and other-references, fewer time-related, but more space-related, negative and positive emotion words, and more motion verbs or negations than truth-tellers. 1. Introduction Meta-analytic findings indicate that human judges are just slightly better than chance at discriminating between truths and lies (Bond, & DePaulo, 2006). Likewise, meta-analyses of training programs designed to teach lie detection have shown a small to medium effect size in improving judges' detection accuracy (e.g., Hauch, Sporer, Michael, & Meissner, 2010). Together, these findings suggest that there is a great need to better understand factors involved in deception and find ways to improve its detection. Attempts at these tasks have led researchers to use computer programs to analyze  linguistic markers in truthful and deceptive statements. A number of verbal cues have been shown to differ in lies and truths (DePaulo, Lindsay, Malone, Muhlenbruck, Charlton, & Cooper, 2003; Sporer, 2004, Vrij, 2008), and teaching content cues has shown to improve detection more effectively than teaching nonverbal or paraverbal cues (Hauch et al., 2010). The automatization of lie detection is appealing for at least two reasons. First, such systems can be considered more objective than human judges who are prone to biases (Levine, Park, & McCornack, 1999). Second, online judgments of various deception cues from videos or transcripts can tax the cognitive capacity of judges and lead to time delays and errors. Researchers have used different computer programs for the evaluation of the truth status. Computers can quickly analyze large amounts of text and provide more reliable data. Moreover, the linguistic categories evaluated across studies have varied. In some cases, the direction of the effect for the same linguistic categories has been opposite across studies, or opposite to theoretically-based predictions. These methodological differences and inconsistencies in findings calls for a quantitative analysis and integration of findings. This is the goal of the present meta-analytic review. 2. Method After a thorough literature search (Social Sciences Citation Index, PsycInfo, Dissertation Abstracts, Google Scholar, and cited reference searches), a large number (k = 84) of published  
Suppose we wanted to create an intelligent machine that somehow drew its intelligence from large collections of text, possibly involving the processing of collections available on the Web such as Wikipedia. Does past research in deception offer a sufficiently robust basis upon which we might develop a means to filter out texts that are deceptive, either partially or entirely? Could we identify, for example, any deliberately deceptive edits to Wikipedia without consulting the edit history? In this paper, we offer a critical review of deception research. We suggest that there are a range of inconsistencies, contradictions, and other difficulties in recent deception research, and identify how we might begin to address deception research in a more systematic manner. 
The present paper addresses the question of the nature of deception language. Specifically, the main aim of this piece of research is the exploration of deceit in Spanish written communication. We have designed an automatic classifier based on Support Vector Machines (SVM) for the identification of deception in an ad hoc opinion corpus. In order to test the effectiveness of the LIWC2001 categories in Spanish, we have drawn a comparison with a Bag-of-Words (BoW) model. The results indicate that the classification of the texts is more successful by means of our initial set of variables than with the latter system. These findings are potentially applicable to areas such as forensic linguistics and opinion mining, where extensive research on languages other than English is needed. 
In this study, we explore several popular techniques for obtaining corpora for deception research. Through a survey of traditional as well as non-gold standard creation approaches, we identify advantages and limitations of these techniques for webbased deception detection and offer crowdsourcing as a novel avenue toward achieving a gold standard corpus. Through an indepth case study of online hotel reviews, we demonstrate the implementation of this crowdsourcing technique and illustrate its applicability to a broad array of online reviews. 
Research in high stakes deception has been held back by the sparsity of ground truth verification for data collected from real world sources. We describe a set of guidelines for acquiring and developing corpora that will enable researchers to build and test models of deceptive narrative while avoiding the problem of sanctioned lying that is typically required in a controlled experiment. Our proposals are drawn from our experience in obtaining data from court cases and other testimony, and uncovering the background information that enabled us to annotate claims made in the narratives as true or false. 
Recent studies on deceptive language suggest that machine learning algorithms can be employed with good results for classiﬁcation of texts as truthful or untruthful. However, the models presented so far do not attempt to take advantage of the differences between subjects. In this paper, models have been trained in order to classify statements issued in Court as false or not-false, not only taking into consideration the whole corpus, but also by identifying more homogenous subsets of producers of deceptive language. The results suggest that the models are effective in recognizing false statements, and their performance can be improved if subsets of homogeneous data are provided. 
Contextual differences present significant challenges when developing computational methods for detecting deception. We conducted a field experiment with border guards from the European Union in order to demonstrate that deception detection can be done robustly using context specific computational models. In the study, some of the participants were given a “fraudulent” document with incorrect data and asked to pass through a checkpoint. An automated system used an embodied conversational agent (ECA) to conduct interviews. Based on the participants’ vocalic and ocular behavior our specific model classified 100% of the imposters while limiting false positive errors. The overall accuracy was 94.47%. 
A person’s expressive behavior is different in situations where he or she is alone, or where an additional person is present. This study looks at the extent to which such physical co-presence effects have an impact on a child’s ability to deceive. Using an experimental digitized puppet show, truthful and deceptive utterances were elicited from children who were interacting with two story characters. The children were sitting alone, or as a couple together with another child. A ﬁrst perception study in which minimal pairs of truthful and deceptive utterances were shown (vision-only) to adult observers revealed that the correct detection of deceptive utterances is dependent on whether the stimuli were produced by a child alone or together with another child (both being visible). A second perception study presented participants with videos from children of the couples condition that were edited so that only one child was visible. The study revealed that the deceptive utterances could more often be detected correctly in the more talkative children than in the more passive ones. 
Research on deception detection has been mainly focused on two kinds of approaches. In one, people consider deception types and taxonomies, and use different counter strategies to detect and reverse deception. In the other, people search for verbal and non-verbal cues in the content of deceptive communication. However, general theories that study fundamental properties of deception which can be applied in computational models are still very rare. In this work, we propose a general model of deception detection guided by a fundamental principle in the formation of communicative deception. Experimental results using our model demonstrate that deception is distinguishable from unintentional misinformation. Introduction Conventional research on deception detection focuses on deception taxonomies and deception cues. Unfortunately, both of them neglect the fact that deception is rooted in the formation of arguments mainly because such formation is not directly observable. However, since the formation of arguments is where the implementation of deception starts, it is necessary to study it in depth. The act of deceiving involves two processes: the formation of deceptive arguments (the reasoning) and the communication of deception. The communication part is intuitive to understand and has been the focus of recent  research efforts in deception detection. The reasoning part is a necessary component of deception because deceiving has been found to require a heavier cognitive load than telling the truth (Greene et. Al, 1985). The reasoning process involves generating and selecting arguments while the communication process involves wording and phrasing of the arguments. Deception detection in the process of communication is not ideal because firstly, it is easy to hide deceptive cues using careful wording and phrasing, and secondly, wording and phrasing of communication are mediated by the framing of the other party’s response (e.g. the answer to the question “Did you go to class today?” always starts with “Yes, I” or “No, I”). On the other hand, it is hard to hide the intent of deception by distorting arguments formed in the reasoning process because it requires higherorder deception that takes the other party’s intent and even the other party’s belief about the speaker’s intent into consideration. Higher-order deception demands much more cognitive load than first-order deception in order to retrieve the memory about the other party’s intent and leverage the original reasoning process behind it. Thus, the reasoning process provides more effective and reliable observations than the communication process. Moreover, it also guides and explains some observations in the communication process such as compellingness and level of detail of a story. We will illustrate the formation of deceptive arguments in the next section, according to which, we propose three hypotheses of the fundamental differences between deception and non-deception. In Section 3, we describe our model of detection and the data simulation process. Experiment setting and results are  63 Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 63–71, Avignon, France, April 23 - 27 2012. c 2012 Association for Computational Linguistics  discusses in Section 4, followed by conclusions and future work in Section 5. 
We applied hierarchical clustering using Rank distance, previously used in computational stylometry, on literary texts written by Mateiu Caragiale and a number of different authors who attempted to impersonate Caragiale after his death, or simply to mimic his style. Their pastiches were consistently clustered opposite to the original work, thereby conﬁrming the performance of the method and proposing an extension of the method from simple authorship attribution to the more complicated problem of pastiche detection. The novelty of our work is the use of frequency rankings of stopwords as features, showing that this idea yields good results for pastiche detection. 
 Introduction  Research syntheses suggest that verbal content cues are more diagnostic than other cues in discriminating between truth and deception. In many studies on content cues, raters are trained to rate the presence of specific content cues, an inherently subjective process. This necessitates to demonstrate inter-coder reliability first. Depending on the statistical coefficient used, establishing adequate inter-rater reliabilities for these subjective judgments often creates a problem. To address some of these problems, a new method for coding these content cues with a computer program developed for qualitative research, MaxQDA (www.maxqda.de), is proprosed. The application of the program is demonstrated using the Aberdeen Report Judgment Scales (ARJS; Sporer, 2004) with a set of 72 deceptive and true accounts of a driving examination. Data on different types of inter-coder reliabilities are presented and implications for future research with computer-assisted qualitative coding procedures as well as training of coders are outlined. Credits This research has been supported by a grant from the German Science Foundation (Deutsche Forschungsgemeinschaft (DFG): Sp262/3-2) to the present author. The author would like to thank Edda Niederstadt and Nina F. Petermann for the coding of the data, and to Jaume Masip, Valerie Hauch, and Sarah Treiber for comments on an earlier version of this manuscript.  Human judges are often only slightly better than chance at discriminating between truths and lies (Bond, & DePaulo, 2006). Likewise, a recent meta-analysis of training programs designed to teach lie detection has shown only small to medium effect sizes in improving judges' detection accuracy (e.g., Hauch, Sporer, Michael, & Meissner, 2010). This meta-analysis has also shown that training effects are larger when the content of messages are considered than when only relying on nonverbal or paraverbal cues. In a series of studies, Reinhard, Sporer, Scharmach, and Marksteiner (2011) further demonstrated that paying attention to verbal content cues improved lie detection accuracy compared to participants who relied on heuristic nonverbal cues. Therefore, particular attention should be paid to find valid content cues to detect deception (DePaulo, Lindsay, Malone, Muhlenbruck, Charlton, & Cooper, 2003; Sporer, 2004; Vrij, 2008). Most of the research to date has relied on Criteria-based Content Analysis (CBCA; Steller & Koehnken, 1989; for a review, see Vrij, 2005) or reality monitoring approaches (e.g., Sporer, 1997; for reviews, see Masip, Sporer, Garrido, & Herrero, 2005; Sporer, 2004). Usually, a small set of raters is trained more or less extensively with these content criteria to apply them to transcripts of oral accounts. Due to the subjective nature of these codings, establishing inter-coder reliability of any such coding system is a necessary prerequisite for its validity (Anson, Golding, & Gully, 1993). 1.1 The Problem of Inter-Coder Reliability 
The ability to detect deceptive statements in predatory communications can help in the identiﬁcation of sexual predators, a type of deception that is recently attracting the attention of the research community. Due to the intention of a pedophile of hiding his/her true identity (name, age, gender and location) its detection is a challenge. According to previous research, ﬁxated discourse is one of the main characteristics inherent to the language of online sexual predation. In this paper we approach this problem by computing sexrelated lexical chains spanning over the conversation. Our study shows a considerable variation in the length of sex-related lexical chains according to the nature of the corpus, which supports our belief that this could be a valuable feature in an automated pedophile detection system. 
Whistleblowers and activists need the ability to communicate without disclosing their identity, as of course do kidnappers and terrorists. Recent advances in the technology of stylometry (the study of authorial style) or “authorship attribution” have made it possible to identify the author with high reliability in a non-confrontational setting. In a confrontational setting, where the author is deliberately masking their identity (i.e. attempting to deceive), the results are much less promising. In this paper, we show that although the speciﬁc author may not be identiﬁable, the intent to deceive and to hide his identity can be. We show this by a reanalysis of the Brennan and Greenstadt (2009) deception corpus and discuss some of the implications of this surprising ﬁnding. 
The paper proposes to use Rhetorical Structure Theory (RST) analytic framework to identify systematic differences between deceptive and truthful stories in terms of their coherence and structure. A sample of 36 elicited personal stories, self-ranked as completely truthful or completely deceptive, is manually analyzed by assigning RST discourse relations among a story’s constituent parts. Vector Space Model (VSM) assesses each story’s position in multi-dimensional RST space with respect to its distance to truth and deceptive centers as measures of the story’s level of deception and truthfulness. Ten human judges evaluate if each story is deceptive or not, and assign their confidence levels, which produce measures of the human expected deception and truthfulness levels. The paper contributes to deception detection research and RST twofold: a) demonstration of discourse structure analysis in pragmatics as a prominent way of automated deception detection and, as such, an effective complement to lexico-semantic analysis, and b) development of RST-VSM methodology to interpret RST analysis in identification of previously unseen deceptive texts. Introduction Automated deception detection is a challenging task (DePaulo, Charlton, Cooper, Lindsay, and Muhlenbruck, 1997), only recently proven feasible with natural language processing and machine learning techniques (Bachenko, Fitzpatrick, and Schonwetter, 2008; Fuller, Biros, and Wilson, 2009; Hancock, Curry, Goorha, and  Woodworth, 2008; Rubin, 2010; Zhou, Burgoon, Nunamaker, and Twitchell, 2004). The idea is to distinguish truthful information from deceptive, where deception usually implies an intentional and knowing attempt on the part of the sender to create a false belief or false conclusion in the mind of the receiver of the information (e.g., Buller and Burgoon, 1996; Zhou, et al., 2004). In this paper we focus solely on textual information, in particular, in computer-mediated personal communications such as e-mails or online posts. Previously suggested techniques for detecting deception in text reach modest accuracy rates at the level of lexico-semantic analysis. Certain lexical items are considered to be predictive linguistic cues, and could be derived, for examples, from the Statement Validity Analysis techniques used in law enforcement for credibility assessments (as in Porter and Yuille, 1996). Though there is no clear consensus on reliable predictors of deception, deceptive cues are identified in texts, extracted and clustered conceptually, for instance, to represent diversity, complexity, specificity, and non-immediacy of the analyzed texts (e.g., Zhou, Burgoon, Nunamaker, and Twitchell (2004)). When implemented with standard classification algorithms (such as neural nets, decision trees, and logistic regression), such methods achieve 74% accuracy (Fuller, et al., 2009). Existing psycholinguistic lexicons (e.g., LWIC by Pennebaker and Francis, 1999) have been adapted to perform binary text classifications for truthful versus deceptive opinions, with an average classifier demonstrating 70% accuracy rate (Mihalcea and Strapparava, 2009). These modest results, though usually achieved on restricted topics, are promising since they supersede notoriously unreliable human abilities in lie-truth discrimination tasks. On average, people are not very good at spotting lies (Vrij, 2000), succeeding generally only about half of the time (Frank, Paolantinio, Feeley, and  97 Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 97–106, Avignon, France, April 23 - 27 2012. c 2012 Association for Computational Linguistics  Servoss, 2004). For instance, a meta-analytical review of over 100 experiments with over 1,000 participants, showed a 54% mean accuracy rate at identifying deception (DePaulo, et al., 1997). Human judges achieve 50 – 63% success rates, depending on what is considered deceptive on a seven-point scale of truth-to-deception continuum (Rubin and Conroy, 2011, Rubin and Conroy, 2012), but the higher the actual selfreported deception level of the story, the more likely a story would be confidently assigned as deceptive. In other words, extreme degrees of deception are more transparent to judges. The task for current automated deception detection techniques has been formulated as binary text categorization – is a message deceptive or truthful – and the decision applies to the whole analyzed text. Since it is an overall discourse level decision, it may be reasonable to consider discourse or pragmatic features of each message. Thus far, discourse is surprisingly rarely considered, if at all, and the majority of the effort has been restricted to lexico-semantic verbal predictors. A rare exception up to date has been a Bachenko, Fitzpatrick and Schonwetter’s (2008) study that focuses on truth or falsity of individual propositions, achieving a finer-grained level of analysis 1 , but the propositional interrelations within the discourse structure are not considered. To the best of our knowledge there have been no advances in that automation deception detection task to incorporate discourse structure features and/or text coherence analysis at the pragmatic levels of story interpretation. Study Objective With the recent advances in the identification of verbal cues of deception in mind, and the realization that they focus on linguistic levels below discourse and pragmatic analysis, the study focuses on one main question:  What is the impact of the relations between discourse constituent parts on the discourse composition of deceptive and truthful messages? We hypothesize that if the relations between discourse constituent parts in deceptive messages differ from the ones in truthful messages, then systematic analysis of such relations will help to 
Keystroke-logging tools are widely used in writing process research. These applications are designed to capture each character and mouse movement as isolated events as an indicator of cognitive processes. The current research project explores the possibilities of aggregating the logged process data from the letter level (keystroke) to the word level by merging them with existing lexica and using NLP tools. Linking writing process data to lexica and using NLP tools enables researchers to analyze the data on a higher, more complex level. In this project the output data of Inputlog are segmented on the sentence level and then tokenized. However, by definition writing process data do not always represent clean and grammatical text. Coping with this problem was one of the  main challenges in the current project. Therefore, a parser has been developed that extracts three types of data from the S-notation: word-level revisions, deleted fragments, and the final writing product. The within-word typing errors are identified and excluded from further analyses. At this stage the Inputlog process data are enriched with the following linguistic information: part-ofspeech tags, lemmas, chunks, syllable boundaries and word frequencies. 
This paper reports on the development of methods for the automated detection of violations of style guidelines for legislative texts, and their implementation in a prototypical tool. To this aim, the approach of error modelling employed in automated style checkers for technical writing is enhanced to meet the requirements of legislative editing. The paper identiﬁes and discusses the two main sets of challenges that have to be tackled in this process: (i) the provision of domain-speciﬁc NLP methods for legislative drafts, and (ii) the concretisation of guidelines for legislative drafting so that they can be assessed by machine. The project focuses on German-language legislative drafting in Switzerland. 
 This essay provides a summary of research related to My Reviewers, a web-based application that can be used for teaching and assessment purposes. The essay concludes with speculation about ongoing development efforts, including a social helpfulness algorithm, a badging system, and Natural Language Processing (NLP) features. 
In this research we explore the possibility of using a large n-gram corpus (Google Books) to derive lexical transition probabilities from the frequency of word n-grams and then use them to check and suggest corrections in a target text without the need for grammar rules. We conduct several experiments in Spanish, although our conclusions also reach other languages since the procedure is corpus-driven. The paper reports on experiments involving different types of grammar errors, which are conducted to test different grammar-checking procedures, namely, spotting possible errors, deciding between different lexical possibilities and ﬁlling-in the blanks in a text. 
This short paper relates the main features of LELIE, phase 1, which detects errors made by technical writers when producing procedures or requirements. This results from ergonomic observations of technical writers in various companies. 
This paper focuses on computer writing tools used during the production of documents in a professional setting. Computer writing tools include language technologies, for example electronic dictionaries and text correction software, as well as information and communication technologies, for example collaborative platforms and search engines. As we will see, professional writing has become an entirely computerised activity. First, we report on a focus group with professional writers, during which they discussed their experience using computer tools to write documents. We will describe their practices, point out the most important problems they encounter, and analyse their needs. Second, we describe LinguisTech, a reference web site for language professionals (translators, writers, language instructors, etc.) that was launched in Canada in September, 2011. We comment on a preliminary evaluation that we conducted to determine if this new platform meets professional writers’ needs. 
In statistical NLP, Semantic Vector Spaces (SVS) are the standard technique for the automatic modeling of lexical semantics. However, it is largely unclear how these black-box techniques exactly capture word meaning. To explore the way an SVS structures the individual occurrences of words, we use a non-parametric MDS solution of a token-by-token similarity matrix. The MDS solution is visualized in an interactive plot with the Google Chart Tools. As a case study, we look at the occurrences of 476 Dutch nouns grouped in 214 synsets. 
Each expanding and developing system requires some feedback to evaluate the normal trends of the system and also the unsystematic steps. In this paper two lexicalsemantic databases – Princeton WordNet (PrWN) and Estonian Wordnet (EstWN)- are being examined from the visualization point of view. The visualization method is described and the aim is to find and to point to possible problems of synsets and their semantic relations. 
This paper presents a novel way of visualising relationships between languages. The key feature of the visualisation is that it brings geographic, phylogenetic, and linguistic data together into a single image, allowing a new visual perspective on linguistic typology. The data presented here is extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2011). After pruning due to low coverage of WALS, we ﬁlter the typological data by geographical proximity in order to ascertain areal typological effects. The data are displayed in heat maps which reﬂect the strength of similarity between languages for different linguistic features. Finally, the heat maps are annotated for language family membership. The images so produced allow a multi-faceted perspective on the data which we hope will facilitate the interpretation of results and perhaps illuminate new areas of research in linguistic typology. 
We demonstrate how data-driven approaches to learner corpora can support Second Language Acquisition research when integrated with visualisation tools. We present a visual user interface supporting the investigation of a set of linguistic features discriminating between pass and fail ‘English as a Second or Other Language’ exam scripts. The system displays directed graphs to model interactions between features and supports exploratory search over a set of learner scripts. We illustrate how the interface can support the investigation of the co-occurrence of many individual features, and discuss how such investigations can shed light on understanding the linguistic abilities that characterise different levels of attainment and, more generally, developmental aspects of learner grammars. 
The present paper describes procedures to visualise diachronic language changes in academic discourse to support analysis. These changes are reﬂected in the distribution of different lexico-grammatical features according to register. Findings about register differences are relevant for both linguistic applications (e.g., discourse analysis and translation studies) and NLP tasks (notably automatic text classiﬁcation). 
Words are important both in historical linguistics and natural language processing. They are not indivisible abstract atoms; much can be gained by considering smaller units such as morphemes, phonemes, syllables, and letters. In this presentation, I attempt to sketch the similarity patterns among a number of diverse research projects in which I participated. 
In this paper, we propose a novel approach to compare languages on the basis of parallel texts. Instead of using word lists or abstract grammatical characteristics to infer (phylogenetic) relationships, we use multilingual alignments of words in sentences to establish measures of language similarity. To this end, we introduce a new method to quickly infer a multilingual alignment of words, using the co-occurrence of words in a massively parallel text (MPT) to simultaneously align a large number of languages. The idea is that a simultaneous multilingual alignment yields a more adequate clustering of words across different languages than the successive analysis of bilingual alignments. Since the method is computationally demanding for a larger number of languages, we reformulate the problem using sparse matrix calculations. The usefulness of the approach is tested on an MPT that has been extracted from pamphlets of the Jehova’s Witnesses. Our preliminary experiments show that this approach can supplement both the historical and the typological comparison of languages. 
This paper proposes a simple metric of dialect distance, based on the ratio between identical word pairs and cognate word pairs occurring in two texts. Different variations of this metric are tested on a corpus containing comparable texts from different Swiss German dialects and evaluated on the basis of spatial autocorrelation measures. The visualization of the results as cluster dendrograms shows that closely related dialects are reliably clustered together, while multidimensional scaling produces graphs that show high agreement with the geographic localization of the original texts. 
A SHIBBOLETH is a pronunciation, or, more generally, a variant of speech that betrays where a speaker is from (Judges 12:6). We propose a generalization of the well-known precision and recall scores to deal with the case of detecting distinctive, characteristic variants when the analysis is based on numerical difference scores. We also compare our proposal to Fisher’s linear discriminant, and we demonstrate its effectiveness on Dutch and German dialect data. It is a general method that can be applied both in synchronic and diachronic linguistics that involve automatic classiﬁcation of linguistic entities. 
The paper reports several studies about quantifying language similarity via phonetic alignment of core vocabulary items (taken from Wichman’s Automated Similarity Judgement Program data base). It turns out that weighted alignment according to the Needleman-Wunsch algorithm yields best results. For visualization and data exploration purposes, we used an implementation of the Fruchterman-Reingold algorithm, a version of force directed graph layout. This software projects large amounts of data points to a two- or three-dimensional structure in such a way that groups of mutually similar items form spatial clusters. The exploratory studies conducted along these ways lead to suggestive results that provide evidence for historical relationships beyond the traditionally recognized language families. 
The principal goal of this paper is to illustrate various ways in which phylogenetic tools can advantageously be put to use in investigating and visualizing the relationships of creole languages to other languages, both creoles and non-creoles. After introducing a test study on the English-based creoles, the major theories seeking to explain the emergence and development of creoles will be reviewed and assessed. The final part of the paper is concerned with the typological status of creoles, where various samples will be used to show that creoles form a typologically coherent group among the world's languages. 1. Introduction Although the linguistic processes underlying creolization remain far from being fully understood, computational methods offer today the opportunity of uncovering complex mechanisms of language evolution on the basis of quantitative investigations. More sophisticated and powerful algorithms now available enable the visualization of patterns in a straightforward manner that was not possible before. Creole languages emerged in situations of intense contact between several languages, more often than not in the context of massive forced population displacements as were typical of European slave-trading ventures. A diglossic situation with a high-prestige variety (the superstrate) and several low-prestige languages (the substrates) characterized the settings in which creoles developed. Therefore, creole languages can be said to have several parents, and possess as well many often recurring features that appear ex nihilo. Thus, the problem of determining relationships between creole languages and other creoles or unrelated languages has long haunted creolists and been recognized as one of the challenges in the field.  Following recent developments in creolistics, where phylogenetic networks were used to investigate questions inherent to the field (Bakker et al. 2011, Daval-Markussen 2011, Daval-Markussen and Bakker 2011), the aim of this paper is to argue that creole languages offer an unparalleled venue for exploratory research in language evolution, and that available computational tools now permit to graphically represent the relationships between the languages considered. In our demonstration, we will exemplify various ways in which phylogenetic networks may advantageously be used to visualize the results. Following the argumentation in DavalMarkussen (2011: 6-13), only structural features will be taken into account in the present study, since the lexical stock of a creole is mainly derived from a single source language, and this would likely be reflected in the resulting graphs. In the first part of the paper, phylogenetic tools are used to represent the relationships between 33 English-based creoles, for which 62 typological features were selected and encoded binarily1. The second part examines the various scenarios proposed to account for the emergence of creole languages in the light of phylogenetic networks. To this end, samples of various sizes and including creoles as well as non-creole languages (mostly languages involved in the emergence of creoles) were used in order to visualize the impact of the various languages present in the contact situation on the new vernaculars. The final part deals with the typological status of creoles, a topic hotly debated in creolistics (e.g. DeGraff 2003; McWhorter 1998, 2011). Basing our analysis on samples of languages selected from the World Atlas of Linguistic Structures (Dryer and 
The first AustKin project (AustKin I) collected a large database of kinship terms from Aboriginal languages all over Australia, endeavouring to maintain standards of spelling, kin formulae and group identities, without losing the details of original sources used. An online geospatial interface has been used to map distributions of forms of terms and their polysemies or equations. The patterns of the latter provide identification of kinship systems as defined in ethnology. The project proposed and tested hypotheses about the evolution of such systems in Australia based on knowledge of the common polysemies and related changes. The next stage, AustKin II, builds on hypotheses from the current authors and others, testing these further by adding two more components to the database: the marriage rules and the social categories used by each group. Of the latter, section and subsection systems are unique to Australia. The aim is to gauge how these different systems fit together and propose how they evolved over time and how they influenced each other. 1.The AustKin project 1.1The design of the AustKin database.  of over 22 000 words that belong to the domain of kinship. Designing a database and an interface to such a database has revealed itself to be a complex matter since the number and diversity of variables that need to be taken into account are considerable. In summary, the following had to be taken into account: A – Systemic variables 1) Kinship terminologies are not just words, but also relationships; and in particular they are related among each other. 2) A kinship terminology constitutes a system; but not all kinship terminologies belong to the same type of system. 3) Kinship terminologies change and they need to be placed against their chronological and historical background. B – Sporadic variables 1) Kinship terminologies are recorded by humans, and often by non-linguists; they include errors. 2) Kinship terminologies are seldom complete, and need to be completed when possible through other sources. 3) Original Informants may not always have been local speakers. 
This paper presents a novel method for aligning etymological data, which models context-sensitive rules governing sound change, and utilizes phonetic features of the sounds. The goal is, for a given corpus of cognate sets, to ﬁnd the best alignment at the sound level. We introduce an imputation procedure to compare the goodness of the resulting models, as well as the goodness of the data sets. We present evaluations to demonstrate that the new model yields improvements in performance, compared to previously reported models. 
In this paper, a new method for automatic cognate detection in multilingual wordlists will be presented. The main idea behind the method is to combine different approaches to sequence comparison in historical linguistics and evolutionary biology into a new framework which closely models the most important aspects of the comparative method. The method is implemented as a Python program and provides a convenient tool which is publicly available, easily applicable, and open for further testing and improvement. Testing the method on a large gold standard of IPAencoded wordlists showed that its results are highly consistent and outperform previous methods. 
This paper describes the experimental combination of traditional Natural Language Processing (NLP) technology with the Semantic Web building stack in order to extend the expert knowledge required for a Machine Translation (MT) task. Therefore, we ﬁrst give a short introduction in the state of the art of MT and the Semantic Web and discuss the problem of disambiguation being one of the common challenges in MT which can only be solved using world knowledge during the disambiguation process. In the following, we construct a sample sentence which demonstrates the need for world knowledge and design a prototypical program as a successful solution for the outlined translation problem. We conclude with a critical view on the developed approach. 
In this paper we present and evaluate three approaches to measure comparability of documents in non-parallel corpora. We develop a task-oriented deﬁnition of comparability, based on the performance of automatic extraction of translation equivalents from the documents aligned by the proposed metrics, which formalises intuitive deﬁnitions of comparability for machine translation research. We demonstrate application of our metrics for the task of automatic extraction of parallel and semiparallel translation equivalents and discuss how these resources can be used in the frameworks of statistical and rule-based machine translation. 
In this paper we present an SMT-based approach to Question Answering (QA). QA is the task of extracting exact answers in response to natural language questions. In our approach, the answer is a translation of the question obtained with an SMT system. We use the n-best translations of a given question to ﬁnd similar sentences in the document collection that contain the real answer. Although it is not the ﬁrst time that SMT inspires a QA system, it is the ﬁrst approach that uses a full Machine Translation system for generating answers. Our approach is validated with the datasets of the TREC QA evaluation. 
In this paper we evaluate the possibility of improving the performance of a statistical machine translation system by relaxing the complexity of the translation task by removing the most frequent and predictable terms from the target language vocabulary. Afterwards, the removed terms are inserted back in the relaxed output by using an n-gram based word predictor. Empirically, we have found that when these words are omitted from the text, the perplexity of the text decreases, which may imply the reduction of confusion in the text. We conducted some machine translation experiments to see if this perplexity reduction produced a better translation output. While the word prediction results exhibits 77% accuracy in predicting 40% of the most frequent words in the text, the perplexity reduction did not help to produce better translations. 
As video contents continue to expand, it is increasingly important to properly annotate videos for effective search, mining and retrieval purposes. While the idea of annotating images with keywords is relatively well explored, work is still needed for annotating videos with natural language to improve the quality of video search. The focus of this work is to present a video dataset with natural language descriptions which is a step ahead of keywords based tagging. We describe our initial experiences with a corpus consisting of descriptions for video segments crafted from TREC video data. Analysis of the descriptions created by 13 annotators presents insights into humans’ interests and thoughts on videos. Such resource can also be used to evaluate automatic natural language generation systems for video. 
In this paper we present a hybrid statistical machine translation (SMT)-example-based MT (EBMT) system that shows signiﬁcant improvement over both SMT and EBMT baseline systems. First we present a runtime EBMT system using a subsentential translation memory (TM). The EBMT system is further combined with an SMT system for effective hybridization of the pair of systems. The hybrid system shows signiﬁcant improvement in translation quality (0.82 and 2.75 absolute BLEU points) for two different language pairs (English–Turkish (En–Tr) and English– French (En–Fr)) over the baseline SMT system. However, the EBMT approach suffers from signiﬁcant time complexity issues for a runtime approach. We explore two methods to make the system scalable at runtime. First, we use an heuristic-based approach. Secondly, we use an IR-based indexing technique to speed up the time-consuming matching procedure of the EBMT system. The index-based matching procedure substantially improves run-time speed without affecting translation quality. 
In this paper we present two approaches for integrating translation into cross-lingual search engines: the first approach relies on term translation via a language ontology, the other one is based on machine translation of specific information. 
This document contains a brief presentation of the PRESEMT project that aims in the development of a novel language-independent methodology for the creation of a flexible and adaptable MT system. 1. Introduction 
The main purpose of the project ATLAS (Applied Technology for Language-Aided CMS) is to facilitate multilingual web content development and management. Its main innovation is the integration of language technologies within a web content management system. The language processing framework, integrated with web content management, provides automatic annotation of important words, phrases and named entities, suggestions for categorisation of documents, automatic summary generation, and machine translation of summaries of documents. A machine translation approach, as well as methods for obtaining and constructing training data for machine translation are under development. 
I present an automatic post-editing approach that combines translation systems which produce syntactic trees as output. The nodes in the generation tree and targetside SCFG tree are aligned and form the basis for computing structural similarity. Structural similarity computation aligns subtrees and based on this alignment, subtrees are substituted to create more accurate translations. Two different techniques have been implemented to compute structural similarity: leaves and tree-edit distance. I report on the translation quality of a machine translation (MT) system where both techniques are implemented. The approach shows signiﬁcant improvement over the baseline for MT systems with limited training data and structural improvement for MT systems trained on Europarl. 
We report on a series of experiments aimed at improving the machine translation of ambiguous lexical items by using wordnet-based unsupervised Word Sense Disambiguation (WSD) and comparing its results to three MT systems. Our experiments are performed for the English-Slovene language pair using UKB, a freely available graph-based word sense disambiguation system. Results are evaluated in three ways: a manual evaluation of WSD performance from MT perspective, an analysis of agreement between the WSDproposed equivalent and those suggested by the three systems, and finally by computing BLEU, NIST and METEOR scores for all translation versions. Our results show that WSD performs with a MT-relevant precision of 71% and that 21% of sense-related MT errors could be prevented by using unsupervised WSD. 
The processing of parallel corpus plays very crucial role for improving the overall performance in Phrase Based Statistical Machine Translation systems (PBSMT). In this paper the automatic alignments of different kind of chunks have been studied that boosts up the word alignment as well as the machine translation quality. Single-tokenization of Noun-noun MWEs, phrasal preposition (source side only) and reduplicated phrases (target side only) and the alignment of named entities and complex predicates provide the best SMT model for bootstrapping. Automatic bootstrapping on the alignment of various chunks makes significant gains over the previous best English-Bengali PB-SMT system. The source chunks are translated into the target language using the PB-SMT system and the translated chunks are compared with the original target chunk. The aligned chunks increase the size of the parallel corpus. The processes are run in a bootstrapping manner until all the source chunks have been aligned with the target chunks or no new chunk alignment is identified by the bootstrapping process. The proposed system achieves significant improvements (2.25 BLEU over the best System and 8.63 BLEU points absolute over the baseline system, 98.74% relative improvement over the baseline system) on an English- Bengali translation task.  
This paper gives an overview of the ongoing FP7 project HyghTra (2010 – 2014). The HyghTra project is conducted in a partnership between academia and industry involving the University of Leeds and Lingenio GmbH (company). It adopts a hybrid and bootstrapping approach to the enhancement of MT quality by applying rule-based analysis and statistical evaluation techniques to both parallel and comparable corpora in order to extract linguistic information and enrich the lexical and syntactic resources of the underlying (rule-based) MT system that is used for analysing the corpora. The project places special emphasis on the extension of systems to new language pairs and corresponding rapid, automated creation of high quality resources. The techniques are fielded and evaluated within an existing commercial MT environment. 
We describe a substitution-based, hybrid machine translation (MT) system that has been extended with a machine learning component controlling its phrase selection. Our approach is based on a rule-based MT (RBMT) system which creates template translations. Based on the generation parse tree of the RBMT system and standard word alignment computation, we identify potential “translation snippets” from one or more translation engines which could be substituted into our translation templates. The substitution process is controlled by a binary classiﬁer trained on feature vectors from the different MT engines. Using a set of manually annotated training data, we are able to observe improvements in terms of BLEU scores over a baseline version of the hybrid system. 
In this paper, we present our linguisticallyaugmented statistical machine translation model from Bulgarian to English, which combines a statistical machine translation (SMT) system (as backbone) with deep linguistic features (as factors). The motivation is to take advantages of the robustness of the SMT system and the linguistic knowledge of morphological analysis and the hand-crafted grammar through system combination approach. The preliminary evaluation has shown very promising results in terms of BLEU scores (38.85) and the manual analysis also conﬁrms the high quality of the translation the system delivers.  the contrary, are in general more robust, but sometimes output ungrammatical sentences. In fact, instead of competing with each other, there is also a line of research trying to combine the advantages of the two sides using a hybrid framework. Although many systems can be put under the umbrella of “hybrid” systems, there are various ways to do the combination/integration. Thurmair (2009) summarized several different architectures of hybrid systems using SMT and RBMT systems. Some widely used ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses.  
This article shows how the automatic disambiguation of discourse connectives can improve Statistical Machine Translation (SMT) from English to French. Connectives are ﬁrstly disambiguated in terms of the discourse relation they signal between segments. Several classiﬁers trained using syntactic and semantic features reach stateof-the-art performance, with F1 scores of 0.6 to 0.8 over thirteen ambiguous English connectives. Labeled connectives are then used into SMT systems either by modifying their phrase table, or by training them on labeled corpora. The best modiﬁed SMT systems improve the translation of connectives without degrading BLEU scores. A threshold-based SMT system using only high-conﬁdence labels improves BLEU scores by 0.2–0.4 points. 
The Origins of New Zealand English Corpora (ONZE) at the University of Canterbury contain recordings spanning 150 years of New Zealand English. These have all been force-aligned at the phonemelevel, and are stored with many layers of annotation some which have been automatically generated, and some which have been manually annotated. We interact with the corpus via our custom LaBB-CAT interface (LAnguage, Brain and Behaviour Corpus Analysis Tool). I will begin the talk by describing and demonstrating the corpus, and its associated LaBB-CAT tool. I will then focus on one particular recent study which has used the corpus, which aims to understand processes of sound change. The combination of the time-depth of the ONZE collection, and the degree of careful annotation it contains, makes it an ideal data-set for the study of mechanisms underlying sound change. In particular, we aim to address the question which has been the subject of long-standing debate in the sound-change literature do sound changes proceed uniformly through the lexicon, or are there word-speciﬁc changes, with some words more ahead in the change than others? I describe a study which aimed to investigate this question by focusing on the mechanisms underpinning the New Zealand English front short vowel shift, of the vowels in words like bat, bet and bit. We automatically extracted formant values for over 100,000 tokens of words containing these vowels, We show that this data contains good evidence for word-speciﬁc effects in sound change, and argue that these are predicted by current models of speech production and perception, in combination with well-established psycholinguistic processes. 
Can two different descriptions refer to the same event or action? Recognising that dissimilar strings are equivalent in meaning for some purpose is something that humans do rather well, but it is a task at which machines often fail. In the Natural Language Processing Group at Microsoft Research, we are attempting to address this challenge at sentence scale by generating semantically equivalent rewrites that can be used in applications ranging from authoring assistance to intent mapping for search or command and control. The Microsoft Translator paraphrase engine, developed in the NLP group, is a large-scale phrasal machine translation system that generates short sentential and phrasal paraphrases in English and has a public API that is available to researchers and developers. I will present the data extraction process, architecture, issues in generating diverse outputs, applications and possible future directions, and discuss the strengths and limitations of the statistical machine translation model as it relates to paraphrasing, how paraphrase is like machine translation, and how it differs in important respects. The statistical machine translation approach also has broad applications in capturing user intent in search, conversational understanding, and the grounding of language in objects and actions, all active areas of investigation in Microsoft Research. Chris Brockett. 2012. Diverse Words, Shared Meanings: Statistical Machine Translation for Paraphrase, Grounding, and Intent. In Proceedings of Australasian Language Technology Association Workshop, pages 3−3. 
Stephen Cranefield  Nigel Stanger  Department of Information Science, University of Otago, Dunedin, New Zealand  (angrosh, scranefield, nstanger}@infoscience.otago.ac.nz  Abstract This paper presents an annotation scheme for modelling citation contexts in scientific articles. We present an argumentation framework based on the Toulmin model for scientific articles and develop an annotation scheme with different context types based on the argumentation model. We present the results of the inter-rater reliability study carried out for studying the reliability of our annotation scheme. 1 Introduction Citations play an important role in scientific writing. However, there are not many tools that provide citation context based information services. The citation services provided by academic search engines such as Google Scholar1 includes information about the number of citing documents and links to citing articles. Search can also be performed for keywords in citing articles. Citation focused tools such as CiteSeerX 2 and Microsoft Academic Search 3 engines go a little further in identifying the passage of citations in citing articles. The objective of such services is to facilitate quick access to citation content to aid the learning process. However, the high volume of research content renders it difficult to achieve optimum use of these services. Identifying this need, we proposed to develop tools for providing intelligent citation context based information services. However, an annotation scheme for citation contexts is one of the key requirements of citation context based information tools. An annotation scheme providing citation contexts can help in classifying citation passages and provide better citation context based information services. Accordingly, we studied the existing annotation 
This paper outlines a novel approach for modelling semantic relationships within medical documents. Medical terminologies contain a rich source of semantic information critical to a number of techniques in medical informatics, including medical information retrieval. Recent research suggests that corpus-driven approaches are effective at automatically capturing semantic similarities between medical concepts, thus making them an attractive option for accessing semantic information. Most previous corpus-driven methods only considered syntagmatic associations. In this paper, we adapt a recent approach that explicitly models both syntagmatic and paradigmatic associations. We show that the implicit similarity between certain medical concepts can only be modelled using paradigmatic associations. In addition, the inclusion of both types of associations overcomes the sensitivity to the training corpus experienced by previous approaches, making our method both more effective and more robust. This ﬁnding may have implications for researchers in the area of medical information retrieval. 
We report on our ongoing work in developing the Irish Dependency Treebank, describe the results of two Inter-annotator Agreement (IAA) studies, demonstrate improvements in annotation consistency which have a knock-on effect on parsing accuracy, and present the ﬁnal set of dependency labels. We then go on to investigate the extent to which active learning can play a role in treebank and parser development by comparing an active learning bootstrapping approach to a passive approach in which sentences are chosen at random for manual revision. We show that active learning outperforms passive learning, but when annotation effort is taken into account, it is not clear how much of an advantage the active learning approach has. Finally, we present results which suggest that adding automatic parses to the training data along with manually revised parses in an active learning setup does not greatly affect parsing accuracy. 
We present a method to estimate word use similarity independent of an external sense inventory. This method utilizes a topicmodelling approach to compute the similarity in usage of a single word across a pair of sentences, and we evaluate our method in terms of its ability to reproduce a humanannotated ranking over sentence pairs. We ﬁnd that our method outperforms a bag-ofwords baseline, and that for certain words there is very strong correlation between our method and human annotators. We also ﬁnd that lemma-speciﬁc models do not outperform general topic models, despite the fact that results with the general model vary substantially by lemma. We provide a detailed analysis of the result, and identify open issues for future research. 
This paper looks at the problem of valence shifting, rewriting a text to preserve much of its meaning but alter its sentiment characteristics. There has only been a small amount of previous work on the task, which appears to be more difﬁcult than researchers anticipated, not least in agreement between human judges regarding whether a text had indeed had its valence shifted in the intended direction. We therefore take a simpler version of the task, and show that sentiment-based lexical paraphrases do consistently change the sentiment for readers. We then also show that the Kullback-Leibler divergence makes a useful preliminary measure of valence that corresponds to human judgements. 
We describe a probabilistic approach that combines information obtained from a lexicon with information obtained from a Na¨ıve Bayes (NB) classiﬁer for multi-way sentiment analysis. Our approach also employs grammatical structures to perform adjustments for negations, modiﬁers and sentence connectives. The performance of this method is compared with that of an NB classiﬁer with feature selection, and MCST – a state-of-the-art system. The results of our evaluation show that the performance of our hybrid approach is at least as good as that of these systems. We also examine the inﬂuence of three factors on performance: (1) sentiment-ambiguous sentences, (2) probability of the most probable star rating, and (3) coverage of the lexicon and the NB classiﬁer. Our results indicate that the consideration of these factors supports the identiﬁcation of regions of improved reliability for sentiment analysis. 
The Japanese language has absorbed large numbers of loanwords from many languages, in particular English. As well as using single loanwords, compound nouns, multiword expressions (MWEs), etc. constructed from loanwords can be found in use in very large quantities. In this paper we describe a system which has been developed to segment Japanese loanword MWEs and construct likely English translations. The system, which leverages the availability of large bilingual dictionaries of loanwords and English n-gram corpora, achieves high levels of accuracy in discriminating between single loanwords and MWEs, and in segmenting MWEs. It also generates useful translations of MWEs, and has the potential to being a major aid to lexicographers in this area. 
Machine translation (MT) systems can only be improved if their performance can be reliably measured and compared. However, measurement of the quality of MT output is not straightforward, and, as we discuss in this paper, relies on correlation with inconsistent human judgments. Even when the question is captured via “is translation A better than translation B” pairwise comparisons, empirical evidence shows that inter-annotator consistency in such experiments is not particularly high; for intra-judge consistency – computed by showing the same judge the same pair of candidate translations twice – only low levels of agreement are achieved. In this paper we review current and past methodologies for human evaluation of translation quality, and explore the ramiﬁcations of current practices for automatic MT evaluation. Our goal is to document how the methodologies used for collecting human judgments of machine translation quality have evolved; as a result, we raise key questions in connection with the low levels of judgment agreement and the lack of mechanisms for longitudinal evaluation. 
We perform a quantitative analysis of data in a corpus that specialises on summarisation for Evidence Based Medicine (EBM). The intent of the analysis is to discover possible directions for performing automatic evidence-based summarisation. Our analysis attempts to ascertain the extent to which good, evidence-based, multidocument summaries can be obtained from individual single-document summaries of the source texts. We deﬁne a set of scores, which we call coverage scores, to estimate the degree of information overlap between the multi-document summaries and source texts of various granularities. Based on our analysis, using several variants of the coverage scores, and the results of a simple task oriented evaluation, we argue that approaches for the automatic generation of evidence-based, bottom-line, multi-document summaries may beneﬁt by utilising a two-step approach: in the ﬁrst step, content-rich, singledocument, query-focused summaries are generated; followed by a step to synthesise the information from the individual summaries. 
We investigated methods for the discovery of clich´es from song lyrics. Trigrams and rhyme features were extracted from a collection of lyrics and ranked using term-weighting techniques such as tf-idf. These attributes were also examined over both time and genre. We present an application to produce a clich´e score for lyrics based on these ﬁndings and show that number one hits are substantially more clich´ed than the average published song. 
We describe the in-class evaluation of two versions of a tutorial dialogue system with 338 volunteers from a ﬁrst-year undergraduate health-sciences class. One version uses supervised machine-learning techniques to classify student free-text responses; the other requires students to select their preferred response from a series of options (menu-based). Our results indicate that both the free-text and menu-based tutors produced signiﬁcant gains on immediate post-test scores compared to a control group. In addition, there was no signiﬁcant difference in performance between students in the free-text and menu-based conditions. We note speciﬁc analysis work still to do as part of this research and speculate brieﬂy on the potential for using tutorial dialogue systems in real class settings. 
Large corpora are crucial resources for building many statistical language technology systems, and the Web is a readilyavailable source of vast amounts of linguistic data from which to construct such corpora. Nevertheless, little research has considered how to best build corpora from the Web. In this study we consider the importance of language identiﬁcation in Web corpus construction. Beginning with a Web crawl consisting of documents identiﬁed as English using a standard language identiﬁcation tool, we build corpora of varying sizes both with, and without, further ﬁltering of non-English documents with a stateof-the-art language identiﬁer. We show that the perplexity of a standard English corpus is lower under a language model trained from a Web corpus built with this extra language identiﬁcation step, demonstrating the importance of state-of-the-art language identiﬁcation in Web corpus construction. 
“ONZE Miner”, an open-source tool for storing and automatically annotating Transcriber transcripts, has been redeveloped to use “annotation graphs” as its data model. The annotation graph framework provides the new software, “LaBB-CAT”, greater flexibility for automatic and manual annotation of corpus data at various independent levels of granularity, and allows more sophisticated annotation structures, opening up new possibilities for corpus mining and conversion between tool formats. 
One of the potentially most relevant pieces of metadata for ﬁltering studies in environmental science is the geographic region in which the study took place (the “study region”). In this paper, we apply support vector machines to the automatic classiﬁcation of study region in a dataset of titles and abstracts from environmental science literature, using features including frequency distributions of resolved toponyms and a bag of word unigrams. We found that we can determine the study region with high accuracy, with the strongest classiﬁer achieving an accuracy of 0.892 combining toponym resolution from DBpedia and GeoNames with the bag-of-toponyms features. 
Linking implicit semantic roles is a challenging problem in discourse processing. Unlike prior work inspired by SRL, we cast this problem as an anaphora resolution task and embed it in an entity-based coreference resolution (CR) architecture. Our experiments clearly show that CR-oriented features yield strongest performance exceeding a strong baseline. We address the problem of data sparsity by applying heuristic labeling techniques, guided by the anaphoric nature of the phenomenon. We achieve performance beyond state-of-the art. 
We present a novel adaptive clustering model for coreference resolution in which the expert rules of a state of the art deterministic system are used as features over pairs of clusters. A signiﬁcant advantage of the new approach is that the expert rules can be easily augmented with new semantic features. We demonstrate this advantage by incorporating semantic compatibility features for neutral pronouns computed from web n-gram statistics. Experimental results show that the combination of the new features with the expert rules in the adaptive clustering approach results in an overall performance improvement, and over 5% improvement in F1 measure for the target pronouns when evaluated on the ACE 2004 newswire corpus. 
This paper explores the hypothesis that semantic relatedness may be more reliably inferred by using a multilingual space, as compared to the typical monolingual representation. Through evaluations using several stateof-the-art semantic relatedness systems, applied on standard datasets, we show that a multilingual approach is better suited for this task, and leads to improvements of up to 47% with respect to the monolingual baseline. 
Wikipedia is a Web based, freely available multilingual encyclopedia, constructed in a collaborative effort by thousands of contributors. Wikipedia articles on the same topic in different languages are connected via interlingual (or translational) links. These links serve as an excellent resource for obtaining lexical translations, or building multilingual dictionaries and semantic networks. As these links are manually built, many links are missing or simply wrong. This paper describes a supervised learning method for generating new links and detecting existing incorrect links. Since there is no dataset available to evaluate the resulting interlingual links, we create our own gold standard by sampling translational links from four language pairs using distance heuristics. We manually annotate the sampled translation links and used them to evaluate the output of our method for automatic link detection and correction. 
This paper presents a novel sentence clustering scheme based on projecting sentences over term clusters. The scheme incorporates external knowledge to overcome lexical variability and small corpus size, and outperforms common sentence clustering methods on two reallife industrial datasets. 
We present the results of several machine learning tasks designed to predict rhetorical relations that hold between clauses in discourse. We demonstrate that organizing rhetorical relations into different granularity categories (based on relative degree of detail) increases average prediction accuracy from 58% to 70%. Accuracy further increases to 80% with the inclusion of clause types. These results, which are competitive with existing systems, hold across several modes of written discourse and suggest that features of information structure are an important consideration in the machine learnability of discourse. 
The correct choice of words has proven challenging for learners of a second language and errors of this kind form a separate category in error typology. This paper focuses on one known example of two verbs that are often confused by non-native speakers of Germanic languages, to make and to do. We conduct experiments using syntactic information and immediate context for Dutch and English. Our results show that the methods exploiting syntactic information and distributional similarity yield the best results. 
Text reuse is common in many scenarios and documents are often based, at least in part, on existing documents. This paper reports an approach to detecting text reuse which identiﬁes not only documents which have been reused verbatim but is also designed to identify cases of reuse when the original has been rewritten. The approach identiﬁes reuse by comparing word n-grams in documents and modiﬁes these (by substituting words with synonyms and deleting words) to identify when text has been altered. The approach is applied to a corpus of newspaper stories and found to outperform a previously reported method. 
Corpus-based thesaurus construction for Morphologically Rich Languages (MRL) is a complex task, due to the morphological variability of MRL. In this paper we explore alternative term representations, complemented by clustering of morphological variants. We introduce a generic algorithmic scheme for thesaurus construction in MRL, and demonstrate the empirical benefit of our methodology for a Hebrew thesaurus. 
In this paper, we investigate a full-ﬂedged supervised machine learning framework for identifying English phrasal verbs in a given context. We concentrate on those that we deﬁne as the most confusing phrasal verbs, in the sense that they are the most commonly used ones whose occurrence may correspond either to a true phrasal verb or an alignment of a simple verb with a preposition. We construct a benchmark dataset1 with 1,348 sentences from BNC, annotated via an Internet crowdsourcing platform. This dataset is further split into two groups, more idiomatic group which consists of those that tend to be used as a true phrasal verb and more compositional group which tends to be used either way. We build a discriminative classiﬁer with easily available lexical and syntactic features and test it over the datasets. The classiﬁer overall achieves 79.4% accuracy, 41.1% error deduction compared to the corpus majority baseline 65%. However, it is even more interesting to discover that the classiﬁer learns more from the more compositional examples than those idiomatic ones. 
We investigate the semantic relationship between a noun and its adjectival modiﬁers. We introduce a class of probabilistic models that enable us to to simultaneously capture both the semantic similarity of nouns and modiﬁers, and adjective-noun selectional preference. Through a combination of novel and existing evaluations we test the degree to which adjective-noun relationships can be categorised. We analyse the effect of lexical context on these relationships, and the efﬁcacy of the latent semantic representation for disambiguating word meaning. 
In this paper we apply existing directional similarity measures to identify hypernyms with a state-of-the-art distributional semantic model. We also propose a new directional measure that achieves the best performance in hypernym identiﬁcation. 
We report ongoing work on the development of agents that can implicitly coordinate with their partners in referential tasks, taking as a case study colour terms. We describe algorithms for generation and resolution of colour descriptions and report results of experiments on how humans use colour terms for reference in production and comprehension. 
 Given a set of images with related captions, our goal is to show how visual features can improve the accuracy of unsupervised word sense disambiguation when the textual context is very small, as this sort of data is common in news and social media. We extend previous work in unsupervised text-only disambiguation with methods that integrate text and images. We construct a corpus by using Amazon Mechanical Turk to caption sensetagged images gathered from ImageNet. Using a Yarowsky-inspired algorithm, we show that gains can be made over text-only disambiguation, as well as multimodal approaches such as Latent Dirichlet Allocation. 
We present a framework, based on Sejane and Eger (2012), for inducing lexical semantic typologies for groups of languages. Our framework rests on lexical semantic association networks derived from encoding, via bilingual corpora, each language in a common reference language, the tertium comparationis, so that distances between languages can easily be determined. 
Semantic role classiﬁcation accuracy for most languages other than English is constrained by the small amount of annotated data. In this paper, we demonstrate how the frame-to-frame relations described in the FrameNet ontology can be used to improve the performance of a FrameNet-based semantic role classiﬁer for Swedish, a low-resource language. In order to make use of the FrameNet relations, we cast the semantic role classiﬁcation task as a non-atomic label prediction task. The experiments show that the cross-frame generalization methods lead to a 27% reduction in the number of errors made by the classiﬁer. For previously unseen frames, the reduction is even more signiﬁcant: 50%. 
We study the task of automatically disambiguating word combinations such as jump the gun which are ambiguous between a literal and MWE interpretation, focusing on the utility of type-level features from an MWE lexicon for the disambiguation task. To this end we combine gold-standard idiomaticity of tokens in the OpenMWE corpus with MWE-type-level information drawn from the recently-published JDMWE lexicon. We ﬁnd that constituent modiﬁability in an MWE-type is more predictive of the idiomaticity of its tokens than other constituent characteristics such as semantic class or part of speech. 
Modeling user preferences is crucial in many real-life problems, ranging from individual and collective decision-making to strategic interactions between agents and game theory. Since agents do not come with their preferences transparently given in advance, we have only two means to determine what they are if we wish to exploit them in reasoning: we can infer them from what an agent says or from his nonlinguistic actions. In this paper, we analyze how to infer preferences from dialogue moves in actual conversations that involve bargaining or negotiation. To this end, we propose a new annotation scheme to study how preferences are linguistically expressed in two different corpus genres. This paper describes the annotation methodology and details the inter-annotator agreement study on each corpus genre. Our results show that preferences can be easily annotated by humans. 
Neurosemantics aims to learn the mapping between concepts and the neural activity which they elicit during neuroimaging experiments. Diﬀerent approaches have been used to represent individual concepts, but current state-of-the-art techniques require extensive manual intervention to scale to arbitrary words and domains. To overcome this challenge, we initiate a systematic comparison of automatically-derived corpus representations, based on various types of textual co-occurrence. We ﬁnd that dependency parse-based features are the most eﬀective, achieving accuracies similar to the leading semi-manual approaches and higher than any published for a corpus-based model. We also ﬁnd that simple word features enriched with directional information provide a close-tooptimal solution at much lower computational cost. 
This paper complements a series of works on implicative verbs such as manage to and fail to. It extends the description of simple implicative verbs to phrasal implicatives as take the time to and waste the chance to. It shows that the implicative signatures of over 300 verb-noun collocations depend both on the semantic type of the verb and the semantic type of the noun in a systematic way. 
We propose an unsupervised system that learns continuous degrees of lexicality for noun-noun compounds, beating a strong baseline on several tasks. We demonstrate that the distributional representations of compounds and their parts can be used to learn a ﬁnegrained representation of semantic contribution. Finally, we argue such a representation captures compositionality better than the current status-quo which treats compositionality as a binary classiﬁcation problem. 
Over the past decade, several underspeciﬁcation frameworks have been proposed that efﬁciently solve a big subset of scopeunderspeciﬁed semantic representations within the realm of the most popular constraint-based formalisms. However, there exists a family of coherent natural language sentences whose underspeciﬁed representation does not belong to this subset. It has remained an open question whether there exists a tractable superset of these frameworks, covering this family. In this paper, we show that the answer to this question is yes. We deﬁne a superset of the previous frameworks, which is solvable by similar algorithms with the same time and space complexity. 
Many types of polysemy are not word speciﬁc, but are instances of general sense alternations such as ANIMAL-FOOD. Despite their pervasiveness, regular alternations have been mostly ignored in empirical computational semantics. This paper presents (a) a general framework which grounds sense alternations in corpus data, generalizes them above individual words, and allows the prediction of alternations for new words; and (b) a concrete unsupervised implementation of the framework, the Centroid Attribute Model. We evaluate this model against a set of 2,400 ambiguous words and demonstrate that it outperforms two baselines. 
We present a rule-based method to automatically create a large-coverage semantic lexicon of French adjectives by extracting paradigmatic relations from lexicographic deﬁnitions. Formalized adjectival resources are, indeed, scarce for French and they mostly focus on morphological and syntactic information. Our objective is, therefore, to contribute enriching the available set of resources by taking advantage of reliable lexicographic data and formalizing it with the well-established lexical functions formalism. The resulting semantic lexicon of French adjectives can be used in NLP tasks such as word sense disambiguation or machine translation. After presenting related work, we describe the extraction method and the formalization procedure of the data. Our method is then quantitatively and qualitatively evaluated. We discuss the results of the evaluation and conclude on some perspectives.  jectival resources are, indeed, scarce for French and they mostly focus on morphological and syntactic information. Our goal is, therefore, to contribute enriching the available set of resources by taking advantage of reliable lexicographic data and formalizing it with the well-established lexical functions formalism of the Meaning-Text theory (Mel’cˇuk, 1996). The resulting semantic lexicon of French adjectives can be used in NLP tasks such as word sense disambiguation or machine translation1. In section 2, we present related work. In section 3, we expose the method used to build the lexicon, i.e. the extraction method and the formalization procedure of the data, and outline the main results. Finally, in section 4, we present a quantitative evaluation of our method and a qualitative evaluation of our data, and discuss their results. We conclude on some perspectives for future work. 2 Related Work  
This paper describes Bayesian selectional preference models that incorporate knowledge from a lexical hierarchy such as WordNet. Inspired by previous work on modelling with WordNet, these approaches are based either on “cutting” the hierarchy at an appropriate level of generalisation or on a “walking” model that selects a path from the root to a leaf. In an evaluation comparing against human plausibility judgements, we show that the models presented here outperform previously proposed comparable WordNet-based models, are competitive with state-of-the-art selectional preference models and are particularly wellsuited to estimating plausibility for items that were not seen in training. 
We present a method for learning syntaxsemantics mappings for verbs from unannotated corpora. We learn linkings, i.e., mappings from the syntactic arguments and adjuncts of a verb to its semantic roles. By learning such linkings, we do not need to model individual semantic roles independently of one another, and we can exploit the relation between different mappings for the same verb, or between mappings for different verbs. We present an evaluation on a standard test set for semantic role labeling. 
Word Sense Disambiguation aims to label the sense of a word that best applies in a given context. Graded word sense disambiguation relaxes the single label assumption, allowing for multiple sense labels with varying degrees of applicability. Training multi-label classiﬁers for such a task requires substantial amounts of annotated data, which is currently not available. We consider an alternate method of annotating graded senses using Word Sense Induction, which automatically learns the senses and their features from corpus properties. Our work proposes three objective to evaluate performance on the graded sense annotation task, and two new methods for mapping between sense inventories using parallel graded sense annotations. We demonstrate that sense induction offers signiﬁcant promise for accurate graded sense annotation. 
We present an ensemble-based framework for semantic lexicon induction that incorporates three diverse approaches for semantic class identiﬁcation. Our architecture brings together previous bootstrapping methods for pattern-based semantic lexicon induction and contextual semantic tagging, and incorporates a novel approach for inducing semantic classes from coreference chains. The three methods are embedded in a bootstrapping architecture where they produce independent hypotheses, consensus words are added to the lexicon, and the process repeats. Our results show that the ensemble outperforms individual methods in terms of both lexicon quality and instance-based semantic tagging. 
We present a novel technique for jointly predicting semantic arguments for lexical predicates. The task is to ﬁnd the best matching between semantic roles and sentential spans, subject to structural constraints that come from expert linguistic knowledge (e.g., in the FrameNet lexicon). We formulate this task as an integer linear program (ILP); instead of using an off-the-shelf tool to solve the ILP, we employ a dual decomposition algorithm, which we adapt for exact decoding via a branch-and-bound technique. Compared to a baseline that makes local predictions, we achieve better argument identiﬁcation scores and avoid all structural violations. Runtime is nine times faster than a proprietary ILP solver. 
Discourse coherence is an important aspect of natural language that is still understudied in computational linguistics. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicateargument structures (PAS) in a model that exceeds the sentence level. In particular, we aim to study the case of non-realized arguments as a coherence inducing factor. This task can be broken down into two subtasks. The ﬁrst aligns predicates across comparable texts, admitting partial argument structure correspondence. The resulting alignments and their contexts can then be used for developing a coherence model for argument realization. This paper introduces a large corpus of comparable monolingual texts as a prerequisite for approaching this task, including an evaluation set with manual predicate alignments. We illustrate the potential of this new resource for the empirical investigation of discourse coherence phenomena. Initial experiments on the task of predicting predicate alignments across text pairs show promising results. Our ﬁndings establish that manual and automatic predicate alignments across texts are feasible and that our data set holds potential for empirical research into a variety of discourse-related tasks. 
We investigate the effects of adding semantic annotations including word sense hypernyms to the source text for use as an extra source of information in HPSG parse ranking for the English Resource Grammar. The semantic annotations are coarse semantic categories or entries from a distributional thesaurus, assigned either heuristically or by a pre-trained tagger. We test this using two test corpora in different domains with various sources of training data. The best reduces error rate in dependency Fscore by 1% on average, while some methods produce substantial decreases in performance. 
Identifying textual inferences, where the meaning of one text follows from another, is a general underlying task within many natural language applications. Commonly, it is approached either by generative syntactic-based methods or by “lightweight” heuristic lexical models. We suggest a model which is conﬁned to simple lexical information, but is formulated as a principled generative probabilistic model. We focus our attention on the task of ranking textual inferences and show substantially improved results on a recently investigated question answering data set. 
Detecting emotions in microblogs and social media posts has applications for industry, health, and security. However, there exists no microblog corpus with instances labeled for emotions for developing supervised systems. In this paper, we describe how we created such a corpus from Twitter posts using emotionword hashtags. We conduct experiments to show that the self-labeled hashtag annotations are consistent and match with the annotations of trained judges. We also show how the Twitter emotion corpus can be used to improve emotion classiﬁcation accuracy in a different domain. Finally, we extract a word–emotion association lexicon from this Twitter corpus, and show that it leads to signiﬁcantly better results than the manually crafted WordNet Affect lexicon in an emotion classiﬁcation task.1 
Previous work on paraphrase extraction and application has relied on either parallel datasets, or on distributional similarity metrics over large text corpora. Our approach combines these two orthogonal sources of information and directly integrates them into our paraphrasing system’s log-linear model. We compare different distributional similarity feature-sets and show signiﬁcant improvements in grammaticality and meaning retention on the example text-to-text generation task of sentence compression, achieving stateof-the-art quality.  lel data (Bannard and Callison-Burch, 2005; Zhao et al., 2008; Callison-Burch, 2008; Ganitkevitch et al., 2011), while others leverage distributional methods on monolingual text corpora (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). So far, however, only preliminary studies have been undertaken to combine the information from these two sources (Chan et al., 2011). In this paper, we describe an extension of Ganitkevitch et al. (2011)’s bilingual data-based approach. We augment the bilingually-sourced paraphrases using features based on monolingual distributional similarity. More speciﬁcally:  
UCM-2 infers the words that are affected by negations by browsing dependency syntactic structures. It ﬁrst makes use of an algorithm that detects negation cues, like no, not or nothing, and the words affected by them by traversing Minipar dependency structures. Second, the scope of these negation cues is computed by using a post-processing rulebased approach that takes into account the information provided by the ﬁrst algorithm and simple linguistic clause boundaries. An initial version of the system was developed to handle the annotations of the Bioscope corpus. For the present version, we have changed, omitted or extended the rules and the lexicon of cues (allowing preﬁx and sufﬁx negation cues, such as impossible or meaningless), to make it suitable for the present task. 
Our system breaks down the problem of ranking a list of lexical substitutions according to how simple they are in a given context into a series of pairwise comparisons between candidates. For this we learn a binary classiﬁer. As only very little training data is provided, we describe a procedure for generating artiﬁcial unlabeled data from Wordnet and a corpus and approach the classiﬁcation task as a semisupervised machine learning problem. We use a co-training procedure that lets each classiﬁer increase the other classiﬁer’s training set with selected instances from an unlabeled data set. Our features include n-gram probabilities of candidate and context in a web corpus, distributional diﬀerences of candidate in a corpus of “easy” sentences and a corpus of normal sentences, syntactic complexity of documents that are similar to the given context, candidate length, and letter-wise recognizability of candidate as measured by a trigram character language model. 
 The goal of semantic dependency parsing is to build dependency structure and label semantic relation between a head and its modiﬁer. To attain this goal, we concentrate on obtaining better dependency structure to predict better semantic relations, and propose a method to combine the results of three state-of-the-art dependency parsers. Unfortunately, we made a mistake when we generate the ﬁnal output that results in a lower score of 56.31% in term of Labeled Attachment Score (LAS), reported by organizers. After giving golden testing set, we ﬁx the bug and rerun the evaluation script, this time we obtain the score of 62.8% which is consistent with the results on developing set. We will report detailed experimental results with correct program as a comparison standard for further research. 
In this paper, we describe the system architecture used in the Semantic Textual Similarity (STS) task 6 pilot challenge. The goal of this challenge is to accurately identify five levels of semantic similarity between two sentences: equivalent, mostly equivalent, roughly equivalent, not equivalent but sharing the same topic and no equivalence. Our participations were two systems. The first system (rule-based) combines both semantic and syntax features to arrive at the overall similarity. The proposed rules enable the system to adequately handle domain knowledge gaps that are inherent when working with knowledge resources. As such one of its main goals, the system suggests a set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences. The second system is our baseline in which we use the Cosine Similarity between the words in each sentence pair. 
In this paper we present our systems for the STS task. Our systems are all based on a simple process of identifying the components that correspond between two sentences. Currently we use words (that is word forms), lemmas, distributional similar words and grammatical relations identiﬁed with a dependency parser. We submitted three systems. All systems only use open class words. Our ﬁrst system (alignheuristic) tries to obtain a mapping between every open class token using all the above sources of information. Our second system (wordsim) uses a different algorithm and unlike alignheuristic, it does not use the dependency information. The third system (average) simply takes the average of the scores for each item from the other two systems to take advantage of the merits of both systems. For this reason we only provide a brief description of that. The results are promising, with Pearson’s coefﬁcients on each individual dataset ranging from .3765 to .7761 for our relatively simple heuristics based systems that do not require training on different datasets. We provide some analysis of the results and also provide results for our data using Spearman’s, which as a nonparametric measure which we argue is better able to reﬂect the merits of the different systems (average is ranked between the others).  Ultimately such a system could be exploited for ranking candidate paraphrases of a chunk of text of any length. We envisage a system as outlined in the future work section. The systems reported are simple baselines to such a system. We have two main systems (alignheuristic and wordsim) and also a system which simply uses the average score for each item from the two main systems (average). In our systems we: • only deal with open class words as tokens i.e. nouns, verbs, adjectives, adverbs. alignheuristic and average also use numbers • assume that tokens have a 1:1 mapping • match: – word forms – lemmas – distributionally similar lemmas – alignheuristic and average use the grammatical relation with a word that has a mapping and the same relation in reverse • score the sentence pair based on the size of the overlap. Different formulations of the score are used by our methods  
The paper aims to come up with a system that examines the degree of semantic equivalence between two sentences. At the core of the paper is the attempt to grade the similarity of two sentences by ﬁnding the maximal weighted bipartite match between the tokens of the two sentences. The tokens include single words, or multiwords in case of Named Entitites, adjectivally and numerically modiﬁed words. Two token similarity measures are used for the task - WordNet based similarity, and a statistical word similarity measure which overcomes the shortcomings of WordNet based similarity. As part of three systems created for the task, we explore a simple bag of words tokenization scheme, a more careful tokenization scheme which captures named entities, times, dates, monetary entities etc., and ﬁnally try to capture context around tokens using grammatical dependencies. 
In this paper we report the results obtained in the Semantic Textual Similarity (STS) task, with a system primarily developed for textual entailment. Our results are quite promising, getting a run ranked 39 in the official results with overall Pearson, and ranking 29 with the Mean metric.  The paper is organized as follows: Section 2 describes the relevant tasks, Section 3 describes the architecture of the system, then Section 4 shows the experiments carried out and the results obtained, and Section 5 presents some conclusions and future work. 2 Related work In this section we briefly describe two different tasks that are closely related and in which our system has participated with very promising results.  
In this paper, we present our system descrip- tion in task of Cross-lingual Textual Entail- ment. The goal of this task is to detect entailment relations between two sentences written in different languages. To accomplish this goal, we first translate sentences written in foreign languages into English. Then, we use EDITS1, an open source package, to recognize entailment relations. Since EDITS only draws monodirectional relations while the task requires bidirectional prediction, thus we exchange the hypothesis and test to detect entailment in another direction. Experimental results show that our method achieves promising results but not perfect results compared to other participants. 
This paper describes our participation in the task denominated Cross-Lingual Textual Entailment (CLTE) for content synchronization. We represent an approach to CLTE using machine translation to tackle the problem of multilinguality. Our system resides on machine learning and in the use of WordNet as semantic source knowledge. Results are very promising always achieving results above mean score. 
 James Pustejovsky Computer Science Department Brandeis University Waltham, MA USA jamesp@cs.brandeis.edu  The ability to understand spatial prepositions and motion in natural language will enable a variety of new applications involving systems that can respond to verbal directions, map travel guides, display incident reports, etc., providing for enhanced information extraction, question-answering, information retrieval, and more principled text to scene rendering. Until now, however, the semantics of spatial relations and motion verbs has been highly problematic. This tutorial presents a new approach to the semantics of spatial descriptions and motion expressions based on linguistically interpreted qualitative reasoning. Our approach allows for formal inference from spatial descriptions in natural language, while leveraging annotation schemes for time, space, and motion, along with machine learning from annotated corpora. We introduce a compositional semantics for motion expressions that integrates spatial primitives drawn from qualitative calculi. No previous exposure to the semantics of spatial prepositions or motion verbs is assumed. The tutorial will sharpen cross-linguistic intuitions about the interpretation of spatial prepositions and motion constructions. The attendees will also learn about qualitative reasoning schemes for static and dynamic spatial information, as well as three annotation schemes: TimeML, SpatialML, and ISO-Space, for time, space, and motion, respectively. While both cognitive and formal linguistics have examined the meaning of motion verbs and spatial prepositions, these earlier approaches do not yield precise computable representations that are expressive enough for natural languages. However, the previous literature makes it clear that communica-  tion of motion relies on imprecise and highly abstract geometric descriptions, rather than Euclidean ones that specify the coordinates and shapes of every object. This property makes these expressions a ﬁt target for the ﬁeld of qualitative spatial reasoning in AI, which has developed a rich set of geometric primitives for representing time, space (including distance, orientation, and topological relations), and motion. The results of such research have yielded a wide variety of spatial and temporal reasoning logics and tools. By reviewing these calculi and resources, this tutorial aims to systematically connect qualitative reasoning to natural language. Tutorial Schedule: I. Introduction. i. Overview of geometric idealizations underlying spatial PPs; ii. Linguistic patterns of motion verbs across languages; iii. A qualitative model for static spatial descriptions and for path verbs; iv. Overview of relevant annotation schemes. II. Calculi for Qualitative Spatial Reasoning. i. Semantics of spatial PPs mapped to qualitative spatial reasoning; ii. Qualitative calculi for representing topological and orientation relations; iii. Qualitative calculi to represent motion. III. Semantics of Motion Expressions. i. Introduction to Dynamic Interval Temporal Logic (DITL); ii. DITL representations for manner-of-motion verbs and path verbs; iii. Compositional semantics for motion expressions in DITL, with the spatial primitives drawn from qualitative calculi. IV. Applications and Research Topics. i. Route navigation, mapping travel narratives, QA, scene rendering from text, and generating event descriptions; ii. Open issues and further research topics.  
Introduction In recent years, machine learning (ML) has been used more and more to solve complex tasks in different disciplines, ranging from Data Mining to Information Retrieval or Natural Language Processing (NLP). These tasks often require the processing of structured input, e.g., the ability to extract salient features from syntactic/semantic structures is critical to many NLP systems. Mapping such structured data into explicit feature vectors for ML algorithms requires large expertise, intuition and deep knowledge about the target linguistic phenomena. Kernel Methods (KM) are powerful ML tools (see e.g., (Shawe-Taylor and Cristianini, 2004)), which can alleviate the data representation problem. They substitute feature-based similarities with similarity functions, i.e., kernels, directly deﬁned between training/test instances, e.g., syntactic trees. Hence feature vectors are not needed any longer. Additionally, kernel engineering, i.e., the composition or adaptation of several prototype kernels, facilitates the design of effective similarities required for new tasks, e.g., (Moschitti, 2004; Moschitti, 2008). Tutorial Content The tutorial aims at addressing the problems above: ﬁrstly, it will introduce essential and simpliﬁed theory of Support Vector Machines and KM with the only aim of motivating practical procedures and interpreting the results. Secondly, it will simply describe the current best practices for designing applications based on effective kernels. For this purpose, it will survey state-of-the-art kernels for diverse NLP applications, reconciling the different ap-  proaches with a uniform and global notation/theory. Such survey will beneﬁt from practical expertise acquired from directly working on many natural language applications, ranging from Text Categorization to Syntactic/Semantic Parsing. Moreover, practical demonstrations using SVM-Light-TK toolkit will nicely support the application-oriented perspective of the tutorial. The latter will lead NLP researchers with heterogeneous background to the acquisition of the KM know-how, which can be used to design any target NLP application. Finally, the tutorial will propose interesting new best practices, e.g., some recent methods for largescale learning with structural kernels (Severyn and Moschitti, 2011), structural lexical similarities (Croce et al., 2011) and reverse kernel engineering (Pighin and Moschitti, 2009). References Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured Lexical Similarity via Convolution Kernels on Dependency Trees. In Proc. of EMNLP. Alessandro Moschitti. 2004. A Study on Convolution Kernels for Shallow Semantic Parsing. In Proceedings of ACL. Alessandro Moschitti. 2008. Kernel Methods, Syntax and Semantics for Relational Text Categorization. In Proceedings of CIKM. Daniele Pighin and Alessandro Moschitti. 2009. Efﬁcient Linearization of Tree Kernel Functions. In Proceedings of CoNLL. 
Subjectivity and sentiment analysis focuses on the automatic identiﬁcation of private states, such as opinions, emotions, sentiments, evaluations, beliefs, and speculations in natural language. While subjectivity classiﬁcation labels text as either subjective or objective, sentiment classiﬁcation adds an additional level of granularity, by further classifying subjective text as either positive, negative or neutral. While much of the research work in this area has been applied to English, research on other languages is growing, including Japanese, Chinese, German, Spanish, Romanian. While most of the researchers in the ﬁeld are familiar with the methods applied on English, few of them have closely looked at the original research carried out in other languages. For example, in languages such as Chinese, researchers have been looking at the ability of characters to carry sentiment information (Ku et al., 2005; Xiang, 2011). In Romanian, due to markers of politeness and additional verbal modes embedded in the language, experiments have hinted that subjectivity detection may be easier to achieve (Banea et al., 2008). These additional sources of information may not be available across all languages, yet, various articles have pointed out that by investigating a synergistic approach for detecting subjectivity and sentiment in multiple languages at the same time, improvements can be achieved not only in other languages, but in English as well. The development and interest in these methods is also highly motivated by the fact that only 27% of Internet users speak English (www.internetworldstats.com/stats.htm,  Oct 11, 2011), and that number diminishes further every year, as more people across the globe gain Internet access. The aim of this tutorial is to familiarize the attendees with the subjectivity and sentiment research carried out on languages other than English in order to enable and promote crossfertilization. Speciﬁcally, we will review work along three main directions. First, we will present methods where the resources and tools have been speciﬁcally developed for a given target language. In this category, we will also brieﬂy overview the main methods that have been proposed for English, but which can be easily ported to other languages. Second, we will describe cross-lingual approaches, including several methods that have been proposed to leverage on the resources and tools available in English by using cross-lingual projections. Finally, third, we will show how the expression of opinions and polarity pervades language boundaries, and thus methods that holistically explore multiple languages at the same time can be effectively considered. 
 †chaolin@nccu.edu.tw, ‡chiaying@gate.sinica.edu.tw  Abstract We demonstrate applications of psycholinguistic and sublexical information for learning Chinese characters. The knowledge about the grapheme-phoneme conversion (GPC) rules of languages has been shown to be highly correlated to the ability of reading alphabetic languages and Chinese. We build and will demo a game platform for strengthening the association of phonological components in Chinese characters with the pronunciations of the characters. Results of a preliminary evaluation of our games indicated significant improvement in learners’ response times in Chinese naming tasks. In addition, we construct a Webbased open system for teachers to prepare their own games to best meet their teaching goals. Techniques for decomposing Chinese characters and for comparing the similarity between Chinese characters were employed to recommend lists of Chinese characters for authoring the games. Evaluation of the authoring environment with 20 subjects showed that our system made the authoring of games more effective and efficient. 
Metaphors pervade our language because they are elastic enough to allow a speaker to express an affective viewpoint on a topic without committing to a specific meaning. This balance of expressiveness and indeterminism means that metaphors are just as useful for eliciting information as they are for conveying information. We explore here, via a demonstration of a system for metaphor interpretation and generation called Metaphor Magnet, the practical uses of metaphor as a basis for formulating affective information queries. We also consider the kinds of deep and shallow stereotypical knowledge that are needed for such a system, and demonstrate how they can be acquired from corpora and the web. 
Tweets have become a comprehensive repository for real-time information. However, it is often hard for users to quickly get information they are interested in from tweets, owing to the sheer volume of tweets as well as their noisy and informal nature. We present QuickView, an NLP-based tweet search platform to tackle this issue. Speciﬁcally, it exploits a series of natural language processing technologies, such as tweet normalization, named entity recognition, semantic role labeling, sentiment analysis, tweet classiﬁcation, to extract useful information, i.e., named entities, events, opinions, etc., from a large volume of tweets. Then, non-noisy tweets, together with the mined information, are indexed, on top of which two brand new scenarios are enabled, i.e., categorized browsing and advanced search, allowing users to effectively access either the tweets or ﬁne-grained information they are interested in. 
We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 
We present langid.py, an off-the-shelf language identiﬁcation tool. We discuss the design and implementation of langid.py, and provide an empirical comparison on 5 longdocument datasets, and 2 datasets from the microblog domain. We ﬁnd that langid.py maintains consistently high accuracy across all domains, making it ideal for end-users that require language identiﬁcation without wanting to invest in preparation of in-domain training data. 
This paper describes the personalized normalization of a multilingual chat system that supports chatting in user defined short-forms or abbreviations. One of the major challenges for multilingual chat realized through machine translation technology is the normalization of non-standard, self-created short-forms in the chat message to standard words before translation. Due to the lack of training data and the variations of short-forms used among different social communities, it is hard to normalize and translate chat messages if user uses vocabularies outside the training data and create short-forms freely. We develop a personalized chat normalizer for English and integrate it with a multilingual chat system, allowing user to create and use personalized short-forms in multilingual chat. 
This system demonstration paper presents IRIS (Informal Response Interactive System), a chat-oriented dialogue system based on the vector space model framework. The system belongs to the class of examplebased dialogue systems and builds its chat capabilities on a dual search strategy over a large collection of dialogue samples. Additional strategies allowing for system adaptation and learning implemented over the same vector model space framework are also described and discussed. 
To facilitate the creation and usage of custom SMT systems we have created a cloud-based platform for do-it-yourself MT. The platform is developed in the EU collaboration project LetsMT!. This system demonstration paper presents the motivation in developing the LetsMT! platform, its main features, architecture, and an evaluation in a practical use case. 
We demonstrate a web-based environment for development and testing of different pedestrian route instruction-giving systems. The environment contains a City Model, a TTS interface, a game-world, and a user GUI including a simulated street-view. We describe the environment and components, the metrics that can be used for the evaluation of pedestrian route instruction-giving systems, and the shared challenge which is being organised using this environment.  This demonstration system brings together existing online data resources and software toolkits to create a low-cost framework for evaluation of pedestrian route instruction systems. We have built a web-based environment containing a simulated real world in which users can simulate walking on the streets of real cities whilst interacting with different navigation systems. This evaluation framework will be used in the near future to evaluate a series of instruction-giving dialogue systems. 2 Related work  
 2 Informal WFST preliminaries  In this paper, we present a new collection of open-source software libraries that provides command line binary utilities and library classes and functions for compiling regular expression and context-sensitive rewrite rules into ﬁnite-state transducers, and for n-gram language modeling. The OpenGrm libraries use the OpenFst library to provide an efﬁcient encoding of grammars and general algorithms for building, modifying and applying models. 
In this paper we present an API for programmatic access to BabelNet – a wide-coverage multilingual lexical knowledge base – and multilingual knowledge-rich Word Sense Disambiguation (WSD). Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction. 
This paper introduces BIUTEE1, an opensource system for recognizing textual entailment. Its main advantages are its ability to utilize various types of knowledge resources, and its extensibility by which new knowledge resources and inference components can be easily integrated. These abilities make BIUTEE an appealing RTE system for two research communities: (1) researchers of end applications, that can beneﬁt from generic textual inference, and (2) RTE researchers, who can integrate their novel algorithms and knowledge resources into our system, saving the time and effort of developing a complete RTE system from scratch. Notable assistance for these researchers is provided by a visual tracing tool, by which researchers can reﬁne and “debug” their knowledge resources and inference components. 
We present a novel text exploration model, which extends the scope of state-of-the-art technologies by moving from standard concept-based exploration to statement-based exploration. The proposed scheme utilizes the textual entailment relation between statements as the basis of the exploration process. A user of our system can explore the result space of a query by drilling down/up from one statement to another, according to entailment relations speciﬁed by an entailment graph and an optional concept taxonomy. As a prominent use case, we apply our exploration system and illustrate its beneﬁt on the health-care domain. To the best of our knowledge this is the ﬁrst implementation of an exploration system at the statement level that is based on the textual entailment relation. 
We present CSNIPER (Corpus Sniper), a tool that implements (i) a web-based multiuser scenario for identifying and annotating non-canonical grammatical constructions in large corpora based on linguistic queries and (ii) evaluation of annotation quality by measuring inter-rater agreement. This annotationby-query approach efﬁciently harnesses expert knowledge to identify instances of linguistic phenomena that are hard to identify by means of existing automatic annotation tools. 
The lack of parallel corpora and linguistic resources for many languages and domains is one of the major obstacles for the further advancement of automated translation. A possible solution is to exploit comparable corpora (non-parallel bi- or multi-lingual text resources) which are much more widely available than parallel translation data. Our presented toolkit deals with parallel content extraction from comparable corpora. It consists of tools bundled in two workflows: (1) alignment of comparable documents and extraction of parallel sentences and (2) extraction and bilingual mapping of terms and named entities. The toolkit pairs similar bilingual comparable documents and extracts parallel sentences and bilingual terminological and named entity dictionaries from comparable corpora. This demonstration focuses on the English, Latvian, Lithuanian, and Romanian languages. Introduction In recent decades, data-driven approaches have significantly advanced the development of machine translation (MT). However, lack of sufficient bilingual linguistic resources for many languages and domains is still one of the major obstacles for further advancement of automated translation. At the same time, comparable corpora, i.e., non-parallel bi- or multilingual text resources such as daily news articles and large knowledge  bases like Wikipedia, are much more widely available than parallel translation data. While methods for the use of parallel corpora in machine translation are well studied (Koehn, 2010), similar techniques for comparable corpora have not been thoroughly worked out. Only the latest research has shown that language pairs and domains with little parallel data can benefit from the exploitation of comparable corpora (Munteanu and Marcu, 2005; Lu et al., 2010; Smith et al., 2010; Abdul-Rauf and Schwenk, 2009 and 2011). In this paper we present the ACCURAT toolkit1 - a collection of tools that are capable of analysing comparable corpora and extracting parallel data which can be used to improve the performance of statistical and rule/example-based MT systems. Although the toolkit may be used for parallel data acquisition for open (broad) domain systems, it will be most beneficial for under-resourced languages or specific domains which are not covered by available parallel resources. The ACCURAT toolkit produces:  comparable document pairs with comparability scores, allowing to estimate the overall comparability of corpora;  parallel sentences which can be used as additional parallel data sources for statistical translation model learning; 
We present IlluMe, a software tool pack which creates a personalized ambient using the music and lighting. IlluMe includes an emotion analysis software, the small space ambient lighting, and a multimedia controller. The software analyzes emotional changes from instant message logs and corresponds the detected emotion to the best sound and light settings. The ambient lighting can sparkle with different forms of light and the smart phone can broadcast music respectively according to different atmosphere. All settings can be modified by the multimedia controller at any time and the new settings will be feedback to the emotion analysis software. The IlluMe system, equipped with the learning function, provides a link between residential situation and personal emotion. It works in a Chinese chatting environment to illustrate the language technology in life. 
We present a component for incremental speech synthesis (iSS) and a set of applications that demonstrate its capabilities. This component can be used to increase the responsivity and naturalness of spoken interactive systems. While iSS can show its full strength in systems that generate output incrementally, we also discuss how even otherwise unchanged systems may proﬁt from its capabilities. 
Information extraction (IE) is becoming a critical building block in many enterprise applications. In order to satisfy the increasing text analytics demands of enterprise applications, it is crucial to enable developers with general computer science background to develop high quality IE extractors. In this demonstration, we present WizIE, an IE development environment intended to reduce the development life cycle and enable developers with little or no linguistic background to write high quality IE rules. WizIE provides an integrated wizard-like environment that guides IE developers step-by-step throughout the entire development process, based on best practices synthesized from the experience of expert developers. In addition, WizIE reduces the manual effort involved in performing key IE development tasks by offering automatic result explanation and rule discovery functionality. Preliminary results indicate that WizIE is a step forward towards enabling extractor development for novice IE developers. 
This paper describes a system for real-time analysis of public sentiment toward presidential candidates in the 2012 U.S. election as expressed on Twitter, a microblogging service. Twitter has become a central site where people express their opinions and views on political parties and candidates. Emerging events or news are often followed almost instantly by a burst in Twitter volume, providing a unique opportunity to gauge the relation between expressed public sentiment and electoral events. In addition, sentiment analysis can help explore how these events affect public opinion. While traditional content analysis takes days or weeks to complete, the system demonstrated here analyzes sentiment in the entire Twitter traffic about the election, delivering results instantly and continuously. It offers the public, the media, politicians and scholars a new and timely perspective on the dynamics of the electoral process and public opinion. 
Argo is a web-based NLP and text mining workbench with a convenient graphical user interface for designing and executing processing workﬂows of various complexity. The workbench is intended for specialists and nontechnical audiences alike, and provides the ever expanding library of analytics compliant with the Unstructured Information Management Architecture, a widely adopted interoperability framework. We explore the ﬂexibility of this framework by demonstrating workﬂows involving three processing components capable of performing self-contained machine learning-based tagging. The three components are responsible for the three distinct tasks of 1) generating observations or features, 2) training a statistical model based on the generated features, and 3) tagging unlabelled data with the model. The learning and tagging components are based on an implementation of conditional random ﬁelds (CRF); whereas the feature generation component is an analytic capable of extending basic token information to a comprehensive set of features. Users deﬁne the features of their choice directly from Argo’s graphical interface, without resorting to programming (a commonly used approach to feature engineering). The experimental results performed on two tagging tasks, chunking and named entity recognition, showed that a tagger with a generic set of features built in Argo is capable of competing with taskspeciﬁc solutions.  
We describe Akamon, an open source toolkit for tree and forest-based statistical machine translation (Liu et al., 2006; Mi et al., 2008; Mi and Huang, 2008). Akamon implements all of the algorithms required for tree/forestto-string decoding using tree-to-string translation rules: multiple-thread forest-based decoding, n-gram language model integration, beam- and cube-pruning, k-best hypotheses extraction, and minimum error rate training. In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al., 2004) and HPSG trees/forests (Wu et al., 2010). 
We present Subgroup Detector, a system for analyzing threaded discussions and identifying the attitude of discussants towards one another and towards the discussion topic. The system uses attitude predictions to detect the split of discussants into subgroups of opposing views. The system uses an unsupervised approach based on rule-based opinion target detecting and unsupervised clustering techniques. The system is open source and is freely available for download. An online demo of the system is available at: http://clair.eecs.umich.edu/SubgroupDetector/  
Error analysis in machine translation is a necessary step in order to investigate the strengths and weaknesses of the MT systems under development and allow fair comparisons among them. This work presents an application that shows how a set of heterogeneous automatic metrics can be used to evaluate a test bed of automatic translations. To do so, we have set up an online graphical interface for the ASIYA toolkit, a rich repository of evaluation measures working at different linguistic levels. The current implementation of the interface shows constituency and dependency trees as well as shallow syntactic and semantic annotations, and word alignments. The intelligent visualization of the linguistic structures used by the metrics, as well as a set of navigational functionalities, may lead towards advanced methods for automatic error analysis. 
In this paper, we introduce a framework that identifies online plagiarism by exploiting lexical, syntactic and semantic features that includes duplication-gram, reordering and alignment of words, POS and phrase tags, and semantic similarity of sentences. We establish an ensemble framework to combine the predictions of each model. Results demonstrate that our system can not only find considerable amount of real-world online plagiarism cases but also outperforms several state-of-the-art algorithms and commercial software. Keywords Plagiarism Detection, Lexical, Syntactic, Semantic 1. Introduction Online plagiarism, the action of trying to create a new piece of writing by copying, reorganizing or rewriting others’ work identified through search engines, is one of the most commonly seen misusage of the highly matured web technologies. As implied by the experiment conducted by (Braumoeller and Gaines, 2001), a powerful plagiarism detection system can effectively discourage people from plagiarizing others’ work. A common strategy people adopt for onlineplagiarism detection is as follows. First they identify several suspicious sentences from the write-up and feed them one by one as a query to a search engine to obtain a set of documents. Then human reviewers can manually examine whether these documents are truly the sources of the suspicious sentences. While it is quite straightforward and effective, the limitation of this  strategy is obvious. First, since the length of search query is limited, suspicious sentences are usually queried and examined independently. Therefore, it is harder to identify document level plagiarism than sentence level plagiarism. Second, manually checking whether a query sentence plagiarizes certain websites requires specific domain and language knowledge as well as considerable amount of energy and time. To overcome the above shortcomings, we introduce an online plagiarism detection system using natural language processing techniques to simulate the above reverse-engineering approach. We develop an ensemble framework that integrates lexical, syntactic and semantic features to achieve this goal. Our system is language independent and we have implemented both Chinese and English versions for evaluation.  2. Related Work  Plagiarism detection has been widely discussed in the past decades (Zou et al., 2010). Table 1. summarizes some of them:  Author  Comparison Unit  Similarity Function  Brin et al., Word + Percentage of matching  1995  Sentence sentences.  White and Joy, 2004  Sentence  Average overlap ratio of the sentence pairs using 2 pre-defined thresholds.  Niezgoda and Way, 2006  A human defined sliding window  Sliding windows ranked by the average length per word.  Cedeno and Sentence + Overlap percentage of nRosso, 2009 n-gram gram in the sentence pairs.  145 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 145–150, Jeju, Republic of Korea, 8-14 July 2012. c 2012 Association for Computational Linguistics  Pera and Ng, 2010  Sentence  A pre-defined resemblance function based on word correlation factor.  Stamatatos, 2011  Passage  Overlap percentage of stopword n-grams.  Grman and Ravas, 2011  Passage  Matching percentage of words with given thresholds on both ratio and absolute number of words in passage.  Table 1. Summary of related works  Comparing to those systems, our system exploits more sophisticated syntactic and semantic information to simulate what plagiarists are trying to do. There are several online or charged/free downloadable plagiarism detection systems such as Turnitin, EVE2, Docol© c, and CATPPDS which detect mainly verbatim copy. Others such as Microsoft Plagiarism Detector (MPD), Safeassign, Copyscape and VeriGuide, claim to be capable of detecting obfuscations. Unfortunately those commercial systems do not reveal the detail strategies used, therefore it is hard to judge and reproduce their results for comparison.  3. Methodology  Figure 1. Detection Flow The data flow is shown above in Figure 1. 3.1 Query a Search Engine We first break down each article into a series of queries to query a search engine. Several systems such as (Liu at al., 2007) have proposed a similar idea. The main difference between our method and theirs is that we send unquoted queries rather than quoted ones. We do not require the search results  to completely match to the query sentence. This strategy allows us to not only identify the copy/paste type of plagiarism but also re-write/edit type of plagiarism.  3.2 Sentence-based Plagiarism Detection  Since not all outputs of a search engine contain an exact copy of the query, we need a model to quantify how likely each of them is the source of plagiarism. For better efficiency, our experiment exploits the snippet of a search output to represent the whole document. That is, we want to measure how likely a snippet is the plagiarized source of the query. We designed several models which utilized rich lexical, syntactic and semantic features to pursue this goal, and the details are discussed below.  3.2.1 Ngram Matching (NM) One straightforward measure is to exploit the ngram similarity between source and target texts. We first enumerate all n-grams in source, and then calculate the overlap percentage with the n-grams in the target. The larger n is, the harder for this feature to detect plagiarism with insertion, replacement, and deletion. In the experiment, we choose n=2.  3.2.2 Reordering of Words (RW)  Plagiarism can come from the reordering of words.  We argue that the permutation distance between S1  and S2 is an important indicator for reordered  plagiarism. The permutation distance is defined as  the minimum number of pair-wise exchanging of  matched words needed to transform a sentence, S2,  to contain the same order of matched words as  another sentence, S1. As mentioned in (Sörensena  and Sevaux, 2005), the permutation distance can  be calculated by the following expression  𝑑 𝑆1, 𝑆2 =  𝑛 −1 𝑖 =1  𝑛 𝑗 =𝑖  +1  𝑧𝑖𝑗  where  𝑧𝑖𝑗 =  1, 𝑖𝑓 𝑆1 𝑗 > 𝑆1 𝑖 𝑎𝑛𝑑 𝑆2 𝑗 < 𝑆2 𝑖 0, 𝑜𝑡𝑕𝑒𝑟𝑤𝑖𝑠𝑒  S1(i) and S2(i) are indices of the ith matched  word in sentences S1 and S2 respectively and n is  the number of matched words between the  sentences  S1  and  S2.  Let  μ = n2− n 2  be  the  normalized term, which is the maximum possible  distance between S1 and S2, then the reordering  146  score of the two sentences, expressed as s(S1, S2),  will be s S1, S2  = 1 − d S1,S2 μ  3.2.3 Alignment of Words (AW)  Besides reordering, plagiarists often insert or  delete words in a sentence. We try to model such  behavior by finding the alignment of two word  sequences. We perform the alignment using a  dynamic programming method as mentioned in  (Wagner and Fischer, 1975).  However, such alignment score does not reflect  the continuity of the matched words, which can be  an important cue to identify plagiarism. To  overcome such drawback, we modify the score as  below.  New Alignment Score =  |𝑀 |−1 𝑖 =1  𝐺𝑖  |𝑀|−1  where  𝐺𝑖  =  # 𝑜𝑓  𝑤𝑜𝑟𝑑𝑠  
We present UWN, a large multilingual lexical knowledge base that describes the meanings and relationships of words in over 200 languages. This paper explains how link prediction, information integration and taxonomy induction methods have been used to build UWN based on WordNet and extend it with millions of named entities from Wikipedia. We additionally introduce extensions to cover lexical relationships, frame-semantic knowledge, and language data. An online interface provides human access to the data, while a software API enables applications to look up over 16 million words and names. 
Writing in English might be one of the most difficult tasks for EFL (English as a Foreign Language) learners. This paper presents FLOW, a writing assistance system. It is built based on first-language-oriented input function and context sensitive approach, aiming at providing immediate and appropriate suggestions including translations, paraphrases, and n-grams during composing and revising processes. FLOW is expected to help EFL writers achieve their writing flow without being interrupted by their insufficient lexical knowledge. 1. Introduction Writing in a second language (L2) is a challenging and complex process for foreign language learners. Insufficient lexical knowledge and limited exposure to English might interrupt their writing flow (Silva, 1993). Numerous writing instructions have been proposed (Kroll, 1990) as well as writing handbooks have been available for learners. Studies have revealed that during the writing process, EFL learners show the inclination to rely on their native languages (Wolfersberger, 2003) to prevent a breakdown in the writing process (Arndt, 1987; Cumming, 1989). However, existing writing courses and instruction materials, almost second-language-oriented, seem unable to directly assist EFL writers while writing. This paper presents FLOW1 (Figure 1), an interactive system for assisting EFL writers in 
Social Event Radar is a new social networking-based service platform, that aim to alert as well as monitor any merchandise flaws, food-safety related issues, unexpected eruption of diseases or campaign issues towards to the Government, enterprises of any kind or election parties, through keyword expansion detection module, using bilingual sentiment opinion analysis tool kit to conclude the specific event social dashboard and deliver the outcome helping authorities to plan “risk control” strategy. With the rapid development of social network, people can now easily publish their opinions on the Internet. On the other hand, people can also obtain various opinions from others in a few seconds even though they do not know each other. A typical approach to obtain required information is to use a search engine with some relevant keywords. We thus take the social media and forum as our major data source and aim at collecting specific issues efficiently and effectively in this work.  
We present a new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of ﬁve centuries, in eight languages; it reﬂects 6% of all books ever published. This new edition introduces syntactic annotations: words are tagged with their part-of-speech, and headmodiﬁer relationships are recorded. The annotations are produced automatically with statistical models that are speciﬁcally adapted to historical text. The corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax.  burnt burnt_VERB burnt_ADJ burned burned_VERB burned_ADJ  Relative Frequency  1800  1850  1900  1950  2000  Figure 1: Usage frequencies of burned and burnt over time, showing that burned became the dominant spelling around 1880. Our new syntactic annotations enable a more reﬁned analysis, suggesting that the crossing-point for the verb usage (burned VERB vs. burnt VERB) was decades earlier.  
This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks conﬁrm its advantage over its ﬁrst-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers. 
We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. The protocol uses distance-based metrics deﬁned for the space of trees over lattices. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios. 
 Stanford dependencies are widely used in natural language processing as a semanticallyoriented representation, commonly generated either by (i) converting the output of a constituent parser, or (ii) predicting dependencies directly. Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies. In this paper, we re-evaluate both methods for Chinese, using more accurate dependency parsers than in previous work. Our comparison of performance and efﬁciency across seven popular open source parsers (four constituent and three dependency) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. We demonstrate also that n-way jackkniﬁng is a useful technique for producing automatic (rather than gold) partof-speech tags to train Chinese dependency parsers. Finally, we analyze the relations produced by both kinds of parsing and suggest which speciﬁc parsers to use in practice. 
We present LLCCM, a log-linear variant of the constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not. 
Some Statistical Machine Translation systems never see the light because the owner of the appropriate training data cannot release them, and the potential user of the system cannot disclose what should be translated. We propose a simple and practical encryption-based method addressing this barrier. 
In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efﬁciency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our approach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% BLEU, whereas our approach yields 20.0% with identical models. 
This paper presents an extension of Chiang’s hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model signiﬁcantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. 
SMT has been used in paraphrase generation by translating a source sentence into another (pivot) language and then back into the source. The resulting sentences can be used as candidate paraphrases of the source sentence. Existing work that uses two independently trained SMT systems cannot directly optimize the paraphrase results. Paraphrase criteria especially the paraphrase rate is not able to be ensured in that way. In this paper, we propose a joint learning method of two SMT systems to optimize the process of paraphrase generation. In addition, a revised BLEU score (called iBLEU ) which measures the adequacy and diversity of the generated paraphrase sentence is proposed for tuning parameters in SMT systems. Our experiments on NIST 2008 testing data with automatic evaluation as well as human judgments suggest that the proposed method is able to enhance the paraphrase quality by adjusting between semantic equivalency and surface dissimilarity. 
Mining retrospective events from text streams has been an important research topic. Classic text representation model (i.e., vector space model) cannot model temporal aspects of documents. To address it, we proposed a novel burst-based text representation model, denoted as BurstVSM. BurstVSM corresponds dimensions to bursty features instead of terms, which can capture semantic and temporal information. Meanwhile, it signiﬁcantly reduces the number of non-zero entries in the representation. We test it via scalable event detection, and experiments in a 10-year news archive show that our methods are both effective and efﬁcient. 
Although researchers have conducted extensive studies on relation extraction in the last decade, supervised approaches are still limited because they require large amounts of training data to achieve high performances. To build a relation extractor without signiﬁcant annotation effort, we can exploit cross-lingual annotation projection, which leverages parallel corpora as external resources for supervision. This paper proposes a novel graph-based projection approach and demonstrates the merits of it by using a Korean relation extraction system based on projected dataset from an English-Korean parallel corpus. 
We describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations. We leverage distant supervision using relations from the knowledge base FreeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision. 
In social psychology, it is generally accepted that one discloses more of his/her personal information to someone in a strong relationship. We present a computational framework for automatically analyzing such self-disclosure behavior in Twitter conversations. Our framework uses text mining techniques to discover topics, emotions, sentiments, lexical patterns, as well as personally identiﬁable information (PII) and personally embarrassing information (PEI). Our preliminary results illustrate that in relationships with high relationship strength, Twitter users show signiﬁcantly more frequent behaviors of self-disclosure. 
We describe an unsupervised approach to the problem of automatically detecting subgroups of people holding similar opinions in a discussion thread. An intuitive way of identifying this is to detect the attitudes of discussants towards each other or named entities or topics mentioned in the discussion. Sentiment tags play an important role in this detection, but we also note another dimension to the detection of people’s attitudes in a discussion: if two persons share the same opinion, they tend to use similar language content. We consider the latter to be an implicit attitude. In this paper, we investigate the impact of implicit and explicit attitude in two genres of social media discussion data, more formal wikipedia discussions and a debate discussion forum that is much more informal. Experimental results strongly suggest that implicit attitude is an important complement for explicit attitudes (expressed via sentiment) and it can improve the sub-group detection performance independent of genre. 
We investigate the problem of ordering medical events in unstructured clinical narratives by learning to rank them based on their time of occurrence. We represent each medical event as a time duration, with a corresponding start and stop, and learn to rank the starts/stops based on their proximity to the admission date. Such a representation allows us to learn all of Allen’s temporal relations between medical events. Interestingly, we observe that this methodology performs better than a classiﬁcation-based approach for this domain, but worse on the relationships found in the Timebank corpus. This ﬁnding has important implications for styles of data representation and resources used for temporal relation learning: clinical narratives may have different language attributes corresponding to temporal ordering relative to Timebank, implying that the ﬁeld may need to look at a wider range of domains to fully understand the nature of temporal ordering. 
Since we can ‘spin’ words and concepts to suit our affective needs, context is a major determinant of the perceived affect of a word or concept. We view this re-profiling as a selective emphasis or de-emphasis of the qualities that underpin our shared stereotype of a concept or a word meaning, and construct our model of the affective lexicon accordingly. We show how a large body of affective stereotypes can be acquired from the web, and also show how these are used to create and interpret affective metaphors.  de-emphasized in a given context – a particular metaphor, say, might describe hackers as terrorists or hackers as artists – we need to be able to recalculate the perceived affect of the word-concept. This paper presents such a stereotype-grounded model of the affective lexicon. After reviewing the relevant background in section 2, we present the basis of the model in section 3. Here we describe how a large body of feature-rich stereotypes is acquired from the web and from local n-grams. The model is evaluated in section 4. We conclude by showing the utility of the model to that most contextual of NLP phenomena – affective metaphor. 2 Related Work and Ideas  
There has been recent interest in the problem of decoding letter substitution ciphers using techniques inspired by natural language processing. We consider a different type of classical encoding scheme known as the running key cipher, and propose a search solution using Gibbs sampling with a word language model. We evaluate our method on synthetic ciphertexts of different lengths, and ﬁnd that it outperforms previous work that employs Viterbi decoding with character-based models. 
We present a novel extension to a recently proposed incremental learning algorithm for the word segmentation problem originally introduced in Goldwater (2006). By adding rejuvenation to a particle ﬁlter, we are able to considerably improve its performance, both in terms of ﬁnding higher probability and higher accuracy solutions. 
Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classiﬁcation, but their performance varies greatly depending on the model variant, features used and task/ dataset. We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets. Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level. 
We propose a new approach for the creation of child language development metrics. A set of linguistic features is computed on child speech samples and used as input in two age prediction experiments. In the ﬁrst experiment, we learn a child-speciﬁc metric and predicts the ages at which speech samples were produced. We then learn a more general developmental index by applying our method across children, predicting relative temporal orderings of speech samples. In both cases we compare our results with established measures of language development, showing improvements in age prediction performance. 
This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation accuracies. 
We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (51.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser. 
We present a Bayesian nonparametric model for estimating tree insertion grammars (TIG), building upon recent work in Bayesian inference of tree substitution grammars (TSG) via Dirichlet processes. Under our general variant of TIG, grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blunsom (2010). We use the Penn treebank for our experiments and ﬁnd that our proposal Bayesian TIG model not only has competitive parsing performance but also ﬁnds compact yet linguistically rich TIG representations of the data. 
We propose an approach that biases machine translation systems toward relevant translations based on topic-speciﬁc contexts, where topics are induced in an unsupervised way using topic models; this can be thought of as inducing subcorpora for adaptation without any human annotation. We use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features. Conditioning lexical probabilities on the topic biases translations toward topicrelevant output, resulting in signiﬁcant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline. 
We address a core aspect of the multilingual content synchronization task: the identiﬁcation of novel, more informative or semantically equivalent pieces of information in two documents about the same topic. This can be seen as an application-oriented variant of textual entailment recognition where: i) T and H are in different languages, and ii) entailment relations between T and H have to be checked in both directions. Using a combination of lexical, syntactic, and semantic features to train a cross-lingual textual entailment system, we report promising results on different datasets. 
We present a system for cross-lingual parse disambiguation, exploiting the assumption that the meaning of a sentence remains unchanged during translation and the fact that different languages have different ambiguities. We simultaneously reduce ambiguity in multiple languages in a fully automatic way. Evaluation shows that the system reliably discards dispreferred parses from the raw parser output, which results in a pre-selection that can speed up manual treebanking. 
In this paper, we present a new method for learning to finding translations and transliterations on the Web for a given term. The approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine, and automatically annotating the snippets with tags and features for training a conditional random field model. At runtime, the model is used to extracting translation candidates for a given term. Preliminary experiments and evaluation show our method cleanly combining various features, resulting in a system that outperforms previous work. 
We investigate how novel English-derived words (anglicisms) are used in a Germanlanguage Internet hip hop forum, and what factors contribute to their uptake. 
In this paper we study unsupervised word sense disambiguation (WSD) based on sense deﬁnition. We learn low-dimensional latent semantic vectors of concept deﬁnitions to construct a more robust sense similarity measure wmfvec. Experiments on four all-words WSD data sets show signiﬁcant improvement over the baseline WSD systems and LDA based similarity measures, achieving results comparable to state of the art WSD systems. 
We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a uniﬁed model. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering. 
 This work presents a ﬁrst step to a general implementation of the Semantic-Script Theory of Humor (SSTH). Of the scarce amount of research in computational humor, no research had focused on humor generation beyond simple puns and punning riddles. We propose an algorithm for mining simple humorous scripts from a semantic network (ConceptNet) by speciﬁcally searching for dual scripts that jointly maximize overlap and incongruity metrics in line with Raskin’s Semantic-Script Theory of Humor. Initial results show that a more relaxed constraint of this form is capable of generating humor of deeper semantic content than wordplay riddles. We evaluate the said metrics through a user-assessed quality of the generated two-liners. 
The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources. However, evaluating such resources has turned out to be a non-trivial task, slowing progress in the ﬁeld. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simpliﬁes a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators. 
This paper presents a two-step approach to compress spontaneous spoken utterances. In the ﬁrst step, we use a sequence labeling method to determine if a word in the utterance can be removed, and generate n-best compressed sentences. In the second step, we use a discriminative training approach to capture sentence level global information from the candidates and rerank them. For evaluation, we compare our system output with multiple human references. Our results show that the new features we introduced in the ﬁrst compression step improve performance upon the previous work on the same data set, and reranking is able to yield additional gain, especially when training is performed to take into account multiple references. 
Most previous studies in computerized deception detection have relied only on shallow lexico-syntactic patterns. This paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature. Over four diﬀerent datasets spanning from the product review to the essay domain, we demonstrate that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data (Ott et al., 2011) reaching 91.2% accuracy with 14% error reduction. 
Natural language questions have become popular in web search. However, various questions can be formulated to convey the same information need, which poses a great challenge to search systems. In this paper, we automatically mined 5w1h question reformulation patterns from large scale search log data. The question reformulations generated from these patterns are further incorporated into the retrieval model. Experiments show that using question reformulation patterns can signiﬁcantly improve the search performance of natural language questions. 
We investigate the potential of Tree Substitution Grammars as a source of features for native language detection, the task of inferring an author’s native language from text in a different language. We compare two state of the art methods for Tree Substitution Grammar induction and show that features from both methods outperform previous state of the art results at native language detection. Furthermore, we contrast these two induction algorithms and show that the Bayesian approach produces superior classiﬁcation results with a smaller feature set. 
As the number of learners of English is constantly growing, automatic error correction of ESL learners’ writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difﬁcult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classiﬁcation on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate contribution to tense/aspect error correction. 
This paper describes Movie-DiC a Movie Dialogue Corpus recently collected for research and development purposes. The collected dataset comprises 132,229 dialogues containing a total of 764,146 turns that have been extracted from 753 movies. Details on how the data collection has been created and how it is structured are provided along with its main statistics and characteristics. 
Blogs and forums are widely adopted by online communities to debate about various issues. However, a user that wants to cut in on a debate may experience some difﬁculties in extracting the current accepted positions, and can be discouraged from interacting through these applications. In our paper, we combine textual entailment with argumentation theory to automatically extract the arguments from debates and to evaluate their acceptability. 
This paper describes a novel approach towards the empirical approximation of discourse relations between different utterances in texts. Following the idea that every pair of events comes with preferences regarding the range and frequency of discourse relations connecting both parts, the paper investigates whether these preferences are manifested in the distribution of relation words (that serve to signal these relations). Experiments on two large-scale English web corpora show that signiﬁcant correlations between pairs of adjacent events and relation words exist, that they are reproducible on different data sets, and for three relation words, that their distribution corresponds to theorybased assumptions. 
Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages. 1. Introduction Arabic exhibits rich morphological phenomena that complicate retrieval. Arabic nouns and verbs are typically derived from a set of 10,000 roots that are cast into stems using templates that may add infixes, double letters, or remove letters. Stems can accept the attachment of clitics, in the form of prefixes or suffixes, such as prepositions, determiners, pronouns, etc. Orthographic rules can cause the addition, deletion, or substitution of letters during suffix and prefix attachment. Further, stems can be inflected to obtain plural forms via the addition of suffixes or through using a different stem form altogether producing socalled broken1 (aka irregular) plurals. For retrieval, we would ideally like to match “related” stem forms regardless of inflected form or attached clitic. Tolerating some form of derivational morphology where nouns are transformed into adjectives via the attachment of  the suffix ‫( يﻱ‬y)2 (ex. ‫( ﻣﺼﺮ‬mSr) è ‫( ﻣﺼﺮيﻱ‬mSry)) is desirable as they are semantically related. Matching all stems that are cast from the same root would introduce undesired ambiguity, because a single root can produce up to 1,000 stems. Two general approaches have been shown to improve Arabic retrieval. The first approach involves stemming, which removes clitics, plural and gender markers, and suffixes such as ‫( يﻱ‬y). Statistical stemming was reported to be the most effective for Arabic retrieval (Darwish et al., 2005). Though effective, stemming has the following drawbacks: 1.Stemming does not handle infixes and hence cannot conflate singular and broken plural word forms. For example, the plural of the Arabic word for book “‫( ”ﻛﺘﺎبﺏ‬ktAb) is “‫( ”ﻛﺘﺐ‬ktb). 2.Stemming of some named entities, which are important for retrieval, and their inflected forms may produce different stems as word endings may change with the attachment of suffixes. Consider the Arabic words for America ‫أﺃﻣﺮﯾﻳﻜﺎ‬ (>mrykA) and American ‫>( أﺃﻣﺮﯾﻳﻜﻲ‬mryky), where the final letter is transformed from “A” to “y”. The second approach involves using character 3or 4-grams (as opposed to words) (Mayfield et al., 2001; Darwish and Oard, 2002). For example, the trigrams of “WORD” are “WOR” and “ORD”. This approach though it has been shown to improve retrieval effectiveness, it has the following drawbacks: 1.It cannot handle broken plurals, though it would handle words where stemming would produce different stems for different inflected forms. 2.It significantly increases index sizes. For example, using a 6 letter word would produce 4 trigram chunks, which would have 12 letters. 3.Longer words would yield more character ngram chunks compared to shorter ones leading to skewed weights for query words.  
We seek to automatically estimate typical durations for events and habits described in Twitter tweets. A corpus of more than 14 million tweets containing temporal duration information was collected. These tweets were classified as to their habituality status using a bootstrapped, decision tree. For each verb lemma, associated duration information was collected for episodic and habitual uses of the verb. Summary statistics for 483 verb lemmas and their typical habit and episode durations has been compiled and made available. This automatically generated duration information is broadly comparable to hand-annotation. 
Interpreting news requires identifying its constituent events. Events are complex linguistically and ontologically, so disambiguating their reference is challenging. We introduce event linking, which canonically labels an event reference with the article where it was ﬁrst reported. This implicitly relaxes coreference to co-reporting, and will practically enable augmenting news archives with semantic hyperlinks. We annotate and analyse a corpus of 150 documents, extracting 501 links to a news archive with reasonable inter-annotator agreement. 
The Web and digitized text sources contain a wealth of information about named entities such as politicians, actors, companies, or cultural landmarks. Extracting this information has enabled the automated construction of large knowledge bases, containing hundred millions of binary relationships or attribute values about these named entities. However, in reality most knowledge is transient, i.e. changes over time, requiring a temporal dimension in fact extraction. In this paper we develop a methodology that combines label propagation with constraint reasoning for temporal fact extraction. Label propagation aggressively gathers fact candidates, and an Integer Linear Program is used to clean out false hypotheses that violate temporal constraints. Our method is able to improve on recall while keeping up with precision, which we demonstrate by experiments with biography-style Wikipedia pages and a large corpus of news articles. 
Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difﬁcult. We propose a simple, efﬁcient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. Unlike previous work, our ﬁnal model does not require any additional resources at run-time. Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis. 
This paper presents the problem within Hittite and Ancient Near Eastern studies of fragmented and damaged cuneiform texts, and proposes to use well-known text classiﬁcation metrics, in combination with some facts about the structure of Hittite-language cuneiform texts, to help classify a number of fragments of clay cuneiform-script tablets into more complete texts. In particular, I propose using Sumerian and Akkadian ideogrammatic signs within Hittite texts to improve the performance of Naive Bayes and Maximum Entropy classiﬁers. The performance in some cases is improved, and in some cases very much not, suggesting that the variable frequency of occurrence of these ideograms in individual fragments makes considerable difference in the ideal choice for a classiﬁcation method. Further, complexities of the writing system and the digital availability of Hittite texts complicate the problem. 
This paper describes the creation of the ﬁrst large-scale corpus containing drafts and ﬁnal versions of essays written by non-native speakers, with the sentences aligned across different versions. Furthermore, the sentences in the drafts are annotated with comments from teachers. The corpus is intended to support research on textual revision by language learners, and how it is inﬂuenced by feedback. This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI). 
“Lightweight” semantic annotation of text calls for a simple representation, ideally without requiring a semantic lexicon to achieve good coverage in the language and domain. In this paper, we repurpose WordNet’s supersense tags for annotation, developing speciﬁc guidelines for nominal expressions and applying them to Arabic Wikipedia articles in four topical domains. The resulting corpus has high coverage and was completed quickly with reasonable inter-annotator agreement. 
In this paper we introduce the novel task of “word epoch disambiguation,” deﬁned as the problem of identifying changes in word usage over time. Through experiments run using word usage examples collected from three major periods of time (1800, 1900, 2000), we show that the task is feasible, and signiﬁcant differences can be observed between occurrences of words in different periods of time. 
Authorship attribution deals with identifying the authors of anonymous texts. Building on our earlier ﬁnding that the Latent Dirichlet Allocation (LDA) topic model can be used to improve authorship attribution accuracy, we show that employing a previously-suggested Author-Topic (AT) model outperforms LDA when applied to scenarios with many authors. In addition, we deﬁne a model that combines LDA and AT by representing authors and documents over two disjoint topic sets, and show that our model outperforms LDA, AT and support vector machines on datasets with many authors.  by Blei et al. (2003) – yield good AA performance. However, LDA does not model authors explicitly, and we are not aware of any previous studies that apply author-aware topic models to traditional AA. This paper aims to address this gap. In addition to being the ﬁrst (to the best of our knowledge) to apply Rosen-Zvi et al.’s (2004) Author-Topic Model (AT) to traditional AA, the main contribution of this paper is our Disjoint Author-Document Topic Model (DADT), which addresses AT’s limitations in the context of AA. We show that DADT outperforms AT, LDA, and linear support vector machines on AA with many authors. 2 Disjoint Author-Document Topic Model  
We use multiple views for cross-domain document classiﬁcation. The main idea is to strengthen the views’ consistency for target data with source training data by identifying the correlations of domain-speciﬁc features from different domains. We present an Information-theoretic Multi-view Adaptation Model (IMAM) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-speciﬁc features from both sides and iteratively boost the consistency between document clusterings based on word and link views. Experiments show that IMAM signiﬁcantly outperforms state-of-the-art baselines. 
Topic modeling with a tree-based prior has been used for a variety of applications because it can encode correlations between words that traditional topic modeling cannot. However, its expressive power comes at the cost of more complicated inference. We extend the SPARSELDA (Yao et al., 2009) inference scheme for latent Dirichlet allocation (LDA) to tree-based topic models. This sampling scheme computes the exact conditional distribution for Gibbs sampling much more quickly than enumerating all possible latent variable assignments. We further improve performance by iteratively reﬁning the sampling distribution only when needed. Experiments show that the proposed techniques dramatically improve the computation time. 
This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation.  S  NP  VP  Word alignment Frontier node  DT NNS VBZ  ADVP  RB VBN  the imports have drastically fallen  进口 jinkou  大幅度 dafudu AD  减少 了 jianshao le VV AS VP  NN  VP  S  Figure 1. A real example of Chinese-English sentence pair with word alignment and both-side parse trees.  
The dominant practice of statistical machine translation (SMT) uses the same Chinese word segmentation specification in both alignment and translation rule induction steps in building Chinese-English SMT system, which may suffer from a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses two different segmentation specifications for alignment and translation respectively: we use Chinese character as the basic unit for alignment, and then convert this alignment to conventional word alignment for translation rule induction. Experimentally, our approach outperformed two baselines: fully word-based system (using word for both alignment and translation) and fully character-based system, in terms of alignment quality and translation performance. 
In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. 
We propose a novel heuristic algorithm for Cube Pruning running in linear time in the beam size. Empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy. 
We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline. 
Bayesian approaches have been shown to reduce the amount of overﬁtting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score. 
Reordering is a difﬁcult task in translating between widely different languages such as Japanese and English. We employ the postordering framework proposed by (Sudoh et al., 2011b) for Japanese to English translation and improve upon the reordering method. The existing post-ordering method reorders a sequence of target language words in a source language word order via SMT, while our method reorders the sequence by: 1) parsing the sequence to obtain syntax structures similar to a source language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language. 
Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We ﬁnd that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank. 
If unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving MT performance on low-resource inﬂected languages. In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system. We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable. Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic. Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005), a leading supervised MSA segmenter. 
We introduce a novel method for grammatical error correction with a number of small corpora. To make the best use of several corpora with different characteristics, we employ a meta-learning with several base classifiers trained on different corpora. This research focuses on a grammatical error correction task for article errors. A series of experiments is presented to show the effectiveness of the proposed approach on two different grammatical error tagged corpora. 1. Introduction As language learning has drawn significant attention in the community, grammatical error correction (GEC), consequently, has attracted a fair amount of attention. Several organizations have built diverse resources including grammatical error (GE) tagged corpora. Although there are some publicly released GE tagged corpora, it is still challenging to train a good GEC model due to the lack of large GE tagged learner corpus. The available GE tagged corpora are mostly small datasets having different characteristics depending on the development methods, e.g. spoken corpus vs. written corpus. This situation forced researchers to utilize native corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers  focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al., 2000; Lee, 2004; Nagata et al., 2006; Han et al., 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al., 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received less focus. In this paper, we present a novel approach to the GEC task using meta-learning. We focus mainly on article errors for two reasons. First, articles are one of the most significant sources of GE for the learners with various L1 backgrounds. Second, the effective features for article error correction are already well engineered allowing for quick analysis of the method. Our approach is distinguished from others by integrating the predictive models trained on several GE tagged learner corpora, rather than just one GE tagged corpus. Moreover, the framework is compatible to any classification technique. In this study, we also use a native corpus employing Dahlmeier and Ng’s approach. We demonstrate the effectiveness of the proposed method against baseline models in article error correction tasks.  328 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 328–332, Jeju, Republic of Korea, 8-14 July 2012. c 2012 Association for Computational Linguistics  Figure 1: Overview of the proposed method  The remainder of this paper is organized as follows: Section 2 explains our proposed method. The experiments are presented in Section 3. Finally, Section 4 concludes the paper. 2. Method Our method predicts the type of article for a noun phrase within three classes: null, definite, and indefinite. A correction arises when the prediction disagrees with the observed article. The meta-learning technique is applied to this task to deal with multiple corpora obtained from different sources. A meta-classifier decides the final output based on the intermediate results obtained from several base classifiers. Each base classifier is trained on a different corpus than are the other classifiers. In this work, the feature extraction processes used for the base classifiers are identical to each other for simplicity, although they need not necessarily be identical. The meta-classifier takes the output scores of the base classifiers as its input and is trained on the held-out development data (Figure 1a). During run time, the trained classifiers are organized in the same manner. For the given features, the base classifiers independently calculate the score, then the meta-classifier makes the final decision based on the scores (Figure 1b). 2.1. Meta-learning Meta-learning is a sequential learning process following the output of other base learners (classifiers). Normally, different classifiers successfully predict results on different parts of the  input space, so researchers have often tried to combine different classifiers together (Breiman, 1996; Cohen et al., 2007; Zhang, 2007; Aydın, 2009; Menahem et al., 2009). To capitalize on the strengths and compensate for the weaknesses of each classifier, we build a meta-learner that takes an input vector consisting of the outputs of the base classifiers. The performance of meta-learning can be improved using output probabilities for every class label from the base classifiers. The meta-classifier for the proposed method consists of multiple linear classifiers. Each classifier takes an input vector consisting of the output scores of each base classifier and calculates a score for each type of article. The meta-classifier finally takes the class having the maximum score. A common design of an ensemble is to train different base classifiers with the same dataset, but in this work one classification technique was used with different datasets each having different characteristics. Although only one classification method was used in this work, different methods each well-tuned to the individual corpora may be used to improve the performance. We employed the meta-learning method to generate synergy among corpora with diverse characteristics. More specifically, it is shown by cross validation that meta-learning performs at a level that is comparable to the best base classifier (Dzeroski and Zenko, 2004). 2.2. Base Classifiers In the meta-learning framework, the performance of the base classifiers is important because the improvement in base classification generally enha-  329  nces the overall performance. The base classifiers can be expected to become more informative as more data are provided. We followed the structural learning approach (Ando and Zhang, 2005), which trains a model from both a native corpus and a GE tagged corpus (Dahlmeire and Ng, 2011), to improve the base classifiers by the additional information extracted from a native corpus. Structural learning is a technique which trains multiple classifiers with common structure. The common structure chooses the hypothesis space of each individual classifier and the individual classifiers are trained separately once the hypothesis space is determined. The common structure can be obtained from auxiliary problems which are closely related to the main problems. A word selection problem is a task to predict the appropriate word given the surrounding context in a native corpus and is a closely related auxiliary problem of the GEC task. We can obtain the common structure from the article selection problem and use it for the correction problem. In this work, all the base classifiers used the same least squares loss function for structural learning. We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005) to extract the features. 2.3. Evaluation Metric The effectiveness of the proposed method is evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances. Precision is the ratio of the suggested corrections that agree with the tagged answer to the total number of the suggested corrections whereas recall is the ratio of the suggested corrections that agree with the tagged answer to the total number of corrections in the corpus. 3. Experiments 3.1. Datasets In this work we used a native corpus and two GE tagged corpora. For the native corpus, we used 
In this paper, we present a structural learning model for joint sentiment classiﬁcation and aspect analysis of text at various levels of granularity. Our model aims to identify highly informative sentences that are aspect-speciﬁc in online custom reviews. The primary advantages of our model are two-fold: ﬁrst, it performs document-level and sentence-level sentiment polarity classiﬁcation jointly; second, it is able to ﬁnd informative sentences that are closely related to some respects in a review, which may be helpful for aspect-level sentiment analysis such as aspect-oriented summarization. The proposed method was evaluated with 9,000 Chinese restaurant reviews. Preliminary experiments demonstrate that our model obtains promising performance. 
Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classiﬁcation. We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 point absolute improvement in accuracy over a bag-of-words classiﬁer on a widely used sentiment corpus. 
This paper brings a marriage of two seemly unrelated topics, natural language processing (NLP) and social network analysis (SNA). We propose a new task in SNA which is to predict the diffusion of a new topic, and design a learning-based framework to solve this problem. We exploit the latent semantic information among users, topics, and social connections as features for prediction. Our framework is evaluated on real data collected from public domain. The experiments show 16% AUC improvement over baseline methods. The source code and dataset are available at http://www.csie.ntu.edu.tw/~d97944007/dif fusion/ 
 For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques. 
This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach. 
We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments. 
This paper presents a novel way of improving POS tagging on heterogeneous data. First, two separate models are trained (generalized and domain-speciﬁc) from the same data set by controlling lexical items with different document frequencies. During decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data. This dynamic model selection approach, coupled with a one-pass, leftto-right POS tagging algorithm, is evaluated on corpora from seven different genres. Even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data. Furthermore, our system is able to tag about 32K tokens per second. We believe that this model selection approach can be applied to more sophisticated tagging algorithms and improve their robustness even further. 
We present a novel approach to the task of word lemmatisation. We formalise lemmatisation as a category tagging task, by describing how a word-to-lemma transformation rule can be encoded in a single label and how a set of such labels can be inferred for a speciﬁc language. In this way, a lemmatisation system can be trained and tested using any supervised tagging model. In contrast to previous approaches, the proposed technique allows us to easily integrate relevant contextual information. We test our approach on eight languages reaching a new state-of-the-art level for the lemmatisation task. 
This paper presents a comparative study of spelling errors that are corrected as you type, vs. those that remain uncorrected. First, we generate naturally occurring online error correction data by logging users’ keystrokes, and by automatically deriving pre- and postcorrection strings from them. We then perform an analysis of this data against the errors that remain in the ﬁnal text as well as across languages. Our analysis shows a clear distinction between the types of errors that are generated and those that remain uncorrected, as well as across languages. 
We examine some of the frequently disregarded subtleties of tokenization in Penn Treebank style, and present a new rule-based preprocessing toolkit that not only reproduces the Treebank tokenization with unmatched accuracy, but also maintains exact stand-off pointers to the original text and allows ﬂexible conﬁguration to diverse use cases (e.g. to genreor domain-speciﬁc idiosyncrasies). 
 2 State of the Art  In this paper, we present an unsupervized segmentation system tested on Mandarin Chinese. Following Harris's Hypothesis in Kempe (1999) and Tanaka-Ishii's (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on diﬀerent corpora available from the Segmentation bake-oﬀ II (Emerson, 2005) and deﬁne a more precise topline for the task using cross-trained supervized system available oﬀ-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 
This paper presents grammar error correction for Japanese particles that uses discriminative sequence conversion, which corrects erroneous particles by substitution, insertion, and deletion. The error correction task is hindered by the difﬁculty of collecting large error corpora. We tackle this problem by using pseudoerror sentences generated automatically. Furthermore, we apply domain adaptation, the pseudo-error sentences are from the source domain, and the real-error sentences are from the target domain. Experiments show that stable improvement is achieved by using domain adaptation.  
We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization. 
With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies 1/ 2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show signiﬁcant improvements over tuning discriminative models on small development sets. 
Parallel data in the domain of interest is the key resource when training a statistical machine translation (SMT) system for a speciﬁc purpose. Since ad-hoc manual translation can represent a signiﬁcant investment in time and money, a prior assesment of the amount of training data required to achieve a satisfactory accuracy level can be very useful. In this work, we show how to predict what the learning curve would look like if we were to manually translate increasing amounts of data. We consider two scenarios, 1) Monolingual samples in the source and target languages are available and 2) An additional small amount of parallel corpus is also available. We propose methods for predicting learning curves in both these scenarios. 
This paper presents a probabilistic framework that combines multiple knowledge sources for Haptic Voice Recognition (HVR), a multimodal input method designed to provide efﬁcient text entry on modern mobile devices. HVR extends the conventional voice input by allowing users to provide complementary partial lexical information via touch input to improve the efﬁciency and accuracy of voice recognition. This paper investigates the use of the initial letter of the words in the utterance as the partial lexical information. In addition to the acoustic and language models used in automatic speech recognition systems, HVR uses the haptic and partial lexical models as additional knowledge sources to reduce the recognition search space and suppress confusions. Experimental results show that both the word error rate and runtime factor can be reduced by a factor of two using HVR. 
We investigate the problem of acoustic modeling in which prior language-speciﬁc knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1% and outperforms a language-mismatched acoustic model. 
Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands for the conventional measures), our FST method shows better performance especially towards the ASR transcription. In addition, we apply the synonyms similarity to expand the FST model. The ﬁnal scoring performance reaches an acceptable level of 0.80 which is only 0.07 lower than the correlation (0.87) between human raters. 
In this paper, we develop an RST-style textlevel discourse parser, based on the HILDA discourse parser (Hernault et al., 2010b). We signiﬁcantly improve its tree-building step by incorporating our own rich linguistic features. We also analyze the difﬁculty of extending traditional sentence-level discourse parsing to text-level parsing by comparing discourseparsing performance under different discourse conditions. 
We describe a discourse annotation scheme for Chinese and report on the preliminary results. Our scheme, inspired by the Penn Discourse TreeBank (PDTB), adopts the lexically grounded approach; at the same time, it makes adaptations based on the linguistic and statistical characteristics of Chinese text. Annotation results show that these adaptations work well in practice. Our scheme, taken together with other PDTB-style schemes (e.g. for English, Turkish, Hindi, and Czech), affords a broader perspective on how the generalized lexically grounded approach can ﬂesh itself out in the context of cross-linguistic annotation of discourse relations. 
One of the key tasks for analyzing conversational data is segmenting it into coherent topic segments. However, most models of topic segmentation ignore the social aspect of conversations, focusing only on the words used. We introduce a hierarchical Bayesian nonparametric model, Speaker Identity for Topic Segmentation (SITS), that discovers (1) the topics used in a conversation, (2) how these topics are shared across conversations, (3) when these topics shift, and (4) a person-speciﬁc tendency to introduce new topics. We evaluate against current unsupervised segmentation models to show that including personspeciﬁc information improves segmentation performance on meeting corpora and on political debates. Moreover, we provide evidence that SITS captures an individual’s tendency to introduce new topics in political contexts, via analysis of the 2008 US presidential debates and the television program Crossﬁre. 
We propose a new approach to characterizing the timeline of a text: temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. We annotate a corpus of children’s stories with temporal dependency trees, achieving agreement (Krippendorff’s Alpha) of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering relation labels. We compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596. Our analysis of the dependency parser errors gives some insights into future research directions. 
Temporal reasoners for document understanding typically assume that a document’s creation date is known. Algorithms to ground relative time expressions and order events often rely on this timestamp to assist the learner. Unfortunately, the timestamp is not always known, particularly on the Web. This paper addresses the task of automatic document timestamping, presenting two new models that incorporate rich linguistic features about time. The ﬁrst is a discriminative classiﬁer with new features extracted from the text’s time expressions (e.g., ‘since 1999’). This model alone improves on previous generative models by 77%. The second model learns probabilistic constraints between time expressions and the unknown document time. Imposing these learned constraints on the discriminative model further improves its accuracy. Finally, we present a new experiment design that facilitates easier comparison by future work. 
Although much work on relation extraction has aimed at obtaining static facts, many of the target relations are actually ﬂuents, as their validity is naturally anchored to a certain time period. This paper proposes a methodological approach to temporally anchored relation extraction. Our proposal performs distant supervised learning to extract a set of relations from a natural language corpus, and anchors each of them to an interval of temporal validity, aggregating evidence from documents supporting the relation. We use a rich graphbased document-level representation to generate novel features for this task. Results show that our implementation for temporal anchoring is able to achieve a 69% of the upper bound performance imposed by the relation extraction step. Compared to the state of the art, the overall system achieves the highest precision reported. 
Learning entailment rules is fundamental in many semantic-inference applications and has been an active ﬁeld of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We ﬁrst identify that entailment graphs exhibit a “tree-like” property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efﬁcient approximation algorithm for learning the graph edges, where each iteration takes linear time. We compare our approximation algorithm to a recently-proposed state-of-the-art exact algorithm and show that it is more efﬁcient and scalable both theoretically and empirically, while its output quality is close to that given by the optimal solution of the exact algorithm. 
Comprehending action preconditions and effects is an essential step in modeling the dynamics of the world. In this paper, we express the semantics of precondition relations extracted from text in terms of planning operations. The challenge of modeling this connection is to ground language at the level of relations. This type of grounding enables us to create high-level plans based on language abstractions. Our model jointly learns to predict precondition relations from text and to perform high-level planning guided by those relations. We implement this idea in the reinforcement learning framework using feedback automatically obtained from plan execution attempts. When applied to a complex virtual world and text describing that world, our relation extraction technique performs on par with a supervised baseline, yielding an F-measure of 66% compared to the baseline’s 65%. Additionally, we show that a high-level planner utilizing these extracted relations signiﬁcantly outperforms a strong, text unaware baseline – successfully completing 80% of planning tasks as compared to 69% for the baseline.1 
Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance. 
When automatically translating from a weakly inﬂected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of ﬁne-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders.  
In this paper we show how to train statistical machine translation systems on reallife tasks using only non-parallel monolingual data from two languages. We present a modiﬁcation of the method shown in (Ravi and Knight, 2011) that is scalable to vocabulary sizes of several thousand words. On the task shown in (Ravi and Knight, 2011) we obtain better results with only 5% of the computational effort when running our method with an n-gram language model. The efﬁciency improvement of our method allows us to run experiments with vocabulary sizes of around 5,000 words, such as a non-parallel version of the VERBMOBIL corpus. We also report results using data from the monolingual French and English GIGAWORD corpora. 
In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efﬁcient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.  
Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difﬁcult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain signiﬁcant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain signiﬁcant speed improvements with both N -best and hill climbing rescoring, and show that up-training leads to WER reduction. 
During early language acquisition, infants must learn both a lexicon and a model of phonetics that explains how lexical items can vary in pronunciation—for instance “the” might be realized as [Di] or [D@]. Previous models of acquisition have generally tackled these problems in isolation, yet behavioral evidence suggests infants acquire lexical and phonetic knowledge simultaneously. We present a Bayesian model that clusters together phonetic variants of the same lexical item while learning both a language model over lexical items and a log-linear model of pronunciation variability based on articulatory features. The model is trained on transcribed surface pronunciations, and learns by bootstrapping, without access to the true lexicon. We test the model using a corpus of child-directed speech with realistic phonetic variation and either gold standard or automatically induced word boundaries. In both cases modeling variability improves the accuracy of the learned lexicon over a system that assumes each lexical item has a unique pronunciation. 
We address the problem of learning the mapping between words and their possible pronunciations in terms of sub-word units. Most previous approaches have involved generative modeling of the distribution of pronunciations, usually trained to maximize likelihood. We propose a discriminative, feature-rich approach using large-margin learning. This approach allows us to optimize an objective closely related to a discriminative task, to incorporate a large number of complex features, and still do inference efﬁciently. We test the approach on the task of lexical access; that is, the prediction of a word given a phonetic transcription. In experiments on a subset of the Switchboard conversational speech corpus, our models thus far improve classiﬁcation error rates from a previously published result of 29.1% to about 15%. We ﬁnd that large-margin approaches outperform conditional random ﬁeld learning, and that the Passive-Aggressive algorithm for largemargin learning is faster to converge than the Pegasos algorithm. 
The integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artiﬁcial context where such expressions have been perfectly pre-identiﬁed. This paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested. Firstly, we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score. However, it has no statistically signiﬁcant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing. Secondly, integrating multiword expressions in the parser grammar followed by a reranker speciﬁc to such expressions slightly improves all evaluation metrics. 
Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-order feature representations for graph-based dependency parsing models using a dependency language model and beam search. The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efﬁciently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features deﬁned over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data. 
We introduce a spectral learning algorithm for latent-variable PCFGs (Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides consistent parameter estimates. 
We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. We empirically analyze the diversity between two representative corpora, i.e. Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature. 
From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline. 
We present a joint model for Chinese word segmentation and new word detection. We present high dimensional new features, including word-based features and enriched edge (label-transition) features, for the joint modeling. As we know, training a word segmentation system on large-scale datasets is already costly. In our case, adding high dimensional new features will further slow down the training speed. To solve this problem, we propose a new training method, adaptive online gradient descent based on feature frequency information, for very fast online training of the parameters, even given large-scale datasets with high dimensional features. Compared with existing training methods, our training method is an order magnitude faster in terms of training time, and can achieve equal or even higher accuracies. The proposed fast training method is a general purpose optimization method, and it is not limited in the speciﬁc task discussed in this paper. 
In this paper, we propose innovative representations for automatic classiﬁcation of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are deﬁned. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture meaningful syntactic/semantic structures, which allows for improving the state-of-the-art. 
Previous research has conﬂicting conclusions on whether word sense disambiguation (WSD) systems can improve information retrieval (IR) performance. In this paper, we propose a method to estimate sense distributions for short queries. Together with the senses predicted for words in documents, we propose a novel approach to incorporate word senses into the language modeling approach to IR and also exploit the integration of synonym relations. Our experimental results on standard TREC collections show that using the word senses tagged by a supervised WSD system, we obtain signiﬁcant improvements over a state-of-the-art IR system. 
This paper addresses the search problem in textual inference, where systems need to infer one piece of text from another. A prominent approach to this task is attempts to transform one text into the other through a sequence of inference-preserving transformations, a.k.a. a proof, while estimating the proof’s validity. This raises a search challenge of ﬁnding the best possible proof. We explore this challenge through a comprehensive investigation of prominent search algorithms and propose two novel algorithmic components speciﬁcally designed for textual inference: a gradient-style evaluation function, and a locallookahead node expansion method. Evaluations, using the open-source system, BIUTEE, show the contribution of these ideas to search efﬁciency and proof quality. 
This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks. 1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al., 2002, 2003, Liang et al., 2006, Blunsom et al., 2008, Chiang et al., 2009, Foster et al, 2010, Xiao et al. 2011). Och (2003) proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al., 2003, Brown et al., 1993). Moreover, the  parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU) (Papineni et al., 2002). Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al., 2006, Chiang et al., 2009). Liang et al. (2006) proposed a large set of lexical and Part-of-Speech features and trained the model weights associated with these features using perceptron. Since many of the reference translations are non-reachable, an empirical local updating strategy had to be devised to fix this problem by picking a pseudo reference. Many such non-desirable heuristics led to moderate gains reported in that work. Chiang et al. (2009) improved a syntactic SMT system by adding as many as ten thousand syntactic features, and used Margin Infused Relaxed Algorithm (MIRA) to train the feature weights. However, the number of parameters in common phrase and lexicon translation models is much larger. In this work, we present a new, highly effective discriminative learning method for phrase and lexicon translation models. The training objective is an expected BLEU score, which is closely linked to translation quality. Further, we apply a Kullback–Leibler (KL) divergence regularization to prevent over-fitting. For effective optimization, we derive updating formulas of growth transformation (GT) for phrase and lexicon translation probabilities. A GT is a transformation of the probabilities that guarantees strict non-decrease of the objective over each GT iteration unless a local maximum is reached. A  292  Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 292–301, Jeju, Republic of Korea, 8-14 July 2012. c 2012 Association for Computational Linguistics  similar GT technique has been successfully used in speech recognition (Gopalakrishnan et al., 1991, Povey, 2004, He et al., 2008). Our work demonstrates that it works with large scale discriminative training of SMT model as well. Our work is based on a phrase-based SMT system. Experiments on the Europarl German-toEnglish dataset show that the proposed method leads to a 1.1 BLEU point improvement over a strong baseline. The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set, where the task is to translate TED talks (www.ted.com). Our experimental results on this open-domain spoken language translation task show that the proposed method leads to significant translation performance improvement over a state-of-the-art baseline, and the system using the proposed method achieved the best single system translation result in the Chineseto-English MT track. 2. Related Work One best known approach in discriminative training for SMT is proposed by Och (2003). In that work, multiple features, most of them are derived from generative models, are incorporated into a log-linear model, and the relative weights of them are tuned discriminatively on a small tuning set. However, in practice, this approach only works with a handful of parameters. More closely related to our work, Liang et al. (2006) proposed a large set of lexical and Part-ofSpeech features in addition to the phrase translation model. Weights of these features are trained using perceptron on a training set of 67K sentences. In that paper, the authors pointed out that forcing the model to update towards the reference translation could be problematic. This is because the hidden structure such as phrase segmentation and alignment could be abused if the system is forced to produce a reference translation. Therefore, instead of pushing the parameter update towards the reference translation (a.k.a. bold updating), the author proposed a local updating strategy where the model parameters are updated towards a pseudo-reference (i.e., the hypothesis in the n-best list that gives the best BLEU score). Experimental results showed that their approach outperformed a baseline by 0.8 BLEU point when using monotonic decoding, but there was no  significant gain over a stronger baseline with a full-distortion model. In our work, we use the expectation of BLEU scores as the objective. This avoids the heuristics of picking the updating reference and therefore gives a more principal way of setting the training objective. As another closely related study, Chiang et al. (2009) incorporated about ten thousand syntactic features in addition to the baseline features. The feature weights are trained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al. (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al., 2008) and lattice-based MERT (Macherey et al., 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al. (2010) proposed a method to train the phrase translation model using ExpectationMaximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on the phrase alignments. To prevent overfitting, the statistics of phrase pairs from a particular sentence was excluded from the phrase table when aligning that sentence. However, as pointed out by Liang et al (2006), the same problem as in the bold updating existed, i.e., forced alignment between a source sentence and its reference translation was tricky, and the proposed alignment was likely to be unreliable. The method presented in this paper is free from this problem. 3. Phrase-based Translation System The translation process of phrase-based SMT can be briefly described in three steps: segment source sentence into a sequence of phrases, translate each  293  source phrase to a target phrase, re-order target phrases into target sentence (Koehn et al., 2003). In decoding, the optimal translation 𝐸 given the source sentence F is obtained according to  𝐸 = argmax 𝑃 𝐸 𝐹  (1)  !  where  
In this paper, we address the issue for learning better translation consensus in machine translation (MT) research, and explore the search of translation consensus from similar, rather than the same, source sentences or their spans. Unlike previous work on this topic, we formulate the problem as structured labeling over a much smaller graph, and we propose a novel structured label propagation for the task. We convert such graph-based translation consensus from similar source strings into useful features both for n-best output reranking and for decoding algorithm. Experimental results show that, our method can significantly improve machine translation performance on both IWSLT and NIST data, compared with a state-ofthe-art baseline. 
Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ℓ0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension eﬃciently for large-scale data (also released as a modiﬁcation to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, signiﬁcant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu). 
We describe a joint model for understanding user actions in natural language utterances. Our multi-layer generative approach uses both labeled and unlabeled utterances to jointly learn aspects regarding utterance’s target domain (e.g. movies), intention (e.g., ﬁnding a movie) along with other semantic units (e.g., movie name). We inject information extracted from unstructured web search query logs as prior information to enhance the generative process of the natural language utterance understanding model. Using utterances from ﬁve domains, our approach shows up to 4.5% improvement on domain and dialog act performance over cascaded approach in which each semantic component is learned sequentially and a supervised joint learning model (which requires fully labeled data). 
Aspect extraction is a central problem in sentiment analysis. Current methods either extract aspects without categorizing them, or extract and categorize them using unsupervised topic modeling. By categorizing, we mean the synonymous aspects should be clustered into the same category. In this paper, we solve the problem in a different setting where the user provides some seed words for a few aspect categories and the model extracts and clusters aspect terms into categories simultaneously. This setting is important because categorizing aspects is a subjective task. For different application purposes, different categorizations may be needed. Some form of user guidance is desired. In this paper, we propose two statistical models to solve this seeded problem, which aim to discover exactly what the user wants. Our experimental results show that the two proposed models are indeed able to perform the task effectively. 
Most information extraction (IE) systems identify facts that are explicitly stated in text. However, in natural language, some facts are implicit, and identifying them requires “reading between the lines”. Human readers naturally use common sense knowledge to infer such implicit information from the explicitly stated facts. We propose an approach that uses Bayesian Logic Programs (BLPs), a statistical relational model combining ﬁrstorder logic and Bayesian networks, to infer additional implicit information from extracted facts. It involves learning uncertain commonsense knowledge (in the form of probabilistic ﬁrst-order rules) from natural language text by mining a large corpus of automatically extracted facts. These rules are then used to derive additional facts from extracted information using BLP inference. Experimental evaluation on a benchmark data set for machine reading demonstrates the efﬁcacy of our approach. 
We present a holistic data-driven approach to image description generation, exploiting the vast amount of (noisy) parallel image data and associated natural language descriptions available on the web. More speciﬁcally, given a query image, we retrieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation process as constraint optimization problems, collectively incorporating multiple interconnected aspects of language composition for content planning, surface realization and discourse structure. Evaluation by human annotators indicates that our ﬁnal system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines. 
This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our approach is to reduce the tasks of content selection (“what to say”) and surface realization (“how to say”) into a common parsing problem. We deﬁne a probabilistic context-free grammar that describes the structure of the input (a corpus of database records and text describing some of them) and represent it compactly as a weighted hypergraph. The hypergraph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for ﬁnding the best scoring derivation and generating in this setting. Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study. 
Methods that measure compatibility between mention pairs are currently the dominant approach to coreference. However, they suffer from a number of drawbacks including difﬁculties scaling to large numbers of mentions and limited representational power. As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU. 
To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Speciﬁcally, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give signiﬁcant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3), resulting in the best results reported to date for the end-to-end task of coreference resolution. 
The rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups. Many of these groups discuss ideological and political topics. It is not uncommon that the participants in such discussions split into two or more subgroups. The members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups. In this paper, we propose an unsupervised approach for automatically detecting discussant subgroups in online communities. We analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic. We use attitude predictions to construct an attitude vector for each discussant. We use clustering techniques to cluster these vectors and, hence, determine the subgroup membership of each participant. We compare our methods to text clustering and other baselines, and show that our method achieves promising results.  
Extracting sentiment and topic lexicons is important for opinion mining. Previous works have showed that supervised learning methods are superior for this task. However, the performance of supervised methods highly relies on manually labeled training data. In this paper, we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. The framework is twofold. In the ﬁrst step, we generate a few high-conﬁdence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation. 
We present a novel approach for building verb subcategorization lexicons using a simple graphical model. In contrast to previous methods, we show how the model can be trained without parsed input or a predeﬁned subcategorization frame inventory. Our method outperforms the state-of-the-art on a verb clustering task, and is easily trained on arbitrary domains. This quantitative evaluation is complemented by a qualitative discussion of verbs and their frames. We discuss the advantages of graphical models for this task, in particular the ease of integrating semantic information about verbs and arguments in a principled fashion. We conclude with future work to augment the approach. 
Learning a semantic lexicon is often an important ﬁrst step in building a system that learns to interpret the meaning of natural language. It is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context. Recent work by Chen and Mooney (2011) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs. While the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets. In this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the stateof-the-art results. We show that by changing the grammar of the formal meaning representation language and training on additional data collected from Amazon’s Mechanical Turk we can further improve the results. We also include experimental results on a Chinese translation of the training data to demonstrate the generality of our approach. 
We propose Symbol-Reﬁned Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be reﬁned (subcategorized) to ﬁt the training data. We aim to provide a uniﬁed model where TSG rules and symbol reﬁnement are learned from training data in a fully automatic and consistent fashion. We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a ﬁne-grained SR-TSG to simpler CFG rules, and develop an efﬁcient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers. 
Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval. In this paper, we propose a new class of kernel functions, referred to as string re-writing kernel, to address the problem. A string re-writing kernel measures the similarity between two pairs of strings, each pair representing re-writing of a string. It can capture the lexical and structural similarity between two pairs of sentences without the need of constructing syntactic trees. We further propose an instance of string rewriting kernel which can be computed efﬁciently. Experimental results on benchmark datasets show that our method can achieve better results than state-of-the-art methods on two sentence re-writing learning tasks: paraphrase identiﬁcation and recognizing textual entailment. 
To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach signiﬁcantly outperforms the baseline system. 
This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much ﬁner degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks. 
 Many semantic parsing models use tree transformations to map between natural language and meaning representation. However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata. This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions. In particular, this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers. 
Optimising for one grammatical representation, but evaluating over a different one is a particular challenge for parsers and n-best CCG parsing. We ﬁnd that this mismatch causes many n-best CCG parses to be semantically equivalent, and describe a hashing technique that eliminates this problem, improving oracle n-best F-score by 0.7% and reranking accuracy by 0.4%. We also present a comprehensive analysis of errors made by the C&C CCG parser, providing the ﬁrst breakdown of the impact of implementation decisions, such as supertagging, on parsing accuracy. 
Recently, it was shown (KUHLMANN, SATTA: Tree-adjoining grammars are not closed under strong lexicalization. Comput. Linguist., 2012) that ﬁnitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol. A more powerful model, the simple context-free tree grammar, admits such a normal form. It can be effectively constructed and the maximal rank of the nonterminals only increases by 1. Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves. 
As one of the most popular micro-blogging services, Twitter attracts millions of users, producing millions of tweets daily. Shared information through this service spreads faster than would have been possible with traditional sources, however the proliferation of user-generation content poses challenges to browsing and ﬁnding valuable information. In this paper we propose a graph-theoretic model for tweet recommendation that presents users with items they may have an interest in. Our model ranks tweets and their authors simultaneously using several networks: the social network connecting the users, the network connecting the tweets, and a third network that ties the two together. Tweet and author entities are ranked following a co-ranking algorithm based on the intuition that that there is a mutually reinforcing relationship between tweets and their authors that could be reﬂected in the rankings. We show that this framework can be parametrized to take into account user preferences, the popularity of tweets and their authors, and diversity. Experimental evaluation on a large dataset shows that our model outperforms competitive approaches by a large margin. 
Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. We study the problem of named entity normalization (NEN) for tweets. Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. Particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related words are mentions of the same entity. We evaluate our method on a manually annotated data set, and show that our method outperforms the baseline that handles these two tasks separately, boosting the F1 from 80.2% to 83.6% for NER, and the Accuracy from 79.4% to 82.6% for NEN, respectively. 
Microblogs such as Twitter reﬂect the general public’s reactions to major events. Bursty topics from microblogs reveal what events have attracted the most online attention. Although bursty event detection from text streams has been studied before, previous work may not be suitable for microblogs because compared with other text streams such as news articles and scientiﬁc publications, microblog posts are particularly diverse and noisy. To ﬁnd topics that have bursty patterns on microblogs, we propose a topic model that simultaneously captures two observations: (1) posts published around the same time are more likely to have the same topic, and (2) posts published by the same user are more likely to have the same topic. The former helps ﬁnd eventdriven posts while the latter helps identify and ﬁlter out “personal” posts. Our experiments on a large Twitter dataset show that there are more meaningful and unique bursty topics in the top-ranked results returned by our model than an LDA baseline and two degenerate variations of our model. We also show some case studies that demonstrate the importance of considering both the temporal information and users’ personal interests for bursty topic detection from microblogs. 
There are a growing number of popular web sites where users submit and review instructions for completing tasks as varied as building a table and baking a pie. In addition to providing their subjective evaluation, reviewers often provide actionable reﬁnements. These reﬁnements clarify, correct, improve, or provide alternatives to the original instructions. However, identifying and reading all relevant reviews is a daunting task for a user. In this paper, we propose a generative model that jointly identiﬁes user-proposed reﬁnements in instruction reviews at multiple granularities, and aligns them to the appropriate steps in the original instructions. Labeled data is not readily available for these tasks, so we focus on the unsupervised setting. In experiments in the recipe domain, our model provides 90.1% F1 for predicting reﬁnements at the review level, and 77.0% F1 for predicting reﬁnement segments within reviews. 
Online forums are becoming a popular resource in the state of the art question answering (QA) systems. Because of its nature as an online community, it contains more updated knowledge than other places. However, going through tedious and redundant posts to look for answers could be very time consuming. Most prior work focused on extracting only question answering sentences from user conversations. In this paper, we introduce the task of sentence dependency tagging. Finding dependency structure can not only help ﬁnd answer quickly but also allow users to trace back how the answer is concluded through user conversations. We use linear-chain conditional random ﬁelds (CRF) for sentence type tagging, and a 2D CRF to label the dependency relation between sentences. Our experimental results show that our proposed approach performs well for sentence dependency tagging. This dependency information can beneﬁt other tasks such as thread ranking and answer summarization in online forums. 
We predict entity type distributions in Web search queries via probabilistic inference in graphical models that capture how entitybearing queries are generated. We jointly model the interplay between latent user intents that govern queries and unobserved entity types, leveraging observed signals from query formulations and document clicks. We apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base. Our models are efﬁciently trained using maximum likelihood estimation over millions of real-world Web search queries. We show that modeling user intent signiﬁcantly improves entity type resolution for head queries over the state of the art, on several metrics, without degradation in tail query performance. 
The amount of labeled sentiment data in English is much larger than that in other languages. Such a disproportion arouse interest in cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) using labeled data in the source language (e.g. English). Most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language. This approach suffers from the limited coverage of vocabulary in the machine translation results. In this paper, we propose a generative cross-lingual mixture model (CLMM) to leverage unlabeled bilingual parallel data. By fitting parameters to maximize the likelihood of the bilingual parallel data, the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage significantly. Experiments on multiple data sets show that CLMM is consistently effective in two settings: (1) labeled data in the target language are unavailable; and (2) labeled data in the target language are also available. 
We present a novel answer summarization method for community Question Answering services (cQAs) to address the problem of “incomplete answer”, i.e., the “best answer” of a complex multi-sentence question misses valuable information that is contained in other answers. In order to automatically generate a novel and non-redundant community answer summary, we segment the complex original multi-sentence question into several sub questions and then propose a general Conditional Random Field (CRF) based answer summary method with group L1 regularization. Various textual and non-textual QA features are explored. Speciﬁcally, we explore four different types of contextual factors, namely, the information novelty and non-redundancy modeling for local and non-local sentence interactions under question segmentation. To further unleash the potential of the abundant cQA features, we introduce the group L1 regularization for feature learning. Experimental results on a Yahoo! Answers dataset show that our proposed method signiﬁcantly outperforms state-of-the-art methods on cQA summarization task. 
In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data. 
This paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from ﬁve Conan Doyle novels. We ﬁnd that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research. 
Sequential modeling has been widely used in a variety of important applications including named entity recognition and shallow parsing. However, as more and more real time large-scale tagging applications arise, decoding speed has become a bottleneck for existing sequential tagging algorithms. In this paper we propose 1-best A*, 1-best iterative A*, k-best A* and k-best iterative Viterbi A* algorithms for sequential decoding. We show the efﬁciency of these proposed algorithms for ﬁve NLP tagging tasks. In particular, we show that iterative Viterbi A* decoding can be several times or orders of magnitude faster than the state-of-the-art algorithm for tagging tasks with a large number of labels. This algorithm makes real-time large-scale tagging applications with thousands of labels feasible. 
Bootstrapping a classiﬁer from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them. This paper introduces a novel variant of the Yarowsky algorithm based on this view. It is a bootstrapping learning method which uses a graph propagation algorithm with a well deﬁned objective function. The experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets. 
We present a novel algorithm for multilingual dependency parsing that uses annotations from a diverse set of source languages to parse a new unannotated language. Our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing resource-rich languages. The algorithm learns which aspects of the source languages are relevant for the target language and ties model parameters accordingly. The model factorizes the process of generating a dependency tree into two steps: selection of syntactic dependents and their ordering. Being largely languageuniversal, the selection component is learned in a supervised fashion from all the training languages. In contrast, the ordering decisions are only inﬂuenced by languages with similar properties. We systematically model this cross-lingual sharing using typological features. In our experiments, the model consistently outperforms a state-of-the-art multilingual parser. The largest improvement is achieved on the non Indo-European languages yielding a gain of 14.4%.1 
Metalanguage is an essential linguistic mechanism which allows us to communicate explicit information about language itself. However, it has been underexamined in research in language technologies, to the detriment of the performance of systems that could exploit it. This paper describes the creation of the first tagged and delineated corpus of English metalanguage, accompanied by an explicit definition and a rubric for identifying the phenomenon in text. This resource will provide a basis for further studies of metalanguage and enable its utilization in language technologies. 
We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. Speciﬁcally, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model. At inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. We propose a simple approximate learning algorithm for our set-up which results in efﬁcient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences. 
This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 
The language MIX consists of all strings over the three-letter alphabet {a, b, c} that contain an equal number of occurrences of each letter. We prove Joshi’s (1985) conjecture that MIX is not a tree-adjoining language. 
We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for parsing. Several types of transformation patterns (TP) are designed to capture the systematic annotation inconsistencies among different treebanks. Based on such TPs, we design quasisynchronous grammar features to augment the baseline parsing models. Our approach can signiﬁcantly advance the state-of-the-art parsing accuracy on two widely used target treebanks (Penn Chinese Treebank 5.1 and 6.0) using the Chinese Dependency Treebank as the source treebank. The improvements are respectively 1.37% and 1.10% with automatic part-of-speech tags. Moreover, an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion. 
We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, ﬁrstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, ﬁnding that it outperforms a simple agglomerative clustering approach and previous work. 
In this paper we propose a method to automatically label multi-lingual data with named entity tags. We build on prior work utilizing Wikipedia metadata and show how to effectively combine the weak annotations stemming from Wikipedia metadata with information obtained through English-foreign language parallel Wikipedia sentences. The combination is achieved using a novel semi-CRF model for foreign sentence tagging in the context of a parallel English sentence. The model outperforms both standard annotation projection methods and methods based solely on Wikipedia metadata. 
In this paper, we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined. We describe all the linguistic resources and natural language processing techniques that we have exploited for this task. Then, we analyze the performance of the system that we have developed. The empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation of the naming process. 
To discover relation types from text, most methods cluster shallow or syntactic patterns of relation mentions, but consider only one possible sense per pattern. In practice this assumption is often violated. In this paper we overcome this issue by inducing clusters of pattern senses from feature representations of patterns. In particular, we employ a topic model to partition entity pairs associated with patterns into sense clusters using local and global features. We merge these sense clusters into semantic relations using hierarchical agglomerative clustering. We compare against several baselines: a generative latent-variable model, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial. 
In relation extraction, distant supervision seeks to extract relations between entities from text by using a knowledge base, such as Freebase, as a source of supervision. When a sentence and a knowledge base refer to the same entity pair, this approach heuristically labels the sentence with the corresponding relation in the knowledge base. However, this heuristic can fail with the result that some sentences are labeled wrongly. This noisy labeled data causes poor extraction performance. In this paper, we propose a method to reduce the number of wrong labels. We present a novel generative model that directly models the heuristic labeling process of distant supervision. The model predicts whether assigned labels are correct or wrong via its hidden variables. Our experimental results show that this model detected wrong labels with higher performance than baseline methods. In the experiment, we also found that our wrong label reduction boosted the performance of relation extraction. 
We present an approach for detecting salient (important) dates in texts in order to automatically build event timelines from a search query (e.g. the name of an event or person, etc.). This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). In order to extract salient dates that warrant inclusion in an event timeline, we ﬁrst recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. We focused only on extracting the dates and not the events to which they are related. 
We propose a latent variable model to enhance historical analysis of large corpora. This work extends prior work in topic modelling by incorporating metadata, and the interactions between the components in metadata, in a general way. To test this, we collect a corpus of slavery-related United States property law judgements sampled from the years 1730 to 1866. We study the language use in these legal cases, with a special focus on shifts in opinions on controversial topics across different regions. Because this is a longitudinal data set, we are also interested in understanding how these opinions change over the course of decades. We show that the joint learning scheme of our sparse mixed-effects model improves on other state-of-the-art generative and discriminative models on the region and time period identiﬁcation tasks. Experiments show that our sparse mixed-effects model is more accurate quantitatively and qualitatively interesting, and that these improvements are robust across different parameter settings. 
Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model signiﬁcantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.  by these probabilities. However, the state-of-theart SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word by word. Since a synchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic model directly at the rule level rather than the word level. Consequently, we propose a topic similarity model for hierarchical phrase-based translation (Chiang, 2007), where each synchronous rule is associated with a topic distribution. In particular, • Given a document to be translated, we calculate the topic similarity between a rule and the document based on their topic distributions. We augment the hierarchical phrase-based system by integrating the proposed topic similarity model as a new feature (Section 3.1).  
In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role of category relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classiﬁers and signiﬁcantly outperform the state-of-the-art. 
Prepositions and conjunctions are two of the largest remaining bottlenecks in parsing. Across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications. Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution. As lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem. By including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve a new state-of-the-art for English dependencies with 93.55% correct attachments on the current standard. Furthermore, conjunctions are attached with an accuracy of 90.8%, and prepositions with an accuracy of 87.4%. 
Treebanks are not large enough to reliably model precise lexical phenomena. This deﬁciency provokes attachment errors in the parsers trained on such data. We propose in this paper to compute lexical afﬁnities, on large corpora, for speciﬁc lexico-syntactic conﬁgurations that are hard to disambiguate and introduce the new information in a parser. Experiments on the French Treebank showed a relative decrease of the error rate of 7.1% Labeled Accuracy Score yielding the best parsing results on this treebank.  is, as mentioned above, a better modeling of bilexical dependencies and the second is a method to adapt a parser to new domains. The paper is organized as follows. Section 2 reviews some work on the same topic and highlights their differences with ours. In section 3, we describe the parser that we use in our experiments and give a detailed description of the frequent attachment errors. Section 4 describes how lexical afﬁnities between lemmas are calculated and their impact is then evaluated with respect to the attachment errors made by the parser. Section 5 describes three ways to integrate the lexical afﬁnities in the parser and reports the results obtained with the three methods.  
The Chinese comma signals the boundary of discourse units and also anchors discourse relations between adjacent text spans. In this work, we propose a discourse structureoriented classiﬁcation of the comma that can be automatically extracted from the Chinese Treebank based on syntactic patterns. We then experimented with two supervised learning methods that automatically disambiguate the Chinese comma based on this classiﬁcation. The ﬁrst method integrates comma classiﬁcation into parsing, and the second method adopts a “post-processing” approach that extracts features from automatic parses to train a classiﬁer. The experimental results show that the second approach compares favorably against the ﬁrst approach. 
Previous work on classifying information status (Nissim, 2006; Rahman and Ng, 2011) is restricted to coarse-grained classiﬁcation and focuses on conversational dialogue. We here introduce the task of classifying ﬁnegrained information status and work on written text. We add a ﬁne-grained information status layer to the Wall Street Journal portion of the OntoNotes corpus. We claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions. Our approach strongly outperforms reimplementations of previous work. 
Large e-commerce enterprises feature millions of items entered daily by a large variety of sellers. While some sellers provide rich, structured descriptions of their items, a vast majority of them provide unstructured natural language descriptions. In the paper we present a 2 steps method for structuring items into descriptive properties. The ﬁrst step consists in unsupervised property discovery and extraction. The second step involves supervised property synonym discovery using a maximum entropy based clustering algorithm. We evaluate our method on a year worth of ecommerce data and show that it achieves excellent precision with good recall. 
The named entity disambiguation task is to resolve the many-to-many correspondence between ambiguous names and the unique realworld entity. This task can be modeled as a classiﬁcation problem, provided that positive and negative examples are available for learning binary classiﬁers. High-quality senseannotated data, however, are hard to be obtained in streaming environments, since the training corpus would have to be constantly updated in order to accomodate the fresh data coming on the stream. On the other hand, few positive examples plus large amounts of unlabeled data may be easily acquired. Producing binary classiﬁers directly from this data, however, leads to poor disambiguation performance. Thus, we propose to enhance the quality of the classiﬁers using ﬁner-grained variations of the well-known ExpectationMaximization (EM) algorithm. We conducted a systematic evaluation using Twitter streaming data and the results show that our classiﬁers are extremely effective, providing improvements ranging from 1% to 20%, when compared to the current state-of-the-art biased SVMs, being more than 120 times faster. 
Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To ﬁll this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically signiﬁcant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically signiﬁcant, but lower, impact on precision and recall. 
This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random ﬁelds for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the success of our task. Our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches. 
This paper presents a joint model for template ﬁlling, where the goal is to automatically specify the ﬁelds of target relations such as seminar announcements or corporate acquisition events. The approach models mention detection, uniﬁcation and ﬁeld extraction in a ﬂexible, feature-rich model that allows for joint modeling of interdependencies at all levels and across ﬁelds. Such an approach can, for example, learn likely event durations and the fact that start times should come before end times. While the joint inference space is large, we demonstrate effective learning with a Perceptron-style approach that uses simple, greedy beam decoding. Empirical results in two benchmark domains demonstrate consistently strong performance on both mention detection and template ﬁlling tasks.  Date Start Time Location Speaker Title End Time  5/5/1995 3:30PM Wean Hall 5409 Raj Reddy Some Necessary Conditions for a Good User Interface –  Figure 1: An example email and its template. Field mentions are highlighted in the text, grouped by color.  
We present a novel approach to the automatic acquisition of a Verbnet like classiﬁcation of French verbs which involves the use (i) of a neural clustering method which associates clusters with features, (ii) of several supervised and unsupervised evaluation metrics and (iii) of various existing syntactic and semantic lexical resources. We evaluate our approach on an established test set and show that it outperforms previous related work with an Fmeasure of 0.70. 
Sentence Similarity is the process of computing a similarity score between two sentences. Previous sentence similarity work ﬁnds that latent semantics approaches to the problem do not perform well due to insufﬁcient information in single sentences. In this paper, we show that by carefully handling words that are not in the sentences (missing words), we can train a reliable latent variable model on sentences. In the process, we propose a new evaluation framework for sentence similarity: Concept Deﬁnition Retrieval. The new framework allows for large scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show signiﬁcant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity. 
Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1 
This paper uses an unsupervised model of grounded language acquisition to study the role that social cues play in language acquisition. The input to the model consists of (orthographically transcribed) child-directed utterances accompanied by the set of objects present in the non-linguistic context. Each object is annotated by social cues, indicating e.g., whether the caregiver is looking at or touching the object. We show how to model the task of inferring which objects are being talked about (and which words refer to which objects) as standard grammatical inference, and describe PCFG-based unigram models and adaptor grammar-based collocation models for the task. Exploiting social cues improves the performance of all models. Our models learn the relative importance of each social cue jointly with word-object mappings and collocation structure, consistent with the idea that children could discover the importance of particular social information sources during word learning. 
Understanding the ways in which information achieves widespread public awareness is a research question of signiﬁcant interest. We consider whether, and how, the way in which the information is phrased — the choice of words and sentence structure — can affect this process. To this end, we develop an analysis framework and build a corpus of movie quotes, annotated with memorability information, in which we are able to control for both the speaker and the setting of the quotes. We ﬁnd that there are signiﬁcant differences between memorable and non-memorable quotes in several key dimensions, even after controlling for situational and contextual factors. One is lexical distinctiveness: in aggregate, memorable quotes use less common word choices, but at the same time are built upon a scaffolding of common syntactic patterns. Another is that memorable quotes tend to be more general in ways that make them easy to apply in new contexts — that is, more portable. We also show how the concept of “memorable language” can be extended across domains. 
Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can signiﬁcantly outperform the baseline phrasebased SMT system. 
In this work, we introduce the TESLACELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis – Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation. For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB signiﬁcantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. 
Many machine translation (MT) evaluation metrics have been shown to correlate better with human judgment than BLEU. In principle, tuning on these metrics should yield better systems than tuning on BLEU. However, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning. This paper presents PORT 1 , a new MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems. PORT does not require external resources and is quick to compute. It has a better correlation with human judgment than BLEU. We compare PORT-tuned MT systems to BLEU-tuned baselines in five experimental conditions involving four language pairs. PORT tuning achieves consistently better performance than BLEU tuning, according to four automated metrics (including BLEU) and to human evaluation: in comparisons of outputs from 300 source sentences, human judges preferred the PORT-tuned output 45.3% of the time (vs. 32.7% BLEU tuning preferences and 22.0% ties). 
We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar deﬁned by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks. 
We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and ﬁnd that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show ﬂuency improvements in a preliminary machine translation experiment. 
The problem addressed in this paper is to segment a given multilingual document into segments for each language and then identify the language of each segment. The problem was motivated by an attempt to collect a large amount of linguistic data for non-major languages from the web. The problem is formulated in terms of obtaining the minimum description length of a text, and the proposed solution ﬁnds the segments and their languages through dynamic programming. Empirical results demonstrating the potential of this approach are presented for experiments using texts taken from the Universal Declaration of Human Rights and Wikipedia, covering more than 200 languages. 
In recent years there has been a growing interest in crowdsourcing methodologies to be used in experimental research for NLP tasks. In particular, evaluation of systems and theories about persuasion is difﬁcult to accommodate within existing frameworks. In this paper we present a new cheap and fast methodology that allows fast experiment building and evaluation with fully-automated analysis at a low cost. The central idea is exploiting existing commercial tools for advertising on the web, such as Google AdWords, to measure message impact in an ecological setting. The paper includes a description of the approach, tips for how to use AdWords for scientiﬁc research, and results of pilot experiments on the impact of affective text variations which conﬁrm the effectiveness of the approach. 
Polarity classiﬁcation of words is important for applications such as Opinion Mining and Sentiment Analysis. A number of sentiment word/sense dictionaries have been manually or (semi)automatically constructed. The dictionaries have substantial inaccuracies. Besides obvious instances, where the same word appears with different polarities in different dictionaries, the dictionaries exhibit complex cases, which cannot be detected by mere manual inspection. We introduce the concept of polarity consistency of words/senses in sentiment dictionaries in this paper. We show that the consistency problem is NP-complete. We reduce the polarity consistency problem to the satisﬁability problem and utilize a fast SAT solver to detect inconsistencies in a sentiment dictionary. We perform experiments on four sentiment dictionaries and WordNet. 
An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read ﬂuently as a whole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show signiﬁcantly improved performance over AESOP 2011 submitted metrics. 
In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simpliﬁcation results to state-of-the-art systems, while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simpliﬁcation systems. 
All types of part-of-speech (POS) tagging errors have been equally treated by existing taggers. However, the errors are not equally important, since some errors affect the performance of subsequent natural language processing (NLP) tasks seriously while others do not. This paper aims to minimize these serious errors while retaining the overall performance of POS tagging. Two gradient loss functions are proposed to reﬂect the different types of errors. They are designed to assign a larger cost to serious errors and a smaller one to minor errors. Through a set of POS tagging experiments, it is shown that the classiﬁer trained with the proposed loss functions reduces serious errors compared to state-of-the-art POS taggers. In addition, the experimental result on text chunking shows that fewer serious errors help to improve the performance of subsequent NLP tasks. 
Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques. A major challenge facing this task is the system coverage, i.e., for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates. In this paper, we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. The system was evaluated on both word- and messagelevel using four SMS and Twitter data sets. Results show that our system achieves over 90% word-coverage across all data sets (a 10% absolute increase compared to state-ofthe-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach. 
We propose the ﬁrst joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efﬁcient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved signiﬁcantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models. 
We show for both English POS tagging and Chinese word segmentation that with proper representation, large number of deterministic constraints can be learned from training examples, and these are useful in constraining probabilistic inference. For tagging, learned constraints are directly used to constrain Viterbi decoding. For segmentation, character-based tagging constraints can be learned with the same templates. However, they are better applied to a word-based model, thus an integer linear programming (ILP) formulation is proposed. For both problems, the corresponding constrained solutions have advantages in both efﬁciency and accuracy. 
This paper proposed an integrated approach for Cross-Language Information Retrieval (CLIR), which integrated with four statistical models: Translation model, Query generation model, Document retrieval model and Length Filter model. Given a certain document in the source language, it will be translated into the target language of the statistical machine translation model. The query generation model then selects the most relevant words in the translated version of the document as a query. Instead of retrieving all the target documents with the query, the length-based model can help to filter out a large amount of irrelevant candidates according to their length information. Finally, the left documents in the target language are scored by the document searching model, which mainly computes the similarities between query and document. Different from the traditional parallel corpora-based model which relies on IBM algorithm, we divided our CLIR model into four independent parts but all work together to deal with the term disambiguation, query generation and document retrieval. Besides, the TQDL method can efficiently solve the problem of translation ambiguity and query expansion for disambiguation, which are the big issues in Cross-Language Information Retrieval. Another contribution is the length filter, which are trained from a parallel corpus according to the ratio of length between two languages. This can not only improve the recall value due to filtering out lots of useless documents dynamically, but also increase the efficiency in a smaller search space. Therefore, the precision can be improved but not at the cost of recall.  ∗ Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S. A. R., China E-mail: vincentwang0229@hotmail.com The author for correspondence is Long-Yue Wang.  16  Long-Yue WANG et al.  In order to evaluate the retrieval performance of the proposed model on cross-languages document retrieval, a number of experiments have been conducted on different settings. Firstly, the Europarl corpus which is the collection of parallel texts in 11 languages from the proceedings of the European Parliament was used for evaluation. And we tested the models extensively to the case that: the lengths of texts are uneven and some of them may have similar contents under the same topic, because it is hard to be distinguished and make full use of the resources. After comparing different strategies, the experimental results show a significant performance of the method. The precision is normally above 90% by using a larger query size. The length-based filter plays a very important role in improving the F-measure and optimizing efficiency. This fully illustrates the discrimination power of the proposed method. It is of a great significance to both cross-language searching on the Internet and the parallel corpus producing for statistical machine translation systems. In the future work, the TQDL system will be evaluated for Chinese language, which is a big changing and more meaningful to CLIR. Keywords: Cross-Language Document Retrieval, Statistical Machine Translation, TF-IDF, Document Translation-Based, Length-Based Filter. 1. Introduction With the flourishing development of the Internet, the amount of information from a variety of domains is rising dramatically. Especially after the advent of the World Wide Web (WWW) in the 1900s, the amount of online information from the government, scientific and business communities has risen dramatically. Although much word has been done to develop effective and efficient retrieval systems for monolingual resources, the diversity and the explosive growth of information in different languages drove a great need for information retrieval that could cross language boundaries (Ballesteros et al., 1988). The issues of CLIR have been discussed for several decades. Its task addresses a situation in which a user tries to search a set of documents written in one language using a query in a different language (Kishida, 2005). It is of great significance, allowing people access information resources written in non-native languages and aligning documents for statistical machine translation (SMT) systems, of which quality is heavily dependent upon the amount of parallel sentences used in constructing the system. In this paper, we focus on the problems of translation ambiguity, query generation and searching score which are keys to the retrieval performance. First of all, in order to increase the probability that the best translation can be selected from multiple ones, which occurs in the  TQDL: Integrated Models for Cross-Language Document Retrieval  17  target documents, the context and the most likely probability of the whole sentence should be considered. So we apply document translation approach using SMT model instead of query translation, although the latter one may require fewer computational resources. After the source documents are translated into the target language, the problem is transformed from bilingual environment to monolingual one, where conventional IR techniques can be used for document retrieval. Secondly, some terms in a certain document will be selected as query, which can distinguish the document from others. However, some of the words occur too frequently to be useful, which cannot distinguish target documents. This mostly includes two cases: one is that the word frequency is high in all the documents of a set, which is usually classified as stop word; the other one is that the frequency is moderate in several documents of a set. These words are poor in the ability of distinguishing documents. Thus, the query generation model should pick the words that occur more frequently in a certain document while less frequently in other documents. Finally, the document searching model evaluates the similarity between the query and each document. This model should give a higher score to the target document which covers the most relevant words in the given query. However, another problem is that word overlap between a query and a wrong document is more probable when the document and the query are expressed in the same language. For example, Document A is larger and contains another smaller document B. So the retrieval system would be confused with a query including the information of B. In order to solve this problem, the length ratio of a language pair is considered. As the search space is reduced, both the speed efficiency and the recall value will be improved clearly. There are two cases to be considered when we investigated the method. In one case, the lengths of documents are uneven, which are hard to balance the scores between large and small documents. In the other case, the contents of the documents are very similar, which are not easy to distinguish for retrieval. The results of experiments reveal that the proposed model shows a very good performance in dealing with both cases. The paper is organized as follows. The related works are reviewed and discussed in Section 2. The proposed CLIR approach based on statistical models is described in Section 3. The resources and configurations of experiments for evaluating the system are detailed in Section 4. Results, discussion and comparison between different strategies are given in Section 5 followed by a conclusion and future improvements to end the paper. 2. Related Work The issues of CLIR have been discussed from different perspectives for several decades. In this section, we briefly describe some related methods. From a statistical perspective, the CLIR problem can be treated as document alignment. Given a set of parallel documents, the alignment that maximizes the probability over all  18  Long-Yue WANG et al.  possible alignments is retrieved (Gale & Church, 1991) as follows:  arg max Pr( A |  D ,D )  s  t  ≈  ∏ arg max Pr(L  A ( Ls ⇔ Lt )∈A  s  ↔  L t  |  LL) st  (1)  where A is an alignment, Ds and Dt are the source and target documents, respectively L1 and L2 are the documents of two languages, Ls↔Lt is an individual aligned pairs, an alignment A is a set consisting of Ls↔Lt pairs.  On the matching strategies for CLIR, query translation is most widely used method due  to its tractability (Gao et al., 2001). However, it is relatively difficult to resolve the problem of  term ambiguity because “queries are often short and short queries provide little context for  disambiguation” (Oard & Diekema, 1998). Hence, some researchers have used document  translation method as the opposite strategies to improve translation quality, since more varied  context within each document is available for translation (Braschler & Schauble, 2001; Franz  et al., 1999).  However, another problem introduced based on this approach is word (term) disambiguation, because a word may have multiple possible translations (Oard & Diekema, 1998). Significant efforts have been devoted to this problem. Davis and Ogden (1997) applied a part-of-speech (POS) method which requires POS tagging software for both languages. Marcello et al. presented a novel statistical method to score and rank the target documents by integrating probabilities computed by query-translation model and query-document model (Federico & Bertoldi, 2002). However, this approach cannot aim at describing how users actually create queries which have a key effect on the retrieval performance. Due to the availability of parallel corpora in multiple languages, some authors have tried to extract beneficial information for CLIR by using SMT techniques. Sánchez-Martínez et al. (Sánchez-Martínez & Carrasco, 2011) applied SMT technology to generate and translate queries in order to retrieve long documents.  Some researchers like Marcello, Sánchez-Martínez et al. have attempted to estimate  translation probability from a parallel corpus according to a well-known algorithm developed  by IBM (Brown et al., 1993). The algorithm can automatically generate a bilingual term list  with a set of probabilities that a term is translated into equivalents in another language from a  set of sentence alignments included in a parallel corpus. The IBM Model 1 is the simplest  among the five models and often used for CLIR. The fundamental idea of the Model 1 is to  estimate each translation probability so that the probability represented is maximized  ∏ ∑ P(t | s) = ε m (l + 1)m j=1  l P(t j | si ) i=0  (2)  where t is a sequence of terms t1, …, tm in the target language, s is a sequence of terms s1, …, sl in the source language, P(tj|si) is the translation probability, and Ɛ is a parameter (Ɛ =P(m|e)),  TQDL: Integrated Models for Cross-Language Document Retrieval  19  where e is target language and m is the length of source language). Eq. (2) tries to balance the probability of translation, and the query selection, in which problem still exists: it tends to select the terms consisting of more words as query because of its less frequency, while cutting the length of terms may affect the quality of translation. Besides, the IBM model 1 only proposes translations word-by-word and ignores the context words in the query. This observation suggests that a disambiguation process can be added to select the correct translation words (Oard & Diekema, 1998). However, in our method, the conflict can be resolved through contexts.  If translated sentences share cognates, then the character lengths of those cognates are correlated (Yang & Li, 2004). Brown et al. (1991) and Gale and Church (1991) have developed the models based on relationship between the lengths of sentences that are mutual translations. Although it has been suggested that length-based methods are language-independent (Gale & Church, 1991), they really rely on length correlations arising from the historical relationships of the languages being aligned.  The length-based model assumes that each term in Ls is responsible for generating some number of terms in Lt. This leads to a further approximation that encapsulates the dependence to a single parameter δ. δ(ls,lt) is function of ls and lt, which can be designed according to different language pairs. The length-based method is developed based on the following approximation to Eq. (3):  Pr(L ↔ L | L , L ) ≈ Pr(L ↔ L | δ (l ,l ))  s  t  st  s  t  st  (3)  3. Proposed Models  Source Document (source language) Target Documents Set  Document Length ( ls) Length Filter Model  SMT Translation Model Source Document (target language) Query Generation Model Query (target language)  Bilingual Corpora  Subset (lt Ԗ[ls‐δ, ls+δ])  Document Searching Model TQDL  Target Document (target language)  Figure 1. The proposed approach for CLIR  20  Long-Yue WANG et al.  The approach relies on four models: translation model which generates the most probable translation of source documents; query generation model which determines what words in a document might be more favorable to use in a query; length filter model dynamically create a subset of candidates for retrieval according to the length information; and document searching model, which evaluates the similarity between a given query and each document in the target document set. The workflow of the approach for CLIR is shown in Fig. 1.  3.1 Translation Model  Currently, the good performing statistical machine translation systems are based on phrase-based models which translate small word sequences at a time. Generally speaking, translation model is common for contiguous sequences of words to translate as a whole. Phrasal translation is certainly significant for CLIR (Ballesteros & Croft, 1997), as stated in Section 1. It can do a good job in dealing with term disambiguation.  In this work, documents are translated using the translation model provided by Moses,  where the log-linear model is considered for training the phrase-based system models (Och &  Ney, 2002), and is represented as:  M  ∑ exp(  λ m  hm  (e1I  ,  fJ 1  ))  p(e1I | f1J ) =  m=1 M  (4)  ∑ ∑ exp(  λ m  hm  (e'1I  ,  fJ 1  ))  e '1I  m=1  where hm indicates a set of different models, λm means the scaling factors, and the denominator can be ignored during the maximization process. The most important models in Eq. (4) normally are phrase-based models which are carried out in source to target and target to source directions. The source document will maximize the equation to generate the translation including the words most likely to occur in the target document set.  3.2 Query Generation Model After translating the source document into the target language of the translation model, the system should select a certain amount of words as a query for searching instead of using the whole translated text. It is for two reasons, one is computational cost, and the other is that the unimportant words will degrade the similarity score. This is also the reason why it often responses nothing from the search engines on the Internet when we choose a whole text as a query. In this paper, we apply a classical algorithm which is commonly used by the search engines as a central tool in scoring and ranking relevance of a document given a user query. Term Frequency–Inverse Document Frequency (TF-IDF) calculates the values for each word in a document through an inverse proportion of the frequency of the word in a particular  TQDL: Integrated Models for Cross-Language Document Retrieval  21  document to the percentage of documents where the word appears (Ramos, 2003). Given a  document collection D, a word w, and an individual document d Ԗ D, we calculate  P(w, d ) = f (w, d ) × log | D | f (w, D)  (5)  where f(w, d) denotes the number of times w that appears in d, |D| is the size of the corpus, and f(w,D) indicates the number of documents in which w appears in D (Berger et al., 2000).  In implementation, if w is an Out-of-Vocabulary term (OOV), the denominator f(w,D) becomes zero, and will be problematic (divided by zero). Thus, our model makes log (|D|/ f(w,D))=1 (IDF=1) when this situation occurs. Additionally, a list of stop-words in the target language is also used in query generation to remove the words which are high frequency but less discrimination power. Numbers are also treated as useful terms in our model, which also play an important role in distinguishing the documents. Finally, after evaluating and ranking all the words in a document by their scores, we take a portion of the (n-best) words for constructing the query and are guided by:  Sizeq = [λpercent × Lend ]  (6)  Sizeq is the number of terms. λpercent is the percentage and is manually defined, which determines the Sizeq according to Lend, the length of the document. The model uses the first Sizeq-th words as the query. In another word, the larger document, the more words are selected as the query.  3.3 Document Retrieval Model  In order to use the generated query for retrieving documents, the core algorithm of the document retrieval model is derived from the Vector Space Model (VSM). Our system takes this model to calculate the similarity of each indexed document according to the input query. The final scoring formula is given by:  Score(q, d ) = coord (q, d ) ∑ tf (t, d ) × idf (t) × bst × norm(t, d )  (7)  tin q  where tf(t,d) is the term frequency factor for term t in document d, idf(t) is the inverse document frequency of term t, while coord(q,d) is frequency of all the terms in query occur in a document. bst is a weight for each term in the query. Norm(t,d) encapsulates a few (indexing time) boost and length factors, for instance, weights for each document and field. As a summary, many factors that could affect the overall score are taken into account in this model.  22  Long-Yue WANG et al.  3.4 Length Filter Model  In order to obtain a suitable filter, we firstly analyzed the golden data1 of ACL Workshop on SMT 2011, which includes Spanish, English, French, German and Czech 5 languages and 10 language pairs. English-Spanish language pair was used for analyzing and the data of the corpus are summarizes in Table 1.  Table 1. Analytical Data of Corpus of ACL Workshop on SMT 2011  Dataset  No. of Sentences  Size of corpus No. of Characters  Ave. No. Characters  English  3,003  74,753  25  Spanish  3,003  79,426  26  Fig. 2 plots the distribution of word number in each aligned sentences. lt is the length of English sentence while ls is the length of sentence in Spanish. So the expectation is c= E (lt/ls) =1.0073, with the correlation R2 = 0.9157. This shows that the data points are not substantially scatter in the plot and many data points are along with the regression line. Therefore, it is suitable to design a filter based on length ratio. 120  100  The Length of Each Spanish Sentence  80  60  40  20  0  0  20  40  60  80  100  120  The Length of Each English Sentence  Figure 2. The length ratio of Spanish-English sentences.  To obtain an estimated length-threshold (δ) for filter model, the function δ (ls, lt) can be designed as follows:  δ  (l  ,l  )  =  |  l t  −  l s  |  st  l  (8)  s  where ls and lt respectively stand for the length of a certain aligned sentence in the corpus we used. Finally, we got the average δ of around 0.15. In implementation, we choose 4δ instead of  δ to avoid some unnormal cases, where the right document would be discarded by the filter.  
Histogram equalization (HEQ) of speech features has received considerable attention in the field of robust speech recognition due to its simplicity and excellent performance. This paper is a continuation of this general line of research, presenting a novel HEQ-based feature normalization framework which takes advantage of joint equalization of spatial-temporal contextual statistics of speech features. In doing so, we explore the use of simple differencing and averaging operations to capture the contextual statistics of feature vector components for speech feature normalization. All experiments are conducted on the Aurora-2 database and task. Experimental results show that for clean-condition training, the methods instantiated from this framework achieve considerable word error rate reductions over the baseline system, which are indeed quite comparable to other conventional methods. Keywords: Speech Recognition, Noise Robustness, Histogram Equalization, Feature Contextual Statistics. 1. 研究動機 『科技始終來自於人性』，這是一家手機廠商的廣告用語；隨著科技不斷的進步，電腦 功能不斷地提升、相關資訊設備也日漸普及並且深入到你我的日常生活中，不僅為人類 生活帶來許多的便利性，更是大大地提升工作效率與生活品質。現今我們可以藉由電腦 或其它資訊設備來完成大部分的工作，如此一來便使得人類與電腦間有著密不可分的關 係。但目前人類與電腦的溝通方式，仍須仰賴鍵盤、滑鼠等工具，因此對於某些特定族 群的使用者而言，這種不友善的操作介面無疑是一個障礙。我們相信以最自然且簡便的 方式來操作這些科技產品，能將科技帶給人們的效益提升到最高。 由於語音是人們最自然且最普遍使用的溝通媒介，因此在不久的將來，語音必然會 扮演著人類與智慧型電子設備間，最重要的互動媒介，而自動語音辨識(Automatic Speech Recognition, ASR)技術將會是一個關鍵的角色。然而在現實生活中，已有許多和自動語 音辨識技術相關的應用，其中最廣為人知的應用為航空公司的語音訂位系統及銀行帳戶 的語音查詢系統等；而這一類的系統能成功運作的原因主要是因為限制系統辨識的詞彙 個數。此外還有許多的自動語音辨識相關的應用，如語音轉譯文字軟體、互動式聲音問 答系統和語音文件檢索等；然而要實現這類技術，將會面臨許多困難與障礙。 對於一套自動語音辨識系統而言，在語音訊號不受雜訊干擾的理想實驗室環境下， 一般皆可獲得良好的辨識結果，但若要應用至日常生活的環境中，常會受到環境中諸多 雜訊的干擾，例如:具有加成性的背景雜訊(Background Noise)或是錄音設備本身所產生的 摺積性的通道效應(Channel Effect)等，皆會造成系統之訓練環境與測試環境之間存在不 匹配(Mismatch)的情況，而嚴重地影響系統的辨識效能。因此，在自動語音辨識技術的  語音辨識使用統計圖等化方法  71  發展上，雜訊強健性(Noise Robustness)一直是一門重要的研究議題。並且，如何能以更 有效的方式來處理雜訊所造成的影響，將是一個既複雜又頗具挑戰性的任務。 如前所述，對於語音訊號而言，環境中雜訊的干擾大致可分為兩種類型: (1)加成性 雜訊(Additive Noise)和(2)摺積性(Convolutional Noise)雜訊。其中加成性雜訊為錄製語音 時，原始語音與背景雜訊呈線性加成的關係一同被收錄進去，例如汽車乎嘯而過或周遭 人們聊天所產生的噪音等；另一方面，摺積性雜訊則是指語音訊號經由不同傳輸通道所 造成的通道效應，例如麥克風通道效應、電話線路或手持式電話所產生的通道效應等。 圖 1 為乾淨語音訊號受加成性雜訊與摺積性雜訊干擾的示意圖。  乾淨語音訊號 s(t)  背景雜訊  通道效應  (加成性雜訊)  (摺積性雜訊)  n(t)  h(t)  圖 1. 雜訊對語音訊號干擾示意圖  雜訊語音訊號 y(t)=(s(t)*h(t))+n(t)  在強健性語音辨識的研究領域裡，過去已有許多學者已成功的發展出許多相關的演 算法，其主要目的為降低雜訊對語音訊號的影響，進而使得辨識結果能夠有效的提升。 依據所發展出方法的特性，約略可分為以下三大研究方向(Gong, 1995): (1) 語音訊號增益法(Speech Enhancement): 考量人耳聽覺的特性，以增加語音訊號在感知上的品質。其主要的目的為將語音 訊號從受雜訊干擾之空間轉換至乾淨語音空間，期望轉換後的語音訊號能與對應 的乾淨語音訊號相似。但此方法不保證一定可使自動語音辨識之效能提高。原因 為大多數的語音增益方法都會導致訊號失真的情形，雖然人耳對於些許訊號的失 真有很好的容忍力，但是這些干擾對自動語音辨識器而言則相當敏感。常見的技 術有頻譜消去法(Spectral Subtraction, SS) (Boll, 1979)、端點偵測(Voice Activity Detection, VAD) (ITU, 1996)等。 (2) 強健性語音特徵(Robust Speech Feature): 主要作法是希望從語音訊號中擷取較不易受到雜訊干擾而失真的強健性語音特 徵參數，進而降低訓練語料和測試語料間存在的不匹配情況，因此可以有效的提 升自動語音辨識的效能。其著名的方法有倒頻譜平均值消去法(Cepstrum Mean Subtraction, CMS) (Furui, 1981)、倒頻譜平均值與變異數正規化法(Cepstrum Mean  72  謝欣汝 等  and Variance Normalization, CMVN) (Viikki & Laurila, 1998)與倒頻譜平均與變異 數 正 規 化 法 結 合 自 動 回 歸 動 態 平 均 濾 波 器 法 (Cepstral Mean and Variance Normalization plus Auto-Regressive-Moving Averaging Filtering, MVA) (Chen & Bilmes, 2006)等。 (3) 聲學模型調適法(Acoustic Model Adaptation): 藉由辨識器的學習，以轉換聲學模型內的分佈，進而獲得與輸入的雜訊語音向量 近似的分佈。常見的技術有最大事後機率法則(Maximum a Posteriori, MAP) (Gauvain & Lee, 1994)、最大相似度線性回歸法(Maximum Likelihood Linear Regression, MLLR) (Leggetter & Woodland, 1995)、平行模型結合法(Parallel Model Combination, PMC) (Hung et al., 2001)等。 本論文所提出之新方法是基於上述第二類的強健性語音特徵所發展出來的。而目前 最廣泛被使用的語音特徵參數包含以人耳之聽覺特性為考量依據而發展出的梅爾倒頻譜 係數(Mel-Frequency Cepstral Coefficients, MFCCs) (Davis & Mermelstein, 1980)、線性預估 倒頻譜係數(Linear Prediction Cepstral Coefficients, LPCC) (Atal, 1974)及感知線性預估倒 頻譜係數(Perceptual Linear Prediction Cepstral Coefficients, PLPCC) (Hermansky, 1991)等。 然而，透過這些特徵參數擷取方法所抽取出來的特徵，往往卻極為容易受到雜訊的干擾 而有所影響，而本論文所提出之方法皆是作用於梅爾倒頻譜係數的架構上。 對於以特徵為基礎的強健性技術而言，由於和其他兩類別的強健性方法比較起來， 無論是在做法上或者對於演算法之運算複雜度上，都相對比較簡易且效果十分顯著，因 此目前已成功發展出一系列相關之演算法，例如倒頻譜平均值消去法(CMS)、倒頻譜平 均值與變異數正規化法(CMVN)與統計圖等化法(HEQ)。基於這三種方法，可消除雜訊所 造成的線性失真方法為倒頻譜平均值消去法和倒頻譜平均值與變異數正規化法，而統計 圖等化法則能補償雜訊所造成的非線性失真。本論文將此類方法歸納為動差正規化法， 將與其他種類的特徵正規化法在第二章節中給予詳盡的介紹。 本論文延續統計圖等化法的研究，提出一套新穎的語音特徵正規化技術，其作法是 在語音之倒頻譜特徵上，利用一個簡易的差分和平均的處理方式，來得到原始語音特徵 之相對應的文脈統計資訊後加以正規化並結合。此新方法的作法有別於傳統之個別維度 獨立正規化的統計圖等化法，而是正規化不同空間與時間之間的特徵分布資訊，因此可 以更進一步的降低不同聲學環境所產生的偏差，並且嘗試消除傳統之統計圖等化法無法 補償的問題，即隨機性雜訊對語音所產生的影響。本論文後續安排如下：第二章節介紹 一些著名的運用於時間序列之特徵正規化法的相關研究介紹；第三章則詳細介紹本論文 所提出的改良式統計圖等化法，其對應之實驗結果與討論則在第四章節中呈現；最後， 第五章節為結論與未來展望。  語音辨識使用統計圖等化方法  73  2. 運用於時間序列之特徵正規化法的相關研究介紹  2.1 相對頻譜法(Relative Spectral, RASTA) (Hermansky & Morgan, 1994) 觀察人類發音的特性，發現其語音訊號之調變頻譜在低於 1Hz 或高於 12Hz 的範圍是屬 於非語音的訊號(Non-Speech)，因此可以使用一個帶通濾波器(Band-Pass Filter)來移除非 語音的成分，針對數個音框的特徵參數進行平滑的動作。此濾波器的轉移函數(Transfer Function)如下所示:  0.1  2 ∑  θ(zθ  −  z−θ  )  HRASTA (z) =  θ=1 1− α z−1  (1)  由式(1)可知此濾波器是由一差量濾波器和一無限長度脈衝響應(Infinite Impulse Response, IIR)之低通濾波器串接而成，當 z = α 時則產生一極點，因此可用參數 α 來控制其頻率響 應之峰值所對應的頻率，且當 α 值愈大時，峰值所對應的頻率則變得更小，所以高頻部 分的響應則會被壓得更低，而在本論文的辨識實驗中設為 0.94。此外，還有一個位於 0 的零點，可以有效的去除極低頻之慢速變化通道失真效應。  2.2 動差正規化法(Moment Normalization)  如前所述之倒頻譜平均值消去法、倒頻譜平均值與變異數正規化法，通常只需很少量的 運算時間即可明顯提升語音辨識的效果因此被廣泛的應用，分別正規化語音特徵參數之 第一階動差與第一、二階動差，其公式如下所示:  d X  =  
This work proposes a unified view of several features based on frequent strings extracted from unlabeled data that improve the conditional random fields (CRF) model for Chinese word segmentation (CWS). These features include character-based n-gram (CNG), accessor variety based string (AVS) and its variation of left-right co-existed feature (LRAVS), term-contributed frequency (TCF), and term-contributed boundary (TCB) with a specific manner of boundary overlapping. For the experiments, the baseline is the 6-tag, a state-of-the-art labeling scheme of CRF-based CWS, and the data set is acquired from the 2005 CWS Bakeoff of Special Interest Group on Chinese Language Processing (SIGHAN) of the Association for Computational Linguistics (ACL) and SIGHAN CWS Bakeoff 2010. The experimental results show that all of these features improve the performance of the baseline system in terms of recall, precision, and their harmonic average as F1 measure score, on both accuracy (F) and out-of-vocabulary recognition (FOOV). In particular, this work presents compound features involving LRAVS/AVS and TCF/TCB that are competitive with other types of features for CRF-based CWS in terms of F and FOOV, respectively. Keywords: Conditional Random Fields, Word Segmentation, Accessor Variety, Term-contributed Frequency, Term-contributed Boundary. ＊Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan. † Institute of Information System and Application, National Tsing Hua University, Hsinchu, Taiwan. ‡ Department of Computer Science & Engineering, Yuan Ze University, Taoyuan, Taiwan. E-mail: thtsai@saturn.yzu.edu.tw + Institute of Information Science, Academia Sinica, Taipei, Taiwan. E-mail: {tmjiang, dapi, tinghaoyang, laybow, hsu}@iis.sinica.edu.tw  46  Mike Tian-Jian Jiang et al.  1. Introduction Background Many intelligent text processing tasks, such as information retrieval, text-to-speech, and machine translation assume the ready availability of a tokenization into words, which is relatively straightforward in languages with word delimiters (e.g., space) but is a little difficult for Asian languages, such as Chinese and Japanese. Chinese word segmentation (CWS) has been an active area of research in computational linguistics for two decades. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, has conducted five word segmentation bakeoffs (Emerson, 2005; Jin & Chen, 2007; Levow, 2006; Sproat & Emerson, 2003; Zhao & Liu, 2010). After years of intensive research, CWS has achieved high accuracy, but the issue of out-of-vocabulary (OOV) word recognition remains. The State of the Art of CWS Traditional approaches for CWS adopt a dictionary and rules to segment unlabeled texts, such as the work of Ma and Chen (2003). In recent years, there has been a potent trend of using statistical machine learning models, especially the conditional random fields (CRF) (Lafferty et al., 2001), which displays moderate performance for the sequential labeling problem and achieves competitive results with character-position based methods(Zhao et al., 2010). Unsupervised Feature Selection for CWS In this work, unsupervised feature selection for CWS is based on frequent strings that are extracted automatically from unlabeled corpora. For convenience, these features are referred to as unsupervised features in the rest of this paper. Unsupervised features are suitable for closed training evaluation where external resources or extra information is not allowed, especially for cross-domain tasks, such as SIGHAN CWS bakeoff 2010(Zhao & Liu, 2010). Without proper knowledge, the closed training evaluation of word segmentation can be difficult with OOV words, where frequent strings collected from the test data may help. For incorporating unsupervised features into character-position based CRF for CWS, Zhao and Kit (2007) tried strings based on accessor variety (AV), which was developed by Feng et al. (2004), and based on co-occurrence strings (COS). Jiang et al. (2010) applied a feature similar to COS, called term-contributed boundary (TCB). According to Zhao and Kit (2007), AV-based string (AVS) is one of the most effective unsupervised features for CWS by character-position based CRF. One motivation here is to seek deeper understanding of AVS’s success. This work suspects that, since AVS is designed to keep overlapping substrings via the outer structure of a string while COS/TCB is usually selected via the inner structure of a string with its longest-first (i.e., non-overlapping) nature before integration into CRF, combining overlapping and outer information with  Enhancement of Feature Engineering for Conditional Random  47  Field Learning in Chinese Word Segmentation Using Unlabeled Data  non-overlapping and inner information may enhance CRF-based CWS. Hence, a series of experiments is conducted to examine this hypothesis. The remainder of the article is organized as follows. Section 2 briefly introduces CRF. Common unsupervised features based on the concept of frequent strings are explained in Section 3. Section 4 discusses related works. Section 5 describes the design of the labeling scheme and feature templates, along with a framework that is able to encode those overlapping features in a unified way. Details about the experiment are reported in Section 6. Finally, the conclusion is presented in Section 7.  2. Conditional Random Fields  Conditional random fields (CRF) are undirected graphical models trained to maximize a  conditional probability of random variables X and Y, and the concept is well established for  the sequential labeling problem (Lafferty et al., 2001). Given an input sequence (or  observation sequence) X = x1...xT and a label sequence Y = y1...yT , a conditional probability of linear-chain CRF with parameters Λ = λ1...λn can be defined as:  Pλ  (Y  |  X)  =  
This paper proposes an approach to identify word candidates that are not Traditional Chinese, including Japanese names (written in Japanese Kanji or Traditional Chinese characters) and word variants, when doing word segmentation on Traditional Chinese text. When handling personal names, a probability model concerning formats of names is introduced. We also propose a method to map Japanese Kanji into the corresponding Traditional Chinese characters. The same method can also be used to detect words written in character variants. After integrating generation rules for various types of special words, as well as their probability models, the F-measure of our word segmentation system rises from 94.16% to 96.06%. Another experiment shows that 83.18% of the 862 Japanese names in a set of 109 human-annotated documents can be successfully detected. Keywords: Semantic Chinese Word Segmentation, Japanese Name Identification, Character Variants. 1. Introduction Word segmentation is an indispensable technique in Chinese NLP. Nevertheless, the processing of Japanese names and Chinese word variants has been studied rarely. At the time when Traditional Chinese text was mostly encoded in BIG5, writers often transcribed a Japanese person’s name into its equivalent Traditional Chinese characters, such as the name “滝沢秀明” (Hideaki Takizawa) in Japanese becoming “瀧澤秀明” in Traditional Chinese. After Unicode was widely adopted, we also could see names written in original Japanese Kanji in Traditional Chinese text. Another issue is how different regions may write a character ∗ Department of Computer Science and Engineering, National Taiwan Ocean University No 2, Pei-Ning Road, Keelung, 20224 Taiwan E-mail: cjlin@ntou.edu.tw; jjt@cyber.ntou.edu.tw; {M97570019, M97570020}@ntou.edu.tw  88  Chuan-Jie Lin et al.  in a different shape. For example, the Traditional Chinese character 圖 (picture) is written as 图 in Simplified Chinese and 図 in Japanese. How these character variants impact Chinese text processing has been mentioned rarely in earlier studies; thus, it has become our interest. Chinese word segmentation has been studied for a long time. Many recent word segmentation systems have been rule-based or probabilistic. The most common rules are longest-word-first or least-segmentation-first. The probability models are often built in Markov's unigram or bigram models, such as in Peng and Chang (1993). Word candidate sets are often vocabulary in a dictionary or a lexicon collected from a large corpus. Some systems also propose possible candidates by morphological rules (Gao et al., 2003), such as NOUN+“們” (plural form of a noun) as a legal word (e.g. “學生們,” students, and “家長們,” parents). Wu and Jiang (1998) even integrated a syntactic parser in their word segmentation system. In addition to word segmentation ambiguity, the out-of-vocabulary problem is another important issue. Unknown words include rare words (e.g. “躉售,” for sale); technical terms (e.g. “三聚氰胺,” Melamine, a chemical compound); newly invented terms (Chien, 1997) (e.g. “ 新 流 感 ,” Swine flu); and named entities, such as personal and location names. NE recognition is an important related technique (Sun et al., 2003). In recent times, machine learning approaches have been the focus of papers on Chinese segmentation, such as using SVM (Lu, 2007) or CRF (Zhao et al., 2006; Shi & Wang, 2007). There have been fewer studies focused on handling words that are not Traditional Chinese words in Traditional Chinese text. The most relevant work is discussion of the impact of the different Chinese vocabulary used in different areas on word segmentation systems. These experiments have been designed to train a system with a Traditional Chinese corpus but test on a Simplified Chinese test set or to increase the robustness of a system using a lexicon expanded by adding new terms in different areas (Lo, 2008). The main problem in this paper is defined as follows. When a word that is not Traditional Chinese appears in a Traditional Chinese document, such as the Japanese name “滝沢秀明” (written in Japanese Kanji) or “瀧澤秀明” (written in its equivalent Traditional Chinese), word variants (e.g. “裡面” vs. “裏面”), and words written in Simplified Chinese, all of these words can be detected and become word segmentation candidates. This paper is constructed as follows. Section 2 introduces the basic architecture of our word segmentation system. Section 3 explains the Chinese and Japanese name processing modules. Section 4 talks about the character-variant clusters with a corresponding Traditional Chinese character. Section 5 delivers the experimental results and discussion, and Section 6 concludes this paper.  Strategies of Processing Japanese Names and  89  Character Variants in Traditional Chinese Text  2. Word Segmentation Strategy This paper focuses on approaches to handling words that are not Traditional Chinese during word segmentation. We first constructed a basic bigram model word segmentation system. We did not build a complicated system because its purpose is only for observing the effect of applying different handling approaches for words that are not written in Traditional Chinese on the performance of word segmentation. Word candidates were identified by searching the lexicon or applying detection rules for special-type words, such as temporal or numerical expressions. Note that identical word candidates may be proposed by different rules or the lexicon. Moreover, if no candidate of any length can be found at a particular position inside the input sentence, the system automatically adds a one-character candidate at that position. Afterward, the probabilities of all of the possible segmentations are calculated according to a bigram model. The highest probable segmentation is proposed as the result.  2.1 Special-Type Word Candidate Generation Rules As there are many special type words, it is impossible to collect them all in a lexicon. Hence, we manually designed many detection rules to recognize such words in an input sentence. The special types handled in our system include the following: address, date, time, monetary, percentage, fraction, Internet address (IP, URL, e-mail, etc.), number, string written in foreign language, and Chinese and Japanese personal name. Numerical digits in the detection rules can be full-sized or Chinese numbers (一,二…壹貳…). Foreign language characters are detected according to the Unicode table; thus, any character sets, such as Korean or Arabic characters, easily can be added into our system. Consequent characters written in the same foreign language are treated as one word, as most languages use the space symbol as the word-segmentation mark. Since the focus of this paper is not on the correctness of these special rules, only personal name detection rules will be explained in Section 3.  2.2 Bigram Probabilistic Model  After enumerating all possible segmentations, the next step is to calculate their probabilities P(S). There have been many probabilistic models proposed in word segmentation research. Our system is built on Markov's bigram probabilistic model, whose definition is:  N  P(S = w1w2...wN ) = P(w1) × ∏ P(wi | wi−1)  (1)  i=2  where P(wi) is the unigram probability of the word wi and P(wi | wi-1) is the probability that wi appears after wi-1. In order to avoid the underflow problem, the equation is often calculated in its logistic form:  90  Chuan-Jie Lin et al.  N  log P(S = w1w2...wN ) = log P(w1) + ∑ log P(wi | wi−1)  (2)  i=2  Data sparseness is an apparent problem, i.e. most word bigrams have no probability. Our solution is a unigram-back-off strategy. That is, when a bigram <wi-1, wi> never occurs in a training corpus, its bigram probability P(wi | wi-1) is measured by αP(wi) instead.  When determining the probability of a bigram containing special-type words, the probability is calculated by Eq. 3. Suppose that wi belongs to a special type T; the equation is defined as:  P(wi | wi−1)P(wi+1 | wi ) = P(T | wi−1) × P(wi+1 | T ) × PG (wi | T )  (3)  where P(T | wk) and P(wk | T) are the special-type bigram probabilities for the type T and a word wk and where PG(wi | T) is the generation probability of wi being in the type T. The generation probabilities are set to 1 for all special types other than the personal names, whose definitions are explained in Section 3. As the boundaries of some special types, including address, monetary, percentage, fraction, Internet address, number, and foreign language string, are deterministic and unambiguous, their special-type bigram probabilities are all set to be 1, which means that we accept the segmentation directly. On the other hand, characters for Chinese numbers often appear as a part of a word, such as “一切” (“一,” one; “一切,” all) and “萬一” (both characters are numbers but together mean “if it happens”). Therefore, the number-type bigram probability is trained from a corpus. Some temporal expressions are unambiguous, such as the date expression “中華民國九 十八年六月二十一日” (“June 21 of the 98th year of the R.O.C.”). Their special-type bigram probabilities are set to 1. For ambiguous temporal expressions, such as “三十年” (meaning “the 30th year” or “thirty years”), their special-type bigram probabilities are obtained by training. Before training a bigram model, words belonging to special types first are identified by detection rules and replaced by labels representing their types so that special-type bigram probabilities can be measured at the same time. Our special-type bigram probability model is very similar to Gao et al. (2003). Nevertheless, they treat all dictionary words as one class and all types of special words as a second class, while we treat different types as different classes.  2.3 Computation Reduction When an input sentence is too long or too many possible segmentations can be found (sometimes hundreds of thousands), the computation time becomes intractable. In order to  Strategies of Processing Japanese Names and  91  Character Variants in Traditional Chinese Text  reduce the computation load, we use the beam search algorithm to prune some low probability segmentations. The main idea of the algorithm is described as follows. Let N be the number of characters in an input sentence. Declare N priority queues (denoted as record[i] where i = 1~N) to record the top k segmentations with the highest probability scores covering the first i characters. For each word candidate w beginning with the (i+1)th character whose length is b, append the word w with every segmentation stored in record[i], compute the probability of the new segmentation, and try to insert it into the queue record[i+b]. If the new segmentation has higher probability than any segmentation stored in the queue record[i+b], the segmentation with the lowest probability in record[i+b] is discarded in order to insert this new segmentation. At the beginning, all priority queues are empty. Start with the first character in the sentence. Recursively perform the steps described in the previous paragraph until all of the word candidates starting with the Nth character have been considered. In the end, the top 1 segmentation stored in record[N] is proposed as the result. The queue size k is set to be 20 in our system. 3. Chinese and Japanese Name Processing In this section, we focus on how to find Japanese names written in Japanese Kanji that appear in a Traditional Chinese article. The method of identifying Japanese names written in corresponding Traditional Chinese characters is discussed in Section 4. As our approach to recognize Japanese personal names is similar to the one to find Chinese names, our Chinese name identification approach is introduced first. 3.1 Chinese Personal Name Identification A Chinese personal name consists of a surname part and a first name part. A Chinese surname can be one or two syllables (one or two characters) long. In some cases, a person may have two surnames (usually both with one syllable) in his or her name for various reasons. The first name part in a Chinese name is also one or two syllables long. All name formats possibly seen in an article are listed in Table 1, where “SN” denotes “surname,” “FN” as “first name,” and “char” is “character”. All strings matching these formats are treated as Chinese name candidates, except the format “1-char FN,” in order to prevent proposing every single character as a personal name candidate. The combination of two surnames is also restricted to two 1-syllable surnames, because one rarely sees a 2-syllable surname combined with another surname. We need to build probabilistic models for each character being in every part of a name, as well as a probabilistic model for the personal name formats.  92  Chuan-Jie Lin et al.  Table 1. Chinese personal name formats (surnames are underlined)  Format SN only FN only  Cases 1-char SN 2-char SN 1-char FN 2-char FN  Examples Prof. 林 Mr. 諸葛 慧 國雄  Format SN+FN  Cases 1-char SN+1-char FN 1-char SN+2-char FN Two SNs+1-char FN Two SNs+2-char FN 2-char SN+1-char FN 2-char SN+2-char FN  Examples 陳登 王小明 張 李娥 張 陳素珠 諸葛亮 司馬中原  To recognize a Chinese name, first we have to prepare a complete list of Chinese surnames. We collected surnames from the Wikipedia entries “中國姓氏列表”1 (List of Chinese Surnames) and “複姓”2 (2-Syllable Surnames), the websites of the Department of Civil Affairs at the Ministry of Interior3, 中華百家姓4 (GreatChinese), and 千家姓5 (Thousand Surnames). 2,471 surnames were collected. As for the first name part, we simply treat all of the Chinese characters as possible first name characters.  The generation probability model of a word being a Chinese name is defined as Eq. 4, where σ is the gender model (male or female), and π is a possible format matching the word w. The name format is represented as π = ‘xxxx,’ where ‘s’ denotes a 1-syllable surname, ‘dd’ a 2-syllable surname, and ‘n’ a character in a first name. For example, the format “two SNs+2-char FN” is represented as π = ‘ssnn’ and the format “2-char SN+1-char FN” is represented as π = ‘ddn’.  PG (w  |  SCHname )  =  max σ ,π  Pσ  (w  |π  )PG (π  |  SCHname )  (4)  In Eq. 4, the Chinese name generation probability Pσ (w|π) is the probability of a word w being a Chinese name whose format is π and gender is σ. The Chinese name format probability PG(π | SCHname) is the probability of the special type SCHname (Chinese personal names) appearing in an article with a format π. The methods of building these probabilistic models are introduced in the following paragraphs.  When computing the Chinese name generation probability Pσ (w|π), we borrowed the idea from Chen et al. (1998), but we assume that the choice of first names is independent of the surname, and the choice of two characters in the first name part is also independent, in order to reduce the complexity. We also assume that the surname is unrelated to the person’s gender. Table 2 lists all of the definitions of the Chinese name generation probabilities for  
This paper explores the relationship between intelligibility and comprehensibility in speech synthesizers, and it designs an appropriate comprehension task for evaluating the speech synthesizers’ comprehensibility. Previous studies have predicted that a speech synthesizer with higher intelligibility will have higher performance in comprehension. Also, since the two most popular speech synthesis methods are HMM-based and unit selection, this study tries to compare whether the HTS-2008 (HMM-based) or Multisyn (unit selection) speech synthesizer has better performance in application. Natural speech is applied in the experiment as a control group to the speech synthesizers. The results in the intelligibility test show that natural speech is better than HTS-2008, which, in turn, is much better than the Multisyn system. In the comprehension task, however, all three of the speech systems display minimal differences in the speech comprehension process. This is because the two speech synthesizers have reached the threshold of having enough intelligibility to provide high speech comprehension quality. Therefore, although there is equal comprehensible speech quality between the HTS-2008 and Multisyn systems, the HTS-2008 speech synthesizer is recommended due to its higher intelligibility. Keywords: Speech Synthesizers, Intelligibility Evaluation, Comprehension Evaluation, HTS-2008, Multisyn.  
 Cheng-Ru Li, Chi-Hsin Yu, and Hsin-Hsi Chen 摘要 詞彙的意見極性是句子及文件層次意見分析的重要基礎，雖然目前已經存在一 些人工標記的中文情緒字典，但如何自動標記詞彙的意見極性，仍是一個重要 的工作。這篇論文的目的是為廣義知網的詞彙自動標記意見極性。我們運用監 督式機器學習的方法，抽取不同來源的各種有用特徵並加以整合，來預測詞彙 的意見極性。實驗結果顯示，廣義知網詞彙意見極性預測的準確率可到達 92.33%。 關鍵字: 廣義知網，情緒分析，情緒字典, 語義傾向, 向量支援機 Abstract The semantic orientation of terms is fundamental for sentiment analysis in sentence and document levels. Although some Chinese sentiment dictionaries are available, how to predict the orientation of terms automatically is still important. In this paper, we predict the semantic orientation of terms of E-HowNet. We extract many useful features from different sources to represent a Chinese term in E-HowNet, and use a supervised machine learning algorithm to predict its orientation. Our experimental results showed that the proposed approach can achieve 92.33% accuracy. Keywords: E-NowNet, Sentiment Analysis, Sentiment dictionary, Semantic orientation, SVM ∗ 國立台灣大學資訊工程系 Department of Computer Science and Information Engineering, National Taiwan University E-mail: {crlee, jsyu}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw  22  李政儒 等  1. 緒論 情緒分析（Sentiment Analysis）在現今的網路世界中，有許多實際且重要的運用，例如 從網路的評論文章中分析消費者對產品的評價，或分析消費者對產品性能的關注焦點等 等。不管對句子或文件層次的情緒分析，意見詞詞典都是一個重要的資源。通常意見詞 詞典是用人工來收集詞彙，並用人工標記詞彙的各種情緒屬性，包括主客觀（subjective or objective）、極性（orientation/polarity)、）及極性的強度（strength）(Esuli & Sebastiani, 2005)。這些情緒屬性對不同的應用有不同的重要性，標記難度也各不相同，通常詞彙的 極性是最容易進行標記的屬性。 標記情緒屬性時，研究者可以從零開始收集詞彙以建立意見詞詞典，如台大意見詞 詞典 NTUSD(Ku & Chen, 2007)。在另一方面，也有研究者嘗試為自然語言處理中的許 多現存的資源，添加情緒屬性，如 SentiWordNet(Esuli & Sebastiani, 2006a)。但現有資源 的語彙量通常很大，如 WordNet 3.0 就包括 206,941 個不同的英文字義（word-sense pair）， 要全部用人工進行標記之成本太高。因此，通常的作法是少量標記一些詞彙，再用機器 學習方法，為剩下的詞彙進行自動標記，雖然自動標記的準確率不如人工標記，但對一 般應用有某種程度的幫助。 在中文自然語言處理，NTUSD 是一部重要的意見詞詞典，但此詞典只包括詞彙及 極性的資訊。另一方面，董振東先生和陳克健教授所建立的知網和廣義知網(Z. Dong & Dong, 2006; 陳克健, 黃, 施, & 陳, 2004)，是重要的語意資源。對於每個詞彙，都用有 限的義原給予精確的定義，但這些定義卻缺乏情緒的語意標記。因此，如何自動為廣義 知網加上情緒標記，成為一個重要的課題，也是本研究的目的。 本研究提出為廣義知網加上情緒標記的方法，首先利用 NTUSD 跟廣義知網詞彙的 交集建立標準答案集，再由標準答案集訓練出分類器，為其他廣義知網詞彙進行標記。 如何有效的運用監督式機器學習演算法，如何為詞彙抽取出有用的特徵，是主要的挑戰 議題。在此研究中，我們有系統的嘗試抽取各種不同的詞彙特徵，最後得到高準確率的 二元分類器（binary classifiers）用以自動標記正負面情緒標記。 第二節介紹廣義知網、及英文和中文相關的情緒屬性標記研究，第三節介紹從 E-HowNet 及 Google Chinese Web 5-gram 抽取特徵的方法，第四節呈現各種實驗的結果 及分析，包括跟 NTUSD 人工標記的比較，最後總結論文的成果。 2. 相關研究 董振東先生於 1998 年創建知網（HowNet），並在 2003 年，跟中央研究院資訊所詞庫小 組在 2003 年，將中研院詞庫小組詞典（CKIP Chinese Lexical Knowledge Base）的詞條 跟知網連結，並作了一些修改，最後形成廣義知網（Extended-HowNet, E-HowNet1）。 詞庫小組修改並擴展知網原先的語義義原角色知識本體，建構出廣義知網知識本體 
This study examines how different dimensions of corpus frequency data may affect the outcome of statistical modeling of lexical items. Our analysis mainly focuses on a recently constructed elderly speaker corpus that is used to reveal patterns of aging people’s language use. A conversational corpus contributed by speakers in their 20s serves as complementary material. The target words examined are temporal expressions, which might reveal how the speech produced by the elderly is organized. We conduct divisive hierarchical clustering analyses based on two different dimensions of corporal data, namely raw frequency distribution and collocation-based vectors. When different dimensions of data were used as the input, results showed that the target terms were clustered in different ways. Analyses based on frequency distributions and collocational patterns are distinct from each other. Specifically, statistically-based collocational analysis generally produces more distinct clustering results that differentiate temporal terms more delicately than do the ones based on raw frequency.  
A morphological family in Chinese is the set of compound words embedding a common morpheme, and Self-organizing maps (SOM) of these Chinese morphological families can be built. Computation of the unified-distance matrices for the SOMs allows us to perform semantic clustering of the members of the morphological families. Such semantic clustering sheds light on the interplay between morphology and semantics in Chinese. We studied how the word lists used in a lexical decision task (LDT) (Chen, Galmar, & Su, 2009) are mapped onto the clusters of the SOMs. We showed that this mapping is helpful to predict whether repetitive processing of members of a morphological family would elicit a satiation in an LDT - habituation - of both morphological and semantic units of the shared morpheme. In their LDT experiment, Chen, Galmar, and Su (2009) found evidence for morphological satiation but not for semantic satiation. Conclusions drawn from our computational experiments and calculations are in accordance with the behavioral experimental results in Chen et al. (2009). Finally, we showed that our work could be helpful to linguists in preparing adequate word lists for behavioral study of Chinese morphological families. Keywords: Self-Organizing Maps, Computational Morphology and Semantics. 1. Introduction In this paper, we call a morphological family the set of compound words embedding a common morpheme. Hence, the compound words in Table 1, which all contain the morpheme ‘明’ (míng) as a first character, belong to the morphological family of ‘明’.  ∗ Institute of Education, National Cheng Kung University, Tainan, Taiwan E-mail: hsuyueshan@gmail.com  56  Bruno Galmar  Table 1.Some examples of the明 morphological family (Chen, Galmar, & Su, 2009).  明朝  明天  明白  明確  明星  明亮  Ming Dynasty tomorrow to understand / clear explicit  star  bright  In Chinese, the meaning of a morpheme can be either transparent or opaque to the meaning of the compound word embedding it. For example, the common morpheme in Table 1 “明” can mean (clear) or (bright) and is transparent to the meaning of “明星” (star) but rather opaque to the meaning of “明天” (tomorrow). If some members of a morphological family are semantically similar, one could advance as a reason for such a similarity that these members are transparent to the same meaning of the shared morpheme. Most Chinese morphemes are polysemous (Chen & Chen, 2000). Hence, in theory, transparent members of a morphological family could belong to different semantic clusters whose centers would be the different meanings of the shared polysemous morpheme. This paper is aimed primarily at using computational linguistics methods to perform semantic clustering of the members of the morphological families. Such a clustering is used to predict the results of a behavioral Lexical Decision Task1 (LDT) designed by Chen, Galmar, & Su (2009) to study the phenomenon of morphological satiation in Chinese. In visual word recognition, morphological satiation is an impairment of morphological processing induced by repetitive exposure to the same morpheme embedded in different Chinese compound words (Chen et al., 2009; Cheng & Lan, 2009). Chen, Galmar, and Su (2009) posited that morphological satiation is due to habituation of the morphological unit of the repeated morpheme. This is represented in Figure 1 by Diagram (a). As a morpheme is thought to be a meaningful unit, it is logical to consider whether a semantic satiation (Kounios, Kotz, & Holcomb, 2000; Smith & Klein, 1990; Tian & Huber, 2010), an impairment of semantic processing causing a temporary loss of the meaning of the common morpheme, would occur concomitantly with morphological satiation.2 In other words, the satiation observed by Chen et al. (2009) could have two loci: a morphological locus and a semantic locus, as represented in Figure 1 by Diagram (d). A morphological satiation could also have its loci of satiation on the links between the morphological, lexical, and semantic units, as represented in Figure 1 by Diagrams (b) and (c). We quickly can rule out the possibility of a locus on the link between morphological and  
This paper brings up an important issue, polysemy problems, in a Chinese to Taiwanese TTS (text-to-speech) system. Polysemy means there are words with more than one meaning or pronunciation, such as “我們” (we), “不” (no), “你” (you), “我” (I), and “要” (want). We first will show the importance of the polysemy problem in a Chinese to Taiwanese (C2T) TTS system. Then, we will propose some approaches to a difficult case of such problems by determining the pronunciation of “我們” (we) in a C2T TTS system. There are two pronunciations of the word “我們” (we) in Taiwanese, /ghun/ and /lan/. The corresponding Chinese words are “阮” (we1) and “咱” (we2). We propose two approaches and a combination of the two to solve the problem. The results show that we have a 93.1% precision in finding the correct pronunciation of the word “我們” (we). Compared to the results of the layered approach, which has been shown to work well in solving other polysemy problems, the results of the combined approach are an improvement. Keywords: Polysemy, Taiwanese, Chinese to Taiwanese TTS System, Layered Approach 1. Introduction Besides Mandarin, Taiwanese is the most widely spoken dialect in Taiwan. According to Liang et al. (2004), about 75% of the population in Taiwan speaks Taiwanese. Currently, it is government policy to encourage people to learn one’s mother tongue in schools because local languages are a part of local culture. Researchers (Bao et al., 2002; Chen et al., 1996; Lin et al., 1998; Lu, 2002; Shih et al., 1996; Wu et al., 2007; Yu et al., 2005) have had outstanding results in developing Mandarin ∗ Department of Computer Science and Engineering, National Chung-Hsing University, Taichung 40227, Taiwan. + Department of Information Management, Chien-Kuo Technology University, Chang-hua 500, Taiwan. E-mail: yclin@ctu.edu.tw  44  Ming-Shing Yu and Yih-Jeng Lin  text-to-speech (TTS) systems over the past ten years. Other researchers (Ho, 2000; Huang, 2001; Hwang, 1996; Lin et al., 1999; Pan & Yu, 2008; Pan, Yu, & Tsai, 2008; Yang, 1999; Zhong, 1999) have just begun to develop Taiwanese TTS systems. There are no formal characters for Taiwanese, so Chinese characters are officially used in Taiwan. Consequently, many researchers have focused on Chinese to Taiwanese (C2T) TTS systems. This means that the input of a so-called Taiwanese TTS system is Chinese text. Yang (1999) developed a method based on machine translation to help solve this problem. Since there are differences between Mandarin and Taiwanese, a C2T TTS system should have a text analysis module that can solve problems specific to Taiwanese. For instance, there is only one pronunciation for “我們” (we) in Chinese, but there are two pronunciations for “我們” (we) in Taiwanese. Figure 1 shows a common structure of a C2T TTS system. In general, a C2T TTS system should contain four basic modules. They are (1) a text analysis module, (2) a tone sandhi module, (3) a prosody generation module, and (4) a speech synthesis module. A C2T TTS system also needs a text analysis module like that of a Mandarin TTS system. This module requires a well-defined bilingual lexicon. We also find that text analysis in a C2T TTS system should have functions not found in a Mandarin TTS system, such as phonetic transcription, digit sequence processing (Liang et al., 2004), and a method for solving the polysemy problem. Solving the polysemy problem is the most complex and difficult of these. There has been little research on solving the polysemy problem. Polysemy means that a word has two or more meanings, which may lead to different pronunciations. For example, the word “他” (he) has two pronunciations in Taiwanese, /yi/ and /yin/. The first pronunciation /yi/ of “他” (he) means “he,” while the second pronunciation /yin/ of “他” (he) means “second-person possessive”. The correct pronunciation of a word affects the comprehensibility and fluency of Taiwanese speech. Many researchers have studied C2T TTS systems (Ho, 2000; Huang, 2001; Hwang, 1996; Lin et al., 1999; Pan & Yu, 2008; Pan, Yu, & Tsai, 2008; Yang, 1999; Zhong, 1999). Nevertheless, none of the researchers considered the polysemy problem in a C2T TTS system. We think that solving the polysemy problem in a C2T TTS system is a fundamental task. The correct meaning of the synthesized words cannot be determined if this problem is not solved properly.  The Polysemy Problem, an Important Issue in a  45  Chinese to Taiwanese TTS System  Input Chinese texts  Bilingual Lexicon  Text Analysis  Tone Sandhi  Prosody Generation  Synthesis units  Speech Synthesis Synthesized Taiwanese Speech  Figure 1. A Common module structure of a C2T TTS System. The remainder of this paper is organized as follows. In Section 2, we will describe the polysemy problem in Taiwanese. We will give examples to show the importance of solving the polysemy problem in a C2T TTS system. Determining the correct pronunciation of the word “我們” (we) is the focus of the challenge in these cases. Section 3 is the description of the layered approach, which has been shown to work well in solving the polysemy problem (Lin et al., 2008). Lin (2006) has also shown that the layered approach works very well in solving the polyphone problem in Chinese. We will apply the layered approach in determining the pronunciation of “我們” (we) in this section. In Section 4 and Section 5, we use two models to determine the pronunciation of the word “我們” (we) in sentences. The first approach in Section 4 is called the word-based unigram model (WU). The second approach, which will be applied in Section 5, is the word-based long-distance bigram model (WLDB). We also make some new inferences in these two sections. Section 6 shows a combination of the two models discussed in Section 4 and Second 5 for a third approach to solving the polysemy problem. Finally, in Section 7, we summarize our major findings and outline some future works.  46  Ming-Shing Yu and Yih-Jeng Lin  2. Polysemy Problems in Taiwanese Unlike in Chinese, the polysemy problem in Taiwanese appears frequently and is complex. We will give some examples to show the importance of solving the polysemy problem in a C2T TTS system. The first examples feature the pronouns “你” (you), “我” (I), and “他” (he) in Taiwanese. These three pronouns have two pronunciations, each of which corresponds to a different meaning. Example 2.1 shows the pronunciations of the word “我” (I) and “你” (you) in Taiwanese. The two pronunciations of “我” (I) are /ghua/ with the meaning of “I” or “me” and /ghun/ with the meaning of “my”. The two pronunciations of “你” (you) are /li/ with the meaning of “you” and /lin/ with the meaning of “your”. If one chooses the wrong pronunciation, the utterance will carry the wrong meaning. Example 2.1 我/ghua/過一會兒會拿幾本有關台語文化的書到你/lin/家給你/li/，你/li/可以 不必到我/ghun/家來找我/ghua/拿。 (I will bring some books about Taiwanese culture to your house for you later; you need not come to my home to get them from me.) Example 2.2 shows the two different pronunciations of “他” (he). They are /yi/, with the meaning of “he” or “him,” and /yin/, with the meaning of “his”. Example 2.2 我看到他/yi/拿一盆蘭花回他/yin/家給他/yin/爸爸。 (I saw him bring an orchid back to his home for his father.) The following examples focus on “不” (no), which has six different pronunciations. They are /bho/, /m/, /bhei/, /bhuaih/, /mai/, and /but/. Examples 2.3 through 2.6 show four of the six pronunciations. Example 2.3 一般人並不/bho/容易看出它的重要性。 (It is not easy for a person to see its importance.) Example 2.4 不/m/知浪費了多少國家資源。 (We do not know how many national resources were wasted.) Example 2.5 讓人聯想不/bhei/到他與機械的關係。 (One would not come to the proper conclusion regarding the relationship between that person and machines.) Example 2.6 華航使用之航空站交通已不/but/如從前方便。 (The traffic at the airport is not as convenient as it was in the past for China Airlines.) Examples 2.7 through 2.9 are examples of pronunciations of the word “上” (up). The word “上” (up) has three pronunciations. They are /ding/, /siong/, and /jiunn/. The meaning of the word “上” (up) in Example 2.7 has the sense of “previous”. Example 2.8 shows a case where “上” (up) means “on”. Example 2.9 is an example of the use of “上” (up) to mean, “get on”.  The Polysemy Problem, an Important Issue in a  47  Chinese to Taiwanese TTS System  Example 2.7 我上/ding/個月花了好多錢去買有關台語的教科書。(Last month, I spent so much money on buying Taiwanese textbooks.) Example 2.8 我是在這地圖上/siong/的哪裡？ (Where am I on this map?) Example 2.9 我上/jiunn/了公車後才發現我搭錯車了。 (After I got on the bus, I realized that I boarded the wrong one.) Another word we want to discuss is “下” (down). The word “下” (down) has four pronunciations. They are /ha/, /ao/, /loh/, and /ei/. Examples 2.10–2.13 are some examples of pronunciations of the word “下” (down). The meaning of “下” (down) in Example 2.10 is “close” or “end”. Example 2.11 shows how the same word can mean “next”. Example 2.12 illustrates the meaning “falling”. Example 2.13 shows another example of it used to mean “next”. Example 2.10 我今天將在十點下/ha/課。 (I will finish my class at ten o’clock today.) Example 2.11 台中下/ao/星期有甚麼音樂會？ (What concerts are scheduled for next week in Taichung?) Example 2.12 彰化已經開始下/loh/大雨了。 (It has begun to rain heavily in Changhua.) Example 2.13 請問下/ei/一列火車何時開出？ (Excuse me. Could you please tell me when the next train will depart?) We have proposed a layered approach in predicting the pronunciations “上” (up), “下” (down), and “不” (no) (Lin et al., 2008). The layered approach works very well in solving the polysemy problems in a C2T TTS system. A more difficult case of the polysemy problem will be encountered in this paper. In addition to the above words, another difficult case is “我們” (we). Taiwanese speakers arrive at the correct pronunciation of the word “我們” (we) by deciding whether to include the listener in the pronoun. Unlike Chinese, “我們” (we) has two pronunciations with different meanings when used in Taiwanese. This word can include (1) both the speaker and listener(s) or (2) just the speaker. These variations lead to two different pronunciations in Taiwanese, /lan/ and /ghun/. The Chinese characters for /lan/ and /ghun/ are “咱” (we) and “阮” (we), respectively. The following example helps to illustrate the different meanings. More examples to illustrate these differences will be used later in this section. Assume first that Jeffrey and his younger brother, Jimmy, ask their father to take them to see a movie then go shopping. Jeffrey can say the following to his father: Example 2.14 爸爸你要記得帶我們一起去看電影, 我們看完電影後, 再一起去逛街。 (Daddy, remember to take us to see a movie and go shopping with us after we see the movie.)  48  Ming-Shing Yu and Yih-Jeng Lin  The pronunciation of the first word “我們” (we) in Example 2.14 is /ghun/ in Taiwanese since the word “我們” (we) does not include the listener, Jeffrey’s father. The second instance of “我們” (we), however, is pronounced /lan/ since this instance includes both the speaker and the listener. The pronunciation of “我們” (we) in Example 2.15 is /ghun/ in Taiwanese since the word “我們” (we) includes Jeffrey and Jimmy but does not include the listener, Jeffrey’s father. Example 2.15 爸爸, 我要和弟弟去看電影, 我們看完電影後, 會一起去逛街。 (Daddy, I will go to see a movie with my younger brother, and the two of us will go shopping after seeing the movie.) If a C2T TTS system cannot identify the correct pronunciation of the word “我們” (we), we cannot understand what the synthesized Taiwanese speech means. In a C2T TTS system, it is necessary to decide the correct pronunciation of the Chinese word “我們” (we) in order to have a clear understanding of synthesized Taiwanese speech. Distinguishing different kinds of meanings of “我們” (we) is a semantic problem. It is a difficult but important issue to be overcome in the text analysis module of a C2T TTS system. As there is only one pronunciation of “我們” (we) in Mandarin, a Mandarin TTS system does not need to identify the meaning of the word “我們” (we). To compare this work with the research in Hwang et al. (2000) and Yu et al. (2003), determining the meaning of the word “我們” (we) may be more difficult than solving the non-text symbol problem. A person can determine the relationship between the listeners and the speaker then determine the meaning of the word “我們” (we). It is more difficult, however, for a computer to recognize the relationship between the listeners and speakers in a sentence. Since determining whether listeners are included is a context-sensitive problem, we need to look at the surrounding words, sentences, or paragraphs to find the answer. Let us examine the following Chinese sentence (Example 2.16) to help clarify the problem. Example 2.16 我們必須加緊腳步改善台北市的交通狀況。 (We should press forward to improve the traffic of Taipei City.) It is difficult to determine the Taiwanese pronunciation of the word “我們” (we) in Example 2.16 from the information in this sentence. To get the correct pronunciation of the word “我們” (we), we need to expand the sentence by adding words to the subject, i.e., look forward, and predicate, i.e., look backward. Assume that, when we add words to the subject and the predicate, we have a sentence that looks like Example 2.17: Example 2.17 台北市長馬英九在接見美國記者時指出「: 我們必須加緊腳步改善台北市的 交通狀況。」 (Taipei city mayor Ma Ying-Jeou said that we should press  The Polysemy Problem, an Important Issue in a  49  Chinese to Taiwanese TTS System  forward to improve the traffic of Taipei city when he received some reporters from the USA.)  As the reporters from the USA have no obligation to improve the traffic of Taipei, we can conclude that “我們” (we) does not include them. Therefore, it is safe to say that the correct pronunciation of the word “我們” (we) in Example 2.17 should be /ghun/.  On the other hand, if the sentence reads as in Example 2.18 and context is included, the pronunciation of the word “我們” (we) should be /lan/. We can find some important keywords such as “台北市長” (the Taipei city mayor) and “市府會議” (a meeting of the city government).  Example 2.18  台北市長馬英九在市府會議中指出「: 我們必須加緊腳步改善台北市的交通 狀況。」 (In a meeting of the city government, the Taipei city mayor, Ma Ying-Jeou, said that we should press forward to improve the traffic of Taipei City.)  When disambiguating the meaning of some non-text symbols, such as “/”, “:”, and “-” the keywords to decide the pronunciation of the special symbols may be within a fixed distance from the given symbol. Nevertheless, the keywords can be at any distance from the word “我們” (we), as per Example 2.19. Some words that could be used to determine the pronunciation of “我們” (we), such as “市府會議” (a meeting of the city government), “台北 市長” (the Taipei city mayor), and “馬英九” (Ma Ying-Jeou), are at various distances from “我們” (we).  Example 2.19  在今天的市府會議中，台北市長馬英九提到關於台北市的交通問題時，馬 市長說:「我們必須加緊腳步改善台北市的交通狀況。」 (In a meeting of the city government, the Taipei city mayor, Ma Ying-Jeou, talked about the problem of the traffic in Taipei city. Mayor Ma said that we should press forward to improve the traffic of Taipei city.)  These examples illustrate the importance of determining the proper pronunciation for each word in a C2T TTS system. Compared to other cases of polysemy, determining the proper pronunciation of the word “我們” (we) in Taiwanese is a difficult task. We will focus on solving the polysemy problem of the word “我們” (we) in this paper.  3. Using the Layered Approach to Determine the Pronunciation of “我們” (we)  Lin (2006) showed that the layered approach worked very well in solving the polyphone problem in Chinese. Lin (2006) also showed that using the layered approach to solve the polyphone problem is more accurate than using the CART decision tree. We also show that using the layered approach in solving the polysemy problems of other words has worked well  50  Ming-Shing Yu and Yih-Jeng Lin  in our research (Lin et al., 2008). We will apply the layered approach in solving the polysemy problem of “我們” (we) in Taiwanese.  3.1 Description of Experimental Data  First, we will describe the experimental data used in this paper. The experimental data is comprised of over forty thousand news items from eight news categories, in which 1,546 articles contain the word “我們” (we). The data was downloaded from the Internet from August 23, 2003 to October 21, 2004. The distribution of these articles is shown in Table 1. We determined the pronunciation of each “我們” (we) manually.  Table 1. Distribution of experimental data  News Category  Number of News Items  Number of News Items Containing the word "我們"  Percentage  International News  2242  326  14.5%  Travel News  9273  181  1.9%  Local News  6066  95  1.5%  Entertainment News  3231  408  12.6%  Scientific News  3520  100  2.8%  Social News  4936  160  3.2%  Sports News  2811  193  6.9%  Stock News  8066  83  1.0%  Total Number of News Items  40145  1546  3.9%  As shown in Table 2, in the 1,546 news articles, “我們” occurred 3,195 times. In our experiment, 2,556 samples were randomly chosen for the training data while the other 639 samples were added to the test data. In the training data, there were 1,916 instances with the pronunciation of /ghun/ for the Chinese character “ 阮 ” and 640 instances with the pronunciation of /lan/ for the Chinese character “咱”.  Table 2. Distribution of training and testing data.  Frequency of “我們”  Pronunciation /lan/ Pronunciation /ghun/  Total Frequency  Training data  640  1,916  2,556  Test data  160  479  639  Token frequency of “我們”  800  2,395  3,195  The Polysemy Problem, an Important Issue in a  51  Chinese to Taiwanese TTS System  3.2 Description of Layered Approach Figure 2 shows the layered approach to the polysemy problem with an input test sentence. We use Example 3.1 to illustrate how the layered approach works. Example 3.1 爸爸 告訴 我們 過 馬路 要 小心。 (Dad told us to be careful when crossing the street.) Example 3.1 is an utterance in Chinese with segmentation information. Spaces were used to separate the words in Example 3.1. We want to predict the correct pronunciation for the word “我們” (we) in Example 3.1. As depicted in Figure 2, there are four layers in our approach. We set ( w−2 , w−1, w0 , w+1, w+2 ) as (爸爸,告訴,我們,過,馬路). This pattern (爸爸,告訴,我們,過,馬路) will be the input for Layer 4. Nevertheless, as this pattern is not found in the training data, we cannot decide the pronunciation of “我們” (we) with this pattern. We then use two patterns ( w−2 , w−1, w0 , w+1 ) and ( w−1, w0 , w+1, w+2 ) to derive (爸爸,告訴,我們,過) and (告訴,我們,過, 馬路), respectively, as the inputs for Layer 3. Since we cannot find any patterns in the training data that match either of these patterns, the pronunciation cannot be decided in this layer. Three patterns are used in Layer 2. They are (爸爸,告訴,我們), (告訴,我們,過), and (我 們,過,馬路). We find that the pattern (爸爸,告訴,我們) has appeared in training data. The frequencies are 2 for pronunciation /ghun/ and 1 for /lan/. Thus, the probabilities for the possible pronunciations of “我們” (we) in Example 3.1 are 2/3 for /ghun/ and 1/3 for /lan/. We can conclude that the predicted pronunciation is /ghun/. The layered approach terminates in Layer 2 in this example. If the process did not terminate prematurely, as in this example, it would have terminated in Layer 1, as shown by the dashed lines in Figure 2.  3.3 Results of Using the Layered Approach  We used the experimental data mentioned in 3.1. There are 3,159 samples in the corpus. We used 2,556 samples to train the four layers. The other 639 samples form the test data. Table 3 shows the accuracy of using the layered approach based on word patterns. Thus, the features in the layered approach are words. The results show that the layered approach does not work well. The overall accuracy is 77.00%.  Table 3. Results of using the layered approach with word pattern.  Number of test samples Number of correct samples Accuracy rate  /ghun/  479  445  92.90%  /lan/  160  47  29.38%  Total  639  492  77.00%  52  Ming-Shing Yu and Yih-Jeng Lin  Word Position  (w‐2 , w‐1 ,w0 , w+1,w+2)  Layer 4  (爸爸,告訴,我們,過,馬路)  No pattern found, go to the next layer  Layer 3  (告訴,我們,過,馬路) ＋ (爸爸,告訴,我們,過)  No pattern found, go to the next layer.  Layer 2  (爸爸,告訴,我們) ＋ (告訴,我們,過) + (我們,過,馬路)  Score is (2/3, 1/3). Output /ghun/.  (爸爸,告訴)  Layer 1  ＋ (告訴,我們) ＋ (我們,過) ＋ (過,馬路)  /ghun/=0 /lan/=0 /ghun/=0 /lan/=0 /ghun/=0 /lan/=0 /ghun/=2 /lan/=1 /ghun/=0 /lan/=0 /ghun/=0 /lan/=0 /ghun/=0 /lan/=0 /ghun/=0 /lan/=0 /ghun/=0 /lan/=0 /ghun/=0 /lan/=0  Figure 2. An example applying the layered approach.  The Polysemy Problem, an Important Issue in a  53  Chinese to Taiwanese TTS System  4. Word-based Unigram Language Model  In this section, we propose a word-based unigram language model (WU). Two statistical  results are needed in this model. Statistical results were compiled for (1) the frequency of  appearance for words that appear to the left of “我們” (we) in the training data and (2) the  frequencies for words that appear to the right. Each punctuation mark was treated as a word.  Each testing sample looks like the following:  w-M w-(M-1) … w-2 w-1 我們 w+1 w+2… w+(N-1) w+N  where w-i is the ith word to the left of “我們” (we) and wi is the ith word to the right. The following formulae were used to find four different scores for each testing sample: SuL(/lan/), SuR(/lan/), SuL(/ghun/), and SuR(/ghun/).  C(/lan / &w− j )  SuL( /lan/ ) =  M  ∑ j=1  C(/lan  /  TuL (/lan/) & w− j ) + C(/ghun / & w− j )  (1)  TuL (/lan/)  TuL (/ghun/)  C(/lan / & w+ j )  SuR (/lan/)  =  N ∑ j =1  C(/lan /  &  TuR (/lan/) w+ j ) + C(/ ghun  /  &  w+  j)  (2)  TuR (/lan/)  TuR (/ ghun/)  C(/ghun / & w− j )  M  SuL (/ ghun/)  =  ∑ j=1  C(/lan  /  TuL (/ ghun/) & w− j ) + C(/ghun /  & w− j )  (3)  TuL (/lan/)  TuL (/ ghun/)  C(/ghun / & w+ j )  SuR( /ghun/  )  =  N ∑ j=1  C(/lan /  &  TuR (/ ghun/) w+ j ) + C(/ghun /  &  w+  j  )  (4)  TuR (/lan/)  TuR (/ ghun/)  where  uL  TuL (/lan/) = ∑ C(/lan / &w−l )  (5)  l =1  uL  TuL (/ghun/) = ∑ C(/ghun / &w− p )  (6)  p=1  uR  TuR (/lan/) = ∑ C(/lan / &w+l )  (7)  l =1  uR  TuR(/ghun/) = ∑ C(/ghun / &w+ p )  (8)  p=1  54  Ming-Shing Yu and Yih-Jeng Lin  uL different kinds of words appear on the left side of “我們” (we) in the training corpus. TuL(/lan/) is the total frequency of these uL words in the training data where the pronunciation of “我們” (we) is /lan/. Similarly, TuL(/ghun/) represents the total frequency of uL words where “我們” (we) is pronounced /ghun/. uR is the number of different words that appear to the right side of “我們” (we) in the training corpus. TuR(/lan/) and TuR(/ghun/) are the total frequencies of these uR words in the training data where pronunciation of “我們” (we) is /lan/ and /ghun/, respectively. C(/ghun/&wp) is the frequency that the word wp appears in the training corpus where the pronunciation of “我們” (we) is /ghun /. C(/lan / &w− j ) in (1) means the significance of pronunciation /lan/ of word w-j in training data. TuL (/lan/) Formulae (1) through (4) were applied to each test sample to produce four scores. The scores were SuL(/lan/) for the words to the left of “我們” (we) when the pronunciation was /lan/, SuR(/lan/) for the words to the right when the pronunciation was /lan/, SuL(/ghun/) for the words to the left of “我們” (we) when the pronunciation was /ghun/, and SuR(/ghun/) for the words to the right when the pronunciation was /ghun/. The pronunciation of “我們” (we) is /lan/ if SuL(/lan/)+ SuR (/lan/) > SuL(/ghun/) + SuR (/ghun/). The result is /ghun/ otherwise. The experiments were inside and outside tests. First, we applied WU with the training data mentioned in Section 3.1 to find the best ranges in determining the pronunciation of “我 們” (we). We defined a window as (M, N), where M was number of words to the left of “我們” (we) and N was the number of words to the right. Three hundred and ninety nine (20*20-1=399) different windows were applied when using the WU model. As shown in Table 4, the best result from an inside test was 87.00%, with a window of (17, 10).  The best result when the correct pronunciation of “我們” (we) was /ghun/ was 94.01%, achieved when the window was (12, 6). Nevertheless, the results when the pronunciation was /lan/ and the window was the same were not good. The highest accuracy achieved was 45.48%. Also, as shown in 4th row of Table 4, the best result when applying WU when the pronunciation was /lan/ was just 77.88%, when the window was (19, 14). This shows that WU did not work well when the pronunciation of “我們” (we) was /lan/. Table 4. The results of the inside test of applying WU.  Window Size (M, N)  Accuracy when the  Accuracy when the  pronunciation is /ghun/ pronunciation is /lan/  Overall accuracy  (17, 10)  91.04%  74.92%  87.00%  (12,6)  94.01%  45.48%  81.85%  (19, 14)  88.75%  77.88%  86.03%  We applied WU with a window of (17, 10) for testing data. The overall accuracy of the outside tests was 75.59%. The accuracies were 90.40% and 31.25% when the pronunciations were /ghun/ and /lan/, respectively.  The Polysemy Problem, an Important Issue in a  55  Chinese to Taiwanese TTS System  5. Word-based Long Distance Bigram Language Model  We will bring up the word-based long-distance bigram language model (WLDB) in this section. According to Section 2 of this paper, there are two different meanings for “我們” (we). The two meanings are different in that one includes the listener(s) and the other does not. We propose a modification of the WU model by having two words appear together in the text to clarify the relationship between the speaker and listener(s). Examples of this modification are “台北市長” (the Taipei city mayor) and “美國記者” (the reporter(s) from the USA) in Example 2.17 and “台北市長” and “市府會議” (a city government meeting) in Examples 2.18 and 2.19.  For each testing sample, w-M w-(M-1) … w-2 w-1 我們 w+1 w+2… w+(N-1) w+N .  The following formulae were used to find four scores for each testing sample, SbL(/lan/),  SbR(/lan/), SbL(/ghun/), and SbR(/ghun/).  C (/lan / & w−i & w− j )  SbL  ( / lan /)  =  M ∑ i =1  M ∑ j=i  C (/lan  /  & w−i  &  TbL (/lan /) w− j ) + C (/ ghun  /  & w−i  &  w−  j  )  (9)  TbL C (/lan /)  TbL C (/ ghun /)  C(/lan / &wi & wj )  SbR (/lan/)  =  N ∑ i=1  N ∑ j=i  C(/lan  /  &w+i  &  TbR (/lan/) w+ j ) + C(/ghun  /  &w+i  & w+  j  )  (10)  TbR (/lan/)  TbR (/ghun/)  C(/ghun / &w−i &w− j )  SbL  (/  ghun/)  =  M ∑ i=1  M ∑ j=i  C(/  ghun  /  &w−i  TbL (/ ghun/) &w− j ) + C(/lan  /  &w−i  &  w−  j  )  (11)  TbL (/ ghun/)  TbL (/lan/)  C(/ghun / &wi & wj )  SbR  (/ ghun/)  =  N ∑ i=1  N ∑ j=i  C(/ ghun  /  &w+i  TbR (/ghun/) & w+ j ) + C(/lan  /  &w+i  &  w+  j  )  (12)  TbR (/ghun/)  TbR (/lan/)  where  bL bL  TbL (/lan/) = ∑ ∑ C(/lan / &w−l & w−k )  (13)  l=1k =l  bR bR  TbR (/lan/) = ∑ ∑ C(/lan / &w+l & w+k )  (14)  l =1 k =l  bL bL  TbL (/ ghun/) = ∑ ∑ C(/ ghun / &w−l & w−k )  (15)  l =1 k =l  56  Ming-Shing Yu and Yih-Jeng Lin  bR bR  TbR (/ ghun/) = ∑ ∑ C(/ ghun / &wl & wk )  (16)  l=1k =l  We assume that bL different words appear to the left of “我們” (we) in the training corpus and bR different words appear to the right. Formulae 9, 10, 11, and 12 were applied to each test sample, and they produced four scores. C(/lan/&wi&wj) in (9) is the frequency at which words wi and wj appear in the training corpus when the pronunciation of “我們” (we) is /lan/. SbL(/lan/) is the score for the words to the left of “我們” (we) when the pronunciation is /lan/, and SbR(/lan/) is the score for the words to the right. Similarly, SbL(/ghun/) and SbR(/ghun/) represent the scores for the words to the left and right, respectively, when “我們” (we) is pronounced /ghun/. In summary, the pronunciation of the word “我們” (we) is /lan/ if SbL(/lan/) + SbR (/lan/) > SbL(/ghun/) + SbR (/ghun/). The pronunciation is /ghun/ otherwise.  We applied WLDB with the training data mentioned in Section 3.1 to find the best ranges in determining the pronunciation of “我們” (we). We defined a window of (M, N), where M was the number of words to the left and N was number of words to the right. Three hundred and sixty (19*19-1=360) different windows were applied in the analysis of using the WLDB model. As shown in the 2nd row of Table 5, the best result of the inside test was 94.25% with the best range being 11 words to the left of “我們” (we) and 7 words to the right.  The best result when the correct pronunciation of “我們” (we) was /lan/ was 99.87%, when the window was (11, 5). Nevertheless, the result for /ghun/ with the same window was not good. The highest accuracy achieved was 89.69%. As shown in the 3rd row of Table 5, the best result when applying WLDB when the pronunciation was /ghun/ was 93.48%, when the window was (4, 13). This shows that WLDB does not work well when the pronunciation of “我們” (we) is /ghun/.  Table 5. The results of the inside test of applying WLDB.  Window Size (kL, kR) (11,7)  Accuracy when the pronunciation is /ghun/ 93.33%  Accuracy when the pronunciation is /lan/ 97.04%  Overall accuracy 94.25%  (4, 13)  93.48%  93.61%  93.52%  (11,5)  89.69%  99.87%  92.15%  We applied the WLDB model to the test data using a window of (11, 7). The overall accuracy of outside tests was 85.72%. The accuracies were 83.26% and 93.10% when the pronunciations were /ghun/ and /lan/, respectively.  6. The combined Approach  Based on the results from the two models, WU and WLDB, we can draw the following  The Polysemy Problem, an Important Issue in a  57  Chinese to Taiwanese TTS System  conclusions: the word-based long distance bigram language model is good when the pronunciation is /lan/, while the word-based unigram language model works well when the pronunciation is /ghun/. In this section, we propose combining the models to achieve better results. According to the inside experimental results shown in Table 4 and Table 5, we will combine the WU model with a window of (12, 6) and the WLDB model with a window of (11, 5) as our combined approach. This combination of WU and WLDB is similar to the approach used by Yu and Huang. We will try to find the possibility of making a correct choice when using WU or WLDB, which will be termed “confidence”. We will adopt the output of the method with higher confidence. 6.1 Confidence Measure The first step in this process is to find a confidence curve for each model. The goal is to estimate the confidence for each approach and assess the difference. The higher score is more likely to be the correct answer. To do so, we measure the accuracy of each division and use a regression to estimate the confidence measure. Algorithm 1, below, will be used to find the confidence curve for the word-based unigram language model. As the total number of words in each input sample is not constant, we must first normalize the scores Sui(/lan/) and Sui(/ghun/). We will find the precision rates (PRk) in the interval [0, 1] for |NSui(/ghun/)- NSui(/lan/)| in Step 2 of Algorithm 1 for each i. We then find a regression curve for the PRk. The regression curve is used to estimate the probability of making a correct decision when using WU. Therefore, it follows that, the higher the probability is, the greater the confidence we can have in the results from WU. Algorithm 1: Finding the confidence curve of WU. Input: The score for each training sample, Sui(/lan/) and Sui(/ghun/), where i=1,2,3, …, n and n is the number of training samples. Output: A function for the confidence curve for the given Sui(/lan/) and Sui(/ghun/), i=1,2,3, …, n. Algorithm: Step 1: Normalize Sui(/lan/) and Sui(/ghun/) for each training sample i using the following formula: NSui(/lan/)=Sui(/lan/)/(Total number of words in training sample i) NSui(/ghun/)=Sui(/ghun/)/(Total number of words in training sample i) Step 2: Let di=| NSui(/ghun/)- NSui(/lan/)| and let D={d1, d2,…,dn}. Find the accuracy rate for each interval using the following formula: PRk= Ck/Nk, k=1, 2, …, 18 Here, Ck is the number of correct conjectures of training sample i with (k-1)/18 ‫أ‬ di < (k+1)/18, and Nk is the number of training sample i with (k-1)/18 ‫ أ‬d i< (k+1)/18. Step 3: Find a regression curve for PR1, PR2, …, PR18. Output the function of the regression curve.  58  Ming-Shing Yu and Yih-Jeng Lin  Precision  1.2  
Topic modeling for information retrieval (IR) has attracted significant attention and demonstrated good performance in a wide variety of tasks over the years. In this paper, we first present a comprehensive comparison of various topic modeling approaches, including the so-called document topic models (DTM) and word topic models (WTM), for Chinese spoken document retrieval (SDR). Moreover, different granularities of index features, including words, subword units, and their combinations, are also exploited to work in conjunction with various extensions of topic modeling presented in this paper, so as to alleviate SDR performance degradation caused by speech recognition errors. All of the experiments were performed on the TDT Chinese collection. Keywords: Information Retrieval, Document Topic Models, Word Topic Models, Spoken Document Retrieval. 1. Introduction Due to the advances in computer technology and the proliferation of Internet activity, huge volumes of multimedia data, such as text files, broadcast radio and television programs, lectures, and digital archives, are continuously growing and filling networks. Development of intelligent and efficient information retrieval techniques to provide people with easy access to all kinds of information is now becoming more and more emphasized. Meanwhile, with the rapid evolution of speech recognition technology, substantial efforts and very encouraging results on spoken document retrieval (SDR) also have been demonstrated in the recent past. Although most retrieval systems participating in the TREC-SDR evaluations claimed that speech recognition errors do not seem to cause much adverse effect on SDR performance + Voice Division Research Center, Delta Electronics E-mail: shlin@csie.ntnu.edu.tw # Department of Computer Science & Information Engineering, National Taiwan Normal University E-mail: berlin@csie.ntnu.edu.tw ∗ Corresponding authors.  66  Shih-Hsiang Lin and Berlin Chen  when merely using imperfect recognition transcripts derived from one-best recognition results from a speech recognizer (Garofolo et al., 2000; Chelba et al., 2008), this is probably attributed to the fact that the TREC-style test queries tend to be quite long and contain different words describing similar concepts that can help the queries match their relevant spoken documents. Furthermore, a query word (or phrase) may occur repeatedly (more than once) within a relevant spoken document, and it is not always the case that all of the occurrences of the word would be misrecognized totally as other words. We, however, believe that SDR would still present a challenge in situations where the queries are relatively short and there exists severe deviation in word usage between the queries and spoken documents. Among several promising information retrieval approaches, statistical language modeling (LM) (Ponte & Croft, 1998), aiming to capture the regularity in human natural language and quantify the acceptability of a given word sequence, has continuously been a focus of active research in the last decade (Miller et al., 1999; Hofmann, 2001). The basic idea is that each individual document in the collection is treated as a probabilistic language model for generating a given query. A document is deemed to be relevant to a query if its corresponding document language model generates the query with higher likelihood. In practice, the relevance measure for the LM approach is usually computed by two different matching strategies, namely, literal term matching and concept matching (Lee & Chen, 2005). The unigram language model (ULM) is perhaps the most representative example for literal term matching strategy (Miller et al., 1999). In the ULM approach, each document is interpreted as a generative model composed of a mixture of unigram (multinomial) distributions for observing a query, while the query is regarded as observations, expressed as a sequence of indexing words (or terms). Nevertheless, these approaches would suffer from the problems of word usage diversity, which might make the retrieval performance of the system degrade severely as a given query and its relevant documents are using quite a different set of words. In contrast, the concept matching strategy tries to explore the topic information conveyed in the query and documents. Based on this, the retrieval process is performed. The probabilistic latent semantic analysis (PLSA) (Hofmann, 2001) and the latent Dirichlet allocation (LDA) (Blei et al., 2003) are often considered to be two basic representatives of this category. They both introduce a set of latent topic variables to describe the “word-document” co-occurrence characteristics. More specifically, the relevance between a query and a document is not computed directly based on the frequency of the query words occurring in the document, but instead based on the frequency of these words appearing in the latent topics as well as the likelihood that the document generates those respective topics, which exhibits some sort of concept matching. Further, although there have been many follow-up studies and extensions of PLSA and LDA, it has been shown that more sophisticated (or complicated) topic models, such as the pachinko  A Comparative Study of Methods for Topic Modeling in  67  Spoken Document Retrieval  allocation model (PAM) and correlated topic model (CTM), do not necessarily offer further retrieval benefits (Zhai, 2008; Blei & Lafferty, 2009). On the other hand, rather than treating each document as a whole as a document topic model (DTM), such as PLSA and LDA, the word topic model (WTM) (Chen, 2009) attempts to discover the long-span co-occurrence dependence “between words” through a set of latent topics, while each document in the collection consequently can be represented as a composite WTM model in an efficient way for predicting an observed query. Interested readers can refer to Griffiths et al. (2007), Zhai (2008), and Blei and Lafferty (2009) for a thorough and updated overview of the major topic-based language models that have been successfully developed and applied to various IR tasks. Although most of the above approaches can be equally applied to both text and spoken documents, the latter presents unique difficulties, such as speech recognition errors, problems posed by spontaneous speech, and redundant information. A straightforward remedy, apart from the conventional approaches target at improving recognition accuracy, is to develop more robust representations of spoken documents for spoken document retrieval (SDR). For example, multiple recognition hypotheses, beyond the top scoring ones, are expected to provide alternative representations for the confusing portions of the spoken documents (Chelba et al., 2008; Chia et al., 2008). Another school of thought attempts to leverage subword units, as well as the combination of words and subword units, for representing the spoken documents, which also has been shown beneficial for SDR. The reason for the fusion of word- and subword-level information is that incorrectly recognized spoken words often include several subword units that are correctly recognized. Hence, the retrieval process based on subword-level representations may take advantage of partial matching (Lin & Chen, 2009). With the above inspiration in mind, we first compare the structural characteristics of various topic models for Chinese SDR, including PLSA and LDA, as well as WTM. The utility of these models is thoroughly examined using both long and short test queries. Moreover, different granularities of index features, including words, subword units, and their combinations, are also exploited to work in conjunction with various extensions of topic modeling presented in this paper, so as to alleviate SDR performance degradation caused by imperfect recognition transcripts. To our knowledge, there is little literature on leveraging various topic decompositions together with various granularities of index features for topic modeling in SDR. The rest of this paper is structured as follows. Section 2 elucidates the structural characteristics of the different types of topic models for the retrieval purpose. Section 3 discusses two different extensions of topic modeling. Section 4 describes the spoken document collection used in this paper, as well as the experimental setup. A series of experiments and associated discussions are presented in Section 5. Finally, Section 6 concludes this paper and  68  Shih-Hsiang Lin and Berlin Chen  suggests possible avenues for future work. 2. Topic Models In this section, we first describe the probabilistic generative framework for information retrieval. We then briefly review the document topic models (DTM), including the probabilistic latent semantic analysis (PLSA) (Hofmann, 2001) and the latent Dirichlet model (LDA) (Blei et al., 2003; Wei & Croft, 2006), followed by an introduction to the word topic model (WTM) (Chen, 2009), as well as the word Dirichlet topic model (WDTM).  2.1 Probabilistic Generative Framework  When the language modeling approach is applied to IR, it basically makes use of a  probabilistic generative framework for ranking each document D in the collection given a query Q , which can be expressed by P ( D Q) . By applying Bayes’ theorem, this ranking criterion can be approximated by the likelihood of Q generated by D , i.e., P (Q D) , when we assume that the prior probability of each document P ( D) is uniformly distributed. For  this idea to work, each document D is treated as a probabilistic language model MD for generating the query. Furthermore, if the query Q is treated as a sequence of words (or terms), Q = w1w2 … wN , where the query words are assumed to be conditionally independent given the document model MD and their order is also assumed to be of no importance (i.e., the so-called “bag-of-words” assumption), the relevance measure P (Q D) can be further  decomposed as a product of the probabilities of the query words generated by the document:  P (Q D) = ∏ ( ) P wi MD c(wi ,Q) ,  (1)  wi∈Q  where c(wi ,Q) is the number of times that each distinct word wi occurs in Q . The document ranking problem has now been reduced to the problem of constructing the document model P(wi MD ) . ( ) The simplest way to construct P wi MD is based on literal term matching, or using the unigram language model (ULM), where each document of the collection can respectively ( ) offer a unigram distribution for observing a query word, i.e., PULM wi MD , which is estimated on the basis of the words occurring in the document:  PULM  ( wi  |  MD  )  =  c  (  wi , D  D)  ,  (2)  where c ( wi , D) is the number of times that word wi occurs in the document D and D is the number of words in the document. In order to avoid the problem of zero probability, the ULM is usually smoothed by a unigram distribution estimated from a general collection, i.e., ( ) PULM wi MC :  A Comparative Study of Methods for Topic Modeling in  69  Spoken Document Retrieval  ( PˆULM wi D) = λ ⋅ PULM (wi MD ) + (1− λ ) ⋅ PULM (wi MC ),  (3)  where λ is a weighting parameter. It turns out that a document with more query words ( ) occurring in it would tend to receive a higher probability; further, the use of PULM wi MC to some extent can help deemphasize common (non-informative) words but instead put more emphasis on discriminative (or informative) words for the purpose of document ranking (Zhai, ( ) ( ) 2008). In the following, PULM wi MD and PULM wi MC will be termed the document model and the background model, respectively.  2.2 Document Topic Model (DTM)  As mentioned earlier, there probably would be word usage mismatch between a query and a  spoken document, even if they are topically related to each other. Therefore, instead of  constructing the document model based on the literal term information, we can exploit  probabilistic topic models to represent each spoken document through a latent topic space  (Blei et al., 2010). In this spectrum of research, each document D is regarded as a document topic model (DTM), consisting of a set of K shared latent topics {T1,…,Tk ,…,TK } with ( ) document-specific weights P Tk MD , where each topic Tk in turn offers a unigram ( ) distribution P wi Tk for observing an arbitrary word of the language. For example, in the PLSA model, the probability of a word wi generated by a document D is expressed by:  ( ) PPLSA wi ΜD  =  K ∑  P (wi  Tk  ) P (Tk  ΜD ).  (4)  k =1  The key idea we wish to illustrate here is that, for PLSA, the relevance measure of a query word wi and a document D is not computed directly based on the frequency of wi occurring in D , but instead based on the frequency of wi in the latent topic Tk as well as the likelihood that D generates the respective topic Tk , which in fact exhibits some sort of concept matching. A document is believed to be more relevant to the query if it has higher weights on some topics and the query words also happen to appear frequently in these topics.  In the practical implementation of PLSA, the corresponding DTM models are usually  trained in an unsupervised way by maximizing the total log-likelihood of the document collection D in terms of the unigram ( ) PPLSA wi MD of all words wi observed in the document collection, or, more specifically, the total likelihood of all documents generated by  their own DTM models:  ( ) LPLSA = ∏ PPLSA D MD  D∈D  ( ) = ∏ ∏ PPLSA wi MD c(wi ,D).  (5)  D∈D wi∈D  70  Shih-Hsiang Lin and Berlin Chen  We can first use the K-means algorithm to partition the entire document collection into K ( ) topical classes. Hence, the initial topical unigram distribution P wi Tk for a topical cluster can be estimated according to the underlying statistical characteristics of the document being ( ) assigned to it and the probabilities for each document generating the topics, i.e., P Tk MD , are measured according to its proximity to the centroid of each respective cluster. Then, (5) can be iteratively optimized by the following three expectation-maximization (EM) (Dempster et al., 1977) updating equations:  - E (Expectation) Step  ( ) ( ) P(Tk  |  wi , MD )  =  P (wi | Tk ) P (Tk | MD ) ∑Tk' P wi | Tk' P Tk' | MD  ,  (6)  - M (Maximization) Step  Pˆ (  wi  |  Tk  )  =  ∑D c(wi , D) P ∑w∑D c(w, D)  (Tk | wi , MD ) P(Tk | w, MD  )  ,  (7)  Pˆ (Tk  |  MD  )  =  ∑wc(w, D) P (Tk | w, MD ∑w'c (w', D)  )  ,  (8)  where P (Tk | wi , MD ) is the probability that the latent topic Tk occurs given the word wi and the document model MD , which is computed using the probability quantities P ( wi | Tk ) and P (Tk | MD ) obtained in the previous training iteration.  On the other hand, LDA, having a formula analogous to PLSA for document ranking, is  regarded as a generalization of PLSA and has enjoyed considerable success in a wide variety  of natural language processing (NLP) tasks. LDA differs from PLSA mainly in the inference  of model parameters: PLSA assumes the model parameters are fixed and unknown; while  LDA places additional a priori constraints on the model parameters, i.e., thinking of them as  random variables that follow Dirichlet distributions. In other words, the total log-likelihood of  all documents generated by LDA models is defined as:  ( ) ( ) K  ⎛D K  ⎞  LLDA  =  ∫∫  ∏ P (ϕz z =1  |  β  )∏ D∈D  p (θD  |  α  )  ⎜⎜⎝  ∏ i =1  ∑ k =1  P  wi  Tk ,ϕz  P Tk θD  ⎟⎟⎠ dθ dϕ  (9)  where θd and ϕz are multinomial distributions with Dirichlet parameter α and β , respectively, and D is the number of words in the document D . LDA possesses fully consistent generative semantics by treating the topic mixture distribution as a K -parameter hidden random variable rather than a large set of individual parameters that are explicitly linked to the training set (Blei et al., 2003). Compared to PLSA, LDA overcomes the problem of overfitting and the problem of generating new documents incurred by PLSA.  A Comparative Study of Methods for Topic Modeling in  71  Spoken Document Retrieval  Since LDA has a more complex form for model optimization, which is difficult to be  solved by exact inference, several approximate inference algorithms, such as the variational  Bayes approximation (Blei et al., 2003), the expectation propagation method (Ypma et al.,  2002), and the Gibbs sampling algorithm (Griffiths, 2004), have been proposed in the  literature for estimating the model parameters of LDA. In this paper, we adopt the Gibbs  sampling algorithm, where θ and ϕ are marginalized out and only the latent variables Tk are sampled, to infer the model parameters. Then, the probability of a word wi generated by a document D in the LDA model is expressed by:  ( ) ( ) ( ) K ∑ PLDA wi φˆ,θˆ, Μ D = P wi Tk ,φˆ P Tk θˆ, Μ D ,  (10)  k =1  where ϕˆ and θˆ are the posterior estimates of θ and ϕ , respectively. We refer the  readers to Griffiths and Steyvers (2004) for a better understanding of the detailed inference  procedure.  2.3 Word Topic Model (WTM)  Rather than treating each document in the collection as a document topic model, we can  regard each word w j of the language as a word topic model (WTM). To get to this point, all words are assumed to share the same set of latent topic distributions but have different weights  over these topics. The WTM model of each word w j for predicting the occurrence of a particular word wi can be expressed by:  ( ) ( ) K  PWTM  wi | Mwj  = ∑ P ( wi | Tk ) P k =1  Tk | Mwj  ,  (11)  ( ) ( ) where P wi Tk and P Tk Mwj are the probability of a word wi occurring in a specific  latent topic Tk and the probability of the topic Tk conditioned on Mwj , respectively. Then,  each document naturally can be viewed as a composite WTM, while the relevance measure  between a word wi and a document D can be expressed by:  ( ) ( ) ( ) PWTM  wi MD  = ∑ PWTM wj∈D  wi Mwj  PULM  wj MD ,  (12)  The resulting composite WTM model for D , in a sense, can be thought of as a kind of language model for translating words in D to wi . The model parameters of WTM can be inferred by unsupervised training as well. More precisely, each WTM model Mwj can be trained by concatenating those words occurring in the vicinity of (or a context window of size S around) each occurrence of wj , which are postulated to be relevant to wj , to form a relevant observation sequence Owj for training Mwj . The words in Owj are also assumed to be conditionally independent, given Mwj .  72  Shih-Hsiang Lin and Berlin Chen  Therefore, the WTM models of the words in the vocabulary set w can be estimated by  maximizing the total likelihood of their corresponding relevant observation sequences  generated by themselves:  LW T M  ( ) ( ) ( ) = ∏ PWTM w j∈w  Owj M wj  = ∏ ∏ PW T M w j ∈w wi∈Ow j  wi M w j  c wi ,Ow j ,  (13)  Then, the parameters of each WTM model can be estimated using the following EM updating formulae:  - E (Expectation) Step  ( ) ∑ ( )( ( ) ) P Tk | wi,Mwj =  P(wi | Tk )P Tk | Mwj , Tk' P wi | Tk' P Tk' | Mwj  (14)  - M (Maximization) Step  ( ) ( ) Pˆ (wi | Tk ) =  ∑wj∈w c wi ,Owj P Tk | wi , Mwj  ,  ( ) ( ) ∑wl∈w ∑wn∈Owl c wn ,Owl P Tk | wn , Mwl  (15)  ( ) ( ( ) ( ) ) Pˆ Tk | Mwj  = ∑w∈Owj c w,Owj P Tk | w, Mwj ∑w'c w',Owj  .  (16)  Along a similar vein to the LDA model, word Dirichlet topic model (WDTM) can be derived as well. WDTM essentially has the same ranking formula as WTM, except that it further assumes the model parameters are governed by some Dirichlet distributions.  2.4 Analytic Comparisons between DTM and WTM DTM (PLSA or LDA) and WTM (WTM or WDTM) can be analyzed from several perspectives. First, DTM models the co-occurrence relationship between words and documents, while WTM models the co-occurrence relationship between words in the collection. More explicitly, we may compare DTM and WTM through nonnegative (or probabilistic) matrix factorizations, as depicted in Figure 1. For DTM models, each column of Matrix A denotes the probability vector of a document in the collection, which offers a probability for every word occurring in the document. For WTM models, each column of Matrix B is the probability vector of a word’s vicinity, which offers a probability for observing every other word occurring in its vicinity. Both Matrices A and B can be decomposed into two matrices standing for the topic mixture components and the topic mixture weights, respectively.  A Comparative Study of Methods for Topic Modeling in  73  Spoken Document Retrieval  do cument s  topics  document s  topics  words  words  DTM  A  ≈  G×  HT  “ word-document ” co-occurrence matrix  mixture components  mixture weights  WTM  vicinit ies of words  B  ≈  topics Q×  vicinit ies of words Q'T  topics  words  words  c“coow--ooo““rccdwwcc-oouudrrrroddrrcee--nnwwumccooeerreddnmm””taa”tt &rriixx  mixture components  mixture weights  Figure 1. A schematic illustration for the matrix factorizations of DTM and WTM.  Furthermore, the topic mixture weights of DTM for a new document have to be estimated online using EM or other more sophisticated algorithms, which would be time-consuming; on the contrary, the topic mixture weights of WTM for a new document D can be obtained on the basis of the topic mixture weights of all words involved in the document without using a complex inference procedure. Finally, if the context window for modeling the vicinity information of WTM is reduced to one word ( S = 1), WTM can be either degenerated to a unigram model as the latent topic number K is set to 1, or viewed as analogous to a bigram model (as K = V ) or an aggregate Markov model (as 1 < K < V ). Thus, with some appropriate values of S and K being chosen, we can show that WTM seems to be a good method of approximating the bigram or skip-bigram models for sparse data (Chen, 2009).  3. Extensions of Topic Modeling  3.1 Hybrid of DTM and WTM As mentioned in the previous section, DTM and WTM are different from each other in their fundamental premises to determine a hidden topical decomposition of the document collection through the exploration of the topical information underlying the “word-document” or “word-word” co-occurrence relationships, respectively. Thus, we may fuse the results of the two different topical decompositions from DTM and WTM together for better ranking of spoken documents.  74  Shih-Hsiang Lin and Berlin Chen  One possible method is to train each of these two models individually and linearly combine their respective document-ranking scores in the log-likelihood domain subsequently (called “Individual Topics” hereafter). Nevertheless, this approach could not arrive at the same ( ) set of topic components (i.e., P wi Tk , k = 1,…, K ) that are potentially associated with the spoken document collection. Alternatively, we may seek to conduct a single (or unique) topical decomposition of the spoken document collection by simultaneously exploiting these two types of co-occurrence relationships (called “Shared Topics” hereafter). This approach tries to estimate the DTM and WTM model parameters by jointly maximizing the total likelihood of words occurring in the spoken documents and the total likelihood of the words occurring in the vicinities of arbitrary words in the vocabulary. A pictorial representation for the probabilistic matrix decomposition of the spoken document collection with this approach is illustrated in Figure 2, where each column of the left hand side matrix denotes either the probability vector of a document in the collection, which offers a probability for every word occurring in the document (i.e., DTM), or the probability vector of the vicinity of a word in the vocabulary, which offers a probability for observing every other word occurring in the vicinity (i.e., WTM). Then, this matrix can be decomposed into two matrices standing for the topic mixture components (i.e., F ) and the topic mixture weights (i.e., H and Q ' ), respectively.  documents vicinities of words  topics  documents vicinities of words  words words topics  AB≈  F × HT Q'T  “ word-document” &  “ word-word”  mixture components  mixture weights  co-occurrence matrix  Figure 2. A schematic illustration for the matrix factorization of hybrids of  DTM and WTM.  3.2 Topic Modeling with Subword-level Units In this paper, we also investigate leveraging subword-level information cues for topic modeling in Chinese SDR. To do this, syllable pairs are taken as basic units for indexing instead of words. In the following paragraphs, we will elucidate the reasons for using syllable-level features for the retrieval purpose before describing how they can be integrated into the DTM and WTM models.  A Comparative Study of Methods for Topic Modeling in  75  Spoken Document Retrieval  Mandarin Chinese is phonologically compact; an inventory of about 400 base syllables provides full phonological coverage of Mandarin audio if the differences in tones are disregarded. On the other hand, an inventory of about 13,000 characters provides full textual coverage of written Chinese. Each word is composed of one or more characters, and each character is pronounced as a monosyllable and is a morpheme with its own meaning. As a result, new words are generated easily by combining a few characters. Such new words also include many proper nouns, like personal names, organization names, and domain-specific terms. The construction of words from characters is often quite flexible. One phenomenon is that different words describing the same or similar concepts can be constructed of slightly different characters. Another phenomenon is that a longer word can be arbitrarily abbreviated into a shorter word. Moreover, there is a many-to-many mapping between characters and syllables; a foreign word can be translated into different Chinese words based on its pronunciation, while different translations usually have some syllables in common, or may have exactly the same syllables. Statistical evidence also shows that, in the Chinese language, about 91% of the top 5,000 most frequently used polysyllabic words are bi-syllabic, i.e., they are pronounced as a segment of two syllables. Therefore, such syllable segments (or syllable pairs) definitely carry a plurality of linguistic information and make great sense to be used as important index terms. The characteristics of the Chinese language mentioned above lead to some special considerations for SDR. Word-level index features possess more semantic information than syllable-level ones; thus, word-based retrieval enhances the precision. On the other hand, syllable-level index features are more robust against the Chinese word tokenization ambiguity, Chinese homophone ambiguity, open vocabulary problem, and speech recognition errors; therefore, the syllable-level information would enhance the recall. Accordingly, there is good reason to fuse the information obtained from index features of different levels. It has been shown that using syllable pairs as the index terms is very effective for Chinese SDR, and the retrieval performance can be further improved by incorporating the information from word-level index features. In this paper, both the manual transcript and the recognition transcript of each spoken document, in the form of a word stream, were automatically converted into a stream of overlapping syllable pairs. Then, all of the distinct syllable pairs occurring in the spoken document collection were identified to form an indexing vocabulary of syllable pairs. Topic modeling with the syllable-level information can be fulfilled in two ways. One is to simply use syllable pairs, as a replacement for words, to represent the spoken documents and to construct the associated probabilistic latent topic distributions for DTM and WTM accordingly. The other is to jointly utilize both words and syllable pairs, as two types of index terms, to represent the spoken documents, as well as to construct the associated probabilistic latent topic  76  Shih-Hsiang Lin and Berlin Chen  distributions. To this end, each spoken document is represented virtually with a spliced text stream, consisting of both words and syllable pairs. Figure 3 takes DTM as an example to graphically illustrate such an attempt, which is expected to discover correlated topic patterns of the spoken document collection when using both word- and syllable-level index features simultaneously.  do cument s  topics  do cument s  ×  HT  A  ≈  G  mixture weights  syllable pairs words syllable pairs words topics  “ word-document” & “ syllable pair-document” co-occurrence matrix  mixture components  Figure 3. A schematic illustration for the matrix factorization of DTM, jointly using words and syllable pairs as the index terms.  4. Experimental Setup  4.1 Corpus and Evaluation Metric We used the Topic Detection and Tracking (TDT-2) collection for the SDR task (LDC, 2000). TDT is a DARPA sponsored program where participating sites tackle tasks, such as identifying the first time a news story is reported on a given topic or grouping news stories with similar topics from audio and textual streams of newswire data. Both the English and Mandarin Chinese corpora have been studied in the recent past. The TDT corpora have also been used for cross-language spoken document retrieval (CLSDR) in the Mandarin English Information (MEI) Project (Meng et al., 2004). In this paper, we used the Mandarin Chinese collections of the TDT corpora for the retrospective retrieval task, such that the statistics for the entire document collection was obtainable. Chinese text news stories from Xinhua News Agency were compiled to form the test queries (or query exemplars). More specifically, in the following experiments, we will either use a whole text news story as “long” query or merely extract the title field from a text news story to form a relatively “short” query. The Mandarin news stories (audio) from Voice of America news broadcasts were used as the spoken documents. All news stories were exhaustively tagged with event-based topic labels, which merely serve as the relevance judgments for performance evaluation and will not be utilized in the training of topic models (cf. Section 2). Table 1 shows some basic statistics about the corpus used in this paper. The Dragon large-vocabulary continuous speech  A Comparative Study of Methods for Topic Modeling in  77  Spoken Document Retrieval  recognizer provided Chinese word transcripts for our Mandarin audio collections. To assess the performance level of the recognizer, we spot-checked a fraction of the spoken document collection set (about 40 hours), and obtained error rates of 35.38% (in word), 17.69% (in character), and 13.00% (in syllable). Since Dragon’s lexicon is not available, we augmented the LDC Mandarin Chinese Lexicon with 24,000 words extracted from Dragon’s word recognition output, and used the augmented LDC lexicon (about 51,000 words) to tokenize the manual transcripts for computing error rates. We also used this augmented LDC lexicon to tokenize the text queries in the retrieval experiments.  Table 1. Statistics for TDT-2 Collections Used for Spoken Document Retrieval  # Spoken documents  2,265 stories 46.03 hours of audio  # Distinct test queries  16 Xinhua text stories (Topics 20001∼20096)  Min.  Max.  Med.  Mean  Document length (in characters)  23  4841  153  287  Length of long query (in characters)  183  2623  329  533  Length of short query (in characters)  8  27  13  14  # Relevant documents per test query  2  95  13  29  The retrieval results are expressed in terms of non-interpolated mean average precision  (mAP) following the TREC evaluation (Harman, 1995), which is computed by the following  equation:  mAP  =  
This study examines the acoustic variability in four 4-year-old children: two with cerebral palsy (CP) and two typically developing (TD). One recording from each child, collected from the picture-naming task and spontaneous interaction with adults was analyzed. Acoustic vowel space, pitch and speech rate in their production were investigated. Study findings indicated the following: 1) children with CP have a smaller vowel space than TD children, and there was a scattered distribution of the formant frequencies in CP; 2) children with CP tend to spend more time producing the utterances and their production of tones was unstable; and 3) both the speech rate and speech intelligibility in CP were lower. Future studies are needed to verify these preliminary findings. The variability features in the production of children with CP provide important references in speech therapy. Keywords: Mandarin-speaking children, cerebral palsy, vowel space, fundamental frequency, speech rate 1. Introduction Cerebral palsy is a common speech motor disability in children, and an umbrella term to indicate a neurologic developmental condition that affects individuals from early childhood throughout their lifespan [1]. Due to the neurologic factors, children with cerebral palsy tend to have several types of speech deficits. According to a previous study [2], 60% of children with CP have some type of speech deficits, among which dysarthria, the most common 15  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) speech disorder found in individuals with CP, has received more attention. This study focuses on the acoustic aspects of dysarthria: vowel space, pitch, and speech rate. Vowel space is an acoustic measure that indicates the jaw’s coordination and the tongue’s controlling ability [3]. Because of poor muscle coordination, individuals with dysarthria tend to have a smaller vowel space, which influences the accuracy of articulation and reduces the intelligibility of their speech. Moreover, because dysarthric speakers have a hard time controlling their respiratory and the laryngeal mechanisms, it is difficult for them to produce correct tones, which plays an important role in the intelligibility of tonal languages ([2], [4], [5]). Furthermore, the stability of the speech rate affects listeners’ intelligibility, but dysarthric speakers usually present a rate disturbance [6]. Therefore, these three acoustic measures are vital to the speech of the individual with dysarthria. By analyzing these three measures, this study provides a preliminary index of cerebral palsied speech and a direction for speech-language intervention. 2. Literature review 2.1 Acoustic vowel space Many researchers have used vowel space as an index for the size of the vowel articulatory working space, the accuracy of vowel articulation, and the tongue’s controlling ability ([3], [7]). Moreover, the influences of dysarthria and unclear speech on the sizes of vowel areas and the relationship between vowel space and speech intelligibility were investigated ([8], [9]). According to a previous study [3], vowel area formed by the 1st formant (F1) and the 2nd formant (F2) can reflect the control ability and mobility of the tongue. In other words, if the mobility of the tongue is abnormal, the F1-F2 area would be reduced. In Higgins and Hodge’s [10] study with 12 participants, six children had been diagnosed with dysarthria, and six were controls. They compared the vowel spaces of the corner vowels /a/, /i/, /ª/ and /u/ produced by the two groups and found that the vowel space of children with dysarthria is smaller. Jeng [9] indicated that the vowel quadrilaterals of the controls are more uniform, while CP groups’ vowel quadrilaterals are variable because of the non-uniform F1-F2 formant values. People with dysarthria tend to speak at a slower rate or at a louder volume to make their speech intelligible, which may expand the vowel space [11]. In clinical treatment, controlling the speech rate is widely employed by speech therapists, and the effects of slowing the speech rate on vowel space and speech intelligibility was discussed in the previous study ([5], [9], [11]). Therefore, it can be inferred that the abnormality of vowel space is a critical reason for the inaccurate articulation and the reduced speech intelligibility of people with CP. 2.2 Pitch Dysprosody, where the control of prosodic variables such as fundamental frequency (Fo) or pitch is impaired, is a common feature of dysarthria [12]. According to Ciocca et al. [2], in tonal languages, such as Cantonese, tonal-level contrast was the second most problematic phonetic contrast that influenced speech intelligibility. In Mandarin Chinese, there are four dominant tones: high-level (tone 1), high-rising (tone 2), low-falling-rising (tone 3), and high-falling (tone 4) [13]. According to Han et al. [14], tone or pitch of each monosyllable makes meaningful contrasts. For instance, changing the four tones of the same syllable, ma, will create meaningful contrasts: “mother” (tone 1), “hemp” (tone 2), “horse” (tone 3), and “scold” (tone4). Therefore, pitch is central to the intelligibility of tonal languages. In order to produce different tones to make meaningful contrasts, speakers alter the tension of the vocal folds and the amount of air flowing from the lungs [2]. However, because dysarthric speakers have difficulty controlling the respiratory and the laryngeal mechanisms, they cannot always produce correct tones ([2], [4], [5]). Bunton et al. [12] found that English-speaking dysarthric adults tended to decrease the duration of their tone units, or 16  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) produce fewer words in a tone unit. In addition, the range of Fo of dysarthric speakers is restricted. Furthermore, Cantonese dysarthric speakers showed errors in Fo level and/or Fo contour due to the lack of control of laryngeal mechanism [2].  2.3 Speech rate Due to the neuromuscular factors, it is not surprising that individuals with dysarthria tend to have a slower and more unstable speech rate ([3], [6], [15], [16]). Many researchers have tried to associate speech rate and speech intelligibility to further discuss the complete index of one’s speech performance [17]. The previous study [4] stated that slower speech rate of individuals with cerebral palsy may contribute to higher speech intelligibility, which also serves as an aid to their communication efficiency. In contrast, other studies have found no significant correlation. Turner, Tjaden, and Weismer [8], by having dysarthric subjects read the passages at habitual, fast, and slow speaking rate, concluded that there is no specific correlation between these two issues. Therefore, there is still no agreement on the relationship between speech rate and speech intelligibility. Whether the slower speech can be a compensatory strategy to increase intelligibility remains unknown. This study explores the relationship between speech rate and speech intelligibility in spontaneous speech production in 4-year-olds with cerebral palsy, and answers the following questions: (1) Is the speech rate of the children with dysarthria slower than that of typically developing children? (2) How is speech rate related to speech intelligibility?  3. Methodology 3.1 The participants Four children participated in this study: two with cerebral palsy (CP1 and CP2, mean age 52.3 months) and two with no specific medical history (TD1 and TD2, mean age 54.8 months). The tables provide background information of CP1 and CP2.  Subject CP1 CP2  Table 1. Descriptive data of the two CP subjects  Gender Months Classification Type of CP  Male  48.3  Male  56.3  Dyskinetic Other  Quadriplegia Quadriplegia  Severity of impairment Moderate Severe  Table 2. Descriptive data of the two TD subjects  Subject Gender Months  TD 1  Male  54.5  TD 2  Male  55.1  All of the subjects are male, in order to avoid any potential gender differences in pitch, and are have normal hearing and intelligence. The two CP subjects were recruited from a hospital. CP1 has the medical diagnosis of dyskinetic quadriplegia with moderate CP. He has been diagnosed with borderline language delay on the basis of Preschool Language Scale-Chinese Version (PLS-C), and has received language therapy. CP2 has the medical diagnosis of quadriplegia with severe CP. He received education in a special education center, but he has never received language therapy. The data of TD subjects were taken from a large-scale study of longitudinal phonetic development. 3.2 Data collection CP1’s data were collected in lab with less noise disturbance, while the data of CP2 and the two TD children were collected in their homes. Although the locations were different, the  17  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) same recording equipment was used. A SHURE Wireless microphone system was linked to TASCAM DR-100 recorders for the purpose of sound recording. During the 50-minute observation period, speech productions in picture naming task were recorded, and the Peabody Picture Vocabulary Test-Revised (PPVT-R) was used to provide a quick assessment of the speech and language ability. 3.3 Data analysis – acoustic vowel space The first 50 utterances with clear quality were transcribed and analyzed with the time-frequency analysis software program, TF32. Vowel formant frequencies were determined with reference to spectrogram, LPC, and FFT with Hillenbrand, Getty, Clark, and Wheeler [18] as the range reference of formant frequencies. F1 and F2 values and bandwidth were measured. Vowels with unrecognized formant patterns or with large bandwidth (larger than 1000Hz) were discarded. All F1 and F2 values of vowels were normalized. The procedure of normalization is intended to reduce the differences caused by extrinsic vowel formant values and remaining the phonological distinctions among different vowels ([19], [20]). The differences of vowel productions of CP and TD were analyzed in three aspects: the F1 and F2 values of individual vowels /i/, /a/, /u/, /ə/, /”/, and /—/, standard deviation of formant frequencies, and vowel space. Overall F1-F2 vowel spaces were calculated to examine the data diversity, and the vowel space formed by the three corner vowels /i/, /a/, and /u/ were captured to illustrate the mobility and control ability of tongue and jaw. 3.4 Data analysis - pitch Pitch values of bi-syllabic or tri-syllabic words were analyzed based on four dominant tones in Mandarin Chinese: high-level (tone 1), high-rising (tone 2), low-falling-rising (tone 3), and high-falling (tone 4) [13]. However, in Mandarin spoken in Taiwan, the low-falling-rising tone or dipping tone (tone 3) is always replaced by low-falling tone. The first 50 intelligible and less disturbed utterances were selected for pitch analysis. The same procedure was administered to all four children. TF32, an acoustic analysis program, was used to estimate fundamental frequency (Fo), mean standard deviation of Fo, mean tone duration (TU), mean slope (in Hz/ms), and the maximum and minimum values of Fo. In addition, the beginning point (BP) and the end point (EP) were measured for tone 1 and 4; the beginning point (BP), the inflectional point (IFP), and the end point (EP) were measured for tone 2 and tone 3. For slope of tones, two functions were used to measure. Function 1: SLP1 (Tone1 and 4) = (EP-BP)/ ( െ ) Function 2: SLP2 (Tone2 and 3) = (IFP-BP)/ (  െ ) SLP3 (Tone2 and 3) = (EP-IFP)/ ( െ  ) Note that in Slope Function 2, tone 3 was in fact the low-falling tone. 3.5 Data analysis – speech rate In speech rate, the target data were the phrases and sentences produced by the four children in spontaneous interaction. To examine speech intelligibility, the target data were 50 randomly chosen words from the picture-naming task in the same recordings. The following principles are based on the data collection procedures in [4]. (1) Syllables per minute (SPM): one judge listened to the phrases and sentences, transcribed the content syllable by syllable, and counted the number of the syllables. SPM is obtained by calculating the total number of the syllables divided by the time duration, and multiplying the quotient by 60. In the case of spontaneous speech, the intra-sentence pauses were included, but the inter-sentences pauses were not. (2) Intelligible syllables per minute (ISPM): ISPM is acquired by counting only the number of the intelligible syllables divided by the duration, and multiplying the quotient by 60. Ten 18  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) percent of the data were re-analyzed by the second judge. The inter-judge was allowed to listen to the data again, and to the relevant context but no more than twice. The result of inter-judge reliability is 86.2%, which exceeds the standard proposed by Kassarjian [21]. Speech intelligibility: Three judges were recruited to transcribe productions of 50 words of each child in the picture naming tasks. The judges could only listen once and then transcribed what they heard. All the judges worked alone, and at their own pace. The total number of correctly transcribed syllables was divided by the total number of the syllables of the 50-word list. Mean intelligibility from the three judges was calculated as speech intelligibility of each child.  4. Results and discussion 4.1 Acoustic vowel space Frequency of occurrence The following results compare CP and TD group in vowel accuracy and the occurrence of main vowels (/i/, /a/, /u/, /ə/, /”/, and /—/).  Vowels /i/ /a/ /u/ /ə/ /”/ /—/  Table 3. The occurrence of main vowel in the four children  CP1  CP2  TD1  TD2  21.33%  17.39%  22.95%  25.86%  22.67%  
In this paper, we introduce a hybrid method to associate English collocations with sense class members chosen from WordNet. Our combinational approach includes a learning-based method, a paraphrase-based method and a sense frequency ranking method. At training time, a set of collocations with their tagged senses is prepared. We use the sentence information extracted from a large corpus and cross-lingual information to train a learning-based model. At run time, the corresponding senses of an input collocation will be decided via majority voting. The three outcomes participated in voting are as follows: 1. the result from a learning-based model; 2. the result from a paraphrase-based model; 3. the result from sense frequency ranking method. The sense with most votes will be associated with the input collocation. Evaluation shows that the hybrid model achieves significant improvement when comparing with the other method described in evaluation time. Our method provides more reliable result on associating collocations with senses that can help lexicographers in 47  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) compilation of collocations dictionaries and assist learners to understand collocation usages. 斄挝娆烉崭婆シ㧁䣢炻㏕惵娆↮栆炻娆⼁婆シ妋㬏炻娆䵚ˣ㚨Ἓ䅝ῤ㧉✳ˣ慵徘 Keywords: supersense tagging, collocation classification, word sense disambiguation, WordNet, maximum entropy model, Paraphrase. 
This study adopts a corpus-based computational linguistic approach to measure individual differences (IDs) in visual word recognition. Word recognition has been a cardinal issue in the field of psycholinguistics. Previous studies examined the IDs by resorting to test-based or questionnaire-based measures. Those measures, however, confined the research within the scope where they can evaluate. To extend the research to approximate to IDs in real life, the present study undertakes the issue from the observations of experiment participants’ daily-life lexical behaviors. Based on participants’ Facebook posts, two types of personal lexical behaviors are computed, including the frequency index of personal word usage and personal word frequency. It is investigated that to what extent each of them accounts for participants’ variances in Chinese word recognition. The data analyses are carried out by mixed-effects models, which can precisely estimate by-subject differences. Results showed that the effects of personal word frequency reached significance; participants responded themselves more rapidly when encountering more frequently used words. People with lower frequency indices of personal word usage had a lower accuracy rates than others, which was contrary to our prediction. Comparison and discussion of the results also reveal methodology issues that can provide noteworthy suggestions for future research on measuring personal lexical behaviors. 61  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) Keywords: individual differences, lexical behaviors, word recognition, computational linguistic approach, naturalistic data 1. Introduction In the field of psycholinguistics, a major research interest is to investigate how people recognize written words or access the corresponding word representations stored in their mental lexicon. Psycholinguists usually undertake the investigation starting from isolated words since less factors are involved, compared to words within sentences. Therefore, research on the isolated word recognition is fundamental for understanding how lexical access takes places. In general, the term ‘visual word recognition’ is used to simply address the recognition of isolated written words. Research of word recognition traditionally have concentrated on how characteristics of words per se (e.g. word length, word frequency, or neighborhood size) affected the procedure of recognition [1] [2] [3] [4] [5], taking the discrepancies between participants’ performance as merely statistical deviation. Recently, however, there has been a growing interest in the individual differences (IDs, henceforth) of experiment participants. Results of the ID studies showed that the issue was noteworthy because personal experiences and knowledge of words (e.g. print-exposure experience [6] [7], reading skills [8], or vocabulary knowledge [9] [10] [11]) accounted for systematic variances between participants in word recognition. Even when participants were homogeneous in their educational level, their IDs sufficiently resulted in distinct performance in word recognition. Furthermore, [8] provided compelling evidence that conflicting results of regularity effects1 in the literature were attributable to lacking control over participants’ IDs of reading skills. To date, the studies of IDs, however, have focused on test-measured or self-rated ID variables. In such approaches, the observed IDs were confined in the boundary of a test or questionnaire design, and the uniqueness of each individual in real life was neglected. In an attempt to examine the approximate real-life IDs, this research measures and analyzes IDs based on each participant's own lexical behaviors. Lexical behaviors here refer to a person’s word usage and preference in his/her daily life. Intuitively, language usage reveals one’s vocabulary knowledge, such as the words the person knows and how to use those words within context. Vocabulary knowledge was proved relating to word recognition [9] [10] [11]; hence, it is highly possible that IDs of lexical behaviors can explain the disparity of participants’ performance in word recognition. The lexical behaviors mainly have two merits over the measure of vocabulary tests. First, people’s lexical knowledge will be evaluated not by a small set of vocabularies in a given test, but by the words used by themselves. In this case, a variable’s value assigned to a given participant is personalized and not confined to the scale or the total score of a test. The other merit resides in that the data of language usage can provide a deeper insight into a person’s lexical knowledge, compared with a vocabulary test. If a person is able to use or produce a given word naturally (and frequently), it suggests that the word’s representation has been firmly established in his/her mental lexicon. Besides, it is worth noting that the stance we take in measuring the ‘individuality’ is naturalistic rather than natural, in that the lexical behaviors we describe are assumedly anchored in the interaction as naturalistic situated interactions, rather than natural ones (like using camera to collect data). A pitfall of the natural ones is that when observers and/or cameras are present those interactions are not quite what they would be in our absence. 
Taiwanese tone sandhi problem is one of the important research issues for Taiwanese Text-to-Speech systems. In word level, we can use the general tone sandhi rules to deal with the Taiwanese tone sandhi problem. The tone sandhi becomes more difficult in sentence level because of that the general tone sandhi rules for words may not apply at each word in a sentence. In this paper we proposed a module to deal with the Taiwanese tone sandhi problem for Chinese to Taiwanese Text-to-Speech systems. We adopt Decision tree C5.0 algorithm accompanied with three Special Cases generated from training data to predict the tone sandhi of each syllable. In this module, the accuracy of the inside test and outside test are 93.42% 92  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) and 91.13%, respectively. ᣂ᝶ဲΚ‫؀‬፿ຑଃ᧢ᓳΔ֮᠏ଃߓอΔެ࿜ᖫ Keywords: Taiwanese Tone Sandhi, Text-to-Speech System, Decision Tree. ԫΕፃᓵ ‫ݺڇ‬ഏ່ൄ๯ࠌ‫ش‬ऱԿՕ፿ߢ։ܑਢഏ፿Ε‫؀‬፿֗ড়፿ΔຍԿጟ፿ߢຟ᥆࣍ᜢᓳ፿ ߢ(Tonal Language)ΔՈຟ‫ڶژ‬ຑଃ᧢ᓳ(Tone Sandhi)ऱ෼ွΖࢬᘯຑଃ᧢ᓳਐऱਢ‫ڇ‬ຑ ᥛ፿ଃխΔਬࠄଃᆏ‫ࠡ࠹ڂ‬ছ৵ଃᆏऱᐙ᥼ۖլ٦অ‫ڶ଺ڶ‬ᓳᇆऱൣ‫ݮ‬Ζࠏ‫ף‬Κ‫ڇ‬խ֮ ᇙΔψ‫۔‬/ԈԜԩ/ωࡉψॡ/ԋԤԩ/ωຟਢԿᜢ‫ڗ‬Δۖᅝຍࠟଡ‫ګิڗ‬ψ‫۔‬ॡωຍଡဲऱ ழଢΔψ‫۔‬ωຍଡ‫ڗ‬ऱᦰଃᄎ᧢‫ګ‬Բᜢऱ/ԈԜԨ/Ζψ୶/ԏԞԩ /ωΕψᥦ/ԈԞԩ/ωΕψ塢/ ԉԤԞԩ/ωิ‫ګ‬ψ୶ᥦ塢ωழψ୶ωࡉψᥦωऱᦰଃຟ᧢‫ګ‬ԲᜢΖ‫؀ڇ‬፿խΔψՒωऱ ᦰଃਢ/to2/Δψ‫چ‬ωऱᦰଃਢ/de7/ΔۖψՒ‫چ‬ωऱᦰଃਢ/to1 de7/Δ‫ݺ‬ଚ‫אױ‬࿇෼ψՒω ຍଡ‫ڗ‬ऱᜢᓳ‫ء଺ط‬ऱԲᜢ᧢‫ګ‬ԫᜢΖ‫ڇ‬ড়፿խΔψಾωࡉψᒵωऱᦰଃ։ܑ੡/ziim2/ ፖ/sien1/Δᅝ،ଚิ‫ګ‬ψಾᒵωຍଡဲழΔψಾωऱᦰଃ૞᧢‫ګ‬Կᜢऱ/ziim3/Ζ ‫؀ڇ‬፿խΔࢬ‫ڶ‬໢‫ဲڗ‬ऱᦰଃᜢᓳጠ੡‫ء‬ᓳΖՕ‫ڍ‬ᑇऱ֮᣸ຟᎁ੡‫؀‬፿ऱᜢᓳ‫ڶ׽‬ ԮጟΔ։ܑ੡Κ ࣟ/dong1/Ε᤻/dong2/Εར/dong3/Εᅮ/dok4/Ε‫ٵ‬/dong5/Ε᤻/dong6/Ε ੐/dong7/Εᗑ/dok8/Δࠡխ‫؀‬፿ऱԲᜢࡉքᜢ‫ٵ‬ᓳΖ‫ױ‬ਢ‫ݺ‬ଚ࿇෼ૉ‫ە‬ᐞຑଃ᧢ᓳࡉত ࿺‫ק‬ᓳऱൣ‫ݮ‬Δ‫؀‬፿ऱᜢᓳ᧢֏೏ሒԼԲጟΔᇡߠ।ԫΖᜰࠏࠐᎅΔ‫؀‬፿Կᦤଃխรԫ ଡ‫ڗ‬ऱᜢᓳΔլ᥆࣍ൄߠᜢᓳऱٚ۶ԫጟΔ‫ڕ‬Κଯ/dong0/ଯଯ‫֗א‬Եᜢଃऱ੅/dok0/੅ ੅Ζۖᜢᓳ 9 ࡉᜢᓳ 8 ঞ੡ত‫ק‬࿺ᓳऱլ‫ٵ‬Ζ‫ဲڇ‬ऱၸᐋΔ‫֟ڶ׽‬ᑇऱဲࠡࢬ‫ڶ‬ऱଃᆏ ຟᦰ‫ء‬ᓳΔࠏ‫ڕ‬ψᙰ࿀/tau5 tiann3/ωΔۖՕ‫ڍ‬ᑇऱ‫؀‬፿ဲຟᙅ༛ဲ‫ݠ‬ଃᆏᦰ‫ء‬ᓳΔॺဲ ‫ݠ‬ଃᆏᦰ᧢ᓳऱ‫؀‬፿ԫ౳᧢ᓳ๵ঞΖ‫؀‬፿᧢ᓳऱൣ‫ױݮ‬։‫ॺګ‬Եᜢ‫ڗ‬ፖԵᜢ‫᧢ڗ‬ᓳຍࠟ ଡຝ։Δᇡาऱ᧢ᓳ๵ঞ‫ױ‬೶‫ە‬ቹԫΖ‫ڇ‬Եᜢ‫ڗ‬ऱ᧢ᓳ๠෻ՂΔԫ౳‫ڍ‬ᎁ੡؄ᜢࡉԶᜢ յངΔۖ‫ݺ‬ଚᎁ੡ऱԵᜢ‫᧢ڗ‬ᓳ๠෻ᚨ੡؄ᜢ᠏੡ԲᜢΔۖԶᜢࡉ԰ᜢ᧢ᓳ৵ᚨ੡Կ ᜢΖ‫؀‬፿ဲփऱ᧢ᓳᒤࠏ‫ױ‬೶‫ە‬।ԲΖ‫؀‬፿਋ଃߓอ‫ڶ‬ৰ‫ڍ‬Δ‫ݺ‬ଚࢬආ‫ش‬ऱਢ‫؀‬᨜਋ଃ ߓอ[3]Δ‫ڂ‬੡،౨‫ٵ‬ழᔞ‫࣍ش‬ഏ፿Ε‫؀‬፿֗ড়፿࿛‫ڍ‬ጟ‫؀ڇ‬᨜ຏ۩ऱ፿ߢΔຍ‫࣍ܓڶ‬ ‫ݺ‬ଚ‫ࠐآ‬ऱ࿇୶Ζ ‫؁ڇ‬՗ऱၸᐋΔ‫؀‬፿ऱຑଃ᧢ᓳൣ‫ݮ‬༉᧢൓ֺለᓤᠧԱΖ‫ڶ‬ழଢԫଡ‫؁‬՗፷‫່ڶ׽‬ ৵ԫଡ‫ءᦰڗ‬ᓳΔࠡ塒ऱ‫ڗ‬ຟ૞᧢ᓳΖ‫؁א‬՗ψ‫ݺ‬၇ੑ۪ᖲω੡ࠏΔࢍՀ‫ݺ‬ଚ٨‫؁נ‬խ ‫ޢ‬ଡ‫ڗ‬ऱ‫ء‬ᓳ࿇ଃ‫؁֗א‬՗‫إ‬ᒔऱ‫؀‬፿ᦰଃʻᆖ᧢ᓳ๠෻ʼΖʳ ࠏ‫؁‬Κʳ ‫ݺ‬ʳ ʳ ၇ʳ ʳ ʳ ੑʳ ʳ ʳ ʳ ۪ʳ ʳ ʳ ᖲʳ ‫ء‬ᓳΚ˺˻̈˴˅ʳ ˵˻˸˅ʳ ̆˸˅ʳ ̆˴́́˄ʳ ˺˼˄ʳ ᧢ᓳΚ˺˻̈˴˄ʳ ˵˻˸˄ʳ ̆˸˄ʳ ̆˴́́ˊʳ ˺˼˄ʳ ‫׼‬ԫጟൣ‫ݮ‬ঞਢ‫ݺ‬ଚ‫אױ‬ലԫଡ‫؁‬՗ီ੡‫ط‬ᑇଡ፿ऄ੄ᆵิ‫ګ‬Δ‫ޢ‬ଡ፿ऄ੄ᆵጠ੡ ᧢ᓳဲิ[4]Ζ᧢ᓳဲิऱ່৵ԫଡ‫ءᦰڗ‬ᓳΔࠡ塒ऱ‫ڗ‬ઃ᧢ᓳΖᓮ઎‫א‬Հऱࠏ՗Δʻࢍ ᒵ।᧢ᓳဲิʼΚʳ ࠏ‫؁‬Κሎʳ ʳ ʳ ʳ ೯ʳ ʳ ʳ ʳ ਢʳ ʳ ԫʳ ʳ ʳ ଡʳ ʳ ʳ ‫ړ‬ʳ ʳ ʳ ฾ʳ ʳ ʳ ʳ ክʳ ‫ء‬ᓳΚ̈́ˊʳ ˷̂́˺ˊʳ ̆˼ˊʳ ̍˼̇ˋʳ ˸˼ˈʳ ˻̂˅ʳ ̆˼̃ˋʳ ˺̈˴́ˆʳ ᧢ᓳΚ̈́ˆʳ ˷̂́˺ˊʳ ̆˼ˆʳ ̍˼̇ˆʳ ˸˼ˈʳ ˻̂˄ʳ ̆˼̃ˆʳ ˺̈˴́ˆʳ 93  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) ‫؁‬՗ψሎ೯ਢԫଡ‫ړ‬฾ክωΔᆖመխઔೃဲ஄՛ิऱᒵՂឰဲᕴ๠෻৵‫ױ‬։ᇞ‫ګ‬քଡဲΔ ։ܑਢψሎ೯ωΕψਢωΕψԫωΕψଡωΕψ‫ړ‬ωΕψ฾ክωΖૉࠉᅃቹԫऱ๵ঞࠐ๠෻ຑଃ᧢ ᓳΔ߷Ꮦࠡ࿨࣠༉լᄎ‫إ‬ᒔΖ‫ڂ‬੡᧢ᓳဲิਢ‫ط‬ԫଡࢨ‫ڍ‬ଡဲࢬิ‫ګۖٽ‬Δࢬ‫א‬ቹԫऱ ๵ঞሎ‫؁ࠩش‬՗Ղழ༉᧩൓լജ‫ݙ‬࿳Ζ‫ݺ‬ଚ‫ط‬ಝᒭ፿ற࿇෼ॺဲ‫ݠ‬ऱଃᆏ‫ˈˌ ڶ‬ʸऱᖲ෷ ᦰ᧢ᓳΔۖဲ‫ݠ‬ऱଃᆏ‫˃ˉ ڶ‬ʸऱᖲ෷ᦰ‫ء‬ᓳΔˇ˃ʸऱᖲ෷ᦰ᧢ᓳΖຍ।‫؁ق‬խऱॺဲ‫ݠ‬ ଃᆏՕીᙅ༛ԫ౳ऱ‫؀‬፿ຑଃ᧢ᓳ๵ঞΔۖဲ‫ݠ‬ଃᆏঞ޲‫ڶ‬ຍጟႜ‫ٻ‬Δ‫؁ڇڼڂ‬՗ၸᐋ ᇙᏁ૞‫ࠋޓڶ‬ऱֱऄࠐ๠෻‫؀‬፿ऱຑଃ᧢ᓳംᠲΖʳ  ।ԫΕ‫؀‬፿ᜢᓳᎅࣔࠏ।  ᜢᓳ  ࠏ‫ڗ‬ ΰ‫؀‬፿࿇ଃα  ᜢᓳ‫ټ‬ጠፖုᇞ  dong0 ଯΰdong0αଯଯ ೏ᜢΰԿຑଃรԫ‫ڗ‬α  dong1  ࣟ  ೏ؓ  dong2  ᤻  dong6  ೏૾  dong3  ར  ‫૾܅‬  dong5  ‫ٵ‬  ‫܅‬ᒷՂ֒  dong7  ੐  խؓ  dok0 ੅ΰdok0α੅੅  ೏ԵΰԵᜢଃα  dok2 ࠬΰdok2αឰ  խ૾ΰԵᜢଃα  
An increasing number of people learn Chinese as second language in the world. About 60% of Chinese characters are picto-phonetic compounds which are composed of a phonetic component (PC) and semantic component. Therefore炻one can make a guess at a character’s pronunciation and meaning from its phonetic and semantic component for a new character. For this reason炻we propose an order of phonetic components based on pronunciation strength炻frequency and number of strokes for efficient learning with proper pronunciation rules and graph recognition. We adopt stem-deriving instructional method which extends each phonetic component with different radical component to derive new picto-phonetic compounds of similar pronunciation. Via simulation, the top 400 phonetic components and their 112  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) picto-phonetic extensions are enough for the recognition of 60% characters in general articles; and top 800 phonetic components can help recognition of 90% characters of general news articles. 斄挝娆烉⼊倚⫿炻倚䫎悐ẞ炻悐ẞ炻ẍ⫿ⷞ⫿ Keywords: picto-phonetic compounds炻phonetic component炻component炻 stem-deriving instructional method. ᶨˣ䵺婾 忁ṃ⸜䓙㕤ᷕ⚳ⶪ⟜䘬ⳃ崟炻≈ᶲ厗Ṣ䫔Ḵẋㆾ䫔ᶱẋ⛐㴟⢾䘬㒜⯽炻⛐⎘ 䀋炻䓙㕤⢾䯵惵„Ṣ㔠䘬⡆≈炻ἧ⬠佺ᷕ㔯䘬Ṣ㔠⡆≈ˤ㻊⫿䘬⫿⼊䷩䐋炻⇅⬠ 侭暋ẍ㌴㎉ˤ℞ᷕ㚨ᷣ天䘬⍇⚈⛐㕤㻊⫿㗗⚾⼊㔯⫿ (pictograph system)炻⚾⼊ 㔯⫿⣏悐↮䁢⽆⫿ᷕ堐䣢↢⫿䘬シ⿅炻劍⫿㛔幓⼊䉨冯枛䘬忋䳸⹎ᶵ檀炻⯙䃉㱽 ⁷劙㔯䫱㊤枛㔯⫿(alphabet system)ᶨ㧋炻㌴㎉Ḯ℞㊤枛夷⇯炻⯙㚱➢㛔傥婆㔯傥 ≃ˤℵ侭炻⚈䁢⼰暋⽆ᶨᾳ㻊⫿ᷕ⼿䞍℞䘤枛炻忂ⷠㆹᾹ怬⽭枰ẍ㻊婆㊤枛 (Hanyu pinyin)ㆾ㗗㲐枛䫎嘇(Chinese phonetic symbols)䫱㊤枛庼≑炻ㇵ⎗䞍忻㭷 ᾳ㻊⫿䘬䘤枛ˤ ẍ⼨ᷕ⚳㚱叿䚠䔞㭼ἳ䘬㔯䚚炻冯䎦Ṳ⮵㕤㴟⢾Ṣ⢓䘬䫔Ḵẋˣ䫔ᶱẋ炻ㆾ 㗗冢䀋⛘⋨䘬㕘䦣㮹炷㕘䦣㮹㊯䘬㗗⢾䯵㕘⧀ㆾ㗗⢾⚳Ṣ⢓炸炻忁ṃṢ䓙㕤⸛㗪 悥㚱㨇㚫㍍妠ἧ䓐ᷕ㔯䘬Ṣ炻冒䃞㚱ᶨ⭂䦳⹎➢㛔⎋婆炻⌣⎗傥⚈䁢ᶵ嬀⫿Ṏㆾ 㗗嬀⫿䦳⹎ᶵ檀侴䃉㱽教嬨ˤ忁ṃṢ⛐⸛ⷠ䓇㳣ᷕ炻晾⶚㚱枛嶇佑ᷕ攻䘬忋㍍斄 Ὢ炻⌣仢⮹冯⼊ᷳ攻䘬忋㍍炻⚈㬌怬㗗䚳ᶵㅪᷕ㔯⫿炷⚾ᶨ炸ˤ ẍ⚾ᶨἮ婒炻娵 嬀ᶨᾳ㕘䘬ᷕ㔯⫿炻㗗暨天⼊枛佑ᶱ侭悥䳸⎰炻ㇵ䫱㕤娵嬀Ḯᶨᾳ㕘䘬⫿ˤ ⚾ᶨ: 婆妨⬠佺⼊枛佑斄Ὢ ⎎ᶨ㕡朊炻劙㔯⛐娵嬀ᶨᾳ㕘╖⫿㗪炻䓙㕤⼊冯枛ᷳ攻㚱冒䃞䘤枛䘬忋㍍炻 ⎒暨天姀⼿℞嬨枛冯シ⿅炻⌛⎗⬠佺㕘䘬婆⼁ˤ䚠庫㕤厗婆⛐⬠佺ᶨᾳ㕘⫿㗪炻 椾⃰天⃰⺢䩳⼊嶇枛ᷕ攻䘬忋䳸炻ℵἮ天枛嶇佑䘬忋䳸炻䔞⼊枛佑ᶱ侭䳸⎰崟Ἦ ᷳ⼴炻ㇵ⬠佺Ḯᶨᾳ㕘䘬⫿炻⎗ẍ婒厗婆㕘⫿⬠佺䘬ㆸ㛔炻怈庫劙㔯╖⫿䘬⬠佺 ㆸ㛔檀ˤ⚈侴㛔䭯婾㔯ᷣ天䚖㧁㗗⸓≑㻊⫿⬠佺侭⻟⊾⼊冯枛䘬忋㍍斄Ὢ炻嬻⶚ 䴻㚱➢㛔⎋婆傥≃䘬ἧ䓐侭⎗ẍ庽檮嬀⫿炻嬻Ṣ⛐⾝↢⫿䘬嬨枛ᷳ⼴炻德忶℞㛔 幓⶚㚱ᷳ枛冯佑䘬忋䳸炻⌛⎗Ḯ妋℞娆⎍㇨⊭⏓䘬シ⿅ˤ 㻊⫿↮ㆸℕ⣏栆[1]㒂䴙妰屯㕁炻㔁做悐妪ⷠ䓐⫿ 4783 ᾳ炻℞ᷕ⼊倚⫿Ỽ 3026 ᾳ炻Ỽ䷥ⷠ䓐⫿ᷕ䘬ℕㆸẍᶲˤ忁湤⣂䘬⼊倚⫿⛐㥳⫿ᶲ炻⣂㍉䓐[1+1]䘬㕡⺷炻 113  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) ḇ⯙㗗ᶨᾳ悐椾悐ẞ≈ᶲ倚䫎悐ẞ炻⮵㕤㬌䧖䎦尉炻ㆹᾹ⎗ẍ⽆倚䫎悐ẞ↢䘤炻 ᷎㒔⭂ᶨ⣿㔁⬠枮⸷炻᷎ᶼ⺢䩳ẍ悐ẞ䁢ᷣᷳ㻊⫿嬀⫿䶂ᶲ㔁⬠ˤ 晾婒悐ẞ㔁⬠シ佑慵⣏炻ḇ⎗旵Ỷ⬠佺䘬ㆸ㛔炻Ữ⌣㛒⍿⇘䎦㚱㔁⬠䘬慵 夾炻ⶪ朊ᶲ悐ẞṳ䳡暞㔋ˣᶵㆸ䲣䴙烊悐ẞ㔁⬠忶㕤晐シˣ仢⮹⬴㔜妰∫⿏[11]ˤ ⚈㬌㛔䭯婾㔯ᷣ天䚖㧁㗗ⶴ㛃䓐悐ẞ㔁⬠䘬奺⹎炻⸓≑㻊⫿⬠佺炻᷎⮎晃姕妰Ḯ ᶨ⣿ẍ悐ẞ䁢ᷣ䘬㻊⫿⬠䲣䲣䴙炻㚜忚ᶨ㬍䘬⻟⊾⼊冯枛䘬忋㍍斄Ὢ炻嬻⶚䴻㚱 ➢㛔⎋婆傥≃䘬ἧ䓐侭⎗ẍ庽檮嬀⫿ˤ 㛔㔯䫔ᶨ悐↮叿慵㕤倚䫎悐ẞ䘬枮⸷炻䷥妰 1453 ᾳⷠ䓐悐ẞ炻冯℞⺞Ỡ⫿ ⌛⎗㵝味⣏悐ấ㔁做悐妪⭂䘬ⷠ䓐⫿炻ὅ䄏㬌䧖悐ẞ㌺⸷ 䁢㔁⬠枮⸷炻⛐⬠佺 ⇵朊䘬⫿㗪炻℞㵝味⺞Ỡ⫿䘬⬠佺㚚䶂䚠⮵℞Ṿ枮⸷㔁⬠炻㚱庫檀䘬㈽屯⟙愔䌯ˤ 䫔Ḵ悐↮⛐㕤⻟⊾⼊嶇枛ᷕ攻䘬忋䳸炻᷎ ㆸᶨ⣿⼊枛佑ᶱ侭䳸⎰炻ᶼẍ倚 䫎悐ẞ䁢ᷣ䘬ᷕ㔯嬀⫿䶂ᶲ㔁⬠ˤ⛐⼊䘬㕡朊炻ㆹᾹ㍸↢Ḯ䚠Ụ㥳⫿䞑昋炻⎗ẍ ᶨ㫉㔁⮶⣂ᾳ䚠Ụ㥳⫿䘬⫿ㆾ悐ẞ烊⛐枛䘬悐↮炻ㆹᾹ⮵㕤㭷ᶨᾳ倚䫎悐ẞ憅⮵ 倚㭵枣㭵⏰䎦夾奢⊾廱枛夷⇯炻⍲⋡惵㭷ᾳ倚䫎悐ẞ䘬⼊倚⫿斄倗夷⇯烊㚨⼴㍸ ὃ㭷ᾳ⫿➢㛔Ⱄ⿏炻᷎ᶼ⍫侫恙⌂䏳 2012[8]婾㔯墉丒墥ẋ堐℞⫿シ佑䘬⚾⼊ˤ 䎮゛ᶲ炻⇵ 400 ᾳ悐ẞᷳ㔁⬠炻℞㔜橼嬀⫿䌯⶚忼⇘ℕㆸẍᶲ炻⮵㕤ⶪ朊ᶲ 䘬ᶨ凔㔯䪈炻⛐⬠佺Ḯ⇵朊 800 ᾳ悐ẞ炻㔜橼嬀⫿䌯㚜檀忼ḅㆸẍᶲ炻㛔㔯晾㛒 傥⊭⏓⮎⛘㔁⬠䘬⮎槿炻Ữ⽆㧉㒔⮎槿ᷕ⼿⇘ᶨṃ㔠㒂⍫侫ˤ  Ḵˣ䚠斄䞼䨞  ᷕ⣖䞼䨞昊屯妲䥹⬠䞼䨞㇨㔯䌣嗽瀲⮎槿⭌⽆ 1993 濜攳⥳炻瀡临⺢㥳⎌Ṳ  㔯⫿䘬㸸瀘㺼嬲ˣ⫿⼊䳸㥳⍲澰橼⫿堐炻 䁢姀澍㻊⫿⼊橼䞍灊䘬屯瀌⹓炻ḇ⯙  㗗㻊⫿㥳⼊屯瀌⹓[5]ˤ  䁢ḮḮ妋㻊⫿ᷕ⼊倚⫿䘤枛夷⇯䘬廱嬲炻ㆹᾹ⽭枰䞍忻㭷ᶨᾳ⼊倚⫿䘬倚䫎  䁢ỽˤ䁢㬌⛐ 2010 ⸜⻝▱よ㔁㌰冯㛶㵹䐑䫱Ṣ㕤 ROCLING 2010[3]炻㍸↢ẍ㚨  Ἓ⊾⍲㨇䌯↮Ự㱽⍣⇌㕟㻊⫿倚䫎炻ṾᾹㅱ䓐ᷕ䞼昊㔯䌣嗽䎮⮎槿⭌㇨⺢䩳䘬  ˬ㻊⫿㥳⼊屯㕁⹓˭炻⺢䩳⼊倚⫿㧁姀䲣䴙炻᷎䓙ᷕ⣖⣏⬠ᷕ㔯㇨䞼䨞䓇冯㔁㌰炻  ẍṢⶍ㧁姀㻊⫿㥳⼊屯㕁⹓ᷕ 14598 㚱㲐枛㧁䣢䘬㻊⫿㗗⏎䁢⼊倚⫿ẍ⍲℞倚  䫎悐ẞˤ  ⛐ ROCLING 2011[2]炻⻝▱よ㔁㌰冯㜿㚠⼍䫱Ṣ㍸↢倚䫎悐ẞ㌺⸷冯⼊倚⫿  䘤枛夷⇯㍊⊀炻⮵㕤⼊倚⫿䘬䘤枛夷⇯炻㈦↢檀㓗㊩⹎冯檀ᾉ岜⹎䘬夷⇯ˤ⎎⢾炻  ὅ㒂悐ẞ䘤枛⻟⹎ˣ⺞Ỡ⫿↢䎦柣䌯冯䫮䔓㔠ᶱ䧖⚈䳈炻㭼庫䶂⿏≈䷥ˣ⸦ỽ⸛  ⛯冯婧␴䳂㔠ᶱ䧖㌺⸷㕡⺷炻䘤䎦⸦ỽ⸛⛯䘬㕡㱽炻℞⺞Ỡ⫿㵝䌯䚠庫℞Ṿℑ䧖  ㆸ攟㚜⾓ˤ⸦ỽ⸛⛯℔⺷⤪ᶳ:  Score1 ( PC )  PC(ᘏఙᏐฟ⌧㢖⋡㻕 * P C䘤( 枛⻟⹎) P C ( 䫮∫㔠)  (1)  ᷣ天㓗㊩ㆹᾹἧ䓐ẍ悐ẞ㔁⬠䘬䞼䨞䁢檀▱ㄏ⛐2011⸜[4]炻㬌婾㔯㭼庫Ḯ  114  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) ⁛䴙↮㔋⺷㔁⬠炻冯ẍ悐ẞ䁢ᷣ䘬ẍ⫿ⷞ⫿嬀⫿㔁⬠ˤ⁛䴙↮㔋⺷㔁⬠㗗ẍᷣ柴 婚㔯䁢ᷣ⬠佺炻⬠佺䘬䓇⫿⣂䁢婚㔯ℏ㇨ⷞ↢䘬䓇⫿ˤ侴ẍ悐ẞ䁢ᷣ䘬ẍ⫿ⷞ⫿ 嬀⫿㔁⬠炻⇯㗗㔁⮶悐ẞ冯悐ẞ䳬⫿夷⇯䘬ᶨ䧖㔁⬠㕡㱽ˤ䴻䓙㔁⬠⮎槿炻⼿⇘ ᶨᾳ慵天䳸婾炻䔞ἧ䓐ẍ悐ẞ䁢ᷣ䘬ẍ⫿ⷞ⫿嬀⫿㔁⬠㗪炻⍿娎侭⛐嬀⫿䌯㕡朊 㚫㚜㚱㓰䌯䘬ㆸ攟炻⯌℞㗗⮵Ỷ⎋婆傥≃䘬Ṣ㚜㗗栗叿ˤ  ᶱˣ⓷柴㍷徘  䁢槿嫱倚䫎悐ẞ⮵㕤㻊⫿⬠佺䘬⸓≑炻㛔㔯⮎晃姕妰ᶨᾳẍ倚䫎悐ẞ䁢ᷣ䘬 㻊⫿嬀⫿䶂ᶲ㔁⬠⸛⎘炻⽆倚䫎悐ẞ䘬㔁⬠枮⸷炻⇘倚䫎悐ẞ冯℞⺞Ỡ⫿䘬䳬⎰ 斄Ὢˣ⺞Ỡ⼊倚⫿䘬䘤枛夷⇯炻≈⻟⼊嶇枛ᷕ攻䘬忋㍍斄Ὢˤ 3.1 ˣ悐ẞ㌺⸷  ⛐ ROCLING2011 ㇨㍸↢䘬悐ẞ㌺⸷㗗ὅ㒂䘤枛⻟⹎ˣ柣䌯ˣ⍲䫮∫ᶱᾳ⚈  
This paper presents a proposed method integrated with three statistical models including Translation model, Query generation model and Document retrieval model for cross-language document retrieval. Given a certain document in the source language, it will be translated into the target language of statistical machine translation model. The query generation model then selects the most relevant words in the translated version of the document as a query. Finally, all the documents in the target language are scored by the document searching model, which mainly computes the similarities between query and document. This method can efficiently solve the problem of translation ambiguity and query expansion for disambiguation, which are critical in Cross-Language Information Retrieval. In addition, the proposed model has been extensively evaluated to the retrieval of documents that: 1) texts are long which, as a result, may cause the model to over generate the queries; and 2) texts are of similar contents under the same topic which is hard to be distinguished by the retrieval model. After comparing different strategies, the experimental results show a significant performance of the method with the average precision close to 100%. It is of a great significance to both cross-language searching on the Internet and the parallel corpus producing for statistical machine translation systems. Keywords: Cross-Language Document Retrieval, Statistical Machine Translation, TF-IDF, Document Translation-Based. Updated: June 9, 2012 144  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) 1. Introduction With the flourishing development of the Internet, the amount of information from a variety of domains is rising dramatically. Although the researchers have done a lot to develop high performance and effective monolingual Information Retrieval (IR), the diversity of information source and the explosive growth of information in different languages drove a great need for IR systems that could cross language boundaries [1]. Cross-Language Information Retrieval (CLIR) has become more important for people to access the information resources written in various languages. Besides, it is of a great significance to alignment documents in multiple languages for Statistical Machine Translation (SMT) systems, of which quality is heavily dependent upon the amount of parallel sentences used in constructing the system. In this paper, we focus on the problems of translation ambiguity, query generation and searching score which are keys to the retrieval performance. First of all, in order to increase the probability that the best translation can be selected from multiple ones, which occurs in the target documents, the context and the most likely probability of the whole sentence should be considered. So we apply document translation approach using SMT model instead of query translation, although the latter one may require fewer computational resources. After the source documents are translated into the target language, the problem is transformed from bilingual environment to monolingual one, where conventional IR techniques can be used for document retrieval. Secondly, some terms in a certain document will be selected as query, which can distinguish the document from others. However, some of the words occur too frequently to be useful, which cannot distinguish target documents. This mostly includes two types, one is that the word frequency is high both in the current and the whole document set, which is usually classified as stop word; the other is that the frequency is moderate in several documents (not the whole document set). This type of words gives low discrimination power to the document, and is known as low discrimination word. Thus, the query generation model should filter the words which are of these types and pick the words that occur more frequently in a certain document while less frequently in the whole document set. Finally, the document searching model scores each document according to the similarity between generated query and the document. This model should give a higher mark to the target document which covers the most relevant words in the given query. There are two cases to be considered when we investigated the method. In one case, both the source and target documents are long text, which are hard to extract exact query from the large amounts of information. In the other case, the contents of the documents are very similar, which are not easy to distinguish for retrieval. The results of experiments reveal that the proposed model shows a very good performance in dealing with both cases. The paper is organized as follows. The related works are reviewed and discussed in Section 2. The proposed CLIR approach based on statistical models is described in Section 3. The resources and configurations of experiments for evaluating the system are detailed in Section 4. Results, discussion and comparison between different strategies are given in Section 5 followed by a conclusion and future improvements to end the paper. 2. Related Work CLIR is the circumstance in which a user tries to search a set of documents written in one Updated: June 9, 2012 145  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)  language for a query in another language [2]. The issues of CLIR have been discussed from different perspectives for several decades. In this section, we briefly describe some related methods. On the matching strategies for CLIR, query translation is most widely used method due to its tractability. However, it is relatively difficult to resolve the problem of term ambiguity because “queries are often short and short queries provide little context for disambiguation” [3]. Hence, some researchers have used document translation method as the opposite strategies to improve translation quality, since more varied context within each document is available for translation [4, 5]. However, another problem introduced based on this approach is word (term) disambiguation, because a word may have multiple possible translations [3]. Significant efforts have been devoted to this problem. Davis and Ogden [6] applied a part-of-speech (POS) method which requires POS tagging software for both languages. Marcello et al. presented a novel statistical method to score and rank the target documents by integrating probabilities computed by query-translation model and query-document model [7]. However, this approach cannot aim at describing how users actually create queries which have a key effect on the retrieval performance. Due to the availability of parallel corpora in multiple languages, some authors have tried to extract beneficial information for CLIR by using SMT techniques. Sánchez-Martínez et al. [8] applied SMT technology to generate and translate queries in order to retrieve long documents. Some researchers like Marcello, Sánchez-Martínez et al. have attempted to estimate translation probability from a parallel corpus according to a well-known algorithm developed by IBM [9]. The algorithm can automatically generate a bilingual term list with a set of probabilities that a term is translated into equivalents in another language from a set of sentence alignments included in a parallel corpus. The IBM Model 1 is the simplest among the five models and often used for CLIR. The fundamental idea of the Model 1 is to estimate each translation probability so that the probability represented is maximized   ¦ H  ml  P(t | s)  (l 1)m j 1  P(t j | si ) i0  (1)  where t is a sequence of terms t1, …, tm in the target language, s is a sequence of terms s1, …, sl in the source language, P(tj|si) is the translation probability, and Ɛ is a parameter (Ɛ =P(m|e)), where e is target language and m is the length of source language). Eq. (1) tries to balance the probability of translation, and the query selection, in which problem still exists: it tends to select the terms consisting of more words as query because of its less frequency, while cutting the length of terms may affect the quality of translation. Besides, the IBM model 1 only proposes translations word-by-word and ignores the context words in the query. This observation suggests that a disambiguation process can be added to select the correct translation words [3]. However, in our method, the conflict can be resolved through contexts.  3. Proposed Model The approach relies on three models: translation model which generates the most probable translation of source documents; query generation model which determines what words in a document might be more favorable to use in a query; and document searching model, which Updated: June 9, 2012 146  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) evaluates the similarity between a given query and each document in the target document set. The workflow of the approach for CLIR is shown in Fig. 1.  Figure 1. Approach for CLIR  3.1. Translation Model Currently, the good performing statistical machine translation systems are based on phrase-based models which translate small word sequences at a time. Generally speaking, translation model is common for contiguous sequences of words to translate as a whole. Phrasal translation is certainly significant for CLIR [10], as stated in Section 1. It can do a good job in dealing with term disambiguation. In this work, documents are translated using the translation model provided by Moses, where the log-linear model is considered for training the phrase-based system models [11], and is represented as:  M  ¦ exp(  Omhm (e1I  ,  fJ 1  ))  p(e1I | f1J )  m1 M  (2)  ¦ ¦ exp(  Omhm (e'1I  ,  fJ 1  ))  e '1I  m1  where hm indicates a set of different models, λm means the scaling factors, and the denominator can be ignored during the maximization process. The most important models in Eq. (2) normally are phrase-based models which are carried out in source to target and target to source directions. The source document will maximize the equation to generate the translation including the words most likely to occur in the target document set.  Updated: June 9, 2012 147  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)  3.2. Query Generation Model  After translating the source document into the target language of the translation model, the system should select a certain amount of words as a query for searching instead of using the whole translated text. It is for two reasons, one is computational cost, and the other is that the unimportant words will degrade the similarity score. This is also the reason why it often responses nothing from the search engines on the Internet when we choose a whole text as a query.  In this paper, we apply a classical algorithm which is commonly used by the search engines as a central tool in scoring and ranking relevance of a document given a user query. Term Frequency–Inverse Document Frequency (TF-IDF) calculates the values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents where the word appears [12]. Given a document collection D, a word w, and an individual document d ϵ D, we calculate  P(w,d) f (w,d) u log | D |  (3)  f (w, D)  where f(w, d) denotes the number of times w that appears in d, |D| is the size of the corpus, and f(w,D) indicates the number of documents in which w appears in D [13]. In implementation, if w is an Out-of-Vocabulary term (OOV), the denominator f(w,D) becomes zero, and will be problematic (divided by zero). Thus, our model makes log (|D|/ f(w,D))=1 (IDF=1) when this situation occurs. Additionally, a list of stop-words in the target language are also used in query generation to remove the words which are high frequency but less discrimination power. Numbers are also treated as useful terms in our model, which also play an important role in distinguishing the documents. Finally, after evaluating and ranking all the words in a document by their scores, we take a portion of the (n-best) words for constructing the query and are guided by:  Sizeq [Opercent u Lend ]  (4)  Sizeq is the number of terms. λpercent is the percentage and is manually defined, which determines the Sizeq according to Lend, the length of the document. The model uses the first Sizeq-th words as the query. In another word, the larger document, the more words are selected as the query.  3.3. Document Retrieval Model  In order to use the generated query for retrieving documents, the core algorithm of the document retrieval model is derived from the Vector Space Model (VSM). Our system takes this model to calculate the similarity of each indexed document according to the input query. The final scoring formula is given by:  Score(q, d) coord(q, d) ¦tf (t, d)uidf (t)ubst u norm(t, d)  (5)  tin q  where tf(t,d) is the term frequency factor for term t in document d, idf(t) is the inverse document frequency of term t, while coord(q,d) is frequency of all the terms in query occur in a document. bst is a weight for each term in the query. Norm(t,d) encapsulates a few (indexing time) boost and length factors, for instance, weights for each document and field.  Updated: June 9, 2012 148  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) As a summary, many factors that could affect the overall score are taken into account in this model.  4. Model Evaluation  4.1. Datasets  In order to evaluate the retrieval performance of the proposed model on text of cross languages, we use the Europarl corpus which is the collection of parallel texts in 11 languages from the proceedings of the European Parliament [13]. The corpus is commonly used for the construction and evaluation of statistical machine translation1. The corpus consists of spoken records held at the European Parliament and are labeled with corresponding IDs (e.g. <CHAPTER id>, <SPEAKER id>). The corpus is quite suitable for use in training the proposed probabilistic models between different language pairs (e.g. English-Spanish, English-French, English-German, etc.), as well as for evaluating retrieval performance of the system.  Among the existing CLIR approaches, the work of Sánchez-Martínez et al. [8] based on SMT techniques and IBM Model 1 is very closed to our approach proposed in this paper. We take it as the benchmark and compare our model against this standard. In order to be able to compare with their results, we used the same datasets (training and testing data) for this evaluation. The chapters from April 1998 to October 2006 were used as a training set for model construction, both for training the Language Model (LM) and Translation Model (TM). While the chapters from April 1996 to March 1998 were considered as the testing set for evaluating the performance of the model.  We split the test set into two parts: (1) TestSet1, where each chapter (split by <CHAPTER id> label) is treated as a document, for tackling the large amount of information in long texts. (2) TestSet2, where each paragraph (split by <SPEAKER id> label) is treated as a document, for dealing with the low discrimination power. The analytical data of the corpus are presented in Table 1. There are 1,022 documents in TestSet1, which is the number chapter that the data contains. The average document length of this dataset is 5,612 words. In TestSet2, after processing, the data contain 23,342 documents (<SPEAKER id> level) which are the splitting 1,022 chapters (<CHAPTER id> level) from TestSet1. 22 out of 100 documents are in the same topic (<CHAPTER id> level). Table 1 summarizes the number of documents, sentences, words and the average word number of each document.  Table 1. Analytical Data of Corpus  Dataset Training Set TestSet1 TestSet2  Documents 2,900 1,022 23,342  Size of corpus  Sentences  Words  1,902,050 23,411,545  80,000 5,735,464  80,000 7,217,827  Ave. words in document 50 6,612 309  4.2. Experimental Setup In order to evaluate our proposed model, the following tools have been used.  
In this paper, we design a processing ﬂow to produce linked data in articles, providing anchorbased term’s additional information and related terms in different languages (English to Chinese). Wikipedia has been a very important corpus and knowledge bank. Although Wikipedia describes itself not a dictionary or encyclopedia, it is if high potential values in applications and data mining researches. Link discovery is a useful IR application, based on Data Mining and NLP algorithms and has been used in several ﬁelds. According to the results of our experiment, this method does make the result has improved. 摘要 本篇論文中提出了一套自動化流程以發掘潛在的關鍵字連結，並且在找出文章關鍵字後能夠提 供關鍵字於跨語言的相關資訊，而我們利用了Wikipedia做為我們的知識庫，藉由Wikipedia的 資料，系統能夠提供相關的關鍵字內容資訊，進而幫助使用者閱讀文章。論文中所提出的系統 整合了相關的資訊檢索技術以及自然語言處理相關的演算法，以利於幫助我們進行關鍵字的識 別以及相關的跨語言翻譯，同時系統整合了跨語連結發掘的技巧來幫助提供跨語言的關鍵字資 訊。經過初步的實驗證實，相較於baseline方法，此方法確實能夠始數據有所提昇。 Keywords: Cross-lingual link discovery, Linked data, Wikipedia, Link Discovery 關鍵字: 跨語連結發掘, 資料連結, 維基百科, 連結發掘 
The current study focuses on the similarities and differences of conceptual metaphor and metonymy between each genre in newspaper headlines. Headlines in news articles in Apple Daily from May 21st to May 27th were collected and analyzed. There are three basic findings. First, blocks for entertainment and sports used, in proportion, more metaphors and metonymies than any other blocks. Second, the idea of fighting was the most basic base for metaphors in Apple Daily. Third, TOPIC FOR SUBJECT was widely implemented to be economic in discourse. However, there may be more genres not included in Apple Daily. Also, the ways of categorization may not be specific enough for each block. Future studies are encouraged to further explore other genres excluded in the current study. Key words: metaphor, metonymy, newspaper, headline 1. Introduction Conceptual metaphor is the process of interpreting or understanding one domain which is relatively abstract by using another domain which is relatively concrete (Lakoff and Johnson, 2003). For example, TIME IS MONEY is a conceptual metaphor. The concrete domain “money” is used to understand abstract domain “time.” We can both spend money and time. Also, we can both waste money and time. Though most people are not aware of metaphors, they are everywhere (Lakoff and Johnson, 2003). In fat, since the rising of Conceptual Metaphor Theory, many scholars have been exploring examples of metaphors in specific contexts. For instance, Hsiao and Su (2010) have explored metaphors in discourse level. Even metaphors in pictorial representations are also the issues involving metaphors (Forceville, 1996). Metonymy is, to some extent, similar to conceptual metaphor, differing in that metonymy uses one concept in one domain to “refer to” or “stand for” another concept within the same domain (Lakoff and Johnson, 2003; Kovecses 2010). Examples of metonymy include HAND FOR PERSON. In Chinese, shou (‘hand’), which is part of body, is often referring to the whole person in example like toushou (pitch hand, ‘the person who pitches the ball’). Though the definition of conceptual metaphor is different from that of metonymy, 176  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) the two ideas are much related. In fact, metonymies serve as basis for, thus blend into, many conceptual metaphors (Kovecses, 2010). Metaphors have been widely used in our daily lives. We can see it everywhere. In fact, abundant examples of conceptual metaphor or metonymy have been provided by Lakoff and Johnson (2003), Kovecses (2010), and Gibbs (1994). In addition to the examples provided by those scholars, a lot more evidence of conceptual metaphor and metonymy can be found in headlines in newspapers. A good news headline presents the main ideas of the text efficiently to the readers. Also, it has to be interesting to attract readers’ attention. Metaphor no doubt plays an important role in the headlines. In other words, conceptual metaphors are implemented to present main ideas efficiently and attract readers’ attention. Since metonymy is, in some degree, related to conceptual metaphor, the fact that metonymy can also be found in newspaper is not implausible. In fact, Shie (2012) has discussed metaphors in headlines of news stories. Shie compared and analyzed the differences between headlines in New York Times, designed for English native speakers, and Times Supplement, designed for English as foreign language learners, in terms of language style, conventionality, and conceptual distance. Shie argued that metaphors in New York Times tend to be grand, unconventional, and long distance while those in Times Supplement prefer plain, conventional, and short distance (2012). Shie also discussed differences in metonymy in headlines in the two newspapers (2011). One of the main findings was that effect-for-cause metonymy was used to foreshadow the whole ideas and arouse reader’s curiosity. Moreover, metonymy was often used to be economic in discourse. Though Shie investigated much on differences of metaphors and metonymies in headlines in two newspapers, he did not pay any attention to the differences in headlines between each genre in one single newspaper. According to Devitt (1993), genre is patterns that writers would base on to categorize different writing tasks. Therefore, articles within one genre share similar features. Then, the application of metaphor and metonymy may be similar within one genre while different between different genres. Therefore, the current study will focus on the similarities and differences of conceptual metaphor and metonymy between each block in newspaper headlines in Chinese newspaper, Apple Daily, which is edited mainly for Chinese native speakers in Taiwan. The main goal is to investigate a) the overall tendency of usages of metaphors and metonymy, b) whether different blocks prefer different metaphors and metonymies, and c) the most basic metaphor and metonymy. There are five sections in this study: Abstract, Introduction, Methodology, Results, Discussions, and Conclusion. Introduction deals with research questions and organization. Methodology will explain the data collection procedure and identification of metaphor and metonymy. Results will report main discoveries based on the analyses of data. Discussions will try to interpret the results. Conclusion will summarize the findings and suggests for 177  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) future studies. 2. Methodology A self-constructed corpus is the main source for the current study. The corpus consists of all the news articles in Apple Daily printed from May 21st to May 27th, 2012. Headlines were identified as metaphors when the intended meaning was inconsistent with the literal meaning, and they were in different domains. Headlines were identified as metonymy when the intended meaning was inconsistent with the literal meaning, but they were still in the same domain. (1) ᯗ૫ҧᲛྪᖽဗ Yuan da xiao qiu ti tie ban Monkey play small ball kick iron board ‘Lamigo Monkeys played bunts but met obstacles.’ (Block D, May 27th, 2012) The headline in (1) serves as an example for identification of metaphor and metonymy. The news story was about the basketball game between Uni Lions and Lamigo Monkeys. Lamigo Monkeys used bunts in order to score. However, this strategy did not work. Uni Lions still performed pretty well to prevent Lamigo Monkeys from scoring. In (1), the literal meaning of verb phrase ti tie ban was ‘to kick iron board.’ However, the intended meaning was ‘to meet obstacles.’ Since the literal meaning ‘to kick iron board’ and the intended meaning ‘to meet obstacles’ were different, and they belonged to two different domains, this expression was identified as an example of metaphor. Headline in (1) also included an example of metonymy. The literal meaning of yuan was ‘monkey.’ However, the intended meaning was the team ‘Lamigo Monkeys.’ Since the literal meaning ‘monkey’ and the intended meaning ‘Lamigo Monkeys’ were different, and they belonged to the same domain, this expression was identified as an example of metonymy. Only non-lexicalized conceptual metaphors and metonymies, whose meanings could not be found in dictionaries, were selected into a sub-corpus. The dictionary the present author used was Chongbian guoyu cidian xiuding ben (Re-edited Chinese Dictionary-Revised Edition), an online dictionary edited by Ministry of Education in Taiwan. Therefore, the dictionary could be regarded as an authoritative dictionary. Therefore, only metaphors and metonymies whose meanings could not be found in Chongbian guoyu cidian xiuding ben were calculated and analyzed in this study. The metaphors and metonymies were categorized based on the blocks they were in. There are six blocks in Apple Daily: A, B, C, D, E, and P. Block A deals with headlines, the big events happened recently. Block B deals with business and stocks. Block C deals with entertainment. Block D deals with sports. Block E deals with life. Block P deals with houses and furniture. (Note that Block P only appears on Fridays and Saturdays.) 178  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) In the following section, the basic descriptive statistics about the numbers of metaphor and metonymy discovered in each block will be presented. Second, one example of metaphor and one example of metonymy from each block will be given and analyzed.  3. Results  First, the basic descriptive statistics about the numbers of metaphor and metonymy  discovered in each block were presented below.  Table 1 The number and percentage of news headlines with metaphor or metonymy in each  block  ġ  ġ  ġ  ġ  ġ  ġ  ġ  A  B  C  D  E  P  headlines with metaphor or  43 29 53 33 9  3  metonymy  all headlines  306 165 229 128 74 23  percentage  14.05% 17.58% 23.14% 25.78% 12.16% 13.04%  Table 1 shows the number and percentage of news headlines with metaphor or  metonymy in each block. As can be seen, Block C and D used more metaphors and  metonymies than other blocks.  Table 2 The number of headlines with metaphor and metonymy in each block  ġ  ġ  ġ  ġ  ġ  ġ  ġ  A  B  C  D  E  P  Metaphor 19  22  36  24  9  2  Metonymy 26  8  18  18  2  
This paper aims to investigate the effect of speech act and tone on rhythm. Participants were asked to produce four sets of words in five speech acts. PVI values of duration, pitch, and intensity were used to test the rhythm of vowels. Two main findings were concluded. First, speech act did not have any effect on rhythm, which may be caused by the fact that speech act were not performed on the controlled words in this study. Second, tone had an effect on rhythm in terms of pitch and intensity on some pairs. However, the comparison between the two pairs, tone1-tone2 and tone2-tone3, did not show any significant difference, which may be explained by the nature of phonetic features for tone1-tone2 pair while Chinese third tone sandhi for tone2-tone3 pair. However, this study only used the sets of words that had the same tone. Future studies can put more focus on different combinations of sets of words. Key words: speech act, tone, rhythm 1. Introduction The analysis of speech acts has been widely discussed since it was brought up by Austin (1962). A speech act consists of locutionary act, illocutionary act, and perlocutionary act (Austin, 1962). The main argument of speech acts have been focusing on semantic and syntactic domains. However, phonetic domain is little discussed. Though Searle (1965) further explored illocutionary act and proposed that the elements of function indicating device include stress and intonation contour, there was no further discussion related to phonetics. Therefore, in current study, it will examine speech acts in terms of phonetics. Rhythm is one of the issues that are dealt with in the field of phonetics. Pike (1946) and Abercrombie (1965, 1967) could be seen as pioneers in investigating the rhythm of language. They claimed that isochronism existed in all languages, and languages could be divided into two categories: stress-timed and syllable-timed. There have been abundant studies on speech rhythm. Grabe and Low (2000) investigated and compared different speech rhythms in eighteen languages. Since then, many scholars have been studying further deep into certain languages. For example, Deterding (2011) investigated the speech rhythm of Malay. So far, 185  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) the focuses have been mainly on the differences of speech rhythms between languages and the issue of second language acquisition. There are also many studies on the reasons for different speech rhythms within one language. Accents are believed to be one of the possible factors which may affect the speech rhythm (Rathcke and Smith, 2011). Nonetheless, many possible factors remain undiscovered. Therefore, the current study discusses the effects of two possible factors, tone and speech act, on speech rhythm in Chinese. The purpose of the current study is to locate whether tone or speech act have effects on speech rhythm of vowels in terms of duration, pitch, and intensity. 2. Method Three male students and seven female students were invited in the study. All of them were students in National Sun Yat-sen University, and they were all Chinese native speakers. The age ranged from 19 to 30. In the experiment, the participants were required to produce four sets of sentences: paobaobao (to throw up the bags), miaochaohao (to depict the person, Chaohao), paobaodao (to run Formosa), and qiaobaogao (to skip the homework).With the same ending vowel / C7 /, the effect of vowel quality was controlled. Also, the words in the same set had the same tone. Therefore, the effects of the tone are also under control. (Since the effect of tone sandhi on two tone 3 words is inevitable, it is not considered here.) In each set, there were five sentences corresponding to five different speech acts: command, warn, invite, refuse, and request. In other words, each participant produced 20 sentences in total. The subjects were asked to produce the sentence as if they were really performing the acts in the real context. They were free to add any words in the front or at the back of the sets of words to make the sentence sound more vivid and real. However, any changes to the sets of words were not allowed. All the sounds were recorded to be the data for current study. After the recording, pairwise variability index (PVI) of duration, pitch, and intensity of each vowel in the set of words was calculated. Analysis of variance (ANOVA) was used to detect if there is any significant difference of PVI values between different tones or different speech acts. Further, post hoc pairwise comparisons of the mean scores were performed using the Tukey HSD test if the result from ANOVA was significant. The significance level was set at .05 for all analyses. 3. Results First, the results of the effects of speech acts on intensity, pitch, and duration are presented as follows, respectively. Table 1 lays out the results of the one-way ANOVA comparing the mean difference between the PVI values for intensity of different speech acts. As shown, there was a non-significant difference in the PVI values for intensity of different speech acts [F (4, 195) = .23, p= 0.92]. 186  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)  Table 1 Results of the one-way ANOVA comparing the mean difference between the PVI values for intensity of different speech acts  Speech Acts  N  M  SD  Skewness Kurtosis  Command  40  3.61  2.59  1.52  2.55  Warn  40  3.66  2.03  .99  .21  Invite  40  3.26  2.29  1.10  .97  Refuse  40  3.40  2.02  .97  .74  Request  40  3.60  2.37  1.20  2.36  Source  of  SS  df  MS  F  Sig.  variation  Between groups 4.64  4  1.16  .23  .92  Within groups 1004.00 195  5.15  Total  1008.64 199  ġ  ġ  ġ  *p< .05  Table 2 lays out the results of the one-way ANOVA comparing the mean difference  
 This paper presents the results of main part-of-speech tagging of Turkish sentences using Conditional Random Fields (CRFs). Although CRFs are applied to many different languages for part-of-speech (POS) tagging, Turkish poses interesting challenges to be modeled with them. The challenges include issues related to the statistical model of the problem as well as issues related to computational complexity and scaling. In this paper, we propose a novel model for main-POS tagging in Turkish. Furthermore, we propose some approaches to reduce the computational complexity and allow better scaling characteristics or improve the performance without increased complexity. These approaches are discussed with respect to their advantages and disadvantages. We show that the best approach is competitive with the current state of the art in accuracy and also in training and test durations. The good results obtained imply a good ﬁrst step towards full morphological disambiguation.  
It is one of most essential issues to extract the keywords from conversational speech for understanding the utterances from speakers. This thesis aims at keyword spotting from spontaneous speech for keyword detecting. We proposed prosodic features that are used for keyword detection. The prosody words are segmented from speaker’s utterance according to the pre-training decision tree. The supported vector machine is further used as the classifier to judge the prosody word is keyword or not. The prosody word boundary segmentation algorithm based on decision tree is illustrated. Besides the data driven feature, the knowledge obtained from the corpus observation is integrated in the decision tree. Finally, the keyword 231  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) in the focus part are extracted using prosody features by sported vector machine (SVM). According to the experimental results, we can find the proposed method outperform the phone verification approach especially in recall and accuracy. This shows the proposed approach is operative for keyword detecting. 斄挝娆烉斄挝娆婆ˣ枛枣Ⱄ⿏ˣ枣⼳娆ˣ⎋徘婆妨ˤ Keywords: Keyword spotting, prosodic feature, prosody word, spoken language. ᶨˣ䵺婾 斄挝娆彐嬀(Keyword spotting)⛐役ẋ婆枛䞼䨞冯ㅱ䓐ᶲ䁢ᶨ枭⼰慵天䘬⬠⓷炻℞䚖 䘬㗗嬻暣儎䲣䴙傥⣈⽆婆枛屯㕁墉朊炻冒≽”㷔↢䈡⭂䘬斄挝娆⼁ˤ㕤ㅱ䓐ᶲḇ⊭⏓Ḯ ⼰⣂Ⰼ朊炻ἳ⤪婆枛屯㕁㩊䳊(㕘倆⟙⮶ˣ⼙䇯屯㕁ˣ暣夾廱㑕䫱)ˣ冒≽婆枛廱㍍䷥㨇 䲣䴙ˣ㞍娊㚵⊁䫱ˤ⛐Ṣ㨇㹅忂㕡朊炻ẍ冒䘤⿏婆枛(Spontaneous speech)䁢ᷣ天廠ℍ㕡 ⺷䘬⮵娙䲣䴙(Dialogue system)墉炻⚈䁢㭷ᾳṢ䘬妨婯桐㟤(Speaking style)悥㚱㇨ⶖ䔘炻 ⼰暋ẍ㔯㱽(Grammar)䘬奺⹎Ἦ⬴㔜↮㜸婆侭㇨天堐忼䘬㵝佑ˤ⛐⮎晃ㅱ䓐ᶲ炻侫慷⇘ ⮵娙䲣䴙ᷳ⌛㗪⿏(Real time)炻⤪ỽ嬻䲣䴙⮵㕤ἧ䓐侭䘬婆枛屯妲 ⃭↮䘬㌴㎉炻ℵℵ ⼙枧Ḯ⮵娙䲣䴙ᷳ⮎䓐冯⏎ˤKawahara 䫱Ṣ⯙㈲斄挝娆婆䘬㒟⍾(Keyword extraction) 冯 䡢 娵 (Verification) 䳸 ⎰ ⇾ 㜸 ☐ ㅱ 䓐 㕤 ⮵ 娙 䲣 䴙 炻 ℞ ᷣ 天 㬍 樇 ↮ 䁢 斄 挝 䇯 婆 ” 㷔 (Key-phrase detection)ˣ斄挝䇯婆槿嫱(Key-phrase verification)ˣ⎍⫸⇾㜸(Sentence parsing) ẍ⍲⎍⫸槿嫱(sentence verification)⚃ᾳ悐↮ˤ湣䚩䎮ⶍ⬠昊⇯㍸↢㻠忚⺷䘬⮵娙䎮妋㝞 㥳(Incremental understanding)炻㚱㓰㒟⍾䲣䴙㇨暨ᷳ屯妲[1]炻侴忁㧋䘬䎮⾝㚨㖑㗗䓙⽫ 䎮⬠侭 Charpter 䫱Ṣ㟡㒂Ṣ栆⎋婆⮵娙ᷳ䎮妋䦳⸷㇨㍸↢䘬[2]ˤ⎗ẍ゛夳斄挝娆冯㻠 忚⺷䎮妋㕡⺷⛐婆枛⮵娙ᷕ㗗⼰斄挝䘬ℑᾳ悐↮ˤ⚈㬌⤪ỽ⮯婆枛䘬斄挝娆㒟⍾↢Ἦ≈ ẍ䡢娵炻ὧ㗗⎋徘婆妨䎮妋(Spoken Language Understanding, SLU)ᶲᶨᾳ慵天䘬婚柴ˤ ⎎ᶨ㕡朊炻䁢Ḯ⡆≈婆枛彐嬀䘬㬋䡢⿏炻⼰⣂⬠侭㍸↢Ḯᶵ⎴䘬柵⢾婆枛䈡⿏Ἦ⸓≑彐 嬀炻╔㱣Ṇ䎮ⶍ㛶拎廅㔁㌰㍸↢ẍ䞍嬀䁢側㘗(Knowledge based)䘬㕡⺷[3]炻⮶ℍḮ婆枛 ⬠ᶲ䘬枛枣Ⱄ⿏(Prosodic attribute)ἄ䁢柵⢾䘬婆枛彐嬀庼≑㕡⺷ˤ䁢Ḯ㚱㓰⛘㕤䲣䴙ᶲ 㒟⍾斄挝娆炻㛔䞼䨞㍸↢枛枣Ⱄ⿏ᷳ斄挝娆㒟⍾㕡㱽炻啱䓙ᷕ䞼昊婆妨⬠䞼䨞㇨惕䥳尓 ㍸↢䘬昶Ⰼ⺷⣂䞕婆婆㳩枣⼳(Hierarchical Prosodic Phrase Grouping, HPG)㝞㥳㤪⾝ [4][5]炻”㷔婆侭䘬枣⼳娆(Prosodic word)炻ẍ”㷔䁢➢䢶䘬㕡⺷炻⍫䄏℞⎬䧖䈡⽝侴彐 ⇍㗗⏎䁢斄挝娆ˤ䁢Ḯ⡆≈婆枛彐嬀䘬㬋䡢䌯炻⚳⢾ḇ㚱⼰⣂⬠侭ḇῇ≑㕤℞Ṿ䘬䞍嬀 側㘗㕡⺷ˤAli ẍ倚⬠ᶲ䘬枛枣䈡⽝䁢➢䢶炻憅⮵㭷ᾳ枛䳈ᶵ⎴䘬䈡⽝Ἦ⋨⇍炻⇑䓐忁 ṃ䈡⽝Ἦẍ枛䳈䁢╖ỵ忚埴忋临婆枛彐嬀[1]ˤWieland ⇯憅⮵婆妨㧉✳炻㍸↢ẍ䴙妰㕡 ⺷᷎䳸⎰侫慷婆シ䘬㕡⺷炻⺢䩳 Bi-gram 㧉✳炻᷎ᶼἧ䓐 Beam-search Viterbi 㕡⺷Ἦ㏄ ⮳㚨Ἓ婆⎍嶗⼹ˤ䳸⎰忁ℑ溆᷎ㅱ䓐㕤⎋徘婆妨䘬彐嬀[6]ˤBitar ㍸↢㍊妶䳸⎰䞍嬀側 㘗炻ẍ忁ṃ䈡⽝Ἦἄ䁢彐嬀㕡㱽䘬⎬䧖⍫㔠炻昌Ḯ⁛䴙 HMM炻᷎䳸⎰⮰㤕䞍嬀ℵ姽Ộ炻 䳸㝄⛐婆枛彐嬀ᶲ㚱ᶵ拗䘬ㆸ㓰[7]ˤRabiner ⛐ 1989 ⸜炻婆枛彐嬀怬⯂㛒⼰ㆸ䅇㗪炻 ㍸↢Ḯ⇑䓐溆ˤ℞ᶨ䁢晙啷⺷楔⎗⣓㧉✳㕡⺷Ἦ彐嬀婆枛䘬㤪⾝冯憅⮵℞ㅱ䓐㕡⺷Ἦ妶 婾炻᷎ᶼ婒㖶Ḯ℞ℑ枭㑩㚱⬴㔜䘬㔠⬠䎮⾝冯㝞㥳炻᷎ᶼ⎗ẍ⺋㲃ㅱ䓐⛐⎬䧖柀➇ˤ䫔 232  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) Ḵ溆⯙㗗⮯℞ㅱ䓐⛐婆枛彐嬀ᶲ䡢⮎㚱列⤥䘬㓰㝄[8]ˤTatsuya Kawahara 冯 Chin-Hui Lee ㍸⇘ Key-Phrase Detection ␴ Verification 䘬䳸⎰炻シ⌛斄挝娆䘬㒟⍾冯䳬⎰炻⛐⮵ 㕤憅⮵⭴⏓㔯㱽䳸㥳檮㔋ᶼ嬲⊾⿏⣏ᷳ䈡⿏䘬⎋徘婆妨炻䲣䴙䎮妋℞⮵娙ℏ⭡ᶲ㚱⼰慵 ⣏䘬⸓≑[9]炻ḇ㚜≈⻟⊾ㆹᾹ⇑䓐㒟⍾斄挝娆Ἦ姽Ộ婆妨埴䁢䘬慵天⿏ˤ Ḵˣ䚠斄䞼䨞 ⛐斄挝娆彐嬀䘬䞼䨞ᶲ炻Rose[10]⇑䓐 HMM ⺢䩳Ḯᶨᾳ斄挝娆彐嬀䲣䴙炻ᾳ⇍妻 䶜斄挝娆冯朆斄挝娆悐↮炻ℵ⇑䓐䁢彐嬀斄挝娆䘬㔠ᾳ䉨ン炻怬㚱朆斄挝娆䘬⠓娆☐ (filler)㔠ᾳ炻㝞㥳㔜ᾳ彐嬀䵚䴉Ἦ彐嬀㔜㭝婆枛䘬斄挝娆悐↮ˤZhang[11]㍸↢ℑ昶㭝⺷ 䘬㕡㱽炻䫔ᶨ昶㭝彐嬀↢⎗傥⿏㚨檀䘬枛䳈⸷↿炻䫔Ḵ昶㭝啱䓙㶟Ḫ䞑昋⇌㕟℞䚠Ụ ⿏炻↿↢㚨㚱⎗傥䘬⸷↿炻㚨⼴⇯㒟⍾↢ᾉ⽫⹎㚨檀↮䘬ἄ䁢斄挝娆ˤBahi[12]㕡㱽ḇ 㗗栆Ụ炻⮯㭷ᾳ䘤枛枛䭨↮攳ㆸ⫿⃫↿䳬⎰炻⃰⮯⎗傥⿏斄挝娆妻䶜⤥䁢⫿⃫↿炻⎴㧋 ⮯彐嬀↢Ἦ䘬⎬䧖⎗傥⫿廱㎃ㆸ⫿⃫↿炻⇑䓐 HMM 㕡⺷⍣㭼⮵ỵ伖”㷔↢㚨㚱⎗傥䘬 斄挝娆᷎㒟⍾↢炻㬌䭯䈡溆⛐㕤᷎㰺㚱䈡シ⮵朆斄挝娆ἄ㧉✳ˤ湣䚩䎮ⶍ⬠昊䘬 Bazzi 䞼䨞⛐ HMM 彐嬀☐ᶳ朆斄挝娆䘬⠓娆☐姕妰[13]炻墉朊㍸⇘斄挝娆彐嬀ᷕ炻娆⹓⢾⫿ 娆ḇ㗗⌈⼰慵天悐↮炻↮㜸Ḯ拗婌嬎⟙☐䘬㬋䡢⿏嶇㔜橼彐嬀㬋䡢䌯䚠⮵斄Ὢ冯㇨⌈㭼 ἳˤLee C.H.[14]⛐㭼⮵䘤倚䚠Ụ⿏㗪炻柵⢾ℵ⍫侫䚠惘䘬䘤枛炻⇑䓐居㮷䎮婾ᷕ䘬居 㮷⚈⫸妰䬿㕡⺷炻Ἦ妰䬿↢䘤枛䚠Ụ⿏ˤKim[15]⇯ẍ居㮷䎮婾Ἦἄ䁢姽Ộ彐嬀⼴䘬䘤 枛℞ᾉ⽫⹎↮㔠ˤ⎎⢾⸦䧖㗗ẍ䈡⽝⍫㔠ἄ䁢”㷔➢䢶妻䶜↮栆☐[16][17]炻⇑䓐妻䶜 ☐Ἦ⮵天彐嬀䘬婆枛ἄ↮栆炻⇌⭂℞㗗⏎䁢天斄挝娆ˤ Haizhou Li, Bin Ma, and Chin-Hui Lee 㕤㛇↲ᶲ䘤堐䘬䞼䨞⣂婆彐嬀[18]炻㍸↢Ḯ㕘 䘬彐嬀╖⃫㥳゛炻ᶵℵẍ枛㧁䁢╖⃫侴㗗䓐⮎晃Ṣ栆䘤枛Ἦ 䁢╖ỵ炻᷎≈ẍ妻䶜㧉 ✳ˤ⼴䪗悐↮⛐⎬ᾳ婆妨䈡⽝ᶲ⺢䩳Ḯ⎹慷䨢攻㧉✳Ἦ⃚⬀⎬䧖婆妨ᶲ㭷䧖䘤枛䘬䚠⮵ 斄Ὢ炻᷎妻䶜↢↮栆☐Ἦ啱㬌↮栆彐嬀䁢ỽ䧖婆妨炻㇨⏰䎦䘬彐嬀䳸㝄㭼䓐⚳晃枛㧁怬 天⤥ᶲ姙⣂ˤ ⛐枛枣䞼䨞㕡朊炻役⸜ẍ⒍ΐ㭼Ṇ⣏⬠䘤⯽↢ᶨᾳ”㷔慵枛悐↮䘬ⶍ℟庇橼 AuToBi炻⎗ẍ憅⮵䞕婆怲䓴”㷔᷎ᶼ”㷔䘤枛慵枛悐ỵ[19]炻℞㔯䪈ḇ㍸⇘忁ṃ⸜Ἦ⎬ 䧖ᶵ⎴䘬”㷔慵枛㕡⺷炻㚱⇑䓐倚⬠Ⱄ⿏ˣ➢柣ˣ傥慷ˣPOS 䫱炻↮㜸ᶲḇ㚱 HMMˣ 㰢䫾㧡䫱㕡⺷ˤἳ⤪ Conkie 䫱Ṣ[20]炻憅⮵➢柣冯傥慷ẍ⍲忁ṃ⍫㔠ⶖῤ冯ⶖῤᷳ delta炻᷎䳸⎰ HMM 㝞㥳”㷔慵枛悐ỵˤ㬌⢾炻ḇ⣂≈ℍḮ婆侭䚠斄䘬倚⬠屯㕁炻⛐㸾 䡢⿏ᶲ䡢⮎㍸⋯ˤSridhar[21]⛐姽Ộ HMM ᷕ墉朊䘬⍫㔠ⶖῤ㗪炻⎴㗪䚋䜋倚⬠Ⱄ⿏冯 ⎍㱽Ⱄ⿏炻᷎ἧ䓐㚨⣏䅝 HMM 㧉✳Ἦ”㷔慵枛悐↮㇨⛐ỵ伖ˤ 䞼䨞⮵娙⽫䎮⬠㕡朊炻ㆹᾹ⍫侫Ḯ Erteschik-shir 䘬叿ἄ[22]炻墉朊㍷徘Ṣ栆⽫䎮⛐ ⮵娙埴䁢ᶲ䘬⎬䧖ね㱩冯⚆ㅱℏ⭡␴ね䵺炻婆侭␴倥侭㚫⛐⮵娙䘬忚埴ᶲᶵ㕟㓡嬲㇨㌴ ㎉䘬ᶵ⎴屯妲␴⽫䎮㇨㛇㛃䘬ℏ⭡ˤ 攟⹂⣏⬠⣂⸜Ἦᶨ䚜⽆ḳ㕤㛔⛇婆枛䘬䞼䨞炻昛⽿⬯[23]㬌䭯婾㔯㍊妶⎴㗪⮵⚳⎘ 暁婆䘬⣏娆⼁忋临婆枛彐嬀䞼䨞ˤ㖑㛇⚳⢾⶚嫱⮎晙啷⺷楔⎗⣓㧉✳ㅱ䓐㕤婆枛彐嬀ᶲ 䘬㓰㝄炻㣲㯠㲘[24]⮯℞ㅱ䓐㕤㓡┬㕤ᷕ㔯䘬婆枛彐嬀炻憅⮵枛䳈䘬㚧㎃㓡嬲⮯℞䓐㕤 ᷕ㔯䲣䴙ᶲˤἁ⭞冰[25]㇨ ᷳ婆枛彐嬀䘬䞼䨞炻㗗⇑䓐㚱旸䉨ン㨇䘬㕡⺷Ἦ忼⇘⣏娆 ⼁忋临婆枛彐嬀炻⮯婆枛彐嬀ᶲ䘬ᶱ䧖ᷣ天㧉✳炻倚⬠㧉✳ˣ录℠ˣ婆妨㧉✳悥⺢䩳ㆸ 㚱旸䉨ン㨇✳⺷炻忁㧋⛐㒜⃭⿏ᶲ冯䳸⎰ᶲ悥㚜䁢⭡㖻ˤ昛拓岊[26]䞼䨞”㷔婆枛ᶲ䘬 倚枣Ⱄ⿏”㷔炻⊭⏓ᶨṃ滣枛ˣ㒎枛ˣ䆮䟜枛䫱ˤ᷎䳸⎰倚枣Ⱄ⿏冯 MFCC Ἦ㍊妶婆 233  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) 枛彐嬀䘬㬋䡢䌯ˤ湫ⅈ忼[27]ẍ枛攟ˣ⍵㧉✳嶅暊ˣ倚⬠↮㔠䫱䈡⽝炻⇑䓐 SVM ↮栆 ☐Ἦ⮵倚㭵槿嫱↮栆㗗⏎Ⱄ㕤斄挝娆炻᷎⎎⢾⛐⮵㟠⽫↥㔠ᾖ㓡Ἦ㍸⋯㚨Ἓ㓰傥ˤ ⚳ℏᷕ䞼昊婆妨⬠䞼䨞㇨惕䥳尓憅⮵㻊婆㍸↢䘬 HPG 㝞㥳⮵ᷕ㔯枣⼳⬠䞼䨞⼙枧 ⼰⣏炻ẍ Fujisaki Model 妰䬿↮㜸᷎嫱⮎㭷ᶨⰌ枣⼳╖ỵ䘬䚠斄⿏ᶼḺ䚠⼙枧炻℞Ṿ䞼 䨞⊭㊔冒≽”㷔枣⼳怲䓴炻⎬⛘㻊婆⛐枣⼳ᶲ晾䃞⬀⛐ⶖ䔘⌣㛔岒䚠⎴䫱 [4][5][28][29][30]炻㬌 HPG 㤪⾝⮵婆枛⬠ᶲ㚱⼰⣂慵天届䌣ˤ ᶱˣ䲣䴙ṳ䳡 㛔䞼䨞䘬䲣䴙㝞㥳ὅ℞嗽䎮䦳⸷↮䁢妻䶜昶㭝冯㷔娎昶㭝ˤ妻䶜昶㭝ᷕ炻⇑䓐婆㕁 妻䶜↢”㷔斄挝娆㧉✳炻ẍ⍲䓐Ἦ⮯婆枛↮√ㆸ枣⼳娆婆枛㭝䘬㰢䫾㧡㧉✳ˤ㷔娎ᷕ炻 ẍ㬌ℑ㧉✳⃰⼴”㷔枣⼳娆怲䓴冯⎬枣⼳娆ᷳ䈡⽝⍫㔠炻㚨⼴”㷔斄挝娆᷎㒟⍾↢ˤ⚾ 1 䁢䲣䴙㝞㥳⚾ˤ ⚾ 1:䲣䴙㝞㥳⚾ 䁢Ḯ⛐⮎槿ᶲ”㷔↢枣⼳娆怲䓴䓐ẍ”㷔↢斄挝娆炻ㆹᾹ⽭枰⃰妻䶜↢㇨暨天䘬㧉 ✳ˤ⇑䓐㓞普䘬婆㕁⹓炻㉥↢⎬䧖枛枣Ⱄ⿏⍫㔠(Prosodic Attributes Extraction)炻⊭㊔Ḯ 枛檀(Pitch)ˣ枛⻟(Intensity)ˣ枛攟(Duration)ˤ”㷔枣⼳娆怲䓴㧉✳⇑䓐昶Ⰼ⺷⣂䞕婆枣 ⼳婆㳩㝞㥳(HPG)䞍嬀炻᷎䴙妰↮㜸⽆婆㕁ᷕ㇨㉥⍾䘬⍫㔠屯㕁炻㚨⼴妻䶜↢枣⼳娆怲 䓴(Prosodic Word Boundary)䘬㰢䫾㧡(Boundary Decision Tree)炻㬌䁢妻䶜枣⼳娆”㷔㧉 234  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) ✳悐↮ˤ侴⎎ᶨ㕡朊炻⎴㧋䓙忁ṃ枛枣Ⱄ⿏炻䴙妰↮㜸㈦↢⎬ᾳ婆枛ḳẞ䈑䎮䎦尉冯䚠 ⮵ㅱ䘬⍫㔠屯㕁炻⇑䓐 SVM 妻䶜↢崭⸛朊㧉✳ẍ䓐Ἦ↮彐斄挝娆冯朆斄挝娆炻㬌Ṏ⌛ 䁢⎬ᾳ䘬斄挝娆”㷔☐(Keyword Detector)炻忁悐↮ὧ䁢斄挝娆”㷔悐↮ˤ 侴㷔娎悐↮ᷕ㚨慵天䘬ℑᾳ悐↮⌛䁢枣⼳娆”㷔(Prosodic Word Detection)冯斄挝 娆”㷔(Keyword Detection)炻”㷔忁ℑ悐↮悥⽭枰ẘ岜妻䶜昶㭝㇨妻䶜↢䘬㧉✳ˤ㔜ᾳ ⮎槿椾⃰⽆ἧ䓐侭廠ℍ䘬婆枛妲嘇ᷕ炻㉥⍾↢⎬ᾳ枛枣Ⱄ⿏ˤ⇑䓐忁ṃ妲嘇⍫㔠庼ẍ怲 䓴㰢䫾㧡㧉✳炻㈦↢枣⼳娆怲䓴炻⚈侴⮯㔜㭝枛妲↮√ㆸ劍⸚ᾳ枣⼳娆䳬⎰ˤ㟡㒂忁ṃ 枣⼳娆䘬枛枣Ⱄ⿏炻徸ᶨẍ⎬ᾳ婆枛ḳẞ”㷔☐↮㜸揹⭂℞㗗⏎Ⱄ㕤斄挝娆ㆾ侭䁢朆斄 挝娆炻”㷔↢斄挝娆䘬㗪攻⋨㭝᷎㒟⍾↢Ἦˤ 庫㖑ᷳ⇵⮵㕤婆婧䘬䞼䨞炻⚳ℏᶨ䚜᷎䃉⮰憅⮵㻊婆Ἦ䞼䨞℞╖ỵ䳬ㆸ炻㇨妪⭂䘬 枣⼳╖ỵḇ㗗㱧䓐㕤⚳⢾䘬炻ᷣ天ἳ⤪枛䭨(syllable)ˣ枣⼳娆(prosodic word)ˣ婆婧䞕 婆(intonation phrase)䫱ˤ役⸦⸜Ἦ炻ᷕ䞼昊婆妨⬠䞼䨞㇨惕䥳尓䞼䨞㻊婆枣⼳䳸㥳炻慵 㕘妪墥枣⼳╖ỵ炻㬌䳸㥳␥⎵䁢ˬ昶Ⰼ⺷⣂䞕婆婆㳩枣⼳˭(Hierarchical Prosodic Phrase Grouping, HPG)[4][5]ˤ ⛐℞䚠斄䞼䨞ᷕ嫱㖶Ḯᷕ㔯⎋婆婆瀘⛐➢柣倚⬠ᶲ炻⎬Ⰼ䳂枣瀦╖ỵ冯怲䓴㓰ㅱ㚱 Ⰼ䳂斄ὪᶼḺ䚠⼙枧ˤ娚㝞㥳Ⰼ䳂䓙ᶳ侴ᶲ炻⮯枣瀦╖ỵ⭂䁢枛䭨(syllable, Syl)ˣ枣瀦 娆(prosodic word, PW)ˣ枣瀦䞕婆(prosodic phrase, PPh)ˣ␤⏠䳬(breath-group) ⍲⣂䞕婆 枣瀦潔佌(prosodic phrase group, PG)炻⌛婆㭝炻ℙḼ䳂䘬枣瀦╖ỵˤ℞怲䓴ḇ↮䁢Ḽ䳂炻 䓙ᶳ侴ᶲὅ⸷䁢 B1ˣB2ˣB3ˣB4 冯 B5ˤ℞叿ἄᷕ㇨㍸↢䘬㝞㥳⚾炻㶭㤂↮㜸↢婆䭯 䘬⎬ᾳⰌ䳂㤪⾝冯斄Ὢ炻⎬Ⰼ䳂㧁姀枮⸷䓙 B5 ⇘ B1炻⎗徸㬍⮯㔜ᾳ婆䭯↮䁢⎬Ⰼ⍲枣 ⼳╖ỵ炻㚨⼴⸽Ⰼ⎗ẍ↮ㆸ⇘㚨⮷枣⼳╖ỵ枛䭨ˤ 㛔䞼䨞⍫侫㬌㝞㥳炻㰚⍾枣⼳娆㤪⾝ἄ䁢⭂佑⎬ᾳ斄挝娆ᷳ╖ỵ炻⚈㬌⎒⮯怲䓴Ⰼ 䳂㓞㔪⇘ B2 Ⰼ䳂炻ἧ枛㭝㧁姀⼴⌛⎗夾䁢䓙⣂ᾳ枣⼳娆㇨䳬⎰侴ㆸˤ᷎ᶼ憅⮵⎬ᾳ枣 ⼳娆㉥⍾枛枣Ⱄ⿏䈡⽝炻⇑䓐忁ṃ䈡⽝䳬⎰Ἦ⇌⭂℞㗗⏎℟⁁㝸ṃ婆枛䈡⿏炻㚨⼴ㇵ揹 ⭂℞㗗⏎䁢㇨暨ᷳ斄挝娆ˤ⚾ 2 䁢⮯ᶨ枛㭝↮ㆸ枣⼳娆䣢シ⚾ˤB 㧁䣢䁢℞怲䓴炻⎗䚳 ↢㔜㭝婆枛↮√ㆸḼᾳ枣⼳娆ˤ ⚾ 2烉枣⼳娆(Prosody word)怲䓴䣢シ⚾ 235  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)  ⛐怲䓴”㷔ᶲ炻⍫侫ᷕ䞼昊婆妨⬠䞼䨞㇨惕䥳尓㇨㍸↢䘬怲䓴䈡⿏⭂佑[4][5]炻᷎ ≈ℍ枛⻟Ⱄ⿏Ἦ庼≑”㷔炻ẍ奨⮇℞䈡⿏⽆婆㕁⹓ᷕ妻䶜↢㰢䫾㧡Ἦἄ䁢怲䓴”㷔㱽 ⇯ˤ↮㜸⍇⇯䁢⛐㭷ᾳ➢柣㭝⯦䪗↮㜸℞⎬枭枛枣Ⱄ⿏炻㵝味Ḯ㬌㭝ᷳ➢柣Ⱄ⿏ˣ枛⻟ Ⱄ⿏ˣẍ⍲冯ᶳᶨ㭝䘬➢柣婧⿏ˤ⚾ 3 䁢㇨妻䶜↢㰢䫾㧡⍇⇯炻怲䓴⇌⭂ᶲℙ↮䁢 9 䧖栆⇍炻”㷔ỵ㕤⒒䧖栆⇍ẍ⇌⭂㗗⏎䁢怲䓴ˤ  ଶႥਔ໔  >Ƞ  ࢂ  ց  ᜐࣚ  ୷ᓎ‫و‬༈  Case 1 (Pause) ‫ݽ‬ϲ  ѳ጗  ߚᜐࣚ Case 2  Π΋ࢤ ୷ᓎ‫و‬༈  ‫ݽ‬ϲ  ѳ጗'Πफ़  ߚᜐࣚ ୷ᓎॶගϲ  Case 3 ࢂ  ց  Πफ़ Π΋ࢤ ୷ᓎ‫و‬༈  ‫ݽ‬ϲ  ॣம  ᡯफ़ᆶӣܹ  ࢂ  ց  ѳ጗'Πफ़  ୷ᓎॶගϲ  ࢂ  ց  ᜐࣚ  ߚᜐࣚ  Case 4 Case 5 (Pitch Reset)  ᜐࣚ Case 6  ߚᜐࣚ Case 7  ᜐࣚ Case 8 (Pitch Reset)  ߚᜐࣚ Case 9  ⚾ 3烉HPG 怲䓴⇌㕟㰢䫾㧡  㰢䫾㧡⮯㭷ᾳ➢柣㭝䘬忋㍍斄Ὢ↮䁢 9 ᾳ䧖栆ẍ⇌⭂㗗⏎䁢怲䓴炻᷎ᶼⷞ㚱ᶵ⎴ 䘬䈡⽝䎦尉炻᷎⎴㗪堐䎦↢枛枣Ⱄ⿏ᶲ䘬ⶖ䔘ˤ➢柣慵姕(Pitch reset)䁢怲䓴㚨慵天ᷳ 䈡⽝炻㚱㬌䎦尉⽭䁢怲䓴嗽炻℞检⇯⽭枰㭼庫⎬䧖➢柣冯枛⻟Ἦ⇌⭂ˤ堐 1 ⌛䁢⎬䧖 ᶵ⎴栆⇍䘬ⶖ䔘ˤ  栆⇍ Case 1 Case 2 Case 3 Case 4  堐 1烉HPG 怲䓴⇌㕟㰢䫾㧡ᷳ↮栆 䈡⽝  枛枣Ⱄ⿏  枣⼳娆冯枣⼳娆ᷳ攻倚婧廱㎃冯 枻  枻㗪攻 >Ƞ 䥺  枣⼳娆䘬攳⥳炻᷎ⷞ㚱ᶲ⋯䘤枛⽭忋㍍ᶳᾳ 忋临䘤枛  枣⼳娆婧⿏ㆸᶲ⋯崘⊊  婆婧⃰⸛䶑侴⼴ᶲ⋯炻⽭䁢忋临婆婧  ➢柣⸛䶑侴⼴ᶲ⋯  㕘䘬枣⼳娆攳⥳ᷳ㚨ᷣ天䈡⽝炻⌛➢柣慵姕 (Pitch reset)炻㬌䁢庫ᶵ㖶栗䘬➢柣慵姕  ➢柣ῤ⚆⇘檀溆  236  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)  Case 5 Case 6 Case 7 Case 8 Case 9  忋屓婆㯋炻婆婧⣏崘⊊⏰䎦忋屓⿏  ➢柣⏰䎦⣏崘⊊⎹ᶳㆾ⸛䶑  枣⼳娆冯枣⼳娆ᷳ攻倚婧㖶栗廱㎃ 枣⼳娆冯枣⼳娆ᷳ攻倚婧廱㎃Ữ婧⿏忋屓 㕘䘬枣⼳娆攳⥳ᷳ㚨ᷣ天䈡⽝炻⌛➢柣慵姕 (Pitch reset)炻㬌䁢㖶栗㖻夳䘬➢柣慵姕  ➢柣ᶳ旵侴⼴ᶲ⋯炻枛⻟⣏ⷭ ⹎旵ᶳ⼴㍸⋯ ➢柣ᶳ旵侴⼴ᶲ⋯炻枛⻟⏰䎦 䨑⭂崘⊊ ➢柣ῤ⚆⇘檀溆  忋屓婆㯋炻婆婧⣏崘⊊⏰䎦忋屓⿏  ➢柣⏰䎦⣏崘⊊⎹ᶳㆾ⸛䶑  ẍᶳ↿↢㰢䫾㧡ᶲ忳䓐⇘䘬枛枣Ⱄ⿏䈡⽝ᷳ⎬枭⍫㔠炻᷎徸ᶨṳ䳡ˤ (1)  枻㗪攻 ȡ=0.04 䥺烉 ⛐妻䶜婆㕁ᷕ䴙妰䘬 枻㗪攻㕤 0.03 ⇘ 0.05 ᷳ攻䷥⎰ỼḮ䳽⣏⣂㔠炻Ữ悐↮㗪 攻㤝䞕䘬 枻⭡㖻冯朆 枻㶟㵮炻⮯㖶栗 枻䘬㗪攻⇌⭂ῤ⭂⛐ 0.04 䥺ˤ  (2)➢柣崘⊊⇌㕟烉 天⇌㕟℞崘⊊暨忳䓐⇘➢柣㭝䘬⚆㬠㕡䦳⺷(⺷ 1)炻᷎⇑䓐䶂⿏徜㬠㕡⺷↢妰䬿 ↢℞㕄䌯(slope)炻⌛ȡiˤ  Pi (t) Di  Eit  (⺷ 1)  ⺷⫸ᷕ Pi(t)堐䫔 i 㭝➢柣㭝㕤㗪攻溆 t ᷳ➢柣ῤ炻⇯ȡi 䁢 i 㭝➢柣㭝ᷳ㕄䌯炻bi 䁢㬌 ➢柣㭝攳⥳㗪攻溆炻ei 䁢㗪攻䳸㜇溆ˤ侴ȡi ℞妰䬿㕡㱽⤪⺷ 2ˤ  ei ¦ (t  t )(Pi (t)  Pi )  E i  t bi  ei  ¦ (t  t )2  , t [bi , ei ] (⺷ 2)  t bi  t 䁢㗪攻⸛⛯ῤ炻⛐㗪攻庠ᶲṎ⎴㕤ᷕ攻ῤ炻妰䬿⤪⺷ 3ˤ Pi 䁢䫔 i 㭝➢柣ῤ⸛⛯ ῤ炻妰䬿⤪⺷ 4ˤn 䁢㬌➢柣枛㭝⍾㧋溆㔠ˤ  t  
In this paper, we propose a method for translating a given verb-noun collocation based on a parallel corpus and an additional monolingual corpus. Our approach involves two models to generate collocation translations. The combination translation model generates combined translations of the collocate and the base word, and filters translations by a target language model from a monolingual corpus, and the bidirectional alignment translation model generates translations using bidirectional alignment information. At run time, each model generates a list of possible translation candidates, and translations in two candidate lists are re-ranked and returned as our system output. We describe the implementation of using method using Hong Kong Parallel Text. The experiment results show that our method improves the quality of top-ranked collocation translations, which could be used to assist ESL learners and bilingual dictionaries editors. Keyword: collocation, statistical machine translation, computer-assisted translation 
This paper presents and experiments a new approach for automatic word sense disambiguation (WSD) applied for French texts. First, we are inspired from possibility theory by taking advantage of a double relevance measure (possibility and necessity) between words and their contexts. Second, we propose, analyze and compare two different training methods: judgment and dictionary based training. Third, we summarize and discuss the overall performance of the various performed tests in a global analysis way. In order to assess and compare our approach with similar WSD systems we performed experiments on the standard ROMANSEVAL test collection. Keywords: Word Sense Disambiguation, Semantic Dictionary of Contexts, Possibility Theory. 1. Introduction Word Sense Disambiguation (WSD) is the ability to identify the meaning of a word in its context in a computational manner. A lexical semantic disambiguation allows to select in a predefined list the significance of a word given its context. In fact, the task of semantic disambiguation requires enormous resources such as labeled corpora, dictionaries, semantic networks or ontologies. This task is important in many fields such as optical character recognition, lexicography, speech recognition, natural language comprehension, accent restoration, content analysis, content categorization, information retrieval and computer aided translation [13] [14]. The problem of WSD has been considered as a difficult task in the field of Natural Language Processing. In fact, a reader is frequently faced to problems of ambiguity in information retrieval or automatic translation tasks. Indeed, the main idea on which were based many researches in this field is to find relations between an occurrence of a word and its context 
In this paper we introduce a method for searching appropriate articles from knowledge bases (e.g. Wikipedia) for a given query and its context. In our approach, this problem is transformed into a multi-class classification of candidate articles. The method involves automatically augmenting smaller knowledge bases using larger ones and learning to choose adequate articles based on hyperlink similarity between article and context. At run-time, keyphrases in given context are extracted and the sense ambiguity of query term is resolved by computing similarity of keyphrases between context and candidate articles. Evaluation shows that the method significantly outperforms the strong baseline of assigning most frequent articles to the query terms. Our method effectively determines adequate articles for given query-context pairs, suggesting the possibility of using our methods in context-aware search engines. Keywords: entity linking, word sense disambiguation, Wikipedia, support vector machine, search engine 
Started from the very beginning, Stemming has been playing significant roles in several Natural Language Processing Applications such as information retrieval (IR), machine translation (MT), morph analysis and deciding the part of speech (POS). Several stemmers have been developed for a large number of languages including Indian languages; however no work has been done in Kokborok, a native language of Tripura. In this paper, we have designed a simple rule based stemmer for Kokborok using an affix stripping algorithm. The reduction of inflected words to the stem or root form is performed in the stemmer by stripping the affixes and applying boundary rules where needed. The stemming algorithm has been tested using a corpus of 32578 words and out of which 13044 were uniquely found to have an overall accuracy of 80.02% for minimum suffix stripping algorithm and 85.13% for maximum suffix stripping algorithm. Keywords: Stemming, part of speech (POS), Kokborok, suffix, prefix. [1. Introduction] Kokborok, an Indian language is spoken mainly in the states of Tripura, Assam, Manipur and Mizoram in India and in the neighbouring countries of Myanmar and Bangladesh by more than 2.5 million speakers1. Kokborok belongs to the Tibeto-Burman (TB) language family. Kokborok shares the genetic features of TB languages that include phonemic tone, widespread stem homophony, subject-object-verb (SOV) word order, agglutinative verb morphology, verb derivational suffixes originating from the semantic bleaching of verbs, duplication or elaboration, evidentiality and emotional attitudes signalled through sentence final particles, aspect rather than tense marking, lack of gender marking and tendency to reduce disyllabic forms to monosyllabic ones. Very specifically, Kokborok has extensive list of suffixes with more limited number of prefixes and different word classes that are formed by affixation of the respective markers. Kokborok is represented either in Roman script or in Bengali script however Bengali script is less preferred as it is difficult to project the actual 
Instead of directly providing the service of Chinese segmentation, some open-source software allows us to train segmentation models with segmented text. The resulting models can perform quite well, if training data of high quality are available. In reality, it is not easy to obtain sufficient and excellent training data, unfortunately. We report an exploration of using parallel corpora and various lexicons with techniques of identifying unknown words and near synonyms to automatically generate training data for such open-source software. We achieved promising results of segmentation in current experiments. Although the results fell short of outperforming the well-known Chinese segmenters, we believe that the proposed approach offers a viable alternative for users of the open-source software to generate their own training data. 斄挝娆烉㨇☐⬠佺炻婆㕁㧁姀炻㨇☐侣嬗 1. 䵺婾 ⮵㕤ᷕ㔯冒䃞婆妨嗽䎮炻ᷕ㔯㕟娆㗗ᶨ枭朆ⷠ慵天ᶼ➢䢶䘬ⶍἄˤᷕ㔯㕟娆㈨埻⣏农⎗↮䁢㱽⇯⺷㕟娆 㱽[10]⍲䴙妰⺷㕟娆㱽[5][16][26]ˤ䴙妰⺷㕟娆㱽⛐妻䶜㕟娆㧉✳㗪劍ẍ⣏慷檀⑩岒䘬妻䶜婆㕁忚埴妻䶜炻 ⇯忂ⷠ⎗㚱⤥䘬㕟娆㓰傥炻Ữ⚈䁢忂ⷠ德忶Ṣⶍ㕟娆㇨⼿䘬妻䶜婆㕁ㇵ傥㚱庫檀䘬⑩岒炻㇨ẍ檀⑩岒䘬 妻䶜婆㕁⼨⼨ᶵ㖻⍾⼿ˤ⚈㬌ㆹᾹ⺢䩳ᶨᾳ德忶ẍᶳ䦳⸷Ἦ㍸ὃᷕ㔯㕟娆㚵⊁䘬䲣䴙烉椾⃰德忶㞍娊⎬ 栆录℠䘬㕡⺷Ἦ䓊䓇ᷕ劙⸛埴婆㕁ᷳ㇨㚱ᷕ㔯⎍䘬⎬䧖㕟娆䳬⎰炻᷎⮯拗婌㕟娆䳬⎰⍣昌炻啱ẍ䓊䓇妻 䶜婆㕁烊䃞⼴ℵ⮯㇨䓊䓇䘬妻䶜婆㕁㍸ὃ䴎䵚嶗ᶲ䘬攳㓦庇橼⍣妻䶜㕟娆㧉✳炻ẍ⺢㥳ᷕ㔯㕟娆㚵⊁ˤ 341  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)  ⛐㛔婾㔯⼴临ℏ⭡炻ㆹᾹ⮯㇨⺢䩳䘬㍸ὃᷕ㔯㕟娆㚵⊁䘬䲣䴙1䯉䧙䁢ㆹᾹ䘬䲣䴙ˤ侴䔞ἧ䓐侭㍸ὃ㛒㕟 娆婆㕁䴎ㆹᾹ䘬䲣䴙㗪炻䲣䴙㚫ẍ妻䶜⤥䘬㕟娆㧉✳⮵㛒㕟娆婆㕁㕟娆ˤ ᷕ㔯㕟娆⬀⛐ℑᾳ慵天⓷柴烉㕟娆㬏䔘⿏⓷柴ˣ㛒䞍娆⓷柴ˤ㕟娆㬏䔘⿏⓷柴㗗㊯䔞ᶨᾳᷕ㔯⫿ᷚ ⎗ẍ塓㕟ㆸ㔠䧖㕟娆䳬⎰㗪炻⇯⊭⏓娚⫿ᷚ䘬⎍⫸⛐㕟娆⼴⎗傥㚫塓㕟ㆸᶵ䫎⎰⎍シ䘬拗婌㕟娆䳸㝄炻 忚侴⼙枧㕟娆㓰傥ˤ㕟娆㬏䔘⿏⓷柴⊭⏓䳬⎰✳㬏䔘(combination ambiguity)␴Ṍ普✳㬏䔘(overlapping ambiguity)炻⛐㛔䞼䨞ᷕㆹᾹ⎒叿慵嗽䎮Ṍ普✳㬏䔘ˤṌ普✳㬏䔘㗗䔞ᶨᾳᷕ㔯⫿ᷚȨABCȩ⎗ẍ塓㕟ㆸ ȨAB/Cȩ⍲ȨA/BCȩ㗪炷AˣBˣC 䘮䁢╖ᶨᷕ㔯⫿炻㕄䶂ẋ堐娆⼁攻䘬㕟娆溆炸炻⇯ȨABȩˣȨBCȩ㚫 㚱ℙ⎴䘬Ṍ普ȨBȩ炻⤪㬌⯙㚫⼊ㆸṌ普✳㬏䔘炻侴ㆹᾹ䧙ȨABCȩ䁢Ṍ普✳㬏䔘⫿ᷚˤLi[17]䫱⇑䓐朆䚋 䜋⺷炷unsupervised炸妻濤䘬㕡㱽嗽䎮Ṍ普✳㬏䔘炻㛔䞼䨞⇯德忶劙㻊侣嬗䘬屯妲⍣嗽䎮Ṍ普✳㬏䔘ˤ 㛒䞍娆㊯䘬㗗㛒㓞抬㕤录℠ᷕ䘬娆⼁炻ἳ⤪Ṣ⎵ˣ⛘⎵ˣ䳬䷼⎵䫱ˤ⛐㖍ⷠ䓇㳣ᷕṢᾹ㚫ᶵ㕟∝忈 ↢㕘䘬娆⼁炻㓭㛒䞍娆䴻ⷠ㚫↢䎦⛐㔯䪈ᷕˤ㕟娆䲣䴙⛐⮵㛒䞍娆㕟娆㗪忂ⷠ㚫↢䎦拗婌㕟娆䘬ね⼊炻 ㇨ẍ⤪㝄゛天㍸⋯㕟娆㓰傥炻⇯嗽䎮㛒䞍娆⓷柴㚫㗗⽭天䘬ⶍἄˤ⛐嗽䎮㛒䞍娆⓷柴䚠斄䞼䨞㕡朊炻Chen 䫱[11]⇑䓐ẍ婆㕁⹓䁢➢䘬⬠佺㱽(corpus-based learning approach)⍣䓊䓇夷⇯ẍ忚埴㛒䞍娆”㷔ˤ㒟⍾㛒 䞍娆㗪炻Chen 䫱[12]憅⮵Ⱄ㕤㛒䞍娆ᶨ悐ấ䘬╖⫿娆炻㚫⇌㕟娚╖⫿娆㗗⏎⎗ẍ␴䚠惘䘬娆⼁忚埴⎰Ἕˤ ㆹᾹ⽆ᷕ劙⸛埴婆㕁ᷕ㒟⍾㛒䞍娆ˣ㕘䘬ᷕ劙娆⮵炻啱㬌嗽䎮㛒䞍娆⓷柴冯㍸⋯⇑䓐劙㻊侣嬗屯妲 ⍣嗽䎮Ṍ普✳㬏䔘䘬㓰㝄ˤ㒟⍾ᷕ劙娆⮵冯㛒䞍娆ᷳ⣏䔍㳩䦳⤪ᶳ烉椾⃰⽆ᷕ劙⸛埴婆㕁ᷕ㒟⍾῁怠ᷕ 劙怢䔁娆⮵ˣ῁怠ᷕ㔯怢䔁⫿娆ˤᷳ⼴⇑䓐⎗傥⿏㭼ἳ(likelihood ratios)冯ℙ䎦柣䌯⮵῁怠ᷕ劙怢䔁娆⮵ 忚埴䮑怠炻⮯忂忶䮑怠䘬῁怠ᷕ劙怢䔁娆⮵夾䁢㬋䡢娆⮵炻≈ℍ军劙㻊录℠㧉䳬烊⇑䓐娆⿏⸷↿夷⇯⮵ ῁怠ᷕ㔯怢䔁⫿娆忚埴䮑怠炻⮯忂忶䮑怠䘬῁怠ᷕ㔯怢䔁⫿娆夾䁢㛒䞍娆炻≈ℍ军ᷕ㔯录℠㧉䳬ˤ  2. 䲣䴙㝞㥳  2.1 䲣䴙㳩䦳冯㝞㥳 ㆹᾹ䘬䲣䴙ᷳ㳩䦳冯㔜橼㝞㥳⤪⚾ᶨ㇨䣢炻侴㳩䦳ᷕ⎬㬍樇䘬娛䳘㕡㱽㚫⛐⼴临䪈䭨娛≈婒㖶烊椾⃰炻 ⮯⽆ᷕ劙⸛埴婆㕁ᷕ㇨䮑怠↢䘬῁怠ᷕ劙怢䔁娆⮵㒜⃭军劙㻊录℠㧉䳬炻⮯⽆ᷕ劙⸛埴婆㕁ᷕ㇨䮑怠↢  ĩŢĪ  ংᒧύЎ ᒪ੮ӷຒ  ύЎᜏ‫ڂ‬ኳಔ  मᅇᜏ‫ڂ‬ኳಔ  ংᒧύम ᒪ੮ຒჹ  ύमѳՉᇟ਑  ύЎѡ मЎѡ  ౢғᘐຒಔӝ  ճҔमᅇᙌ᝿ၗૻѐೀ౛ Ҭ໣ࠠ‫ݔ‬౦Ǵ٠ѐନᒱᇤ ᘐຒಔӝ  ૽ግᇟ਑  ૽ግᘐຒኳࠠ LingPipeύЎᘐຒᏔ ‫܈‬ ўϏՕύЎᘐຒᏔ  ᘐຒኳࠠ  ĩţĪ  ҂ᘐຒϐύЎ  ෳ၂ᇟ਑  ᘐຒኳࠠ  ᘐຒࡕࡑຑ ՗ϐᇟ਑  ⚾ᶨˣ䲣䴙䘬㳩䦳冯㝞㥳  
In this study, we propose and implement a concatenation-based audio signal synthesis system for the engine noises of continuously varying speed. A user simply draws the engine speed curve through an interface, and the corresponding audio signal is synthesized as output. This drawable interface makes the input function ﬂexible and reduces the input time. The implemented system was evaluated with subjective tests. Overall, the performance was good regarding quality and similarity. The proposed method can be feasibly applied to the synthesis of any sound objects which are produced with a clear and simple physical process. Furthermore, the technology can be integrated to virtual reality, such as in training and gaming applications. keywords: audio object synthesis, concatenation synthesis method, engine noise synthesis, virtual reality 3156  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) 一、緒論 (一)、研究背景、動機 聲音合成技術在人機介面裡扮演著重要的角色，目的是將聲音用人為的方式產生， 其中串接式合成方式為主要的合成技術之一。此合成方法是從錄製的聲音中找出所需 的合成單元，接著再做一些韻律方面的處理，之後將聲音單元串接。通常使用此方法 得到的聲音自然度和品質都相當不錯。在虛擬實境(Virtual Reality, VR)的機車引擎聲或 是坊間的賽車遊戲，往往用到的引擎聲都是預先錄製好的 [1]，這些錄製好的音檔， 雖然品質較佳，但在錄製時往往需要大量的時間和人力，且缺乏彈性。因此在這裡提 出一個手動繪圖的合成方式，來簡化輸入合成資訊的步驟，以四行程檔車的引擎聲為 例，利用最短時間和最少資源，來合成上述應用程式所需要的音檔。 (二)、相關研究 1、聲音合成 在聲音合成技術裡，基週同步疊加法(Pitch Synchronous Overlap Add, PSOLA) [2]為串接 式合成常用的調整動作。此方法先將波形分解成許多的基本波形，再將基本波形疊加 以得到合成的聲音波形。關於基本頻率和音長的調整，可利用基本波形的重疊間隔和 數目來達到，為現在常見的合成方法之一。但此方法的缺點為，在相鄰的合成單元的 串接邊界上，若建立合成單元庫時採用自動作切割的話，可能會造成共振峰軌跡銜接 不平順，降低合成聲音的流暢度。 除了PSOLA 的方式之外，還有語料庫為主(Corpus-based)的合成方式 [3]。其方法為 先錄製大量的語料，然後在合成時根據演算法從許多候選單元中選出一組會讓合成音 最為自然的組合。由於合成單元的選擇法並不會對錄製的語音作太多的信號處理動 作，此外可供候選的合成單元數目很多，使得語音單元間的不連續被降低很多，因此 合成音的自然度上是相當不錯的。在本文，我們簡化串接式語料庫為主的合成方式， 改以引擎聲音來當作合成單元，因此可以原音重現，具有極佳的合成音質，進而合成 出特定範圍的引擎聲。近年來，上述串接合成方式已應用在不少系統中且都有不錯的 表現，如微軟亞洲公司之木蘭(MULAN)系統 [4]和訊飛中文語音系統。 2、引擎合成 在國外，諧波同步疊加法(Harmonic Synchronous Overlap and Add, HSOLA) [5]被使用來 合成引擎噪音。此篇論文提到先採樣一個不斷變化預錄的引擎聲，然後使用諧波同步 累加法的方式。該方法的目的主要是減少階段式的不連續性，使其聽起來更具有連續 性。合成信號的和諧性被保留，提高了恢復原狀的音質。在其他的研究中發現到，車 輛產生的聲波波形，是由兩個部份的總和所組成 [6]。第一個是由引擎旋轉部件所產生 諧波相關的一連串音調，而第二個是由輪胎摩擦所產生的噪音。但在本文的引擎噪音 合成裡，為了減少合成的複雜度，故不考慮輪胎摩擦所產生的噪音。 (三)、系統概述及研究方向 本文的研究重點是嘗試以繪圖的方式輸入所需要的資訊，希望能減少輸入資訊所需 要的時間。也希望能更有彈性的，在特定轉速範圍間，能夠合成出想要的轉速音檔， 本文中的轉速皆以每一分鐘的轉速(rpm)為單位。在此篇論文中，因為採用串接的方 357  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) 式，合成出來的聲音在音色的自然度上有不錯的表現。圖1為系統概述圖，一開始可以 選擇兩種使用者介面來輸入所需要的資訊，分別是以文字的方式或是以繪圖的方式輸 入資訊。文字輸入的資訊包括開始時轉速、結束時轉速和合成時間。繪圖輸入的資訊 包括合成時間以及繪圖的曲線。採用繪圖輸入資訊的方式能更有彈性且快速的產生欲 合成的音檔。 圖 1、 輸入轉速資訊和信號輸出系統架構圖 (四)、四行程引擎簡介 四行程引擎(Four Stoke Engine)完成一次循環，必須經過「吸入、壓縮、點火、排 氣」四個步驟 [7]，其運作的程序分別是： ♦ 吸入行程：活塞往下，進氣閥打開，將空氣與燃料的混合氣吸入汽缸中。 ♦ 壓縮行程：進氣閥關閉且活塞往上，壓縮此混合氣使體積變小。 ♦ 點火行程：在壓縮的混合氣中點火，使氣體燃燒爆發並推動活塞往外作用。 ♦ 排氣行程：此時排氣閥打開且活塞再度往上，將燃燒後之廢氣排出汽缸。 根據以上四個行程，可以發現到當完成一個循環時，引擎轉了兩次。 二、合成單元收集 由於引擎聲的轉速在時域上主要為遞增或遞減的連續性變化，故在錄製音檔時，盡 可能的收錄大量的連續遞增或遞減音檔。在這一節裡主要是說明音檔的錄製、分析和 合成單元產生的過程。 (一)、音檔錄製 本文所收錄的音檔為野狼125 檔車的引擎聲，音檔共分為兩個部份。第一個部份為 一個長達3 分鐘左右遞增的引擎轉速音檔，將它令為S etA ；第二個部份為評測時所需 要合成的測試音檔，將它令為S etB 。S etA 錄製的方式為，採用人為的方式來線性增加 油閥的大小，以達到線性成長的轉速。但由於是以人為的方式來增加轉速，故很難達 到線性增加轉速，所以合成單元無法依照線性的時間來做切割，故我們將在之後的章 節來解決這個問題。S etB 為2 到16 秒共10 個不同轉速範圍的音檔，且轉速的變化為人 為隨機產生。轉速的範圍介於1000 轉到3000 轉之間，其轉速變化與時間資訊如表1 所 示。 358  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)  編號 1 2 3 4 5 6 7 8 9 10  轉速範圍 1000-2700 1160-2990-1530-2379-1250 1235-2783-1585 1454-1520-2961-2259 1030-2852-1113-2213-1208 -2786-1123-2570-1213-2790 -1206-2208-1310-2785-1630 1651-2772-1498 1635-1635-2901-1954 1628-1736-2493-1978 1972-1972 1111-2706  秒數範圍 (0)-(16) (0)-(1.8)-(2.7)-(4.1)-(6.6) (0)-(3.4)-(8.8) (0)-(3)-(3.8)-(4.1) (0)-(0.2)-(0.8)-(1.3)-(1.5)-(2.2) -(2.7)-(3.1)-(3.4)-(3.9)-(4.5) -(4.8)-(5.1)-(5.6)-(6) (0)-(1.6)-(5.8) (0)-(1.5)-(1.9)-(2.6) (0)-(2.3)-(3.5)-(4.3) (0)-(2.1) (0)-(2.7)  表 1、 S etB 音檔概要資訊  (二)、音檔分析 若將引擎的聲音以waveform 的形式表示，會發現到聲音的變換是非常具有規律性 的。將此音檔改以在頻譜上顯示，更容易發現其規律性的變化，因此我們著重於頻譜 的部份。圖2 為S etA 音檔其中一段引擎聲音的片段，所產生的waveform 和所對映的頻 譜圖。  圖 2、 上半部為SetA 其中一片段的waveform ，下半部為其對映的頻譜圖。  根據之前四行程的引擎運作原理，我們發現到完成一次循環，引擎共轉了兩次。且 此一循環也是引擎聲變換的一個週期，故我們可以根據此訊息來計算引擎的轉速。也 就是說我們只需要計算一個週期當下的sample 數，就可以得知其當下的轉速，轉速的 計算公式如下：  cycle  per  minute  =  sample rate sample in the cycle  ∗  2  ∗  60,  (1)  其中，在本文裡的sample rate 為44100Hz ，cyclesamples 為一個合成單元的sample 數，cycle per minute 為此合成單元每一分鐘的轉速。  359  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) (三)、合成單元的產生 根據轉速計算公式，找出S etA 音檔1000 轉到3000 轉的範圍，並以overlap 的方式切 成2000 個一秒左右的片段。但為了方便起見，我們將其編號為1000 至3000 並且只選取 以10 為單位的編號，共201 個片段。 接著將這些片段做頻譜的擷取來分析其頻率，如圖3(b) 所示。根據matlab 頻譜圖的 色度表，能量大到能量小顏色的變化為紅色到藍色，其中引擎聲的能量都集中於黃色 和紅色。黃色的色度值為-25 ，故我們將色度大於-25 的部份設為1 ，小於-25 部份設 為0 。然後將縱軸上的值累加起來，重新產生一個根據能量分佈的曲線圖，如圖3(c) 所 示。 之後，再根據此圖以人為的方式找出橫軸的切值。判斷的規則分別為要能切出最多 週期，並且要能接近最大峰值。將大於此值以上的部份保留，小於此值的部份設為0 。 並重新繪製出多個錐狀的圖，如圖3(d) 所示。 接著將每個錐狀體一開始非零的部分標記起來，最後將相鄰錐狀體標記的值相減， 就可以得出此一編號多個合成單元。  圖 3、 編號1000 的音檔片段所產生的waveform(a)，頻譜圖(b)，經由色度表重繪的能量 分布曲線圖(c)，根據適當切值重新繪製的錐狀圖(d)。  經由以上的方法共切出2015 個合成單元。但根據轉速計算公式，因為重複的關係， 只產生260 個不同轉速的合成單元。令其轉速為U = {ui|i = 1, ..., 260} 。接者，我們 令V 為欲找的轉速，如下式所示：  V = {v j|1000 + ( j − 1) ∗ 10, j = 1, ..., 201},  (2)  之後再根據|v j − U|, j = 1, ..., 201 取差值最小的ui 來代替v j 。部分對映如表2 所示。 且其轉速與sample 數的關係為近似一個如圖4 的反曲線。  360  Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)  表 2、 編號1 至編號10 的轉速對照表  編號 
Source code re-use has become an important problem in academia. The amount of code available makes necessary to develop systems supporting education that could address the problem of detection of source code re-use. We present the DeSoCoRe tool based on techniques of Natural Language Processing (NLP) applied to detect source code re-use. DeSoCoRe compares two source codes at the level of methods or functions even when written in different programming languages. The system provides an understandable output to the human reviewer in order to help a teacher to decide whether a source code is re-used. 
In this paper, we present XOpin, a graphical user interface that have been developed to provide a smart access to the results of a feature-based opinion detection system, build on top of a parser. 
Comment threads contain fascinating and useful insights into public reactions, but are challenging to read and understand without computational assistance. We present a tool for exploring large, community-created comments threads in an efficient manner. 
At present, online shopping is typically a search-oriented activity where a user gains access to products which best match their query. Instead, we propose a surf-oriented online shopping paradigm, which links associated products allowing users to ”wander around” the online store and enjoy browsing a variety of items. As an initial step in creating this experience, we constructed a prototype of an online shopping interface which combines product ontology information with topic model results to allow users to explore items from the food and kitchen domain. As a novel task for topic model application, we also discuss possible approaches to the task of selecting the best product categories to illustrate the hidden topics discovered for our product domain. 
We demonstrate a conversational humanoid robot that allows users to follow their own dialogue structures. Our system uses a hierarchy of reinforcement learning dialogue agents, which support transitions across sub-dialogues in order to relax the strictness of hierarchical control and therefore support ﬂexible interactions. We demonstrate our system with the Nao robot playing two versions of a Quiz game. Whilst language input and dialogue control is autonomous or wizarded, language output is provided by the robot combining verbal and non-verbal contributions. The novel features in our system are (a) the ﬂexibility given to users to navigate ﬂexibly in the interaction; and (b) a framework for investigating adaptive and ﬂexible dialogues. 
We describe MSR SPLAT, a toolkit for language analysis that allows easy access to the linguistic analysis tools produced by the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 
This paper presents a demonstration of a temporal reasoning system that addresses three fundamental tasks related to temporal expressions in text: extraction, normalization to time intervals and comparison. Our system makes use of an existing state-of-the-art temporal extraction system, on top of which we add several important novel contributions. In addition, we demonstrate that our system can perform temporal reasoning by comparing normalized temporal expressions with respect to several temporal relations. Experimental study shows that the system achieves excellent performance on all the tasks we address. 
This demonstration presents AttitudeMiner, a system for mining attitude from online discussions. AttitudeMiner uses linguistic techniques to analyze the text exchanged between participants of online discussion threads at different levels of granularity: the word level, the sentence level, the post level, and the thread level. The goal of this analysis is to identify the polarity of the attitude the discussants carry towards one another. Attitude predictions are used to construct a signed network representation of the discussion thread. In this network, each discussant is represented by a node. An edge connects two discussants if they exchanged posts. The sign (positive or negative) of the edge is set based on the polarity of the attitude identiﬁed in the text associated with the edge. The system can be used in different applications such as: word polarity identiﬁcation, identifying attitudinal sentences and their signs, signed social network extraction from text, subgroup detect in discussion. The system is publicly available for download and has an online demonstration at http://clair.eecs.umich.edu/AttitudeMiner/. 
Effective knowledge management is a key factor in the development and success of any organisation. Many different methods have been devised to address this need. Applying these methods to identify the experts within an organisation has attracted a lot of attention. We look at one such problem that arises within universities on a daily basis but has attracted little attention in the literature, namely the problem of a searcher who is trying to identify a potential PhD supervisor, or, from the perspective of the university’s research ofﬁce, to allocate a PhD application to a suitable supervisor. We reduce this problem to identifying a ranked list of experts for a given query (representing a research area). We report on experiments to ﬁnd experts in a university domain using two different methods to extract a ranked list of candidates: a database-driven method and a data-driven method. The ﬁrst one is based on a ﬁxed list of experts (e.g. all members of academic staff) while the second method is based on automatic Named-Entity Recognition (NER). We use a graded weighting based on proximity between query and candidate name to rank the list of candidates. As a baseline, we use a system that ranks candidates simply based on frequency of occurrence within the top documents. 
We introduce the automatic annotation of noun phrases in parsed sentences with tags from a ﬁne-grained semantic animacy hierarchy. This information is of interest within lexical semantics and has potential value as a feature in several NLP tasks. We train a discriminative classiﬁer on an annotated corpus of spoken English, with features capturing each noun phrase’s constituent words, its internal structure, and its syntactic relations with other key words in the sentence. Only the ﬁrst two of these three feature sets have a substantial impact on performance, but the resulting model is able to fairly accurately classify new data from that corpus, and shows promise for binary animacy classiﬁcation and for use on automatically parsed text. 
The preferred order of pre-nominal adjectives in English is determined primarily by semantics. Nevertheless, Adjective Ordering (AO) systems do not generally exploit semantic features. This paper describes a system that orders adjectives with significantly abovechance accuracy (73.0%) solely on the basis of semantic features pertaining to the cognitive-semantic dimension of subjectivity. The results indicate that combining such semantic approaches with current methods could result in more accurate and robust AO systems. 
We propose a technique to prepare the Google 1T n-gram data set for wildcarded frequency queries with a very low turnaround time, making unbatched applications possible. Our method supports token-level wildcarding and – given a cache of 3.3 GB of RAM – requires only a single read of less than 4 KB from the disk to answer a query. We present an indexing structure, a way to generate it, and suggestions for how it can be tuned to particular applications. 
This paper discusses a method for identifying diabetes symptoms and conditions in free text electronic health records in Bulgarian. The main challenge is to automatically recognise phrases and paraphrases for which no ”canonical forms” exist in any dictionary. The focus is on extracting blood sugar level and body weight change which are some of the dominant factors when diagnosing diabetes. A combined machine-learning and rule-based approach is applied. The experiment is performed on 2031 sentences of diabetes case history. The F-measure varies between 60 and 96% in the separate processing phases. 
This paper seeks to quantitatively evaluate the degree to which a number of popular metrics provide overlapping information to parser designers. Two routine tasks are considered: optimizing a machine learning regularization parameter and selecting an optimal machine learning feature set. The main result is that the choice of evaluation metric used to optimize these problems (with one exception among popular metrics) has little effect on the solution to the optimization. 
Semantic relatedness, or its inverse, semantic distance, measures the degree of closeness between two pieces of text determined by their meaning. Related work typically measures semantics based on a sparse knowledge base such as WordNet1 or CYC that requires intensive manual efforts to build and maintain. Other work is based on the Brown corpus, or more recently, Wikipedia. Wikipediabased measures, however, typically do not take into account the rapid growth of that resource, which exponentially increases the time to prepare and query the knowledge base. Furthermore, the generalized knowledge domain may be difﬁcult to adapt to a speciﬁc domain. To address these problems, this paper proposes a domain-speciﬁc semantic relatedness measure based on part of Wikipedia that analyzes course descriptions to suggest whether a course can be transferred from one institution to another. We show that our results perform well when compared to previous work. 
This paper presents a thesis proposal on approaches to automatically scoring non-native speech from second language tests. Current speech scoring systems assess speech by primarily using acoustic features such as fluency and pronunciation; however content features are barely involved. Motivated by this limitation, the study aims to investigate the use of content features in speech scoring systems. For content features, a central question is how speech content can be represented in appropriate means to facilitate automated speech scoring. The study proposes using ontologybased representation to perform concept level representation on speech transcripts, and furthermore the content features computed from ontology-based representation may facilitate speech scoring. One baseline and two ontology-based representations are compared in experiments. Preliminary results show that ontology-based representation slightly improves performance of one content feature for automated scoring over the baseline system. 
Statistical natural language processing (NLP) builds models of language based on statistical features extracted from the input text. We investigate deep learning methods for unsupervised feature learning for NLP tasks. Recent results indicate that features learned using deep learning methods are not a silver bullet and do not always lead to improved results. In this work we hypothesise that this is the result of a disjoint training protocol which results in mismatched word representations and classiﬁers. We also hypothesise that modelling long-range dependencies in the input and (separately) in the output layers would further improve performance. We suggest methods for overcoming these limitations, which will form part of our ﬁnal thesis work.  x∈X φ(x) y∈Y NE Tags  the  cat  φ(x1) φ(x2)  B-NP I-NP  [the cat] NP  sits φ(x3 ) B-VP [sits] VP  on φ(x4 ) O [on] O  the  mat  φ(x5) φ(x6)  B-NP I-NP  [the mat] NP  Table 1: Example NLP syntactic chunking task for the sentence “the cat sits on the mat”. X represents the words in the input space, Y represents labels in the output space. φ(x) is a feature representation for the input text x and the bottom row represents the output named entity tags in a more standard form.  is generally that features represent strong discriminating characteristics of the problem gained through manual engineering and domain-speciﬁc insight. As a concrete example, consider the task of syntactic chunking, also called “shallow parsing”, (Gildea and Jurafsky, 2002): Given an input string, e.g.  
To date, researchers have proposed different ways to compute the readability and coherence of a text using a variety of lexical, syntax, entity and discourse properties. But these metrics have not been deﬁned with special relevance to any particular genre but rather proposed as general indicators of writing quality. In this thesis, we propose and evaluate novel text quality metrics that utilize the unique properties of different genres. We focus on three genres: academic publications, news articles about science, and machine generated text, in particular the output from automatic text summarization systems. 
We study1 the problem of extracting all possible relations among named entities from unstructured text, a task known as Open Information Extraction (Open IE). A state-of-theart Open IE system consists of natural language processing tools to identify entities and extract sentences that relate such entities, followed by using text clustering to identify the relations among co-occurring entity pairs. In particular, we study how the current weighting scheme used for Open IE affects the clustering results and propose a term weighting scheme that signiﬁcantly improves on the state-of-theart in the task of relation extraction both when used in conjunction with the standard tf · idf scheme, and also when used as a pruning ﬁlter. 
Much has been written about humor and even sarcasm automatic recognition on Twitter. The task of classifying humorous tweets according to the type of humor has not been confronted so far, as far as we know. This research is aimed at applying classiﬁcation and other NLP algorithms to the challenging task of automatically identifying the type and topic of humorous messages on Twitter. To achieve this goal, we will extend the related work surveyed hereinafter, adding different types of humor and characteristics to distinguish between them, including stylistic, syntactic, semantic and pragmatic ones. We will keep in mind the complex nature of the task at hand, which emanates from the informal language applied in tweets and variety of humor types and styles. These tend to be remarkably different from the type speciﬁc ones recognized in related works. We will use semi-supervised classiﬁers on a dataset of humorous tweets driven from different Twitter humor groups or funny tweet sites. Using a Mechanical Turk we will create a gold standard in which each tweet will be tagged by several annotators, in order to achieve an agreement between them, although the nature of the humor might allow one tweet to be classiﬁed under more than one class and topic of humor. 
Many works (of both ﬁction and non-ﬁction) span multiple, intersecting narratives, each of which constitutes a story in its own right. In this work I introduce the task of multiple narrative disentanglement (MND), in which the aim is to tease these narratives apart by assigning passages from a text to the sub-narratives to which they belong. The motivating example I use is David Foster Wallace’s ﬁctional text Inﬁnite Jest. I selected this book because it contains multiple, interweaving narratives within its sprawling 1,000-plus pages. I propose and evaluate a novel unsupervised approach to MND that is motivated by the theory of narratology. This method achieves strong empirical results, successfully disentangling the threads in Inﬁnite Jest and signiﬁcantly outperforming baseline strategies in doing so. 
Argumentative discourse contains not only language expressing claims and evidence, but also language used to organize these claims and pieces of evidence. Differentiating between the two may be useful for many applications, such as those that focus on the content (e.g., relation extraction) of arguments and those that focus on the structure of arguments (e.g., automated essay scoring). We propose an automated approach to detecting high-level organizational elements in argumentative discourse that combines a rule-based system and a probabilistic sequence model in a principled manner. We present quantitative results on a dataset of human-annotated persuasive essays, and qualitative analyses of performance on essays and on political debates. 
Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost. For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow. We ﬁrst show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding. We then consider a more ﬂexible, non-ITG matching constraint which is less efﬁcient for exact inference but more efﬁcient for BP. With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up. 
The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufﬁcient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture. In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering signiﬁcant improvements in performance.  
Arabic Dialects present many challenges for machine translation, not least of which is the lack of data resources. We use crowdsourcing to cheaply and quickly build LevantineEnglish and Egyptian-English parallel corpora, consisting of 1.1M words and 380k words, respectively. The dialectal sentences are selected from a large corpus of Arabic web text, and translated using Amazon’s Mechanical Turk. We use this data to build Dialectal Arabic MT systems, and ﬁnd that small amounts of dialectal data have a dramatic impact on translation quality. When translating Egyptian and Levantine test sets, our Dialectal Arabic MT system performs 6.3 and 7.0 BLEU points higher than a Modern Standard Arabic MT system trained on a 150M-word Arabic-English parallel corpus. 
Standard entity clustering systems commonly rely on mention (string) matching, syntactic features, and linguistic resources like English WordNet. When co-referent text mentions appear in diﬀerent languages, these techniques cannot be easily applied. Consequently, we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters. Our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures. Crucially, we do not assume a pre-existing entity list (knowledge base), so entity characteristics are unknown. On an Arabic-English corpus that contains seven different text genres, our best model yields a 24.3% F1 gain over the baseline. 
This paper addresses the extraction of event records from documents that describe multiple events. Speciﬁcally, we aim to identify the ﬁelds of information contained in a document and aggregate together those ﬁelds that describe the same event. To exploit the inherent connections between ﬁeld extraction and event identiﬁcation, we propose to model them jointly. Our model is novel in that it integrates information from separate sequential models, using global potentials that encourage the extracted event records to have desired properties. While the model contains high-order potentials, efﬁcient approximate inference can be performed with dualdecomposition. We experiment with two data sets that consist of newspaper articles describing multiple terrorism events, and show that our model substantially outperforms traditional pipeline models. 
A citing sentence is one that appears in a scientiﬁc article and cites previous work. Citing sentences have been studied and used in many applications. For example, they have been used in scientiﬁc paper summarization, automatic survey generation, paraphrase identiﬁcation, and citation function classiﬁcation. Citing sentences that cite multiple papers are common in scientiﬁc writing. This observation should be taken into consideration when using citing sentences in applications. For instance, when a citing sentence is used in a summary of a scientiﬁc paper, only the fragments of the sentence that are relevant to the summarized paper should be included in the summary. In this paper, we present and compare three different approaches for identifying the fragments of a citing sentence that are related to a given target reference. Our methods are: word classiﬁcation, sequence labeling, and segment classiﬁcation. Our experiments show that segment classiﬁcation achieves the best results. 
We present a model for detecting user disengagement during spoken dialogue interactions. Intrinsic evaluation of our model (i.e., with respect to a gold standard) yields results on par with prior work. However, since our goal is immediate implementation in a system that already detects and adapts to user uncertainty, we go further than prior work and present an extrinsic evaluation of our model (i.e., with respect to the real-world task). Correlation analyses show crucially that our automatic disengagement labels correlate with system performance in the same way as the gold standard (manual) labels, while regression analyses show that detecting user disengagement adds value over and above detecting only user uncertainty when modeling performance. Our results suggest that automatically detecting and adapting to user disengagement has the potential to signiﬁcantly improve performance even in the presence of noise, when compared with only adapting to one affective state or ignoring affect entirely. 
Most previous research on automated speech scoring has focused on restricted, predictable speech. For automated scoring of unrestricted spontaneous speech, speech proﬁciency has been evaluated primarily on aspects of pronunciation, ﬂuency, vocabulary and language usage but not on aspects of content and topicality. In this paper, we explore features representing the accuracy of the content of a spoken response. Content features are generated using three similarity measures, including a lexical matching method (Vector Space Model) and two semantic similarity measures (Latent Semantic Analysis and Pointwise Mutual Information). All of the features exhibit moderately high correlations with human proﬁciency scores on human speech transcriptions. The correlations decrease somewhat due to recognition errors when evaluated on the output of an automatic speech recognition system; however, the additional use of word conﬁdence scores can achieve correlations at a similar level as for human transcriptions. 
This study aims to infer the social nature of conversations from their content automatically. To place this work in context, our motivation stems from the need to understand how social disengagement affects cognitive decline or depression among older adults. For this purpose, we collected a comprehensive and naturalistic corpus comprising of all the incoming and outgoing telephone calls from 10 subjects over the duration of a year. As a ﬁrst step, we learned a binary classiﬁer to ﬁlter out business related conversation, achieving an accuracy of about 85%. This classiﬁcation task provides a convenient tool to probe the nature of telephone conversations. We evaluated the utility of openings and closing in differentiating personal calls, and ﬁnd that empirical results on a large corpus do not support the hypotheses by Schegloff and Sacks that personal conversations are marked by unique closing structures. For classifying different types of social relationships such as family vs other, we investigated features related to language use (entropy), hand-crafted dictionary (LIWC) and topics learned using unsupervised latent Dirichlet models (LDA). Our results show that the posteriors over topics from LDA provide consistently higher accuracy (60-81%) compared to LIWC or language use features in distinguishing different types of conversations. 
Conditional Random Fields (CRFs) are a popular formalism for structured prediction in NLP. It is well known how to train CRFs with certain topologies that admit exact inference, such as linear-chain CRFs. Some NLP phenomena, however, suggest CRFs with more complex topologies. Should such models be used, considering that they make exact inference intractable? Stoyanov et al. (2011) recently argued for training parameters to minimize the task-speciﬁc loss of whatever approximate inference and decoding methods will be used at test time. We apply their method to three NLP problems, showing that (i) using more complex CRFs leads to improved performance, and that (ii) minimumrisk training learns more accurate models. 
Most existing theory of structured prediction assumes exact inference, which is often intractable in many practical problems. This leads to the routine use of approximate inference such as beam search but there is not much theory behind it. Based on the structured perceptron, we propose a general framework of “violation-ﬁxing” perceptrons for inexact search with a theoretical guarantee for convergence under new separability conditions. This framework subsumes and justiﬁes the popular heuristic “early-update” for perceptron with beam search (Collins and Roark, 2004). We also propose several new update methods within this framework, among which the “max-violation” method dramatically reduces training time (by 3 fold as compared to earlyupdate) on state-of-the-art part-of-speech tagging and incremental parsing systems. 
We propose a new segmentation evaluation metric, called segmentation similarity (S), that quantiﬁes the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. We propose several adapted inter-annotator agreement coefﬁcients which use S that are suitable for segmentation. We show that S is conﬁgurable enough to suit a wide variety of segmentation evaluations, and is an improvement upon the state of the art. We also propose using inter-annotator agreement coefﬁcients to evaluate automatic segmenters in terms of human performance. 
It is common knowledge that translation is an ambiguous, 1-to-n mapping process, but to date, our community has produced no empirical estimates of this ambiguity. We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations for a sentence. Our ﬁndings show that naturally occurring sentences have billions of translations. Having access to such large sets of meaning-equivalent translations enables us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics. 
We examine evaluation methods for systems that automatically annotate images using cooccurring text. We compare previous datasets for this task using a series of baseline measures inspired by those used in information retrieval, computer vision, and extractive summarization. Some of our baselines match or exceed the best published scores for those datasets. These results illuminate incorrect assumptions and improper practices regarding preprocessing, evaluation metrics, and the collection of gold image annotations. We conclude with a list of recommended practices for future research combining language and vision processing techniques. 
We propose to re-examine the hypothesis that automated metrics developed for MT evaluation can prove useful for paraphrase identiﬁcation in light of the signiﬁcant work on the development of new MT metrics over the last 4 years. We show that a meta-classiﬁer trained using nothing but recent MT metrics outperforms all previous paraphrase identiﬁcation approaches on the Microsoft Research Paraphrase corpus. In addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. Finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identiﬁcation approach relying solely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community. 
As interest grows in the use of linguistically annotated corpora in research and teaching of foreign languages and literature, treebanks of various historical texts have been developed. We introduce the first large-scale dependency treebank for Classical Chinese literature. Derived from the Stanford dependency types, it consists of over 32K characters drawn from a collection of poems written in the 8th century CE. We report on the design of new dependency relations, discuss aspects of the annotation process and evaluation, and illustrate its use in a study of parallelism in Classical Chinese poetry. 
In a large-scale study of how people ﬁnd topical shifts in written text, 27 annotators were asked to mark topically continuous segments in 20 chapters of a novel. We analyze the resulting corpus for inter-annotator agreement and examine disagreement patterns. The results suggest that, while the overall agreement is relatively low, the annotators show high agreement on a subset of topical breaks – places where most prominent topic shifts occur. We recommend taking into account the prominence of topical shifts when evaluating topical segmentation, effectively penalizing more severely the errors on more important breaks. We propose to account for this in a simple modiﬁcation of the windowDiff metric. We discuss the experimental results of evaluating several topical segmenters with and without considering the importance of the individual breaks, and emphasize the more insightful nature of the latter analysis. 
This paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning, speciﬁcally the framework of empirical risk minimization. We review well-known algorithms, arguing that they do not optimize the loss functions they are assumed to optimize when applied to machine translation. Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches. 
We propose an algorithm to ﬁnd the best path through an intersection of arbitrarily many weighted automata, without actually performing the intersection. The algorithm is based on dual decomposition: the automata attempt to agree on a string by communicating about features of the string. We demonstrate the algorithm on the Steiner consensus string problem, both on synthetic data and on consensus decoding for speech recognition. This involves implicitly intersecting up to 100 automata. 
Much previous work on Transliteration Mining (TM) was conducted on short parallel snippets using limited training data, and successful methods tended to favor recall. For such methods, increasing training data may impact precision and application on large comparable texts may impact precision and recall. We adapt a state-of-the-art TM technique with the best reported scores on the ACL 2010 NEWS workshop dataset, namely graph reinforcement, to work with large training sets. The method models observed character mappings between language pairs as a bipartite graph and unseen mappings are induced using random walks. Increasing training data yields more correct initial mappings but induced mappings become more error prone. We introduce parameterized exponential penalty to the formulation of graph reinforcement and we estimate the proper parameters for training sets of varying sizes. The new formulation led to sizable improvements in precision. Mining from large comparable texts leads to the presence of phonetically similar words in target and source texts that may not be transliterations or may adversely impact candidate ranking. To overcome this, we extracted related segments that have high translation overlap, and then we performed TM on them. Segment extraction produced significantly higher precision for three different TM methods. 1. Introduction Transliteration Mining (TM) is the process of finding transliterations in parallel or comparable  texts of different languages. For example, given the Arabic-English word sequence pairs: ( ‫اﺍﻟﻤﻠﻚ ھﮪﮬﻫﺎﻟﻲ‬ ‫ﺳﻼﺳﻲ‬, Haile Selassie I of Ethiopia), successful TM would mine the transliterations: (‫ھﮪﮬﻫﺎﻟﻲ‬, Haile) and (‫ﺳﻼﺳﻲ‬, Selassie). TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications. For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al., 2009a). In Machine Translation (MT), TM can improve alignment at training time and help enrich phrase tables with named entities that may not appear in parallel training data. More broadly, TM is a character mapping problem. Having good character mapping models can be beneficial in a variety of applications such as learning stemming models, learning spelling transformations between similar languages, and finding variant spellings of names (Udupa and Kumar, 2010b). TM has attracted interest in recent years with a dedicated evaluation in the ACL 2010 NEWS workshop. In that evaluation, TM was performed using limited training data, namely 1,000 parallel transliteration word-pairs, on short parallel text segments, namely cross-language Wikipedia titles which were typically a few words long. Since TM was performed on very short parallel segments, the chances that two phonetically similar words would appear within such a short text segment in one language were typically very low. Also, since TM training datasets were small, many valid mappings were not observed in training. For these two reasons, most of the successful techniques related to that evaluation have focused on improving recall, while hurting precision slightly. Some of these techniques involved the use of letter conflation based on a SOUNDEX like scheme (Darwish, 2010; Oh and Choi, 2006) and character  243 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 243–252, Montre´al, Canada, June 3-8, 2012. c 2012 Association for Computational Linguistics  n-gram similarity. The most successful technique on ACL-NEWS dataset, involved the use of graph reinforcement in which observed mappings between language pairs were modeled using a bipartite graph and unseen mappings were induced using random walks (El-Kahki et al., 2011). In this paper, we focus on improving TM between Arabic and English in more realistic settings, compared to the NEWS workshop dataset. Specifically, we focus on the cases where: 1. Relatively large TM training sets, which are typical of production systems, are available. As we will show, using more training data in conjunction with recall-oriented techniques that perform well on small training sets can adversely hurt precision, leading to drops in F-measure. A more fundamental question is what constitutes “large” versus “small” training sets. Ideally, we want a unified solution for training sets of varying sizes. 2. TM is performed on large comparable texts which are ubiquitously available from different sources such as cross language news and Wikipedia articles. In this case, there are two phenomena that arise. First, there is an increased probability (compared to short texts) that words in the target and source texts may be phonetically similar, while not being transliterations of each other. One such example is the Arabic word “‫”ﻣﻦ‬, which means “in” and is pronounced as “min” and the English word “men”. Such cases adversely affect precision. Second, given a source language word, there may be multiple target language words that are phonetically similar and TM may rank a wrong word higher than the correct one. For example, consider the Arabic word “‫”ﺟﻮ‬, which is pronounced as “joe” but is in fact the rendition of the Chinese name “Zhou”. If the English text has words such as “jaw”, “joe”, “jo”, “joy”, etc., one of them may rank higher than “Zhou”. Since only the top choice is considered, this phenomenon would hurt precision and recall. We address these two situations by making the following two contributions: 1. Modifying the TM technique with the best reported results on the ACL 2010 NEWS workshop, namely graph reinforcement (El-Kahki et al., 2011) to handle training sets of arbitrary sizes by introducing parameterized exponential penalty to the mapping induction process. We show that we can effectively learn the parameters that tune the penalty for two different training sets  of varying sizes. In doing so, we achieve better results for graph reinforcement with larger training sets. 2. For large comparable texts, we use contextual clues, namely translations of neighboring words, to constrain TM and to preserve precision. Specifically, we initially extract text segments that are “related” based on cross lingual lexical overlap, and then we perform TM on these segments. Though there have been some papers on extracting sub-sentence alignments from comparable text (Hewavitharana and Vogel, 2011; Munteanu and Marcu, 2006), extracting related (as opposed to parallel) text segments may be preferable because: 1) transliterations may not occur in parallel contexts; 2) using simple lexical overlap is efficient; and as we will show 3) simultaneous use of phonetic and contextual evidences may be sufficient to produce high TM precision. Alternate solutions focused on performing TM on extracted named entities only (Udupa et al., 2009b). Some drawbacks of such an approach are: 1) named entity recognition (NER) may not be available for many languages; and 2) NER has inherently low recall for languages such as Arabic where no discriminating features such as capitalization exist. The remainder of the paper is organized as follows: Section 2 provides background on TM; Section 3 describes the basic TM system that is used in the paper; Section 4 describes graph reinforcements, shows how it fairs in the presence of a large training set, and introduces modifications to graph reinforcement to improve its effectiveness with such data; Section 5 introduces the use of contextual clues to improve TM and reports on its effectiveness; and Section 6 concludes the paper. 2. Background Much work has been done on TM for different language pairs such as English-Chinese (Kuo et al., 2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 2008;), English-Tamil (Saravanan and Kumaran, 2008; Udupa and Khapra, 2010), English-Korean (Oh and Isahara, 2006; Oh and Choi, 2006), English-Japanese (Qu et al., 2000; Brill et al., 2001; Oh and Isahara, 2006), English-Hindi (Fei et al., 2003; Mahesh and Sinha, 2009), and EnglishRussian (Klementiev and Roth, 2006). TM typically involves finding character mappings  244  between two languages and using these mappings to ascertain if two words are transliterations or not.  2.1 Finding Character Mappings To find character sequence mappings between two languages, the most common approach entails using automatic letter alignment of transliteration pairs. Automatic alignment can be performed using different algorithms such as EM (Kuo et al., 2008; Lee and Chang, 2003) or HMM-based alignment (Udupa et al., 2009a; Udupa et al., 2009b). Another method uses automatic speech recognition confusion tables to extract phonetically equivalent character sequences to discover monolingual and cross-lingual pronunciation variations (Kuo and Yang, 2005). Alternatively, letters can be mapped into a common character set using a predefined transliteration scheme (Darwish, 2010; Oh and Choi, 2006). 2.2 Transliteration Mining For the problem of ascertaining if two words can be transliterations of each other, a common approach involves using a generative model that attempts to generate all possible transliterations of a source word, given the character mappings between two languages, and restricting the output to words in the target language (Fei et al., 2003; Lee and Chang, 2003, Udupa et al., 2009a). This is similar to the baseline approach that we used in this paper. Noeman and Madkour (2010) implemented this technique using a finite state automaton by generating all possible transliterations along with weighted edit distance and then filtered them using appropriate thresholds and target language words. El-Kahki et al. (2011) combined a generative model with so-called graph reinforcement, which is described in greater detail in Section 4. They reported the best TM results on the ACL 2010 NEWS workshop dataset for 4 different languages. Alternatively backtransliteration can be used to determine if one sequence could have been generated by successively mapping character sequences from one language into another (Brill et al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 2006). Udupa and Khapra (2010) proposed a method in which transliteration candidates are mapped into a “low-dimensional common representation space”. Then, the similarity between the resultant feature vectors for both candidates can be computed. A  similar approach uses context sensitive hashing (Udupa and Kumar, 2010). Jiampojamarn et al. (2010) used classification to determine if source and target language words were valid transliterations. They used a variety of features including edit distance between an English token and the Romanized versions of the foreign token, forward and backward transliteration probabilities, and character n-gram similarity. Udupa et al. (2009b) used a similar classificationbased approach. 3. Baseline Transliteration Mining 3.1 Description of the Baseline System We used a generative TM model that was trained on a set of transliteration pairs. We automatically aligned these pairs at character level using an HMM-based aligner akin to that of He (2007). Alignment produced mappings between characters from both languages with associated probabilities. We restricted individual source language character sequences to be 3 characters at most. We always treated English as the target language and Arabic as the source language. Briefly, we produced all possible segmentations of a source word along with their associated mappings into the target language. Valid target sequences were retained and sorted by the product of the constituent mapping probabilities. The candidate with the highest probability was generated given that the product of the mapping probabilities was higher than a certain threshold. Otherwise, no candidate was chosen. The search for transliterated pairs was implemented as a variant of depth-first search (Pearl, 1984), where states represented valid mappings between source and target substrings. At each step, the mapping with the best score was selected and expanded using the mappings learnt from alignment. This process ran until mapping combinations produced target word(s) from a source word or until all possible states were explored. The pseudo code in Figure 1 describes the details of the algorithm. The implementation was optimized via incremental left to right processing of source words, the use of a radix tree to prune invalid paths, and the use of a sorted priority queue to insure that the highest weighing candidate was at the top of the queue.  245  1: Input: Mappings, set of mappings from source fragment to a list of target fragments and mapping Probability . 2: Input: SourceWord (𝐹𝑖 ∈ 𝐹1𝑛 ), Source language word 3: Input: TargetWords, radix tree containing all target language words (𝐸1𝑚 ) 4: Data Structures: DFS, Priority queue to store candidate transliterations pair ordered by their transliteration score –  Each candidate transliteration tuple = (SourceFragment, TargetTransliteration, TransliterationScore).  5: StartSymbol = (“”, “”, 1.0); DFS={StartSymbol}  7: While (DFS is not empty)  8:  SourceFragment= DFS.Top().SourceFragment  9:  TargetFragment= DFS.Top().TargetTransliteration  10:  FragmentScore =DFS.Top().TransliterationScore  11:  If (SourceWord == SourceFragment)  12:  If (FragmentScore > Threshold) Return (SourceWord, TargetTransliteration, FragmentScore)  14:  Else Return Null  16:  DFS.RemoveTop()  17:  For SubFragmentLength = 1 to 3  18:  SourceSubString = SubString( SourceWord, SourceFragment.Length , SubFragmentLength)  19:  Foreach mapping in Mappings[SourceSubString]  20:  If ((TargetFragment + mapping.TargetFragemnt) is a sub-string in TargetWords)  21:  DFS.Add(SourceFragment + SourceSubString, TargetFragment + mapping.TargetFragement,  mapping.Score * FragmentScore)  22:  DFS.RemoveTop()  23: End While  24: Return Null  Figure 1: Pseudo code for transliteration mining  3.2 Thresholding We used a threshold on the minimum acceptable transliteration score to filter out unreliable transliterations. Fixing a uniform threshold would have caused the model to filter out long transliterations. Thus, we tied the threshold to the length of transliterated words. We assumed a threshold d for single character mappings and the transliteration threshold for a target word of length l would be 𝑑!. Since we did not have a validation set to estimate d, we created a synthetic validation set from the training set and then used crossvalidation to estimate d as follows: we split the training data into 5 folds for cross validation; we modified each validation fold by adding 5 random words to each target word in the transliteration pair; then we performed TM with varying thresholds on the validation fold and computed Fmeasure; and we ascertained the threshold that led to the highest F-measure for each fold and then took the average threshold. 3.3 Linguistic Processing For Arabic, we performed letter normalization of the different forms of alef, alef maqsoura and ya, and ta marbouta and ha. For English, we casefolded all letters and removed accents, umlaut, and similar diacritic like marks (ex. á, â, ä, à, ã, ā, ą).  4. Modifying Graph Reinforcement  4.1 Original Graph Reinforcement To motivate graph reinforcement, consider the following example: if alignment produced the mappings (‫طﻁ‬, ti), (‫طﻁ‬, ta), (‫تﺕ‬, ti), and (‫تﺕ‬, t), then the mappings (‫طﻁ‬, t) and (‫تﺕ‬, ta) are likely valid – though not observed. These mappings can be induced by traversing the following paths: ‫ طﻁ‬è ti è ‫ تﺕ‬è t and ‫ تﺕ‬è ti è ‫ طﻁ‬è ta respectively. In graph reinforcement, observed mappings were modeled as a bipartite graph with source (S) and target (T) character sequences and weighted with the learnt alignment probabilities (M). Thus the mapping between s ∈ S and t ∈ T was m(s,t). Graph reinforcement was performed by traversing the graph from S è T è S è T in order to deduce new mappings. Given a source sequence s'∈ S and a target sequence t' ∈ T, the deduced mapping weights were computed as follows:  𝑚 𝑡′ 𝑠′ = 1 −  
We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efﬁciently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate signiﬁcantly better translation results. 
A tree transformation is sensible if the size of each output tree is uniformly bounded by a linear function in the size of the corresponding input tree. Every sensible tree transformation computed by an arbitrary weighted extended top-down tree transducer can also be computed by a weighted multi bottom-up tree transducer. This further motivates weighted multi bottom-up tree transducers as suitable translation models for syntax-based machine translation. 
The important mass of textual documents is in perpetual growth and requires strong applications to automatically process information. Automatic titling is an essential task for several applications: ’No Subject’ e-mails titling, text generation, summarization, and so forth. This study presents an original approach consisting in titling journalistic articles by nominalizing. In particular, morphological and semantic processing are employed to obtain a nominalized form which has to respect titles characteristics (in particular, relevance and catchiness). The evaluation of the approach, described in the paper, indicates that titles stemming from this method are informative and/or catchy. 
While the ﬁeld of grammatical error detection has progressed over the past few years, one area of particular difﬁculty for both native and non-native learners of English, comma placement, has been largely ignored. We present a system for comma error correction in English that achieves an average of 89% precision and 25% recall on two corpora of unedited student essays. This system also achieves state-of-theart performance in the sister task of restoring commas in well-formed text. For both tasks, we show that the use of novel features which encode long-distance information improves upon the more lexically-driven features used in prior work. 
We apply Combinatory Categorial Grammar to wide-coverage parsing in Chinese with the new Chinese CCGbank, bringing a formalism capable of transparently recovering non-local dependencies to a language in which they are particularly frequent. We train two state-of-the-art English  parsers: the parser of Petrov and Klein (P&K), and the Clark and Curran (C&C) parser, uncovering a surprising performance gap between them not observed in English — 72.73 (P&K) and 67.09 (C&C) F -score on  6. We explore the challenges of Chinese  parsing through three novel ideas: developing corpus variants rather than treating the corpus as ﬁxed; controlling noun/verb and other  ambiguities; and quantifying the impact of constructions like pro-drop. 
We investigate the problem of automatically converting from a dependency representation to a phrase structure representation, a key aspect of understanding the relationship between these two representations for NLP work. We implement a new approach to this problem, based on a small number of supertags, along with an encoding of some of the underlying principles of the Penn Treebank guidelines. The resulting system signiﬁcantly outperforms previous work in such automatic conversion. We also achieve comparable results to a system using a phrase-structure parser for the conversion. A comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing. 
We propose a linguistically motivated set of features to capture morphological agreement and add them to the MSTParser dependency parser. Compared to the built-in morphological feature set, ours is both much smaller and more accurate across a sample of 20 morphologically annotated treebanks. We ﬁnd increases in accuracy of up to 5.3% absolute. While some of this results from the feature set capturing information unrelated to morphology, there is still signiﬁcant improvement, up to 4.6% absolute, due to the agreement model. 
We present an approach to automatically recover hidden attributes of scientiﬁc articles, such as whether the author is a native English speaker, whether the author is a male or a female, and whether the paper was published in a conference or workshop proceedings. We train classiﬁers to predict these attributes in computational linguistics papers. The classiﬁers perform well in this challenging domain, identifying non-native writing with 95% accuracy (over a baseline of 67%). We show the beneﬁts of using syntactic features in stylometry; syntax leads to signiﬁcant improvements over bag-of-words models on all three tasks, achieving 10% to 25% relative error reduction. We give a detailed analysis of which words and syntax most predict a particular attribute, and we show a strong correlation between our predictions and a paper’s number of citations. 
First story detection (FSD) involves identifying ﬁrst stories about events from a continuous stream of documents. A major problem in this task is the high degree of lexical variation in documents which makes it very difﬁcult to detect stories that talk about the same event but expressed using different words. We suggest using paraphrases to alleviate this problem, making this the ﬁrst work to use paraphrases for FSD. We show a novel way of integrating paraphrases with locality sensitive hashing (LSH) in order to obtain an efﬁcient FSD system that can scale to very large datasets. Our system achieves state-of-the-art results on the ﬁrst story detection task, beating both the best supervised and unsupervised systems. To test our approach on large data, we construct a corpus of events for Twitter, consisting of 50 million documents, and show that paraphrasing is also beneﬁcial in this domain. 
We investigate insertion and deletion models for hierarchical phrase-based statistical machine translation. Insertion and deletion models are designed as a means to avoid the omission of content words in the hypotheses. In our case, they are implemented as phrase-level feature functions which count the number of inserted or deleted words. An English word is considered inserted or deleted based on lexical probabilities with the words on the foreign language side of the phrase. Related techniques have been employed before by Och et al. (2003) in an n-best reranking framework and by Mauser et al. (2006) and Zens (2008) in a standard phrase-based translation system. We propose novel thresholding methods in this work and study insertion and deletion features which are based on two different types of lexicon models. We give an extensive experimental evaluation of all these variants on the NIST Chinese→English translation task.  symbols in α and Iβ the number of terminal symbols in β. Indexing α with j, i.e. the symbol αj, 1 ≤ j ≤ Jα, denotes the j-th terminal symbol on the source side of the phrase pair α, β , and analogous with βi, 1 ≤ i ≤ Iβ, on the target side. With these notational conventions, we now deﬁne our insertion and deletion models, each in both source-to-target and target-to-source direction. We give phrase-level scoring functions for the four features. In our implementation, the feature values are precomputed and written to the phrase table. The features are then incorporated directly into the loglinear model combination of the decoder. Our insertion model in source-to-target direction ts2tIns(·) counts the number of inserted words on the target side β of a hierarchical rule with respect to the source side α of the rule:  Iβ Jα  ts2tIns(α, β) =  p(βi|αj) < ταj (1)  i=1 j=1  
We introduce a method for learning to predict text completion given a source text and partial translation. In our approach, predictions are offered aimed at alleviating users’ burden on lexical and grammar choices, and improving productivity. The method involves learning syntax-based phraseology and translation equivalents. At run-time, the source and its translation prefix are sliced into ngrams to generate and rank completion candidates, which are then displayed to users. We present a prototype writing assistant, TransAhead, that applies the method to computer-assisted translation and language learning. The preliminary results show that the method has great potentials in CAT and CALL with significant improvement in translation quality across users. 
We present a classiﬁer that discriminates between types of corrections made by teachers of English in student essays. We deﬁne a set of linguistically motivated feature templates for a log-linear classiﬁcation model, train this classiﬁer on sentence pairs extracted from the Cambridge Learner Corpus, and achieve 89% accuracy improving upon a 33% baseline. Furthermore, we incorporate our classiﬁer into a novel application that takes as input a set of corrected essays that have been sentence aligned with their originals and outputs the individual corrections classiﬁed by error type. We report the F-Score of our implementation on this task. 
We introduce a new segmentation evaluation measure, WinPR, which resolves some of the limitations of WindowDiff. WinPR distinguishes between false positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 
Motivated by the fact that the pronunciation of a name may be inﬂuenced by its language of origin, we present methods to improve pronunciation prediction of proper names using word origin information. We train grapheme-to-phoneme (G2P) models on language-speciﬁc data sets and interpolate the outputs. We perform experiments on US surnames, a data set where word origin variation occurs naturally. Our methods can be used with any G2P algorithm that outputs posterior probabilities of phoneme sequences for a given word. 
We evaluate the performance of an morphological analyser for Inuktitut across a mediumsized corpus, where it produces a useful analysis for two out of every three types. We then compare its segmentation to that of simpler approaches to morphology, and use these as a pre-processing step to a word alignment task. Our observations show that the richer approaches provide little as compared to simply ﬁnding the head, which is more in line with the particularities of the task.  morphosyntax of Inuktitut is particularly marked by a rich polysynthetic sufﬁxing morphology, including incorporation of arguments into verbal tokens, as in natsiviniqtulauqsimavilli in (1). This phenomenon causes an individual token in Inuktitut to be approximately equivalent to an entire clause in English. (1) natsiq- -viniq- -tuq- -lauq- -simaseal meat eat before ever -vit -li INT-2s but “But have you ever eaten seal meat before?”  
This paper proposes an improved approach to extractive summarization of spoken multi-party interaction, in which integrated random walk is performed on a graph constructed on topical/ lexical relations. Each utterance is represented as a node of the graph, and the edges’ weights are computed from the topical similarity between the utterances, evaluated using probabilistic latent semantic analysis (PLSA), and from word overlap. We model intra-speaker topics by partially sharing the topics from the same speaker in the graph. In this paper, we perform experiments on automatically and manually generated transcripts. For automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances. 
We report on a pilot experiment to improve the performance of an automatic speech recognizer (ASR) by using a single-channel EEG signal to classify the speaker’s mental state as reading easy or hard text. We use a previously published method (Mostow et al., 2011) to train the EEG classiﬁer. We use its probabilistic output to control weighted interpolation of separate language models for easy and difﬁcult reading. The EEG-adapted ASR achieves higher accuracy than two baselines. We analyze how its performance depends on EEG classiﬁcation accuracy. This pilot result is a step towards improving ASR more generally by using EEG to distinguish mental states. 
We investigate a language model that combines morphological and shape features with a Kneser-Ney model and test it in a large crosslingual study of European languages. Even though the model is generic and we use the same architecture and features for all languages, the model achieves reductions in perplexity for all 21 languages represented in the Europarl corpus, ranging from 3% to 11%. We show that almost all of this perplexity reduction can be achieved by identifying sufﬁxes by frequency. 
Sequential transduction tasks, such as grapheme-to-phoneme conversion and machine transliteration, are usually addressed by inducing models from sets of input-output pairs. Supplemental representations offer valuable additional information, but incorporating that information is not straightforward. We apply a uniﬁed reranking approach to both grapheme-to-phoneme conversion and machine transliteration demonstrating substantial accuracy improvements by utilizing heterogeneous transliterations and transcriptions of the input word. We describe several experiments that involve a variety of supplemental data and two state-of-the-art transduction systems, yielding error rate reductions ranging from 12% to 43%. We further apply our approach to system combination, with error rate reductions between 4% and 9%. 
In this paper we present a fully unsupervised nonparametric Bayesian model that jointly induces POS tags and morphological segmentations. The model is essentially an inﬁnite HMM that infers the number of states from data. Incorporating segmentation into the same model provides the morphological features to the system and eliminates the need to ﬁnd them during preprocessing step. We show that learning both tasks jointly actually leads to better results than learning either task with gold standard data from the other task provided. The evaluation on multilingual data shows that the model produces state-of-the-art results on POS induction. 
It has long been observed that monolingual text exhibits a tendency toward “one sense per discourse,” and it has been argued that a related “one translation per discourse” constraint is operative in bilingual contexts as well. In this paper, we introduce a novel method using forced decoding to conﬁrm the validity of this constraint, and we demonstrate that it can be exploited in order to improve machine translation quality. Three ways of incorporating such a preference into a hierarchical phrase-based MT model are proposed, and the approach where all three are combined yields the greatest improvements for both Arabic-English and ChineseEnglish translation experiments. 
There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach. We analyze a number of these algorithms in terms of their sentencelevel loss functions, which motivates several new approaches, including a Structured SVM. We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings. Among other results, we ﬁnd that a simple and efﬁcient batch version of MIRA performs at least as well as training online, and consistently outperforms other options. 
In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. In this work, we address the problem of incremental speech-to-speech translation (S2S) that enables cross-lingual communication between two remote participants over a telephone. We investigate the problem in a novel real-time Session Initiation Protocol (SIP) based S2S framework. The speech translation is performed incrementally based on generation of partial hypotheses from speech recognition. We describe the statistical models comprising the S2S system and the SIP architecture for enabling real-time two-way cross-lingual dialog. We present dialog experiments performed in this framework and study the tradeoff in accuracy versus latency in incremental speech translation. Experimental results demonstrate that high quality translations can be generated with the incremental approach with approximately half the latency associated with nonincremental approach. 
We present a probabilistic approach for learning to interpret temporal phrases given only a corpus of utterances and the times they reference. While most approaches to the task have used regular expressions and similar linear pattern interpretation rules, the possibility of phrasal embedding and modiﬁcation in time expressions motivates our use of a compositional grammar of time expressions. This grammar is used to construct a latent parse which evaluates to the time the phrase would represent, as a logical parse might evaluate to a concrete entity. In this way, we can employ a loosely supervised EM-style bootstrapping approach to learn these latent parses while capturing both syntactic uncertainty and pragmatic ambiguity in a probabilistic framework. We achieve an accuracy of 72% on an adapted TempEval-2 task – comparable to state of the art systems. 
Negated statements often carry positive implicit meaning. Regardless of the semantic representation one adopts, pinpointing the positive concepts within a negated statement is needed in order to encode the statement’s meaning. In this paper, novel ideas to reveal positive implicit meaning using focus of negation are presented. The concept of granularity of focus is introduced and justiﬁed. New annotation and features to detect ﬁne-grained focus are discussed and results reported. 
This paper presents a novel approach for inducing lexical taxonomies automatically from text. We recast the learning problem as that of inferring a hierarchy from a graph whose nodes represent taxonomic terms and edges their degree of relatedness. Our model takes this graph representation as input and ﬁts a taxonomy to it via combination of a maximum likelihood approach with a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular ﬂat and hierarchical clustering algorithms. 
It has been established that incorporating word cluster features derived from large unlabeled corpora can signiﬁcantly improve prediction of linguistic structure. While previous work has focused primarily on English, we extend these results to other languages along two dimensions. First, we show that these results hold true for a number of languages across families. Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters signiﬁcantly improve the accuracy of cross-lingual structure prediction. Speciﬁcally, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%. When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%. 
We introduce lightly supervised learning for dependency parsing. In this paradigm, the algorithm is initiated with a parser, such as one that was built based on a very limited amount of fully annotated training data. Then, the algorithm iterates over unlabeled sentences and asks only for a single bit of feedback, rather than a full parse tree. Speciﬁcally, given an example the algorithm outputs two possible parse trees and receives only a single bit indicating which of the two alternatives has more correct edges. There is no direct information about the correctness of any edge. We show on dependency parsing tasks in 14 languages that with only 1% of fully labeled data, and light-feedback on the remaining 99% of the training data, our algorithm achieves, on average, only 5% lower performance than when training with fully annotated training set. We also evaluate the algorithm in different feedback settings and show its robustness to noise. 
Coarse-to-ﬁne inference has been shown to be a robust approximate method for improving the efﬁciency of structured prediction models while preserving their accuracy. We propose a multi-pass coarse-to-ﬁne architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. Our ﬁrst-, second-, and third-order models achieve accuracies comparable to those of their unpruned counterparts, while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search. Our pruned third-order model is twice as fast as an unpruned ﬁrst-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages. 
We present an active learning method for coreference resolution that is novel in three respects. (i) It uses bootstrapped neighborhood pooling, which ensures a class-balanced pool even though gold labels are not available. (ii) It employs neighborhood selection, a selection strategy that ensures coverage of both positive and negative links for selected markables. (iii) It is based on a query-by-committee selection strategy in contrast to earlier uncertainty sampling work. Experiments show that this new method outperforms random sampling in terms of both annotation effort and peak performance. 
Recent exploratory efforts in discourse-level language modeling have relied heavily on calculating Pointwise Mutual Information (PMI), which involves signiﬁcant computation when done over large collections. Prior work has required aggressive pruning or independence assumptions to compute scores on large collections. We show the method of Conditional Random Sampling, thus far an underutilized technique, to be a space-efﬁcient means of representing the sufﬁcient statistics in discourse that underly recent PMI-based work. This is demonstrated in the context of inducing Shankian script-like structures over news articles.  et al. (2005), Talbot and Osborne (2007), Goyal et al. (2010), Talbot and Brants (2008),Van Durme and Lall (2009a), Levenberg and Osborne (2009), Goyal et al. (2010), Petrovic et al. (2010), Van Durme and Lall (2010), or Goyal and Daume´ (2011)), we propose the method of Conditional Random Sampling (CRS) by Li and Church (2007) as an efﬁcient way to store approximations of the statistics used to calculate PMI for applications in inducing rudimentary script-like structures. Efﬁciently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story understanding (Gordon et al., 2011).  
We analyze overt displays of power (ODPs) in written dialogs. We present an email corpus with utterances annotated for ODP and present a supervised learning system to predict it. We obtain a best cross validation F-measure of 65.8 using gold dialog act features and 55.6 without using them.  present manual annotations for ODP in a small subset of Enron email corpus. In Section 5, we present a supervised learning system using word and part-ofspeech features along with features indicating dialog acts. 2 Related Work  
This paper describes our ongoing work on resolving third person pronouns and deictic words in a multi-modal corpus. We show that about two thirds of these referring expressions have antecedents that are introduced by pointing gestures or by haptic-ostensive actions (actions that involve manipulating an object). After describing our annotation scheme, we discuss the co-reference models we learn from multi-modal features. The usage of hapticostensive actions in a co-reference model is a novel contribution of our work.  As far as we know, no computational models of coreference have been developed that include H-O actions: (Landragin et al., 2002) focused on perceptual salience and (Foster et al., 2008) on generation rather than interpretation. We should point out that at the time of writing we only focus on resolving third person pronouns and deictics. The rest of this paper is organized as follows. In Section 2 we describe our multi-modal annotation scheme. In Section 3 we present the pronoun/deictic resolution system. In Section 4, we discuss experiments and results. 2 The Data Set  
In the area of machine translation (MT) system combination, previous work on generating input hypotheses has focused on varying a core aspect of the MT system, such as the decoding algorithm or alignment algorithm. In this paper, we propose a new method for generating diverse hypotheses from a single MT system using traits. These traits are simple properties of the MT output such as “average output length” and “average rule length.” Our method is designed to select hypotheses which vary in trait value but do not significantly degrade in BLEU score. These hypotheses can be combined using standard system combination techniques to produce a 1.21.5 BLEU gain on the Arabic-English NIST MT06/MT08 translation task. 
Shallow-n grammars (de Gispert et al., 2010) were introduced to reduce over-generation in the Hiero translation model (Chiang, 2005) resulting in much faster decoding and restricting reordering to a desired level for speciﬁc language pairs. However, Shallow-n grammars require parameters which cannot be directly optimized using minimum error-rate tuning by the decoder. This paper introduces some novel improvements to the translation model for Shallow-n grammars. We introduce two rules: a BITG-style reordering glue rule and a simpler monotonic concatenation rule. We use separate features for the new rules in our loglinear model allowing the decoder to directly optimize the feature weights. We show this formulation of Shallow-n hierarchical phrasebased translation is comparable in translation quality to full Hiero-style decoding (without shallow rules) while at the same time being considerably faster. 
 We present a novel method to detect parallel fragments within noisy parallel corpora. Isolating these parallel fragments from the noisy data in which they are contained frees us from noisy alignments and stray links that can severely constrain translation-rule extraction. We do this with existing machinery, making use of an existing word alignment model for this task. We evaluate the quality and utility of the extracted data on large-scale Chinese-English and Arabic-English translation tasks and show significant improvements over a state-of-the-art baseline. 
We propose a tuning method for statistical machine translation, based on the pairwise ranking approach. Hopkins and May (2011) presented a method that uses a binary classiﬁer. In this work, we use linear regression and show that our approach is as effective as using a binary classiﬁer and converges faster. 
Determining the reading level of children’s literature is an important task for providing educators and parents with an appropriate reading trajectory through a curriculum. Automating this process has been a challenge addressed before in the computational linguistics literature, with most studies attempting to predict the particular grade level of a text. However, guided reading levels developed by educators operate at a more ﬁne-grained level, with multiple levels corresponding to each grade. We ﬁnd that ranking performs much better than classiﬁcation at the ﬁne-grained leveling task, and that features derived from the visual layout of a book are just as predictive as standard text features of level; including both sets of features, we ﬁnd that we can predict the reading level up to 83% of the time on a small corpus of children’s books. 
This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms. We show that semantic information added by Topic Models signiﬁcantly improves the performance of two wordbased algorithms, namely TextTiling and C99. Additionally, we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information. We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset. 
Parallel corpora have applications in many areas of Natural Language Processing, but are very expensive to produce. Much information can be gained from comparable texts, and we present an algorithm which, given any bodies of text in multiple languages, uses existing named entity recognition software and topic detection algorithm to generate pairs of comparable texts without requiring a parallel corpus training phase. We evaluate the system’s performance ﬁrstly on data from the online newspaper domain, and secondly on Wikipedia cross-language links. 
This paper describes a user study where humans interactively train automatic text classiﬁers. We attempt to replicate previous results using multiple “average” Internet users instead of a few domain experts as annotators. We also analyze user annotation behaviors to ﬁnd that certain labeling actions have an impact on classiﬁer accuracy, drawing attention to the important role these behavioral factors play in interactive learning systems. 
We describe and evaluate several methods for estimating the conﬁdence in the per-edge correctness of a predicted dependency parse. We show empirically that the conﬁdence is associated with the probability that an edge is selected correctly and that it can be used to detect incorrect edges very efﬁciently. We evaluate our methods on parsing text in 14 languages. 
We investigate models for unsupervised learning with concave log-likelihood functions. We begin with the most well-known example, IBM Model 1 for word alignment (Brown et al., 1993) and analyze its properties, discussing why other models for unsupervised learning are so seldom concave. We then present concave models for dependency grammar induction and validate them experimentally. We ﬁnd our concave models to be effective initializers for the dependency model of Klein and Manning (2004) and show that we can encode linguistic knowledge in them for improved performance. 
This paper reports on an implementation of a multimodal grammar of speech and co-speech gesture within the LKB/PET grammar engineering environment. The implementation extends the English Resource Grammar (ERG, Flickinger (2000)) with HPSG types and rules that capture the form of the linguistic signal, the form of the gestural signal and their relative timing to constrain the meaning of the multimodal action. The grammar yields a single parse tree that integrates the spoken and gestural modality thereby drawing on standard semantic composition techniques to derive the multimodal meaning representation. Using the current machinery, the main challenge for the grammar engineer is the nonlinear input: the modalities can overlap temporally. We capture this by identical speech and gesture token edges. Further, the semantic contribution of gestures is encoded by lexical rules transforming a speech phrase into a multimodal entity of conjoined spoken and gestural semantics. 
Are word-level affect lexicons useful in detecting emotions at sentence level? Some prior research ﬁnds no gain over and above what is obtained with ngram features—arguably the most widely used features in text classiﬁcation. Here, we experiment with two very different emotion lexicons and show that even in supervised settings, an affect lexicon can provide signiﬁcant gains. We further show that while ngram features tend to be accurate, they are often unsuitable for use in new domains. On the other hand, affect lexicon features tend to generalize and produce better results than ngrams when applied to a new domain. 
Public debate functions as a forum for both expressing and forming opinions, an important aspect of public life. We present results for automatically classifying posts in online debate as to the position, or STANCE that the speaker takes on an issue, such as Pro or Con. We show that representing the dialogic structure of the debates in terms of agreement relations between speakers, greatly improves performance for stance classiﬁcation, over models that operate on post content and parentpost context alone. 
Sentiment analysis of citations in scientiﬁc papers and articles is a new and interesting problem which can open up many exciting new applications in bibliographic search and bibliometrics. Current work on citation sentiment detection focuses on only the citation sentence. In this paper, we address the problem of context-enhanced citation sentiment detection. We present a new citation sentiment corpus which has been annotated to take the dominant sentiment in the entire citation context into account. We believe that this gold standard is closer to the truth than annotation that looks only at the citation sentence itself. We then explore the effect of context windows of different lengths on the performance of a stateof-the-art citation sentiment detection system when using this context-enhanced gold standard deﬁnition.  However, there is a problem with the expression of sentiment in scientiﬁc text. Conventionally, the writing style in scientiﬁc writing is meant to be objective. Any personal bias by authors has to be hedged (Hyland, 1995). Negative sentiment is politically particularly dangerous (Ziman, 1968), and some authors have documented the strategy of prefacing the intended criticism by slightly disingenuous praise (MacRoberts and MacRoberts, 1984). This makes the problem of identifying such opinions particularly challenging. This non-local expression of sentiment has been observed in other genres as well (Wilson et al., 2009; Polanyi and Zaenen, 2006).  
Microblogging networks serve as vehicles for reaching and inﬂuencing users. Predicting whether a message will elicit a user response opens the possibility of maximizing the virality, reach and effectiveness of messages and ad campaigns on these networks. We propose a discriminative model for predicting the likelihood of a response or a retweet on the Twitter network. The approach uses features derived from various sources, such as the language used in the tweet, the user’s social network and history. The feature design process leverages aggregate statistics over the entire social network to balance sparsity and informativeness. We use real-world tweets to train models and empirically show that they are capable of generating accurate predictions for a large number of tweets.  In this work, we describe methods to predict if a given tweet will elicit a response. Twitter provides two methods to respond to messages: replies and retweets (re-posting of a message to one’s followers). Responses thus serve both as a measure of distribution and as a way to increase it. Being able to predict responses is valuable for any content generator, including advertisers and celebrities, who use Twitter to increase their exposure and maintain their brand. Furthermore, this prediction ability can be used for ranking, allowing the creation of better optimized news feeds. To predict if a tweet will receive a response prior to its posting we use features of the individual tweet together with features aggregated over the entire social network. These features, in combination with historical activity, are used to train a prediction model.  
Although ﬁrst names and nicknames in the United States have been well documented, there has been almost no quantitative analysis on the usage and association of these names amongst themselves. In this paper we introduce the Intelius Nickname Collection, a quantitative compilation of millions of namenickname associations based on information gathered from billions of public records. To the best of our knowledge, this is the largest collection of its kind, making it a natural resource for tasks such as coreference resolution, record linkage, named entity recognition, people and expert search, information extraction, demographic and sociological studies, etc. The collection will be made freely available. 
This paper compares a number of recently proposed models for computing context sensitive word similarity. We clarify the connections between these models, simplify their formulation and evaluate them in a uniﬁed setting. We show that the models are essentially equivalent if syntactic information is ignored, and that the substantial performance differences previously reported disappear to a large extent when these simpliﬁed variants are evaluated under identical conditions. Furthermore, our reformulation allows for the design of a straightforward and fast implementation. 
Noticing that different information sources often provide complementary coverage of word sense and meaning, we propose a simple and yet effective strategy for measuring lexical semantics. Our model consists of a committee of vector space models built on a text corpus, Web search results and thesauruses, and measures the semantic word relatedness using the averaged cosine similarity scores. Despite its simplicity, our system correlates with human judgements better or similarly compared to existing methods on several benchmark datasets, including WordSim353. 
Given a parallel corpus, if two distinct words in language A, a1 and a2, are aligned to the same word b1 in language B, then this might signal that b1 is polysemous, or it might signal a1 and a2 are synonyms. Both assumptions with successful work have been put forward in the literature. We investigate these assumptions, along with other questions of word sense, by looking at sampled parallel sentences containing tokens of the same type in English, asking how often they mean the same thing when they are: 1. aligned to the same foreign type; and 2. aligned to different foreign types. Results for French-English and Chinese-English parallel corpora show similar behavior: Synonymy is only very weakly the more prevalent scenario, where both cases regularly occur. 
It is well known that the output quality of statistical machine translation (SMT) systems increases with more training data. To obtain more parallel text for translation modeling, researchers have turned to the web to mine parallel sentences, but most previous approaches have avoided the difﬁcult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded signiﬁcant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case). 
 2 Related Work  In this paper, we investigate the use of temporal information for improving extractive summarization of historical articles. Our method clusters sentences based on their timestamps and temporal similarity. Each resulting cluster is assigned an importance score which can then be used as a weight in traditional sentence ranking techniques. Temporal importance weighting offers consistent improvements over baseline systems. 
Natural Language Generation (NLG) systems often use a pipeline architecture for sequential decision making. Recent studies however have shown that treating NLG decisions jointly rather than in isolation can improve the overall performance of systems. We present a joint learning framework based on Hierarchical Reinforcement Learning (HRL) which uses graphical models for surface realisation. Our focus will be on a comparison of Bayesian Networks and HMMs in terms of user satisfaction and naturalness. While the former perform best in isolation, the latter present a scalable alternative within joint systems. 
Generating referring expressions has received considerable attention in Natural Language Generation. In recent years we start seeing deployments of referring expression generators moving away from limited domains with custom-made ontologies. In this work, we explore the feasibility of using large scale noisy ontologies (folksonomies) for open domain referring expression generation, an important task for summarization by re-generation. Our experiments on a fully annotated anaphora resolution training set and a larger, volunteersubmitted news corpus show that existing algorithms are efﬁcient enough to deal with large scale ontologies but need to be extended to deal with undeﬁned values and some measure for information salience. 
Microblog streams often contain a considerable amount of information about local, regional, national, and global events. Most existing microblog search capabilities are focused on recent happenings and do not provide the ability to search and explore past events. This paper proposes the problem of structured retrieval of historical event information over microblog archives. Rather than retrieving individual microblog messages in response to an event query, we propose retrieving a ranked list of historical event summaries by distilling high quality event representations using a novel temporal query expansion technique. The results of an exploratory study carried out over a large archive of Twitter messages demonstrates both the value of the microblog event retrieval task and the effectiveness of our proposed search methodologies. 
We introduce the social study of bullying to the NLP community. Bullying, in both physical and cyber worlds (the latter known as cyberbullying), has been recognized as a serious national health issue among adolescents. However, previous social studies of bullying are handicapped by data scarcity, while the few computational studies narrowly restrict themselves to cyberbullying which accounts for only a small fraction of all bullying episodes. Our main contribution is to present evidence that social media, with appropriate natural language processing techniques, can be a valuable and abundant data source for the study of bullying in both worlds. We identify several key problems in using such data sources and formulate them as NLP tasks, including text classiﬁcation, role labeling, sentiment analysis, and topic modeling. Since this is an introductory paper, we present baseline results on these tasks using off-the-shelf NLP solutions, and encourage the NLP community to contribute better models in the future. 
Existing work in ﬁne-grained sentiment analysis focuses on sentences and phrases but ignores the contribution of individual words and their grammatical connections. This is because of a lack of both (1) annotated data at the word level and (2) algorithms that can leverage syntactic information in a principled way. We address the ﬁrst need by annotating articles from the information technology business press via crowdsourcing to provide training and testing data. To address the second need, we propose a sufﬁx-tree data structure to represent syntactic relationships between opinion targets and words in a sentence that are opinion-bearing. We show that a factor graph derived from this data structure acquires these relationships with a small number of word-level features. We demonstrate that our supervised model performs better than baselines that ignore syntactic features and constraints. 
We present novel methods to construct compact natural language lexicons within a graphbased semi-supervised learning framework, an attractive platform suited for propagating soft labels onto new natural language types from seed data. To achieve compactness, we induce sparse measures at graph vertices by incorporating sparsity-inducing penalties in Gaussian and entropic pairwise Markov networks constructed from labeled and unlabeled data. Sparse measures are desirable for high-dimensional multi-class learning problems such as the induction of labels on natural language types, which typically associate with only a few labels. Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach produces signiﬁcantly smaller lexicons and obtains better predictive performance. 
We present a general framework containing a graded spectrum of Expectation Maximization (EM) algorithms called Uniﬁed Expectation Maximization (UEM.) UEM is parameterized by a single parameter and covers existing algorithms like standard EM and hard EM, constrained versions of EM such as ConstraintDriven Learning (Chang et al., 2007) and Posterior Regularization (Ganchev et al., 2010), along with a range of new EM algorithms. For the constrained inference step in UEM we present an efﬁcient dual projected gradient ascent algorithm which generalizes several dual decomposition and Lagrange relaxation algorithms popularized recently in the NLP literature (Ganchev et al., 2008; Koo et al., 2010; Rush and Collins, 2011). UEM is as efﬁcient and easy to implement as standard EM. Furthermore, experiments on POS tagging, information extraction, and word-alignment show that often the best performing algorithm in the UEM family is a new algorithm that wasn’t available earlier, exhibiting the beneﬁts of the UEM framework. 
The accuracy of many natural language processing tasks can be improved by a reranking step, which involves selecting a single output from a list of candidate outputs generated by a baseline system. We propose a novel family of reranking algorithms based on learning separate low-dimensional embeddings of the task’s input and output spaces. This embedding is learned in such a way that prediction becomes a low-dimensional nearest-neighbor search, which can be done computationally efﬁciently. A key quality of our approach is that feature engineering can be done separately on the input and output spaces; the relationship between inputs and outputs is learned automatically. Experiments on part-of-speech tagging task in four languages show signiﬁcant improvements over a baseline decoder and existing reranking approaches. 
Text input aids such as automatic correction systems play an increasingly important role in facilitating fast text entry and efﬁcient communication between text message users. Although these tools are beneﬁcial when they work correctly, they can cause signiﬁcant communication problems when they fail. To improve its autocorrection performance, it is important for the system to have the capability to assess its own performance and learn from its mistakes. To address this, this paper presents a novel task of self-assessment of autocorrection performance based on interactions between text message users. As part of this investigation, we collected a dataset of autocorrection mistakes from true text message users and experimented with a rich set of features in our self-assessment task. Our experimental results indicate that there are salient cues from the text message discourse that allow systems to assess their own behaviors with high precision. 
To build a coreference resolver for a new language, the typical approach is to ﬁrst coreference-annotate documents from this target language and then train a resolver on these annotated documents using supervised learning techniques. However, the high cost associated with manually coreference-annotating documents needed by a supervised approach makes it difﬁcult to deploy coreference technologies across a large number of natural languages. To alleviate this corpus annotation bottleneck, we examine a translation-based projection approach to multilingual coreference resolution. Experimental results on two target languages demonstrate the promise of our approach. 
We investigate the task of medical concept coreference resolution in clinical text using two semi-supervised methods, co-training and multi-view learning with posterior regularization. By extracting semantic and temporal features of medical concepts found in clinical text, we create conditionally independent data views; co-training MaxEnt classiﬁers on this data works almost as well as supervised learning for the task of pairwise coreference resolution of medical concepts. We also train MaxEnt models with expectation constraints, using posterior regularization, and ﬁnd that posterior regularization performs comparably to or slightly better than co-training. We describe the process of semantic and temporal feature extraction and demonstrate our methods on a corpus of case reports from the New England Journal of Medicine and a corpus of patient narratives obtained from The Ohio State University Wexner Medical Center. 
Not all learning takes place in an educational setting: more and more self-motivated learners are turning to on-line text to learn about new topics. Our goal is to provide such learners with the well-known benefits of testing by automatically generating quiz questions for online text. Prior work on question generation has focused on the grammaticality of generated questions and generating effective multiple-choice distractors for individual question targets, both key parts of this problem. Our work focuses on the complementary aspect of determining what part of a sentence we should be asking about in the first place; we call this “gap selection.” We address this problem by asking human judges about the quality of questions generated from a Wikipedia-based corpus, and then training a model to effectively replicate these judgments. Our data shows that good gaps are of variable length and span all semantic roles, i.e., nouns as well as verbs, and that a majority of good questions do not focus on named entities. Our resulting system can generate fill-in-the-blank (cloze) questions from generic source materials. 
Concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. We present a joint model that captures content selection (“what to say”) and surface realization (“how to say”) in an unsupervised domain-independent fashion. Rather than breaking up the generation process into a sequence of local decisions, we deﬁne a probabilistic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). We represent our grammar compactly as a weighted hypergraph and recast generation as the task of ﬁnding the best derivation tree for a given input. Experimental evaluation on several domains achieves competitive results with state-of-the-art systems that use domain speciﬁc constraints, explicit feature engineering or labeled data. 
When people describe a scene, they often include information that is not visually apparent; sometimes based on background knowledge, sometimes to tell a story. We aim to separate visual text—descriptions of what is being seen—from non-visual text in natural images and their descriptions. To do so, we ﬁrst concretely deﬁne what it means to be visual, annotate visual text and then develop algorithms to automatically classify noun phrases as visual or non-visual. We ﬁnd that using text alone, we are able to achieve high accuracies at this task, and that incorporating features derived from computer vision algorithms improves performance. Finally, we show that we can reliably mine visual nouns and adjectives from large corpora and that we can use these effectively in the classiﬁcation task. 
We propose an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense. Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the soft K-Means algorithm. In addition to describing our approach, we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation. By comparing our induced clusters to reference clusters generated from WordNet, we demonstrate that our method effectively identiﬁes sense-based translation clusters and beneﬁts from both monolingual and parallel corpora. Finally, we describe a method for annotating clusters with usage examples. 
With a few exceptions, extensions to latent Dirichlet allocation (LDA) have focused on the distribution over topics for each document. Much less attention has been given to the underlying structure of the topics themselves. As a result, most topic models generate topics independently from a single underlying distribution and require millions of parameters, in the form of multinomial distributions over the vocabulary. In this paper, we introduce the Shared Components Topic Model (SCTM), in which each topic is a normalized product of a smaller number of underlying component distributions. Our model learns these component distributions and the structure of how to combine subsets of them into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters. 
A U.S. Congressional bill is a textual artifact that must pass through a series of hurdles to become a law. In this paper, we focus on one of the most precarious and least understood stages in a bill’s life: its consideration, behind closed doors, by a Congressional committee. We construct predictive models of whether a bill will survive committee, starting with a strong, novel baseline that uses features of the bill’s sponsor and the committee it is referred to. We augment the model with information from the contents of bills, comparing different hypotheses about how a committee decides a bill’s fate. These models give signiﬁcant reductions in prediction error and highlight the importance of bill substance in explanations of policy-making and agenda-setting. 
In this paper we describe the goals of the Estonian corpus collection and analysis activities, and introduce the recent collection of Estonian First Encounters data. The MINT project aims at deepening our understanding of the conversational properties and practices in human interactions. We especially investigate conversational engagement and cooperation, and discuss some observations on the participants' views concerning the interaction they have been engaged.
A monitoring system to detect emotional outbursts in day-to-day communication is presented. The anger monitor was tested in a household and in parallel in an office surrounding. Although the state of the art of emotion recognition seems sufficient for practical applications, the acquisition of good training material remains a difficult task, as cross database performance is too low to be used in this context. A solution will probably consist of the combination of carefully drafted general training databases and the development of usability concepts to (re-) train the monitor in the field.
We describe a software tool named Speechalyzer which is optimized to process large speech data sets with respect to transcription, labeling and annotation. It is implemented as a client server based framework in Java and interfaces software for speech recognition, synthesis, speech classification and quality evaluation. The application is mainly the processing of training data for speech recognition and classification models and performing benchmarking tests on speech to text, text to speech and speech categorization software systems.
In this paper we report on the past evaluation of the STEVIN programme in the field of Human Language Technology for Dutch (HLTD). STEVIN was a 11.4 M euro programme that was jointly organised and financed by the Flemish and Dutch governments. The aim was to provide academia and industry with basic building blocks for a linguistic infrastructure for the Dutch language. An independent evaluation has been carried out. The evaluators concluded that the most important targets of the STEVIN programme have been achieved to a very high extent. In this paper, we summarise the context, the evaluation method, the resulting resources and the highlights of the STEVIN final evaluation.
For a reliable keyword extraction on firefighter radio communication, a strong automatic speech recognition system is needed. However, real-life data poses several challenges like a distorted voice signal, background noise and several different speakers. Moreover, the domain is out-of-scope for common language models, and the available data is scarce. In this paper, we introduce the PRONTO corpus, which consists of German firefighter exercise transcriptions. We show that by standard adaption techniques the recognition rate already rises from virtually zero to up to 51.7{\%} and can be further improved by domain-specific rules to 47.9{\%}. Extending the acoustic material by semi-automatic transcription and crawled in-domain written material, we arrive at a WER of 45.2{\%}.
Bilingual dictionaries are key resources in several fields such as translation, language learning or various NLP tasks. However, only major languages have such resources. Automatically built dictionaries by using pivot languages could be a useful resource in these circumstances. Pivot-based bilingual dictionary building is based on merging two bilingual dictionaries which share a common language (e.g. LA-LB, LB-LC) in order to create a dictionary for a new language pair (e.g LA-LC). This process may include wrong translations due to the polisemy of words. We built Basque-Chinese (Mandarin) dictionaries automatically from Basque-English and Chinese-English dictionaries. In order to prune wrong translations we used different methods adequate for less resourced languages. Inverse Consultation and Distributional Similarity methods are used because they just depend on easily available resources. Finally, we evaluated manually the quality of the built dictionaries and the adequacy of the methods. Both Inverse Consultation and Distributional Similarity provide good precision of translations but recall is seriously damaged. Distributional similarity prunes rare translations more accurately than other methods.
The conversations between posters and repliers in microblogs form a valuable writer-reader emotion corpus. This paper adopts a log relative frequency ratio to investigate the linguistic features which affect emotion transitions, and applies the results to predict writers' and readers' emotions. A 4-class emotion transition predictor, a 2-class writer emotion predictor, and a 2-class reader emotion predictor are proposed and compared.
In our paper, we present main results of the Czech grant project Internet as a Language Corpus, whose aim was to build a corpus of Czech web texts and to develop and publicly release related software tools. Our corpus may not be the largest web corpus of Czech, but it maintains very good language quality due to high portion of human work involved in the corpus development process. We describe the corpus contents (2.65 billions of words divided into three parts -- 450 millions of words from news and magazines articles, 1 billion of words from blogs, diaries and other non-reviewed literary units, 1.1 billion of words from discussions messages), particular steps of the corpus creation (crawling, HTML and boilerplate removal, near duplicates removal, language filtering) and its automatic language annotation (POS tagging, syntactic parsing). We also describe our software tools being released under an open source license, especially a fast linear-time module for removing near-duplicates on a paragraph level.
We describe an electronic lexical resource for German and the structure of its lexicon entries, notably the structure of verbal single-word and multi-word entries. The verb as the center of the sentence structure, as held by dependency models, is also a basic principle of the JAKOB narrative analysis application, for which the dictionary is the background. Different linguistic layers are combined for construing lexicon entries with a rich set of syntactic and semantic properties, suited to represent the syntactic and semantic behavior of verbal expressions (verb patterns), extracted from transcripts of real discourse, thereby lexicalizing the specific meaning of a specific verb pattern in a specific context. Verb patterns are built by the lexicographer by using a parser analyzing the input of a test clause and generating a machine-readable property string with syntactic characteristics and propositions for semantic characteristics grounded in an ontology. As an example, the German idiomatic expression ''''''``an den Karren fahren'''''''' (to come down hard on somebody) demonstrates the overall structure of a dictionary entry. The goal is to build unique dictionary entries (verb patterns) with reference to the whole of their properties.
This paper describes a tool developed to improve access to the enormous volume of data housed at the UK's National Archives, both for the general public and for specialist researchers. The system we have developed, TNA-Search, enables a multi-paradigm search over the entire electronic archive (42TB of data in various formats). The search functionality allows queries that arbitrarily mix any combination of full-text, structural, linguistic and semantic queries. The archive is annotated and indexed with respect to a massive semantic knowledge base containing data from the LOD cloud, data.gov.uk, related TNA projects, and a large geographical database. The semantic annotation component achieves approximately 83{\%} F-measure, which is very reasonable considering the wide range of entities and document types and the open domain. The technologies are being adopted by real users at The National Archives and will form the core of their suite of search tools, with additional in-house interfaces.
This paper presents QurAna: a large corpus created from the original Quranic text, where personal pronouns are tagged with their antecedence. These antecedents are maintained as an ontological list of concepts, which have proved helpful for information retrieval tasks. QurAna is characterized by: (a) comparatively large number of pronouns tagged with antecedent information (over 24,500 pronouns), and (b) maintenance of an ontological concept list out of these antecedents. We have shown useful applications of this corpus. This corpus is first of its kind considering classical Arabic text, which could be used for interesting applications for Modern Standard Arabic as well. This corpus would benefit researchers in obtaining empirical and rules in building new anaphora resolution approaches. Also, such corpus would be used to train, optimize and evaluate existing approaches.
We present a novel graph-theoretic method for the initial annotation of high-confidence training data for bootstrapping sentiment classifiers. We estimate polarity using topic-specific PageRank. Sentiment information is propagated from an initial seed lexicon through a joint graph representation of words and documents. We report improved classification accuracies across multiple domains for the base models and the maximum entropy model bootstrapped from the PageRank annotation.
In this paper, we describe MLSA, a publicly available multi-layered reference corpus for German-language sentiment analysis. The construction of the corpus is based on the manual annotation of 270 German-language sentences considering three different layers of granularity. The sentence-layer annotation, as the most coarse-grained annotation, focuses on aspects of objectivity, subjectivity and the overall polarity of the respective sentences. Layer 2 is concerned with polarity on the word- and phrase-level, annotating both subjective and factual language. The annotations on Layer 3 focus on the expression-level, denoting frames of private states such as objective and direct speech events. These three layers and their respective annotations are intended to be fully independent of each other. At the same time, exploring for and discovering interactions that may exist between different layers should also be possible. The reliability of the respective annotations was assessed using the average pairwise agreement and Fleiss' multi-rater measures. We believe that MLSA is a beneficial resource for sentiment analysis research, algorithms and applications that focus on the German language.
This paper presents three new speech databases for standard Basque. They are designed primarily for corpus-based synthesis but each database has its specific purpose: 1) AhoSyn: high quality speech synthesis (recorded also in Spanish), 2) AhoSpeakers: voice conversion and 3) AhoEmo3: emotional speech synthesis. The whole corpus design and the recording process are described with detail. Once the databases were collected all the data was automatically labelled and annotated. Then, an HMM-based TTS voice was built and subjectively evaluated. The results of the evaluation are pretty satisfactory: 3.70 MOS for Basque and 3.44 for Spanish. Therefore, the evaluation assesses the quality of this new speech resource and the validity of the automated processing presented.
Temporal expressions are words or phrases that describe a point, duration or recurrence in time. Automatically annotating these expressions is a research goal of increasing interest. Recognising them can be achieved with minimally supervised machine learning, but interpreting them accurately (normalisation) is a complex task requiring human knowledge. In this paper, we present TIMEN, a community-driven tool for temporal expression normalisation. TIMEN is derived from current best approaches and is an independent tool, enabling easy integration in existing systems. We argue that temporal expression normalisation can only be effectively performed with a large knowledge base and set of rules. Our solution is a framework and system with which to capture this knowledge for different languages. Using both existing and newly-annotated data, we present results showing competitive performance and invite the IE community to contribute to a knowledge base in order to solve the temporal expression normalisation problem.
The task of native language (L1) identification suffers from a relative paucity of useful training corpora, and standard within-corpus evaluation is often problematic due to topic bias. In this paper, we introduce a method for L1 identification in second language (L2) texts that relies only on much more plentiful L1 data, rather than the L2 texts that are traditionally used for training. In particular, we do word-by-word translation of large L1 blog corpora to create a mapping to L2 forms that are a possible result of language transfer, and then use that information for unsupervised classification. We show this method is effective in several different learner corpora, with bigram features being particularly useful.
In this paper, we present the foundations and the properties of the DISLOG language, a logic-based language designed to describe and implement discourse structure analysis. Dislog has the flexibility and the expressiveness of a rule-based system, it offers the possibility to include knowledge and reasoning capabilities and the expression a variety of well-formedness constraints proper to discourse. Dislog is embedded into the platform that offers an engine with various processing capabilities and a programming environment.
We present a gold standard for semantic relation extraction in the food domain for German. The relation types that we address are motivated by scenarios for which IT applications present a commercial potential, such as virtual customer advice in which a virtual agent assists a customer in a supermarket in finding those products that satisfy their needs best. Moreover, we focus on those relation types that can be extracted from natural language text corpora, ideally content from the internet, such as web forums, that are easy to retrieve. A typical relation type that meets these requirements are pairs of food items that are usually consumed together. Such a relation type could be used by a virtual agent to suggest additional products available in a shop that would potentially complement the items a customer has already in their shopping cart. Our gold standard comprises structural data, i.e. relation tables, which encode relation instances. These tables are vital in order to evaluate natural language processing systems that extract those relations.
In this paper, we present an analysis method, a set of rules, lexical resources dedicated to discourse relation identification, in particular for explanation analysis. The following relations are described with prototypical rules: instructions, advice, warnings, illustration, restatement, purpose, condition, circumstance, concession, contrast and some forms of causes. Rules are developed for French and English. The approach used to describe the analysis of such relations is basically generative and also provides a conceptual view of explanation. The implementation is realized in Dislog, using the logic-based platform, and the Dislog language, that also allows for the integration of knowledge and reasoning into rules describing the structure of explanation.
In this paper, we present the first phase of the LELIE project. A tool that detects business errors in technical documents such as procedures or requirements is introduced. The objective is to improve readability and to check for some elements of contents so that risks that could be entailed by misunderstandings or typos can be prevented. Based on a cognitive ergonomics analysis, we survey a number of frequently encountered types of errors and show how they can be detected using the discourse analysis platform. We show how errors can be annotated, give figures on error frequencies and analyze how technical writers perceive our system.
This article presents WebAnnotator, a new tool for annotating Web pages. WebAnnotator is implemented as a Firefox extension, allowing annotation of both offline and inline pages. The HTML rendering fully preserved and all annotations consist in new HTML spans with specific styles. WebAnnotator provides an easy and general-purpose framework and is made available under CeCILL free license (close to GNU GPL), so that use and further contributions are made simple. All parts of an HTML document can be annotated: text, images, videos, tables, menus, etc. The annotations are created by simply selecting a part of the document and clicking on the relevant type and subtypes. The annotated elements are then highlighted in a specific color. Annotation schemas can be defined by the user by creating a simple DTD representing the types and subtypes that must be highlighted. Finally, annotations can be saved (HTML with highlighted parts of documents) or exported (in a machine-readable format).
The aim of this paper is to present current efforts towards the creation of a comprehensive open repository of Polish language resources and tools (LRTs). The work described here is carried out within the CESAR project, member of the META-NET consortium. It has already resulted in the creation of the Computational Linguistics in Poland site containing an exhaustive collection of Polish LRTs. Current work is focused on the creation of new LRTs and, esp., the enhancement of existing LRTs, such as parallel corpora, annotated corpora of written and spoken Polish and morphological dictionaries to be made available via the META-SHARE repository.
While it is possible to build a formal grammar manually from scratch or, going to another extreme, to derive it automatically from a treebank, the development of the LFG grammar of Polish presented in this paper is different from both of these methods as it relies on extensive reuse of existing language resources for Polish. LFG grammars minimally provide two levels of representation: constituent structure (c-structure) produced by context-free phrase structure rules and functional structure (f-structure) created by functional descriptions. The c-structure was based on a DCG grammar of Polish, while the f-structure level was mainly inspired by the available HPSG analyses of Polish. The morphosyntactic information needed to create a lexicon may be taken from one of the following resources: a morphological analyser, a treebank or a corpus. Valence information from the dictionary which accompanies the DCG grammar was converted so that subcategorisation is stated in terms of grammatical functions rather than categories; additionally, missing valence frames may be extracted from the treebank. The obtained grammar is evaluated using constructed testsuites (half of which were provided by previous grammars) and the treebank.
The paper discusses mechanisms for topic management in conversations, concentrating on interactions where the interlocutors react to each other's presentation of new information and construct a shared context in which to exchange information about interesting topics. This is illustrated with a robot simulator that can talk about unrestricted (open-domain) topics that the human interlocutor shows interest in. Wikipedia is used as the source of information from which the robotic agent draws its world knowledge.
In Natural Language Generation, the task of attribute selection (AS) consists of determining the appropriate attribute-value pairs (or semantic properties) that represent the contents of a referring expression. Existing work on AS includes a wide range of algorithmic solutions to the problem, but the recent availability of corpora annotated with referring expressions data suggests that corpus-based AS strategies become possible as well. In this work we tentatively discuss a number of AS strategies using both semantic and surface information obtained from a corpus of this kind. Relying on semantic information, we attempt to learn both global and individual AS strategies that could be applied to a standard AS algorithm in order to generate descriptions found in the corpus. As an alternative, and perhaps less traditional approach, we also use surface information to build statistical language models of the referring expressions that are most likely to occur in the corpus, and let the model probabilities guide attribute selection.
In the implementation of a surface realisation engine, many of the computational techniques seen in other AI fields have been widely applied. Among these, the use of statistical methods has been particularly successful, as in the so-called 'generate-and-select', or 2-stages architectures. Systems of this kind produce output strings from possibly underspecified input data by over-generating a large number of alternative realisations (often including ungrammatical candidate sentences.) These are subsequently ranked with the aid of a statistical language model, and the most likely candidate is selected as the output string. Statistical approaches may however face a number of difficulties. Among these, there is the issue of data sparseness, a problem that is particularly evident in cases such as our target language - Brazilian Portuguese - which is not only morphologically-rich, but relatively poor in NLP resources such as large, publicly available corpora. In this work we describe a first implementation of a shallow surface realisation system for this language that deals with the issue of data sparseness by making use of factored language models built from a (relatively) large corpus of Brazilian newspapers articles.
Subtitling and audiovisual translation have been recognized as areas that could greatly benefit from the introduction of Statistical Machine Translation (SMT) followed by post-editing, in order to increase efficiency of subtitle production process. The FP7 European project SUMAT (An Online Service for SUbtitling by MAchine Translation: http://www.sumat-project.eu) aims to develop an online subtitle translation service for nine European languages, combined into 14 different language pairs, in order to semi-automate the subtitle translation processes of both freelance translators and subtitling companies on a large scale. In this paper we discuss the data collection and parallel corpus compilation for training SMT systems, which includes several procedures such as data partition, conversion, formatting, normalization and alignment. We discuss in detail each data pre-processing step using various approaches. Apart from the quantity (around 1 million subtitles per language pair), the SUMAT corpus has a number of very important characteristics. First of all, high quality both in terms of translation and in terms of high-precision alignment of parallel documents and their contents has been achieved. Secondly, the contents are provided in one consistent format and encoding. Finally, additional information such as type of content in terms of genres and domain is available.
Thanks to the advent of Web 2.0, the potential for opinion sharing today is unmatched in history. Making meaning out of the huge amount of unstructured information available online, however, is extremely difficult as web-contents, despite being perfectly suitable for human consumption, still remain hardly accessible to machines. To bridge the cognitive and affective gap between word-level natural language data and the concept-level sentiments conveyed by them, affective common sense knowledge is needed. In sentic computing, the general common sense knowledge contained in ConceptNet is usually exploited to spread affective information from selected affect seeds to other concepts. In this work, besides exploiting the emotional content of the Open Mind corpus, we also collect new affective common sense knowledge through label sequential rules, crowd sourcing, and games-with-a-purpose techniques. In particular, we develop Open Mind Common Sentics, an emotion-sensitive IUI that serves both as a platform for affective common sense acquisition and as a publicly available NLP tool for extracting the cognitive and affective information associated with short texts.
In this study, the use of alternative acoustic sensors in human-robot communication is investigated. In particular, a Non-Audible Murmur (NAM) microphone was applied in teleoperating Geminoid HI-1 robot in noisy environments. The current study introduces the methodology and the results of speech intelligibility subjective tests when a NAM microphone was used in comparison with using a standard microphone. The results show the advantage of using NAM microphone when the operation takes place in adverse environmental conditions. In addition, the effect of Geminoid's lip movements on speech intelligibility is also investigated. Subjective speech intelligibility tests show that the operator's speech can be perceived with higher intelligibility scores when operator's audio speech is perceived along with the lip movements of robots.
Keystroke logging tools are a valuable aid to monitor written language production. These tools record all keystrokes, including backspaces and deletions together with timing information. In this paper we report on an extension to the keystroke logging program Inputlog in which we aggregate the logged process data from the keystroke (character) level to the word level. The logged process data are further enriched with different kinds of linguistic information: part-of-speech tags, lemmata, chunk boundaries, syllable boundaries and word frequency. A dedicated parser has been developed that distils from the logged process data word-level revisions, deleted fragments and final product data. The linguistically-annotated output will facilitate the linguistic analysis of the logged data and will provide a valuable basis for more linguistically-oriented writing process research. The set-up of the extension to Inputlog is largely language-independent. As proof-of-concept, the extension has been developed for English and Dutch. Inputlog is freely available for research purposes.
The present paper explores a wide range of word sense disambiguation (WSD) algorithms for German. These WSD algorithms are based on a suite of semantic relatedness measures, including path-based, information-content-based, and gloss-based methods. Since the individual algorithms produce diverse results in terms of precision and thus complement each other well in terms of coverage, a set of combined algorithms is investigated and compared in performance to the individual algorithms. Among the single algorithms considered, a word overlap method derived from the Lesk algorithm that uses Wiktionary glosses and GermaNet lexical fields yields the best F-score of 56.36. This result is outperformed by a combined WSD algorithm that uses weighted majority voting and obtains an F-score of 63.59. The WSD experiments utilize the German wordnet GermaNet as a sense inventory as well as WebCAGe (short for: Web-Harvested Corpus Annotated with GermaNet Senses), a newly constructed, sense-annotated corpus for this language. The WSD experiments also confirm that WSD performance is lower for words with fine-grained sense distinctions compared to words with coarse-grained senses.
Natural language generation in the medical domain is heavily influenced by domain knowledge and genre-specific text characteristics. We present SemScribe, an implemented natural language generation system that produces doctor's letters, in particular descriptions of cardiological findings. Texts in this domain are characterized by a high density of information and a relatively telegraphic style. Domain knowledge is encoded in a medical ontology of about 80,000 concepts. The ontology is used in particular for concept generalizations during referring expression generation. Architecturally, the system is a generation pipeline that uses a corpus-informed syntactic frame approach for realizing sentences appropriate to the domain. The system reads XML documents conforming to the HL7 Clinical Document Architecture (CDA) Standard and enhances them with generated text and references to the used data elements. We conducted a first clinical trial evaluation with medical staff and report on the findings.
This paper presents the Tu{\`I}bingen Baumbank des Deutschen Diachron (Tu{\`I}Ba-D/DC), a linguistically annotated corpus of selected diachronic materials from the German Gutenberg Project. It was automatically annotated by a suite of NLP tools integrated into WebLicht, the linguistic chaining tool used in CLARIN-D. The annotation quality has been evaluated manually for a subcorpus ranging from Middle High German to Modern High German. The integration of the Tu{\`I}Ba-D/DC into the CLARIN-D infrastructure includes metadata provision and harvesting as well as sustainable data storage in the Tu{\`I}bingen CLARIN-D center. The paper further provides an overview of the possibilities of accessing the Tu{\`I}Ba-D/DC data. Methods for full-text search of the metadata and object data and for annotation-based search of the object data are described in detail. The WebLicht Service Oriented Architecture is used as an integrated environment for annotation based search of the Tu{\`I}Ba-D/DC. WebLicht thus not only serves as the annotation platform for the Tu{\`I}Ba-D/DC, but also as a generic user interface for accessing and visualizing it.
Since September 2007, a large scale lexical network for French is under construction with methods based on popular consensus by means of games (under the JeuxDeMots project). To assess the resource quality, we decided to adopt an approach similar to its construction, that is to say an evaluation by laymen on open class vocabulary with a Tip of the Tongue tool.
The goal of our research is to build a grammatical error-tagged corpus for Korean learners of Spoken English dubbed Postech Learner Corpus. We collected raw story-telling speech from Korean university students. Transcription and annotation using the Cambridge Learner Corpus tagset were performed by six Korean annotators fluent in English. For the annotation of the corpus, we developed an annotation tool and a validation tool. After comparing human annotation with machine-recommended error tags, unmatched errors were rechecked by a native annotator. We observed different characteristics between the spoken language corpus built in this study and an existing written language corpus.
In this paper we present three approaches towards adaptive speech understanding. The target system is a model-based Adaptive Spoken Dialogue Manager, the OwlSpeak ASDM. We enhanced this system in order to properly react on non-understandings in real-life situations where intuitive communication is required. OwlSpeak provides a model-based spoken interface to an Intelligent Environment depending on and adapting to the current context. It utilises a set of ontologies used as dialogue models that can be combined dynamically during runtime. Besides the benefits the system showed in practice, real-life evaluations also conveyed some limitations of the model-based approach. Since it is unfeasible to model all variations of the communication between the user and the system beforehand, various situations where the system did not correctly understand the user input have been observed. Thus we present three enhancements towards a more sophisticated use of the ontology-based dialogue models and show how grammars may dynamically be adapted in order to understand intuitive user utterances. The evaluation of our approaches revealed the incorporation of a lexical-semantic knowledgebase into the recognition process to be the most promising approach.
Although lexicography of Latin has a long tradition dating back to ancient grammarians, and almost all Latin grammars devote to wordformation at least one part of the section(s) concerning morphology, none of the today available lexical resources and NLP tools of Latin feature a wordformation-based organization of the Latin lexicon. In this paper, we describe the first steps towards the semi-automatic development of a wordformation-based lexicon of Latin, by detailing several problems occurring while building the lexicon and presenting our solutions. Developing a wordformation-based lexicon of Latin is nowadays of outmost importance, as the last years have seen a large growth of annotated corpora of Latin texts of different eras. While these corpora include lemmatization, morphological tagging and syntactic analysis, none of them features segmentation of the word forms and wordformation relations between the lexemes. This restricts the browsing and the exploitation of the annotated data for linguistic research and NLP tasks, such as information retrieval and heuristics in PoS tagging of unknown words.
Parallel corpora ― original texts aligned with their translations ― are a widely used resource in computational linguistics. Translation studies have shown that translated texts often differ systematically from comparable original texts. Translators tend to be faithful to structures of the original texts, resulting in a ''''''``shining through'''''''' of the original language preferences in the translated text. Translators also tend to make their translations most comprehensible with the effect that translated texts can be more explicit than their source texts. Motivated by the need to use a parallel resource for cross-linguistic feature induction in abstract anaphora resolution, this paper investigates properties of English and German texts in the Europarl corpus, taking into account both general features such as sentence length as well as task-dependent features such as the distribution of demonstrative noun phrases. The investigation is based on the entire Europarl corpus as well as on a small subset thereof, which has been manually annotated. The results indicate English translated texts are sufficiently ''''''``authentic'''''''' to be used as training data for anaphora resolution; results for German texts are less conclusive, though.
Explicitly conveyed knowledge represents only a portion of the information communicated by a text snippet. Automated mechanisms for deriving explicit information exist; however, the implicit assumptions and default inferences that capture our intuitions about a normal interpretation of a communication remain hidden for automated systems, despite the communication participants' ease of grasping the complete meaning of the communication. In this paper, we describe a reasoning framework for the automatic identification of conversational implicatures conveyed by real-world English and Arabic conversations carried via twitter.com. Our system transforms given utterances into deep semantic logical forms. It produces a variety of axioms that identify lexical connections between concepts, define rules of combining semantic relations, capture common-sense world knowledge, and encode Grice's Conversational Maxims. By exploiting this rich body of knowledge and reasoning within the context of the conversation, our system produces entailments and implicatures conveyed by analyzed utterances with an F-measure of 70.42{\%} for English conversations.
Semantic representation of text is key to text understanding and reasoning. In this paper, we present Polaris, Lymba's semantic parser. Polaris is a supervised semantic parser that given text extracts semantic relations. It extracts relations from a wide variety of lexico-syntactic patterns, including verb-argument structures, noun compounds and others. The output can be provided in several formats: XML, RDF triples, logic forms or plain text, facilitating interoperability with other tools. Polaris is implemented using eight separate modules. Each module is explained and a detailed example of processing using a sample sentence is provided. Overall results using a benchmark are discussed. Per module performance, including errors made and pruned by each module are also analyzed.
In this paper, we describe the first English-Hungarian parallel corpus annotated for light verb constructions, which contains 14,261 sentence alignment units. Annotation principles and statistical data on the corpus are also provided, and English and Hungarian data are contrasted. On the basis of corpus data, a database containing pairs of English-Hungarian light verb constructions has been created as well. The corpus and the database can contribute to the automatic detection of light verb constructions and it is also shown how they can enhance performance in several fields of NLP (e.g. parsing, information extraction/retrieval and machine translation).
Speech-text alignment tools are frequently used in speech technology and research. In this paper, we propose a GPL software CoALT (Comparing Automatic Labelling Tools) for comparing two automatic labellers or two speech-text alignment tools, ranking them and displaying statistics about their differences. The main feature of CoALT is that a user can define its own criteria for evaluating and comparing the speech-text alignment tools since the required quality for labelling depends on the targeted application. Beyond ranking, our tool provides useful statistics for each labeller and above all about their differences and can emphasize the drawbacks and advantages of each labeller. We have applied our software for the French and English languages but it can be used for another language by simply defining the list of the phonetic symbols and optionally a set of phonetic rules. In this paper we present the usage of the software for comparing two automatic labellers on the corpus TIMIT. Moreover, as automatic labelling tools are configurable (number of GMMs, phonetic lexicon, acoustic parameterisation), we then present how CoALT allows to determine the best parameters for our automatic labelling tool.
The paper presents data repository that will be used as a source of data for ORAL2013, a new corpus of spontaneous spoken Czech. The corpus is planned to be published in 2013 within the framework of the Czech National Corpus and it will contain both the audio recordings and their transcriptions manually aligned with time stamps. The corpus will be designed as a representation of contemporary spontaneous spoken language used in informal, real-life situations on the area of the whole Czech Republic and thus balanced in the main sociolinguistic categories of speakers. Therefore, the data repository features broad regional coverage with large variety of speakers, as well as precise and uniform processing. The repository is already built, basically balanced and sized 3 million words proper (i.e. tokens not including punctuation). Before the publication, another set of overall consistency checks will be carried out, as well as final selection of the transcriptions to be included into ORAL2013 as the final product.
Recent years have witnessed a growing interest in annotating linguistic data at the semantic level, including the annotation of dialogue corpus data. The annotation scheme developed as international standard for dialogue act annotation ISO 24617-2 is based on the DIT++ scheme (Bunt, 2006; 2009) which combines the multidimensional DIT scheme (Bunt, 1994) with concepts from DAMSL (Allen and Core , 1997) and various other schemes. This scheme is designed in a such way that it can be applied not only to spoken dialogue, as is the case for most of the previously defined dialogue annotation schemes, but also to multimodal dialogue. This paper describes how the ISO 24617-2 annotation scheme can be used, together with the DIT++ method of multidimensional segmentation', to annotate nonverbal and multimodal dialogue behaviour. We analyse the fundamental distinction between (a) the coding of surface features; (b) form-related semantic classification; and (c) semantic annotation in terms of dialogue acts, supported by experimental studies of (a) and (b). We discuss examples of specification languages for representing the results of each of these activities, show how dialogue act annotations can be attached to XML representations of functional segments of multimodal data.
We present a 3-step framework that learns categories and their instances from natural language text based on given training examples. Step 1 extracts contexts of training examples as rules describing this category from text, considering part of speech, capitalization and category membership as features. Step 2 selects high quality rules using two consequent filters. The first filter is based on the number of rule occurrences, the second filter takes two non-independent characteristics into account: a rule's precision and the amount of instances it acquires. Our framework adapts the filter's threshold values to the respective category and the textual genre by automatically evaluating rule sets resulting from different filter settings and selecting the best performing rule set accordingly. Step 3 then identifies new instances of a category using the filtered rules applied within a previously proposed algorithm. We inspect the rule filters' impact on rule set quality and evaluate our framework by learning first names, last names, professions and cities from a hitherto unexplored textual genre -- search engine result snippets -- and achieve high precision on average.
Language statistics are widely used to characterize and better understand language. In parallel, the amount of text mining and information retrieval methods grew rapidly within the last decades, with many algorithms evaluated on standardized corpora, often drawn from newspapers. However, up to now there were almost no attempts to link the areas of natural language processing and language statistics in order to properly characterize those evaluation corpora, and to help others to pick the most appropriate algorithms for their particular corpus. We believe no results in the field of natural language processing should be published without quantitatively describing the used corpora. Only then the real value of proposed methods can be determined and the transferability to corpora originating from different genres or domains can be estimated. We lay ground for a language engineering process by gathering and defining a set of textual characteristics we consider valuable with respect to building natural language processing systems. We carry out a case study for the analysis of automotive repair orders and explicitly call upon the scientific community to provide feedback and help to establish a good practice of corpus-aware evaluations.
With the rising amount of digitally available text, the need for efficient processing algorithms is growing fast. Although a lot of libraries are commonly available, their modularity and interchangeability is very limited, therefore forcing a lot of reimplementations and modifications not only in research areas but also in real world application scenarios. In recent years, different NLP frameworks have been proposed to provide an efficient, robust and convenient architecture for information processing tasks. This paper will present an overview over the most common approaches with their advantages and shortcomings, and will discuss them with respect to the first standardized architecture - the Unstructured Information Management Architecture (UIMA).
Resource development mainly focuses on well-described languages with a large amount of speakers. However, smaller languages may also profit from language resources which can then be used in applications such as electronic dictionaries or computer-assisted language learning materials. The development of resources for such languages may face various challenges. Often, not enough data is available for a successful statistical approach and the methods developed for other languages may not be suitable for this specific language. This paper presents a morphological analyzer for Murrinh-Patha, a polysynthetic language spoken in the Northern Territory of Australia. While nouns in Murrinh-Patha only show minimal inflection, verbs in this language are very complex. The complexity makes it very difficult if not impossible to handle data in Murrinh-Patha with statistical, surface-oriented methods. I therefore present a rule-based morphological analyzer built in XFST and LEXC (Beesley and Karttunen, 2003) which can handle the inflection on nouns and adjectives as well as the complexities of the Murrinh-Patha verb.
Word Sense Disambiguation (WSD) systems require large sense-tagged corpora along with lexical databases to reach satisfactory results. The number of English language resources for developed WSD increased in the past years while most other languages are still under-resourced. The situation is no different for Dutch. In order to overcome this data bottleneck, the DutchSemCor project will deliver a Dutch corpus that is sense-tagged with senses from the Cornetto lexical database. In this paper, we discuss the different conflicting requirements for a sense-tagged corpus and our strategies to fulfill them. We report on a first series of experiments to sup- port our semi-automatic approach to build the corpus.
Translation studies rely more and more on corpus data to examine specificities of translated texts, that can be translated from different original languages and compared to original texts. In parallel, more and more multilingual corpora are becoming available for various natural language processing tasks. This paper questions the use of these multilingual corpora in translation studies and shows the methodological steps needed in order to obtain more reliably comparable sub-corpora that consist of original and directly translated text only. Various experiments are presented that show the advantage of directional sub-corpora.
This paper presents a large corpus created from the original Quranic text, where semantically similar or related verses are linked together. This corpus will be a valuable evaluation resource for computational linguists investigating similarity and relatedness in short texts. Furthermore, this dataset can be used for evaluation of paraphrase analysis and machine translation tasks. Our dataset is characterised by: (1) superior quality of relatedness assignment; as we have incorporated relations marked by well-known domain experts, this dataset could thus be considered a gold standard corpus for various evaluation tasks, (2) the size of our dataset; over 7,600 pairs of related verses are collected from scholarly sources with several levels of degree of relatedness. This dataset could be extended to over 13,500 pairs of related verses observing the commutative property of strongly related pairs. This dataset was incorporated into online query pages where users can visualize for a given verse a network of all directly and indirectly related verses. Empirical experiments showed that only 33{\%} of related pairs shared root words, emphasising the need to go beyond common lexical matching methods, and incorporate -in addition- semantic, domain knowledge, and other corpus-based approaches.
We present a preliminary study where we use eye tracking as a complement to machine translation (MT) error analysis, the task of identifying and classifying MT errors. We performed a user study where subjects read short texts translated by three MT systems and one human translation, while we gathered eye tracking data. The subjects were also asked comprehension questions about the text, and were asked to estimate the text quality. We found that there are a longer gaze time and a higher number of fixations on MT errors, than on correct parts. There are also differences in the gaze time of different error types, with word order errors having the longest gaze time. We also found correlations between eye tracking data and human estimates of text quality. Overall our study shows that eye tracking can give complementary information to error analysis, such as aiding in ranking error types for seriousness.
We describe an automatic face tracker plugin for the ANVIL annotation tool. The face tracker produces data for velocity and for acceleration in two dimensions. We compare annotations generated by the face tracking algorithm with independently made manual annotations for head movements. The annotations are a useful supplement to manual annotations and may help human annotators to quickly and reliably determine onset of head movements and to suggest which kind of head movement is taking place.
This paper describes representing translations in the Finnish wordnet, FinnWordNet (FiWN), and constructing the FiWN database. FiWN was created by translating all the word senses of the Princeton WordNet (PWN) into Finnish and by joining the translations with the semantic and lexical relations of PWN extracted into a relational (database) format. The approach naturally resulted in a translation relation between PWN and FiWN. Unlike many other multilingual wordnets, the translation relation in FiWN is not primarily on the synset level, but on the level of an individual word sense, which allows more precise translation correspondences. This can easily be projected into a synset-level translation relation, used for linking with other wordnets, for example, via Core WordNet. Synset-level translations are also used as a default in the absence of word-sense translations. The FiWN data in the relational database can be converted to other formats. In the PWN database format, translations are attached to source-language words, allowing the implementation of a Web search interface also working as a bilingual dictionary. Another representation encodes the translation relation as a finite-state transducer.
Statistical post-editing has been shown in several studies to increase BLEU score for rule-based MT systems. However, previous studies have relied solely on BLEU and have not conducted further study to determine whether those gains indicated an increase in quality or in score alone. In this work we conduct a human evaluation of statistical post-edited output from a weak rule-based MT system, comparing the results with the output of the original rule-based system and a phrase-based statistical MT system trained on the same data. We show that for this weak rule-based system, despite significant BLEU score increases, human evaluators prefer the output of the original system. While this is not a generally conclusive condemnation of statistical post-editing, this result does cast doubt on the efficacy of statistical post-editing for weak MT systems and on the reliability of BLEU score for comparison between weak rule-based and hybrid systems built from them.
The studies on dependency parsing of Turkish so far gave their results on the Turkish Dependency Treebank. This treebank consists of sentences where gold standard part-of-speech tags are manually assigned to each word and the words forming multi word expressions are also manually determined and combined into single units. For the first time, we investigate the results of parsing Turkish sentences from scratch and observe the accuracy drop at the end of processing raw data. We test one state-of-the art morphological analyzer together with two different morphological disambiguators. We both show separately the accuracy drop due to the automatic morphological processing and to the lack of multi word unit extraction. With this purpose, we use and present a new version of the Turkish Treebank where we detached the multi word expressions (MWEs) into multiple tokens and manually annotated the missing part-of-speech tags of these new tokens.
A framework is proposed for the detection of reduplication in digital videos of American Sign Language (ASL). In ASL, reduplication is used for a variety of linguistic purposes, including overt marking of plurality on nouns, aspectual inflection on verbs, and nominalization of verbal forms. Reduplication involves the repetition, often partial, of the articulation of a sign. In this paper, the apriori algorithm for mining frequent patterns in data streams is adapted for finding reduplication in videos of ASL. The proposed algorithm can account for varying weights on items in the apriori algorithm's input sequence. In addition, the apriori algorithm is extended to allow for inexact matching of similar hand motion subsequences and to provide robustness to noise. The formulation is evaluated on 105 lexical signs produced by two native signers. To demonstrate the formulation, overall hand motion direction and magnitude are considered; however, the formulation should be amenable to combining these features with others, such as hand shape, orientation, and place of articulation.
We present the architecture and the current state of InterCorp, a multilingual parallel corpus centered around Czech, intended primarily for human users and consisting of written texts with a focus on fiction. Following an outline of its recent development and a comparison with some other multilingual parallel corpora we give an overview of the data collection procedure that covers text selection criteria, data format, conversion, alignment, lemmatization and tagging. Finally, we show a sample query using the web-based search interface and discuss challenges and prospects of the project.
The rise of micro-blogging in recent years has resulted in significant access to emotion-laden text. Unlike emotion expressed in other textual sources (e.g., blogs, quotes in newswire, email, product reviews, or even clinical text), micro-blogs differ by (1) placing a strict limit on length, resulting radically in new forms of emotional expression, and (2) encouraging users to express their daily thoughts in real-time, often resulting in far more emotion statements than might normally occur. In this paper, we introduce a corpus collected from Twitter with annotated micro-blog posts (or tweets) annotated at the tweet-level with seven emotions: ANGER, DISGUST, FEAR, JOY, LOVE, SADNESS, and SURPRISE. We analyze how emotions are distributed in the data we annotated and compare it to the distributions in other emotion-annotated corpora. We also used the annotated corpus to train a classifier that automatically discovers the emotions in tweets. In addition, we present an analysis of the linguistic style used for expressing emotions our corpus. We hope that these observations will lead to the design of novel emotion detection techniques that account for linguistic style and psycholinguistic theories.
Assessing the correctness of extracted data requires performance evaluation, which is accomplished by calculating quality metrics. The evaluation process must cope with the challenges posed by information extraction and natural language processing. In the previous work most of the existing methodologies have been shown that they support only traditional scoring metrics. Our research work addresses requirements, which arose during the development of three productive rule-based information extraction systems. The main contribution is twofold: First, we developed a proposal for an evaluation methodology that provides the flexibility and effectiveness needed for comprehensive performance measurement. The proposal extends state-of-the-art scoring metrics by measuring string and semantic similarities and by parameterization of metric scoring, and thus simulating with human judgment. Second, we implemented an IE evaluation tool named EVALIEX, which integrates these measurement concepts and provides an efficient user interface that supports evaluation control and the visualization of IE results. To guarantee domain independence, the tool additionally provides a Generic Mapper for XML Instances (GeMap) that maps domain-dependent XML files containing IE results to generic ones. Compared to other tools, it provides more flexible testing and better visualization of extraction results for the comparison of different (versions of) information extraction systems.
Sign language is used by many people who were born deaf or who became deaf early in life use as their first and/or preferred language. There is no writing system for sign languages; texts are signed on video. As a consequence, texts in sign language are hard to navigate, search and annotate. The BiBiKit project is an easy to use authoring kit which is being developed and enables students, teachers, and virtually everyone to write and read bilingual bimodal texts and thereby creating electronic productions, which link text to sign language video. The main purpose of the project is to develop software that enables the user to link text to video, at the word, phrase and/or sentence level. The software will be developed for sign language and vice versa, but can be used to easily link text to any video: e.g. to add annotations, captions, or navigation points. The three guiding principles are: Software that is 1) stable, 2) easy to use, and 3) foolproof. A web based platform will be developed so the software is available whenever and wherever.
Virtual Worlds (VW) are online environments where people come together to interact and perform various tasks. The chat transcripts of interactions in VWs pose unique opportunities and challenges for language analysis: Firstly, the language of the transcripts is very brief, informal, and task-oriented. Secondly, in addition to chat, a VW system records users' in-world activities. Such a record could allow us to analyze how the language of interactions is linked to the users actions. For example, we can make the language analysis of the users dialogues more effective by taking into account the context of the corresponding action or we can predict or detect users actions by analyzing the content of conversations. Thirdly, a joined analysis of both the language and the actions would empower us to build effective modes of the users and their behavior. In this paper we present a corpus constructed from logs from an online multiplayer game BladeMistress. We describe the original logs, annotations that we created on the data, and summarize some of the experiments.
Within the general purpose of information extraction, detection of event descriptions is an important clue. A word refering to an event is more powerful than a single word, because it implies a location, a time, protagonists (persons, organizations{\textbackslash}dots). However, if verbal designations of events are well studied and easier to detect than nominal ones, nominal designations do not claim as much definition effort and resources. In this work, we focus on nominals desribing events. As our application domain is information extraction, we follow a named entity approach to describe and annotate events. In this paper, we present a typology and annotation guidelines for event nominals annotation. We applied them to French newswire articles and produced an annotated corpus. We present observations about the designations used in our manually annotated corpus and the behavior of their triggers. We provide statistics concerning word ambiguity and context of use of event nominals, as well as machine learning experiments showing the difficulty of using lexicons for extracting events.
This paper aims at assessing the automatic labeling of an undocumented, unknown, unwritten and under-resourced language (Mo Piu) of the North Vietnam, by an expert phonetician. In the previous stage of the work, 7 sets of languages were chosen among Mandarin, Vietnamese, Khmer, English, French, to compete in order to select the best models of languages to be used for the phonetic labeling of Mo Piu isolated words. Two sets of languages (1{\^A}{\mbox{$^\circ$}} Mandarin + French, 2{\^A}{\mbox{$^\circ$}} Vietnamese + French) which got the best scores showed an additional distribution of their results. Our aim is now to study this distribution more precisely and more extensively, in order to statistically select the best models of languages and among them, the best sets of phonetic units which minimize the wrong phonetic automatic labeling.
The paper introduces an ongoing project for the development of a parallel treebank for Italian, English and French, i.e. Parallel--TUT, or simply ParTUT. For the development of this resource, both the dependency and constituency-based formats of the Italian Turin University Treebank (TUT) have been applied to a preliminary dataset, which includes the whole text of the Universal Declaration of Human Rights, and sentences from the JRC-Acquis Multilingual Parallel Corpus and the Creative Commons licence. The focus of the project is mainly on the quality of the annotation and the investigation of some issues related to the alignment of data that can be allowed by the TUT formats, also taking into account the availability of conversion tools for display data in standard ways, such as Tiger--XML and CoNLL formats. It is, in fact, our belief that increasing the portability of our treebank could give us the opportunity to access resources and tools provided by other research groups, especially at this stage of the project, where no particular tool -- compatible with the TUT format -- is available in order to tackle the alignment problems.
Games with a purpose are an increasingly popular mechanism for leveraging the wisdom of the crowds to address tasks which are trivial for humans but still not solvable by computer algorithms in a satisfying manner. As a novel mechanism for structuring human-computer interactions, a key challenge when creating them is motivating users to participate while generating useful and unbiased results. This paper focuses on important design choices and success factors of effective games with a purpose. Our findings are based on lessons learned while developing and deploying Sentiment Quiz, a crowdsourcing application for creating sentiment lexicons (an essential component of most sentiment detection algorithms). We describe the goals and structure of the game, the underlying application framework, the sentiment lexicons gathered through crowdsourcing, as well as a novel approach to automatically extend the lexicons by means of a bootstrapping process. Such an automated extension further increases the efficiency of the acquisition process by limiting the number of terms that need to be gathered from the game participants.
In this work we show that there is a need of using multimodal resources during human-computer interaction (HCI) in intelligent systems. We propose that not only creating multimodal output for the user is important, but to take multimodal input resources into account for the decision when and how to interact. Especially the use of multimodal input resources for the decision when and how to provide assistance in HCI is important. The use of assistive functionalities like providing adaptive explanations to keep the user motivated and cooperative is more than a side-effect and demands a closer look. In this paper we introduce our approach on how to use multimodal input ressources in an adaptive and generic explanation pipeline. We do not only concentrate on using explanations as a way to manage user knowledge, but to maintain the cooperativeness, trust and motivation of the user to continue a healthy and well-structured HCI.
The main goal of the CLT Cloud project is to equip lexica, morphological processors, parsers and other software components developed within CLT (Centre of Language Technology) with so called web API:s, thus making them available on the Internet in the form of web services. We present a proof-of-concept implementation of the CLT Cloud server where we use the logic programming language Prolog for composing and aggregating existing web services into new web services in a way that encourages creative exploration and rapid prototyping of LT applications.
In this paper we present AnIta, a powerful morphological analyser for Italian implemented within the framework of finite-state-automata models. It is provided by a large lexicon containing more than 110,000 lemmas that enable it to cover relevant portions of Italian texts. We describe our design choices for the management of inflectional phenomena as well as some interesting new features to explicitly handle derivational and compositional processes in Italian, namely the wordform segmentation structure and Derivation Graph. Two different evaluation experiments, for testing coverage (Recall) and Precision, are described in detail, comparing the AnIta performances with some other freely available tools to handle Italian morphology. The experiments results show that the AnIta Morphological Analyser obtains the best performances among the tested systems, with Recall = 97.21{\%} and Precision = 98.71{\%}. This tool was a fundamental building block for designing a performant PoS-tagger and Lemmatiser for the Italian language that participated to two EVALITA evaluation campaigns ranking, in both cases, together with the best performing systems.
The current study works at the interface of theoretical and computational linguistics to explore the semantic properties of an particle verbs, i.e., German particle verbs with the particle an. Based on a thorough analysis of the particle verbs from a theoretical point of view, we identified empirical features and performed an automatic semantic classification. A focus of the study was on the mutual profit of theoretical and empirical perspectives with respect to salient semantic properties of the an particle verbs: (a) how can we transform the theoretical insights into empirical, corpus-based features, (b) to what extent can we replicate the theoretical classification by a machine learning approach, and (c) can the computational analysis in turn deepen our insights to the semantic properties of the particle verbs? The best classification result of 70{\%} correct class assignments was reached through a GermaNet-based generalization of direct object nouns plus a prepositional phrase feature. These particle verb features in combination with a detailed analysis of the results at the same time confirmed and enlarged our knowledge about salient properties.
This paper describes our research into methods for inferring social and instrumental roles and relationships from document and discourse corpora. The goal is to identify the roles of initial authors and participants in internet discussions with respect to leadership, influence and expertise. Web documents, forums and blogs provide data from which the relationships between these concepts are empirically derived and compared. Using techniques from Natural Language Processing (NLP), characterizations of authority and expertise are hypothesized and then tested to see if these pick out the same or different participants as may be chosen by techniques based on social network analysis (Huffaker 2010) see if they pick out the same discourse participants for any given level of these qualities (i.e. leadership, expertise and influence). Our methods could be applied, in principle, to any domain topic, but this paper will describe an initial investigation into two subject areas where a range of differing opinions are available and which differ in the nature of their appeals to authority and truth: genetic engineering' and a Muslim Forum'. The available online corpora for these topics contain discussions from a variety of users with different levels of expertise, backgrounds and personalities.
This paper presents CAT - CELCT Annotation Tool, a new general-purpose web-based tool for text annotation developed by CELCT (Center for the Evaluation of Language and Communication Technologies). The aim of CAT is to make text annotation an intuitive, easy and fast process. In particular, CAT was created to support human annotators in performing linguistic and semantic text annotation and was designed to improve productivity and reduce time spent on this task. Manual text annotation is, in fact, a time-consuming activity, and conflicts may arise with the strict deadlines annotation projects are frequently subject to. Thanks to its adaptability and user-friendly interface, CAT can positively contribute to improve time management in annotation project. Further, the tool has a number of features which make it an easy-to-use tool for many types of annotations. Even if the first prototype of CAT has been used to perform temporal and event annotation following the It-TimeML specifications, the tool is general enough to be used for annotating a broad range of linguistic and semantic phenomena. CAT is freely available for research purposes.
There have been increasing interests in recent years in analyzing tweet messages relevant to political events so as to understand public opinions towards certain political issues. We analyzed tweet messages crawled during the eight weeks leading to the UK General Election in May 2010 and found that activities at Twitter is not necessarily a good predictor of popularity of political parties. We then proceed to propose a statistical model for sentiment detection with side information such as emoticons and hash tags implying tweet polarities being incorporated. Our results show that sentiment analysis based on a simple keyword matching against a sentiment lexicon or a supervised classifier trained with distant supervision does not correlate well with the actual election results. However, using our proposed statistical model for sentiment analysis, we were able to map the public opinion in Twitter with the actual offline sentiment in real world.
This article describes the collecting, processing and validation of a large balanced corpus for Romanian. The annotation types and structure of the corpus are briefly reviewed. It was constructed at the Research Institute for Artificial Intelligence of the Romanian Academy in the context of an international project (METANET4U). The processing covers tokenization, POS-tagging, lemmatization and chunking. The corpus is in XML format generated by our in-house annotation tools; the corpus encoding schema is XCES compliant and the metadata specification is conformant to the METANET recommendations. To the best of our knowledge, this is the first large and richly annotated corpus for Romanian. ROMBAC is intended to be the foundation of a linguistic environment containing a reference corpus for contemporary Romanian and a comprehensive collection of interoperable processing tools.
In this paper we explore a task-driven approach to interfacing NLP components, where language processing is guided by the end-task that each application requires. The core idea is to generalize feature values into feature value distributions, representing under-specified feature values, and to fit linguistic pipelines with a back-channel of specification requests through which subsequent components can declare to preceding ones the importance of narrowing the value distribution of particular features that are critical for the current task.
Fairy tales, folktales and more generally children stories have lately attracted the Natural Language Processing (NLP) community. As such, very few corpora exist and linguistic resources are lacking. The work presented in this paper aims at filling this gap by presenting a syntactically and semantically annotated corpus. It focuses on the linguistic analysis of a Fairy Tales Corpus, and provides the description of the syntactic and semantic resources developed for Information Extraction. Resources include syntactic dependency relation annotation for 120 verbs; referential annotation, which is concerned with annotating each anaphoric occurrence and Proper Name with the most specific noun in the text; ontology matching for a substantial part of the nouns in the corpus; semantic role labelling for 41 verbs using the FrameNet database. The article also sums up previous analyses of this corpus and indicates possible uses of this corpus for the NLP community.
In this paper we present ConanDoyle-neg, a corpus of stories by Conan Doyle annotated with negation information. The negation cues and their scope, as well as the event or property that is negated have been annotated by two annotators. The inter-annotator agreement is measured in terms of F-scores at scope level. It is higher for cues (94.88 and 92.77), less high for scopes (85.04 and 77.31), and lower for the negated event (79.23 and 80.67). The corpus is publicly available.
Determining the real-world referents for name mentions of persons, organizations and other named entities in texts has become an important task in many information retrieval scenarios and is referred to as Named Entity Disambiguation (NED). While comprehensive datasets support the development and evaluation of NED approaches for English, there are no public datasets to assess NED systems for other languages, such as German. This paper describes the construction of an NED dataset based on a large corpus of German news articles. The dataset is closely modeled on the datasets used for the Knowledge Base Population tasks of the Text Analysis Conference, and contains gold standard annotations for the NED tasks of Entity Linking, NIL Detection and NIL Clustering. We also present first experimental results on the new dataset for each of these tasks in order to establish a baseline for future research efforts.
Subjective language detection is one of the most important challenges in Sentiment Analysis. Because of the weight and frequency in opinionated texts, adjectives are considered a key piece in the opinion extraction process. These subjective units are more and more frequently collected in polarity lexicons in which they appear annotated with their prior polarity. However, at the moment, any polarity lexicon takes into account prior polarity variations across domains. This paper proves that a majority of adjectives change their prior polarity value depending on the domain. We propose a distinction between domain dependent and domain independent adjectives. Moreover, our analysis led us to propose a further classification related to subjectivity degree: constant, mixed and highly subjective adjectives. Following this classification, polarity values will be a better support for Sentiment Analysis.
The following work describes a voting system to automatically classify the sense selection of the complex types Location/Organization and Container/Content, which depend on regular polysemy, as described by the Generative Lexicon (Pustejovsky, 1995) . This kind of sense alternations very often presents semantic underspecificacion between its two possible selected senses. This kind of underspecification is not traditionally contemplated in word sense disambiguation systems, as disambiguation systems are still coping with the need of a representation and recognition of underspecification (Pustejovsky, 2009) The data are characterized by the morphosyntactic and lexical enviroment of the headwords and provided as input for a classifier. The baseline decision tree classifier is compared against an eight-member voting scheme obtained from variants of the training data generated by modifications on the class representation and from two different classification algorithms, namely decision trees and k-nearest neighbors. The voting system improves the accuracy for the non-underspecified senses, but the underspecified sense remains difficult to identify
The management of language resources requires several legal aspects to be taken into consideration. In this paper we discuss a number of these aspects which lead towards the formation of a legal framework for a language resources management agency. The legal framework entails examination of; the agency's stakeholders and the relationships that exist amongst them, the privacy and intellectual property rights that exist around the language resources offered by the agency, and the external (e.g. laws, acts, policies) and internal legal instruments (e.g. end user licence agreements) required for the agency's operation.
This contribution explores the subgroup of text structuring expressions with the form preposition + demonstrative pronoun, thus it is devoted to an aspect of the interaction of coreference relations and relations signaled by discourse connectives (DCs) in a text. The demonstrative pronoun typically signals a referential link to an antecedent, whereas the whole expression can, but does not have to, carry a discourse meaning in sense of discourse connectives. We describe the properties of these phrases/expressions with regard to their antecedents, their position among the text-structuring language means and their features typical for the connective function of them compared to their non-connective function. The analysis is carried out on Czech data from the approx. 50,000 sentences of the Prague Dependency Treebank 2.0, directly on the syntactic trees. We explore the characteristics of these phrases/expressions discovered during two projects: the manual annotation of 1, coreference relations (Nedoluzhko et al. 2011) and 2, discourse connectives, their scopes and meanings (Mladov{\'a} et al. 2008).
The paper describes a rule-based system for tagging clause boundaries, implemented for annotating the Estonian Reference Corpus of the University of Tartu, a collection of written texts containing ca 245 million running words and available for querying via Keeleveeb language portal. The system needs information about parts of speech and grammatical categories coded in the word-forms, i.e. it takes morphologically annotated text as input, but requires no information about the syntactic structure of the sentence. Among the strong points of our system we should mention identifying parenthesis and embedded clauses, i.e. clauses that are inserted into another clause dividing it into two separate parts in the linear text, for example a relative clause following its head noun. That enables a corpus query system to unite the otherwise divided clause, a feature that usually presupposes full parsing. The overall precision of the system is 95{\%} and the recall is 96{\%}. If ordinary clause boundary detection and parenthesis and embedded clause boundary detection are evaluated separately, then one can say that detecting an ordinary clause boundary (recall 98{\%}, precision 96{\%}) is an easier task than detecting an embedded clause (recall 79{\%}, precision 100{\%}).
Evaluations of speech intelligibility based on a read passage are often used in the clinical situation to assess the impact of the disease and/or treatment on spoken communication. Although scale-based measures are often used in the clinical setting, these measures are susceptible to listener response bias. Automatic evaluation tools are being developed in response to some of the drawbacks of perceptual evaluation, however, large corpora judged by listeners are needed to improve and test these tools. To this end, the NKI-CCRT corpus with individual listener judgements on the intelligibility of recordings of 55 speakers treated for cancer of the head and neck will be made available for restricted scientific use. The corpus contains recordings and perceptual evaluations of speech intelligibility over three evaluation moments: before treatment and after treatment (10-weeks and 12-months). Treatment was by means of chemoradiotherapy (CCRT). Thirteen recently graduated speech pathologists rated the speech intelligibility of the recordings on a 7-point scale. Information on recording and perceptual evaluation procedures is presented in addition to preliminary rater reliability and agreement information. Preliminary results show that for many speakers speech intelligibility is rated low before cancer treatment.
The importance of parallel corpora in the NLP field is fully acknowledged. This paper presents a tool that can build parallel corpora given just a seed word list and a pair of languages. Our approach is similar to others proposed in the literature, but introduces a new phase to the process. While most of the systems leave the task of finding websites containing parallel content up to the user, PaCo2 (Parallel Corpora Collector) takes care of that as well. The tool is language independent as far as possible, and adapting the system to work with new languages is fairly straightforward. Evaluation of the different modules has been carried out for Basque-Spanish, Spanish-English and Portuguese-English language pairs. Even though there is still room for improvement, results are positive. Results show that the corpora created have very good quality translations units, and the quality is maintained for the various language pairs. Details of the corpora created up until now are also provided.
Lexical knowledge bases (LKBs), such as WordNet, have been shown to be useful for a range of language processing tasks. Extending these resources is an expensive and time-consuming process. This paper describes an approach to address this problem by automatically generating a mapping from WordNet synsets to Wikipedia articles. A sample of synsets has been manually annotated with article matches for evaluation purposes. The automatic methods are shown to create mappings with precision of 87.8{\%} and recall of 46.9{\%}. These mappings can then be used as a basis for enriching WordNet with new relations based on Wikipedia links. The manual and automatically created data is available online.
This paper presents work on the evaluation of online available machine translation (MT) service, i.e. Google Translate, for English-Croatian language pair in the domain of legislation. The total set of 200 sentences, for which three reference translations are provided, is divided into short and long sentences. Human evaluation is performed by native speakers, using the criteria of adequacy and fluency. For measuring the reliability of agreement among raters, Fleiss' kappa metric is used. Human evaluation is enriched by error analysis, in order to examine the influence of error types on fluency and adequacy, and to use it in further research. Translation errors are divided into several categories: non-translated words, word omissions, unnecessarily translated words, morphological errors, lexical errors, syntactic errors and incorrect punctuation. The automatic evaluation metric BLEU is calculated with regard to a single and multiple reference translations. System level Pearson's correlation between BLEU scores based on a single and multiple reference translations is given, as well as correlation between short and long sentences BLEU scores, and correlation between the criteria of fluency and adequacy and each error category.
This paper presents SentiSense, a concept-based affective lexicon. It is intended to be used in sentiment analysis-related tasks, specially in polarity and intensity classification and emotion identification. SentiSense attaches emotional meanings to concepts from the WordNet lexical database, instead of terms, thus allowing to address the word ambiguity problem using one of the many WordNet-based word sense disambiguation algorithms. SentiSense consists of 5,496 words and 2,190 synsets labeled with an emotion from a set of 14 emotional categories, which are related by an antonym relationship. SentiSense has been developed semi-automatically using several semantic relations between synsets in WordNet. SentiSense is endowed with a set of tools that allow users to visualize the lexicon and some statistics about the distribution of synsets and emotions in SentiSense, as well as to easily expand the lexicon. SentiSense is available for research purposes.
Authorship verification is the task of, given a document and a candi- date author, determining whether or not the document was written by the candi- date author. Traditional approaches to authorship verification have revolved around a candidate author vs. everything else approach. Thus, perhaps the most important aspect of performing authorship verification on a document is the development of an appropriate distractor set to represent everything not the candidate author. The validity of the results of such experiments hinges on the ability to develop an appropriately representative set of distractor documents. Here, we propose a method for performing authorship verification without the use of a distractor set. Using only training data from the candidate author, we are able to perform authorship verification with high confidence (greater than 90{\%} accuracy rates across a large corpus).
We train and test two probabilistic taggers for Arabic phrase break prediction on a purpose-built, gold standard, boundary-annotated and PoS-tagged Qur'an corpus of 77430 words and 8230 sentences. In a related LREC paper (Brierley et al., 2012), we cover dataset build. Here we report on comparative experiments with off-the-shelf N-gram and HMM taggers and coarse-grained feature sets for syntax and prosody, where the task is to predict boundary locations in an unseen test set stripped of boundary annotations by classifying words as breaks or non-breaks. The preponderance of non-breaks in the training data sets a challenging baseline success rate: 85.56{\%}. However, we achieve significant gains in accuracy with the trigram tagger, and significant gains in performance recognition of minority class instances with both taggers via Balanced Classification Rate. This is initial work on a long-term research project to produce annotation schemes, language resources, algorithms, and applications for Classical and Modern Standard Arabic.
A boundary-annotated and part-of-speech tagged corpus is a prerequisite for developing phrase break classifiers. Boundary annotations in English speech corpora are descriptive, delimiting intonation units perceived by the listener. We take a novel approach to phrase break prediction for Arabic, deriving our prosodic annotation scheme from Tajw{\=\i}d (recitation) mark-up in the Qur'an which we then interpret as additional text-based data for computational analysis. This mark-up is prescriptive, and signifies a widely-used recitation style, and one of seven original styles of transmission. Here we report on version 1.0 of our Boundary-Annotated Qur'an dataset of 77430 words and 8230 sentences, where each word is tagged with prosodic and syntactic information at two coarse-grained levels. In (Sawalha et al., 2012), we use the dataset in phrase break prediction experiments. This research is part of a larger-scale project to produce annotation schemes, language resources, algorithms, and applications for Classical and Modern Standard Arabic.
This study introduces and evaluates a computerized approach to measuring Japanese L2 oral proficiency. We present a testing and scoring method that uses a type of structured speech called elicited imitation (EI) to evaluate accuracy of speech productions. Several types of language resources and toolkits are required to develop, administer, and score responses to this test. First, we present a corpus-based test item creation method to produce EI items with targeted linguistic features in a principled and efficient manner. Second, we sketch how we are able to bootstrap a small learner speech corpus to generate a significantly large corpus of training data for language model construction. Lastly, we show how newly created test items effectively classify learners according to their L2 speaking capability and illustrate how our scoring method computes a metric for language proficiency that correlates well with more traditional human scoring methods.
Procedural knowledge is the knowledge required to perform certain tasks, and forms an important part of expertise. A major source of procedural knowledge is natural language instructions. While these readable instructions have been useful learning resources for human, they are not interpretable by machines. Automatically acquiring procedural knowledge in machine interpretable formats from instructions has become an increasingly popular research topic due to their potential applications in process automation. However, it has been insufficiently addressed. This paper presents an approach and an implemented system to assist users to automatically acquire procedural knowledge in structured forms from instructions. We introduce a generic semantic representation of procedures for analysing instructions, using which natural language techniques are applied to automatically extract structured procedures from instructions. The method is evaluated in three domains to justify the generality of the proposed semantic representation as well as the effectiveness of the implemented automatic system.
Multilingual terminological resources do not always include the equivalents of specialized verbs that occur in legal texts. This study aims to bridge that gap by proposing a methodology to assign the equivalents of this kind of predicative units. We use a comparable corpus of judgments produced by the Supreme Court of Canada and by the Supremo Tribunal de Justi{\c{c}}a de Portugal. From this corpus, 200 English and Portuguese verbs are selected. The description of the verbs is based on the theory of Frame Semantics (Fillmore 1977, 1977, 1982, 1985) as well as on the FrameNet methodology (Ruppenhofer et al. 2010). Specialized verbs are said to evoke a semantic frame, a sort of conceptual scenario in which a number of mandatory elements play specific roles (e.g. the role of judge, the role of defendant). Given that semantic frames are language independent to a fair degree (Boas 2005; Baker 2009), the labels attributed to each of the 76 identified frames (e.g. [Crime], [Regulations]) were used to group together 165 pairs of candidate equivalents. 71{\%} of them are full equivalents, whereas 29{\%} are only partial equivalents.
In this paper, we introduce TimeBankPT, a TimeML annotated corpus of Portuguese. It has been produced by adapting an existing resource for English, namely the data used in the first TempEval challenge. TimeBankPT is the first corpus of Portuguese with rich temporal annotations (i.e. it includes annotations not only of temporal expressions but also about events and temporal relations). In addition, it was subjected to an automated error mining procedure that checks the consistency of the annotated temporal relations based on their logical properties. This procedure allowed for the detection of some errors in the annotations, that also affect the original English corpus. The Portuguese language is currently undergoing a spelling reform, and several countries where Portuguese is official are in a transitional period where old and new orthographies are valid. TimeBankPT adopts the recent spelling reform. This decision is to preserve its future usefulness. TimeBankPT is freely available for download.
This paper discusses how information on properties in a currently developed Danish thesaurus can be transferred to the Danish wordnet, DanNet, and in this way enrich the wordnet with the highly relevant links between properties and their external arguments (i.e. tasty ― food). In spite of the fact that the thesaurus is still under development (two thirds still to be compiled) we perform an automatic transfer of relations from the thesaurus to the wordnet which shows promising results. In all, 2,362 property relations are automatically transferred to DanNet and 2{\%} of the transferred material is manually validated. The pilot validation indicates that approx. 90 {\%} of the transferred relations are correctly assigned whereas around 10{\%} are either erroneous or just not very informative, a fact which, however, can partly be explained by the incompleteness of the material at its current stage. As a further consequence, the experiment has led to a richer specification of the editor guidelines to be used in the last compilation phase of the thesaurus.
We present Korp, the corpus infrastructure of Spr{\aa}kbanken (the Swedish Language Bank). The infrastructure consists of three main components: the Korp corpus pipeline, the Korp backend, and the Korp frontend. The Korp corpus pipeline is used for importing corpora, annotating them, and then exporting the annotated corpora into different formats. An essential feature of the pipeline is the ability to leave existing annotations untouched, both structural and word level annotations, and to use the existing annotations as the foundation of other annotations. The Korp backend consists of a set of REST-based web services for searching in and retrieving information about the corpora. Finally, the Korp frontend is a graphical search interface that interacts with the Korp backend. The interface has been inspired by corpus search interfaces such as SketchEngine, Glossa, and DeepDict, and it uses State Chart XML (SCXML) in order to enable users to bookmark interaction states. We give a functional and technical overview of the three components, followed by a discussion of planned future work.
We present our ongoing work on Karp, Spr{\aa}kbanken's (the Swedish Language Bank) open lexical infrastructure, which has two main functions: (1) to support the work on creating, curating, and integrating our various lexical resources; and (2) to publish daily versions of the resources, making them searchable and downloadable. An important requirement on the lexical infrastructure is also that we maintain a strong bidirectional connection to our corpus infrastructure. At the heart of the infrastructure is the SweFN++ project with the goal to create free Swedish lexical resources geared towards language technology applications. The infrastructure currently hosts 15 Swedish lexical resources, including historical ones, some of which have been created from scratch using existing free resources, both external and in-house. The resources are integrated through links to a pivot lexical resource, SALDO, a large morphological and lexical-semantic resource for modern Swedish. SALDO has been selected as the pivot partly because of its size and quality, but also because its form and sense units have been assigned persistent identifiers (PIDs) to which the lexical information in other lexical resources and in corpora are linked.
This work describes how derivation tree fragments based on a variant of Tree Adjoining Grammar (TAG) can be used to check treebank consistency. Annotation of word sequences are compared both for their internal structural consistency, and their external relation to the rest of the tree. We expand on earlier work in this area in three ways. First, we provide a more complete description of the system, showing how a naive use of TAG structures will not work, leading to a necessary refinement. We also provide a more complete account of the processing pipeline, including the grouping together of structurally similar errors and their elimination of duplicates. Second, we include the new experimental external relation check to find an additional class of errors. Third, we broaden the evaluation to include both the internal and external relation checks, and evaluate the system on both an Arabic and English treebank. The evaluation has been successful enough that the internal check has been integrated into the standard pipeline for current English treebank construction at the Linguistic Data Consortium
This paper presents the Turk Bootstrap Word Sense Inventory (TWSI) 2.0. This lexical resource, created by a crowdsourcing process using Amazon Mechanical Turk (http://www.mturk.com), encompasses a sense inventory for lexical substitution for 1,012 highly frequent English common nouns. Along with each sense, a large number of sense-annotated occurrences in context are given, as well as a weighted list of substitutions. Sense distinctions are not motivated by lexicographic considerations, but driven by substitutability: two usages belong to the same sense if their substitutions overlap considerably. After laying out the need for such a resource, the data is characterized in terms of organization and quantity. Then, we briefly describe how this data was used to create a system for lexical substitutions. Training a supervised lexical substitution system on a smaller version of the resource resulted in well over 90{\%} acceptability for lexical substitutions provided by the system. Thus, this resource can be used to set up reliable, enabling technologies for semantic natural language processing (NLP), some of which we discuss briefly.
The Multiphonia Corpus consists of audio-video classroom recordings comparing two methods of phonetic correction (the traditional' articulatory method, and the Verbo-Tonal Method) This database was created not only to remedy the crucial lack of information and pedagogical resources on teaching pronunciation but also to test the benefit of VTM on Second Language pronunciation. The VTM method emphasizes the role of prosody cues as vectors of second language acquisition of the phonemic system. This method also provides various and unusual procedures including facilitating gestures in order to work on spotting and assimilating the target language prosodic system (rhythm, accentuation, intonation). In doing so, speech rhythm is apprehended in correlation with body/gestural rhythm. The student is thus encouraged to associate gestures activating the motor memory at play during the repetition of target words or phrases. In turn, pedagogical gestures have an impact on second language lexical items' recollection (Allen, 1995; Tellier, 2008). Ultimately, this large corpus (96 hours of class sessions' recordings) will be made available to the scientific community, with several layers of annotations available for the study of segmental, prosodic and gestural aspects of L2 speech.
This paper describes methods and results for the annotation of two discourse-level phenomena, connectives and pronouns, over a multilingual parallel corpus. Excerpts from Europarl in English and French have been annotated with disambiguation information for connectives and pronouns, for about 3600 tokens. This data is then used in several ways: for cross-linguistic studies, for training automatic disambiguation software, and ultimately for training and testing discourse-aware statistical machine translation systems. The paper presents the annotation procedures and their results in detail, and overviews the first systems trained on the annotated resources and their use for machine translation.
The Knowledge Based Population (KBP) evaluation track of the Text Analysis Conferences (TAC) has been held for the past 3 years. One of the two tasks of KBP is slot filling: finding within a large corpus the values of a set of attributes of given people and organizations. This task has proven very challenging, with top systems rarely exceeding 30{\%} F-measure. In this paper, we present an error analysis and classification for those answers which could be found by a manual corpus search but were not found by any of the systems participating in the 2010 evaluation. The most common sources of failure were limitations on inference, errors in coreference (particularly with nominal anaphors), and errors in named entity recognition. We relate the types of errors to the characteristics of the task and show the wide diversity of problems that must be addressed to improve overall performance.
Logical metonymies like ''''''``The author began the book'''''''' involve the interpretation of events that are not realized in the sentence (Covert events: -{\textgreater} ''''''``writing the book''''''''). The Generative Lexicon (Pustejovsky 1995) provides a qualia-based account of covert event interpretation, claiming that the covert event is retrieved from the qualia structure of the object. Such a theory poses the question of to what extent covert events in logical metonymies can be accounted for by qualia structures. Building on previous work on English, we present a corpus study for German verbs (''''''``anfangen (mit)'''''''', ''''''``aufhoeren (mit)'''''''', ''''''``beenden'''''''', ''''''``beginnen (mit)'''''''', ''''''``geniessen'''''''', based on data obtained from the deWaC corpus. We built a corpus of logical metonymies, which were manually annotated and compared with the qualia structures of their objects, then we contrasted annotation results from two expert annotators for metonymies (''''''``The author began the book'''''''') and long forms (''''''``The author began reading the book'''''''') across verbs. Our annotation was evaluated on a sample of sentences annotated by a group of naive annotators on a crowdsourcing platform. The logical metonymy database (2661 metonymies and 1886 long forms) with two expert annotations is freely available for scientific research purposes.
In this paper, we present HunOr, the first multi-domain Hungarian―Russian parallel corpus. Some of the corpus texts have been manually aligned and split into sentences, besides, named entities also have been annotated while the other parts are automatically aligned at the sentence level and they are POS-tagged as well. The corpus contains texts from the domains literature, official language use and science, however, we would like to add texts from the news domain to the corpus. In the future, we are planning to carry out a syntactic annotation of the HunOr corpus, which will further enhance the usability of the corpus in various NLP fields such as transfer-based machine translation or cross lingual information retrieval.
This paper presents preliminary results of an effort aiming at the creation of a morphological dictionary of Polish, PoliMorf, available under a very liberal BSD-style license. The dictionary is a result of a merger of two existing resources, SGJP and Morfologik and was prepared within the CESAR/META-NET initiative. The work completed so far includes re-licensing of the two dictionaries and filling the new resource with the morphological data semi-automatically unified from both sources. The merging process is controlled by the collaborative dictionary development web application Ku{\'z}nia, also implemented within the project. The tool involves several advanced features such as using SGJP inflectional patterns for form generation, possibility of attaching dictionary labels and classification schemes to lexemes, dictionary source record and change tracking. Since SGJP and Morfologik are already used in a significant number of Natural Language Processing projects in Poland, we expect PoliMorf to become the Polish morphological dictionary of choice for many years to come.
Frequency lists and/or lexicons contain information about the words and their statistics. They tend to find their readers among language learners, language teachers, linguists and lexicographers. Making them available in electronic format helps to expand the target group to cover language engineers, computer programmers and other specialists working in such areas as information retrieval, spam filtering, text readability analysis, test generation etc. This article describes a new freely available electronic frequency list of modern Swedish which was created in the EU project KELLY. We provide a short description of the KELLY project; examine the methodological approach and mention some details on the compiling of the corpus from which the list has been derived. Further, we discuss the type of information the list contains; describe the steps for list generation; provide information on the coverage and some other statistics over the items in the list. Finally, some practical information on the license for the Swedish Kelly-list distribution is given; potential application areas are suggested; and future plans for its expansion are mentioned. We hope that with some publicity we can help this list find its users.
We present a resource for automatically associating strings of text with English Wikipedia concepts. Our machinery is bi-directional, in the sense that it uses the same fundamental probabilistic methods to map strings to empirical distributions over Wikipedia articles as it does to map article URLs to distributions over short, language-independent strings of natural language text. For maximal inter-operability, we release our resource as a set of flat line-based text files, lexicographically sorted and encoded with UTF-8. These files capture joint probability distributions underlying concepts (we use the terms article, concept and Wikipedia URL interchangeably) and associated snippets of text, as well as other features that can come in handy when working with Wikipedia articles and related information.
We have built a corpus containing texts in 106 languages from texts available on the Internet and on Wikipedia. The W2C Web Corpus contains 54.7{\textasciitilde}GB of text and the W2C Wiki Corpus contains 8.5{\textasciitilde}GB of text. The W2C Web Corpus contains more than 100{\textasciitilde}MB of text available for 75 languages. At least 10{\textasciitilde}MB of text is available for 100 languages. These corpora are a unique data source for linguists, since they outclass all published works both in the size of the material collected and the number of languages covered. This language data resource can be of use particularly to researchers specialized in multilingual technologies development. We also developed software that greatly simplifies the creation of a new text corpus for a given language, using text materials freely available on the Internet. Special attention was given to components for filtering and de-duplication that allow to keep the material quality very high.
In this paper, we present corpus-based procedures to semi-automatically discover features relevant for the study of recent language change in scientific registers. First, linguistic features potentially adherent to recent language change are extracted from the SciTex Corpus. Second, features are assessed for their relevance for the study of recent language change in scientific registers by means of correspondence analysis. The discovered features will serve for further investigations of the linguistic evolution of newly emerged scientific registers.
We develop a model for predicting verb sense from subcategorization information and integrate it into SSI-Dijkstra, a wide-coverage knowledge-based WSD algorithm. Adding syntactic knowledge in this way should correct the current poor performance of WSD systems on verbs. This paper also presents, for the first time, an evaluation of SSI-Dijkstra on a standard data set which enables a comparison of this algorithm with other knowledge-based WSD systems. Our results show that our system is competitive with current graph-based WSD algorithms, and that the subcategorization model can be used to achieve better verb sense disambiguation performance.
We present DSim, a new sentence aligned Danish monolingual parallel corpus extracted from 3701 pairs of news telegrams and corresponding professionally simplified short news articles. The corpus is intended for building automatic text simplification for adult readers. We compare DSim to different examples of monolingual parallel corpora, and we argue that this corpus is a promising basis for future development of automatic data-driven text simplification systems in Danish. The corpus contains both the collection of paired articles and a sentence aligned bitext, and we show that sentence alignment using simple tf*idf weighted cosine similarity scoring is on line with state―of―the―art when evaluated against a hand-aligned sample. The alignment results are compared to state of the art for English sentence alignment. We finally compare the source and simplified sides of the corpus in terms of lexical and syntactic characteristics and readability, and find that the one―to―many sentence aligned corpus is representative of the sentence simplifications observed in the unaligned collection of article pairs.
This paper reports the annotation of a Brazilian Portuguese Treebank with semantic role labels following Propbank guidelines. A different language and a different parser output impact the task and require some decisions on how to annotate the corpus. Therefore, a new annotation guide ― called Propbank-Br - has been generated to deal with specific language phenomena and parser problems. In this phase of the project, the corpus was annotated by a unique linguist. The annotation task reported here is inserted in a larger projet for the Brazilian Portuguese language. This project aims to build Brazilian verbs frames files and a broader and distributed annotation of semantic role labels in Brazilian Portuguese, allowing inter-annotator agreement measures. The corpus, available in web, is already being used to build a semantic tagger for Portuguese language.
To facilitate future research in unsupervised induction of syntactic structure and to standardize best-practices, we propose a tagset that consists of twelve universal part-of-speech categories. In addition to the tagset, we develop a mapping from 25 different treebank tagsets to this universal set. As a result, when combined with the original treebank data, this universal tagset and mapping produce a dataset consisting of common parts-of-speech for 22 different languages. We highlight the use of this resource via three experiments, that (1) compare tagging accuracies across languages, (2) present an unsupervised grammar induction approach that does not use gold standard part-of-speech tags, and (3) use the universal tags to transfer dependency parsers between languages, achieving state-of-the-art results.
Large scale annotated corpora of child language can be of great value in assessing theoretical proposals regarding language acquisition models. For example, they can help determine whether the type and amount of data required by a proposed language acquisition model can actually be found in a naturalistic data sample. To this end, several recent efforts have augmented the CHILDES child language corpora with POS tagging and parsing information for languages such as English. With the increasing availability of robust NLP systems and electronic resources, these corpora can be further annotated with more detailed information about the properties of words, verb argument structure, and sentences. This paper describes such an initiative for combining information from various sources to extend the annotation of the English CHILDES corpora with linguistic, psycholinguistic and distributional information, along with an example illustrating an application of this approach to the extraction of verb alternation information. The end result, the English CHILDES Verb Construction Database, is an integrated resource containing information such as grammatical relations, verb semantic classes, and age of acquisition, enabling more targeted complex searches involving different levels of annotation that can facilitate a more detailed analysis of the linguistic input available to children.
Parallel aligned treebanks (PAT) are linguistic corpora annotated with morphological and syntactic structures that are aligned at sentence as well as sub-sentence levels. They are valuable resources for improving machine translation (MT) quality. Recently, there has been an increasing demand for such data, especially for divergent language pairs. The Linguistic Data Consortium (LDC) and its academic partners have been developing Arabic-English and Chinese-English PATs for several years. This paper describes the PAT corpus creation effort for the program GALE (Global Autonomous Language Exploitation) and introduces the potential issues of scaling up this PAT effort for the program BOLT (Broad Operational Language Translation). Based on existing infrastructures and in the light of current annotation process, challenges and approaches, we are exploring new methodologies to address emerging challenges in constructing PATs, including data volume bottlenecks, dialect issues of Arabic languages, and new genre features related to rapidly changing social media. Preliminary experimental results are presented to show the feasibility of the approaches proposed.
To advance information extraction and question answering technologies toward a more realistic path, the U.S. NIST (National Institute of Standards and Technology) initiated the KBP (Knowledge Base Population) task as one of the TAC (Text Analysis Conference) evaluation tracks. It aims to encourage research in automatic information extraction of named entities from unstructured texts with the ultimate goal of integrating such information into a structured Knowledge Base. The KBP track consists of two types of evaluation: Named Entity Linking (NEL) and Slot Filling. This paper describes the linguistic resource creation efforts at the Linguistic Data Consortium (LDC) in support of Named Entity Linking evaluation of KBP, focusing on annotation methodologies, process, and features of corpora from 2009 to 2011, with a highlighted analysis of the cross-lingual NEL data. Progressing from monolingual to cross-lingual Entity Linking technologies, the 2011 cross-lingual NEL evaluation targeted multilingual capabilities. Annotation accuracy is presented in comparison with system performance, with promising results from cross-lingual entity linking systems.
This paper proposes a new method of constructing arbitrary class-based related word dictionaries on interactive topic models; we assume that each class is described by a topic. We propose a new semi-supervised method that uses the simplest topic model yielded by the standard EM algorithm; model calculation is very rapid. Furthermore our approach allows a dictionary to be modified interactively and the final dictionary has a hierarchical structure. This paper makes three contributions. First, it proposes a word-based semi-supervised topic model. Second, we apply the semi-supervised topic model to interactive learning; this approach is called the Interactive Topic Model. Third, we propose a score function; it extracts the related words that occupy the middle layer of the hierarchical structure. Experiments show that our method can appropriately retrieve the words belonging to an arbitrary class.
We developed a dialogue-based tutoring system for teaching English to Japanese students and plan to transfer the current software tutoring agent into an embodied robot in the hope that the robot will enrich conversation by allowing more natural interactions in small group learning situations. To enable smooth communication between an intelligent agent and the user, the agent must have realistic models on when to take turns, when to interrupt, and how to catch the partner's attention. For developing the realistic models applicable for computer assisted language learning systems, we also need to consider the differences between the mother tongue and second language that affect communication style. We collected a multimodal corpus of multi-party conversations in English as the second language to investigate the differences in communication styles. We describe our multimodal corpus and explore features of communication style e.g. filled pauses, and non-verbal information, such as eye-gaze, which show different characteristics between the mother tongue and second language.
The Japanese language has rich variety and quantity of word variant. Since 1980s, it has been recognized that this richness becomes an obstacle against natural language processing. A complete solution, however, has not been presented yet. This paper proposes a method to recognize Katakana variants―a major type of word variant in Japanese―in the process of dictionary look-up. For a given set of variant generation rules, the method executes variant generation and entry retrieval simultaneously and efficiently. We have developed the seven-layered rule set (216 rules in total) according to the specification manual of UniDic-2.1.0 and other sources. An experiment shows that the spelling-variant generator with 102 rules in the first five layers is almost perfect. Another experiment shows that the form-variant generator with all 216 rules is powerful and 77.7{\%} of multiple spellings of Katakana loanwords are unnecessary (i.e., can be removed). This result means that the proposed method can drastically reduce the number of variants that we have to register into a dictionary in advance.
We describe SUTIME, a temporal tagger for recognizing and normalizing temporal expressions in English text. SUTIME is available as part of the Stanford CoreNLP pipeline and can be used to annotate documents with temporal information. It is a deterministic rule-based system designed for extensibility. Testing on the TempEval-2 evaluation corpus shows that this system outperforms state-of-the-art techniques.
This paper addresses the problem of automatically recognizing linguistically significant nonmanual expressions in American Sign Language from video. We develop a fully automatic system that is able to track facial expressions and head movements, and detect and recognize facial events continuously from video. The main contributions of the proposed framework are the following: (1) We have built a stochastic and adaptive ensemble of face trackers to address factors resulting in lost face track; (2) We combine 2D and 3D deformable face models to warp input frames, thus correcting for any variation in facial appearance resulting from changes in 3D head pose; (3) We use a combination of geometric features and texture features extracted from a canonical frontal representation. The proposed new framework makes it possible to detect grammatically significant nonmanual expressions from continuous signing and to differentiate successfully among linguistically significant expressions that involve subtle differences in appearance. We present results that are based on the use of a dataset containing 330 sentences from videos that were collected and linguistically annotated at Boston University.
This research focuses on text processing in the sphere of English-language social media. We introduce two database resources. The first, CECS (Casual English Conversion System) database, a lexicon-type resource of 1,255 entries, was constructed for use in our experimental system for the automated normalization of casual, irregularly-formed English used in communications such as Twitter. Our rule-based approach primarily aims to avoid problems caused by user creativity and individuality of language when Twitter-style text is used as input in Machine Translation, and to aid comprehension for non-native speakers of English. Although the database is still under development, we have so far carried out two evaluation experiments using our system which have shown positive results. The second database, CEGS (Casual English Generation System) phoneme database contains sets of alternative spellings for the phonemes in the CMU Pronouncing Dictionary, designed for use in a system for generating phoneme-based casual English text from regular English input; in other words, automatically producing humanlike creative sentences as an AI task. This paper provides an overview of the necessity, method, application and evaluation of both resources.
In this paper we present the definition of a conceptual approach for the information space entailed by a multidisciplinary and collaborative project, ''''''``Cimbrian as a test case for synchronic and diachronic language variation'', which provides linguists with a test bed for formal hypotheses concerning human language. Aims of the project are to collect, digitize and tag linguistic data from the German variety of Cimbrian - spoken in three areas of northern Italy: Giazza (VR), Luserna (TN), and Roana (VI) - and to make available on-line a valuable and innovative linguistic resource for the in-depth study of Cimbrian. The task is addressed by a multidisciplinary team of linguists and computer scientists who, combining their competence, aim to make available new tools for linguistic analysis
The number of applied Dialogue Systems is ever increasing in several service providing and other applications as a way to efficiently and inexpensively serve large numbers of customers. A DS that employs some form of adaptation to the environment and its users is called an Adaptive Dialogue System (ADS). A significant part of the research community has lately focused on ADS and many existing or novel techniques are being applied to this problem. One of the most promising techniques is Reinforcement Learning (RL) and especially online RL. This paper focuses on online RL techniques used to achieve adaptation in Dialogue Management and provides an evaluation of various such methods in an effort to aid the designers of ADS in deciding which method to use. To the best of our knowledge there is no other work to compare online RL techniques on the dialogue management problem.
The logistics of collecting resources for Machine Translation (MT) has always been a cause of concern for some of the resource deprived languages of the world. The recent advent of crowdsourcing platforms provides an opportunity to explore the large scale generation of resources for MT. However, before venturing into this mode of resource collection, it is important to understand the various factors such as, task design, crowd motivation, quality control, etc. which can influence the success of such a crowd sourcing venture. In this paper, we present our experiences based on a series of experiments performed. This is an attempt to provide a holistic view of the different facets of translation crowd sourcing and identifying key challenges which need to be addressed for building a practical crowdsourcing solution for MT.
This paper describes the upgrading process of the Multilingual Central Repository (MCR). The new MCR uses WordNet 3.0 as Interlingual-Index (ILI). Now, the current version of the MCR integrates in the same EuroWordNet framework wordnets from five different languages: English, Spanish, Catalan, Basque and Galician. In order to provide ontological coherence to all the integrated wordnets, the MCR has also been enriched with a disparate set of ontologies: Base Concepts, Top Ontology, WordNet Domains and Suggested Upper Merged Ontology. The whole content of the MCR is freely available.
Significant breakthroughs in machine translation only seem possible if human translators are taken into the loop. While automatic evaluation and scoring mechanisms such as BLEU have enabled the fast development of systems, it is not clear how systems can meet real-world (quality) requirements in industrial translation scenarios today. The taraX{\"U
Abstract Evaluating a taxonomy learned automatically against an existing gold standard is a very complex problem, because differences stem from the number, label, depth and ordering of the taxonomy nodes. In this paper we propose casting the problem as one of comparing two hierarchical clusters. To this end we defined a variation of the Fowlkes and Mallows measure (Fowlkes and Mallows, 1983). Our method assigns a similarity value B{\textasciicircum}i{\_}(l,r) to the learned (l) and reference (r) taxonomy for each cut i of the corresponding anonymised hierarchies, starting from the topmost nodes down to the leaf concepts. For each cut i, the two hierarchies can be seen as two clusterings C{\textasciicircum}i{\_}l , C{\textasciicircum}i{\_}r of the leaf concepts. We assign a prize to early similarity values, i.e. when concepts are clustered in a similar way down to the lowest taxonomy levels (close to the leaf nodes). We apply our method to the evaluation of the taxonomy learning methods put forward by Navigli et al. (2011) and Kozareva and Hovy (2010).
Regularities in position and level of prosodic prominences associated to patterns of Information Structure are identified for some Italian varieties. The experiments' results suggest a possibly new structural hypothesis on the role and function of the main prominence in marking information patterns. (1) An abstract and merely structural, ''''''``topologic'''''''' concept of Prominence location can be conceived of, as endowed with the function of demarcation between units, before their culmination and ''''''``description''''''''. This may suffice to explain much of the process by which speakers interpret the IS of utterances in discourse. Further features, such as the specific intonational contours of the different IS units, may thus represent a certain amount of redundancy. (2) Real utterances do not always signal the distribution of Topic and Focus clearly. Acoustically, many remain underspecified in this respect. This is especially true for the distinction between Topic-Focus and Broad Focus, which indeed often has no serious effects on the progression of communicative dynamism in the subsequent discourse. (3) The consistency of such results with the law of least effort, and the very high percent of matching between perceptual evaluations and automatic measurement, seem to validate the used algorithm.
We describe two constraint-based methods that can be used to improve the recall of a shallow discourse parser based on conditional random field chunking. These method uses a set of natural structural constraints as well as others that follow from the annotation guidelines of the Penn Discourse Treebank. We evaluated the resulting systems on the standard test set of the PDTB and achieved a rebalancing of precision and recall with improved F-measures across the board. This was especially notable when we used evaluation metrics taking partial matches into account; for these measures, we achieved F-measure improvements of several points.
Keeping pace with other wordnets development, we present the challenges raised by the Romanian derivational system and our methodology for identifying derived words and their stems in the Romanian Wordnet. To attain this aim we rely only on the list of literals in the wordnet and on a list of Romanian affixes; the automatically obtained pairs require automatic and manual validation, based on a few heuristics. The correct members of the pairs are linked together and the relation is associated a semantic label whenever necessary. This label is proved to have cross-language validity. The work reported here contributes to the increase of the number of relations both between literals and between synsets, especially the cross-part-of-speech links. Words belonging to the same lexical family are identified easily. The benefits of thus improving a language resource such as wordnet become self-evident. The paper also contains an overview of the current status of the Romanian wordnet and an envisaged plan for continuing the research.
It is well-known that human listeners significantly outperform machines when it comes to transcribing speech. This paper presents a progress report of the joint research in the automatic vs human speech transcription and of the perceptual experiments developed at LIMSI that aims to increase our understanding of automatic speech recognition errors. Two paradigms are described here in which human listeners are asked to transcribe speech segments containing words that are frequently misrecognized by the system. In particular, we sought to gain information about the impact of increased context to help humans disambiguate problematic lexical items, typically homophone or near-homophone words. The long-term aim of this research is to improve the modeling of ambiguous contexts so as to reduce automatic transcription errors.
The Swedish FrameNet project, SweFN, is a lexical resource under development, designed to support both humans and different applications within language technology, such as text generation, text understanding and information extraction. SweFN is constructed in line with the Berkeley FrameNet and the project is aiming to make it a free, full-scale, multi-functional lexical resource covering morphological, syntactic, and semantic descriptions of 50,000 entries. Frames populated by lexical units belonging to the general vocabulary dominate in SweFN, but there are also frames from the medical and the art domain. As Swedish is a language with very productive compounding, special attention is paid to semantic relations within the one word compounds which populate the frames. This is of relevance for understanding the meaning of the compounds and for capturing the semantic and syntactic alternations which are brought about in the course of compounding. SweFN is a component within a complex of modern and historical lexicon resources named SweFN++, available at .
We have created a synchronous corpus of acoustic and 3D facial marker data from multiple speakers for adaptive audio-visual text-to-speech synthesis. The corpus contains data from one female and two male speakers and amounts to 223 Austrian German sentences each. In this paper, we first describe the recording process, using professional audio equipment and a marker-based 3D facial motion capturing system for the audio-visual recordings. We then turn to post-processing, which incorporates forced alignment, principal component analysis (PCA) on the visual data, and some manual checking and corrections. Finally, we describe the resulting corpus, which will be released under a research license at the end of our project. We show that the standard PCA based feature extraction approach also works on a multi-speaker database in the adaptation scenario, where there is no data from the target speaker available in the PCA step.
In different fields of the humanities annotations of multimodal resources are a necessary component of the research workflow. Examples include linguistics, psychology, anthropology, etc. However, creation of those annotations is a very laborious task, which can take 50 to 100 times the length of the annotated media, or more. This can be significantly improved by applying innovative audio and video processing algorithms, which analyze the recordings and provide automated annotations. This is the aim of the AVATecH project, which is a collaboration of the Max Planck Institute for Psycholinguistics (MPI) and the Fraunhofer institutes HHI and IAIS. In this paper we present a set of results of automated annotation together with an evaluation of their quality.
Text simplification is the process of reducing the lexical and syntactic complexity of a text while attempting to preserve (most of) its information content. It has recently emerged as an important research area, which holds promise for enhancing the text readability for the benefit of a broader audience as well as for increasing the performance of other applications. Our work focuses on syntactic complexity reduction and deals with the task of corpus-based acquisition of syntactic simplification rules for the French language. We show that the data-driven manual acquisition of simplification rules can be complemented by the semi-automatic detection of syntactic constructions requiring simplification. We provide the first comprehensive set of syntactic simplification rules for French, whose size is comparable to similar resources that exist for English and Brazilian Portuguese. Unlike these manually-built resources, our resource integrates larger lists of lexical cues signaling simplifiable constructions, that are useful for informing practical systems.
Named Entities (NEs) that occur in natural language text are important especially due to the advent of social media, and they play a critical role in the development of many natural language technologies. In this paper, we systematically analyze the patterns of occurrence and co-occurrence of NEs in standard large English news corpora - providing valuable insight for the understanding of the corpus, and subsequently paving way for the development of technologies that rely critically on handling NEs. We use two distinctive approaches: normal statistical analysis that measure and report the occurrence patterns of NEs in terms of frequency, growth, etc., and a complex networks based analysis that measures the co-occurrence pattern in terms of connectivity, degree-distribution, small-world phenomenon, etc. Our analysis indicates that: (i) NEs form an open-set in corpora and grow linearly, (ii) presence of a kernel and peripheral NE's, with the large periphery occurring rarely, and (iii) a strong evidence of small-world phenomenon. Our findings may suggest effective ways for construction of NE lexicons to aid efficient development of several natural language technologies.
Chinese characters are used both in Japanese and Chinese, which are called Kanji and Hanzi respectively. Chinese characters contain significant semantic information, a mapping table between Kanji and Hanzi can be very useful for many Japanese-Chinese bilingual applications, such as machine translation and cross-lingual information retrieval. Because Kanji characters are originated from ancient China, most Kanji have corresponding Chinese characters in Hanzi. However, the relation between Kanji and Hanzi is quite complicated. In this paper, we propose a method of making a Chinese characters mapping table of Japanese, Traditional Chinese and Simplified Chinese automatically by means of freely available resources. We define seven categories for Kanji based on the relation between Kanji and Hanzi, and classify mappings of Chinese characters into these categories. We use a resource from Wiktionary to show the completeness of the mapping table we made. Statistical comparison shows that our proposed method makes a more complete mapping table than the current version of Wiktionary.
Due to the increase in the number and depth of analyses required over the text, like entity recognition, POS tagging, syntactic analysis, etc. the annotation in-line has become unpractical. In Natural Language Processing (NLP) some emphasis has been placed in finding an annotation method to solve this problem. A possibility is the standoff annotation. With this annotation style it is possible to add new levels of annotation without disturbing exiting ones, with minimal knock on effects. This annotation will increase the possibility of adding more linguistic information as well as more possibilities for sharing textual resources. In this paper we present a tool developed in the framework of the European Metanet4u (Enhancing the European Linguistic Infrastructure, GA 270893) for creating a multi-layered XML annotation scheme, based on the GrAF proposal for standoff annotations.
This article presents work carried out within the framework of the ongoing ANR (French National Research Agency) project Chronolines, which focuses on the temporal processing of large news-wire corpora in English and French. The aim of the project is to create new and innovative interfaces for visualizing textual content according to temporal criteria. Extracting and normalizing the temporal information in texts through linguistic annotation is an essential step towards attaining this objective. With this goal in mind, we developed a set of guidelines for the annotation of temporal and event expressions that is intended to be compatible with the TimeML markup language, while addressing some of its pitfalls. We provide results of an initial application of these guidelines to real news-wire texts in French over several iterations of the annotation process. These results include inter-annotator agreement figures and an error analysis. Our final inter-annotator agreement figures compare favorably with those reported for the TimeBank 1.2 annotation project.
We present our work in processing the Reference Corpus of Contemporary Portuguese and its publication online. After discussing how the corpus was built and our choice of meta-data, we turn to the processes and tools involved for the cleaning, preparation and annotation to make the corpus suitable for linguistic inquiries. The Web platform is described, and we show examples of linguistic resources that can be extracted from the platform for use in linguistic studies or in NLP.
In this paper, we investigate the usage of a non-canonical German passive alternation for ditransitive verbs, the recipient passive, in naturally occuring corpus data. We propose a classifier that predicts the voice of a ditransitive verb based on the contextually determined properties its arguments. As the recipient passive is a low frequent phenomenon, we first create a special data set focussing on German ditransitive verbs which are frequently used in the recipient passive. We use a broad-coverage grammar-based parser, the German LFG parser, to automatically annotate our data set for the morpho-syntactic properties of the involved predicate arguments. We train a Maximum Entropy classifier on the automatically annotated sentences and achieve an accuracy of 98.05{\%}, clearly outperforming the baseline that always predicts active voice baseline (94.6{\%}).
We present a new open source subjectivity lexicon for Dutch adjectives. The lexicon is a dictionary of 1,100 adjectives that occur frequently in online product reviews, manually annotated with polarity strength, subjectivity and intensity, for each word sense. We discuss two machine learning methods (using distributional extraction and synset relations) to automatically expand the lexicon to 5,500 words. We evaluate the lexicon by comparing it to the user-given star rating of online product reviews. We show promising results in both in-domain and cross-domain evaluation. The lexicon is publicly available as part of the PATTERN software package (http://www.clips.ua.ac.be/pages/pattern).
With our experiment, we show how we can detect and annotate clausal coordinate ellipsis with Constraint Grammar rules. We focus on such an elliptical structure in which there are two coordinated clauses, and the latter one lacks a verb. For example, the sentence This belongs to me and that to you demonstrates the ellipsis in question, namely gapping. The Constraint Grammar rules are made for a Finnish parsebank, FinnTreeBank. The FinnTreeBank project is building a parsebank in the dependency syntactic framework in which verbs are central since other sentence elements depend on them. Without correct detection of omitted verbs, the syntactic analysis of the whole sentence fails. In the experiment, we detect gapping based on morphology and linear order of the words without using syntactic or semantic information. The test corpus, Finnish Wikipedia, is morphologically analyzed but not disambiguated. Even with an ambiguous morphological analysis, the results show that 89,9{\%} of the detected sentences are elliptical, making the rules accurate enough to be used in the creation of FinnTreeBank. Once we have a morphologically disambiguated corpus, we can write more accurate rules and expect better results.
Within the general purpose of information extraction, detection of event descriptions is often an important clue. An important characteristic of event designation in texts, and especially in media, is that it changes over time. Understanding how these designations evolve is important in information retrieval and information extraction. Our first hypothesis is that, when an event first occurs, media relate it in a very descriptive way (using verbal designations) whereas after some time, they use shorter nominal designations instead. Our second hypothesis is that the number of different nominal designations for an event tends to stabilize itself over time. In this article, we present our methodology concerning the study of the evolution of event designations in French documents from the news agency AFP. For this preliminary study, we focused on 7 topics which have been relatively important in France. Verbal and nominal designations of events have been manually annotated in manually selected topic-related passages. This French corpus contains a total of 2064 annotations. We then provide preliminary interesting statistical results and observations concerning these evolutions.
This paper describes a methodology for testing and evaluating the performance of Machine Reading systems through Question Answering and Reading Comprehension Tests. The methodology is being used in QA4MRE (QA for Machine Reading Evaluation), one of the labs of CLEF. The task was to answer a series of multiple choice tests, each based on a single document. This allows complex questions to be asked but makes evaluation simple and completely automatic. The evaluation architecture is completely multilingual: test documents, questions, and their answers are identical in all the supported languages. Background text collections are comparable collections harvested from the web for a set of predefined topics. Each test received an evaluation score between 0 and 1 using c@1. This measure encourages systems to reduce the number of incorrect answers while maintaining the number of correct ones by leaving some questions unanswered. 12 groups participated in the task, submitting 62 runs in 3 different languages (German, English, and Romanian). All runs were monolingual; no team attempted a cross-language task. We report here the conclusions and lessons learned after the first campaign in 2011.
Cross-lingual information retrieval (CLIR) involving the Chinese language has been thoroughly studied in the general language domain, but rarely in the biomedical domain, due to the lack of suitable linguistic resources and parsing tools. In this paper, we describe a Chinese-English CLIR system for biomedical literature, which exploits a bilingual ontology, the ``eCMeSH Tree''''''''. This is an extension of the Chinese Medical Subject Headings (CMeSH) Tree, based on Medical Subject Headings (MeSH). Using the 2006 and 2007 TREC Genomics track data, we have evaluated the performance of the eCMeSH Tree in expanding queries. We have compared our results to those obtained using two other approaches, i.e. pseudo-relevance feedback (PRF) and document translation (DT). Subsequently, we evaluate the performance of different combinations of these three retrieval methods. Our results show that our method of expanding queries using the eCMeSH Tree can outperform the PRF method. Furthermore, combining this method with PRF and DT helps to smooth the differences in query expansion, and consequently results in the best performance amongst all experiments reported. All experiments compare the use of two different retrieval models, i.e. Okapi BM25 and a query likelihood language model. In general, the former performs slightly better.
WordNet Domains (WND) is a lexical resource where synsets have been semi-automatically annotated with one or more domain labels from a set of 165 hierarchically organized domains. The uses of WND include the power to reduce the polysemy degree of the words, grouping those senses that belong to the same domain. But the semi-automatic method used to develop this resource was far from being perfect. By cross-checking the content of the Multilingual Central Repository (MCR) it is possible to find some errors and inconsistencies. Many are very subtle. Others, however, leave no doubt. Moreover, it is very difficult to quantify the number of errors in the original version of WND. This paper presents a novel semi-automatic method to propagate domain information through the MCR. We also compare both labellings (the original and the new one) allowing us to detect anomalies in the original WND labels.
We present a web-based tool for retrieving and annotating audio fragments of e.g. interviews. Our collection contains 250 interviews with veterans of Dutch conflicts and military missions. The audio files of the interviews were disclosed using ASR technology focussed at keyword retrieval. Resulting transcripts were stored in a MySQL database together with metadata, summary texts, and keywords, and carefully indexed. Retrieved fragments can be made audible and annotated. Annotations can be kept personal or be shared with other users. The tool and formats comply with CLARIN standards. A demo version of the tool is available at http://wwwlands2.let.kun.nl/spex/annotationtooldemo.
We consider a non-intrusive computer-vision method for measuring the motion of a person performing natural signing in video recordings. The quality and usefulness of the method is compared to a traditional marker-based motion capture set-up. The accuracy of descriptors extracted from video footage is assessed qualitatively in the context of sign language analysis by examining if the shape of the curves produced by the different means resemble one another in sequences where the shape could be a source of valuable linguistic information. Then, quantitative comparison is performed first by correlating the computer-vision-based descriptors with the variables gathered with the motion capture equipment. Finally, multivariate linear and non-linar regression methods are applied for predicting the motion capture variables based on combinations of computer vision descriptors. The results show that even the simple computer vision method evaluated in this paper can produce promisingly good results for assisting researchers working on sign language analysis.
This article describes the development of a bidirectional shallow-transfer based machine translation system for Spanish and Aragonese, based on the Apertium platform, reusing the resources provided by other translators built for the platform. The system, and the morphological analyser built for it, are both the first resources of their kind for Aragonese. The morphological analyser has coverage of over 80{\textbackslash}{\%}, and is being reused to create a spelling checker for Aragonese. The translator is bidirectional: the Word Error Rate for Spanish to Aragonese is 16.83{\%}, while Aragonese to Spanish is 11.61{\%}.
The Leipzig Corpora Collection offers free online access to 136 monolingual dictionaries enriched with statistical information. In this paper we describe current advances of the project in collecting and processing text data automatically for a large number of languages. Our main interest lies in languages of low density, where only few text data exists online. The aim of this approach is to create monolingual dictionaries and statistical information for a high number of new languages and to expand the existing dictionaries, opening up new possibilities for linguistic typology and other research. Focus of this paper will be set on the infrastructure for the automatic acquisition of large amounts of monolingual text in many languages from various sources. Preliminary results of the collection of text data will be presented. The mainly language-independent framework for preprocessing, cleaning and creating the corpora and computing the necessary statistics will also be depicted.
In this paper, we present the newly established Danish speech corpus PiTu. The corpus consists of recordings of 28 native Danish talkers (14 female and 14 male) each reproducing (i) a series of nonsense syllables, and (ii) a set of authentic natural language sentences. The speech corpus is tailored for investigating the relationship between early stages of the speech perceptual process and later stages. We present our considerations involved in preparing the experimental set-up, producing the anechoic recordings, compiling the data, and exploring the materials in linguistic research. We report on a small pilot experiment demonstrating how PiTu and similar speech corpora can be used in studies of prosody as a function of semantic content. The experiment addresses the issue of whether the governing principles of Danish prosody assignment is mainly talker-specific or mainly content-typical (under the specific experimental conditions). The corpus is available in its entirety for download at http://amtoolbox.sourceforge.net/pitu/.
In this paper, we present the acquisition and labeling processes of the EDECAN-SPORTS corpus, which is a corpus that is oriented to the development of multimodal dialog systems acquired in Spanish and Catalan. Two Wizards of Oz were used in order to better simulate the behavior of an actual system in terms of both the information used by the different modules and the communication mechanisms between these modules. User and system dialog-act labeling, as well as other information, have been obtained automatically using this acquisition method Some preliminary experimental results with the acquired corpus show the appropriateness of the proposed acquisition method for the development of dialog systems
Standardized corpora are the foundation for spoken language research. In this work, we introduce an annotated and standardized corpus in the Spoken Dialog Systems (SDS) domain. Data from the Let's Go Bus Information System from the Carnegie Mellon University in Pittsburgh has been formatted, parameterized and annotated with quality, emotion, and task success labels containing 347 dialogs with 9,083 system-user exchanges. A total of 46 parameters have been derived automatically and semi-automatically from Automatic Speech Recognition (ASR), Spoken Language Understanding (SLU) and Dialog Manager (DM) properties. To each spoken user utterance an emotion label from the set garbage, non-angry, slightly angry, very angry has been assigned. In addition, a manual annotation of Interaction Quality (IQ) on the exchange level has been performed with three raters achieving a Kappa value of 0.54. The IQ score expresses the quality of the interaction up to each system-user exchange on a score from 1-5. The presented corpus is intended as a standardized basis for classification and evaluation tasks regarding task success prediction, dialog quality estimation or emotion recognition to foster comparability between different approaches on these fields.
Among the readings available for NL sentences, those where two or more sets of entities are independent of one another are particularly challenging from both a theoretical and an empirical point of view. Those readings are termed here as Independent Set (IS) readings'. Standard examples of such readings are the well-known Collective and Cumulative Readings. (Robaldo, 2011) proposes a logical framework that can properly represent the meaning of IS readings in terms of a set-Skolemization of the witness sets. One of the main assumptions of Robaldo's logical framework, drawn from (Schwarzschild, 1996), is that pragmatics plays a crucial role in the identification of such witness sets. Those are firstly identified on pragmatic grounds, then logical clauses are asserted on them in order to trigger the appropriate inferences. In this paper, we present the results of an experimental analysis that appears to confirm Robaldo's hypotheses concerning the pragmatic identification of the witness sets.
Vector space models benefit from using an outside corpus to train the model. It is, however, unclear what constitutes a good training corpus. We have investigated the effect on summary quality when using various language resources to train a vector space based extraction summarizer. This is done by evaluating the performance of the summarizer utilizing vector spaces built from corpora from different genres, partitioned from the Swedish SUC-corpus. The corpora are also characterized using a variety of lexical measures commonly used in readability studies. The performance of the summarizer is measured by comparing automatically produced summaries to human created gold standard summaries using the ROUGE F-score. Our results show that the genre of the training corpus does not have a significant effect on summary quality. However, evaluating the variance in the F-score between the genres based on lexical measures as independent variables in a linear regression model, shows that vector spaces created from texts with high syntactic complexity, high word variation, short sentences and few long words produce better summaries.
We present a complex, open source tool for detailed machine translation error analysis providing the user with automatic error detection and classification, several monolingual alignment algorithms as well as with training and test corpus browsing. The tool is the result of a merge of automatic error detection and classification of Hjerson (Popovi{\'c}, 2011) and Addicter (Zeman et al., 2011) into the pipeline and web visualization of Addicter. It classifies errors into categories similar to those of Vilar et al. (2006), such as: morphological, reordering, missing words, extra words and lexical errors. The graphical user interface shows alignments in both training corpus and test data; the different classes of errors are colored. Also, the summary of errors can be displayed to provide an overall view of the MT system's weaknesses. The tool was developed in Linux, but it was tested on Windows too.
We present an annotated resource consisting of open-domain translation requests, automatic translations and user-provided corrections collected from casual users of the translation portal http://reverso.net. The layers of annotation provide: 1) quality assessments for 830 correction suggestions for translations into English, at the segment level, and 2) 814 usefulness assessments for English-Spanish and English-French translation suggestions, a suggestion being useful if it contains at least local clues that can be used to improve translation quality. We also discuss the results of our preliminary experiments concerning 1) the development of an automatic filter to separate useful from non-useful feedback, and 2) the incorporation in the machine translation pipeline of bilingual phrases extracted from the suggestions. The annotated data, available for download from ftp://mi.eng.cam.ac.uk/data/faust/LW-UPC-Oct11-FAUST-feedback-annotation.tgz, is released under a Creative Commons license. To our best knowledge, this is the first resource of this kind that has ever been made publicly available.
Persian with its about 100,000,000 speakers in the world belongs to the group of languages with less developed linguistically annotated resources and tools. The few existing resources and tools are neither open source nor freely available. Thus, our goal is to develop open source resources such as corpora and treebanks, and tools for data-driven linguistic analysis of Persian. We do this by exploring the reusability of existing resources and adapting state-of-the-art methods for the linguistic annotation. We present fully functional tools for text normalization, sentence segmentation, tokenization, part-of-speech tagging, and parsing. As for resources, we describe the Uppsala PErsian Corpus (UPEC) which is a modified version of the Bijankhan corpus with additional sentence segmentation and consistent tokenization modified for more appropriate syntactic annotation. The corpus consists of 2,782,109 tokens and is annotated with parts of speech and morphological features. A treebank is derived from UPEC with an annotation scheme based on Stanford Typed Dependencies and is planned to consist of 10,000 sentences of which 215 have already been annotated. Keywords: BLARK for Persian, PoS tagged corpus, Persian treebank
We propose a pre-processing stage for Statistical Machine Translation (SMT) systems where the words of the source sentence are re-ordered as per the syntax of the target language prior to the alignment process, so that the alignment found by the statistical system is improved. We take a dependency parse of the source sentence and linearize it as per the syntax of the target language, before it is used in either the training or the decoding phase. During this linearization, the ordering decisions among dependency nodes having a common parent are done based on two aspects: parent-child positioning and relation priority. To make the linearization process rule-driven, we assume that the relative word order of a dependency relation's relata does not depend either on the semantic properties of the relata or on the rest of the expression. We also assume that the relative word order of various relations sharing a relata does not depend on the rest of the expression. We experiment with a publicly available English-Hindi parallel corpus and show that our scheme improves the BLEU score.
This paper describes the derivation of distributional semantic representations for open class words relative to a concept inventory, and of concepts relative to open class words through grammatical relations extracted from Wikipedia articles. The concept inventory comes from WikiNet, a large-scale concept network derived from Wikipedia. The distinctive feature of these representations are their relation to a concept network, through which we can compute selectional preferences of open-class words relative to general concepts. The resource thus derived provides a meaning representation that complements the relational representation captured in the concept network. It covers English open-class words, but the concept base is language independent. The resource can be extended to other languages, with the use of language specific dependency parsers. Good results in metonymy resolution show the resource's potential use for NLP applications.
In this paper, we address the problem of extracting technical terms automatically from an unannotated corpus. We introduce a technology term tagger that is based on Liblinear Support Vector Machines and employs linguistic features including Part of Speech tags and Dependency Structures, in addition to user feedback to perform the task of identification of technology related terms. Our experiments show the applicability of our approach as witnessed by acceptable results on precision and recall.
Within the framework of the Quaero project, we proposed a new definition of named entities, based upon an extension of the coverage of named entities as well as the structure of those named entities. In this new definition, the extended named entities we proposed are both hierarchical and compositional. In this paper, we focused on the annotation of a corpus composed of press archives, OCRed from French newspapers of December 1890. We present the methodology we used to produce the corpus and the characteristics of the corpus in terms of named entities annotation. This annotated corpus has been used in an evaluation campaign. We present this evaluation, the metrics we used and the results obtained by the participants.
We present the ongoing development of MCG, a linguistically deep and precise grammar for Mandarin Chinese together with its accompanying treebank, both based on the linguistic framework of HPSG, and using MRS as the semantic representation. We highlight some key features of our grammar design, and review a number of challenging phenomena, with comparisons to alternative linguistic treatments and implementations. One of the distinguishing characteristics of our approach is the tight integration of grammar and treebank development. The two-step treebank annotation procedure benefits from the efficiency of the discriminant-based annotation approach, while giving the annotators full freedom of producing extra-grammatical structures. This not only allows the creation of a precise and full-coverage treebank with an imperfect grammar, but also provides prompt feedback for grammarians to identify the errors in the grammar design and implementation. Preliminary evaluation and error analysis shows that the grammar already covers most of the core phenomena for Mandarin Chinese, and the treebank annotation procedure reaches a stable speed of 35 sentences per hour with satisfying quality.
In the paper we present a long-term on-going project of a lexicon-grammar of Polish. It is based on our former research focusing mainly on morphological dictionaries, text understanding and related tools. By Lexicon Grammars we mean grammatical formalisms which are based on the idea that sentence is the fundamental unit of meaning and that grammatical information should be closely related to words. Organization of the grammatical knowledge into a lexicon results in a powerful NLP tool, particularly well suited to support heuristic parsing. The project is inspired by the achievements of Maurice Gross, Kazimierz Polanski and George Miller. We present the actual state of the project of a wordnet-like lexical network PolNet with particular emphasis on its verbal component, now being converted into the kernel of a lexicon grammar for Polish. We present various aspects of PolNet development and validation within the POLINT-112-SMS project. The reader is precisely informed on the current stage of the project.
This paper describes the process of constructing a trilingual parallel treebank. While for two of the involved languages, Spanish and German, there are already corpora with well-established annotation schemes available, this is not the case with the third language: Cuzco Quechua (ISO 639-3:quz), a low-resourced, non-standardized language for which we had to define a linguistically plausible annotation scheme first.
In this paper we investigated differences in language use of speakers yielding different verbal intelligence when they describe the same event. The work is based on a corpus containing descriptions of a short film and verbal intelligence scores of the speakers. For analyzing the monologues and the film transcript, the number of reused words, lemmas, n-grams, cosine similarity and other features were calculated and compared to each other for different verbal intelligence groups. The results showed that the similarity of monologues of higher verbal intelligence speakers was greater than of lower and average verbal intelligence participants. A possible explanation of this phenomenon is that candidates yielding higher verbal intelligence have a good short-term memory. In this paper we also checked a hypothesis that differences in vocabulary of speakers yielding different verbal intelligence are sufficient enough for good classification results. For proving this hypothesis, the Nearest Neighbor classifier was trained using TF-IDF vocabulary measures. The maximum achieved accuracy was 92.86{\%}.
In this work we investigated whether there is a relationship between dominant behaviour of dialogue participants and their verbal intelligence. The analysis is based on a corpus containing 56 dialogues and verbal intelligence scores of the test persons. All the dialogues were divided into three groups: H-H is a group of dialogues between higher verbal intelligence participants, L-L is a group of dialogues between lower verbal intelligence participant and L-H is a group of all the other dialogues. The dominance scores of the dialogue partners from each group were analysed. The analysis showed that differences between dominance scores and verbal intelligence coefficients for L-L were positively correlated. Verbal intelligence scores of the test persons were compared to other features that may reflect dominant behaviour. The analysis showed that number of interruptions, long utterances, times grabbed the floor, influence diffusion model, number of agreements and several acoustic features may be related to verbal intelligence. These features were used for the automatic classification of the dialogue partners into two groups (lower and higher verbal intelligence participants); the achieved accuracy was 89.36{\%}.
A syntactically complex text may represent a problem for both comprehension by humans and various NLP tasks. A large number of studies in text simplification are concerned with this problem and their aim is to transform the given text into a simplified form in order to make it accessible to the wider audience. In this study, we were investigating what the natural tendency of texts is in 20th century English language. Are they becoming syntactically more complex over the years, requiring a higher literacy level and greater effort from the readers, or are they becoming simpler and easier to read? We examined several factors of text complexity (average sentence length, Automated Readability Index, sentence complexity and passive voice) in the 20th century for two main English language varieties - British and American, using the `Brown family' of corpora. In British English, we compared the complexity of texts published in 1931, 1961 and 1991, while in American English we compared the complexity of texts published in 1961 and 1992. Furthermore, we demonstrated how the state-of-the-art NLP tools can be used for automatic extraction of some complex features from the raw text version of the corpora.
The task of Statistical Machine Translation depends on large amounts of training corpora. Despite the availability of several parallel corpora, these are typically composed of declarative sentences, which may not be appropriate when the goal is to translate other types of sentences, e.g., interrogatives. There have been efforts to create corpora of questions, specially in the context of the evaluation of Question-Answering systems. One of those corpora is the UIUC dataset, composed of nearly 6,000 questions, widely used in the task of Question Classification. In this work, we make available the Portuguese version of the UIUC dataset, which we manually translated, as well as the translation guidelines. We show the impact of this corpus in the performance of a state-of-the-art SMT system when translating questions. Finally, we present a taxonomy of translation errors, according to which we analyze the output of the automatic translation before and after using the corpus as training data.
We present the speech corpus SMALLWorlds (Spoken Multi-lingual Accounts of Logically Limited Worlds), newly established and still growing. SMALLWorlds contains monologic descriptions of scenes or worlds which are simple enough to be formally describable. The descriptions are instances of content-controlled monologue: semantically ''''''``pre-specified'''''''' but still bearing most hallmarks of spontaneous speech (hesitations and filled pauses, relaxed syntax, repetitions, self-corrections, incomplete constituents, irrelevant or redundant information, etc.) as well as idiosyncratic speaker traits. In the paper, we discuss the pros and cons of data so elicited. Following that, we present a typical SMALLWorlds task: the description of a simple drawing with differently coloured circles, squares, and triangles, with no hints given as to which description strategy or language style to use. We conclude with an example on how SMALLWorlds may be used: unsupervised lexical learning from phonetic transcription. At the time of writing, SMALLWorlds consists of more than 250 recordings in a wide range of typologically diverse languages from many parts of the world, some unwritten and endangered.
We are presenting VPS-30-En, a small lexical resource that contains the following 30 English verbs: access, ally, arrive, breathe, claim, cool, crush, cry, deny, enlarge, enlist, forge, furnish, hail, halt, part, plough, plug, pour, say, smash, smell, steer, submit, swell, tell, throw, trouble, wake and yield. We have created and have been using VPS-30-En to explore the interannotator agreement potential of the Corpus Pattern Analysis. VPS-30-En is a small snapshot of the Pattern Dictionary of English Verbs (Hanks and Pustejovsky, 2005), which we revised (both the entries and the annotated concordances) and enhanced with additional annotations. It is freely available at http://ufal.mff.cuni.cz/spr. In this paper, we compare the annotation scheme of VPS-30-En with the original PDEV. We also describe the adjustments we have made and their motivation, as well as the most pervasive causes of interannotator disagreements.
This paper describes work on a rule-based, open-source parser for Swedish. The central component is a wide-coverage grammar implemented in the GF formalism (Grammatical Framework), a dependently typed grammar formalism based on Martin-L{\"o
We propose an annotation framework to explicitly identify dropped subject pronouns in Chinese. We acknowledge and specify 10 concrete pronouns that exist as words in Chinese and 4 abstract pronouns that do not correspond to Chinese words, but that are recognized conceptually, to native Chinese speakers. These abstract pronouns are identified as ''''''``unspecified'''''''', ''''''``pleonastic'''''''', ''''''``event'''''''', and ''''''``existential'''''''' and are argued to exist cross-linguistically. We trained two annotators, fluent in Chinese, and adjudicated their annotations to form a gold standard. We achieved an inter-annotator agreement kappa of .6 and an observed agreement of .7. We found that annotators had the most difficulty with the abstract pronouns, such as ''''''``unspecified'''''''' and ''''''``event'''''''', but we posit that further specification and training has the potential to significantly improve these results. We believe that this annotated data will serve to help improve Machine Translation models that translate from Chinese to a non pro-drop language, like English, that requires all subject pronouns to be explicit.
Natural languages possess a wealth of indefinite forms that typically differ in distribution and interpretation. Although formal semanticists have strived to develop precise meaning representations for different indefinite functions, to date there has hardly been any corpus work on the topic. In this paper, we present the results of a small corpus study where English indefinite forms `any' and `some' were labelled with fine-grained semantic functions well-motivated by typological studies. We developed annotation guidelines that could be used by non-expert annotators and calculated inter-annotator agreement amongst several coders. The results show that the annotation task is hard, with agreement scores ranging from 52{\%} to 62{\%} depending on the number of functions considered, but also that each of the independent annotations is in accordance with theoretical predictions regarding the possible distributions of indefinite functions. The resulting annotated corpus is available upon request and can be accessed through a searchable online database.
This paper describes a method to mine Hindi-English transliteration pairs from online Hindi song lyrics. The technique is based on the observations that lyrics are transliterated word-by-word, maintaining the precise word order. The mining task is nevertheless challenging because the Hindi lyrics and its transliterations are usually available from different, often unrelated, websites. Therefore, it is a non-trivial task to match the Hindi lyrics to their transliterated counterparts. Moreover, there are various types of noise in lyrics data that needs to be appropriately handled before songs can be aligned at word level. The mined data of 30823 unique Hindi-English transliteration pairs with an accuracy of more than 92{\%} is available publicly. Although the present work reports mining of Hindi-English word pairs, the same technique can be easily adapted for other languages for which song lyrics are available online in native and Roman scripts.
Terminological databases do not always provide detailed information on the linguistic behaviour of terms, although this is important for potential users such as translators or students. In this paper we describe a project that aims to fill this gap by proposing a method for annotating terms in sentences based on that developed within the FrameNet project (Ruppenhofer et al. 2010) and by implementing it in an online resource called DiCoInfo. We focus on the methodology we devised, and show with some preliminary results how similar actantial (i.e. argumental) structures can provide evidence for defining lexical relations in specific languages and capturing cross-linguistic equivalents. The paper argues that the syntactico-semantic annotation of the contexts in which the terms occur allows lexicographers to validate their intuitions concerning the linguistic behaviour of terms as well as interlinguistic relations between them. The syntactico-semantic annotation of contexts could, therefore, be considered a good starting point in terminology work that aims to describe the linguistic functioning of terms and offer a sounder basis to define interlinguistic relationships between terms that belong to different languages.
We present a corpus consisting of 11,292 real-world English to Spanish automatic translations annotated with relative (ranking) and absolute (adequate/non-adequate) quality assessments. The translation requests, collected through the popular translation portal http://reverso.net, provide a most variated sample of real-world machine translation (MT) usage, from complete sentences to units of one or two words, from well-formed to hardly intelligible texts, from technical documents to colloquial and slang snippets. In this paper, we present 1) a preliminary annotation experiment that we carried out to select the most appropriate quality criterion to be used for these data, 2) a graph-based methodology inspired by Interactive Genetic Algorithms to reduce the annotation effort, and 3) the outcomes of the full-scale annotation experiment, which result in a valuable and original resource for the analysis and characterization of MT-output quality.
We present an approach to annotating timelines in stories where events are linked together by temporal relations into a temporal dependency tree. This approach avoids the disconnected timeline problems of prior work, and results in timelines that are more suitable for temporal reasoning. We show that annotating timelines as temporal dependency trees is possible with high levels of inter-annotator agreement - Krippendorff's Alpha of 0.822 on selecting event pairs, and of 0.700 on selecting temporal relation labels - even with the moderately sized relation set of BEFORE, AFTER, INCLUDES, IS-INCLUDED, IDENTITY and OVERLAP. We also compare several annotation schemes for identifying story events, and show that higher inter-annotator agreement can be reached by focusing on only the events that are essential to forming the timeline, skipping words in negated contexts, modal contexts and quoted speech.
This paper focuses on the representation and querying of knowledge-based multimodal data. This work stands in the OTIM project which aims at processing multimodal annotation of a large conversational French speech corpus. Within OTIM, we aim at providing linguists with a unique framework to encode and manipulate numerous linguistic domains (from prosody to gesture). Linguists commonly use Typed Feature Structures (TFS) to provide an uniform view of multimodal annotations but such a representation cannot be used within an applicative framework. Moreover TFS expressibility is limited to hierarchical and constituency relations and does not suit to any linguistic domain that needs for example to represent temporal relations. To overcome these limits, we propose an ontological approach based on Description logics (DL) for the description of linguistic knowledge and we provide an applicative framework based on OWL DL (Ontology Web Language) and the query language SPARQL.
With the CINTIL-International Corpus of Portuguese, an ongoing corpus annotated with fully flegded grammatical representation, sentences get not only a high level of lexical, morphological and syntactic annotation but also a semantic analysis that prepares the data to a manual specification step and thus opens the way for a number of tools and resources for which there is a great research focus at the present. This paper reports on the construction of a propbank that builds on CINTIL-DeepGramBank, with nearly 10 thousand sentences, on the basis of a deep linguistic grammar and on the process and the linguistic criteria guiding that construction, which makes possible to obtain a complete PropBank with both syntactic and semantic levels of linguistic annotation. Taking into account this and the promising scores presented in this study for inter-annotator agreement, CINTIL-PropBank presents itself as a great resource to train a semantic role labeller, one of our goals with this project.
This paper presents deepKnowNet, a new fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources. Basically, the method applies a knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate WordNet sense to large sets of topically related words acquired from the web, named TSWEB. This Word Sense Disambiguation algorithm is the personalized PageRank algorithm implemented in UKB. This new method improves by automatic means the current content of WordNet by creating large volumes of new and accurate semantic relations between synsets. KnowNet was our first attempt towards the acquisition of large volumes of semantic relations. However, KnowNet had some limitations that have been overcomed with deepKnowNet. deepKnowNet disambiguates the first hundred words of all Topic Signatures from the web (TSWEB). In this case, the method highlights the most relevant word senses of each Topic Signature and filter out the ones that are not so related to the topic. In fact, the knowledge it contains outperforms any other resource when is empirically evaluated in a common framework based on a similarity task annotated with human judgements.
This paper outlines the main features of Bibli{\v{s}}a, a tool that offers various possibilities of enhancing queries submitted to large collections of TMX documents generated from aligned parallel articles residing in multilingual digital libraries of e-journals. The queries initiated by a simple or multiword keyword, in Serbian or English, can be expanded by Bibli{\v{s}}a, both semantically and morphologically, using different supporting monolingual and multilingual resources, such as wordnets and electronic dictionaries. The tool operates within a complex system composed of several modules including a web application, which makes it readily accessible on the web. Its functionality has been tested on a collection of 44 TMX documents generated from articles published bilingually by the journal INFOtecha, yielding encouraging results. Further enhancements of the tool are underway, with the aim of transforming it from a powerful full-text and metadata search tool, to a useful translator's aid, which could be of assistance both in reviewing terminology used in context and in refining the multilingual resources used within the system.
The WordNet knowledge model is currently implemented in multiple software frameworks providing procedural access to language instances of it. Frameworks tend to be focused on structural/design aspects of the model thus describing low level interfaces for linguistic knowledge retrieval. Typically the only high level feature directly accessible is word lookup while traversal of semantic relations leads to verbose/complex combinations of data structures, pointers and indexes which are irrelevant in an NLP context. Here is described an extension to the JWNL framework that hides technical requirements of access to WordNet features with an essentially word/sense based API applying terminology from the official online interface. This high level API is applied to the original English version of WordNet and to an SQL based Portuguese lexicon, translated into a WordNet based representation usable by JWNL.
In criminal proceedings, sometimes it is not easy to evaluate the sincerity of oral testimonies. DECOUR - DEception in COURt corpus - has been built with the aim of training models suitable to discriminate, from a stylometric point of view, between sincere and deceptive statements. DECOUR is a collection of hearings held in four Italian Courts, in which the speakers lie in front of the judge. These hearings become the object of a specific criminal proceeding for calumny or false testimony, in which the deceptiveness of the statements of the defendant is ascertained. Thanks to the final Court judgment, that points out which lies are told, each utterance of the corpus has been annotated as true, uncertain or false, according to its degree of truthfulness. Since the judgment of deceptiveness follows a judicial inquiry, the annotation has been realized with a greater degree of confidence than ever before. Moreover, in Italy this is the first corpus of deceptive texts not relying on mock' lies created in laboratory conditions, but which has been collected in a natural environment.
Language resources are essential for linguistic research and the development of NLP applications. Low-density languages, such as Irish, therefore lack significant research in this area. This paper describes the early stages in the development of new language resources for Irish ― namely the first Irish dependency treebank and the first Irish statistical dependency parser. We present the methodology behind building our new treebank and the steps we take to leverage upon the few existing resources. We discuss language-specific choices made when defining our dependency labelling scheme, and describe interesting Irish language characteristics such as prepositional attachment, copula, and clefting. We manually develop a small treebank of 300 sentences based on an existing POS-tagged corpus and report an inter-annotator agreement of 0.7902. We train MaltParser to achieve preliminary parsing results for Irish and describe a bootstrapping approach for further stages of development.
Enterprise content analysis and platform configuration for enterprise content management is often carried out by external consultants that are not necessarily domain experts. In this paper, we propose a set of methods for automatic content analysis that allow users to gain a high level view of the enterprise content. Here, a main concern is the automatic identification of key stakeholders that should ideally be involved in analysis interviews. The proposed approach employs recent advances in term extraction, semantic term grounding, expert profiling and expert finding in an enterprise content management setting. Extracted terms are evaluated using human judges, while term grounding is evaluated using a manually created gold standard for the DBpedia datasource.
Word alignment is an important step for machine translation systems. Although the alignment performance between grammatically similar languages is reported to be very high in many studies, the case is not the same for language pairs from different language families. In this study, we are focusing on English-Turkish language pairs. Turkish is a highly agglutinative language with a very productive and rich morphology whereas English has a very poor morphology when compared to this language. As a result of this, one Turkish word is usually aligned with several English words. The traditional models which use word-level alignment approaches generally fail in such circumstances. In this study, we evaluate a Giza++ system by splitting the words into their morphological units (stem and suffixes) and compare the model with the traditional one. For the first time, we evaluate the performance of our aligner on gold standard parallel sentences rather than in a real machine translation system. Our approach reduced the alignment error rate by 40{\%} relative. Finally, a new test corpus of 300 manually aligned sentences is released together with this study.
Set covering algorithms are efficient tools for solving an optimal linguistic corpus reduction. The optimality of such a process is directly related to the descriptive features of the sentences of a reference corpus. This article suggests to verify experimentally the behaviour of three algorithms, a greedy approach and a lagrangian relaxation based one giving importance to rare events and a third one considering the Kullback-Liebler divergence between a reference and the ongoing distribution of events. The analysis of the content of the reduced corpora shows that the both first approaches stay the most effective to compress a corpus while guaranteeing a minimal content. The variant which minimises the Kullback-Liebler divergence guarantees a distribution of events close to a reference distribution as expected; however, the price for this solution is a much more important corpus. In the proposed experiments, we have also evaluated a mixed-approach considering a random complement to the smallest coverings.
Extracting parallel data from comparable corpora in order to enrich existing statistical translation models is an avenue that attracted a lot of research in recent years. There are experiments that convincingly show how parallel data extracted from comparable corpora is able to improve statistical machine translation. Yet, the existing body of research on parallel sentence mining from comparable corpora does not take into account the degree of comparability of the corpus being processed or the computation time it takes to extract parallel sentences from a corpus of a given size. We will show that the performance of a parallel sentence extractor crucially depends on the degree of comparability such that it is more difficult to process a weakly comparable corpus than a strongly comparable corpus. In this paper we describe PEXACC, a distributed (running on multiple CPUs), trainable parallel sentence/phrase extractor from comparable corpora. PEXACC is freely available for download with the ACCURAT Toolkit, a collection of MT-related tools developed in the ACCURAT project.
There are several methods offered for spelling correction in Farsi (Persian) Language. Unfortunately no powerful framework has been implemented because of lack of a large training set in Farsi as an accurate model. A training set consisting of erroneous and related correction string pairs have been obtained from a large number of instances of the books each of which were typed two times in Computer Research Center of Islamic Sciences. We trained our error model using this huge set. In testing part after finding erroneous words in sample text, our program proposes some candidates for related correction. The paper focuses on describing the method of ranking related corrections. This method is customized version of Noisy Channel Spelling Correction for Farsi. This ranking method attempts to find intended correction c from a typo t, that maximizes P(c) P(t | c). In this paper different methods are described and analyzed to obtain a wide overview of the field. Our evaluation results show that Noisy Channel Model using our corpus and training set in this framework works more accurately and improves efficiently in comparison with other methods.
Contributive resources, such as wikipedia, have proved to be valuable in Natural Language Processing or Multilingual Information Retrieval applications.This article focusses on Wiktionary, the dictionary part of the collaborative resources sponsored by the Wikimedia foundation. In this article we present a word net that has been extracted from French, English and German wiktionaries. We present the structure of this word net and discuss the specific extraction problems induced by this kind of contributive resources and the method used to overcome them. Then we show how we represent the extracted data as a Lexical Markup Framework (LMF) compatible lexical network represented in Resource Description Framework (RDF) format.
This paper describes the creation of a dysarthric speech database which has been done as part of a national program to help the disabled lead a better life ― a challenging endeavour that is targeting development of speech technologies for people with articulation disabilities. The additional aims of this database are to study the phonetic characteristics of the different types of the disabled persons, develop the automatic method to assess degrees of disability, and investigate the phonetic features of dysarthric speech. For these purposes, a large database of about 600 mildly or moderately severe dysarthric persons is planned for a total of 4 years (2010.06. 01 {\textasciitilde} 2014.05.31). At present a dysarthric speech database of 120 speakers has been collected and we are continuing to record new speakers with cerebral paralysis of mild and moderate severity. This paper also introduces the prompting items, the assessment of the speech disability severity of the speakers, and other considerations for the creation of a dysarthric speech.
Cross-lingual topic detection within text is a feasible solution to resolving the language barrier in accessing the information. This paper presents a Chinese-English cross-lingual topic corpus (CLTC), in which 90,000 Chinese articles and 90,000 English articles are organized within 150 topics. Compared with TDT corpora, CLTC has three advantages. First, CLTC is bigger in size. This makes it possible to evaluate the large-scale cross-lingual text clustering methods. Second, articles are evenly distributed within the topics. Thus it can be used to produce test datasets for different purposes. Third, CLTC can be used as a cross-lingual comparable corpus to develop methods for cross-lingual information access. A preliminary evaluation with CLTC corpus indicates that the corpus is effective in evaluating cross-lingual topic detection methods.
Lexica of predicate-argument structures constitute a useful tool for several tasks in NLP. This paper describes a web-service system for automatic acquisition of verb subcategorization frames (SCFs) from parsed data in Italian. The system acquires SCFs in an unsupervised manner. We created two gold standards for the evaluation of the system, the first by mixing together information from two lexica (one manually created and the second automatically acquired) and manual exploration of corpus data and the other annotating data extracted from a specialized corpus (environmental domain). Data filtering is accomplished by means of the maximum likelihood estimate (MLE). The evaluation phase has allowed us to identify the best empirical MLE threshold for the creation of a lexicon (P=0.653, R=0.557, F1=0.601). In addition to this, we assigned to the extracted entries of the lexicon a confidence score based on the relative frequency and evaluated the extractor on domain specific data. The confidence score will allow the final user to easily select the entries of the lexicon in terms of their reliability: one of the most interesting feature of this work is the possibility the final users have to customize the results of the SCF extractor, obtaining different SCF lexica in terms of size and accuracy.
In this study we investigate the idea to automatically evaluate newly created pronunciation encodings for being correct or containing a potential error. Using a cascaded triphone detector and phonotactical n-gram modeling with an optimal Bayesian threshold we classify unknown pronunciation transcripts into the classes 'probably faulty' or 'probably correct'. Transcripts tagged 'probably faulty' are forwarded to a manual inspection performed by an expert, while encodings tagged 'probably correct' are passed without further inspection. An evaluation of the new method on the German PHONOLEX lexical resource shows that with a tolerable error margin of approximately 3{\%} faulty transcriptions a major reduction in work effort during the production of a new lexical resource can be achieved.
Corpus linguistic and language technological research needs empirical corpus data with nearly correct annotation and high volume to enable advances in language modelling and theorising. Recent work on improving corpus annotation accuracy presents semiautomatic methods to correct some of the analysis errors in available annotated corpora, while leaving the remaining errors undetected in the annotated corpus. We review recent advances in linguistics-based partial tagging and parsing, and regard the achieved analysis performance as sufficient for reconsidering a previously proposed method: combining nearly correct but partial automatic analysis with a minimal amount of human postediting (disambiguation) to achieve nearly correct corpus annotation accuracy at a competitive annotation speed. We report a pilot experiment with morphological (part-of-speech) annotation using a partial linguistic tagger of a kind previously reported with a very attractive precision-recall ratio, and observe that a desired level of annotation accuracy can be reached by using human disambiguation for less than 10{\textbackslash}{\%} of the words in the corpus.
This article presents the methodology and results of the analysis of terms referring to processes expressed by verbs or nouns in a corpus of specialized texts dealing with ceramics. Both noun and verb terms are explored in context in order to identify and represent the semantic roles held by their participants (arguments and circumstants), and therefore explore some of the relations established by these terms. We present a methodology for the identification of related terms that take part in the development of specialized processes and the annotation of the semantic roles expressed in these contexts. The analysis has allowed us to identify participants in the process, some of which were already present in our previous work, but also some new ones. This method is useful in the distinction of different meanings of the same verb. Contexts in which processes are expressed by verbs have proved to be very informative, even if they are less frequent in the corpus. This work is viewed as a first step in the implementation ― in ontologies ― of conceptual relations in which activities are involved.
Relations among phenomena at different linguistic levels are at the essence of language properties but today we focus mostly on one specific linguistic layer at a time, without (having the possibility of) paying attention to the relations among the different layers. At the same time our efforts are too much scattered without much possibility of exploiting other people's achievements. To address the complexities hidden in multilayer interrelations even small amounts of processed data can be useful, improving the performance of complex systems. Exploiting the current trend towards sharing we want to initiate a collective movement that works towards creating synergies and harmonisation among different annotation efforts that are now dispersed. In this paper we present the general architecture of the Language Library, an initiative which is conceived as a facility for gathering and making available through simple functionalities the linguistic knowledge the field is able to produce, putting in place new ways of collaboration within the LRT community. In order to reach this goal, a first population round of the Language Library has started around a core of parallel/comparable texts that have been annotated by several contributors submitting a paper for LREC2012. The Language Library has also an ancillary aim related to language documentation and archiving and it is conceived as a theory-neutral space which allows for several language processing philosophies to coexist.
This paper presents MISTRAL+, the upgraded version of an automatic tool created in 2004 named INTSMEL then MELISM. Since MELISM, the entire process has been modified in order to simplify and enhance the study of languages. MISTRAL+ is a combinaison of two modules: a Praat plugin MISTRAL{\_}Praat, and MISTRAL{\_}xls. For specific corpora, it performs phonological annotation based on the F0 variation in prominent words, but also in any chunk of speech, prominent or not. So this tool while being specialized can also be used as a generic one. Now among others, new functionalities allow to use API symbols while labeling, and to provide a semi-automatic melodic annotation in the frame of tonal languages. The program contains several functions which compute target points (or significant points) to model F0 contour, perform automatic annotation of different shapes and export all data in an xls file. In a first part of this paper, the MISTRAL+ functionalities will be described, and in a second part, an example of application will be presented about a study of the Mo Piu endangered language in the frame of the MICA Au Co Project.
We present a treebank and lexicon for German and English, which have been developed for PLTAG parsing. PLTAG is a psycholinguistically motivated, incremental version of tree-adjoining grammar (TAG). The resources are however also applicable to parsing with other variants of TAG. The German PLTAG resources are based on the TIGER corpus and, to the best of our knowledge, constitute the first scalable German TAG grammar. The English PLTAG resources go beyond existing resources in that they include the NP annotation by (Vadas and Curran, 2007), and include the prediction lexicon necessary for PLTAG.
This study describes the construction of a manually annotated speech corpus that focuses on the sound profiles of repair/disfluency in Mandarin conversational interaction. Specifically, the paper focuses on how the tag set of prosodic profiles of the recycling repair culled from both audio-tapped and video-tapped, face-to-face Mandarin interaction are decided. By the methodology of both acoustic records and impressionistic judgements, 260 instances of Mandarin recycling repair are annotated with sound profiles including: pitch, duration, loudness, silence, and other observable prosodic cues (i.e. sound stretch and cut-offs). The study further introduces some possible applications of the current corpus, such as the implementation of the annotated data for analyzing the correlation between sound profiles of Mandarin repair and the interactional function of the repair. The goal of constructing the corpus is to facilitate an interdisciplinary study that concentrates on broadening the interactional linguistic theory by simultaneously paying close attention to the sound profiles emerged from interaction.
The Australian National Corpus has been established in an effort to make currently scattered and relatively inaccessible data available to researchers through an online portal. In contrast to other national corpora, it is conceptualised as a linked collection of many existing and future language resources representing language use in Australia, unified through common technical standards. This approach allows us to bootstrap a significant collection and add value to existing resources by providing a unified, online tool-set to support research in a number of disciplines. This paper provides an outline of the technical platform being developed to support the corpus and a brief overview of some of the collections that form part of the initial version of the Australian National Corpus.
We adopt the corpus-informed approach to example sentence selections for the construction of a reference grammar. In the process, a database containing sentences that are carefully selected by linguistic experts including the full range of linguistic facts covered in an authoritative Chinese Reference Grammar is constructed and structured according to the reference grammar. A search engine system is developed to facilitate the process of finding the most typical examples the users need to study a linguistic problem or prove their hypotheses. The database can also be used as a training corpus by computational linguists to train models for Chinese word segmentation, POS tagging and sentence parsing.
ELAN is a versatile multimedia annotation tool that is being developed at the Max Planck Institute for Psycholinguistics. About a decade ago it emerged out of a number of corpus tools and utilities and it has been extended ever since. This paper focuses on the efforts made to ensure that the application keeps up with the growing needs of that era in linguistics and multimodality research; growing needs in terms of length and resolution of recordings, the number of recordings made and transcribed and the number of levels of annotation per transcription.
In this paper, a computational model based on concept polarity is proposed to investigate the influence of communications across the diacultural groups. The hypothesis of this work is that there are communities or groups which can be characterized by a network of concepts and the corresponding valuations of those concepts that are agreed upon by the members of the community. We apply an existing research tool, ECO, to generate text representative of each community and create community specific Valuation Concept Networks (VCN). We then compare VCNs across the communities, to attempt to find contentious concepts, which could subsequently be the focus of further exploration as points of contention between the two communities. A prototype, CPAM (Changing Positions, Altering Minds), was implemented as a proof of concept for this approach. The experiment was conducted using blog data from pro-Palestinian and pro-Israeli communities. A potential application of this method and future work are discussed as well.
The SignWriting improved fast transcriber (SWift), presented in this paper, is an advanced editor for computer-aided writing and transcribing of any Sign Language (SL) using the SignWriting (SW). The application is an editor which allows composing and saving desired signs using the SW elementary components, called glyphs. These make up a sort of alphabet, which does not depend on the national Sign Language and which codes the basic components of any sign. The user is guided through a fully automated procedure making the composition process fast and intuitive. SWift pursues the goal of helping to break down the electronic barriers that keep deaf people away from the web, and at the same time to support linguistic research about Sign Languages features. For this reason it has been designed with a special attention to deaf user needs, and to general usability issues. The editor has been developed in a modular way, so it can be integrated everywhere the use of the SW as an alternative to written verbal language may be advisable.
Typically, accuracy is used to represent the performance of an NLP system. However, accuracy attainment is a function of investment in annotation. Typically, the more the amount and sophistication of annotation, higher is the accuracy. However, a moot question is ''''''``is the accuracy improvement commensurate with the cost incurred in annotation''''''''? We present an economic model to assess the marginal benefit accruing from increase in cost of annotation. In particular, as a case in point we have chosen the sentiment analysis (SA) problem. In SA, documents normally are polarity classified by running them through classifiers trained on document vectors constructed from lexeme features, i.e., words. If, however, instead of words, one uses word senses (synset ids in wordnets) as features, the accuracy improves dramatically. But is this improvement significant enough to justify the cost of annotation? This question, to the best of our knowledge, has not been investigated with the seriousness it deserves. We perform a cost benefit study based on a vendor-machine model. By setting up a cost price, selling price and profit scenario, we show that although extra cost is incurred in sense annotation, the profit margin is high, justifying the cost.
Rembrandt is a named entity recognition system specially crafted to annotate documents by classifying named entities and ground them into unique identifiers. Rembrandt played an important role within our research over geographic IR, thus evolving into a more capable framework where documents can be annotated, manually curated and indexed. The goal of this paper is to present Rembrandt's simple but powerful annotation framework to the NLP community.
We have developed a new OSGi-based platform for Named Entity Recognition (NER) which uses a voting strategy to combine the results produced by several existing NER systems (currently OpenNLP, LingPipe and Stanford). The different NER systems have been systematically decomposed and modularized into the same pipeline of preprocessing components in order to support a flexible selection and ordering of the NER processing flow. This high modular and component-based design supports the possibility to setup different constellations of chained processing steps including alternative voting strategies for combining the results of parallel running components.
In October 2009, was launched the Quebec French part of the international sms4science project, called texto4science. Over a period of 10 months, we collected slightly more than 7000 SMSs that we carefully annotated. This database is now ready to be used by the community. The purpose of this article is to relate the efforts put into designing this database and provide some data analysis of the main linguistic phenomenon that we have annotated. We also report on a socio-linguistic survey we conducted within the project.
In this paper a collection of chats and tweets from the Netherlands and Flanders is described. The chats and tweets are part of the freely available SoNaR corpus, a 500 million word text corpus of the Dutch language. Recruitment, metadata, anonymisation and IPR issues are discussed. To illustrate the difference of language use between the various text types and other parameters (like gender and age) simple text analysis in the form of unigram frequency lists is carried out. Furthermore a website is presented with which users can retrieve their own frequency lists.
The paper concentrates on which language means may be included into the annotation of discourse relations in the Prague Dependency Treebank (PDT) and tries to examine the so called alternative lexicalizations of discourse markers (AltLex's) in Czech. The analysis proceeds from the annotated data of PDT and tries to draw a comparison between the Czech AltLex's from PDT and English AltLex's from PDTB (the Penn Discourse Treebank). The paper presents a lexico-syntactic and semantic characterization of the Czech AltLex's and comments on the current stage of their annotation in PDT. In the current version, PDT contains 306 expressions (within the total 43,955 of sentences) that were labeled by annotators as being an AltLex. However, as the analysis demonstrates, this number is not final. We suppose that it will increase after the further elaboration, as AltLex's are not restricted to a limited set of syntactic classes and some of them exhibit a great degree of variation.
Speakers' gender and age-group were predicted using the symbolic information of the X-JToBI prosodic labelling scheme as applied to the Core of the Corpus of Spontaneous Japanese (44 hours, 155 speakers, 201 talks). The correct prediction rate of speaker gender by means of logistic regression analysis was about 80{\%}, and, the correct discrimination rate of speaker age-group (4 groups) by means of linear discriminant analysis was about 50 {\%}. These results, in conjunction with the previously reported result of the prediction experiment of 4 speech registers from the X-JToBI information, shows convincingly the superiority of X-JToBI over the traditional J{\_}ToBI. Clarification of the mechanism by which gender- and/or age-group information were reflected in the symbolic representations of prosody largely remains as open question, although some preliminary analyses were presented in the current paper.
In this paper we present a language, PEARL, for projecting annotations based on the Unstructured Information Management Architecture (UIMA) over RDF triples. The language offer is twofold: first, a query mechanism, built upon (and extending) the basic FeaturePath notation of UIMA, allows for efficient access to the standard annotation format of UIMA based on feature structures. PEARL then provides a syntax for projecting the retrieved information onto an RDF Dataset, by using a combination of a SPARQL-like notation for matching pre-existing elements of the dataset and of meta-graph patterns, for storing new information into it. In this paper we present the basics of this language and how a PEARL document is structured, discuss a simple use-case and introduce a wider project about automatic acquisition of knowledge, in which PEARL plays a pivotal role.
In the last years, temporal tagging has received increasing attention in the area of natural language processing. However, most of the research so far concentrated on processing news documents. Only recently, two temporal annotated corpora of narrative-style documents were developed, and it was shown that a domain shift results in significant challenges for temporal tagging. Thus, a temporal tagger should be aware of the domain associated with documents that are to be processed and apply domain-specific strategies for extracting and normalizing temporal expressions. In this paper, we analyze the characteristics of temporal expressions in different domains. In addition to news- and narrative-style documents, we add two further document types, namely colloquial and scientific documents. After discussing the challenges of temporal tagging on the different domains, we describe some strategies to tackle these challenges and describe their integration into our publicly available temporal tagger HeidelTime. Our cross-domain evaluation validates the benefits of domain-sensitive temporal tagging. Furthermore, we make available two new temporally annotated corpora and a new version of HeidelTime, which now distinguishes between four document domain types.
Wikipedia articles in different languages have been mined to support various tasks, such as Cross-Language Information Retrieval (CLIR) and Statistical Machine Translation (SMT). Articles on the same topic in different languages are often connected by inter-language links, which can be used to identify similar or comparable content. In this work, we investigate the correlation between similarity measures utilising language-independent and language-dependent features and respective human judgments. A collection of 800 Wikipedia pairs from 8 different language pairs were collected and judged for similarity by two assessors. We report the development of this corpus and inter-assessor agreement between judges across the languages. Results show that similarity measured using language independent features is comparable to using an approach based on translating non-English documents. In both cases the correlation with human judgments is low but also dependent upon the language pair. The results and corpus generated from this work also provide insights into the measurement of cross-language similarity.
This paper outlines a proposal for encoding and describing verb phrase constructions in the knowledge base on the environment EcoLexicon, with the objective of helping translators in specialized text production. In order to be able to propose our own template, the characteristics and limitations of the most representative terminographic resources that include phraseological information were analyzed, along with the theoretical background that underlies the verb meaning argument structure in EcoLexicon. Our description provides evidence of the fact that this kind of entry structure can be easily encoded in other languages.
Action verbs, which are highly frequent in speech, cause disambiguation problems that are relevant to Language Technologies. This is a consequence of the peculiar way each natural language categorizes Action i.e. it is a consequence of semantic factors. Action verbs are frequently general, since they extend productively to actions belonging to different ontological types. Moreover, each language categorizes action in its own way and therefore the cross-linguistic reference to everyday activities is puzzling. This paper briefly sketches the IMAGACT project, which aims at setting up a cross-linguistic Ontology of Action for grounding disambiguation tasks in this crucial area of the lexicon. The project derives information on the actual variation of action verbs in English and Italian from spontaneous speech corpora, where references to action are high in frequency. Crucially it makes use of the universal language of images to identify action types, avoiding the underdeterminacy of semantic definitions. Action concept entries are implemented as prototypic scenes; this will make it easier to extend the Ontology to other languages.
We propose HamleDT ― HArmonized Multi-LanguagE Dependency Treebank. HamleDT is a compilation of existing dependency treebanks (or dependency conversions of other treebanks), transformed so that they all conform to the same annotation style. While the license terms prevent us from directly redistributing the corpora, most of them are easily acquirable for research purposes. What we provide instead is the software that normalizes tree structures in the data obtained by the user from their original providers.
FreeLing is an open-source multilingual language processing library providing a wide range of analyzers for several languages. It offers text processing and language annotation facilities to NLP application developers, lowering the cost of building those applications. FreeLing is customizable, extensible, and has a strong orientation to real-world applications in terms of speed and robustness. Developers can use the default linguistic resources (dictionaries, lexicons, grammars, etc.), extend/adapt them to specific domains, or --since the library is open source-- develop new ones for specific languages or special application needs. This paper describes the general architecture of the library, presents the major changes and improvements included in FreeLing version 3.0, and summarizes some relevant industrial projects in which it has been used.
We describe an experiment carried out using a French version of CALL-SLT, a web-enabled CALL game in which students at each turn are prompted to give a semi-free spoken response which the system then either accepts or rejects. The central question we investigate is whether the response is appropriate; we do this by extracting pairs of utterances where both members of the pair are responses by the same student to the same prompt, and where one response is accepted and one rejected. When the two spoken responses are presented in random order, native speakers show a reasonable degree of agreement in judging that the accepted utterance is better than the rejected one. We discuss the significance of the results and also present a small study supporting the claim that native speakers are nearly always recognised by the system, while non-native speakers are rejected a significant proportion of the time.
We describe a scalable architecture, particularly well-suited to cloud-based computing, which can be used for Web-deployment of spoken dialogue systems. In common with similar platforms, like WAMI and the Nuance Mobile Developer Platform, we use a client/server approach in which speech recognition is carried out on the server side; our architecture, however, differs from these systems in offering considerably more elaborate server-side functionality, based on large-scale grammar-based language processing and generic dialogue management. We describe two substantial applications, built using our framework, which we argue would have been hard to construct in WAMI or NMDP. Finally, we present a series of evaluations carried out using CALL-SLT, a speech translation game, where we contrast performance in Web and desktop versions. Task Error Rate in the Web version is only slightly inferior that in the desktop one, and the average additional latency is under half a second. The software is generally available for research purposes.
In this paper we present the Virtual Language Observatory (VLO), a metadata-based portal for language resources. It is completely based on the Component Metadata (CMDI) and ISOcat standards. This approach allows for the use of heterogeneous metadata schemas while maintaining the semantic compatibility. We describe the metadata harvesting process, based on OAI-PMH, and the conversion from several formats (OLAC, IMDI and the CLARIN LRT inventory) to their CMDI counterpart profiles. Then we focus on some post-processing steps to polish the harvested records. Next, the ingestion of the CMDI files into the VLO facet browser is described. We also include an overview of the changes since the first version of the VLO, based on user feedback from the CLARIN community. Finally there is an overview of additional ideas and improvements for future versions of the VLO.
We describe the background for and building of IcePaHC, a one million word parsed historical corpus of Icelandic which has just been finished. This corpus which is completely free and open contains fragments of 60 texts ranging from the late 12th century to the present. We describe the text selection and text collecting process and discuss the quality of the texts and their conversion to modern Icelandic spelling. We explain why we choose to use a phrase structure Penn style annotation scheme and briefly describe the syntactic anno-tation process. We also describe a spin-off project which is only in its beginning stages: a parsed historical corpus of Faroese. Finally, we advocate the importance of an open source policy as regards language resources.
This paper examines both linguistic behavior and practical implication of empty argument insertion in the Hindi PropBank. The Hindi PropBank is annotated on the Hindi Dependency Treebank, which contains some empty categories but not the empty arguments of verbs. In this paper, we analyze four kinds of empty arguments, *PRO*, *REL*, *GAP*, *pro*, and suggest effective ways of annotating these arguments. Empty arguments such as *PRO* and *REL* can be inserted deterministically; we present linguistically motivated rules that automatically insert these arguments with high accuracy. On the other hand, it is difficult to find deterministic rules to insert *GAP* and *pro*; for these arguments, we introduce a new annotation scheme that concurrently handles both semantic role labeling and empty category insertion, producing fast and high quality annotation. In addition, we present algorithms for finding antecedents of *REL* and *PRO*, and discuss why finding antecedents for some types of *PRO* is difficult.
In this paper, we present the founding elements of a formal model of the evaluation paradigm in natural language processing. We propose an abstract model of objective quantitative evaluation based on rough sets, as well as the notion of potential performance space for describing the performance variations corresponding to the ambiguity present in hypothesis data produced by a computer program, when comparing it to the reference data created by humans. A formal model of the evaluation paradigm will be useful for comparing evaluations protocols, investigating evaluation constraint relaxation and getting a better understanding of the evaluation paradigm, provided it is general enough to be able to represent any natural language processing task.
In recent years, machine translation (MT) research has focused on investigating how hybrid machine translation as well as system combination approaches can be designed so that the resulting hybrid translations show an improvement over the individual component translations. As a first step towards achieving this objective we have developed a parallel corpus with source text and the corresponding translation output from a number of machine translation engines, annotated with metadata information, capturing aspects of the translation process performed by the different MT systems. This corpus aims to serve as a basic resource for further research on whether hybrid machine translation algorithms and system combination techniques can benefit from additional (linguistically motivated, decoding, and runtime) information provided by the different systems involved. In this paper, we describe the annotated corpus we have created. We provide an overview on the component MT systems and the XLIFF-based annotation format we have developed. We also report on first experiments with the ML4HMT corpus data.
The paper presents a gold-standard reference corpus of historical Slovene containing 1,000 sampled pages from over 80 texts, which were, for the most part, written between 1750-1900. Each page of the transcription has an associated facsimile and the words in the texts have been manually annotated with their modern-day equivalent, lemma and part-of-speech. The paper presents the structure of the text collection, the sampling procedure, annotation process and encoding of the corpus. The corpus is meant to facilitate HLT research and enable corpus based diachronic studies for historical Slovene. The corpus is encoded according to the Text Encoding Initiative Guidelines (TEI P5), is available via a concordancer and for download from http://nl.ijs.si/imp/ under the Creative Commons Attribution licence.
The aim of this paper is to present a system for semantic text annotation called Inforex. Inforex is a web-based system designed for managing and annotating text corpora on the semantic level including annotation of Named Entities (NE), anaphora, Word Sense Disambiguation (WSD) and relations between named entities. The system also supports manual text clean-up and automatic text pre-processing including text segmentation, morphosyntactic analysis and word selection for word sense annotation. Inforex can be accessed from any standard-compliant web browser supporting JavaScript. The user interface has a form of dynamic HTML pages using the AJAX technology. The server part of the system is written in PHP and the data is stored in MySQL database. The system make use of some external tools that are installed on the server or can be accessed via web services. The documents are stored in the database in the original format ― either plain text, XML or HTML. Tokenization and sentence segmentation is optional and is stored in a separate table. Tokens are stored as pairs of values representing indexes of first and last character of the tokens and sets of features representing the morpho-syntactic information.
In this article, we describe a new sense-tagged corpus for Word Sense Disambiguation. The corpus is constituted of instances of 20 French polysemous verbs. Each verb instance is annotated with three sense labels: (1) the actual translation of the verb in the english version of this instance in a parallel corpus, (2) an entry of the verb in a computational dictionary of French (the Lexicon-Grammar tables) and (3) a fine-grained sense label resulting from the concatenation of the translation and the Lexicon-Grammar entry.
In this paper we describe recent developments in the CLARIN-NL project with the goal of sharing information on and experiences in this project with the community outside of the Netherlands. We discuss a variety of subprojects to actually implement the infrastructure, to provide functionality for search in metadata and the actual data, resource curation and demonstration projects, the Data Curation Service, actions to improve semantic interoperability and coordinate work on it, involvement of CLARIN Data Providers, education and training, outreach activities, and cooperation with other projects. Based on these experiences, we provide some recommendations for related projects. The recommendations concern a variety of topics including the organisation of an infrastructure project as a function of the types of tasks that have to be carried out, involvement of the targeted users, metadata, semantic interoperability and the role of registries, measures to maximally ensure sustainability, and cooperation with similar projects in other countries.
Among the linguistic resources formalizing a language, morphological rules are among those that can be achieved in a reasonable time. Nevertheless, since the construction of such resource can require linguistic expertise, morphological rules are still lacking for many languages. The automatized acquisition of morphology is thus an open topic of interest within the NLP field. We present an approach that allows to automatically compute, from raw corpora, a data-representative description of the concatenative mechanisms of a morphology. Our approach takes advantage of phenomena that are observable for all languages using morphological inflection and derivation but are more easy to exploit when dealing with concatenative mechanisms. Since it has been developed toward the objective of being used on as many languages as possible, applying this approach to a varied set of languages needs very few expert work. The results obtained for our first participation in the 2010 edition of MorphoChallenge have confirmed both the practical interest and the potential of the method.
Automatic annotation of temporal expressions is a research challenge of great interest in the field of information extraction. Gold standard temporally-annotated resources are limited in size, which makes research using them difficult. Standards have also evolved over the past decade, so not all temporally annotated data is in the same format. We vastly increase available human-annotated temporal expression resources by converting older format resources to TimeML/TIMEX3. This task is difficult due to differing annotation methods. We present a robust conversion tool and a new, large temporal expression resource. Using this, we evaluate our conversion process by using it as training data for an existing TimeML annotation tool, achieving a 0.87 F1 measure - better than any system in the TempEval-2 timex recognition exercise.
With the advent of massive online encyclopedic corpora such as Wikipedia, it has become possible to apply a systematic analysis to a wide range of documents covering a significant part of human knowledge. Using semantic parsers, it has become possible to extract such knowledge in the form of propositions (predicate―argument structures) and build large proposition databases from these documents. This paper describes the creation of multilingual proposition databases using generic semantic dependency parsing. Using Wikipedia, we extracted, processed, clustered, and evaluated a large number of propositions. We built an architecture to provide a complete pipeline dealing with the input of text, extraction of knowledge, storage, and presentation of the resulting propositions.
Sentiment analysis, or opinion mining, is the process of extracting sentiment from documents or sentences, where the expressed sentiment is typically categorized as positive, negative, or neutral. Many different techniques have been proposed. In this paper, we report the reimplementation of nine algorithms and their evaluation across four corpora to assess the sentiment at the sentence level. We extracted the named entities from each sentence and we associated them with the sentence sentiment. We built a graphical module based on the Qlikview software suite to visualize the sentiments attached to named entities mentioned in Internet forums and follow opinion changes over time.
Automatic annotation of gesture strokes is important for many gesture and sign language researchers. The unpredictable diversity of human gestures and video recording conditions require that we adopt a more adaptive case-by-case annotation model. In this paper, we present a work-in progress annotation model that allows a user to a) track hands/face b) extract features c) distinguish strokes from non-strokes. The hands/face tracking is done with color matching algorithms and is initialized by the user. The initialization process is supported with immediate visual feedback. Sliders are also provided to support a user-friendly adjustment of skin color ranges. After successful initialization, features related to positions, orientations and speeds of tracked hands/face are extracted using unique identifiable features (corners) from a window of frames and are used for training a learning algorithm. Our preliminary results for stroke detection under non-ideal video conditions are promising and show the potential applicability of our methodology.
We present the first results on semantic role labeling using the Swedish FrameNet, which is a lexical resource currently in development. Several aspects of the task are investigated, including the {\%}design and selection of machine learning features, the effect of choice of syntactic parser, and the ability of the system to generalize to new frames and new genres. In addition, we evaluate two methods to make the role label classifier more robust: cross-frame generalization and cluster-based features. Although the small amount of training data limits the performance achievable at the moment, we reach promising results. In particular, the classifier that extracts the boundaries of arguments works well for new frames, which suggests that it already at this stage can be useful in a semi-automatic setting.
Annotated corpora such as treebanks are important for the development of parsers, language applications as well as understanding of the language itself. Only very few languages possess these scarce resources. In this paper, we describe our efforts in syntactically annotating a small corpora (600 sentences) of Tamil language. Our annotation is similar to Prague Dependency Treebank (PDT) and consists of annotation at 2 levels or layers: (i) morphological layer (m-layer) and (ii) analytical layer (a-layer). For both the layers, we introduce annotation schemes i.e. positional tagging for m-layer and dependency relations for a-layers. Finally, we discuss some of the issues in treebank development for Tamil.
In this paper, we present the architecture of a distributed resource repository developed for collecting training data for building customized statistical machine translation systems. The repository is designed for the cloud-based translation service integrated in the Let'sMT! platform which is about to be launched to the public. The system includes important features such as automatic import and alignment of textual documents in a variety of formats, a flexible database for meta-information using modern key-value stores and a grid-based backend for running off-line processes. The entire system is very modular and supports highly distributed setups to enable a maximum of flexibility and scalability. The system uses secure connections and includes an effective permission management to ensure data integrity. In this paper, we also take a closer look at the task of sentence alignment. The process of alignment is extremely important for the success of translation models trained on the platform. Alignment decisions significantly influence the quality of SMT engines.
This paper presents CINTIL-QATreebank, a treebank composed of Portuguese sentences that can be used to support the development of Question Answering systems. To create this treebank, we use declarative sentences from the pre-existing CINTIL-Treebank and manually transform their syntactic structure into a non-declarative sentence. Our corpus includes two clause types: interrogative and imperative clauses. CINTIL-QATreebank can be used in language science and techology general research, but it was developed particularly for the development of automatic Question Answering systems. The non-declarative entences are annotated with several layers of linguistic information, namely (i) trees with information on constituency and grammatical function; (ii) sentence type; (iii) interrogative pronoun; (iv) question type; and (v) semantic type of expected answer. Moreover, these non-declarative sentences are paired with their declarative counterparts and associated with the expected answer snippets.
The Linguistic Data Consortium and Georgetown University Press are collaborating to create updated editions of bilingual diction- aries that had originally been published in the 1960's for English-speaking learners of Moroccan, Syrian and Iraqi Arabic. In their first editions, these dictionaries used ad hoc Latin-alphabet orthography for each colloquial Arabic dialect, but adopted some proper- ties of Arabic-based writing (collation order of Arabic headwords, clitic attachment to word forms in example phrases); despite their common features, there are notable differences among the three books that impede comparisons across the dialects, as well as com- parisons of each dialect to Modern Standard Arabic. In updating these volumes, we use both Arabic script and International Pho- netic Alphabet orthographies; the former provides a common basis for word recognition across dialects, while the latter provides dialect-specific pronunciations. Our goal is to preserve the full content of the original publications, supplement the Arabic headword inventory with new usages, and produce a uniform lexicon structure expressible via the Lexical Markup Framework (LMF, ISO 24613). To this end, we developed a relational database schema that applies consistently to each dialect, and HTTP-based tools for searching, editing, workflow, review and inventory management.
This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.
We investigate the creation of corpora from web-harvested data following a scalable approach that has linear query complexity. Individual web queries are posed for a lexicon that includes thousands of nouns and the retrieved data are aggregated. A lexical network is constructed, in which the lexicon nouns are linked according to their context-based similarity. We introduce the notion of semantic neighborhoods, which are exploited for the computation of semantic similarity. Two types of normalization are proposed and evaluated on the semantic tasks of: (i) similarity judgement, and (ii) noun categorization and taxonomy creation. The created corpus along with a set of tools and noun similarities are made publicly available.
We present an annotation and morphological segmentation scheme for Egyptian Colloquial Arabic (ECA) in which we annotate user-generated content that significantly deviates from the orthographic and grammatical rules of Modern Standard Arabic and thus cannot be processed by the commonly used MSA tools. Using a per letter classification scheme in which each letter is classified as either a segment boundary or not, and using a memory-based classifier, with only word-internal context, prove effective and achieve a 92{\%} exact match accuracy at the word level. The well-known MADA system achieves 81{\%} while the per letter classification scheme using the ATB achieves 82{\%}. Error analysis shows that the major problem is that of character ambiguity since the ECA orthography overloads the characters which would otherwise be more specific in MSA, like the differences between y ({\`U}) and Y ({\`U}) and A ({\O}{\S}) , {\textgreater} ( {\O}{\pounds}), and {\textless} ({\O}¥) which are collapsed to y ({\`U}) and A ({\O}{\S}) respectively or even totally confused and interchangeable. While normalization helps alleviate orthographic inconsistencies, it aggravates the problem of ambiguity.
In order to extract meaningful phrases from corpora (e. g. in an information retrieval context) intensive knowledge of the domain in question and the respective documents is generally needed. When moving to a new domain or language the underlying knowledge bases and models need to be adapted, which is often time-consuming and labor-intensive. This paper adresses the described challenge of phrase extraction from documents in different domains and languages and proposes an approach, which does not use comprehensive lexica and therefore can be easily transferred to new domains and languages. The effectiveness of the proposed approach is evaluated on user generated content and documents from the patent domain in English and German.
The web is composed of a gigantic amount of documents that can be very useful for information extraction systems. Most of them are written in HTML and have to be rendered by an HTML engine in order to display the data they contain on a screen. HTML file thus mix both informational and rendering content. Our goal is to design a tool for informational content extraction. A linear extraction with only a basic filtering of rendering content would not be enough as objects such as lists and tables are linearly coded but need to be read in a non-linear way to be well interpreted. Besides these HTML pages are often incorrectly coded from an HTML point of view and use a segmentation of blocks based on blank space that cannot be transposed in a text filewithout confusing syntactic parsers. For this purpose, we propose the Kitten tool that first normalizes HTML file into unicode XHTML file, then extracts the informational content into a text filewith a special processing for sentences, lists and tables.
Creating and maintaining metadata for various kinds of resources requires appropriate tools to assist the user. The paper presents the metadata editor ProFormA for the creation and editing of CMDI (Component Metadata Infrastructure) metadata in web forms. This editor supports a number of CMDI profiles currently being provided for different types of resources. Since the editor is based on XForms and server-side processing, users can create and modify CMDI files in their standard browser without the need for further processing. Large parts of ProFormA are implemented as web services in order to reuse them in other contexts and programs.
This paper presents a comparable corpus of Portuguese and Spanish consisting of legal and health texts. We describe the annotation of zero subject, impersonal constructions and explicit subjects in the corpus. We annotated 12,492 examples using a scheme that distinguishes between different linguistic levels (phonology, syntax, semantics, etc.) and present a taxonomy of instances on which annotators disagree. The high level of inter-annotator agreement (83{\%}-95{\%}) and the performance of learning algorithms trained on the corpus show that our corpus is a reliable and useful resource.
This paper presents the system architecture as well as the underlying workflow of the Extensible Repository System of Digital Objects (ERDO) which has been developed for the sustainable archiving of language resources within the T{\"u
This paper will present the design of a Galician syntactic corpus with application to intonation modeling. A corpus of around {\$}3000{\$} sentences was designed with variation in the syntactic structure and the number of accent groups, and recorded by a professional speaker to study the influence on the prosodic structure.
When dealing with languages of South Asia from an NLP perspective, a problem that repeatedly crops up is the treatment of complex predicates. This paper presents a first approach to the analysis of complex predicates (CPs) in the context of dependency bank development. The efforts originate in theoretical work on CPs done within Lexical-Functional Grammar (LFG), but are intended to provide a guideline for analyzing different types of CPs in an independent framework. Despite the fact that we focus on CPs in Hindi and Urdu, the design of the dependencies is kept general enough to account for CP constructions across languages.
We present UBY-LMF, an LMF-based model for large-scale, heterogeneous multilingual lexical-semantic resources (LSRs). UBY-LMF allows the standardization of LSRs down to a fine-grained level of lexical information by employing a large number of Data Categories from ISOCat. We evaluate UBY-LMF by converting nine LSRs in two languages to the corresponding format: the English WordNet, Wiktionary, Wikipedia, OmegaWiki, FrameNet and VerbNet and the German Wikipedia, Wiktionary and GermaNet. The resulting LSR, UBY (Gurevych et al., 2012), holds interoperable versions of all nine resources which can be queried by an easy to use public Java API. UBY-LMF covers a wide range of information types from expert-constructed and collaboratively constructed resources for English and German, also including links between different resources at the word sense level. It is designed to accommodate further resources and languages as well as automatically mined lexical-semantic knowledge.
The quality of statistical measurements on corpora is strongly related to a strict definition of the measuring process and to corpus quality. In the case of multiple result inspections, an exact measurement of previously specified parameters ensures compatibility of the different measurements performed by different researchers on possibly different objects. Hence, the comparison of different values requires an exact description of the measuring process. To illustrate this correlation the influence of different definitions for the concepts ''''''``word'''''''' and ''''''``sentence'''''''' is shown for several properties of large text corpora. It is also shown that corpus pre-processing strongly influences corpus size and quality as well. As an example near duplicate sentences are identified as source of many statistical irregularities. The problem of strongly varying results especially holds for Web corpora with a large set of pre-processing steps. Here, a well-defined and language independent pre-processing is indispensable for language comparison based on measured values. Conversely, irregularities found in such measurements are often a result of poor pre-processing and therefore such measurements can help to improve corpus quality.
The automatic development of semantic resources constitutes an important challenge in the NLP community. The methods used generally exploit existing large-scale resources, such as Princeton WordNet, often combined with information extracted from multilingual resources and parallel corpora. In this paper we show how Cross-Lingual Word Sense Disambiguation can be applied to wordnet development. We apply the proposed method to WOLF, a free wordnet for French still under construction, in order to fill synsets that did not contain any literal yet and increase its coverage.
The goal of this paper is to provide an annotation scheme for compounds based on generative lexicon theory (GL, Pustejovsky, 1995; Bassac and Bouillon, 2001). This scheme has been tested on a set of compounds automatically extracted from the Europarl corpus (Koehn, 2005) both in Italian and French. The motivation is twofold. On the one hand, it should help refine existing compound classifications and better explain lexicalization in both languages. On the other hand, we hope that the extracted generalizations can be used in NLP, for example for improving MT systems or for query reformulation (Claveau, 2003). In this paper, we focus on the annotation scheme and its on going evaluation.
Recently the first methods of automatic diagnostics of machine translation have emerged; since this area of research is relatively young, the efforts are not coordinated. We present a collection of translation error-annotated corpora, consisting of automatically produced translations and their detailed manual translation error analysis. Using the collected corpora we evaluate the available state-of-the-art methods of MT diagnostics and assess, how well the methods perform, how they compare to each other and whether they can be useful in practice.
This paper describes an approach to automatic nuggetization and implemented system employed in GALE Distillation evaluation to measure the information content of text returned in response to an open-ended question. The system identifies nuggets, or atomic units of information, categorizes them according to their semantic type, and selects different types of nuggets depending on the type of the question. We further show how this approach addresses the main challenges for using automatic nuggetization for QA evaluation: the variability of relevant nuggets and their dependence on the question. Specifically, we propose a template-based approach to nuggetization, where different semantic categories of nuggets are extracted dependent on the template of a question. During evaluation, human annotators judge each snippet returned in response to a query as relevant or irrelevant, whereas automatic template-based nuggetization is further used to identify the semantic units of information that people would have selected as relevant' or irrelevant' nuggets for a given query. Finally, the paper presents the performance results of the nuggetization system which compare the number of automatically generated nuggets and human nuggets and show that our automatic nuggetization is consistent with human judgments.
This paper contains information about the ''''''``Sabanci University Turkish Audio-Visual (SUTAV)'''''''' database. The main aim of collecting SUTAV database was to obtain a large audio-visual collection of spoken words, numbers and sentences in Turkish language. The database was collected between 2006 and 2010 during ''''''``Novel approaches in audio-visual speech recognition'''''''' project which is funded by The Scientific and Technological Research Council of Turkey (TUBITAK). First part of the database contains a large corpus of Turkish language and contains standart quality videos. The second part is relatively small compared to the first one and contains recordings of spoken digits in high quality videos. Although the main aim to collect SUTAV database was to obtain a database for audio-visual speech recognition applications, it also contains useful data that can be used in other kinds of multimodal research like biometric security and person verification. The paper presents information about the data collection process and the the spoken content. It also contains a sample evaluation protocol and recognition results that are obtained with a small portion of the database.
A syllable-based language model reduces the lexicon size by hundreds of times. It is especially beneficial in case of highly inflective languages like Russian due to the abundance of word forms according to various grammatical categories. However, the main arising challenge is the concatenation of recognised syllables into the originally spoken sentence or phrase, particularly in the presence of syllable recognition mistakes. Natural fluent speech does not usually incorporate clear information about the outside borders of the spoken words. In this paper a method for the syllable concatenation and error correction is suggested and tested. It is based on the designed co-evolutionary asymptotic probabilistic genetic algorithm for the determination of the most likely sentence corresponding to the recognized chain of syllables within an acceptable time frame. The advantage of this genetic algorithm modification is the minimum number of settings to be manually adjusted comparing to the standard algorithm. Data used for acoustic and language modelling are also described here. A special issue is the preprocessing of the textual data, particularly, handling of abbreviations, Arabic and Roman numerals, since their inflection mostly depends on the context and grammar.
This paper presents the main features (design issues, recording setup, etc.) of KALAKA-2, a TV broadcast speech database specifically designed for the development and evaluation of language recognition systems in clean and noisy environments. KALAKA-2 was created to support the Albayzin 2010 Language Recognition Evaluation (LRE), organized by the Spanish Network on Speech Technologies from June to November 2010. The database features 6 target languages: Basque, Catalan, English, Galician, Portuguese and Spanish, and includes segments in other (Out-Of-Set) languages, which allow to perform open-set verification tests. The best performance attained in the Albayzin 2010 LRE is presented and briefly discussed. The performance of a state-of-the-art system in various tasks defined on the database is also presented. In both cases, results highlight the suitability of KALAKA-2 as a benchmark for the development and evaluation of language recognition technology.
We have been creating large-scale manual word alignment corpora for Arabic-English and Chinese-English language pairs in genres such as newsire, broadcast news and conversation, and web blogs. We are now meeting the challenge of word aligning further varieties of web data for Chinese and Arabic ''''''``dialects''''''''. Human word alignment annotation can be costly and arduous. Alignment guidelines may be imprecise or underspecified in cases where parallel sentences are hard to compare -- due to non-literal translations or differences between language structures. In order to speed annotation, we examine the effect that seeding manual alignments with automatic aligner output has on annotation speed and accuracy. We use automatic alignment methods that produce alignment results which are high precision and low recall to minimize annotator corrections. Results suggest that annotation time can be reduced by up to 20{\%}, but we also found that reviewing and correcting automatic alignments requires more time than anticipated. We discuss throughout the paper crucial decisions on data structures for word alignment that likely have a significant impact on our results.
The present paper describes the development and evaluation of the Polish emergency dialogue corpus recorded for studying alignment phenomena in stress scenarios. The challenge is that emergency dialogues are more complex on many levels than standard information negotiation dialogues, different resources are needed for differential investigation, and resources for this kind of corpus are rare. Currently there is no comparable corpus for Polish. In the present context, alignment is meant as adaptation on the syntactic, semantic and pragmatic levels of communication between the two interlocutors, including choice of similar lexical items and speaking style. Four different dialogue scenarios were arranged and prompt speech material was created. Two maps for the map-tasks and one emergency diapix were design to prompt semi-spontaneous dialogues simulating stress and natural communicative situations. The dialogue corpus was recorded taking into account the public character of conversations in the emergency setting. The linguistic study of alignment in this kind of dialogue made it possible to design and implement a prototype of a Polish adaptive dialogue system to support stress scenario communication (not described in this paper).
The following paper presents a lexical analysis component as implemented in the PANACEA project. The goal is to automatically extract lexicon entries from crawled corpora, in an attempt to use corpus-based methods for high-quality linguistic text processing, and to focus on the quality of data without neglecting quantitative aspects. Lexical analysis has the task to assign linguistic information (like: part of speech, inflectional class, gender, subcategorisation frame, semantic properties etc.) to all parts of the input text. If tokens are ambiguous, lexical analysis must provide all possible sets of annotation for later (syntactic) disambiguation, be it tagging, or full parsing. The paper presents an approach for assigning part-of-speech tags for German and English to large input corpora ({\textgreater} 50 mio tokens), providing a workflow which takes as input crawled corpora and provides POS-tagged lemmata ready for lexicon integration. Tools include sentence splitting, lexicon lookup, decomposition, and POS defaulting. Evaluation shows that the overall error rate can be brought down to about 2{\%} if language resources are properly designed. The complete workflow is implemented as a sequence of web services integrated into the PANACEA platform.
This paper proposes a method to construct an evaluation dataset from microblogs for the development of recommendation systems. We extract the relationships among three main entities in a recommendation event, i.e., who recommends what to whom. User-to-user friend relationships and user-to-resource interesting relationships in social media and resource-to-metadata descriptions in an external ontology are employed. In the experiments, the resources are restricted to visual entertainment media, movies in particular. A sequence of ground truths varying with time is generated. That reflects the dynamic of real world.
The paper presents a comprehensive overview of existing data for the evaluation of spoken content processing in a multimedia framework for the French language. We focus on the ETAPE corpus which will be made publicly available by ELDA mid 2012, after completion of the evaluation campaign, and recall existing resources resulting from previous evaluation campaigns. The ETAPE corpus consists of 30 hours of TV and radio broadcasts, selected to cover a wide variety of topics and speaking styles, emphasizing spontaneous speech and multiple speaker areas.
The work we present here addresses cue-based noun classification in English and Spanish. Its main objective is to automatically acquire lexical semantic information by classifying nouns into previously known noun lexical classes. This is achieved by using particular aspects of linguistic contexts as cues that identify a specific lexical class. Here we concentrate on the task of identifying such cues and the theoretical background that allows for an assessment of the complexity of the task. The results show that, despite of the a-priori complexity of the task, cue-based classification is a useful tool in the automatic acquisition of lexical semantic classes.
Current search engines are used for retrieving relevant documents from the huge amount of data available and have become an essential tool for the majority of Web users. Standard search engines do not consider semantic information that can help in recognizing the relevance of a document with respect to the meaning of a query. In this paper, we present our system architecture and a first user study, where we show that the use of semantics can help users in finding relevant information, filtering it ad facilitating quicker access to data.
This paper reports on a research carried out at the Institute for Computational Linguistics (ILC) on a set of idiomatic nautical expressions in Italian and English. A total of 200 Italian expressions were first selected and examined, using both monolingual and bilingual dictionaries, as well as specific lexicographical works dealing with the subject of idiomaticity, especially of the maritime type, and a similar undertaking was then conducted for the English expressions. We discuss the possibility of including both the Italian and English idiomatic expressions in the semantic database Mariterm, which contains terms belonging to the maritime domain. We describe the terminological database and the way in which the idiomatic expressions can be organised within the system, so that, similarly to the other synsets, they are connected to other concepts represented in the database, but at the same time continue to belong to a group of particular linguistic expressions. Furthermore, we study similarities and differences in meaning and usage of some idiomatic expressions in the two languages.
ESICT (Experience-oriented Sharing of health knowledge via Information and Communication Technology) is an ongoing research project funded by the Danish Council for Strategic Research. It aims at developing a health/disease related information system based on information technology, language technology, and formalized medical knowledge. The formalized medical knowledge consists partly of the terminology database SNOMED CT and partly of authorized medical texts on the domain. The system will allow users to ask questions in Danish and will provide natural language answers. Currently, the project is pursuing three basically different methods for question answering, and they are all described to some extent in this paper. A system prototype will handle questions related to diabetes and heart diseases. This paper concentrates on the methods employed for question answering and the language resources that are utilized. Some resources were existing, such as SNOMED CT, others, such as a corpus of sample questions, have had to be created or constructed.
We describe the acquisition of a dialog corpus for French based on multi-task human-machine interactions in a serious game setting. We present a tool for data collection that is configurable for multiple games; describe the data collected using this tool and the annotation schema used to annotate it; and report on the results obtained when training a classifier on the annotated data to associate each player turn with a dialog move usable by a rule based dialog manager. The collected data consists of approximately 1250 dialogs, 10454 utterances and 168509 words and will be made freely available to academic and nonprofit research.
Corpus-based approaches to machine translation (MT) rely on the availability of parallel corpora. To produce user-acceptable translation outputs, such systems need high quality data to be efficiency trained, optimized and evaluated. However, building high quality dataset is a relatively expensive task. In this paper, we describe the data collection and analysis of a large database of 10.881 SMT translation output hypotheses manually corrected. These post-editions were collected using Amazon's Mechanical Turk, following some ethical guidelines. A complete analysis of the collected data pointed out a high quality of the corrections with more than 87 {\%} of the collected post-editions that improve hypotheses and more than 94 {\%} of the crowdsourced post-editions which are at least of professional quality. We also post-edited 1,500 gold-standard reference translations (of bilingual parallel corpora generated by professional) and noticed that 72 {\%} of these translations needed to be corrected during post-edition. We computed a proximity measure between the differents kind of translations and pointed out that reference translations are as far from the hypotheses than from the corrected hypotheses (i.e. the post-editions). In light of these last findings, we discuss the adequation of text-based generated reference translations to train setence-to-sentence based SMT systems.
In this paper, we present a database-supported corpus study where we combine automatically obtained linguistic information from a statistical dependency parser, namely the occurrence of a dative argument, with predictions from a theory on the argument structure of German particle verbs with ''''''``nach''''''''. The theory predicts five readings of ''''''``nach'''''''' which behave differently with respect to dative licensing in their argument structure. From a huge German web corpus, we extracted sentences for a subset of ''''''``nach''''''''-particle verbs for which no dative is expected by the theory. Making use of a relational database management system, we bring together the corpus sentences and the lemmas manually annotated along the lines of the theory. We validate the theoretical predictions against the syntactic structure of the corpus sentences, which we obtained from a statistical dependency parser. We find that, in principle, the theory is borne out by the data, however, manual error analysis reveals cases for which the theory needs to be extended.
Wikipedia pages typically contain inter-language links to the corresponding pages in other languages. These links, however, are often incomplete. This paper describes a set of experiments in which the viability of discovering such missing inter-language links for ambiguous nouns by means of a cross-lingual Word Sense Disambiguation approach is investigated. The input for the inter-language link detection system is a set of Dutch pages for a given ambiguous noun and the output of the system is a set of links to the corresponding pages in three target languages (viz. French, Spanish and Italian). The experimental results show that although it is a very challenging task, the system succeeds to detect missing inter-language links between Wikipedia documents for a manually labeled test set. The final goal of the system is to provide a human editor with a list of possible missing links that should be manually verified.
Arabic segmentation was already applied successfully for the task of statistical machine translation (SMT). Yet, there is no consistent comparison of the effect of different techniques and methods over the final translation quality. In this work, we use existing tools and further re-implement and develop new methods for segmentation. We compare the resulting SMT systems based on the different segmentation methods over the small IWSLT 2010 BTEC and the large NIST 2009 Arabic-to-English translation tasks. Our results show that for both small and large training data, segmentation yields strong improvements, but, the differences between the top ranked segmenters are statistically insignificant. Due to the different methodologies that we apply for segmentation, we expect a complimentary variation in the results achieved by each method. As done in previous work, we combine several segmentation schemes of the same model but achieve modest improvements. Next, we try a different strategy, where we combine the different segmentation methods rather than the different segmentation schemes. In this case, we achieve stronger improvements over the best single system. Finally, combining schemes and methods has another slight gain over the best combination strategy.
We introduce a substantial update of the Prague Czech-English Dependency Treebank, a parallel corpus manually annotated at the deep syntactic layer of linguistic representation. The English part consists of the Wall Street Journal (WSJ) section of the Penn Treebank. The Czech part was translated from the English source sentence by sentence. This paper gives a high level overview of the underlying linguistic theory (the so-called tectogrammatical annotation) with some details of the most important features like valency annotation, ellipsis reconstruction or coreference.
Manual annotation of large textual corpora can be cost-prohibitive, especially for rare and under-resourced languages. One potential solution is pre-annotation: asking human annotators to correct sentences that have already been annotated, usually by a machine. Another potential solution is correction propagation: using annotator corrections to bad pre-annotations to dynamically improve to the remaining pre-annotations within the current sentence. The research presented in this paper employs a controlled user study to discover under what conditions these two machine-assisted annotation techniques are effective in increasing annotator speed and accuracy and thereby reducing the cost for the task of morphologically annotating texts written in classical Syriac. A preliminary analysis of the data indicates that pre-annotations improve annotator accuracy when they are at least 60{\%} accurate, and annotator speed when they are at least 80{\%} accurate. This research constitutes the first systematic evaluation of pre-annotation and correction propagation together in a controlled user study.
In this paper we present a corpus of audio and video recordings of spontaneous, face-to-face multi-party conversation in two languages. Freely available high quality recordings of mundane, non-institutional, multi-party talk are still sparse, and this corpus aims to contribute valuable data suitable for study of multiple aspects of spoken interaction. In particular, it constitutes a unique resource for spoken Bosnian Serbo-Croatian (BSC), an under-resourced language with no spoken resources available at present. The corpus consists of just over 3 hours of free conversation in each of the target languages, BSC and British English (BE). The audio recordings have been made on separate channels using head-set microphones, as well as using a microphone array, containing 8 omni-directional microphones. The data has been segmented and transcribed using segmentation notions and transcription conventions developed from those of the conversation analysis research tradition. Furthermore, the transcriptions have been automatically aligned with the audio at the word and phone level, using the method of forced alignment. In this paper we describe the procedures behind the corpus creation and present the main features of the corpus for the study of conversation.
We present several experiments aiming at measuring the semantic compositionality of NV expressions in Basque. Our approach is based on the hypothesis that compositionality can be related to distributional similarity. The contexts of each NV expression are compared with the contexts of its corresponding components, by means of different techniques, as similarity measures usually used with the Vector Space Model (VSM), Latent Semantic Analysis (LSA) and some measures implemented in the Lemur Toolkit, as Indri index, tf-idf, Okapi index and Kullback-Leibler divergence. Using our previous work with cooccurrence techniques as a baseline, the results point to improvements using the Indri index or Kullback-Leibler divergence, and a slight further improvement when used in combination with cooccurrence measures such as {\$}t{\$}-score, via rank-aggregation. This work is part of a project for MWE extraction and characterization using different techniques aiming at measuring the properties related to idiomaticity, as institutionalization, non-compositionality and lexico-syntactic fixedness.
A scientific vocabulary is a set of terms that designate scientific concepts. This set of lexical units can be used in several applications ranging from the development of terminological dictionaries and machine translation systems to the development of lexical databases and beyond. Even though automatic term recognition systems exist since the 80s, this process is still mainly done by hand, since it generally yields more accurate results, although not in less time and at a higher cost. Some of the reasons for this are the fairly low precision and recall results obtained, the domain dependence of existing tools and the lack of available semantic knowledge needed to validate these results. In this paper we present a method that uses Wikipedia as a semantic knowledge resource, to validate term candidates from a set of scientific text books used in the last three years of high school for mathematics, health education and ecology. The proposed method may be applied to any domain or language (assuming there is a minimal coverage by Wikipedia).
The SERENOA project is aimed at developing a novel, open platform for enabling the creation of context-sensitive Service Front-Ends (SFEs). A context-sensitive SFE provides a user interface (UI) that allows users to interact with remote services, and which exhibits some capability to be aware of the context and to react to changes of this context in a continuous way. As a result, such UI will be adapted to e.g. a person's devices, tasks, preferences, abilities, and social relationships, as well as the conditions of the surrounding physical environment, thus improving people's satisfaction and performance compared to traditional SFEs based on manually designed UIs. The final aim is to support humans in a more effective, personalized and consistent way, thus improving the quality of life for citizens. In this scenario, we envisage SERENOA as the reference implementation of a SFE adaptation platform for the 'Future Internet'.
This paper presents some of the results of the CLASSYN project which investigated the classification of text according to audience-related text types. We describe the design principles and the properties of the French and German linguistically annotated corpora that we have created. We report on tools used to collect the data and on the quality of the syntactic annotation. The CLASSYN corpora comprise two text collections to investigate general text types difference between scientific and popular science text on the two domains of medical and computer science.
This paper describes on-going work for the construction of a new treebank for Spanish, The IULA Treebank. This new resource will contain about 60,000 richly annotated sentences as an extension of the already existing IULA Technical Corpus which is only PoS tagged. In this paper we have focused on describing the work done for defining the annotation process and the treebank design principles. We report on how the used framework, the DELPH-IN processing framework, has been crucial in the design principles and in the bootstrapping strategy followed, especially in what refers to the use of stochastic modules for reducing parsing overgeneration. We also report on the different evaluation experiments carried out to guarantee the quality of the already available results.
We present a annotation scheme for modality in Portuguese. In our annotation scheme we have tried to combine a more theoretical linguistic viewpoint with a practical annotation scheme that will also be useful for NLP research but is not geared towards one specific application. Our notion of modality focuses on the attitude and opinion of the speaker, or of the subject of the sentence. We validated the annotation scheme on a corpus sample of approximately 2000 sentences that we fully annotated with modal information using the MMAX2 annotation tool to produce XML annotation. We discuss our main findings and give attention to the difficult cases that we encountered as they illustrate the complexity of modality and its interactions with other elements in the text.
Named entity recognition of the clinical entities disorders, findings and body structures is needed for information extraction from unstructured text in health records. Clinical notes from a Swedish emergency unit were annotated and used for evaluating a rule- and terminology-based entity recognition system. This system used different preprocessing techniques for matching terms to SNOMED CT, and, one by one, four other terminologies were added. For the class body structure, the results improved with preprocessing, whereas only small improvements were shown for the classes disorder and finding. The best average results were achieved when all terminologies were used together. The entity body structure was recognised with a precision of 0.74 and a recall of 0.80, whereas lower results were achieved for disorder (precision: 0.75, recall: 0.55) and for finding (precision: 0.57, recall: 0.30). The proportion of entities containing abbreviations were higher for false negatives than for correctly recognised entities, and no entities containing more than two tokens were recognised by the system. Low recall for disorders and findings shows both that additional methods are needed for entity recognition and that there are many expressions in clinical text that are not included in SNOMED CT.
Relation extraction (RE) is an important text mining task which is the basis for further complex and advanced tasks. In state-of-the-art RE approaches, syntactic information obtained through parsing plays a crucial role. In the context of biomedical RE previous studies report usage of various automatic preprocessing techniques applied before parsing the input text. However, these studies do not specify to what extent such techniques improve RE results and to what extent they are corpus specific as well as parser specific. In this paper, we aim at addressing these issues by using various preprocessing techniques, two syntactic tree kernel based RE approaches and two different parsers on 5 widely used benchmark biomedical corpora of the protein-protein interaction (PPI) extraction task. We also provide analyses of various corpus characteristics to verify whether there are correlations between these characteristics and the RE results obtained. These analyses of corpus characteristics can be exploited to compare the 5 PPI corpora.
Within scientific institutes there exist many language resources. These resources are often quite specialized and relatively unknown. The current infrastructural initiatives try to tackle this issue by collecting metadata about the resources and establishing centers with stable repositories to ensure the availability of the resources. It would be beneficial if the researcher could, by means of a simple query, determine which resources and which centers contain information useful to his or her research, or even work on a set of distributed resources as a virtual corpus. In this article we propose an architecture for a distributed search environment allowing researchers to perform searches in a set of distributed language resources.
We present some evaluation results for four French syntactic lexica, obtained through their conversion to the Alexina format used by the Lefff lexicon, and their integration within the large-coverage TAG-based FRMG parser. The evaluations are run on two test corpora, annotated with two distinct annotation formats, namely EASy/Passage chunks and relations and CoNLL dependencies. The information provided by the evaluation results provide valuable feedback about the four lexica. Moreover, when coupled with error mining techniques, they allow us to identify how these lexica might be improved.
This paper presents methodologies and tools for language resource (LR) construction. It describes a database of interactive speech collected over a three-month period at the Science Gallery in Dublin, where visitors could take part in a conversation with a robot. The system collected samples of informal, chatty dialogue -- normally difficult to capture under laboratory conditions for human-human dialogue, and particularly so for human-machine interaction. The conversations were based on a script followed by the robot consisting largely of social chat with some task-based elements. The interactions were audio-visually recorded using several cameras together with microphones. As part of the conversation the participants were asked to sign a consent form giving permission to use their data for human-machine interaction research. The multimodal corpus will be made available to interested researchers and the technology developed during the three-month exhibition is being extended for use in education and assisted-living applications.
In this paper, a previous work on the enlargement of monolingual dictionaries of rule-based machine translation systems by non-expert users is extended to tackle the complete task of adding both source-language and target-language words to the monolingual dictionaries and the bilingual dictionary. In the original method, users validate whether some suffix variations of the word to be inserted are correct in order to find the most appropriate inflection paradigm. This method is now improved by taking advantage from the strong correlation detected between paradigms in both languages to reduce the search space of the target-language paradigm once the source-language paradigm is known. Results show that, when the source-language word has already been inserted, the system is able to more accurately predict which is the right target-language paradigm, and the number of queries posed to users is significantly reduced. Experiments also show that, when the source language and the target language are not closely related, it is only the source-language part-of-speech category, but not the rest of information provided by the source-language paradigm, which helps to correctly classify the target-language word.
This paper presents two toolsets for transcribing and annotating spoken language: the EXMARaLDA system, developed at the University of Hamburg, and the FOLK tools, developed at the Institute for the German Language in Mannheim. Both systems are targeted at users interested in the analysis of spontaneous, multi-party discourse. Their main user community is situated in conversation analysis, pragmatics, sociolinguistics and related fields. The paper gives an overview of the individual tools of the two systems ― the Partitur-Editor, a tool for multi-level annotation of audio or video recordings, the Corpus Manager, a tool for creating and administering corpus metadata, EXAKT, a query and analysis tool for spoken language corpora, FOLKER, a transcription editor optimized for speed and efficiency of transcription, and OrthoNormal, a tool for orthographical normalization of transcription data. It concludes with some thoughts about the integration of these tools into the larger tool landscape.
This paper summarizes the latest, final version of ISO standard 24617-2 ``Semantic annotation framework, Part 2: Dialogue acts''''''''. Compared to the preliminary version ISO DIS 24617-2:2010, described in Bunt et al. (2010), the final version additionally includes concepts for annotating rhetorical relations between dialogue units, defines a full-blown compositional semantics for the Dialogue Act Markup Language DiAML (resulting, as a side-effect, in a different treatment of functional dependence relations among dialogue acts and feedback dependence relations); and specifies an optimally transparent XML-based reference format for the representation of DiAML annotations, based on the systematic application of the notion of `ideal concrete syntax'. We describe these differences and briefly discuss the design and implementation of an incremental method for dialogue act recognition, which proves the usability of the ISO standard for automatic dialogue annotation.
The purpose of this paper is to identify automatically hypernyms for dictionary entries by exploring their definitions. In order to do this, we propose a weighting methodology that lets us assign to each lexeme a weight in a definition. This fact allows us to predict that lexemes with the highest weight are the closest hypernyms of the defined lexeme in the dictionary. The extracted semantic relation is-a is used for the automatic construction of a thesaurus for image indexing and retrieval. We conclude the paper by showing some experimental results to validate our method and by presenting our methodology of automatic thesaurus construction.
This paper presents a freely available resource for research on handling negation and speculation in review texts. The SFU Review Corpus, consisting of 400 documents of movie, book, and consumer product reviews, was annotated at the token level with negative and speculative keywords and at the sentence level with their linguistic scope. We report statistics on corpus size and the consistency of annotations. The annotated corpus will be useful in many applications, such as document mining and sentiment analysis.
What would be a good method to provide a large collection of semantically annotated texts with formal, deep semantics rather than shallow? We argue that a bootstrapping approach comprising state-of-the-art NLP tools for parsing and semantic interpretation, in combination with a wiki-like interface for collaborative annotation of experts, and a game with a purpose for crowdsourcing, are the starting ingredients for fulfilling this enterprise. The result is a semantic resource that anyone can edit and that integrates various phenomena, including predicate-argument structure, scope, tense, thematic roles, rhetorical relations and presuppositions, into a single semantic formalism: Discourse Representation Theory. Taking texts rather than sentences as the units of annotation results in deep semantic representations that incorporate discourse structure and dependencies. To manage the various (possibly conflicting) annotations provided by experts and non-experts, we introduce a method that stores ``Bits of Wisdom'' in a database as stand-off annotations.
E-infrastructure projects such as CLARIN do not only make research data available to the scientific community, but also deliver a growing number of web services. While the standard methods for deploying web services using dedicated (virtual) server may suffice in many circumstances, CLARIN centers are also faced with a growing number of services that are not frequently used and for which significant compute power needs to be reserved. This paper describes an alternative approach towards service deployment capable of delivering on demand services in a workflow using cloud infrastructure capabilities. Services are stored as disk images and deployed on a workflow scenario only when needed this helping to reduce the overall service footprint.
We address the problem of automatic classification of associative and semantic relations between words, and particularly those that hold between nouns. Lexical relations such as synonymy, hypernymy/hyponymy, constitute the fundamental types of semantic relations. Associative relations are harder to define, since they include a long list of diverse relations, e.g., ''''''``Cause-Effect'''''''', ''''''``Instrument-Agency''''''''. Motivated by findings from the literature of psycholinguistics and corpus linguistics, we propose features that take advantage of general linguistic properties. For evaluation we merged three datasets assembled and validated by cognitive scientists. A proposed priming coefficient that measures the degree of asymmetry in the order of appearance of the words in text achieves the best classification results, followed by context-based similarity metrics. The web-based features achieve classification accuracy that exceeds 85{\%}.
In this paper we present the first freely available corpus of Dutch text messages containing data originating from the Netherlands and Flanders. This corpus has been collected in the framework of the SoNaR project and constitutes a viable part of this 500-million-word corpus. About 53,000 text messages were collected on a large scale, based on voluntary donations. These messages will be distributed as such. In this paper we focus on the data collection processes involved and after studying the effect of media coverage we show that especially free publicity in newspapers and on social media networks results in more contributions. All SMS are provided with metadata information. Looking at the composition of the corpus, it becomes visible that a small number of people have contributed a large amount of data, in total 272 people have contributed to the corpus during three months. The number of women contributing to the corpus is larger than the number of men, but male contributors submitted larger amounts of data. This corpus will be of paramount importance for sociolinguistic research and normalisation studies.
The lexicon is the store of words in long-term memory. Any attempt at modelling lexical competence must take issues of string storage seriously. In the present contribution, we discuss a few desiderata that any biologically-inspired computational model of the mental lexicon has to meet, and detail a multi-task evaluation protocol for their assessment. The proposed protocol is applied to a novel computational architecture for lexical storage and acquisition, the ''''''``Topological Temporal Hebbian SOMs'''''''' (T2HSOMs), which are grids of topologically organised memory nodes with dedicated sensitivity to time-bound sequences of letters. These maps can provide a rigorous and testable conceptual framework within which to provide a comprehensive, multi-task protocol for testing the performance of Hebbian self-organising memories, and a comprehensive picture of the complex dynamics between lexical processing and the acquisition of morphological structure.
Speech and hand gestures offer the most natural modalities for everyday human-to-human interaction. The availability of diverse spoken dialogue applications and the proliferation of accelerometers on consumer electronics allow the introduction of new interaction paradigms based on speech and gestures. Little attention has been paid however to the manipulation of spoken dialogue systems through gestures. Situation-induced disabilities or real disabilities are determinant factors that motivate this type of interaction. In this paper we propose six concise and intuitively meaningful gestures that can be used to trigger the commands in any SDS. Using different machine learning techniques we achieve a classification error for the gesture patterns of less than 5{\%}, and we also compare our own set of gestures to ones proposed by users. Finally, we examine the social acceptability of the specific interaction scheme and encounter high levels of acceptance for public use.
This paper presents the platform developed in the PANACEA project, a distributed factory that automates the stages involved in the acquisition, production, updating and maintenance of Language Resources required by Machine Translation and other Language Technologies. We adopt a set of tools that have been successfully used in the Bioinformatics field, they are adapted to the needs of our field and used to deploy web services, which can be combined to build more complex processing chains (workflows). This paper describes the platform and its different components (web services, registry, workflows, social network and interoperability). We demonstrate the scalability of the platform by carrying out a set of massive data experiments. Finally, a validation of the platform across a set of required criteria proves its usability for different types of users (non-technical users and providers).
The creation of language resources is a time-consuming process requiring the efforts of many people. The use of resources collaboratively created by non-linguistists can potentially ameliorate this situation. However, such resources often contain more errors compared to resources created by experts. For the particular case of lexica, we analyse the case of Wiktionary, a resource created along wiki principles and argue that through the use of a principled lexicon model, namely Lemon, the resulting data could be better understandable to machines. We then present a platform called Lemon Source that supports the creation of linked lexical data along the Lemon model. This tool builds on the concept of a semantic wiki to enable collaborative editing of the resources by many users concurrently. In this paper, we describe the model, the tool and present an evaluation of its usability based on a small group of users.
We have developed DBpedia Spotlight, a flexible concept tagging system that is able to annotate entities, topics and other terms in natural language text. The system starts by recognizing phrases to annotate in the input text, and subsequently disambiguates them to a reference knowledge base extracted from Wikipedia. In this paper we evaluate the impact of the phrase recognition step on the ability of the system to correctly reproduce the annotations of a gold standard in an unsupervised setting. We argue that a combination of techniques is needed, and we evaluate a number of alternatives according to an existing evaluation set.
Language resource centers allow researchers to reliably deposit their structured data together with associated meta data and run services operating on this deposited data. We are looking into possibilities to create long-term persistency of both the deposited data and the services operating on this data. Challenges, both technical and non-technical, that need to be solved are the need to replicate more than just the data, proper identification of the digital objects in a distributed environment by making use of persistent identifiers and the set-up of a proper authentication and authorization domain including the management of the authorization information on the digital objects. We acknowledge the investment that most language resource centers have made in their current infrastructure. Therefore one of the most important requirements is the loose coupling with existing infrastructures without the need to make many changes. This shift from a single language resource center into a federated environment of many language resource centers is discussed in the context of a real world center: The Language Archive supported by the Max Planck Institute for Psycholinguistics.
This paper introduces the RIDIRE-CPI, an open source tool for the building of web corpora with a specific design through a targeted crawling strategy. The tool has been developed within the RIDIRE Project, which aims at creating a 2 billion word balanced web corpus for Italian. RIDIRE-CPI architecture integrates existing open source tools as well as modules developed specifically within the RIDIRE project. It consists of various components: a robust crawler (Heritrix), a user friendly web interface, several conversion and cleaning tools, an anti-duplicate filter, a language guesser, and a PoS tagger. The RIDIRE-CPI user-friendly interface is specifically intended for allowing collaborative work performance by users with low skills in web technology and text processing. Moreover, RIDIRE-CPI integrates a validation interface dedicated to the evaluation of the targeted crawling. Through the content selection, metadata assignment, and validation procedures, the RIDIRE-CPI allows the gathering of linguistic data with a supervised strategy that leads to a higher level of control of the corpus contents. The modular architecture of the infrastructure and its open-source distribution will assure the reusability of the tool for other corpus building initiatives.
This article details work aiming at evaluating the quality of the manual annotation of gene renaming couples in scientific abstracts, which generates sparse annotations. To evaluate these annotations, we compare the results obtained using the commonly advocated inter-annotator agreement coefficients such as S, κ and {\"I
We report about design and characteristics of the LAST MINUTE corpus. The recordings in this data collection are taken from a WOZ experiment that allows to investigate how users interact with a companion system in a mundane situation with the need for planning, re-planning and strategy change. The resulting corpus is distinguished with respect to aspects of size (e.g. number of subjects, length of sessions, number of channels, total length of records) as well as quality (e.g. balancedness of cohort, well designed scenario, standard based transcripts, psychological questionnaires, accompanying in-depth interviews).
This paper presents a first corpus of French annotated for animacy and for verb semantic classes. The resource consists of 1,346 sentences extracted from three different corpora: the French Treebank (Abeill{\'e} and Barrier, 2004), the Est-R{\'e}publicain corpus (CNRTL) and the ESTER corpus (ELRA). It is a set of parsed sentences, containing a verbal head subcategorizing two complements, with annotations on the verb and on both complements, in the TIGER XML format (Mengel and Lezius, 2000). The resource was manually annotated and manually corrected by three annotators. Animacy has been annotated following the categories of Zaenen et al. (2004). Measures of inter-annotator agreement are good (Multi-pi = 0.82 and Multi-kappa = 0.86 (k = 3, N = 2360)). As for verb semantic classes, we used three of the five levels of classification of an existing dictionary: 'Les Verbes du Fran{\c{c}}ais' (Dubois and Dubois-Charlier, 1997). For the higher level (generic classes), the measures of agreement are Multi-pi = 0.84 and Multi-kappa = 0.87 (k = 3, N = 1346). The inter-annotator agreements show that the annotated data are reliable for both animacy and verbal semantic classes.
Unsupervised methods gain more and more attention nowadays in information extraction area, which allows to design more open extraction systems. In the domain of unsupervised information extraction, clustering methods are of particular importance. However, evaluating the results of clustering remains difficult at a large scale, especially in the absence of reliable reference. On the basis of our experiments on unsupervised relation extraction, we first discuss in this article how to evaluate clustering quality without a reference by relying on internal measures. Then we propose a method, supported by a dedicated annotation tool, for building a set of reference clusters of relations from a corpus. Moreover, we apply it to our experimental framework and illustrate in this way how to build a significant reference for unsupervised relation extraction, more precisely made of 80 clusters gathering more than 4,000 relation instances, in a short time. Finally, we present how such reference is exploited for the evaluation of clustering with external measures and analyze the results of the application of these measures to the clusters of relations produced by our unsupervised relation extraction system.
This paper addresses the issue of what approach should be used for building a corpus of sententential paraphrases depending on one's requirements. Six strategies are studied: (1) multiple translations into a single language from another language; (2) multiple translations into a single language from different other languages; (3) multiple descriptions of short videos; (4) multiple subtitles for the same language; (5) headlines for similar news articles; and (6) sub-sentential paraphrasing in the context of a Web-based game. We report results on French for 50 paraphrase pairs collected for all these strategies, where corpora were manually aligned at the finest possible level to define oracle performance in terms of accessible sub-sentential paraphrases. The differences observed will be used as criteria for motivating the choice of a given approach before attempting to build a new paraphrase corpus.
Treebanking a large corpus of relatively structured speech transcribed from various Arabic Broadcast News (BN) sources has allowed us to begin to address the many challenges of annotating and parsing a speech corpus in Arabic. The now completed Arabic Treebank BN corpus consists of 432,976 source tokens (517,080 tree tokens) in 120 files of manually transcribed news broadcasts. Because news broadcasts are predominantly scripted, most of the transcribed speech is in Modern Standard Arabic (MSA). As such, the lexical and syntactic structures are very similar to the MSA in written newswire data. However, because this is spoken news, cross-linguistic speech effects such as restarts, fillers, hesitations, and repetitions are common. There is also a certain amount of dialect data present in the BN corpus, from on-the-street interviews and similar informal contexts. In this paper, we describe the finished corpus and focus on some of the necessary additions to our annotation guidelines, along with some of the technical challenges of a treebanked speech corpus and an initial parsing evaluation for this data. This corpus will be available to the community in 2012 as an LDC publication.
This paper presents a method to elicit spelling error corpora using an online typing race game. After being tested for their native language, English-native participants were instructed to retype stimuli as quickly and as accurately as they could. The participants were informed that the system was keeping a score based on accuracy and speed, and that a high score would result in a position on a public scoreboard. Words were presented on the screen one at a time from a queue, and the queue was advanced by pressing the ENTER key following the stimulus. Responses were recorded and compared to the original stimuli. Responses that differed from the stimuli were considered a typographical or spelling error, and added to an error corpus. Collecting a corpus using a game offers several unique benefits. 1) A game attracts engaged participants, quickly. 2) The web-based delivery reduces the cost and decreases the time and effort of collecting the corpus. 3) Participants have fun. Spelling error corpora have been difficult and expensive to obtain for many languages and this research was performed to fill this gap. In order to evaluate the methodology, we compare our game data against three existing spelling corpora for English.
The aim of this paper is to contribute to the debate on the issues raised by Morphologically Rich Languages, and more precisely to investigate, in a cross-paradigm perspective, the influence of the constituent order on the data-driven parsing of one of such languages(i.e. Italian). It shows therefore new evidence from experiments on Italian, a language characterized by a rich verbal inflection, which leads to a widespread diffusion of the pro―drop phenomenon and to a relatively free word order. The experiments are performed by using state-of-the-art data-driven parsers (i.e. MaltParser and Berkeley parser) and are based on an Italian treebank available in formats that vary according to two dimensions, i.e. the paradigm of representation (dependency vs. constituency) and the level of detail of linguistic information.
The current practice in virtual human dialogue systems is to use professional human recordings or limited-domain speech synthesis. Both approaches lead to good performance but at a high cost. To determine the best trade-off between performance and cost, we perform a systematic evaluation of human and synthesized voices with regard to naturalness, conversational aspect, and likability. We vary the type (in-domain vs. out-of-domain), length, and content of utterances, and take into account the age and native language of raters as well as their familiarity with speech synthesis. We present detailed results from two studies, a pilot one and one run on Amazon's Mechanical Turk. Our results suggest that a professional human voice can supersede both an amateur human voice and synthesized voices. Also, a high-quality general-purpose voice or a good limited-domain voice can perform better than amateur human recordings. We do not find any significant differences between the performance of a high-quality general-purpose voice and a limited-domain voice, both trained with speech recorded by actors. As expected, the high-quality general-purpose voice is rated higher than the limited-domain voice for out-of-domain sentences and lower for in-domain sentences. There is also a trend for long or negative-content utterances to receive lower ratings.
The Berkeley FrameNet Project (BFN, https://framenet.icsi.berkeley.edu/fndrupal/) created descriptions of 73 non-core grammatical constructions, annotation of 50 of these constructions and about 1500 example sentences in its one year project Beyond the Core: A Pilot Project on Cataloging Grammatical Constructions and Multiword Expressions in English supported by the National Science Foundation. The project did not aim at building a full-fledged Construction Grammar, but the registry of English constructions created by this project, which is called Constructicon, provides a representative sample of the current coverage of English constructions (Lee-Goldman {\&} Rhodes 2009). CxN Viewer is a search tool which I have developed for Constructicon and the tool shows its typical English constructions on the web browser. CxN Viewer is a web application consisting of HTML files and JavaScript codes. The tool is a useful program that will benefit researchers working with the data annotated within the framework of BFN. CxN Viewer is a unicode-compliant application, and it can deal with constructions of other languages such as Spanish.
This paper presents work in progress to automatically extract quotation sentences from newspaper articles. The focus is the extraction and annotation of unmarked quotation sentences. A linguistic study shows that unmarked quotation sentences can be formalised into 16 patterns that can be used to develop an extraction grammar. The question of unmarked quotation boundaries identification is also raised as they are often ambiguous. An annotation scheme allowing to describe all the elements that can take place in a quotation sentence is defined. This paper presents the creation of two resources necessary to our system. A dictionary of verbs introducing quotations has been automatically built using a grammar of marked quotations sentences to identify the verbs able to introduce quotations. A grammar formalising the patterns of unmarked quotation sentences ― using the tool Unitex, based on finite state machines ― has been developed. A short experiment has been performed on two patterns and shows some promising results.
Terminology is assigned to play a more and more important role in the Information Society. The need for a computational representation of terminology for IT applications raises new challenges for terminology. Ontology appears to be one of the most suitable solutions for such an issue. But an ontology is not a terminology as well as a terminology is not an ontology. Terminology, especially for technical domains, relies on two different semiotic systems: the linguistic one, which is directly linked to the Language for Special Purposes and the conceptual system that describes the domain knowledge. These two systems must be both separated and linked. The new paradigm of ontoterminology, i.e. a terminology whose conceptual system is a formal ontology, emphasizes the difference between the linguistic and conceptual dimensions of terminology while unifying them. A double semantic triangle is introduced in order to link terms (signifiers) to concept names on a first hand and meanings (signified) to concepts on the other hand. Such an approach allows two kinds of definition to be introduced. The definition of terms written in natural language is considered as a linguistic explanation while the definition of concepts written in a formal language is viewed as a formal specification that allows operationalization of terminology.
Annotation studies in CL are generally unscientific: they are mostly not reproducible, make use of too few (and often non-independent) annotators and use guidelines that are often something of a moving target. Additionally, the notion of expert annotators' invariably means only that the annotators have linguistic training. While this can be acceptable in some special contexts, it is often far from ideal. This is particularly the case when subtle judgements are required or when, as increasingly, one is making use of corpora originating from technical texts that have been produced by, and intended to be consumed by, an audience of technical experts in the field. We outline a more rigorous approach to collecting human annotations, using as our example a study designed to capture judgements on the meaning of hedge words in medical records.
The DBpedia project extracts structured information from Wikipedia editions in 97 different languages and combines this information into a large multi-lingual knowledge base covering many specific domains and general world knowledge. The knowledge base contains textual descriptions (titles and abstracts) of concepts in up to 97 languages. It also contains structured knowledge that has been extracted from the infobox systems of Wikipedias in 15 different languages and is mapped onto a single consistent ontology by a community effort. The knowledge base can be queried using the SPARQL query language and all its data sets are freely available for download. In this paper, we describe the general DBpedia knowledge base and as well as the DBpedia data sets that specifically aim at supporting computational linguistics tasks. These task include Entity Linking, Word Sense Disambiguation, Question Answering, Slot Filling and Relationship Extraction. These use cases are outlined, pointing at added value that the structured data of DBpedia provides.
This paper reports on the design and implementation of a morphological analyzer for Wolof. The main motivation for this work is to obtain a linguistically motivated tool using finite-state techniques. The finite-state technology is especially attractive in dealing with human language morphologies. Finite-state transducers (FST) are fast, efficient and can be fully reversible, enabling users to perform analysis as well as generation. Hence, I use this approach to construct a new FST tool for Wolof, as a first step towards a computational grammar for the language in the Lexical Functional Grammar framework. This article focuses on the methods used to model complex morphological issues and on developing strategies to limit ambiguities. It discusses experimental evaluations conducted to assess the performance of the analyzer with respect to various statistical criteria. In particular, I also wanted to create morphosyntactically annotated resources for Wolof, obtained by automatically analyzing text corpora with a computational morphology.
This article summarizes the evaluation process of an interface under development to consult an oral corpus of learners of Spanish as a Foreign Language. The databank comprises 40 interviews with students with over 9 different mother tongues collected for Error Analysis. XML mark-up is used to code the information about the learners and their errors (with an explanation), and the search tool makes it is possible to look up these errors and to listen to the utterances where they appear. The formative evaluation was performed to improve the interface during the design stage by means of a questionnaire which addressed issues related to the teachers' beliefs about languages, their opinion about the Error Analysis methodology, and specific points about the interface design and usability. The results unveiled some deficiencies of the current prototype as well as the interests of the teaching professionals which should be considered to bridge the gap between technology development and its pedagogical applications.
This paper discusses the design and compilation of the TRIS corpus, a specialized parallel corpus of Spanish and German texts. It will be used for phraseological research aimed at improving statistical machine translation. The corpus is based on the European database of Technical Regulations Information System (TRIS), containing 995 original documents written in German and Spanish and their translations into Spanish and German respectively. This parallel corpus is under development and the first version with 97 aligned file pairs was released in the first META-NORD upload of metadata and resources in November 2011. The second version of the corpus, described in the current paper, contains 205 file pairs which have been completely aligned at sentence level, which account for approximately 1,563,000 words and 70,648 aligned sentence pairs.
Dialectal Arabic (DA) refers to the day-to-day vernaculars spoken in the Arab world. DA lives side-by-side with the official language, Modern Standard Arabic (MSA). DA differs from MSA on all levels of linguistic representation, from phonology and morphology to lexicon and syntax. Unlike MSA, DA has no standard orthography since there are no Arabic dialect academies, nor is there a large edited body of dialectal literature that follows the same spelling standard. In this paper, we present CODA, a conventional orthography for dialectal Arabic; it is designed primarily for the purpose of developing computational models of Arabic dialects. We explain the design principles of CODA and provide a detailed description of its guidelines as applied to Egyptian Arabic.
This paper describes the status of the standardization efforts of a Component Metadata approach for describing Language Resources with metadata. Different linguistic and Language {\&} Technology communities as CLARIN, META-SHARE and NaLiDa use this component approach and see its standardization of as a matter for cooperation that has the possibility to create a large interoperable domain of joint metadata. Starting with an overview of the component metadata approach together with the related semantic interoperability tools and services as the ISOcat data category registry and the relation registry we explain the standardization plan and efforts for component metadata within ISO TC37/SC4. Finally, we present information about uptake and plans of the use of component metadata within the three mentioned linguistic and L{\&}T communities.
The emergence of crowdsourcing as a commonly used approach to collect vast quantities of human assessments on a variety of tasks represents nothing less than a paradigm shift. This is particularly true in academic research where it has suddenly become possible to collect (high-quality) annotations rapidly without the need of an expert. In this paper we investigate factors which can influence the quality of the results obtained through Amazon's Mechanical Turk crowdsourcing platform. We investigated the impact of different presentation methods (free text versus radio buttons), workers' base (USA versus India as the main bases of MTurk workers) and payment scale (about {\$}4, {\$}8 and {\$}10 per hour) on the quality of the results. For each run we assessed the results provided by 25 workers on a set of 10 tasks. We run two different experiments using objective tasks: maths and general text questions. In both tasks the answers are unique, which eliminates the uncertainty usually present in subjective tasks, where it is not clear whether the unexpected answer is caused by a lack of worker's motivation, the worker's interpretation of the task or genuine ambiguity. In this work we present our results comparing the influence of the different factors used. One of the interesting findings is that our results do not confirm previous studies which concluded that an increase in payment attracts more noise. We also find that the country of origin only has an impact in some of the categories and only in general text questions but there is no significant difference at the top pay.
This paper introduces association norms of German noun compounds as a lexical semantic resource for cognitive and computational linguistics research on compositionality. Based on an existing database of German noun compounds, we collected human associations to the compounds and their constituents within a web experiment. The current study describes the collection process and a part-of-speech analysis of the association resource. In addition, we demonstrate that the associations provide insight into the semantic properties of the compounds, and perform a case study that predicts the degree of compositionality of the experiment compound nouns, as relying on the norms. Applying a comparatively simple measure of association overlap, we reach a Spearman rank correlation coefficient of rs=0.5228; p{\textless}000001, when comparing our predictions with human judgements.
Word sketches are one-page, automatic, corpus-based summaries of a word's grammatical and collocational behaviour. In this paper we present word sketches for Turkish. Until now, word sketches have been generated using a purpose-built finite-state grammars. Here, we use an existing dependency parser. We describe the process of collecting a 42 million word corpus, parsing it, and generating word sketches from it. We evaluate the word sketches in comparison with word sketches from a language independent sketch grammar on an external evaluation task called topic coherence, using Turkish WordNet to derive an evaluation set of coherent topics.
This article describes the morphosyntactic annotation of the C-ORAL-BRASIL speech corpus, using an adapted version of the Palavras parser. In order to achieve compatibility with annotation rules designed for standard written Portuguese, transcribed words were orthographically normalized, and the parsing lexicon augmented with speech-specific material, phonetically spelled abbreviations etc. Using a two-level annotation approach, speech flow markers like overlaps, retractions and non-verbal productions were separated from running, annotatable text. In the absence of punctuation, syntactic segmentation was achieved by exploiting prosodic break markers, enhanced by a rule-based distinctions between pause and break functions. Under optimal conditions, the modified parsing system achieved correctness rates (F-scores) of 98.6{\%} for part of speech, 95{\%} for syntactic function and 99{\%} for lemmatization. Especially at the syntactic level, a clear connection between accessibility of prosodic break markers and annotation performance could be documented.
The paper presents the methodology and the outcome of the compilation and the processing of the Bulgarian X-language Parallel Corpus (Bul-X-Cor) which was integrated as part of the Bulgarian National Corpus (BulNC). We focus on building representative parallel corpora which include a diversity of domains and genres, reflect the relations between Bulgarian and other languages and are consistent in terms of compilation methodology, text representation, metadata description and annotation conventions. The approaches implemented in the construction of Bul-X-Cor include using readily available text collections on the web, manual compilation (by means of Internet browsing) and preferably automatic compilation (by means of web crawling ― general and focused). Certain levels of annotation applied to Bul-X-Cor are taken as obligatory (sentence segmentation and sentence alignment), while others depend on the availability of tools for a particular language (morpho-syntactic tagging, lemmatisation, syntactic parsing, named entity recognition, word sense disambiguation, etc.) or for a particular task (word and clause alignment). To achieve uniformity of the annotation we have either annotated raw data from scratch or transformed the already existing annotation to follow the conventions accepted for BulNC. Finally, actual uses of the corpora are presented and conclusions are drawn with respect to future work.
The MASC project has produced a multi-genre corpus with multiple layers of linguistic annotation, together with a sentence corpus containing WordNet 3.1 sense tags for 1000 occurrences of each of 100 words produced by multiple annotators, accompanied by indepth inter-annotator agreement data. Here we give an overview of the contents of MASC and then focus on the word sense sentence corpus, describing the characteristics that differentiate it from other word sense corpora and detailing the inter-annotator agreement studies that have been performed on the annotations. Finally, we discuss the potential to grow the word sense sentence corpus through crowdsourcing and the plan to enhance the content and annotations of MASC through a community-based collaborative effort.
How do people behave in their everyday information seeking tasks, which often involve Wikipedia? Are there systems which can help them, or do a similar job? In this paper we describe P{\'a}gico, an evaluation contest with the main purpose of fostering research in these topics. We describe its motivation, the collection of documents created, the evaluation setup, the topics chosen and their choice, the participation, as well as the measures used for evaluation and the gathered resources. The task―between information retrieval and question answering―can be further described as answering questions related to Portuguese-speaking culture in the Portuguese Wikipedia, in a number of different themes and geographic and temporal angles. This initiative allowed us to create interesting datasets and perform some assessment of Wikipedia, while also improving a public-domain open-source system for further wikipedia-based evaluations. In the paper, we provide examples of questions, we report the results obtained by the participants, and provide some discussion on complex issues.
There has been much debate, both theoretical and practical, on how to link ontologies and lexicons in natural language processing (NLP) applications. In this paper, we focus on an application in which lexicon and ontology are used to generate teaching material. We briefly describe the application (a serious game for language learning). We then zoom in on the representation and interlinking of the lexicon and of the ontology. We show how the use of existing standards and of good practice principles facilitates the design of our resources while satisfying the expressivity requirements set by natural language generation.
In this paper, we describe an ongoing effort in collecting and annotating a multilingual speech database of natural stress emotion from university students. The goal is to detect natural stress emotions and study the stress expression differences in different languages, which may help psychologists in the future. We designed a common questionnaire of stress-inducing and non-stress-inducing questions in English, Mandarin and Cantonese and collected a first ever, multilingual corpus of natural stress emotion. All of the students are native speakers of the corresponding language. We asked native language speakers to annotate recordings according to the participants' self-label states and obtained a very good kappa inter labeler agreement. We carried out human perception tests where listeners who do not understand Chinese were asked to detect stress emotion from the Mandarin Chinese database. Compared to the annotation labels, these human perceived emotions are of low accuracy, which shows a great necessity for natural stress detection research.
The Twins corpus is a collection of utterances spoken in interactions with two virtual characters who serve as guides at the Museum of Science in Boston. The corpus contains about 200,000 spoken utterances from museum visitors (primarily children) as well as from trained handlers who work at the museum. In addition to speech recordings, the corpus contains the outputs of speech recognition performed at the time of utterance as well as the system interpretation of the utterances. Parts of the corpus have been manually transcribed and annotated for question interpretation. The corpus has been used for improving performance of the museum characters and for a variety of research projects, such as phonetic-based Natural Language Understanding, creation of conversational characters from text resources, dialogue policy learning, and research on patterns of user interaction. It has the potential to be used for research on children's speech and on language used when talking to a virtual human.
Web provides a large-scale corpus for researchers to study the language usages in real world. Developing a web-scale corpus needs not only a lot of computation resources, but also great efforts to handle the large variations in the web texts, such as character encoding in processing Chinese web texts. In this paper, we aim to develop a web-scale Chinese word N-gram corpus with parts of speech information called NTU PN-Gram corpus using the ClueWeb09 dataset. We focus on the character encoding and some Chinese-specific issues. The statistics about the dataset is reported. We will make the resulting corpus a public available resource to boost the Chinese language processing.
This paper tests two different strategies for medical term extraction in an Arabic Medical Corpus. The experiments and the corpus are developed within the framework of Multimedica project funded by the Spanish Ministry of Science and Innovation and aiming at developing multilingual resources and tools for processing of newswire texts in the Health domain. The first experiment uses a fixed list of medical terms, the second experiment uses a list of Arabic equivalents of very limited list of common Latin prefix and suffix used in medical terms. Results show that using equivalents of Latin suffix and prefix outperforms the fixed list. The paper starts with an introduction, followed by a description of the state-of-art in the field of Arabic Medical Language Resources (LRs). The third section describes the corpus and its characteristics. The fourth and the fifth sections explain the lists used and the results of the experiments carried out on a sub-corpus for evaluation. The last section analyzes the results outlining the conclusions and future work.
Human motion is challenging to analyze due to the many degrees of freedom of the human body. While the qualitative analysis of human motion lies at the core of many research fields, including multimodal communication, it is still hard to achieve reliable results when human coders transcribe motion with abstract categories. In this paper we tackle this problem in two respects. First, we provide facilities for qualitative and quantitative comparison of annotations. Second, we provide facilities for exploring highly precise recordings of human motion (motion capture) using a low-cost consumer device (Kinect). We present visualization and analysis methods, integrated in the existing ANVIL video annotation tool (Kipp 2001), and provide both a precision analysis and a ''''''``cookbook'''''''' for Kinect-based motion analysis.
This paper presents an approach to extract translation equivalents from comparable corpora for polysemous nouns. As opposed to the standard approaches that build a single context vector for all occurrences of a given headword, we first disambiguate the headword with third-party sense taggers and then build a separate context vector for each sense of the headword. Since state-of-the-art word sense disambiguation tools are still far from perfect, we also tried to improve the results by combining the sense assignments provided by two different sense taggers. Evaluation of the results shows that we outperform the baseline (0.473) in all the settings we experimented with, even when using only one sense tagger, and that the best-performing results are indeed obtained by taking into account the intersection of both sense taggers (0.720).
Arabic is a language known for its rich and complex morphology. Although many research projects have focused on the problem of Arabic morphological analysis using different techniques and approaches, very few have addressed the issue of generation of fully inflected words for the purpose of text authoring. Available open-source spell checking resources for Arabic are too small and inadequate. Ayaspell, for example, the official resource used with OpenOffice applications, contains only 300,000 fully inflected words. We try to bridge this critical gap by creating an adequate, open-source and large-coverage word list for Arabic containing 9,000,000 fully inflected surface words. Furthermore, from a large list of valid forms and invalid forms we create a character-based tri-gram language model to approximate knowledge about permissible character clusters in Arabic, creating a novel method for detecting spelling errors. Testing of this language model gives a precision of 98.2{\%} at a recall of 100{\%}. We take our research a step further by creating a context-independent spelling correction tool using a finite-state automaton that measures the edit distance between input words and candidate corrections, the Noisy Channel Model, and knowledge-based rules. Our system performs significantly better than Hunspell in choosing the best solution, but it is still below the MS Spell Checker.
In this paper, we propose a new scheme for annotating response tokens (RTs) and their triggering expressions in Japanese multi-party conversations. In the proposed scheme, RTs are first identified and classified according to their forms, and then sub-classified according to their sequential positions in the discourse. To deeply study the contexts in which RTs are used, the scheme also provides procedures for annotating triggering expressions, which are considered to trigger the listener's production of RTs. RTs are classified according to whether or not there is a particular object or proposition in the speaker's turn for which the listener shows a positive or aligned stance. Triggering expressions are then identified in the speaker's turn; they include surprising facts and other newsworthy things, opinions and assessments, focus of a response to a question or repair initiation, keywords in narratives, and embedded propositions quoted from other's statement or thought, which are to be agreed upon, assessed, or noticed. As an illustrative application of our scheme, we present a preliminary analysis on the distribution of the latency of the listener's response to the triggering expression, showing how it differs according to RT's forms and positions.
The aim of our software presentation is to demonstrate that corpus-driven bilingual dictionaries generated fully by automatic means are suitable for human use. Need for such dictionaries shows up specifically in the case of lesser used languages where due to the low demand it does not pay off for publishers to invest into the production of dictionaries. Previous experiments have proven that bilingual lexicons can be created by applying word alignment on parallel corpora. Such an approach, especially the corpus-driven nature of it, yields several advantages over more traditional approaches. Most importantly, automatically attained translation probabilities are able to guarantee that the most frequently used translations come first within an entry. However, the proposed technique have to face some difficulties, as well. In particular, the scarce availability of parallel texts for medium density languages imposes limitations on the size of the resulting dictionary. Our objective is to design and implement a dictionary building workflow and a query system that is apt to exploit the additional benefits of the method and overcome the disadvantages of it.
This study was carried out to improve the quality of acted emotional speech. In the recent paradigm shift in speech collection techniques, methods for the collection of high-quality and spontaneous speech has been strongly focused on. However, such methods involve various constraints: such as the difficulty in controlling utterances and sound quality. Hence, our study daringly focuses on acted speech because of its high operability. In this paper, we propose a new method for speech collection by refining acting scripts. We compared the speech collected using our proposed method and that collected using an imitation of the legacy method that was implemented with traditional basic emotional words. The results show the advantage of our proposed method, i.e., the possibility of the generating high F0 fluctuations in acoustical expressions, which is one of the important features of the expressive speech, while ensuring that there is no decline in the naturalness and other psychological features.
Although the possibility of referring or citing on-line data from publications is seen at least theoretically as an important means to provide immediate testable proof or simple illustration of a line of reasoning, the practice has not been wide-spread yet and no extensive experience has been gained about the possibilities and problems of referring to raw data-sets. This paper makes a case to investigate the possibility and need of persistent data visualization services that facilitate the inspection and evaluation of the cited data.
This paper presents the results of an approach to automatically acquire large-scale, probabilistic Lexical-Functional Grammar (LFG) resources for Arabic from the Penn Arabic Treebank (ATB). Our starting point is the earlier, work of (Tounsi et al., 2009) on automatic LFG f(eature)-structure annotation for Arabic using the ATB. They exploit tree configuration, POS categories, functional tags, local heads and trace information to annotate nodes with LFG feature-structure equations. We utilize this annotation to automatically acquire grammatical function (dependency) based subcategorization frames and paths linking long-distance dependencies (LDDs). Many state-of-the-art treebank-based probabilistic parsing approaches are scalable and robust but often also shallow: they do not capture LDDs and represent only local information. Subcategorization frames and LDD paths can be used to recover LDDs from such parser output to capture deep linguistic information. Automatic acquisition of language resources from existing treebanks saves time and effort involved in creating such resources by hand. Moreover, data-driven automatic acquisition naturally associates probabilistic information with subcategorization frames and LDD paths. Finally, based on the statistical distribution of LDD path types, we propose empirical bounds on traditional regular expression based functional uncertainty equations used to handle LDDs in LFG.
This paper evaluates the impact of external lexical resources into a CRF-based joint Multiword Segmenter and Part-of-Speech Tagger. We especially show different ways of integrating lexicon-based features in the tagging model. We display an absolute gain of 0.5{\%} in terms of f-measure. Moreover, we show that the integration of lexicon-based features significantly compensates the use of a small training corpus.
Direct quotations from business leaders can provide a rich sample of language which is in common use in the world of commerce. This language used by business leaders often uses: metaphors, euphemisms, slang, obscenities and invented words. In addition the business lexicon is dynamic because new words or terms will gain popularity with businessmen whilst obsolete words will exit their common vocabulary. In addition to being a rich source of language direct quotations from business leaders can have ''real world'' consequences. For example, Gerald Ratner nearly bankrupted his company with an infamous candid comment at an Institute of Directors meeting in 1993. Currently, there is no ''direct quotations from business leaders'' resource freely available to the research community. The ''Minho Quotation Resource'' captures the business lexicon with in excess of 500,000 quotations from individuals from the business world. The quotations were captured from October 2009 and April 2011. The resource is available in a searchable Lucene index and will be available for download in May 2012
This paper presents a novel implementation of Translog-II. Translog-II is a Windows-oriented program to record and study reading and writing processes on a computer. In our research, it is an instrument to acquire objective, digital data of human translation processes. As their predecessors, Translog 2000 and Translog 2006, also Translog-II consists of two main components: Translog-II Supervisor and Translog-II User, which are used to create a project file, to run a text production experiments (a user reads, writes or translates a text) and to replay the session. Translog produces a log files which contains all user activity data of the reading, writing, or translation session, and which can be evaluated by external tools. While there is a large body of translation process research based on Translog, this paper gives an overview of the Translog-II functions and its data visualization options.
This paper describes the construction and usage of the MOR and GRASP programs for part of speech tagging and syntactic dependency analysis of the corpora in the CHILDES and TalkBank databases. We have written MOR grammars for 11 languages and GRASP analyses for three. For English data, the MOR tagger reaches 98{\%} accuracy on adult corpora and 97{\%} accuracy on child language corpora. The paper discusses the construction of MOR lexicons with an emphasis on compounds and special conversational forms. The shape of rules for controlling allomorphy and morpheme concatenation are discussed. The analysis of bilingual corpora is illustrated in the context of the Cantonese-English bilingual corpora. Methods for preparing data for MOR analysis and for developing MOR grammars are discussed. We believe that recent computational work using this system is leading to significant advances in child language acquisition theory and theories of grammar identification more generally.
In the authorship identification task, examples of short writings of N authors and an anonymous document written by one of these N authors are given. The task is to determine the authorship of the anonymous text. Practically all approaches solved this problem with machine learning methods. The input attributes for the machine learning process are usually formed by stylistic or grammatical properties of individual documents or a defined similarity between a document and an author. In this paper, we present the results of an experiment to extend the machine learning attributes by ranking the similarity between a document and an author: we transform the similarity between an unknown document and one of the N authors to the order in which the author is the most similar to the document in the set of N authors. The comparison of similarity probability and similarity ranking was made using the Support Vector Machines algorithm. The results show that machine learning methods perform slightly better with attributes based on the ranking of similarity than with previously used similarity between an author and a document.
The present paper describes an ongoing effort to compile and annotate a large corpus of computer-mediated communication (CMC) in Hindi. It describes the process of the compilation of the corpus, the basic structure of the corpus and the annotation of the corpus and the challenges faced in the creation of such a corpus. It also gives a description of the technologies developed for the processing of the data, addition of the metadata and annotation of the corpus. Since it is a corpus of written communication, it provides quite a distinctive challenge for the annotation process. Besides POS annotation, it will also be annotated at higher levels of representation. Once completely developed it will be a very useful resource of Hindi for research in the areas of linguistics, NLP and other social sciences research related to communication, particularly computer-mediated communication..Besides this the challenges discussed here and the way they are tackled could be taken as the model for developing the corpus of computer-mediated communication in other Indian languages. Furthermore the technologies developed for the construction of this corpus will also be made available publicly.
The aim of this paper is to introduce LexIt, a computational framework for the automatic acquisition and exploration of distributional information about Italian verbs, nouns and adjectives, freely available through a web interface at the address http://sesia.humnet.unipi.it/lexit. LexIt is the first large-scale resource for Italian in which subcategorization and semantic selection properties are characterized fully on distributional ground: in the paper we describe both the process of data extraction and the evaluation of the subcategorization frames extracted with LexIt.
In this paper, we present an annotation campaign of football (soccer) matches, from a heterogeneous text corpus of both match minutes and video commentary transcripts, in French. The data, annotations and evaluation process are detailed, and the quality of the annotated corpus is discussed. In particular, we propose a new technique to better estimate the annotator agreement when few elements of a text are to be annotated. Based on that, we show how the source medium influenced the process and the quality.
C-ORAL-BRASIL I is a Brazilian Portuguese spontaneous speech corpus compiled following the same architecture adopted by the C-ORAL-ROM resource. The main goal is the documentation of the diaphasic and diastratic variations in Brazilian Portuguese. The diatopic variety represented is that of the metropolitan area of Belo Horizonte, capital city of Minas Gerais. Even though it was not a primary goal, a nice balance was achieved in terms of speakers' diastratic features (sex, age and school level). The corpus is entirely dedicated to informal spontaneous speech and comprises 139 informal speech texts, 208,130 words and 21:08:52 hours of recording, distributed into family/private (80{\%}) and public (20{\%}) contexts. The LR includes audio files, transcripts in text format and text-to-speech alignment (accessible with WinPitch Pro software). C-ORAL-BRASIL I also provides transcripts with Part-of-Speech annotation implemented through the parser system Palavras. Transcripts were validated regarding the proper application of transcription criteria and also for the annotation of prosodic boundaries. Some quantitative features of C-ORAL-BRASIL I in comparison with the informal C-ORAL-ROM are reported.
Statistical Machine Translation (SMT) relies on the availability of rich parallel corpora. However, in the case of under-resourced languages, parallel corpora are not readily available. To overcome this problem previous work has recognized the potential of using comparable corpora as training data. The process of obtaining such data usually involves (1) downloading a separate list of documents for each language, (2) matching the documents between two languages usually by comparing the document contents, and finally (3) extracting useful data for SMT from the matched document pairs. This process requires a large amount of time and resources since a huge volume of documents needs to be downloaded to increase the chances of finding good document pairs. In this work we aim to reduce the amount of time and resources spent for tasks 1 and 2. Instead of obtaining full documents we first obtain just titles along with some meta-data such as time and date of publication. Titles can be obtained through Web Search and RSS News feed collections so that download of the full documents is not needed. We show experimentally that titles can be used to approximate the comparison between documents using full document contents.
We present work in progress aiming to build tools for the normalization of User-Generated Content (UGC). As we will see, the task requires the revisiting of the initial steps of NLP processing, since UGC (micro-blog, blog, and, generally, Web 2.0 user texts) presents a number of non-standard communicative and linguistic characteristics, and is in fact much closer to oral and colloquial language than to edited text. We present and characterize a corpus of UGC text in Spanish from three different sources: Twitter, consumer reviews and blogs. We motivate the need for UGC text normalization by analyzing the problems found when processing this type of text through a conventional language processing pipeline, particularly in the tasks of lemmatization and morphosyntactic tagging, and finally we propose a strategy for automatically normalizing UGC using a selector of correct forms on top of a pre-existing spell-checker.
Language resources extracted from structured data (e.g. Linked Open Data) have already been used in various scenarios to improve conventional Natural Language Processing techniques. The meanings of words and the relations between them are made more explicit in RDF graphs, in comparison to human-readable text, and hence have a great potential to improve legacy applications. In this paper, we describe an approach that can be used to extend or clarify the semantic meaning of a word by constructing a list of contextually related terms. Our approach is based on exploiting the structure inherent in an RDF graph and then applying the methods from statistical semantics, and in particular, Random Indexing, in order to discover contextually related terms. We evaluate our approach in the domain of life science using the dataset generated with the help of domain experts from a large pharmaceutical company (AstraZeneca). They were involved in two phases: firstly, to generate a set of keywords of interest to them, and secondly to judge the set of generated contextually similar words for each keyword of interest. We compare our proposed approach, exploiting the semantic graph, with the same method applied on the human readable text extracted from the graph.
This paper describes an empirical study of coreference in spoken vs. written text. We focus on the comparison of two particular text types, interviews and popular science texts, as instances of spoken and written texts since they display quite different discourse structures. We believe in fact, that the correlation of difficulties in coreference resolution and varying discourse structures requires a deeper analysis that accounts for the diversity of coreference strategies or their sub-phenomena as indicators of text type or genre. In this work, we therefore aim at defining specific parameters that classify differences in genres of spoken and written texts such as the preferred segmentation strategy, the maximal allowed distance in or the length and size of coreference chains as well as the correlation of structural and syntactic features of coreferring expressions. We argue that a characterization of such genre dependent parameters might improve the performance of current state-of-art coreference resolution technology.
Building speech corpora is a first and crucial step for every text-to-speech synthesis system. Nowadays, the use of statistical models implies the use of huge sized corpora that need to be recorded, transcribed, annotated and segmented to be usable. The variety of corpora necessary for recent applications (content, style, etc.) makes the use of existing digital audio resources very attractive. Among all available resources, audiobooks, considering their quality, are interesting. Considering this framework, we propose a complete acquisition, segmentation and annotation chain for audiobooks that tends to be fully automatic. The proposed process relies on a data structure, Roots, that establishes the relations between the different annotation levels represented as sequences of items. This methodology has been applied successfully on 11 hours of speech extracted from an audiobook. A manual check, on a part of the corpus, shows the efficiency of the process.
Motivation: Gold Standards for named entities are, ironically, not standard themselves. Some specify the one perfect annotation. Others specify perfectly good alternatives. The concept of Silver standard is relatively new. The objective is consensus rather than perfection. How should the two concepts be best represented and related? Approach: We examine several Biomedical Gold Standards and motivate a new representational format, centroids, which simply and effectively represents name distributions. We define an algorithm for finding centroids, given a set of alternative input annotations and we test the outputs quantitatively and qualitatively. We also define a metric of relatively acceptability on top of the centroid standard. Results: Precision, recall and F-scores of over 0.99 are achieved for the simple sanity check of giving the algorithm Gold Standard inputs. Qualitative analysis of the differences very often reveals errors and incompleteness in the original Gold Standard. Given automatically generated annotations, the centroids effectively represent the range of those contributions and the quality of the centroid annotations is highly competitive with the best of the contributors. Conclusion: Centroids cleanly represent alternative name variations for Silver and Gold Standards. A centroid Silver Standard is derived just like a Gold Standard, only from imperfect inputs.
In this article, we compare feedback-related multimodal behaviours in two different types of interactions: first encounters between two participants who do not know each other in advance, and naturally-occurring conversations between two and three participants recorded at their homes. All participants are Danish native speakers. The interactions are transcribed using the same methodology, and the multimodal behaviours are annotated according to the same annotation scheme. In the study we focus on the most frequently occurring feedback expressions in the interactions and on feedback-related head movements and facial expressions. The analysis of the corpora, while confirming general facts about feedback-related head movements and facial expressions previously reported in the literature, also shows that the physical setting, the number of participants, the topics discussed, and the degree of familiarity influence the use of gesture types and the frequency of feedback-related expressions and gestures.
Innovations in localisation have focused on the collection and leverage of language resources. However, smaller localisation clients and Language Service Providers are poorly positioned to exploit the benefits of language resource reuse in comparison to larger companies. Their low throughput of localised content means they have little opportunity to amass significant resources, such as Translation memories and Terminology databases, to reuse between jobs or to train statistical machine translation engines tailored to their domain specialisms and language pairs. We propose addressing this disadvantage via the sharing and pooling of language resources. However, the current localisation standards do not support multiparty sharing, are not well integrated with emerging language resource standards and do not address key requirements in determining ownership and license terms for resources. We survey standards and research in the area of Localisation, Language Resources and Language Technologies to leverage existing localisation standards via Linked Data methodologies. This points to the potential of using semantic representation of existing data models for localisation workflow metadata, terminology, parallel text, provenance and access control, which we illustrate with an RDF example.
This paper presents a system that is designed to make possible the organization and search within the collected digitized material of intangible cultural heritage. The motivation for building the system was a vast quantity of multimedia documents collected by a team from the Institute for Balkan Studies in Belgrade. The main topic of their research were linguistic properties of speeches that are used in various places in the Balkans by different groups of people. This paper deals with a prototype system that enables the annotation of the collected material and its organization into a native XML database through a graphical interface. The system enables the search of the database and the presentation of digitized multimedia documents and spatial as well as non-spatial information of the queried data. The multimedia content can be read, listened to or watched while spatial properties are presented on the graphics that consists of geographic regions in the Balkans. The system also enables spatial queries by consulting the graph of geographic regions.
We introduce ANALEC, a tool which aim is to bring together corpus annotation, visualization and query management. Our main idea is to provide a unified and dynamic way of annotating textual data. ANALEC allows researchers to dynamically build their own annotation scheme and use the possibilities of scheme revision, data querying and graphical visualization during the annotation process. Each query result can be visualized using a graphical representation that puts forward a set of annotations that can be directly corrected or completed. Text annotation is then considered as a cyclic process. We show that statistics like frequencies and correlations make it possible to verify annotated data on the fly during the annotation. In this paper we introduce the annotation functionalities of ANALEC, some of the annotated data visualization functionalities, and three statistical modules: frequency, correlation and geometrical representations. Some examples dealing with reference and coreference annotation illustrate the main contributions of ANALEC.
The paper compares how feedback is expressed via speech and head movements in comparable corpora of first encounters in three Nordic languages: Danish, Finnish and Swedish. The three corpora have been collected following common guidelines, and they have been annotated according to the same scheme in the NOMCO project. The results of the comparison show that in this data the most frequent feedback-related head movement is Nod in all three languages. Two types of Nods were distinguished in all corpora: Down-nods and Up-nods; the participants from the three countries use Down- and Up-nods with different frequency. In particular, Danes use Down-nods more frequently than Finns and Swedes, while Swedes use Up-nods more frequently than Finns and Danes. Finally, Finns use more often single Nods than repeated Nods, differing from the Swedish and Danish participants. The differences in the frequency of both Down-nods and Up-Nods in the Danish, Finnish and Swedish interactions are interesting given that Nordic countries are not only geographically near, but are also considered to be very similar culturally. Finally, a comparison of feedback-related words in the Danish and Swedish corpora shows that Swedes and Danes use common feedback words corresponding to yes and no with similar frequency.
This paper presents an approach to construction of an annotated corpus for German political news for the opinion mining task. The annotated corpus has been applied to learn relation extraction rules for extraction of opinion holders, opinion content and classification of polarities. An adapted annotated schema has been developed on top of the state-of-the-art research. Furthermore, a general tool for annotating relations has been utilized for the annotation task. An evaluation of the inter-annotator agreement has been conducted. The rule learning is realized with the help of a minimally supervised machine learning framework DARE.
MultiUN is a multilingual parallel corpus extracted from the official documents of the United Nations. It is available in the six official languages of the UN and a small portion of it is also available in German. This paper presents a major update on the first public version of the corpus released in 2010. This version 2 consists of over 513,091 documents, including more than 9{\%} of new documents retrieved from the United Nations official document system. We applied several modifications to the corpus preparation method. In this paper, we describe the methods we used for processing the UN documents and aligning the sentences. The most significant improvement compared to the previous release is the newly added multilingual sentence alignment information. The alignment information is encoded together with the text in XML instead of additional files. Our representation of the sentence alignment allows quick construction of aligned texts parallel in arbitrary number of languages, which is essential for building machine translation systems.
Text summarization and information extraction systems require adaptation to new domains and languages. This adaptation usually depends on the availability of language resources such as corpora. In this paper we present a comparable corpus in Spanish and English for the study of cross-lingual information extraction and summarization: the CONCISUS Corpus. It is a rich human-annotated dataset composed of comparable event summaries in Spanish and English covering four different domains: aviation accidents, rail accidents, earthquakes, and terrorist attacks. In addition to the monolingual summaries in English and Spanish, we provide automatic translations and ``comparable'' full event reports of the events. The human annotations are concepts marked in the textual sources representing the key event information associated to the event type. The dataset has also been annotated using text processing pipelines. It is being made freely available to the research community for research purposes.
Language resources that include semantic equivalences at word level are common, and its usefulness is well established in text processing applications, as in the case of search. Named entities also play an important role for text based applications, but are not usually covered by the previously mentioned resources. The present work describes the WES base, Wikipedia Entity Synonym base, a freely available resource based on the Wikipedia. The WES base was built for the Portuguese Language, with the same format of another freely available thesaurus for the same language, the TeP base, which allows integration of equivalences both at word level and entity level. The resource has been built in a language independent way, so that it can be extended to different languages. The WES base was used in a Question Answering system, enhancing significantly its performance.
This paper describes the creation process of an Indonesian-English parallel corpus (IDENTIC). The corpus contains 45,000 sentences collected from different sources in different genres. Several manual text preprocessing tasks, such as alignment and spelling correction, are applied to the corpus to assure its quality. We also apply language specific text processing such as tokenization on both sides and clitic normalization on the Indonesian side. The corpus is available in two different formats: plain', stored in text format and morphologically enriched', stored in CoNLL format. Some parts of the corpus are publicly available at the IDENTIC homepage.
CzEng 1.0 is an updated release of our Czech-English parallel corpus, freely available for non-commercial research or educational purposes. In this release, we approximately doubled the corpus size, reaching 15 million sentence pairs (about 200 million tokens per language). More importantly, we carefully filtered the data to reduce the amount of non-matching sentence pairs. CzEng 1.0 is automatically aligned at the level of sentences as well as words. We provide not only the plain text representation, but also automatic morphological tags, surface syntactic as well as deep syntactic dependency parse trees and automatic co-reference links in both English and Czech. This paper describes key properties of the released resource including the distribution of text domains, the corpus data formats, and a toolkit to handle the provided rich annotation. We also summarize the procedure of the rich annotation (incl. co-reference resolution) and of the automatic filtering. Finally, we provide some suggestions on exploiting such an automatically annotated sentence-parallel corpus.
This paper describes the underlying software platform used to develop and publish annotations for the Quranic Arabic Corpus (QAC). The QAC (Dukes, Atwell and Habash, 2011) is a multimodal language resource that integrates deep tagging, interlinear translation, multiple speech recordings, visualization and collaborative analysis for the Classical Arabic language of the Quran. Available online at http://corpus.quran.com, the website is a popular study guide for Quranic Arabic, used by over 1.2 million visitors over the past year. We provide a description of the underlying software system that has been used to develop the corpus annotations. The multimodal data is made available online through an accessible cross-referenced web interface. Although our Linguistic Analysis Multimodal Platform (LAMP) has been applied to the Classical Arabic language of the Quran, we argue that our annotation model and software architecture may be of interest to other related corpus linguistics projects. Work related to LAMP includes recent efforts for annotating other Classical languages, such as Ancient Greek and Latin (Bamman, Mambrini and Crane, 2009), as well as commercial systems (e.g. Logos Bible study) that provide access to syntactic tagging for the Hebrew Bible and Greek New Testament (Brannan, 2011).
This paper presents a robust linguistic Web service framework for Polish, combining several mature offline linguistic tools in a common online platform. The toolset comprise paragraph-, sentence- and token-level segmenter, morphological analyser, disambiguating tagger, shallow and deep parser, named entity recognizer and coreference resolver. Uniform access to processing results is provided by means of a stand-off packaged adaptation of National Corpus of Polish TEI P5-based representation and interchange format. A concept of asynchronous handling of requests sent to the implemented Web service (Multiservice) is introduced to enable processing large amounts of text by setting up language processing chains of desired complexity. Apart from a dedicated API, a simpleWeb interface to the service is presented, allowing to compose a chain of annotation services, run it and periodically check for execution results, made available as plain XML or in a simple visualization. Usage examples and results from performance and scalability tests are also included.
Suffix trees are data structures that can be used to index a corpus. In this paper, we explore how some properties of suffix trees naturally provide the functionality of an n-gram language model with variable n. We explain these properties of suffix trees, which we leverage for our Suffix Tree Language Model (STLM) implementation and explain how a suffix tree implicitly contains the data needed for n-gram language modeling. We also discuss the kinds of smoothing techniques appropriate to such a model. We then show that our suffix-tree language model implementation is competitive when compared to the state-of-the-art language model SRILM (Stolke, 2002) in statistical machine translation experiments.
Romanian has been traditionally seen as bearing three lexical genders: masculine, feminine and neuter, although it has always been known to have only two agreement patterns (for masculine and feminine). A recent analysis of the Romanian gender system described in (Bateman and Polinsky, 2010), based on older observations, argues that there are two lexically unspecified noun classes in the singular and two different ones in the plural and that what is generally called neuter in Romanian shares the class in the singular with masculines, and the class in the plural with feminines based not only on agreement features but also on form. Previous machine learning classifiers that have attempted to discriminate Romanian nouns according to gender have so far taken as input only the singular form, presupposing the traditional tripartite analysis. We propose a classifier based on two parallel support vector machines using n-gram features from the singular and from the plural which outperforms previous classifiers in its high ability to distinguish the neuter. The performance of our system suggests that the two-gender analysis of Romanian, on which it is based, is on the right track.
The overall goal of this project is to evaluate the performance of word sense alignment (WSA) systems, focusing on obtaining examples appropriate to language learners. Building a gold standard dataset based on human expert judgments is costly in time and labor, and thus we gauge the utility of using semi-experts in performing the annotation. In an online survey, we present a sense of a target word from one dictionary with senses from the other dictionary, asking for judgments of relatedness. We note the difficulty of agreement, yet the utility in using such results to evaluate WSA work. We find that one's treatment of related senses heavily impacts the results for WSA.
This document presents the first edition of the Polish Sejm Corpus -- a new specialized resource containing transcribed, automatically annotated utterances of the Members of Polish Sejm (lower chamber of the Polish Parliament). The corpus data encoding is inherited from the National Corpus of Polish and enhanced with session metadata and structure. The multi-layered stand-off annotation contains sentence- and token-level segmentation, disambiguated morphosyntactic information, syntactic words and groups resulting from shallow parsing and named entities. The paper also outlines several novel ideas for corpus preparation, e.g. the notion of a live corpus, constantly populated with new data or the concept of linking corpus data with external databases to enrich content. Although initial statistical comparison of the resource with the balanced corpus of general Polish reveals substantial differences in language richness, the resource makes a valuable source of linguistic information as a large (300 M segments) collection of quasi-spoken data ready to be aligned with the audio/video recording of sessions, currently being made publicly available by Sejm.
To stimulate research in cross-language entity linking, we present a new test collection for evaluating the accuracy of cross-language entity linking in twenty-one languages. This paper describes an efficient way to create and curate such a collection, judiciously exploiting existing language resources. Queries are created by semi-automatically identifying person names on the English side of a parallel corpus, using judgments obtained through crowdsourcing to identify the entity corresponding to the name, and projecting the English name onto the non-English document using word alignments. Name projections are then curated, again through crowdsourcing. This technique resulted in the first publicly available multilingual cross-language entity linking collection. The collection includes approximately 55,000 queries, comprising between 875 and 4,329 queries for each of twenty-one non-English languages.
We present and demonstrate the updated version of the TARSQI Toolkit, a suite of temporal processing modules that extract temporal information from natural language texts. It parses the document and identifies temporal expressions, recognizes events, anchor events to temporal expressions and orders events relative to each other. The toolkit was previously demonstrated at COLING 2008, but has since seen substantial changes including: (1) incorporation of a new time expression tagger, (2){\textasciitilde}embracement of stand-off annotation, (3) application to the medical domain and (4) introduction of narrative containers.
We present a corpus of sentences from news articles that are annotated as general or specific. We employed annotators on Amazon Mechanical Turk to mark sentences from three kinds of news articles―reports on events, finance news and science journalism. We introduce the resulting corpus, with focus on annotator agreement, proportion of general/specific sentences in the articles and results for automatic classification of the two sentence types.
In recent months, LDC has developed a web-based annotation infrastructure centered around a tree model of annotations and a Ruby on Rails application called the LDC User Interface (LUI). The effort aims to centralize all annotation into this single platform, which means annotation is always available remotely, with no more software required than a web browser. While the design is monolithic in the sense of handling any number of annotation projects, it is also scalable, as it is distributed over many physical and virtual machines. Furthermore, minimizing customization was a core design principle, and new functionality can be plugged in without writing a full application. The creation and customization of GUIs is itself done through the web interface, without writing code, with the aim of eventually allowing project managers to create a new task without developer intervention. Many of the desirable features follow from the model of annotations as trees, and the operationalization of annotation as tree modification.
The ability to reliably identify sarcasm and irony in text can improve the performance of many Natural Language Processing (NLP) systems including summarization, sentiment analysis, etc. The existing sarcasm detection systems have focused on identifying sarcasm on a sentence level or for a specific phrase. However, often it is impossible to identify a sentence containing sarcasm without knowing the context. In this paper we describe a corpus generation experiment where we collect regular and sarcastic Amazon product reviews. We perform qualitative and quantitative analysis of the corpus. The resulting corpus can be used for identifying sarcasm on two levels: a document and a text utterance (where a text utterance can be as short as a sentence and as long as a whole document).
This paper presents the first phase of building YADAC ― a multi-genre Dialectal Arabic (DA) corpus ― that is compiled using Web data from microblogs (i.e. Twitter), blogs/forums and online knowledge market services in which both questions and answers are user-generated. In addition to introducing two new genres to the current efforts of building DA corpora (i.e. microblogs and question-answer pairs extracted from online knowledge market services), the paper highlights and tackles several new issues related to building DA corpora that have not been handled in previous studies: function-based Web harvesting and dialect identification, vowel-based spelling variation, linguistic hypercorrection and its effect on spelling variation, unsupervised Part-of-Speech (POS) tagging and base phrase chunking for DA. Although the algorithms for both POS tagging and base-phrase chunking are still under development, the results are promising.
Natural Language Processing continues to grow in popularity in a range of research and commercial applications, yet managing the wide array of potential NLP components remains a difficult problem. This paper describes Curator, an NLP management framework designed to address some common problems and inefficiencies associated with building NLP process pipelines; and Edison, an NLP data structure library in Java that provides streamlined interactions with Curator and offers a range of useful supporting functionality.
Fast and effective automated indexing is critical for search and personalized services. Key phrases that consist of one or more words and represent the main concepts of the document are often used for the purpose of indexing. In this paper, we investigate the use of additional semantic features and pre-processing steps to improve automatic key phrase extraction. These features include the use of signal words and freebase categories. Some of these features lead to significant improvements in the accuracy of the results. We also experimented with 2 forms of document pre-processing that we call light filtering and co-reference normalization. Light filtering removes sentences from the document, which are judged peripheral to its main content. Co-reference normalization unifies several written forms of the same named entity into a unique form. We also needed a Gold Standard ― a set of labeled documents for training and evaluation. While the subjective nature of key phrase selection precludes a true Gold Standard, we used Amazon's Mechanical Turk service to obtain a useful approximation. Our data indicates that the biggest improvements in performance were due to shallow semantic features, news categories, and rhetorical signals (nDCG 78.47{\%} vs. 68.93{\%}). The inclusion of deeper semantic features such as Freebase sub-categories was not beneficial by itself, but in combination with pre-processing, did cause slight improvements in the nDCG scores.
With recent developments in web technologies, percentage web content in Hindi is growing up at a lighting speed. This information can prove to be very useful for researchers, governments and organization to learn what's on public mind, to make sound decisions. In this paper, we present a graph based wordnet expansion method to generate a full (adjective and adverb) subjective lexicon. We used synonym and antonym relations to expand the initial seed lexicon. We show three different evaluation strategies to validate the lexicon. We achieve 70.4{\%} agreement with human annotators and {\^a}¼79{\%} accuracy on product review classification. Main contribution of our work 1) Developing a lexicon of adjectives and adverbs with polarity scores using Hindi Wordnet. 2) Developing an annotated corpora of Hindi Product Reviews.
We present an extension of the coreference annotation in the English NP4E and the Catalan AnCora-CA corpora with near-identity relations, which are borderline cases of coreference. The annotated subcorpora have 50K tokens each. Near-identity relations, as presented by Recasens et al. (2010; 2011), build upon the idea that identity is a continuum rather than an either/or relation, thus introducing a middle ground category to explain currently problematic cases. The first annotation effort that we describe shows that it is not possible to annotate near-identity explicitly because subjects are not fully aware of it. Therefore, our second annotation effort used an indirect method, and arrived at near-identity annotations by inference from the disagreements between five annotators who had only a two-alternative choice between coreference and non-coreference. The results show that whereas as little as 2-6{\%} of the relations were explicitly annotated as near-identity in the former effort, up to 12-16{\%} of the relations turned out to be near-identical following the indirect method of the latter effort.
This paper describes a collection of multimodal corpora of referring expressions, the REX corpora. The corpora have two notable features, namely (1) they include time-aligned extra-linguistic information such as participant actions and eye-gaze on top of linguistic information, (2) dialogues were collected with various configurations in terms of the puzzle type, hinting and language. After describing how the corpora were constructed and sketching out each, we present an analysis of various statistics for the corpora with respect to the various configurations mentioned above. The analysis showed that the corpora have different characteristics in the number of utterances and referring expressions in a dialogue, the task completion time and the attributes used in the referring expressions. In this respect, we succeeded in constructing a collection of corpora that included a variety of characteristics by changing the configurations for each set of dialogues, as originally planned. The corpora are now under preparation for publication, to be used for research on human reference behaviour.
Statistical machine translation (SMT) requires a parallel corpus between the source and target languages. Although a pivot-translation approach can be applied to a language pair that does not have a parallel corpus directly between them, it requires both source―pivot and pivot―target parallel corpora. We propose a novel approach to apply SMT to a resource-limited source language that has no parallel corpus but has only a word dictionary for the pivot language. The problems with dictionary-based translations lie in their ambiguity and incompleteness. The proposed method uses a word lattice representation of the pivot-language candidates and word lattice decoding to deal with the ambiguity; the lattice expansion is accomplished by using a pivot―target phrase translation table to compensate for the incompleteness. Our experimental evaluation showed that this approach is promising for applying SMT, even when a source-side parallel corpus is lacking.
This paper presents ongoing Phd thesis work dealing with the extraction of knowledge-rich contexts from text corpora for terminographic purposes. Although notable progress in the field has been made over recent years, there is yet no methodology or integrated workflow that is able to deal with multiple, typologically different languages and different domains, and that can be handled by non-expert users. Moreover, while a lot of work has been carried out to research the KRC extraction step, the selection and further analysis of results still involves considerable manual work. In this view, the aim of this paper is two-fold. Firstly, the paper presents a ranking algorithm geared at supporting the selection of high-quality contexts once the extraction has been finished and describes ranking experiments with Russian context candidates. Secondly, it presents the KnowPipe framework for context extraction: KnowPipe aims at providing a processing environment that allows users to extract knowledge-rich contexts from text corpora in different languages using shallow and deep processing techniques. In its current state of development, KnowPipe provides facilities for preprocessing Russian and German text corpora, for pattern-based knowledge-rich context extraction from these corpora using shallow analysis as well as tools for ranking Russian context candidates.
The name of a company or a brand is the key element to a successful business. A good name is able to state the area of competition and communicate the promise given to customers by evoking semantic associations. Although various resources provide distinct tips for inventing creative names, little research was carried out to investigate the linguistic aspects behind the naming mechanism. Besides, there might be latent methods that copywriters unconsciously use. In this paper, we describe the annotation task that we have conducted on a dataset of creative names collected from various resources to create a gold standard for linguistic creativity in naming. Based on the annotations, we compile common and latent methods of naming and explore the correlations among linguistic devices, provoked effects and business domains. This resource represents a starting point for a corpus based approach to explore the art of naming.
In this paper we present the first corpus where one million Dutch words from a variety of text genres have been annotated with semantic roles. 500K have been completely manually verified and used as training material to automatically label another 500K. All data has been annotated following an adapted version of the PropBank guidelines. The corpus's rich text type diversity and the availability of manually verified syntactic dependency structures allowed us to experiment with an existing semantic role labeler for Dutch. In order to test the system's portability across various domains, we experimented with training on individual domains and compared this with training on multiple domains by adding more data. Our results show that training on large data sets is necessary but that including genre-specific training material is also crucial to optimize classification. We observed that a small amount of in-domain training data is already sufficient to improve our semantic role labeler.
This paper describes the syntactic annotation process of the DECODA corpus. This corpus contains manual transcriptions of spoken conversations recorded in the French call-center of the Paris Public Transport Authority (RATP). Three levels of syntactic annotation have been performed with a semi-supervised approach: POS tags, Syntactic Chunks and Dependency parses. The main idea is to use off-the-shelf NLP tools and models, originaly developped and trained on written text, to perform a first automatic annotation on the manually transcribed corpus. At the same time a fully manual annotation process is performed on a subset of the original corpus, called the GOLD corpus. An iterative process is then applied, consisting in manually correcting errors found in the automatic annotations, retraining the linguistic models of the NLP tools on this corrected corpus, then checking the quality of the adapted models on the fully manual annotations of the GOLD corpus. This process iterates until a certain error rate is reached. This paper describes this process, the main issues raising when adapting NLP tools to process speech transcriptions, and presents the first evaluations performed with these new adapted tools.
This paper introduces a corpus of Korean-accented English speech produced by children (the Korean Children's Spoken English Corpus: the KC-SEC), which is constructed by Seoul National University. The KC-SEC was developed in support of research and development of CALL systems for Korean learners of English, especially for elementary school learners. It consists of read-speech produced by 96 Korean learners aged from 9 to 12. Overall corpus size is 11,937 sentences, which amount to about 16 hours of speech. Furthermore, a statistical analysis of pronunciation variability appearing in the corpus is performed in order to investigate the characteristics of the Korean children's spoken English. The realized phonemes (hypothesis) are extracted through time-based phoneme alignment, and are compared to the targeted phonemes (reference). The results of the analysis show that: i) the pronunciation variations found frequently in Korean children's speech are devoicing and changing of articulation place or/and manner; and ii) they largely correspond to those of general Korean learners' speech presented in previous studies, despite some differences.
The goal of the DECODA project is to reduce the development cost of Speech Analytics systems by reducing the need for manual annotat ion. This project aims to propose robust speech data mining tools in the framework of call-center monitoring and evaluation, by means of weakl y supervised methods. The applicative framework of the project is the call-center of the RATP (Paris public transport authority). This project tackles two very important open issues in the development of speech mining methods from spontaneous speech recorded in call-centers : robus tness (how to extract relevant information from very noisy and spontaneous speech messages) and weak supervision (how to reduce the annotation effort needed to train and adapt recognition and classification models). This paper describes the DECODA corpus collected at the RATP during the project. We present the different annotation levels performed on the corpus, the methods used to obtain them, as well as some evaluation o f the quality of the annotations produced.
In this paper, we present a trilingual parallel corpus for German, Italian and Romansh, a Swiss minority language spoken in the canton of Grisons. The corpus called ALLEGRA contains press releases automatically gathered from the website of the cantonal administration of Grisons. Texts have been preprocessed and aligned with a current state-of-the-art sentence aligner. The corpus is one of the first of its kind, and can be of great interest, particularly for the creation of natural language processing resources and tools for Romansh. We illustrate the use of such a trilingual resource for automatic induction of bilingual lexicons, which is a real challenge for under-represented languages. We induce a bilingual lexicon for German-Romansh by phrase alignment and evaluate the resulting entries with the help of a reference lexicon. We then show that the use of the third language of the corpus ― Italian ― as a pivot language can improve the precision of the induced lexicon, without loss in terms of quality of the extracted pairs.
In the context of forensic phonetics the transcription of intercepted signals is particularly important. However, these signals are often degraded and the transcript may not reflect what was actually pronounced. In the absence of the original signal, the only way to see the level of accuracy that can be obtained in the transcription of poor recordings is to develop an objective methodology for intelligibility measurements. This study has been carried out on a corpus specially built to simulate the real conditions of forensic signals. With reference to this corpus a measurement system of intelligibility based on STI (Speech Transmission Index) has been evaluated so as to assess its performance. The result of the experiment shows a high correlation between objective measurements and subjective evaluations. Therefore it is recommended to use the proposed methodology in order to establish whether a given intercepted signal can be transcribed with sufficient reliability.
This paper presents the results of a terminological work on a reference corpus in the domain of Biomedicine. In particular, the research tends to analyse the use of certain terms in Biomedicine in order to verify their change over the time with the aim of retrieving from the net the very essence of documentation. The terminological sample contains words used in BioNLP and biomedicine and identifies which terms are passing from scientific publications to the daily press and which are rather reserved to scientific production. The final scope of this work is to determine how scientific dissemination to an ever larger part of the society enables a public of common citizens to approach communication on biomedical research and development; and its main source is a reference corpus made up of three main repositories from which information related to BioNLP and Biomedicine is extracted. The paper is divided in three sections: 1) an introduction dedicated to data extracted from scientific documentation; 2) the second section devoted to methodology and data description; 3) the third part containing a statistical representation of terms extracted from the archive: indexes and concordances allow to reflect on the use of certain terms in this field and give possible keys for having access to the extraction of knowledge in the digital era.
We describe the evaluation framework for spoken document retrieval for the IR for the Spoken Documents Task, conducted in the ninth NTCIR Workshop. The two parts of this task were a spoken term detection (STD) subtask and an ad hoc spoken document retrieval subtask (SDR). Both subtasks target search terms, passages and documents included in academic and simulated lectures of the Corpus of Spontaneous Japanese. Seven teams participated in the STD subtask and five in the SDR subtask. The results obtained through the evaluation in the workshop are discussed.
This paper presents a method for designing, compiling and annotating corpora intended for language learners. In particular, we focus on spoken corpora for being used as complementary material in the classroom as well as in examinations. We describe the three corpora (Spanish, Chinese and Japanese) compiled by the Laboratorio de Ling{\"u
This paper presents the corpus developed by the LIUM for Automatic Speech Recognition (ASR), based on the TED Talks. This corpus was built during the IWSLT 2011 Evaluation Campaign, and is composed of 118 hours of speech with its accompanying automatically aligned transcripts. We describe the content of the corpus, how the data was collected and processed, how it will be publicly available and how we built an ASR system using this data leading to a WER score of 17.4 {\%}. The official results we obtained at the IWSLT 2011 evaluation campaign are also discussed.
The huge amount of the available information in the Web creates the need of effective information extraction systems that are able to produce metadata that satisfy user's information needs. The development of such systems, in the majority of cases, depends on the availability of an appropriately annotated corpus in order to learn or evaluate extraction models. The production of such corpora can be significantly facilitated by annotation tools, that provide user-friendly facilities and enable annotators to annotate documents according to a predefined annotation schema. However, the construction of annotation tools that operate in a distributed environment is a challenging task: the majority of these tools are implemented as Web applications, having to cope with the capabilities offered by browsers. This paper describes the SYNC3 collaborative annotation tool, which implements an alternative architecture: it remains a desktop application, fully exploiting the advantages of desktop applications, but provides collaborative annotation through the use of a centralised server for storing both the documents and their metadata, and instance messaging protocols for communicating events among all annotators. The annotation tool is implemented as a component of the Ellogon language engineering platform, exploiting its extensive annotation engine, its cross-platform abilities and its linguistic processing components, if such a need arises. Finally, the SYNC3 annotation tool is distributed with an open source license, as part of the Ellogon platform.
This paper aims at giving an overview of ELRAs recent activities. The first part elaborates on ELRAs means of boosting the sharing Language Resources (LRs) within the HLT community through its catalogues, LRE-Map initiative, as well as its work towards the integration of its LRs within the META-SHARE open infrastructure. The second part shows how ELRA helps in the development and evaluation of HLT, in particular through its numerous participations to collaborative projects for the production of resources and platforms to facilitate their production and exploitation. A third part focuses on ELRAs work for clearing IPR issues in a HLT-oriented context, one of its latest initiative being its involvement in a Fair Research Act proposal to promote the easy access to LRs to the widest community. Finally, the last part elaborates on recent actions for disseminating information and promoting cooperation in the field, e.g. an the Language Library being launched at LREC2012 and the creation of an International Standard LR Number, a LR unique identifier to enable the accurate identification of LRs. Among the other messages ELRA will be conveying the attendees are the announcement of a set of freely available resources, the establishment of a LR and Evaluation forum, etc.
This paper describes the development of a statistical machine translation system between French and English for scientific papers. This system will be closely integrated into the French HAL open archive, a collection of more than 100.000 scientific papers. We describe the creation of in-domain parallel and monolingual corpora, the development of a domain specific translation system with the created resources, and its adaptation using monolingual resources only. These techniques allowed us to improve a generic system by more than 10 BLEU points.
The paper presents an approach to extract irregularities in document corpora, where the documents originate from different sources and the analyst's interest is to find documents which are atypical for the given source. The main contribution of the paper is a voting-based approach to irregularity detection and its evaluation on a collection of newspaper articles from two sources: Western (UK and US) and local (Kenyan) media. The evaluation of a domain expert proves that the method is very effective in uncovering interesting irregularities in categorized document corpora.
The REPERE Challenge aims to support research on people recognition in multimodal conditions. To assess the technology progression, annual evaluation campaigns will be organized from 2012 to 2014. In this context, the REPERE corpus, a French videos corpus with multimodal annotation, has been developed. This paper presents datasets collected for the dry run test that took place at the beginning of 2012. Specific annotation tools and guidelines are mainly described. At the time being, 6 hours of data have been collected and annotated. Last section presents analyses of annotation distribution and interaction between modalities in the corpus.
We present a novel tool for morphological analysis of Serbian, which is a low-resource language with rich morphology. Our tool produces lemmatisation and morphological analysis reaching accuracy that is considerably higher compared to the existing alternative tools: 83.6{\%} relative error reduction on lemmatisation and 8.1{\%} relative error reduction on morphological analysis. The system is trained on a small manually annotated corpus with an approach based on Bidirectional Sequence Classification and Guided Learning techniques, which have recently been adapted with success to a broad set of NLP tagging tasks. In the system presented in this paper, this general approach to tagging is applied to the lemmatisation task for the first time thanks to our novel formulation of lemmatisation as a category tagging task. We show that learning lemmatisation rules from annotated corpus and integrating the context information in the process of morphological analysis provides a state-of-the-art performance despite the lack of resources. The proposed system can be used via a web GUI that deploys its best scoring configuration
State-of-the-art dependency representations such as the Stanford Typed Dependencies may represent the grammatical relations in a sentence as directed, possibly cyclic graphs. Querying a syntactically annotated corpus for grammatical structures that are represented as graphs requires graph matching, which is a non-trivial task. In this paper, we present an algorithm for graph matching that is tailored to the properties of large, syntactically annotated corpora. The implementation of the algorithm is built on top of the popular IMS Open Corpus Workbench, allowing corpus linguists to re-use existing infrastructure. An evaluation of the resulting software, CWB-treebank, shows that its performance in real world applications, such as a web query interface, compares favourably to implementations that rely on a relational database or a dedicated graph database while at the same time offering a greater expressive power for queries. An intuitive graphical interface for building the query graphs is available via the Treebank.info project.
This paper describes the different strategies used to improve the results obtained by an off-line speaker diarisation tool with the Albayzin 2010 diarisation database. The errors made by the system have been analyzed and different strategies have been proposed to reduce each kind of error. Very short segments incorrectly labelled and different appearances of one speaker labelled with different identifiers are the most common errors. A post-processing module that refines the segmentation by retraining the GMM models of the speakers involved has been built to cope with these errors. This post-processing module has been tuned with the training dataset and improves the result of the diarisation system by 16.4{\%} in the test dataset.
Reflecting the rapid growth of science, technology, and culture, it has become common practice to consult tools on the World Wide Web for various terms. Existing search engines provide an enormous volume of information, but retrieved information is not organized. Hand-compiled encyclopedias provide organized information, but the quantity of information is limited. In this paper, aiming to integrate the advantages of both tools, we propose a method to organize a search result based on multiple viewpoints as in Wikipedia. Because viewpoints required for explanation are different depending on the type of a term, such as animal and disease, we model articles in Wikipedia to extract a viewpoint structure for each term type. To identify a set of term types, we independently use manual annotation and automatic document clustering for Wikipedia articles. We also propose an effective feature for clustering of Wikipedia articles. We experimentally show that the document clustering reduces the cost for the manual annotation while maintaining the accuracy for modeling Wikipedia articles.
The AT{\&}T VoiceBuilder provides a new tool to researchers and practitioners who want to have their voices synthesized by a high-quality commercial-grade text-to-speech system without the need to install, configure, or manage speech processing software and equipment.It is implemented as a web service on the AT{\&}T Speech Mashup Portal.The system records and validates users' utterances, processes them to build a synthetic voice and provides a web service API to make the voice available to real-time applications through a scalable cloud-based processing platform. All the procedures are automated to avoid human intervention. We present experimental comparisons of voices built using the system.
Error analysis is a means to assess machine translation output in qualitative terms, which can be used as a basis for the generation of error profiles for different systems. As for other subjective approaches to evaluation it runs the risk of low inter-annotator agreement, but very often in papers applying error analysis to MT, this aspect is not even discussed. In this paper, we report results from a comparative evaluation of two systems where agreement initially was low, and discuss the different ways we used to improve it. We compared the effects of using more or less fine-grained taxonomies, and the possibility to restrict analysis to short sentences only. We report results on inter-annotator agreement before and after measures were taken, on error categories that are most likely to be confused, and on the possibility to establish error profiles also in the absence of a high inter-annotator agreement.
We present the current state of development of the Croatian Dependency Treebank ― with special empahsis on adapting the Prague Dependency Treebank formalism to Croatian language specifics ― and illustrate its possible applications in an experiment with dependency parsing using MaltParser. The treebank currently contains approximately 2870 sentences, out of which the 2699 sentences and 66930 tokens were used in this experiment. Three linear-time projective algorithms implemented by the MaltParser system ― Nivre eager, Nivre standard and stack projective ― running on default settings were used in the experiment. The highest performing system, implementing the Nivre eager algorithm, scored (LAS 71.31 UAS 80.93 LA 83.87) within our experiment setup. The results obtained serve as an illustration of treebank's usefulness in natural language processing research and as a baseline for further research in dependency parsing of Croatian.
The paper describes the evaluation of the KomParse system. KomParse is a dialogue system embedded in a 3-D massive multiplayer online game, allowing conversations between non player characters (NPCs) and game users. In a field test with game users, the system was evaluated with respect to acceptability and usability of the overall system as well as task completion, dialogue control and efficiency of three conversational tasks. Furthermore, subjective feedback has been collected for evaluating the single communication components of the system such as natural language understanding. The results are very satisfying and promising. In general, both the usability and acceptability tests show that the tested NPC is useful and well-accepted by the users. Even if the NPC does not always understand the users well and expresses things unexpected, he could still provide appropriate responses to help users to solve their problems or entertain them.
In this paper we present two real cases, in the fields of discourse analysis of newspapers and communication research which demonstrate the impact of Language Resources (LR) and NLP in the humanities. We describe our collaboration with (i) the Feminario research group from the UAB which has been investigating androcentric practices in Spanish general press since the 80s and whose research suggests that Spanish general press has undergone a dehumanization process that excludes women and men and (ii) the Municipals'11 online project which investigates the Spanish local election campaign in the blogosphere. We will see how NLP tools and LRs make possible the so called e-Humanities research' as they provide Humanities with tools to perform intensive and automatic text analyses. Language technologies have evolved a lot and are mature enough to provide useful tools to researchers dealing with large amount of textual data. The language resources that have been developed within the field of NLP have proven to be useful for other disciplines that are unaware of their existence and nevertheless would greatly benefit from them as they provide (i) exhaustiveness -to guarantee that data coverage is wide and representative enough- and (ii) reliable and significant results -to guarantee that the reported results are statistically significant.
The paper presents a treebank-driven approach to the construction of a Bulgarian valence lexicon with ontological restrictions over the inner participants of the event. First, the underlying ideas behind the Bulgarian Ontology-based lexicon are outlined. Then, the extraction and manipulation of the valence frames is discussed with respect to the BulTreeBank annotation scheme and DOLCE ontology. Also, the most frequent types of syntactic frames are specified as well as the most frequent types of ontological restrictions over the verb arguments. The envisaged application of such a lexicon would be: in assigning ontological labels to syntactically parsed corpora, and expanding the lexicon and lexical information in the Bulgarian Resource Grammar.
The present paper tackles the issue of PoS tag conversion within the framework of a distributed web service platform for the automatic creation of language resources. PoS tagging is now considered a ''''''``solved problem''''''''; yet, because of the differences in the tagsets, interchange of the various PoS tagger available is still hampered. In this paper we describe the implementation of a pos-tagged-corpus converter, which is needed for chaining together in a workflow the Freeling PoS tagger for Italian and the DESR dependency parser, given that these two tools have been developed independently. The conversion problems experienced during the implementation, related to the properties of the different tagsets and of tagset conversion in general, are discussed together with the heuristics implemented in the attempt to solve them. Finally, the converter is evaluated by assessing the impact of conversion on the performance of the dependency parser. From this we learn that in most cases parsing errors are due to actual tagging errors, and not to conversion itself. Besides, information on accuracy loss is an important feature in a distributed environment of (NLP) services, where users need to decide which services best suit their needs.
This paper presents some novel results on Chinese spell checking. In this paper, a concise algorithm based on minimized-path segmentation is proposed to reduce the cost and suit the needs of current Chinese input systems. The proposed algorithm is actually derived from a simple assumption that spelling errors often make the number of segments larger. The experimental results are quite positive and implicitly verify the effectiveness of the proposed assumption. Finally, all approaches work together to output a result much better than the baseline with 12{\%} performance improvement.
Currently, the area of translation studies lacks corpora by which translation scholars can validate their theoretical claims, for example, regarding the scope of the characteristics of the translation relation. In this paper, we describe a customized resource in the area of translation studies that mainly addresses research on the properties of the translation relation. Our experimental results show that the Type-Token-Ratio (TTR) is not a universally valid indicator of the simplification of translation.
In this paper, we introduce a novel parallel corpus of music and lyrics, annotated with emotions at line level. We first describe the corpus, consisting of 100 popular songs, each of them including a music component, provided in the MIDI format, as well as a lyrics component, made available as raw text. We then describe our work on enhancing this corpus with emotion annotations using crowdsourcing. We also present some initial experiments on emotion classification using the music and the lyrics representations of the songs, which lead to encouraging results, thus demonstrating the promise of using joint music-lyric models for song processing.
This paper describes the steps of construction of a shallow lexical ontology of Italian Linguistics, set to be used by a meta-search engine for query refinement. The ontology was constructed with the software Prot{\'e}g{\'e} 4.0.2 and is in OWL format; its construction has been carried out following the steps described in the well-known Ontology Learning From Text (OLFT) layer cake. The starting point was the automatic term extraction from a corpus of web documents concerning the domain of interest (304,000 words); as regards corpus construction, we describe the main criteria of the web documents selection and its critical points, concerning the definition of user profile and of degrees of specialisation. We describe then the process of term validation and construction of a glossary of terms of Italian Linguistics; afterwards, we outline the identification of synonymic chains and the main criteria of ontology design: top classes of ontology are Concept (containing taxonomy of concepts) and Terms (containing terms of the glossary as instances), while concepts are linked through part-whole and involved-role relation, both borrowed from Wordnet. Finally, we show some examples of the application of the ontology for query refinement.
Language resources can be divided into structural resources treating phonology, morphosyntax, semantics etc. and resources treating the social, demographic, ethnic, political context. A third type are meta-resources, like bibliographies, which provide access to the resources of the first two kinds. This poster will present the Glottolog/Langdoc project, a comprehensive bibliography providing web access to 180k bibliographical records to (mainly) low visibility resources from low-density languages. The resources are annotated for macro-area, content language, and document type and are available in XHTML and RDF.
The relevance of automatically identifying rhetorical moves in scientific texts has been widely acknowledged in the literature. This study focuses on abstracts of standard research papers written in English and aims to tackle a fundamental limitation of current machine-learning classifiers: they are mono-labeled, that is, a sentence can only be assigned one single label. However, such approach does not adequately reflect actual language use since a move can be realized by a clause, a sentence, or even several sentences. Here, we present MAZEA (Multi-label Argumentative Zoning for English Abstracts), a multi-label classifier which automatically identifies rhetorical moves in abstracts but allows for a given sentence to be assigned as many labels as appropriate. We have resorted to various other NLP tools and used two large training corpora: (i) one corpus consists of 645 abstracts from physical sciences and engineering (PE) and (ii) the other corpus is made up of 690 from life and health sciences (LH). This paper presents our preliminary results and also discusses the various challenges involved in multi-label tagging and works towards satisfactory solutions. In addition, we also make our two training corpora publicly available so that they may serve as benchmark for this new task.
This paper describes the implementation of a generalized query language on Google Ngram database. This language allows for very expressive queries that exploit semantic similarity acquired both from corpora (e.g. LSA) and from WordNet, and phonetic similarity available from the CMU Pronouncing Dictionary. It contains a large number of new operators, which combined in a proper query can help users to extract n-grams having similarly close syntactic and semantic relational properties. We also characterize the operators with respect to their corpus affiliation and their functionality. The query syntax is considered next given in terms of Backus-Naur rules followed by a few interesting examples of how the tool can be used. We also describe the command-line arguments the user could input comparing them with the ones for retrieving n-grams through the interface of Google Ngram database. Finally we discuss possible improvements on the extraction process and some relevant query completeness issues.
We present a study of Polish-English machine translation, where the impact of various types of errors on cohesion and comprehensibility of the translations were investigated. The following phenomena are in focus: (i) The most common errors produced by current state-of-the-art MT systems for Polish-English MT. (ii) The effect of different types of errors on text cohesion. (iii) The effect of different types of errors on readers' understanding of the translation. We found that errors of incorrect and missing translations are the most common for current systems, while the category of non-translated words had the most negative impact on comprehension. All three of these categories contributed to the breaking of cohesive chains. The correlation between number of errors found in a translation and number of wrong answers in the comprehension tests was low. Another result was that non-native speakers of English performed at least as good as native speakers on the comprehension tests.
In face to face interaction, people refer to objects and events not only by means of speech but also by means of gesture. The present paper describes building a corpus of referential gestures. The aim is to investigate gestural reference by incorporating insights from semantic ontologies and by employing a more holistic view on referential gestures. The paper's focus is on presenting the data collection procedure and discussing the corpus' design; additionally the first insights from constructing the annotation scheme are described.
In this paper, we describe DEGELS1, a comparable corpus of French Sign Language and co-speech gestures that has been created to serve as a testbed corpus for the DEGELS workshops. These workshop series were initiated in France for researchers studying French Sign Language and co-speech gestures in French, with the aim of comparing methodologies for corpus annotation. An extract was used for the first event DEGELS2011 dedicated to the annotation of pointing, and the same extract will be used for DEGELS2012, dedicated to segmentation.
Nowadays many researches focus on the automatic recognition of sign language. High recognition rates are achieved using lot of training data. This data is, generally, collected by manual annotating SL video corpus. However this is time consuming and the results depend on the annotators knowledge. In this work we intend to assist the annotation in terms of glosses which consist on writing down the sign meaning sign for sign thanks to automatic video processing techniques. In this case using learning data is not suitable since at the first step it will be needed to manually annotate the corpus. Also the context dependency of signs and the co-articulation effect in continuous SL make the collection of learning data very difficult. Here we present a novel approach which uses lexical representations of sign to overcome these problems and image processing techniques to match sign performances to sign representations. Signs are described using Zeebede (ZBD) which is a descriptor of signs that considers the high variability of signs. A ZBD database is used to stock signs and can be queried using several characteristics. From a video corpus sequence features are extracted using a robust body part tracking approach and a semi-automatic sign segmentation algorithm. Evaluation has shown the performances and limitation of the proposed approach.
We discuss a previously proposed method for augmenting parallel corpora of limited size for the purposes of machine translation through monolingual paraphrasing of the source language. We develop a three-stage shallow paraphrasing procedure to be applied to the Swedish-Bulgarian language pair for which limited parallel resources exist. The source language exhibits specifics not typical of high-density languages already studied in a similar setting. Paraphrases of a highly productive type of compound nouns in Swedish are generated by a corpus-based technique. Certain Swedish noun-phrase types are paraphrased using basic heuristics. Further we introduce noun-phrase morphological variations for better wordform coverage. We evaluate the performance of a phrase-based statistical machine translation system trained on a baseline parallel corpus and on three stages of artificial enlargement of the source-language training data. Paraphrasing is shown to have no effect on performance for the Swedish-English translation task. We show a small, yet consistent, increase in the BLEU score of Swedish-Bulgarian translations of larger token spans on the first enlargement stage. A small improvement in the overall BLEU score of Swedish-Bulgarian translation is achieved on the second enlargement stage. We find that both improvements justify further research into the method for the Swedish-Bulgarian translation task.
The META-NORD project has contributed to an open infrastructure for language resources (data and tools) under the META-NET umbrella. This paper presents the key objectives of META-NORD and reports on the results achieved in the first year of the project. META-NORD has mapped and described the national language technology landscape in the Nordic and Baltic countries in terms of language use, language technology and resources, main actors in the academy, industry, government and society; identified and collected the first batch of language resources in the Nordic and Baltic countries; documented, processed, linked, and upgraded the identified language resources to agreed standards and guidelines. The three horizontal multilingual actions in META-NORD are overviewed in this paper: linking and validating Nordic and Baltic wordnets, the harmonisation of multilingual Nordic and Baltic treebanks, and consolidating multilingual terminology resources across European countries. This paper also touches upon intellectual property rights for the sharing of language resources.
We present techniques for monolingual term candidate extraction which are being developed in the EU project TTC. We designed an application for German and English data that serves as a first evaluation of the methods for terminology extraction used in the project. The application situation highlighted the need for tools to handle lemmatization errors and to remove incomplete word sequences from multi-word term candidate lists, as well as the fact that the provision of German citation forms requires more morphological knowledge than TTC's slim approach can provide. We show a detailed evaluation of our extraction results and discuss the method for the evaluation of terminology extraction systems.
In this paper we report on the experiences gained in the recent construction of the SoNaR corpus, a 500 MW reference corpus of contemporary, written Dutch. It shows what can realistically be done within the confines of a project setting where there are limitations to the duration in time as well to the budget, employing current state-of-the-art tools, standards and best practices. By doing so we aim to pass on insights that may be beneficial for anyone considering to undertake an effort towards building a large, varied yet balanced corpus for use by the wider research community. Various issues are discussed that come into play while compiling a large corpus, including approaches to acquiring texts, the arrangement of IPR, the choice of text formats, and steps to be taken in the preprocessing of data from widely different origins. We describe FoLiA, a new XML format geared at rich linguistic annotations. We also explain the rationale behind the investment in the high-quali ty semi-automatic enrichment of a relatively small (1 MW) subset with very rich syntactic and semantic annotations. Finally, we present some ideas about future developments and the direction corpus development may take, such as setting up an integrated work flow between web services and the potential role for ISOcat. We list tips for potential corpus builders, tricks they may want to try and further recommendations regarding technical developments future corpus builders may wish to hope for.
The PORTMEDIA project is intended to develop new corpora for the evaluation of spoken language understanding systems. The newly collected data are in the field of human-machine dialogue systems for tourist information in French in line with the MEDIA corpus. Transcriptions and semantic annotations, obtained by low-cost procedures, are provided to allow a thorough evaluation of the systems' capabilities in terms of robustness and portability across languages and domains. A new test set with some adaptation data is prepared for each case: in Italian as an example of a new language, for ticket reservation as an example of a new domain. Finally the work is complemented by the proposition of a new high level semantic annotation scheme well-suited to dialogue data.
A treebank is an important resource for developing many NLP based tools. Errors in the treebank may lead to error in the tools that use it. It is essential to ensure the quality of a treebank before it can be deployed for other purposes. Automatic (or semi-automatic) detection of errors in the treebank can reduce the manual work required to find and remove errors. Usually, the errors found automatically are manually corrected by the annotators. There is not much work reported so far on error correction tools which helps the annotators in correcting errors efficiently. In this paper, we present such an error correction tool that is an extension of the error detection method described earlier (Ambati et al., 2010; Ambati et al., 2011; Agarwal et al., 2012).
This article presents the problem of diacritic restoration (or diacritization) in the context of spell-checking, with the focus on an orthographically rich language such as Spanish. We argue that despite the large volume of work published on the topic of diacritization, currently available spell-checking tools have still not found a proper solution to the problem in those cases where both forms of a word are listed in the checker's dictionary. This is the case, for instance, when a word form exists with and without diacritics, such as continuo continuous' and continu{\'o} he/she/it continued', or when different diacritics make other word distinctions, as in contin{\'u}o I continue'. We propose a very simple solution based on a word bigram model derived from correctly typed Spanish texts and evaluate the ability of this model to restore diacritics in artificial as well as real errors. The case of diacritics is only meant to be an example of the possible applications for this idea, yet we believe that the same method could be applied to other kinds of orthographic or even grammatical errors. Moreover, given that no explicit linguistic knowledge is required, the proposed model can be used with other languages provided that a large normative corpus is available.
We here discuss a methodology for dealing with the annotation of semantically hard to delineate, i.e., sloppy, named entity types. To illustrate sloppiness of entities, we treat an example from the medical domain, namely pathological phenomena. Based on our experience with iterative guideline refinement we propose to carefully characterize the thematic scope of the annotation by positive and negative coding lists and allow for alternative, short vs. long mention span annotations. Short spans account for canonical entity mentions (e.g., standardized disease names), while long spans cover descriptive text snippets which contain entity-specific elaborations (e.g., anatomical locations, observational details, etc.). Using this stratified approach, evidence for increasing annotation performance is provided by kappa-based inter-annotator agreement measurements over several, iterative annotation rounds using continuously refined guidelines. The latter reflects the increasing understanding of the sloppy entity class both from the perspective of guideline writers and users (annotators). Given our data, we have gathered evidence that we can deal with sloppiness in a controlled manner and expect inter-annotator agreement values around 80{\%} for PathoJen, the pathological phenomena corpus currently under development in our lab.
The recent construction of large linguistic treebanks for spoken and written Dutch (e.g. CGN, LASSY, Alpino) has created new and exciting opportunities for the empirical investigation of Dutch syntax and semantics. However, the exploitation of those treebanks requires knowledge of specific data structures and query languages such as XPath. Linguists who are unfamiliar with formal languages are often reluctant towards learning such a language. In order to make treebank querying more attractive for non-technical users we developed GrETEL (Greedy Extraction of Trees for Empirical Linguistics), a query engine in which linguists can use natural language examples as a starting point for searching the Lassy treebank without knowledge about tree representations nor formal query languages. By allowing linguists to search for similar constructions as the example they provide, we hope to bridge the gap between traditional and computational linguistics. Two case studies are conducted to provide a concrete demonstration of the tool. The architecture of the tool is optimised for searching the LASSY treebank, but the approach can be adapted to other treebank lay-outs.
We have created a scheme for annotating corpora designed to capture relevant aspects of factivity in verb-complement constructions. Factivity constructions are a well-known linguistic phenomenon that embed presuppositions about the state of the world into a clause. These embedded presuppositions provide implicit information about facts assumed to be true in the world, and are thus potentially valuable in areas of research such as textual entailment. We attempt to address both clear-cut cases of factivity and non-factivity, as well as account for the fluidity and ambiguous nature of some realizations of this construction. Our extensible scheme is designed to account for distinctions between claims, performatives, atypical uses of factivity, and the authority of the one making the utterance. We introduce a simple XML-based syntax for the annotation of factive verbs and clauses, in order to capture this information. We also provide an analysis of the issues which led to these annotative decisions, in the hope that these analyses will be beneficial to those dealing with factivity in a practical context.
We are developing and annotating a learner corpus of Hungarian, composed of student journals from three different proficiency levels written at Indiana University. Our annotation marks learner errors that are of different linguistic categories, including phonology, morphology, and syntax, but defining the annotation for an agglutinative language presents several issues. First, we must adapt an analysis that is centered on the morpheme rather than the word. Second, and more importantly, we see a need to distinguish errors from secondary corrections. We argue that although certain learner errors require a series of corrections to reach a target form, these secondary corrections, conditioned on those that come before, are our own adjustments that link the learner's productions to the target form and are not representative of the learner's internal grammar. In this paper, we report the annotation scheme and the principles that guide it, as well as examples illustrating its functionality and directions for expansion.
Operational intelligence applications in specific domains are developed using numerous natural language processing technologies and tools. A challenge for this integration is to take into account the limitations of each of these technologies in the global evaluation of the application. We present in this article a complex intelligence application for the gathering of information from the Web about recent seismic events. We present the different components needed for the development of such system, including Information Extraction, Filtering and Clustering, and the technologies behind each component. We also propose an independent evaluation of each component and an insight of their influence in the overall performance of the system.
In this paper we describe the development of a text simplification system for Spanish. Text simplification is the adaptation of a text to the special needs of certain groups of readers, such as language learners, people with cognitive difficulties and elderly people, among others. There is a clear need for simplified texts, but manual production and adaptation of existing texts is labour intensive and costly. Automatic simplification is a field which attracts growing attention in Natural Language Processing, but, to the best of our knowledge, there are no simplification tools for Spanish. We present a prototype for automatic simplification, which shows that the most important structural simplification operations can be successfully treated with an approach based on rules which can potentially be improved by statistical methods. For the development of this prototype we carried out a corpus study which aims at identifying the operations a text simplification system needs to carry out in order to produce an output similar to what human editors produce when they simplify texts.
In the last decades, a wide range of automatic metrics that use linguistic knowledge has been developed. Some of them are based on lexical information, such as METEOR; others rely on the use of syntax, either using constituent or dependency analysis; and others use semantic information, such as Named Entities and semantic roles. All these metrics work at a specific linguistic level, but some researchers have tried to combine linguistic information, either by combining several metrics following a machine-learning approach or focusing on the combination of a wide variety of metrics in a simple and straightforward way. However, little research has been conducted on how to combine linguistic features from a linguistic point of view. In this paper we present VERTa, a metric which aims at using and combining a wide variety of linguistic features at lexical, morphological, syntactic and semantic level. We provide a description of the metric and report some preliminary experiments which will help us to discuss the use and combination of certain linguistic features in order to improve the metric performance
Corpus-based treebank annotation is known to result in incomplete coverage of mid- and low-frequency linguistic constructions: the linguistic representation and corpus annotation quality are sometimes suboptimal. Large descriptive grammars cover also many mid- and low-frequency constructions. We argue for use of large descriptive grammars and their sample sentences as a basis for specifying higher-coverage grammatical representations. We present an sample case from an ongoing project (FIN-CLARIN FinnTreeBank) where an grammatical representation is documented as an annotator's manual alongside manual annotation of sample sentences extracted from a large descriptive grammar of Finnish. We outline the linguistic representation (morphology and dependency syntax) for Finnish, and show how the resulting `Grammar Definition Corpus' and the documentation is used as a task specification for an external subcontractor for building a parser engine for use in morphological and dependency syntactic analysis of large volumes of Finnish for parsebanking purposes. The resulting corpus, FinnTreeBank 3, is due for release in June 2012, and will contain tens of millions of words from publicly available corpora of Finnish with automatic morphological and dependency syntactic analysis, for use in research on the corpus linguistics and language engineering.
This paper presents a web platform with an its own graphic environment to visualize and filter multilevel phonetic annotations. The tool accepts as input Annotation Graph XML and Praat TextGrids files and converts these files into a specific XML format. XML output is used to browse data by means of a web tool using a visualization metaphor, namely a timeline. A timeline is a graphical representation of a period of time, on which relevant events are marked. Events are usually distributed over many layers in a geometrical metaphor represented by segments and points spatially distributed with reference to a temporal axis. The tool shows all the annotations included in the uploaded dataset, allowing the listening of the entire file or of its parts. Filtering is allowed on annotation labels by means of string pattern matching. The web service includes cloud services to share data with other users. The tool is available at http://w-phamt.fisica.unina.it
Accurate and reliable documentation of Language Resources is an undisputable need: documentation is the gateway to discovery of Language Resources, a necessary step towards promoting the data economy. Language resources that are not documented virtually do not exist: for this reason every initiative able to collect and harmonise metadata about resources represents a valuable opportunity for the NLP community. In this paper we describe the LRE Map, reporting statistics on resources associated with LREC2012 papers and providing comparisons with LREC2010 data. The LRE Map, jointly launched by FLaReNet and ELRA in conjunction with the LREC 2010 Conference, is an instrument for enhancing availability of information about resources, either new or already existing ones. It wants to reinforce and facilitate the use of standards in the community. The LRE Map web interface provides the possibility of searching according to a fixed set of metadata and to view the details of extracted resources. The LRE Map is continuing to collect bottom-up input about resources from authors of other conferences through standard submission process. This will help broadening the notion of language resources and attract to the field neighboring disciplines that so far have been only marginally involved by the standard notion of language resources.
The paper describes the main steps for the construction, annotation and validation of the Romanian version of the TimeBank corpus. Starting from the English TimeBank corpus ― the reference annotated corpus in the temporal domain, we have translated all the 183 English news texts into Romanian and mapped the English annotations onto Romanian, with a success rate of 96.53{\%}. Based on ISO-Time - the emerging standard for representing temporal information, which includes many of the previous annotations schemes -, we have evaluated the automatic transfer onto Romanian and, and, when necessary, corrected the Romanian annotations so that in the end we obtained a 99.18{\%} transfer rate for the TimeML annotations. In very few cases, due to language peculiarities, some original annotations could not be transferred. For the portability of the temporal annotation standard to Romanian, we suggested some additions for the ISO-Time standard, concerning especially the EVENT tag, based on linguistic evidence, the Romanian grammar, and also on the localisations of TimeML to other Romance languages. Future improvements to the Ro-TimeBank will take into consideration all temporal expressions, signals and events in texts, even those with a not very clear temporal anchoring.
We present a framework for the acquisition of sentential paraphrases based on crowdsourcing. The proposed method maximizes the lexical divergence between an original sentence s and its valid paraphrases by running a sequence of paraphrasing jobs carried out by a crowd of non-expert workers. Instead of collecting direct paraphrases of s, at each step of the sequence workers manipulate semantically equivalent reformulations produced in the previous round. We applied this method to paraphrase English sentences extracted from Wikipedia. Our results show that, keeping at each round n the most promising paraphrases (i.e. the more lexically dissimilar from those acquired at round n-1), the monotonic increase of divergence allows to collect good-quality paraphrases in a cost-effective manner.
In this paper, we describe the Nordic Dialect Corpus, which has recently been completed. The corpus has a variety of features that combined makes it an advanced tool for language researchers. These features include: Linguistic contents (dialects from five closely related languages), annotation (tagging and two types of transcription), search interface (advanced possibilities for combining a large array of search criteria and results presentation in an intuitive and simple interface), many search variables (linguistics-based, informant-based, time-based), multimedia display (linking of sound and video to transcriptions), display of results in maps, display of informant details (number of words and other information on informants), advanced results handling (concordances, collocations, counts and statistics shown in a variety of graphical modes, plus further processing). Finally, and importantly, the corpus is freely available for research on the web. We give examples of both various kinds of searches, of displays of results and of results handling.
We present the WeSearch Data Collection (WDC)―a freely redistributable, partly annotated, comprehensive sample of User-Generated Content. The WDC contains data extracted from a range of genres of varying formality (user forums, product review sites, blogs and Wikipedia) and covers two different domains (NLP and Linux). In this article, we describe the data selection and extraction process, with a focus on the extraction of linguistic content from different sources. We present the format of syntacto-semantic annotations found in this resource and present initial parsing results for these data, as well as some reflections following a first round of treebanking.
In the paper the results of the project of Czech Legal Electronic dictionary (PES) are presented. During the 4 year project the large legal terminological dictionary of Czech was created in the form of the electronic lexical database enriched with a hierarchical ontology of legal terms. It contains approx. 10,000 entries ― legal terms together with their ontological relations and hypertext references. In the second part of the project the web interface based on the platform DEBII has been designed and implemented that allows users to browse and search effectively the database. At the same time the Czech Dictionary of Legal Terms will be generated from the database and later printed as a book. Inter-annotator's agreement in manual selection of legal terms was high ― approx. 95 {\%}.
Although previous studies have shown that errors occur in texts summarized by extraction based summarizers, no study has investigated how common different types of errors are and how that changes with degree of summarization. We have conducted studies of errors in extraction based single document summaries using 30 texts, summarized to 5 different degrees and tagged for errors by human judges. The results show that the most common errors are absent cohesion or context and various types of broken or missing anaphoric references. The amount of errors is dependent on the degree of summarization where some error types have a linear relation to the degree of summarization and others have U-shaped or cut-off linear relations. These results show that the degree of summarization has to be taken into account to minimize the amount of errors by extraction based summarizers.
The FLaReNet Strategic Agenda highlights the most pressing needs for the sector of Language Resources and Technologies and presents a set of recommendations for its development and progress in Europe, as issued from a three-year consultation of the FLaReNet European project. The FLaReNet recommendations are organised around nine dimensions: a) documentation b) interoperability c) availability, sharing and distribution d) coverage, quality and adequacy e) sustainability f) recognition g) development h) infrastructure and i) international cooperation. As such, they cover a broad range of topics and activities, spanning over production and use of language resources, licensing, maintenance and preservation issues, infrastructures for language resources, resource identification and sharing, evaluation and validation, interoperability and policy issues. The intended recipients belong to a large set of players and stakeholders in Language Resources and Technology, ranging from individuals to research and education institutions, to policy-makers, funding agencies, SMEs and large companies, service and media providers. The main goal of these recommendations is to serve as an instrument to support stakeholders in planning for and addressing the urgencies of the Language Resources and Technologies of the future.
This paper presents an annotation scheme for English modal verbs together with sense-annotated data from the news domain. We describe our annotation scheme and discuss problematic cases for modality annotation based on the inter-annotator agreement during the annotation. Furthermore, we present experiments on automatic sense tagging, showing that our annotations do provide a valuable training resource for NLP systems.
Laughter is a significant paralinguistic cue that is largely ignored in multimodal affect analysis. In this work, we investigate how a multimodal laughter corpus can be constructed and annotated both with discrete and dimensional labels of emotions for acted and spontaneous laughter. Professional actors enacted emotions to produce acted clips, while spontaneous laughter was collected from volunteers. Experts annotated acted laughter clips, while volunteers who possess an acceptable empathic quotient score annotated spontaneous laughter clips. The data was pre-processed to remove noise from the environment, and then manually segmented starting from the onset of the expression until its offset. Our findings indicate that laughter carries distinct emotions, and that emotion in laughter is best recognized using audio information rather than facial information. This may be explained by emotion regulation, i.e. laughter is used to suppress or regulate certain emotions. Furthermore, contextual information plays a crucial role in understanding the kind of laughter and emotion in the enactment.
The LAST MINUTE corpus comprises multimodal recordings (e.g. video, audio, transcripts) from WOZ interactions in a mundane planning task (R{\"o
We describe, and make public, large-scale language resources and the toolchain used in their creation, for fifteen medium density European languages: Catalan, Czech, Croatian, Danish, Dutch, Finnish, Lithuanian, Norwegian, Polish, Portuguese, Romanian, Serbian, Slovak, Spanish, and Swedish. To make the process uniform across languages, we selected tools that are either language-independent or easily customizable for each language, and reimplemented all stages that were taking too long. To achieve processing times that are insignificant compared to the time data collection (crawling) takes, we reimplemented the standard sentence- and word-level tokenizers and created new boilerplate and near-duplicate detection algorithms. Preliminary experiments with non-European languages indicate that our methods are now applicable not just to our sample, but the entire population of digitally viable languages, with the main limiting factor being the availability of high quality stemmers.
One of the main resources used for the task of bilingual lexicon extraction from comparable corpora is : the bilingual dictionary, which is considered as a bridge between two languages. However, no particular attention has been given to this lexicon, except its coverage, and the fact that it can be issued from the general language, the specialised one, or a mix of both. In this paper, we want to highlight the idea that a better consideration of the bilingual dictionary by studying its entries and filtering the non-useful ones, leads to a better lexicon extraction and thus, reach a higher precision. The experiments are conducted on a medical domain corpora. The French-English specialised corpus 'breast cancer' of 1 million words. We show that the empirical results obtained with our filtering process improve the standard approach traditionally dedicated to this task and are promising for future work.
We describe efforts to create corpora to support development and evaluation of handwriting recognition and translation technology. LDC has developed a stable pipeline and infrastructures for collecting and annotating handwriting linguistic resources to support the evaluation of MADCAT and OpenHaRT. We collect and annotate handwritten samples of pre-processed Arabic and Chinese data that has been already translated in English that is used in the GALE program. To date, LDC has recruited more than 600 scribes and collected, annotated and released more than 225,000 handwriting images. Most linguistic resources created for these programs will be made available to the larger research community by publishing in LDC's catalog. The phase 1 MADCAT corpus is now available.
Prosodic research in recent years has been supported by a number of automatic analysis tools aimed at simplifying the work that is requested to study intonation. The need to analyze large amounts of data and to inspect phenomena that are often ambiguous and difficult to model makes the prosodic research area an ideal application field for computer based processing. One of the main challenges in this field is to model the complex relations occurring between the segmental level, mainly in terms of syllable nuclei and boundaries, and the supra-segmental level, mainly in terms of tonal movements. The goal of our contribution is to provide a tool for automatic annotation of prosodic data, the Prosomarker, designed to give a visual representation of both segmental and suprasegmental events. The representation is intended to be as generic as possible to let researchers analyze specific phenomena without being limited by assumptions introduced by the annotation itself. A perceptual account of the pitch curve is provided along with an automatic segmentation of the speech signal into syllable-like segments and the tool can be used both for data exploration, in semi-automatic mode, and to process large sets of data, in automatic mode.
In this paper we describe the research that was carried out and the resources that were developed within the DISCO (Development and Integration of Speech technology into COurseware for language learning) project. This project aimed at developing an ASR-based CALL system that automatically detects pronunciation and grammar errors in Dutch L2 speaking and generates appropriate, detailed feedback on the errors detected. We briefly introduce the DISCO system and present its design, architecture and speech recognition modules. We then describe a first evaluation of the complete DISCO system and present some results. The resources generated through DISCO are subsequently described together with possible ways of efficiently generating additional resources in the future.
In this paper, the METU Turkish Discourse Bank Browser, a tool developed for browsing the annotated annotated discourse relations in Middle East Technical University (METU) Turkish Discourse Bank (TDB) project is presented. The tool provides both a clear interface for browsing the annotated corpus and a wide range of search options to analyze the annotations.
The present article describes the first stage of the KorAP project, launched recently at the Institut f{\"u
Natural language interfaces to data services will be a key technology to guarantee access to huge data repositories in an effortless way. This involves solving the complex problem of recognizing a relevant service or service composition given an ambiguous, potentially ungrammatical natural language question. As a first step toward this goal, we study methods for identifying the salient terms (or foci) in natural language questions, classifying the latter according to a taxonomy of services and extracting additional relevant information in order to route them to suitable data services. While current approaches deal with single-focus (and therefore single-domain) questions, we investigate multi-focus questions in the aim of supporting conjunctive queries over the data services they refer to. Since such complex queries have seldom been studied in the literature, we have collected an ad-hoc dataset, SeCo-600, containing 600 multi-domain queries annotated with a number of linguistic and pragmatic features. Our experiments with the dataset have allowed us to reach very high accuracy in different phases of query analysis, especially when adopting machine learning methods.
The Semantic Search research field aims to query metadata and to identify relevant subgraphs. While in traditional search engines queries are composed by lists of keywords connected through boolean operators, Semantic Search instead, requires the submission of semantic queries that are structured as a graph of concepts, entities and relations. Submission of this graph is however not trivial as while a list of keywords of interest can be provided by any user, the formulation of semantic queries is not easy as well. One of the main challenges of RDF Browsers lies in the implementation of interfaces that allow the common user to submit semantic queries by hiding their complexity. Furthermore a good semantic search algorithm is not enough to fullfil user needs, it is worthwhile to implement visualization methods which can support users in intuitively understanding why and how the results were retrieved. In this paper we present a novel solution to query RDF datasets and to browse the results of the queries in an appealing manner.
Collocations can be defined as words that occur together significantly more often than it would be expected by chance. Many natural language processing applications such as natural language generation, word sense disambiguation and machine translation can benefit from having access to information about collocated words. We approach collocation extraction as a classification problem where the task is to classify a given n-gram as either a collocation (positive) or a non-collocation (negative). Among the features used are word frequencies, classical association measures (Dice, PMI, chi2), and POS tags. In addition, semantic word relatedness modeled by latent semantic analysis is also included. We apply wrapper feature subset selection to determine the best set of features. Performance of various classification algorithms is tested. Experiments are conducted on a manually annotated set of bigrams and trigrams sampled from a Croatian newspaper corpus. Best results obtained are 79.8 F1 measure for bigrams and 67.5 F1 measure for trigrams. The best classifier for bigrams was SVM, while for trigrams the decision tree gave the best performance. Features which contributed the most to overall performance were PMI, semantic relatedness, and POS information.
This paper documents a pilot study conducted as part of the development of a new corpus processing system at the Institut f{\"u
An increased number of machine translation services are now available. Unfortunately, none of them can provide adequate translation quality for all input sources. This forces the user to select from among the services according to his needs. However, it is tedious and time consuming to perform this manual selection. Our solution, proposed here, is an automatic mechanism that can select the most appropriate machine translation service. Although evaluation methods are available, such as BLEU, NIST, WER, etc., their evaluation results are not unanimous regardless of the translation sources. We proposed a two-phase architecture for selecting translation services. The first phase uses a data-driven classification to allow the most appropriate evaluation method to be selected according to each translation source. The second phase selects the most appropriate machine translation result by the selected evaluation method. We describe the architecture, detail the algorithm, and construct a prototype. Tests show that the proposal yields better translation quality than employing just one machine translation service.
In this paper we present a metric that measures comparability of documents across different languages. The metric is developed within the FP7 ICT ACCURAT project, as a tool for aligning comparable corpora on the document level; further these aligned comparable documents are used for phrase alignment and extraction of translation equivalents, with the aim to extend phrase tables of statistical MT systems without the need to use parallel texts. The metric uses several features, such as lexical information, document structure, keywords and named entities, which are combined in an ensemble manner. We present the results by measuring the reliability and effectiveness of the metric, and demonstrate its application and the impact for the task of parallel phrase extraction from comparable corpora.
Navigation in large scholarly paper collections is tedious and not well supported in most scientific digital libraries. We describe a novel browser-based graphical tool implemented using HTML5 Canvas. It displays citation information extracted from the paper text to support useful navigation. The tool is implemented using a client/server architecture. A citation graph of the digital library is built in the memory of the server. On the client side, egdes of the displayed citation (sub)graph surrounding a document are labeled with keywords signifying the kind of citation made from one document to another. These keywords were extracted using NLP tools such as tokenizer, sentence boundary detection and part-of-speech tagging applied to the text extracted from the original PDF papers (currently 22,500). By clicking on an egde, the user can inspect the corresponding citation sentence in context, in most cases even also highlighted in the original PDF layout. The system is publicly accessible as part of the ACL Anthology Searchbench.
Increases in the use of web data for corpus-building, coupled with the use of specialist, single-use corpora, make for an increasing reliance on language that changes quickly, affecting the long-term validity of studies based on these methods. This drift' through time affects both users of open-source corpora and those attempting to interpret the results of studies based on web data. The attrition of documents online, also called link rot or document half-life, has been studied many times for the purposes of optimising search engine web crawlers, producing robust and reliable archival systems, and ensuring the integrity of distributed information stores, however, the affect that attrition has upon corpora of varying construction remains largely unknown. This paper presents a preliminary investigation into the differences in attrition rate between corpora selected using different corpus construction methods. It represents the first step in a larger longitudinal analysis, and as such presents URI-based content clues, chosen to relate to studies from other areas. The ultimate goal of this larger study is to produce a detailed enumeration of the primary biases online, and identify sampling strategies which control and minimise unwanted effects of document attrition.
Due to instant availability and low cost, machine translation is becoming popular. Machine translation mediated communication plays a more and more important role in international collaboration. However, machine translators cannot guarantee high quality translation. In a multilingual communication task, many in-domain resources, for example domain dictionaries, are needed to promote translation quality. This raises the problem of how to help communication task designers provide higher quality translation systems, systems that can take advantage of various in-domain resources. The Language Grid, a service-oriented collective intelligent platform, allows in-domain resources to be wrapped into language services. For task-oriented translation, we propose service composition scenarios for the composition of different language services, where various in-domain resources are utilized effectively. We design the architecture, provide a script language as the interface for the task designer, which is easy for describing the composition scenario, and make a case study of a Japanese-English campus orientation task. Based on the case study, we analyze the increase in translation quality possible and the usage of in-domain resources. The results demonstrate a clear improvement in translation accuracy when the in-domain resources are used.
Use of language resources including annotated corpora and tools is not easy for users, as it requires expert knowledge to determine which resources are compatible and interoperable. Sometimes it requires programming skill in addition to the expert knowledge to make the resources compatible and interoperable when the resources are not created so. If a platform system could provide automation features for using language resources, users do not have to waste their time as the above issues are not necessarily essential for the users' goals. While our system, Kachako, provides such automation features for single-modal resources, multi-modal resources are more difficult to combine automatically. In this paper, we discuss designs of multi-modal resource compatibility and interoperability from such an automation point of view in order for the Kachako system to provide automation features of multi-modal resources. Our discussion is based on the UIMA framework, and focuses on resource metadata description optimized for ideal automation features while harmonizing with the UIMA framework using other standards as well.
The Quaero program has organized a set of evaluations for terminology extraction systems in 2010 and 2011. Three objectives were targeted in this initiative: the first one was to evaluate the behavior and scalability of term extractors regarding the size of corpora, the second goal was to assess progress between different versions of the same systems, the last one was to measure the influence of corpus type. The protocol used during this initiative was a comparative analysis of 32 runs against a gold standard. Scores were computed using metrics that take into account gradual relevance. Systems produced by Quaero partners and publicly available systems were evaluated on pharmacology corpora composed of European Patents or abstracts of scientific articles, all in English. The gold standard was an unstructured version of the pharmacology thesaurus used by INIST-CNRS for indexing purposes. Most systems scaled with large corpora, contrasted differences were observed between different versions of the same systems and with better results on scientific articles than on patents. During the ongoing adjudication phase domain experts are enriching the thesaurus with terms found by several systems.
Thanks to their rich morphology, Italian and Spanish allow pro-drop pronouns, i.e., non lexically-realized subject pronouns. Here we distinguish between two different types of null subjects: personal pro-drop and impersonal pro-drop. We evaluate the translation of these two categories into French, a non pro-drop language, using Its-2, a transfer-based system developed at our laboratory; and Moses, a statistical system. Three different corpora are used: two subsets of the Europarl corpus and a third corpus built using newspaper articles. Null subjects turn out to be quantitatively important in all three corpora, but their distribution varies depending on the language and the text genre though. From a MT perspective, translation results are determined by the type of pro-drop and the pair of languages involved. Impersonal pro-drop is harder to translate than personal pro-drop, especially for the translation from Italian into French, and a significant portion of incorrect translations consists of missing pronouns.
The European Commission's (EC) Directorate General for Translation, together with the EC's Joint Research Centre, is making available a large translation memory (TM; i.e. sentences and their professionally produced translations) covering twenty-two official European Union (EU) languages and their 231 language pairs. Such a resource is typically used by translation professionals in combination with TM software to improve speed and consistency of their translations. However, this resource has also many uses for translation studies and for language technology applications, including Statistical Machine Translation (SMT), terminology extraction, Named Entity Recognition (NER), multilingual classification and clustering, and many more. In this reference paper for DGT-TM, we introduce this new resource, provide statistics regarding its size, and explain how it was produced and how to use it.
The Arabic language is a collection of dialectal variants along with the standard form, Modern Standard Arabic (MSA). MSA is used in official Settings while the dialectal variants (DA) correspond to the native tongue of the Arabic speakers. Arabic speakers typically code switch between DA and MSA, which is reflected extensively in written online social media. Automatic processing such Arabic genre is very difficult for automated NLP tools since the linguistic difference between MSA and DA is quite profound. However, no annotated resources exist for marking the regions of such switches in the utterance. In this paper, we present a simplified Set of guidelines for detecting code switching in Arabic on the word/token level. We use these guidelines in annotating a corpus that is rich in DA with frequent code switching to MSA. We present both a quantitative and qualitative analysis of the annotations.
We describe META-SHARE which aims at providing an open, distributed, secure, and interoperable infrastructure for the exchange of language resources, including both data and tools. The application has been designed and is developed as part of the T4ME Network of Excellence. We explain the underlying motivation for such a distributed repository for metadata storage and give a detailed overview on the META-SHARE application and its various components. This includes a discussion of the technical architecture of the system as well as a description of the component-based metadata schema format which has been developed in parallel. Development of the META-SHARE infrastructure adopts state-of-the-art technology and follows an open-source approach, allowing the general community to participate in the development process. The META-SHARE software package including full source code has been released to the public in March 2012. We look forward to present an up-to-date version of the META-SHARE software at the conference.
In this paper, we present and evaluate an approach for the compositional alignment of compound nouns using comparable corpora from technical domains. The task of term alignment consists in relating a source language term to its translation in a list of target language terms with the help of a bilingual dictionary. Compound splitting allows to transform a compound into a sequence of components which can be translated separately and then related to multi-word target language terms. We present and evaluate a method for compound splitting, and compare two strategies for term alignment (bag-of-word vs. pattern-based). The simple word-based approach leads to a considerable amount of erroneous alignments, whereas the pattern-based approach reaches a decent precision. We also assess the reasons for alignment failures: in the comparable corpora used for our experiments, a substantial number of terms has no translation in the target language data; furthermore, the non-isomorphic structures of source and target language terms cause alignment failures in many cases.
Due to the rapid growth in the volume of biomedical literature, there is an increasing requirement for high-performance semantic search systems, which allow biologists to perform precise searches for events of interest. Such systems are usually trained on corpora of documents that contain manually annotated events. Until recently, these corpora, and hence the event extraction systems trained on them, focussed almost exclusively on the identification and classification of event arguments, without taking into account how the textual context of the events could affect their interpretation. Previously, we designed an annotation scheme to enrich events with several aspects (or dimensions) of interpretation, which we term meta-knowledge, and applied this scheme to the entire GENIA corpus. In this paper, we report on our experiments to automate the assignment of one of these meta-knowledge dimensions, i.e. Manner, to recognised events. Manner is concerned with the rate, strength intensity or level of the event. We distinguish three different values of manner, i.e., High, Low and Neutral. To our knowledge, our work represents the first attempt to classify the manner of events. Using a combination of lexical, syntactic and semantic features, our system achieves an overall accuracy of 99.4{\%}.
Depending on the nature of a linguistic theory, empirical investigations of its soundness may focus on corpus studies related to lexical, syntactic, semantic or other phenomena. Especially work in research networks usually comprises analyses of different levels of description, where each one must be as reliable as possible when the same sentences and texts are investigated under very different perspectives. This paper describes an infrastructure that interfaces an analysis tool for multi-level annotation with a generic relational database. It supports three dimensions of analysis-handling and thereby builds an integrated environment for quality assurance in corpus based linguistic analysis: a vertical dimension relating analysis components in a pipeline, a horizontal dimension taking alternative results of the same analysis level into account and a temporal dimension to follow up cases where analyses for the same input have been produced with different versions of a tool. As an example we give a detailed description of a typical workflow for the vertical dimension.
We describe the usefulness of Wiktionary, the freely available web-based lexical resource, in providing multilingual extensions to catalogues that serve content-based indexing of folktales and related narratives. We develop conversion tools between Wiktionary and TEI, using ISO standards (LMF, MAF), to make such resources available to both the Digital Humanities community and the Language Resources community. The converted data can be queried via a web interface, while the tools of the workflow are to be released with an open source license. We report on the actual state and functionality of our tools and analyse some shortcomings of Wiktionary, as well as potential domains of application.
This paper presents a method to build CAPT systems for under resourced languages, as Basque, using a general purpose ASR speech database. More precisely, the proposed method consists in automatically determine the threshold of GOP (Goodness Of Pronunciation) scores, which have been used as pronunciation scores in phone-level. Two score distributions have been obtained for each phoneme corresponding to its correct and incorrect pronunciations. The distribution of the scores for erroneous pronunciation has been calculated inserting controlled errors in the dictionary, so that each changed phoneme has been randomly replaced by a phoneme from the same group. These groups have been obtained by means of a phonetic clustering performed using regression trees. After obtaining both distributions, the EER (Equal Error Rate) of each distribution pair has been calculated and used as a decision threshold for each phoneme. The results show that this method is useful when there is no database specifically designed for CAPT systems, although it is not as accurate as those specifically designed for this purpose.
Uncertainty language permeates biomedical research and is fundamental for the computer interpretation of unstructured text. And yet, a coherent, cognitive-based theory to interpret Uncertainty language and guide Natural Language Processing is, to our knowledge, non-existing. The aim of our project was therefore to detect and annotate Uncertainty markers ― which play a significant role in building knowledge or beliefs in readers' minds ― in a biomedical research corpus. Our corpus includes 80 manually annotated articles from the British Medical Journal randomly sampled from a 168-year period. Uncertainty markers have been classified according to a theoretical framework based on a combined linguistic and cognitive theory. The corpus was manually annotated according to such principles. We performed preliminary experiments to assess the manually annotated corpus and establish a baseline for the automatic detection of Uncertainty markers. The results of the experiments show that most of the Uncertainty markers can be recognized with good accuracy.
This paper reports on the development of new language resources for the Pashto language, a very low-resource language spoken in Afghanistan and Pakistan. In the scope of a multilingual data collection project, three large corpora are collected for Pashto. Firstly a monolingual text corpus of 100 million words is produced. Secondly a 100 hours speech database is recorded and manually transcribed. Finally a bilingual Pashto-French parallel corpus of around 2 million is produced by translating Pashto texts into French. These resources will be used to develop Human Language Technology systems for Pashto with a special focus on Machine Translation.
In this paper, we report our efforts in building a multi-lingual multi-party online chat corpus in order to develop a firm understanding in a set of social constructs such as agenda control, influence, and leadership as well as to computationally model such constructs in online interactions. These automated models will help capture the dialogue dynamics that are essential for developing, among others, realistic human-machine dialogue systems, including autonomous virtual chat agents. In this paper, we first introduce our experiment design and data collection method in Chinese and Urdu, and then report on the current stage of our data collection. We annotated the collected corpus on four levels: communication links, dialogue acts, local topics, and meso-topics. Results from the analyses of annotated data on different languages indicate some interesting phenomena, which are reported in this paper.
A number of gold standard corpora for named entity recognition are available to the public. However, the existing gold standard corpora are limited in size and semantic entity types. These usually lead to implementation of trained solutions (1) for a limited number of semantic entity types and (2) lacking in generalization capability. In order to overcome these problems, the CALBC project has aimed to automatically generate large scale corpora annotated with multiple semantic entity types in a community-wide manner based on the consensus of different named entity solutions. The generated corpus is called the silver standard corpus since the corpus generation process does not involve any manual curation. In this publication, we announce the release of the final CALBC corpora which include the silver standard corpus in different versions and several gold standard corpora for the further usage of the biomedical text mining community. The gold standard corpora are utilised to benchmark the methods used in the silver standard corpora generation process and released in a shared format. All the corpora are released in a shared format and accessible at www.calbc.eu.
This paper presents a web-based multimedia search engine built within the Buceador (www.buceador.org) research project. A proof-of-concept tool has been implemented which is able to retrieve information from a digital library made of multimedia documents in the 4 official languages in Spain (Spanish, Basque, Catalan and Galician). The retrieved documents are presented in the user language after translation and dubbing (the four previous languages + English). The paper presents the tool functionality, the architecture, the digital library and provide some information about the technology involved in the fields of automatic speech recognition, statistical machine translation, text-to-speech synthesis and information retrieval. Each technology has been adapted to the purposes of the presented tool as well as to interact with the rest of the technologies involved.
This paper presents a linguistic processing pipeline for Bulgarian including morphological analysis, lemmatization and syntactic analysis of Bulgarian texts. The morphological analysis is performed by three modules ― two statistical-based and one rule-based. The combination of these modules achieves the best result for morphological tagging of Bulgarian over a rich tagset (680 tags). The lemmatization is based on rules, generated from a large morphological lexicon of Bulgarian. The syntactic analysis is implemented via MaltParser. The two statistical morphological taggers and MaltParser are trained on datasets constructed within BulTreeBank project. The processing pipeline includes also a sentence splitter and a tokenizer. All tools in the pipeline are packed in modules that can also perform separately. The whole pipeline is designed to be able to serve as a back-end of a web service oriented interface, but it also supports the user tasks with a command-line interface. The processing pipeline is compatible with the Text Corpus Format, which allows it to delegate the management of the components to the WebLicht platform.
We present a new way to get more morphologically and syntactically annotated data. We have developed an annotation editor tailored to school children to involve them in text annotation. Using this editor, they practice morphology and dependency-based syntax in the same way as they normally do at (Czech) schools, without any special training. Their annotation is then automatically transformed into the target annotation schema. The editor is designed to be language independent, however the subsequent transformation is driven by the annotation framework we are heading for. In our case, the object language is Czech and the target annotation scheme corresponds to the Prague Dependency Treebank annotation framework.
This demo presents the second prototype of WordNet Atlas, a web application that gives users the ability to navigate and visualize the 146,312 word senses of the nouns contained within the Princeton WordNet. Two complementary, interlinked visualizations are provided: an hypertextual dictionary to represent detailed information about a word sense, such as lemma, definition and depictions, and a zoomable map representing the taxonomy of noun synsets in a circular layout. The application could help users unfamiliar with WordNet to get oriented in the large amount of data it contains.
Over the last decade, methods of web corpus construction and the evaluation of web corpora have been actively researched. Prominently, the WaCky initiative has provided both theoretical results and a set of web corpora for selected European languages. We present a software toolkit for web corpus construction and a set of siginificantly larger corpora (up to over 9 billion tokens) built using this software. First, we discuss how the data should be collected to ensure that it is not biased towards certain hosts. Then, we describe our software toolkit which performs basic cleanups as well as boilerplate removal, simple connected text detection as well as shingling to remove duplicates from the corpora. We finally report evaluation results of the corpora built so far, for example w.r.t. the amount of duplication contained and the text type/genre distribution. Where applicable, we compare our corpora to the WaCky corpora, since it is inappropriate, in our view, to compare web corpora to traditional or balanced corpora. While we use some methods applied by the WaCky initiative, we can show that we have introduced incremental improvements.
This paper describes the ANNODIS resource, a discourse-level annotated corpus for French. The corpus combines two perspectives on discourse: a bottom-up approach and a top-down approach. The bottom-up view incrementally builds a structure from elementary discourse units, while the top-down view focuses on the selective annotation of multi-level discourse structures. The corpus is composed of texts that are diversified with respect to genre, length and type of discursive organisation. The methodology followed here involves an iterative design of annotation guidelines in order to reach satisfactory inter-annotator agreement levels. This allows us to raise a few issues relevant for the comparison of such complex objects as discourse structures. The corpus also serves as a source of empirical evidence for discourse theories. We present here two first analyses taking advantage of this new annotated corpus --one that tested hypotheses on constraints governing discourse structure, and another that studied the variations in composition and signalling of multi-level discourse structures.
The work presented in this paper explores the use of Indonesian transliteration to support English pronunciation practice. It is mainly aimed for Indonesian speakers who have no or minimum English language skills. The approach implemented combines a rule-based and a statistical method. The rules of English-Phone-to-Indonesian-Grapheme mapping are implemented with a Finite State Transducer (FST), followed by a statistical method which is a grapheme-based trigram language model. The Indonesian transliteration generated was used as a means to support the learners where their speech were then recorded. The speech recordings have been evaluated by 19 participants: 8 English native and 11 non-native speakers. The results show that the transliteration positively contributes to the improvement of their English pronunciation.
In this article, we present a distributional analysis method for extracting nominalization relations from monolingual corpora. The acquisition method makes use of distributional and morphological information to select nominalization candidates. We explain how the learning is performed on a dependency annotated corpus and describe the nominalization results. Furthermore, we show how these results served to enrich an existing lexical resource, the WOLF (Wordnet Libre du Franc{\^A}{\c{}}ais). We present the techniques that we developed in order to integrate the new information into WOLF, based on both its structure and content. Finally, we evaluate the validity of the automatically obtained information and the correctness of its integration into the semantic resource. The method proved to be useful for boosting the coverage of WOLF and presents the advantage of filling verbal synsets, which are particularly difficult to handle due to the high level of verbal polysemy.
This paper describes the International Standard Language Resource Number (ISLRN), a new identification schema for Language Resources where a Language Resource is provided with a unique and universal name using a standardized nomenclature. This will ensure that Language Resources be identified, accessed and disseminated in a unique manner, thus allowing them to be recognized with proper references in all activities concerning Human Language Technologies as well as in all documents and scientific papers. This would allow, for instance, the formal identification of potentially repeated resources across different repositories, the formal referencing of language resources and their correct use when different versions are processed by tools.
Custom machine translation (MT) engines systematically outperform general-domain MT engines when translating within the relevant custom domain. This paper investigates the use of the Jensen-Shannon divergence measure for automatically routing new documents within a translation system with multiple MT engines to the appropriate custom MT engine in order to obtain the best translation. Three distinct domains are compared, and the impact of the language, size, and preprocessing of the documents on the Jensen-Shannon score is addressed. Six test datasets are then compared to the three known-domain corpora to predict which of the three custom MT engines they would be routed to at runtime given their Jensen-Shannon scores. The results are promising for incorporating this divergence measure into a translation workflow.
This paper introduces the RWTH-PHOENIX-Weather corpus, a video-based, large vocabulary corpus of German Sign Language suitable for statistical sign language recognition and translation. In contrastto most available sign language data collections, the RWTH-PHOENIX-Weather corpus has not been recorded for linguistic research but for the use in statistical pattern recognition. The corpus contains weather forecasts recorded from German public TV which are manually annotated using glosses distinguishing sign variants, and time boundaries have been marked on the sentence and the gloss level. Further, the spoken German weather forecast has been transcribed in a semi-automatic fashion using a state-of-the-art automatic speech recognition system. Moreover, an additional translation of the glosses into spoken German has been created to capture allowable translation variability. In addition to the corpus, experimental baseline results for hand and head tracking, statistical sign language recognition and translation are presented.
This paper describes the KnowledgeStore, a large-scale infrastructure for the combined storage and interlinking of multimedia resources and ontological knowledge. Information in the KnowledgeStore is organized around entities, such as persons, organizations and locations. The system allows (i) to import background knowledge about entities, in form of annotated RDF triples; (ii) to associate resources to entities by automatically recognizing, coreferring and linking mentions of named entities; and (iii) to derive new entities based on knowledge extracted from mentions. The KnowledgeStore builds on state of art technologies for language processing, including document tagging, named entity extraction and cross-document coreference. Its design provides for a tight integration of linguistic and semantic features, and eases the further processing of information by explicitly representing the contexts where knowledge and mentions are valid or relevant. We describe the system and report about the creation of a large-scale KnowledgeStore instance for storing and integrating multimedia contents and background knowledge relevant to the Italian Trentino region.
This paper elaborates on a sustainability model for Language Resources, both at a descriptive and analytical level. The first part, devoted to the descriptive model, elaborates on the definition of this concept both from a general point of view and from the Human Language Technology and Language Resources perspective. The paper also intends to list an exhaustive number of factors that have an impact on this sustainability. These factors will be clustered into Pillars so as ease understanding as well as the prediction of LR sustainability itself. Rather than simply identifying a set of LRs that have been in use for a while and that one can consider as sustainable, the paper aims at first clarifying and (re)defining the concept of sustainability by also connecting it to other domains. Then it also presents a detailed decomposition of all dimensions of Language Resource features that can contribute and/or have an impact on such sustainability. Such analysis will also help anticipate and forecast sustainability for a LR before taking any decisions concerning design and production.
This paper introduces the CLIMB (Comparative Libraries of Implementations with Matrix Basis) methodology and grammars. The basic idea behind CLIMB is to use code generation as a general methodology for grammar development in order to create a more systematic approach to grammar development. The particular method used in this paper is closely related to the LinGO Grammar Matrix. Like the Grammar Matrix, resulting grammars are HPSG grammars that can map bidirectionally between strings and MRS representations. The main purpose of this paper is to provide insight into the process of using CLIMB for grammar development. In addition, we describe three projects that make use of this methodology or have concrete plans to adapt CLIMB in the future: CLIMB for Germanic languages, CLIMB for Slavic languages and CLIMB to combine two grammars of Mandarin Chinese. We present the first results that indicate feasibility and development time improvements for creating a medium to large coverage precision grammar.
In this paper, we present new bibliographical reference corpora in digital humanities (DH) that have been developed under a research project, Robust and Language Independent Machine Learning Approaches for Automatic Annotation of Bibliographical References in DH Books supported by Google Digital Humanities Research Awards. The main target is the bibliographical references in the articles of Revues.org site, an oldest French online journal platform in DH field. Since the final object is to provide automatic links between related references and articles, the automatic recognition of reference fields like author and title is essential. These fields are therefore manually annotated using a set of carefully defined tags. After providing a full description of three corpora, which are separately constructed according to the difficulty level of annotation, we briefly introduce our experimental results on the first two corpora. A popular machine learning technique, Conditional Random Field (CRF) is used to build a model, which automatically annotates the fields of new references. In the experiments, we first establish a standard for defining features and labels adapted to our DH reference data. Then we show our new methodology against less structured references gives a meaningful result.
This article presents a corpus featuring children playing games in interaction with the humanoid robot Nao: children have to express emotions in the course of a storytelling by the robot. This corpus was collected to design an affective interactive system driven by an interactional and emotional representation of the user. We evaluate here some mid-level markers used in our system: reaction time, speech duration and intensity level. We also question the presence of affect bursts, which are quite numerous in our corpus, probably because of the young age of the children and the absence of predefined lexical content.
Arabic is a morphologically rich language, and Arabic texts abound of complex word forms built by concatenation of multiple subparts, corresponding for instance to prepositions, articles, roots prefixes, or suffixes. The development of Arabic Natural Language Processing applications, such as Machine Translation (MT) tools, thus requires some kind of morphological analysis. In this paper, we compare various strategies for performing such preprocessing, using generic machine learning techniques. The resulting tool is compared with two open domain alternatives in the context of a statistical MT task and is shown to be faster than its competitors, with no significant difference in MT quality.
The ''''''``AAC - Austrian Academy Corpus'''''''' is a diachronic German language digital text corpus of more than 500 million tokens. The text corpus has collected several thousands of texts representing a wide range of different text types. The primary research aim is to develop text language resources for the study of texts. For corpus linguistics and corpus based language research large text corpora need to be structured in a systematic way. For this structural purpose the AAC is making use of the notion of container. By container in the context of corpus research we understand a flexible system of pragmatic representation, manipulation, modification and structured storage of annotated items of text. The issue of representing a large corpus in formats that offer only limited space is paradigmatic for the general task of representing a language by just a small collection of text or a small sample of the language. Methods based upon structural normalization and standardization have to be developed in order to provide useful instruments for text studies.
This paper proposes a basic scheme for annotating anaphoric relations in Japanese conversations. More specifically, we propose methods of (i) dividing discourse segments into meaningful units, (ii) identifying zero pronouns and other overt anaphors, (iii) classifying zero pronouns, and (iv) identifying anaphoric relations. We discuss various kinds of problems involved in the annotation mainly caused by on-line processing of discourse and/or interactions between the participants. These problems do not arise in annotating written languages. This paper also proposes a method to compute topic continuity based on anaphoric relations. The topic continuity involves the information status of the noun in question (given, accessible, and new) and persistence (whether the noun is mentioned multiple times or not). We show that the topic continuity correlates with short-utterance units, which are determined prosodically through the previous annotations; nouns of high topic continuity tend to be prosodically separated from the predicates. This result indicates the validity of our annotations of anaphoric relations and topic continuity and the usefulness for further studies on discourse and interaction.
iIt is often argued that a set of standard linguistic processing functionalities should be identified,with each of them given a formal specification. We would benefit from the formal specifications; for example, the semi-automated composition of a complex language processing workflow could be enabled in due time. This paper extracts a standard set of linguistic processing functionalities and tries to classify them formally. To do this, we first investigated prominent types of language Web services/linguistic processors by surveying a Web-based language service infrastructure and published NLP toolkits. We next induced a set of standard linguistic processing functionalities by carefully investigating each of the linguistic processor types. The standard linguistic processing functionalities was then characterized by the input/output data types, as well as the required data operation types, which were also derived from the investigation. As a result, we came up with an ontological depiction that classifies linguistic processors and linguistic processing functionalities with respect to the fundamental data operation types. We argue that such an ontological depiction can explicitly describe the functional aspects of a linguistic processing functionality.
Audiobooks are a rich resource of large quantities of natural sounding, highly expressive speech. In our previous research we have shown that it is possible to detect different expressive voice styles represented in a particular audiobook, using unsupervised clustering to group the speech corpus of the audiobook into smaller subsets representing the detected voice styles. These subsets of corpora of different voice styles reflect the various ways a speaker uses their voice to express involvement and affect, or imitate characters. This study is an evaluation of the detection of voice styles in an audiobook in the application of expressive speech synthesis. A further aim of this study is to investigate the usability of audiobooks as a language resource for expressive speech synthesis of utterances of conversational speech. Two evaluations have been carried out to assess the effect of the genre transfer: transmitting expressive speech from read aloud literature to conversational phrases with the application of speech synthesis. The first evaluation revealed that listeners have different voice style preferences for a particular conversational phrase. The second evaluation showed that it is possible for users of speech synthesis systems to learn the characteristics of a voice style well enough to make reliable predictions about what a certain utterance will sound like when synthesised using that voice style.
In this article the I3Media corpus is presented, a trilingual (Catalan, English, Spanish) speech database of neutral and emotional material collected for analysis and synthesis purposes. The corpus is actually made up of six different subsets of material: a neutral subcorpus, containing emotionless utterances; a dialog' subcorpus, containing typical call center utterances; an emotional' corpus, a set of sentences representative of pure emotional states; a football' subcorpus, including utterances imitating a football broadcasting situation; a SMS' subcorpus, including readings of SMS texts; and a paralinguistic elements' corpus, including recordings of interjections and paralinguistic sounds uttered in isolation. The corpus was read by professional speakers (male, in the case of Spanish and Catalan; female, in the case of the English corpus), carefully selected to meet criteria of language competence, voice quality and acting conditions. It is the result of a collaboration between the Speech Technology Group at Telef{\'o}nica Investigaci{\'o}n y Desarrollo (TID) and the Speech and Language Group at Barcelona Media Centre d'Innovaci{\'o} (BM), as part of the I3Media project.
We describe the Story Intention Graph, a set of discourse relations designed to represent aspects of narrative. Compared to prior models, ours is a novel synthesis of the notions of goal, plan, intention, outcome, affect and time that is amenable to corpus annotation. We describe a collection project, DramaBank, which includes encodings of texts ranging from small fables to epic poetry and contemporary nonfiction.
In order to handle the increasing amount of textual information today available on the web and exploit the knowledge latent in this mass of unstructured data, a wide variety of linguistic knowledge and resources (Language Identification, Morphological Analysis, Entity Extraction, etc.). is crucial. In the last decade LRaas (Language Resource as a Service) emerged as a novel paradigm for publishing and sharing these heterogeneous software resources over the Web. In this paper we present an overview of Linguagrid, a recent initiative that implements an open network of linguistic and semantic Web Services for the Italian language, as well as a new approach for enabling customizable corpus-based linguistic services on Linguagrid LRaaS infrastructure. A corpus ingestion service in fact allows users to upload corpora of documents and to generate classification/clustering models tailored to their needs by means of standard machine learning techniques applied to the textual contents and metadata from the corpora. The models so generated can then be accessed through proper Web Services and exploited to process and classify new textual contents.
Light verb constructions (LVCs), such as take a walk and make a decision, are a common subclass of multiword expressions (MWEs), whose distinct syntactic and semantic properties call for a special treatment within a computational system. In particular, LVCs are formed semi-productively: often a semantically-general verb (such as take) combines with a number of semantically-similar nouns to form semantically-related LVCs, as in make a decision/choice/commitment. Nonetheless, there are restrictions as to which verbs combine with which class of nouns. A proper computational account of LVCs is even more important for languages such as Persian, in which most verbs are of the form of LVCs. Recently, there has been some work on the automatic identification of MWEs (including LVCs) in resource-rich languages, such as English and Dutch. We adapt such existing techniques for the automatic identification of LVCs in Persian, an under-resourced language. Specifically, we extend an existing statistical measure of the acceptability of English LVCs (Fazly et al., 2007) to make explicit use of semantic classes of noun, and show that such classes are in particular useful for determining the LVC acceptability of new combinations.
For some years now, web services have been employed in Natural Language Processing (NLP) for a number of uses and within a number of sub-areas. Web services allow users to gain access to distant applications without having the need to install them on their local machines. A large paradigm of advantages can be obtained from a practical and development point of view. However, the legal aspects behind this sharing should not be neglected and should be openly discussed so as to understand the implications behind such data exchanges and tool uses. In the framework of PANACEA, this paper highlights the different points involved and describes the work done in order to handle all the legal aspects behind those points.
EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700 hierarchically organised subject domains used by European Institutions and many authorities in Member States of the European Union (EU) for the classification and retrieval of official documents. JEX is JRC-developed multi-label classification software that learns from manually labelled data to automatically assign EuroVoc descriptors to new documents in a profile-based category-ranking task. The JEX release consists of trained classifiers for 22 official EU languages, of parallel training data in the same languages, of an interface that allows viewing and amending the assignment results, and of a module that allows users to re-train the tool on their own document collections. JEX allows advanced users to change the document representation so as to possibly improve the categorisation result through linguistic pre-processing. JEX can be used as a tool for interactive EuroVoc descriptor assignment to increase speed and consistency of the human categorisation process, or it can be used fully automatically. The output of JEX is a language-independent EuroVoc feature vector lending itself also as input to various other Language Technology tasks, including cross-lingual clustering and classification, cross-lingual plagiarism detection, sentence selection and ranking, and more.
Text and speech corpora for training a tale telling robot have been designed, recorded and annotated. The aim of these corpora is to study expressive storytelling behaviour, and to help in designing expressive prosodic and co-verbal variations for the artificial storyteller). A set of 89 children tales in French serves as a basis for this work. The tales annotation principles and scheme are described, together with the corpus description in terms of coverage and inter-annotator agreement. Automatic analysis of a new tale with the help of this corpus and machine learning is discussed. Metrics for evaluation of automatic annotation methods are discussed. A speech corpus of about 1 hour, with 12 tales has been recorded and aligned and annotated. This corpus is used for predicting expressive prosody in children tales, above the level of the sentence.
The RELISH project promotes language-oriented research by addressing a two-pronged problem: (1) the lack of harmonization between digital standards for lexical information in Europe and America, and (2) the lack of interoperability among existing lexicons of endangered languages, in particular those created with the Shoebox/Toolbox lexicon building software. The cooperation partners in the RELISH project are the University of Frankfurt (FRA), the Max Planck Institute for Psycholinguistics (MPI Nijmegen), and Eastern Michigan University, the host of the Linguist List (ILIT). The project aims at harmonizing key European and American digital standards whose divergence has hitherto impeded international collaboration on language technology for resource creation and analysis, as well as web services for archive access. Focusing on several lexicons of endangered languages, the project will establish a unified way of referencing lexicon structure and linguistic concepts, and develop a procedure for migrating these heterogeneous lexicons to a standards-compliant format. Once developed, the procedure will be generalizable to the large store of lexical resources involved in the LEGO and DoBeS projects.
The present paper addresses the process and the results of the interpretation of the integral text of Le Petit Prince (Little Prince), the famous novel by Antoine de Saint-Exup{\'e}ry, from French into UNL. The original text comprised 1,684 interpretation units (15,513 words), which were sorted according to their similarity, from the shortest to the longest ones, and which were then projected into a UNL graph structure, composed of semantic directed binary relations linking nodes associated to the synsets of the corresponding original lexical items. The whole UNL-ization process was carried-out manually and the results have been used as the main resource in a natural language generation project involving already 27 languages.
Document zone identification aims to automatically classify sequences of text-spans (e.g. sentences) within a document into predefined zone categories. Current approaches to document zone identification mostly rely on supervised machine learning methods, which require a large amount of annotated data, which is often difficult and expensive to obtain. In order to overcome this bottleneck, we propose graphical models based on the popular Latent Dirichlet Allocation (LDA) model. The first model, which we call zoneLDA aims to cluster the sentences into zone classes using only unlabelled data. We also study an extension of zoneLDA called zoneLDAb, which makes distinction between common words and non-common words within the different zone types. We present results on two different domains: the scientific domain and the technical domain. For the latter one we propose a new document zone classification schema, which has been annotated over a collection of 689 documents, achieving a Kappa score of 85{\%}. Overall our experiments show promising results for both of the domains, outperforming the baseline model. Furthermore, on the technical domain the performance of the models are comparable to the supervised approach using the same feature sets. We thus believe that graphical models are a promising avenue of research for automatic document zoning.
This article reports an intrinsic automatic summarization evaluation in the scientific lecture domain. The lecture takes place in a Smart Room that has access to different types of documents produced from different media. An evaluation framework is presented to analyze the performance of systems producing summaries answering a user need. Several ROUGE metrics are used and a manual content responsiveness evaluation was carried out in order to analyze the performance of the evaluated approaches. Various multilingual summarization approaches are analyzed showing that the use of different types of documents outperforms the use of transcripts. In fact, not using any part of the spontaneous speech transcription in the summary improves the performance of automatic summaries. Moreover, the use of semantic information represented in the different textual documents coming from different media helps to improve summary quality.
We analyze how different conceptions of lexical semantics affect sense annotations and how multiple sense inventories can be compared empirically, based on annotated text. Our study focuses on the MASC project, where data has been annotated using WordNet sense identifiers on the one hand, and FrameNet lexical units on the other. This allows us to compare the sense inventories of these lexical resources empirically rather than just theoretically, based on their glosses, leading to new insights. In particular, we compute contingency matrices and develop a novel measure, the Expected Jaccard Index, that quantifies the agreement between annotations of the same data based on two different resources even when they have different sets of categories.
Linguistic Data Consortium and the National Institute of Standards and Technology are collaborating to create a large, heterogeneous annotated multimodal corpus to support research in multimodal event detection and related technologies. The HAVIC (Heterogeneous Audio Visual Internet Collection) Corpus will ultimately consist of several thousands of hours of unconstrained user-generated multimedia content. HAVIC has been designed with an eye toward providing increased challenges for both acoustic and video processing technologies, focusing on multi-dimensional variation inherent in user-generated multimedia content. To date the HAVIC corpus has been used to support the NIST 2010 and 2011 TRECVID Multimedia Event Detection (MED) Evaluations. Portions of the corpus are expected to be released in LDC's catalog in the coming year, with the remaining segments being published over time after their use in the ongoing MED evaluations.
MultiWord Expressions (MWEs) repesent a key issue for numerous applications in Natural Language Processing (NLP) especially for Machine Translation (MT). In this paper, we describe a strategy for detecting translation pairs of MWEs in a French-English parallel corpus. In addition we introduce three methods aiming to integrate extracted bilingual MWE S in M OSES, a phrase based Statistical Machine Translation (SMT) system. We experimentally show that these textual units can improve translation quality.
We have compiled a corpus of 80 Dutch texts from expository and persuasive genres, which we annotated for rhetorical and genre-specific discourse structure, and lexical cohesion with the goal of creating a gold standard for further research. The annota{\^A}{\neg}tions are based on a segmentation of the text in elementary discourse units that takes into account cues from syntax and punctuation. During the labor-intensive discourse-structure annotation (RST analysis), we took great care to thoroughly reconcile the initial analyses. That process and the availability of two independent initial analyses for each text allows us to analyze our disagreements and to assess the confusability of RST relations, and thereby improve the annotation guidelines and gather evidence for the classification of these relations into larger groups. We are using this resource for corpus-based studies of discourse relations, discourse markers, cohesion, and genre differences, e.g., the question of how discourse structure and lexical cohesion interact for different genres in the overall organization of texts. We are also exploring automatic text segmentation and semi-automatic discourse annotation.
The extraction of dictionaries from parallel text corpora is an established technique. However, as parallel corpora are a scarce resource, in recent years the extraction of dictionaries using comparable corpora has obtained increasing attention. In order to find a mapping between languages, almost all approaches suggested in the literature rely on a seed lexicon. The work described here achieves competitive results without requiring such a seed lexicon. Instead it presupposes mappings between comparable documents in different languages. For some common types of textual resources (e.g. encyclopedias or newspaper texts) such mappings are either readily available or can be established relatively easily. The current work is based on Wikipedias where the mappings between languages are determined by the authors of the articles. We describe a neural-network inspired algorithm which first characterizes each Wikipedia article by a number of keywords, and then considers the identification of word translations as a variant of word alignment in a noisy environment. We present results and evaluations for eight language pairs involving Germanic, Romanic, and Slavic languages as well as Chinese.
This contribution presents The Language Archive (TLA), a new unit at the MPI for Psycholinguistics, discussing the current developments in management of scientific data, considering the need for new data research infrastructures. Although several initiatives worldwide in the realm of language resources aim at the integration, preservation and mobilization of research data, the state of such scientific data is still often problematic. Data are often not well organized and archived and not described by metadata ― even unique data such as field-work observational data on endangered languages is still mostly on perishable carriers. New data centres are needed that provide trusted, quality-reviewed, persistent services and suitable tools and that take legal and ethical issues seriously. The CLARIN initiative has established criteria for suitable centres. TLA is in a good position to be one of such centres. It is based on three essential pillars: (1) A data archive; (2) management, access and annotation tools; (3) archiving and software expertise for collaborative projects. The archive hosts mostly observational data on small languages worldwide and language acquisition data, but also data resulting from experiments.
Achieving accurate translation, especially in multiple domain documents with statistical machine translation systems, requires more and more bilingual texts and this need becomes more critical when training such systems for language pairs with scarce training data. In the recent years, there have been some researches on new sources of parallel texts that are documents which are not necessarily parallel but are comparable. Since these methods search for possible translation equivalences in a greedy manner, they are unable to consider all possible parallel texts in comparable documents. This paper investigates a different approach for this need by considering relationships between all words of two comparable documents, which works fairly well even in the worst case of comparability. We represent each document pair in a matrix and then transform it to a new space to find parallel fragments. Evaluations show that the system is successful in extraction of useful fragment pairs.
In this paper we argue that the automatic term extraction procedure is an inherently multifactor process and the term extraction models needs to be based on multiple features including a specific type of a terminological resource under development. We proposed to use three types of features for extraction of two-word terms and showed that all these types of features are useful for term extraction. The set of features includes new features such as features extracted from an existing domain-specific thesaurus and features based on Internet search results. We studied the set of features for term extraction in two different domains and showed that the combination of several types of features considerably enhances the quality of the term extraction procedure. We found that for developing term extraction models in a specific domain, it is important to take into account some properties of the domain.
We describe our experiments on evaluating recently proposed modifications to the discourse relation annotation scheme of the Penn Discourse Treebank (PDTB), in the context of annotating discourse relations in Hindi Discourse Relation Bank (HDRB). While the proposed modifications were driven by the desire to introduce greater conceptual clarity in the PDTB scheme and to facilitate better annotation quality, our findings indicate that overall, some of the changes render the annotation task much more difficult for the annotators, as also reflected in lower inter-annotator agreement for the relevant sub-tasks. Our study emphasizes the importance of best practices in annotation task design and guidelines, given that a major goal of an annotation effort should be to achieve maximally high agreement between annotators. Based on our study, we suggest modifications to the current version of the HDRB, to be incorporated in our future annotation work.
In this paper, we present a methodology for the extraction of formulaic expressions, which goes beyond the mere extraction of candidate patterns. Using a pipeline we are able to extract information about the usage of formulaic expressions automatically from text corpora. According to Biber and Barbieri (2007) formulaic expressions are important building blocks of discourse in spoken and written registers. The automatic extraction procedure can help to investigate the usage and function of these recurrent patterns in different registers and domains. Formulaic expressions are commonplace not only in every- day language but also in scientific writing. Patterns such as 'in this paper', 'the number of', 'on the basis of' are often used by scientists to convey research interests, the theoretical basis of their studies, results of experiments, sci- entific findings as well as conclusions and are used as dis- course organizers. For Hyland (2008) they help to shape meanings in specific context and contribute to our sense of coherence in a text. We are interested in: (i) which and what type of formulaic expressions are used in scientific texts? (ii) the distribution of formulaic expression across different scien- tific disciplines, (iii) where do formulaic expressions occur within a text?
Due to the increasing number of emergency situations which can have substantial consequences, both financially and fatally, the Crisis Management (CM) domain is developing at an exponential speed. The efficient management of emergency situations relies on clear communication between all of the participants in a crisis situation. For these reasons the Text Complexity (TC) of the CM domain needed to be investigated and showed that CM domain texts exhibit high TC levels. This article presents a new linguistic resource in the form of Controlled Language (CL) guidelines for manual text simplification in the CM domain which aims to address high TC in the CM domain and produce clear messages to be used in crisis situations. The effectiveness of the resource has been tested via evaluation from several different perspectives important for the domain. The overall results show that the CLCM simplification has a positive impact on TC, reading comprehension, manual translation and machine translation. Additionally, an investigation of the cognitive difficulty in applying manual simplification operations led to interesting discoveries. This article provides details of the evaluation methods, the conducted experiments, their results and indications about future work.
Most of the reliable language resources are developed via human supervision. Developing supervised annotated data is hard and tedious, and it will be very time consuming when it is done totally manually; as a result, various types of annotated data, including treebanks, are not available for many languages. Considering that a portion of the language is regular, we can define regular expressions as grammar rules to recognize the strings which match the regular expressions, and reduce the human effort to annotate further unseen data. In this paper, we propose an incremental bootstrapping approach via extracting grammar rules when no treebank is available in the first step. Since Persian suffers from lack of available data sources, we have applied our method to develop a treebank for this language. Our experiment shows that this approach significantly decreases the amount of manual effort in the annotation process while enlarging the treebank.
The Japanese language has various types of functional expressions. In order to organize Japanese functional expressions with various surface forms, a lexicon of Japanese functional expressions with hierarchical organization was compiled. This paper proposes how to design the framework of identifying more than 16,000 functional expressions in Japanese texts by utilizing hierarchical organization of the lexicon. In our framework, more than 16,000 functional expressions are roughly divided into canonical / derived functional expressions. Each derived functional expression is intended to be identified by referring to the most similar occurrence of its canonical expression. In our framework, contextual occurrence information of much fewer canonical expressions are expanded into the whole forms of derived expressions, to be utilized when identifying those derived expressions. We also empirically show that the proposed method can correctly identify more than 80{\%} of the functional / content usages only with less than 38,000 training instances of manually identified canonical expressions.
One of the common processes in the field of text mining is text classification. Because of the complex nature of Farsi language, words with separate parts and combined verbs, the most of text classification systems are not applicable to Farsi texts. K-Nearest Neighbors (KNN) is one of the most popular used methods for text classification and presents good performance in experiments on different datasets. A method to improve the classification performance of KNN is proposed in this paper. Effects of removing or maintaining stop words, applying N-Grams with different lengths are also studied. For this study, a portion of a standard Farsi corpus called Hamshahri1 and articles of some archived newspapers are used. As the results indicate, classification efficiency improves by applying this approach especially when eight-grams indexing method and removing stop words are applied. Using N-grams with lengths more than 3 characters, presented very encouraging results for Farsi text classification. The Results of classification using our method are compared with the results obtained by mentioned related works.
The Ubiquitous Lexicon concept (ULex) has two sides. In the first kind of ubiquity, ULex combines prelexical corpus based lexicon extraction and formatting techniques from speech technology and corpus linguistics for both language documentation and basic speech technology (e.g. speech synthesis), and proposes new XML models for the basic datatypes concerned, in order to enable standardisastion and data interchange in these areas. The prelexical data types range from basic wordlists through diphone tables to concordance and interlinear glossing structures. While several proposals for standardising XML models of lexicon types are available, these more basic pre-lexical, data types, which are important in lexical acquisition, have received little attention. In the second area of ubiquity, ULex is implemented in a novel mobile environment to enable collaborative cross-platform use via a web application, either on the internet or, via a local hotspot, on an intranet, which runs not only on standard PC types but also on tablet computers and smartphones and is thereby also rendered truly ubiquitous in a geographical sense.
In order to construct an annotated diachronic corpus of Japanese, we propose to create a new dictionary for morphological analysis of Early Middle Japanese (Classical Japanese) based on UniDic, a dictionary for Contemporary Japanese. Differences between the Early Middle Japanese and Contemporary Japanese, which prevent a na{\"\i
The Lack of written representation for Italian Sign Language (LIS) makes it difficult to do perform tasks like looking up a new word in a dictionary. Most of the paper dictionaries show LIS signs in drawings or pictures. It's not a simple proposition to understand the meaning of sign from paper dictionaries unless one already knows the meanings. This paper presents the LIS dictionary which provides the facility to translate Italian text into sign language. LIS signs are shown as video animations performed by a virtual character. The LIS dictionary provides the integration with MultiWordNet database. The integration with MultiWordNet allows a rich extension with the meanings and senses of the words existing in MultiWordNet. The dictionary allows users to acquire information about lemmas, synonyms and synsets in the Sign Language (SL). The application is platform independent and can be used on any operating system. The results of input lemmas are displayed in groups of grammatical categories.
This paper describes the connection of WordNet to a generic ontology based on DOLCE. We developed a complete set of heuristics for mapping all WordNet nouns, verbs and adjectives to the ontology. Moreover, the mapping also allows to represent predicates in a uniform and interoperable way, regardless of the way they are expressed in the text and in which language. Together with the ontology, the WordNet mappings provide a extremely rich and powerful basis for semantic processing of text in any domain. In particular, the mapping has been used in a knowledge-rich event-mining system developed for the Asian-European project KYOTO.
Building a wordnet is a serious undertaking. Fortunately, Language Technology (LT) can improve the process of wordnet construction both in terms of quality and cost. In this paper we present LT tools used during the construction of plWordNet and their influence on the lexicographer's work-flow. LT is employed in plWordNet development on every possible step: from data gathering through data analysis to data presentation. Nevertheless, every decision requires input from the lexicographer, but the quality of supporting tools is an important factor. Thus a limited evaluation of usefulness of employed tools is carried out on the basis of questionnaires.
We describe the development of a test collection for the investigation of speech retrieval beyond identification of relevant content. This collection focuses on satisfying user information needs for queries associated with specific types of speech acts. The collection is based on an archive of the Internet video from Internet video sharing platform (blip.tv), and was provided by the MediaEval benchmarking initiative. A crowdsourcing approach was used to identify segments in the video data which contain speech acts, to create a description of the video containing the act and to generate search queries designed to refind this speech act. We describe and reflect on our experiences with crowdsourcing this test collection using the Amazon Mechanical Turk platform. We highlight the challenges of constructing this dataset, including the selection of the data source, design of the crowdsouring task and the specification of queries and relevant items.
This paper announces the release of the Ontologies of Linguistic Annotation (OLiA). The OLiA ontologies represent a repository of annotation terminology for various linguistic phenomena on a great band-width of languages. This paper summarizes the results of five years of research, it describes recent developments and directions for further research.
This paper describes the Open Linguistics Working Group (OWLG) of the Open Knowledge Foundation (OKFN). The OWLG is an initiative concerned with linguistic data by scholars from diverse fields, including linguistics, NLP, and information science. The primary goal of the working group is to promote the idea of open linguistic resources, to develop means for their representation and to encourage the exchange of ideas across different disciplines. This paper summarizes the progress of the working group, goals that have been identified, problems that we are going to address, and recent activities and ongoing developments. Here, we put particular emphasis on the development of a Linked Open Data (sub-)cloud of linguistic resources that is currently being pursued by several OWLG members.
Annotating natural language sentences with quantifier scoping has proved to be very hard. In order to overcome the challenge, previous work on building scope-annotated corpora has focused on sentences with two explicitly quantified noun phrases (NPs). Furthermore, it does not address the annotation of scopal operators or complex NPs such as plurals and definites. We present the first annotation scheme for quantifier scope disambiguation where there is no restriction on the type or the number of scope-bearing elements in the sentence. We discuss some of the most prominent complex scope phenomena encountered in annotating the corpus, such as plurality and type-token distinction, and present mechanisms to handle those phenomena.
This paper describes POWLA, a generic formalism to represent linguistic corpora by means of RDF and OWL/DL. Unlike earlier approaches in this direction, POWLA is not tied to a specific selection of annotation layers, but rather, it is designed to support any kind of text-oriented annotation. POWLA inherits its generic character from the underlying data model PAULA (Dipper, 2005; Chiarcos et al., 2009) that is based on early sketches of the ISO TC37/SC4 Linguistic Annotation Framework (Ide and Romary, 2004). As opposed to existing standoff XML linearizations for such generic data models, it uses RDF as representation formalism and OWL/DL for validation. The paper discusses advantages of this approach, in particular with respect to interoperability and queriability, which are illustrated for the MASC corpus, an open multi-layer corpus of American English (Ide et al., 2008).
The Linguistic Data Consortium (LDC) creates and provides language resources (LRs) including data, tools and specifications. In order to assess the impact of these LRs and to support both LR users and authors, LDC is collecting metadata about and URLs for research papers that introduce, describe, critique, extend or rely upon LDC LRs. Current collection efforts focus on papers published in journals and conference proceedings that are available online. To date, nearly 300, or over half of the LRs LDC distributes have been searched for extensively and almost 8000 research papers about these LRs have been documented. This paper discusses the issues with collecting references and includes preliminary analysis of those results. The remaining goals of the project are also outlined.
We examine speaker independent emotion classification from speech, reporting experiments on the Berlin database across six basic emotions. Our approach is novel in a number of ways: First, it is hierarchical, motivated by our belief that the most suitable feature set for classification is different for each pair of emotions. Further, it uses a large number of feature sets of different types, such as prosodic, spectral, glottal flow based, and AM-FM ones. Finally, it employs a two-stage feature selection strategy to achieve discriminative dimensionality reduction. The approach results to a classification rate of 85{\%}, comparable to the state-of-the-art on this dataset.
The usefulness of annotated corpora is greatly increased if there is an associated tool that can allow various kinds of operations to be performed in a simple way. Different kinds of annotation frameworks and many query languages for them have been proposed, including some to deal with multiple layers of annotation. We present here an easy to learn query language for a particular kind of annotation framework based on threaded trees', which are somewhere between the complete order of a tree and the anarchy of a graph. Through 'typed' threads, they can allow multiple levels of annotation in the same document. Our language has a simple, intuitive and concise syntax and high expressive power. It allows not only to search for complicated patterns with short queries but also allows data manipulation and specification of arbitrary return values. Many of the commonly used tasks that otherwise require writing programs, can be performed with one or more queries. We compare the language with some others and try to evaluate it.
MT systems typically use parsers to help reorder constituents. However most languages do not have adequate treebank data to learn good parsers, and such training data is extremely time-consuming to annotate. Our earlier work has shown that a reordering model learned from word-alignments using POS tags as features can improve MT performance (Visweswariah et al., 2011). In this paper, we investigate the effect of word-classing on reordering performance using this model. We show that unsupervised word clusters perform somewhat worse but still reasonably well, compared to a part-of-speech (POS) tagger built with a small amount of annotated data; while a richer tag set including case and gender-number-person further improves reordering performance by around 1.2 monolingual BLEU points. While annotating this richer tagset is more complicated than annotating the base tagset, it is much easier than annotating treebank data.
We present a collection of parallel treebanks that have been automatically aligned on both the terminal and the nonterminal constituent level for use in syntax-based machine translation. We describe how they were constructed and applied to a syntax- and example-based machine translation system called Parse and Corpus-Based Machine Translation (PaCo-MT). For the language pair Dutch to English, we present evaluation scores of both the nonterminal constituent alignments and the MT system itself, and in the latter case, compare them with those of Moses, a current state-of-the-art statistical MT system, when trained on the same data.
Lack of sufficient parallel data for many languages and domains is currently one of the major obstacles to further advancement of automated translation. The ACCURAT project is addressing this issue by researching methods how to improve machine translation systems by using comparable corpora. In this paper we present tools and techniques developed in the ACCURAT project that allow additional data needed for statistical machine translation to be extracted from comparable corpora. We present methods and tools for acquisition of comparable corpora from the Web and other sources, for evaluation of the comparability of collected corpora, for multi-level alignment of comparable corpora and for extraction of lexical and terminological data for machine translation. Finally, we present initial evaluation results on the utility of collected corpora in domain-adapted machine translation and real-life applications.
The paper presents construction of {\textbackslash}emph{Derywator} -- a language tool for the recognition of Polish derivational relations. It was built on the basis of machine learning in a way following the bootstrapping approach: a limited set of derivational pairs described manually by linguists in plWordNet is used to train {\textbackslash}emph{Derivator}. The tool is intended to be applied in semi-automated expansion of plWordNet with new instances of derivational relations. The training process is based on the construction of two transducers working in the opposite directions: one for prefixes and one for suffixes. Internal stem alternations are recognised, recorded in a form of mapping sequences and stored together with transducers. Raw results produced by {\textbackslash}emph{Derivator} undergo next corpus-based and morphological filtering. A set of derivational relations defined in plWordNet is presented. Results of tests for different derivational relations are discussed. A problem of the necessary corpus-based semantic filtering is analysed. The presented tool depends to a very little extent on the hand-crafted knowledge for a particular language, namely only a table of possible alternations and morphological filtering rules must be exchanged and it should not take longer than a couple of working days.
This work studies conceptual structures based on the Formal Concept Analysis method. We build these structures based on lexico-semantic information extracted from texts, among which we highlight the semantic roles. In our research, we propose ways to include semantic roles in concepts produced by this formal method. We analyze the contribution of semantic roles and verb classes in the composition of these concepts through structural measures. In these studies, we use the Penn Treebank Sample and SemLink 1.1 corpora, both in English.
Recognizing similar or close meaning on different surface form is a common challenge in various Natural Language Processing and Information Access applications. However, we identified multiple limitations in existing resources that can be used for solving the vocabulary mismatch problem. To this end, we will propose the Diversifiable Bootstrapping algorithm that can learn paraphrase patterns with a high lexical coverage. The algorithm works in a lightly-supervised iterative fashion, where instance and pattern acquisition are interleaved, each using information provided by the other. By tweaking a parameter in the algorithm, resulting patterns can be diversifiable with a specific degree one can control.
Although in recent years numerous forms of Internet communication ― such as e-mail, blogs, chat rooms and social network environments ― have emerged, balanced corpora of Internet speech with trustworthy meta-information (e.g. age and gender) or linguistic annotations are still limited. In this paper we present a large corpus of Flemish Dutch chat posts that were collected from the Belgian online social network Netlog. For all of these posts we also acquired the users' profile information, making this corpus a unique resource for computational and sociolinguistic research. However, for analyzing such a corpus on a large scale, NLP tools are required for e.g. automatic POS tagging or lemmatization. Because many NLP tools fail to correctly analyze the surface forms of chat language usage, we propose to normalize this anomalous' input into a format suitable for existing NLP solutions for standard Dutch. Additionally, we have annotated a substantial part of the corpus (i.e. the Chatty subset) to provide a gold standard for the evaluation of future approaches to automatic (Flemish) chat language normalization.
This paper describes a rule-based approach to segment Arabic texts into clauses. Our method relies on an extensive analysis of a large set of lexical cues as well as punctuation marks. Our analysis was carried out on two different corpus genres: news articles and elementary school textbooks. We propose a three steps segmentation algorithm: first by using only punctuation marks, then by relying only on lexical cues and finally by using both typology and lexical cues. The results were compared with manual segmentations elaborated by experts.
This paper introduces our study on creating a Japanese corpus that is annotated using semantically-motivated predicate-argument structures. We propose an annotation framework based on Lexical Conceptual Structure (LCS), where semantic roles of arguments are represented through a semantic structure decomposed by several primitive predicates. As a first stage of the project, we extended Jackendoff 's LCS theory to increase generality of expression and coverage for verbs frequently appearing in the corpus, and successfully created LCS structures for 60 frequent Japanese predicates in Kyoto university Text Corpus (KTC). In this paper, we report our framework for creating the corpus and the current status of creating an LCS dictionary for Japanese predicates.
We present a methodology for analyzing cross-cultural similarities and differences using language as a medium, love as domain, social media as a data source and 'Terms' and 'Topics' as cultural features. We discuss the techniques necessary for the creation of the social data corpus from which emotion terms have been extracted using NLP techniques. Topics of love discussion were then extracted from the corpus by means of Latent Dirichlet Allocation (LDA). Finally, on the basis of these features, a cross-cultural comparison was carried out. For the purpose of cross-cultural analysis, the experimental focus was on comparing data from a culture from the East (India) with a culture from the West (United States of America). Similarities and differences between these cultures have been analyzed with respect to the usage of emotions, their intensities and the topics used during love discussion in social media.
Several corpora annotated for coreference have been made available in the past decade. These resources differ with respect to their size and the underlying structure: the number of domains and their similarity. Our study compares domain-specific models, learned from small heterogeneous subsets of the investigated corpora, against uniform models, that utilize all the available data. We show that for knowledge-poor baseline systems, domain-specific and uniform modeling yield same results. Systems, relying on large amounts of linguistic knowledge, however, exhibit differences in their performance: with all the designed features in use, domain-specific models suffer from over-fitting, whereas with pre-selected feature sets they tend to outperform union models.
Sentiment analysis is one of the recent, highly dynamic fields in Natural Language Processing. Although much research has been performed in this area, most existing approaches are based on word-level analysis of texts and are mostly able to detect only explicit expressions of sentiment. However, in many cases, emotions are not expressed by using words with an affective meaning (e.g. happy), but by describing real-life situations, which readers (based on their commonsense knowledge) detect as being related to a specific emotion. Given the challenges of detecting emotions from contexts in which no lexical clue is present, in this article we present a comparative analysis between the performance of well-established methods for emotion detection (supervised and lexical knowledge-based) and a method we extend, which is based on commonsense knowledge stored in the EmotiNet knowledge base. Our extensive comparative evaluations show that, in the context of this task, the approach based on EmotiNet is the most appropriate.
We present an extension of the adverbial entries of the French morphological lexicon DELA (Dictionnaires Electroniques du LADL / LADL electronic dictionaries). Adverbs were extracted from LGLex, a NLP-oriented syntactic resource for French, which in its turn contains all adverbs extracted from the Lexicon-Grammar tables of both simple adverbs ending in -ment (i.e., '-ly') and compound adverbs. This work exploits fine-grained linguistic information provided in existing resources. The resulting resource is reviewed in order to delete duplicates and is freely available under the LGPL-LR license.
This work represents a first step in the direction of reconstructing a diachronic morphology for Romanian. The main resource used in this task is the digital version of Romanian Language Dictionary (eDTLR). This resource offers various usage examples for its entries, citations extracted from popular Romanian texts, which often present diachronic and inflected forms of the word they are provided for. The concept of word deformation is introduced and classified into more categories. The research conducted aims at detecting one type of such deformations occurring in the citations ― changes only in the stem of the current word, without the migration to another paradigm. An algorithm is presented which automatically infers old stem forms. This uses a paradigmatic data model of the current Romanian morphology. Having the inferred roots and the paradigms that they are part of, old flexion forms of the words can be deduced. Even more, by considering the years in which the citations were published, the inferred old word forms can be framed in certain periods of time, creating a great resource for research in the evolution of the Romanian language.
In this paper the author presents TildeNER ― an open source freely available named entity recognition toolkit and the first multi-class named entity recognition system for Latvian and Lithuanian languages. The system is built upon a supervised conditional random field classifier and features heuristic and statistical refinement methods that improve supervised classification, thus boosting the overall system's performance. The toolkit provides means for named entity recognition model bootstrapping, plaintext document and also pre-processed (morpho-syntactically tagged) tab-separated document named entity tagging and evaluation on test data. The paper presents the design of the system, describes the most important data formats and briefly discusses extension possibilities to different languages. It also gives evaluation on human annotated gold standard test corpora for Latvian and Lithuanian languages as well as comparative performance analysis to a state-of-the art English named entity recognition system using parallel and strongly comparable corpora. The author gives analysis of the Latvian and Lithuanian named entity tagged corpora annotation process and the created named entity annotated corpora.
We proposed a method of collecting humorous expressions from an online community-based question-answering (CQA) corpus where some users post a variety of questions and other users post relevant answers. Although the service is created for the purpose of knowledge exchange, there are users who enjoy posting humorous responses. Therefore, the corpus contains many interesting humour communication examples that might be useful in understanding the nature of online communications and variations in humour. Considering the size of 3; 116; 009 topics, it is necessary to introduce automation in the collection process. However, due to the context dependency of humour expressions, it is hard to collect them automatically by using keywords or key phrases. Our method uses natural language processing based on dissimilarity criteria between answer texts. By using this method, we can collect humour expressions more efficiently than by manual exploration: 30 times more examples per hour.
The ISOcat Data Category Registry contains basically a flat and easily extensible list of data category specifications. To foster reuse and standardization only very shallow relationships among data categories are stored in the registry. However, to assist crosswalks, possibly based on personal views, between various (application) domains and to overcome possible proliferation of data categories more types of ontological relationships need to be specified. RELcat is a first prototype of a Relation Registry, which allows storing arbitrary relationships. These relationships can reflect the personal view of one linguist or a larger community. The basis of the registry is a relation type taxonomy that can easily be extended. This allows on one hand to load existing sets of relations specified in, for example, an OWL (2) ontology or SKOS taxonomy. And on the other hand allows algorithms that query the registry to traverse the stored semantic network to remain ignorant of the original source vocabulary. This paper describes first experiences with RELcat and explains some initial design decisions.
The paper introduces the Political Speech Corpus of Bulgarian. First, its current state has been discussed with respect to its size, coverage, genre specification and related online services. Then, the focus goes to the annotation details. On the one hand, the layers of linguistic annotation are presented. On the other hand, the compatibility with CLARIN technical Infrastructure is explained. Also, some user-based scenarios are mentioned to demonstrate the corpus services and applicability.
The importance of attribution is becoming evident due to its relevance in particular for Opinion Analysis and Information Extraction applications. Attribution would allow to identify different perspectives on a given topic or retrieve the statements of a specific source of interest, but also to select more relevant and reliable information. However, the scarce and partial resources available to date to conduct attribution studies have determined that only a portion of attribution structures has been identified and addressed. This paper presents the collection and further annotation of a database of over 9800 attributions relations from the Penn Discourse TreeBank (PDTB). The aim is to build a large and complete resource that fills a key gap in the field and enables the training and testing of robust attribution extraction systems.
Generally the existing monolingual corpora are not suitable for large vocabulary continuous speech recognition (LVCSR) of code-switching speech. The motivation of this paper is to study the rules and constraints code-switching follows and design a corpus for code-switching LVCSR task. This paper presents the development of a Mandarin-English code-switching corpus. This corpus consists of four parts: 1) conversational meeting speech and its data; 2) project meeting speech data; 3) student interviews speech; 4) text data of on-line news. The speech was transcribed by an annotator and verified by Mandarin-English bilingual speakers manually. We propose an approach for automatically downloading from the web text data that contains code-switching. The corpus includes both intra-sentential code-switching (switch in the middle of a sentence) and inter-sentential code-switching (switch at the end of the sentence). The distribution of part-of-speech (POS) tags and code-switching reasons are reported.
This paper presents our efforts aimed at collecting and annotating a free Polish corpus. The corpus will serve for us as training and testing material for experiments with Machine Learning algorithms. As others may also benefit from the resource, we are going to release it under a Creative Commons licence, which is hoped to remove unnecessary usage restrictions, but also to facilitate reproduction of our experimental results. The corpus is being annotated with various types of linguistic entities: chunks and named entities, selected syntactic and semantic relations, word senses and anaphora. We report on the current state of the project as well as our ultimate goals.
This paper presents a novel approach to deal with dictionary retrieval. This new approach is based on a very efficient and scalable theoretical structure called Multi-Terminal Multi-valued Decision Diagrams (MTMDD). Such tool allows the definition of very large, even multilingual, dictionaries without significant increase in memory demands, and also with virtually no additional processing cost. Besides the general idea of the novel approach, this paper presents a description of the technologies involved, and their implementation in a software package called WAGGER. Finally, we also present some examples of usage and possible applications of this dictionary retriever.
Text alignment is one of the main processes for obtaining parallel corpora. When aligning two versions of a book, results are often affected by unpaired sections ― sections which only exist in one of the versions of the book. We developed Text::Perfide::BookSync, a Perl module which performs books synchronization (structural alignment based on section delimitation), provided they have been previously annotated by Text::Perfide::BookCleaner. We discuss the need for such a tool and several implementation decisions. The main functions are described, and examples of input and output are presented. Text::Perfide::PartialAlign is an extension of the partialAlign.py tool bundled with hunalign which proposes an alternative methods for splitting bitexts.
Paraphrases are alternative syntactic forms in the same language expressing the same semantic content. Speakers of all languages are inherently familiar with paraphrases at different levels of granularity (lexical, phrasal, and sentential). For quite some time, the concept of paraphrasing is getting a growing attention by the research community and its potential use in several natural language processing applications (such as text summarization and machine translation) is being investigated. In this paper, we present, what is to our best knowledge, the first Turkish paraphrase corpus. The corpus is gleaned from four different sources and currently contains 1270 paraphrase pairs. All paraphrase pairs are carefully annotated by native Turkish speakers with the identified semantic correspondences between paraphrases. The work for expanding the corpus is still under way.
This paper describes different aspects of an open competition to evaluate multicultural name matching software, including the contest design, development of the test data, different phases of the competition, behavior of the participating teams, results of the competition, and lessons learned throughout. The competition, known as The MITRE Challenge{\^a}{\textcent}, was informally announced at LREC 2010 and was recently concluded. Contest participants used the competition website (http://mitrechallenge.mitre.org) to download the competition data set and guidelines, upload results, and to view accuracy metrics for each result set submitted. Participants were allowed to submit unlimited result sets, with their top-scoring set determining their overall ranking. The competition website featured a leader board that displayed the top score for each participant, ranked according to the principal contest metric - mean average precision (MAP). MAP and other metrics were calculated in near-real time on a remote server, based on ground truth developed for the competition data set. Additional measures were taken to guard against gaming the competition metric or overfitting to the competition data set. Lessons learned during running this first MITRE Challenge will be valuable to others considering running similar evaluation campaigns.
Syntactic parses can provide valuable information for many NLP tasks, such as machine translation, semantic analysis, etc. However, most of the world's languages do not have large amounts of syntactically annotated corpora available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora between resource-poor and resource-rich languages, bootstrapping the resource-poor language with the syntactic analysis of the resource-rich language. In this paper, we investigate the possibility of using small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. These patterns can then be used to improve structural projection algorithms, allowing for better performing NLP tools for resource-poor languages, in particular those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that important instances of divergence are picked up with minimal prior knowledge of a given language pair.
Domain adaptation is an important topic for natural language processing. There has been extensive research on the topic and various methods have been explored, including training data selection, model combination, semi-supervised learning. In this study, we propose to use a goodness measure, namely, description length gain (DLG), for domain adaptation for Chinese word segmentation. We demonstrate that DLG can help domain adaptation in two ways: as additional features for supervised segmenters to improve system performance, and also as a similarity measure for selecting training data to better match a test set. We evaluated our systems on the Chinese Penn Treebank version 7.0, which has 1.2 million words from five different genres, and the Chinese Word Segmentation Bakeoff-3 data.
This paper describes a prototype of a computer-assisted pronunciation training system called MySpeech. The interface of the MySpeech system is web-based and it currently enables users to practice pronunciation by listening to speech spoken by native speakers and tuning their speech production to correct any mispronunciations detected by the system. This practice exercise is facilitated in different topics and difficulty levels. An experiment was conducted in this work that combines the MySpeech service with the WebWOZ Wizard-of-Oz platform (http://www.webwoz.com), in order to improve the human-computer interaction (HCI) of the service and the feedback that it provides to the user. The employed Wizard-of-Oz method enables a human (who acts as a wizard) to give feedback to the practising user, while the user is not aware that there is another person involved in the communication. This experiment permitted to quickly test an HCI model before its implementation on the MySpeech system. It also allowed to collect input data from the wizard that can be used to improve the proposed model. Another outcome of the experiment was the preliminary evaluation of the pronunciation learning service in terms of user satisfaction, which would be difficult to conduct before integrating the HCI part.
This paper describes an open-source Latvian resource grammar implemented in Grammatical Framework (GF), a programming language for multilingual grammar applications. GF differentiates between concrete grammars and abstract grammars: translation among concrete languages is provided via abstract syntax trees. Thus the same concrete grammar is effectively used for both language analysis and language generation. Furthermore, GF differentiates between general-purpose resource grammars and domain-specific application grammars that are built on top of the resource grammars. The GF resource grammar library (RGL) currently supports more than 20 languages that implement a common API. Latvian is the 13th official European Union language that is made available in the RGL. We briefly describe the grammatical features of Latvian and illustrate how they are handled in the multilingual framework of GF. We also illustrate some application areas of the Latvian resource grammar, and briefly discuss the limitations of the RGL and potential long-term improvements using frame semantics.
Human-human spoken dialogues are considered an important tool for effective speech interface design and are often used for stochastic model training in speech based applications. However, the less restricted nature of human-human interaction compared to human-system interaction may undermine the usefulness of such corpora for creating effective and usable interfaces. In this respect, this work examines the differences between corpora collected from human-human interaction and corpora collected from actual system use, in order to formally assess the appropriateness of the former for both the design and implementation of spoken dialogue systems. Comparison results show that there are significant differences with respect to vocabulary, sentence structure and speech recognition success rate among others. Nevertheless, compared to other available tools and techniques, human-human dialogues may still be used as a temporary at least solution for building more effective working systems. Accordingly, ways to better utilize such resources are presented.
We describe a new GATE-based linguistic annotation pipeline for Early Modern German, which can be used to annotate historical texts with word tokens, sentence boundaries, lemmas, and POS tags. The pipeline is based on a customisation of the freely available ANNIE system for English (Cunningham et al., 2002), in combination with a version of the TreeTagger (Schmid, 1994) trained on gold standard Early Modern German data. The POS-tagging and lemmatisation components of the pipeline achieve an average accuracy of 89.44{\%} and 83.16{\%}, respectively, on unseen historical data from various genres and publication dates within the Early Modern period. We show that normalisation of spelling variation can further improve these results. With no specialised tools available for processing this particular stage of the language, this pipeline will be of particular interest to smaller, humanities-based projects wishing to add linguistic annotations to their historical data but which lack the means or resources to develop such tools themselves.
In Statistical Machine Translation, words that were not seen during training are unknown words, that is, words that the system will not know how to translate. In this paper we contribute to this research problem by profiting from orthographic cues given by words. Thus, we report a study of the impact of word distance metrics in cognates' detection and, in addition, on the possibility of obtaining possible translations of unknown words through Logical Analogy. Our approach is tested in the translation of corpora from Portuguese to English (and vice-versa).
The Semantic Annotation (SA) task consists in establishing the relation between a textual entity (word or group of words designating a named entity of the real world or a concept) and its corresponding entity in an ontology. The main difficulty of this task is that a textual entity might be highly polysemic and potentially related to many different ontological representations. To solve this specific problem, various Information Retrieval techniques can be used. Most of those involves contextual words to estimate wich exact textual entity have to be recognized. In this paper, we present a resource of contextual words that can be used by IR algorithms to establish a link between a named entity (NE) in a text and an entry point to its semantic description in the LinkedData Network.
Given the significant improvements in Machine Translation (MT) quality and the increasing demand for translations, post-editing of automatic translations is becoming a popular practice in the translation industry. It has been shown to allow for much larger volumes of translations to be produced, saving time and costs. In addition, the post-editing of automatic translations can help understand problems in such translations and this can be used as feedback for researchers and developers to improve MT systems. Finally, post-editing can be used as a way of evaluating the quality of translations in terms of how much post-editing effort these translations require. We describe a standalone tool that has two main purposes: facilitate the post-editing of translations from any MT system so that they reach publishable quality and collect sentence-level information from the post-editing process, e.g.: post-editing time and detailed keystroke statistics.
The paper describes the design and the results of a manual annotation methodology devoted to enrich the ISST--TANL Corpus, derived from the Italian Syntactic--Semantic Treebank (ISST), with Semantic Frames information. The main issues encountered in applying the English FrameNet annotation criteria to a corpus of Italian language are discussed together with the choice of anchoring the semantic annotation layer to the underlying dependency syntactic structure. The results of a case study aimed at extending and specialising this methodology for the annotation of a corpus of legislative texts are also discussed.
Large-scale spontaneous speech corpora are crucial resource for various domains of spoken language processing. However, the available corpora are usually limited because their construction cost is quite expensive especially in transcribing speech precisely. On the other hand, loosely transcribed corpora like shorthand notes, meeting records and closed captions are more widely available than precisely transcribed ones, because their imperfectness reduces their construction cost. Because these corpora contain both precisely transcribed regions and edited regions, it is difficult to use them directly as speech corpora for learning acoustic models. Under this background, we have been considering to build an efficient semi-automatic framework to convert loose transcriptions to precise ones. This paper describes an improved automatic detection method of precise regions from loosely transcribed corpora for the above framework. Our detection method consists of two steps: the first step is a force alignment between loose transcriptions and their utterances to discover the corresponding utterance for the certain loose transcription, and the second step is a detector of precise regions with a support vector machine using several features obtained from the first step. Our experimental result shows that our method achieves a high accuracy of detecting precise regions, and shows that the precise regions extracted by our method are effective as training labels of lightly supervised speaker adaptation.
This paper addresses theoretical and practical issues experienced in the construction of Turkish National Corpus (TNC). TNC is designed to be a balanced, large scale (50 million words) and general-purpose corpus for contemporary Turkish. It has benefited from previous practices and efforts for the construction of corpora. In this sense, TNC generally follows the framework of British National Corpus, yet necessary adjustments in corpus design of TNC are made whenever needed. All throughout the process, different types of open-source software are used for specific tasks, and the resulting corpus is a free resource for non-commercial use. This paper presents TNC's design features, web-based corpus management system, carefully planned workflow and its web-based user-friendly search interface.
The paper describes a corpus of texts produced by non-native speakers of Czech. We discuss its annotation scheme, consisting of three interlinked levels to cope with a wide range of error types present in the input. Each level corrects different types of errors; links between the levels allow capturing errors in word order and complex discontinuous expressions. Errors are not only corrected, but also classified. The annotation scheme is tested on a doubly-annotated sample of approx. 10,000 words with fair inter-annotator agreement results. We also explore options of application of automated linguistic annotation tools (taggers, spell checkers and grammar checkers) on the learner text to support or even substitute manual annotation.
This paper presents a metadata model for the description of language resources proposed in the framework of the META-SHARE infrastructure, aiming to cover both datasets and tools/technologies used for their processing. It places the model in the overall framework of metadata models, describes the basic principles and features of the model, elaborates on the distinction between minimal and maximal versions thereof, briefly presents the integrated environment supporting the LRs description and search and retrieval processes and concludes with work to be done in the future for the improvement of the model.
Expressing opinions and emotions on social media becomes a frequent activity in daily life. People express their opinions about various targets via social media and they are also interested to know about other opinions on the same target. Automatically identifying the sentiment of these texts and also the strength of the opinions is an enormous help for people and organizations who are willing to use this information for their goals. In this paper, we present a rule-based approach for German sentiment analysis. The proposed model provides a fine-grained annotation for German texts, which represents the sentiment strength of the input text using two scores: positive and negative. The scores show that if the text contains any positive or negative opinion as well as the strength of each positive and negative opinions. To this aim, a German opinion dictionary of 1,864 words is prepared and compared with other opinion dictionaries for German. We also introduce a new dataset for German sentiment analysis. The dataset contains 500 short texts from social media about German celebrities and is annotated by three annotators. The results show that the proposed unsupervised model outperforms the supervised machine learning techniques. Moreover, the new dictionary performs better than other German opinion dictionaries.
We present a method for improving word alignment quality for phrase-based statistical machine translation by reordering the source text according to the target word order suggested by an initial word alignment. The reordered text is used to create a second word alignment which can be an improvement of the first alignment, since the word order is more similar. The method requires no other pre-processing such as part-of-speech tagging or parsing. We report improved Bleu scores for English-to-German and English-to-Swedish translation. We also examined the effect on word alignment quality and found that the reordering method increased recall while lowering precision, which partly can explain the improved Bleu scores. A manual evaluation of the translation output was also performed to understand what effect our reordering method has on the translation system. We found that where the system employing reordering differed from the baseline in terms of having more words, or a different word order, this generally led to an improvement in translation quality.
Data-driven machine translation (MT) approaches became very popular during last years, especially for language pairs for which it is difficult to find specialists to develop transfer rules. Statistical (SMT) or example-based (EBMT) systems can provide reasonable translation quality for assimilation purposes, as long as a large amount of training data is available. Especially SMT systems rely on parallel aligned corpora which have to be statistical relevant for the given language pair. The construction of large domain specific parallel corpora is time- and cost-consuming; the current practice relies on one or two big such corpora per language pair. Recent developed strategies ensure certain portability to other domains through specialized lexicons or small domain specific corpora. In this paper we discuss the influence of different discourse styles on statistical machine translation systems. We investigate how a pure SMT performs when training and test data belong to same domain but the discourse style varies.
Social relations like power and influence are difficult concepts to define, but are easily recognizable when expressed. In this paper, we describe a multi-layer annotation scheme for social power relations that are recognizable from online written interactions. We introduce a typology of four types of power relations between dialog participants: hierarchical power, situational power, influence and control of communication. We also present a corpus of Enron emails comprising of 122 threaded conversations, manually annotated with instances of these power relations between participants. Our annotations also capture attempts at exercise of power or influence and whether those attempts were successful or not. In addition, we also capture utterance level annotations for overt display of power. We describe the annotation definitions using two example email threads from our corpus illustrating each type of power relation. We also present detailed instructions given to the annotators and provide various statistics on annotations in the corpus.
In times of mass emergency, vast amounts of data are generated via computer-mediated communication (CMC) that are difficult to manually collect and organize into a coherent picture. Yet valuable information is broadcast, and can provide useful insight into time- and safety-critical situations if captured and analyzed efficiently and effectively. We describe a natural language processing component of the EPIC (Empowering the Public with Information in Crisis) Project infrastructure, designed to extract linguistic and behavioral information from tweet text to aid in the task of information integration. The system incorporates linguistic annotation, in the form of Named Entity Tagging, as well as behavioral annotations to capture tweets contributing to situational awareness and analyze the information type of the tweet content. We show classification results and describe future integration of these classifiers in the larger EPIC infrastructure.
The paper defines the notion of pedagogical stance, viewed as the type of position taken, the role assumed, the image projected and the types of social behaviours performed by a teacher in her teaching interaction with a pupil. Two aspects of pedagogical stance, didactic and affective ― relational, are distinguished and a hypothesis is put forward about their determinant factors (the teacher's personality, idea of one's role and of the learning process, and model of the pupil). Based on a qualitative analysis of the verbal and bodily behaviour of teachers in a corpus of teacher-pupil interactions, the paper singles out two didactic stances (maieutic and efficient) and four affective-relational ones (friendly, dominating, paternalistic, and secure base). Some examples of these stances are analysed in detail and the respective patterns of verbal and behavioural signals that typically characterize the six types of stances are outlined.
We investigate the influence of audiovisual features on the perception of speaking style and performance of politicians, utilizing a large publicly available dataset of German parliament recordings. We conduct a human perception experiment involving eye-tracker data to evaluate human ratings as well as behavior in two separate conditions, i.e. audiovisual and video only. The ratings are evaluated on a five dimensional scale comprising measures of insecurity, monotony, expressiveness, persuasiveness, and overall performance. Further, they are statistically analyzed and put into context in a multimodal feature analysis, involving measures of prosody, voice quality and motion energy. The analysis reveals several statistically significant features, such as pause timing, voice quality measures and motion energy, that highly positively or negatively correlate with certain human ratings of speaking style. Additionally, we compare the gaze behavior of the human subjects to evaluate saliency regions in the multimodal and visual only conditions. The eye-tracking analysis reveals significant changes in the gaze behavior of the human subjects; participants reduce their focus of attention in the audiovisual condition mainly to the region of the face of the politician and scan the upper body, including hands and arms, in the video only condition.
Tajik Persian is a dialect of Persian spoken primarily in Tajikistan and written with a modified Cyrillic alphabet. Iranian Persian, or Farsi, as it is natively called, is the lingua franca of Iran and is written with the Persian alphabet, a modified Arabic script. Although the spoken versions of Tajik and Farsi are mutually intelligible to educated speakers of both languages, the difference between the writing systems constitutes a barrier to text compatibility between the two languages. This paper presents a system to transliterate text between these two different Persian dialects that use incompatible writing systems. The system also serves as a mechanism to facilitate sharing of computational linguistic resources between the two languages. This is relevant because of the disparity in resources for Tajik versus Farsi.
Nowadays, Wordnet is used in natural language processing as one of the major linguistic resources. Having such a resource for Persian language helps researchers in computational linguistics and natural language processing fields to develop more accurate systems with higher performances. In this research, we propose a model for semi-automatic construction of Persian wordnet of verbs. Compound verbs are a very productive structure in Persian and number of compound verbs is much greater than simple verbs in this language This research is aimed at finding the structure of Persian compound verbs and the relations between verb components. The main idea behind developing this system is using the wordnet of other POS categories (here means noun and adjective) to extract Persian compound verbs, their synsets and their relations. This paper focuses on three main tasks: 1.extracting compound verbs 2.extracting verbal synsets and 3.extracting the relations among verbal synsets such as hypernymy, antonymy and cause.
In this paper, we explore different strategies for implementing a crowdsourcing methodology for a single-step construction of an empirically-derived sense inventory and the corresponding sense-annotated corpus. We report on the crowdsourcing experiments using implementation strategies with different HIT costs, worker qualification testing, and other restrictions. We describe multiple adjustments required to ensure successful HIT design, given significant changes within the crowdsourcing community over the last three years.
In order to understand and model the non-verbal communicative conduct of humans, it seems fruitful to combine qualitative methods (Conversation Analysis) and quantitative techniques (motion capturing). A Tools for data visualization and annotation is important as they constitute a central interface between different research approaches and methodologies. We have developed the pre-annotation tool PAMOCAT that detects motion segments of individual joints. A sophisticated user interface easily allows the annotating person to find correlations between different joints and to export combined qualitative and quantitative annotations to standard annotation tools. Using this technique we are able to examine complex setups with three persons in tight conversion. A functionality to search for special postures of interest and display the frames in an overview makes it easy to analyze difference phenomenas in Conversation Analysis.
Automatically segmenting and classifying clinical free text into sections is an important first step to automatic information retrieval, information extraction and data mining tasks, as it helps to ground the significance of the text within. In this work we describe our approach to automatic section segmentation of clinical records such as hospital discharge summaries and radiology reports, along with section classification into pre-defined section categories. We apply machine learning to the problems of section segmentation and section classification, comparing a joint (one-step) and a pipeline (two-step) approach. We demonstrate that our systems perform well when tested on three data sets, two for hospital discharge summaries and one for radiology reports. We then show the usefulness of section information by incorporating it in the task of extracting comorbidities from discharge summaries.
Finding useful questions is a challenging task in Community Question Answering (CQA). There are two key issues need to be resolved: 1) what is a useful question to the given reference question; and furthermore 2) what kind of relations exist between a given pair of questions. In order to answer these two questions, in this paper, we propose a fine-grained inventory of textual semantic relations between questions and annotate a corpus constructed from the WikiAnswers website. We also extract large archives of question pairs with user-generated links and use them as labeled data for separating useful questions from neutral ones, achieving 72.2{\%} of accuracy. We find such online CQA repositories valuable resources for related research.
In this paper we propose a method to build fine-grained subjectivity lexicons including nouns, verbs and adjectives. The method, which is applied for Dutch, is based on the comparison of word frequencies of three corpora: Wikipedia, News and News comments. Comparison of the corpora is carried out with two measures: log-likelihood ratio and a percentage difference calculation. The first step of the method involves subjectivity identification, i.e. determining if a word is subjective or not. The second step aims at the identification of more fine-grained subjectivity which is the distinction between actor subjectivity and speaker / writer subjectivity. The results suggest that this approach can be usefully applied producing subjectivity lexicons of high quality.
The present paper gives an account of the approach we have led so far to build a database of frozen units. Although it has long been absent from linguistic studies and grammatical tradition, linguistic frozenness is currently a major research issue for linguistic studies, as frozen markers ensure the economy of the language system. The objective of our study is twofold: we first aim to build a comprehensive database of completely frozen units for the French language ― what is traditionally called absolute or total frozenness. We started the project with the description of adverbial units ― in the long term, we will also naturally describe adjectival, verbal and nominal phrases ― and we will first present the database we have developed so far. This first objective is necessarily followed by the second one, which aims to assess the frozenness degree of the other units (i.e. relative frozenness). In this perspective, we resorted to two sets of methods: linguistic tests and statistical methods processed on two corpora (political and scientific discourse).
Digitised Cultural Heritage (CH) items usually have short descriptions and lack rich contextual information. Wikipedia articles, on the contrary, include in-depth descriptions and links to related articles, which motivate the enrichment of CH items with information from Wikipedia. In this paper we explore the feasibility of finding matching articles in Wikipedia for a given Cultural Heritage item. We manually annotated a random sample of items from Europeana, and performed a qualitative and quantitative study of the issues and problems that arise, showing that each kind of CH item is different and needs a nuanced definition of what ``matching article'' means. In addition, we test a well-known wikification (aka entity linking) algorithm on the task. Our results indicate that a substantial number of items can be effectively linked to their corresponding Wikipedia article.
ATLIS (short for  ATLIS Tags Locations in Strings) is a tool being developed using a maximum-entropy machine learning model for automatically identifying information relating to spatial and locational information in natural language text. It is being developed in parallel with the ISO-Space standard for annotation of spatial information (Pustejovsky, Moszkowicz {\&} Verhagen 2011). The goal of ATLIS is to be able to take in a document as raw text and mark it up with ISO-Space annotation data, so that another program could use the information in a standardized format to reason about the semantics of the spatial information in the document. The tool (as well as ISO-Space itself) is still in the early stages of development. At present it implements a subset of the proposed ISO-Space annotation standard: it identifies expressions that refer to specific places, as well as identifying prepositional constructions that indicate a spatial relationship between two objects. In this paper, the structure of the ATLIS tool is presented, along with preliminary evaluations of its performance.
Ambient Assisted Living (AAL) is the name for a European technology and innovation funding programme. AAL research field is about intelligent assistant systems for a healthier and safer life in the preferred living environments through the use of Information and Communication Technologies (ICT). We focus specifically on speech and gesture interaction which can enhance the quality of lifestyle of people living in assistive environments, be they seniors or people with physical or cognitive disabilities. In this paper we describe our user study conducted in a lab at the University of Bremen in order to collect empirical speech and gesture data and later create and analyse a multimodal corpus. The user study is about a human user sitting in a wheelchair and performing certain inherently spatial tasks.
We present an attempt at using 3rd party observer gaze to get a measure of how appropriate each segment in a dialogue is for a speaker change. The method is a step away from the current dependency of speaker turns or talkspurts towards a more general view of speaker changes. We show that 3rd party observers do indeed largely look at the same thing (the speaker), and how this can be captured and utilized to provide insights into human communication. In addition, the results also suggest that there might be differences in the distribution of 3rd party observer gaze depending on how information-rich an utterance is.
We present an approach to the description of Polish Multi-word Expressions (MWEs) which is based on expressions in the WCCL language of morpho-syntactic constraints instead of grammar rules or transducers. For each MWE its basic morphological form and the base forms of its constituents are specified but also each MWE is assigned to a class on the basis of its syntactic structure. For each class a WCCL constraint is defined which is parametrised by string variables referring to MWE constituent base forms or inflected forms. The constraint specifies a minimal set of conditions that must be fulfilled in order to recognise an occurrence of the given MWE in text with high accuracy. Our formalism is focused on the efficient description of large MWE lexicons for the needs of utilisation in text processing. The formalism allows for the relatively easy representation of flexible word order and discontinuous constructions. Moreover, there is no necessity for the full specification of the MWE grammatical structure. Only some aspects of the particular MWE structure can be selected in way facilitating the target accuracy of recognition. On the basis of a set of simple heuristics, WCCL-based representation of MWEs can be automatically generated from a list of MWE base forms. The proposed representation was applied on a practical scale for the description of a large set of Polish MWEs included in plWordNet.
Abstract In this paper, we describe a multilingual open-source computational grammar of Persian, developed in Grammatical Framework (GF) ― A type-theoretical grammar formalism. We discuss in detail the structure of different syntactic (i.e. noun phrases, verb phrases, adjectival phrases, etc.) categories of Persian. First, we show how to structure and construct these categories individually. Then we describe how they are glued together to make well-formed sentences in Persian, while maintaining the grammatical features such as agreement, word order, etc. We also show how some of the distinctive features of Persian, such as the ezafe construction, are implemented in GF. In order to evaluate the grammar's correctness, and to demonstrate its usefulness, we have added support for Persian in a multilingual application grammar (the Tourist Phrasebook) using the reported resource grammar.
This paper concerns the presentation of two projects that aim to make available an online archive of 4,000 original private letters, mainly having in mind research in Linguistics (Corpus Linguistics, Historical Linguistics, Pragmatics, Sociolinguistics, General Linguistics), History and Sociology. Our corpus is prepared for each research area and provides a diachronic archive of the Portuguese language. Projects CARDS and FLY have the main goal of making available an online electronic edition of each letter, which is completely open source, searchable and available. Users can search for an individual letter, a text by type, a group of letters by year or even the whole archive as a corpus for research or other purposes. The means of corpus presentation is a multimodal framework, since it joins together both the manuscript's image and the written text: the letter's material representation in facsimile and the letter's digital transcription. This editing method allows for the possibility of creating an annotated corpus where the textual unity is not lost.
This paper outlines the design principles and choices, as well as the ongoing development process of the Common Orthographic Vocabulary of the Portuguese Language (VOC), a large scale electronic lexical database which was adopted by the Community of Portuguese-Speaking Countries' (CPLP) Instituto Internacional da L{\'\i}ngua Portuguesa to implement a spelling reform that is currently taking place. Given the different available resources and lexicographic traditions within the CPLP countries, a range of different solutions was adopted for different countries and integrated into a common development framework. Although the publication of lexicographic resources to implement spelling reforms has always been done for Portuguese, VOC represents a paradigm change, switching from idiosyncratic, closed source, paper-format official resources to standardized, open, free, web-accessible and reusable ones. We start by outlining the context that justifies the resource development and its requirements, then focusing on the description of the methodology, workflow and tools used, showing how a collaborative project in a common web-based platform and administration interface make the creation of such a long-sought and ambitious project possible.
In this paper, we describe how NLP can semi-automate the construction and analysis of knowledge in Eunomos, a legal knowledge management service which enables users to view legislation from various sources and find the right definitions and explanations of legal concepts in a given context. NLP can semi-automate some routine tasks currently performed by knowledge engineers, such as classifying norm, or linking key terms within legislation to ontological concepts. This helps overcome the resource bottleneck problem of creating specialist knowledge management systems. While accuracy is of the utmost importance in the legal domain, and the information should be verified by domain experts as a matter of course, a semi-automated approach can result in considerable efficiency gains.
We present a novel corpus for identifying individuals within a group setting that are attempting to gain power within the group. The corpus is entirely in Arabic and is derived from the on-line WikiTalk discussion forums. Entries on the forums were annotated at multiple levels, top-level annotations identified whether an individual was pursuing power on the forum, and low level annotations identified linguistic indicators that signaled an individuals social intentions. An analysis of our annotations reflects a high-degree of overlap between current theories on power and conflict within a group and the behavior of individuals within the transcripts. The described datasource provides an appropriate means for modeling an individual's pursuit of power within an on-line discussion group and also allows for enumeration and validation of current theories on the ways in which individuals strive for power.
When training semantic role labeling systems, the syntax of example sentences is of particular importance. Unfortunately, for the FrameNet annotated sentences, there is no standard parsed version. The integration of the automatic parse of an annotated sentence with its semantic annotation, while conceptually straightforward, is complex in practice. We present a standard dataset that is publicly available and that can be used in future research. This dataset contains parser-generated dependency structures (with POS tags and lemmas) for all FrameNet 1.5 sentences, with nodes automatically associated with FrameNet annotations.
This paper discusses the ongoing development of a new Maltese spell checker, highlighting the methodologies which would best suit such a language. We thus discuss several previous attempts, highlighting what we believe to be their weakest point: a lack of attention to context. Two developments are of particular interest, both of which concern the availability of language resources relevant to spellchecking: (i) the Maltese Language Resource Server (MLRS) which now includes a representative corpus of c. 100M words extracted from diverse documents including the Maltese Legislation, press releases and extracts from Maltese web-pages and (ii) an extensive and detailed corpus of spelling errors that was collected whilst part of the MLRS texts were being prepared. We describe the structure of these resources as well as the experimental approaches focused on context that we are now in a position to adopt. We describe the framework within which a variety of different approaches to spellchecking and evaluation will be carried out, and briefly discuss the first baseline system we have implemented. We conclude the paper with a roadmap for future improvements.
METANET4U is a European project aiming at supporting language technology for European languages and multilingualism. It is a project in the META-NET Network of Excellence, a cluster of projects aiming at fostering the mission of META, which is the Multilingual Europe Technology Alliance, dedicated to building the technological foundations of a multilingual European information society. This paper describe the resources produced at our lab to provide Synthethic voices. Using existing 10h corpus for a male and a female Spanish speakers, voices have been developed to be used in Festival, both with unit-selection and with statistical-based technologies. Furthermore, using data produced for supporting research on intra and inter-lingual voice conversion, four bilingual voices (English/Spanish) have been developed. The paper describes these resources which are available through META. Furthermore, an evaluation is presented to compare different synthesis techniques, influence of amount of data in statistical speech synthesis and the effect of sharing data in bilingual voices.
Retrieving research papers and patents is important for any researcher assessing the scope of a field with high industrial relevance. However, the terms used in patents are often more abstract or creative than those used in research papers, because they are intended to widen the scope of claims. Therefore, a method is required for translating scholarly terms into patent terms. In this paper, we propose six methods for translating scholarly terms into patent terms using two synonym extraction methods: a statistical machine translation (SMT)-based method and a distributional similarity (DS)-based method. We conducted experiments to confirm the effectiveness of our method using the dataset of the Patent Mining Task from the NTCIR-7 Workshop. The aim of the task was to classify Japanese language research papers (pairs of titles and abstracts) using the IPC system at the subclass (third level), main group (fourth level), and subgroup (the fifth and most detailed level). The results showed that an SMT-based method (SMT{\_}ABST+IDF) performed best at the subgroup level, whereas a DS-based method (DS+IDF) performed best at the subclass level.
In this paper we deal with named entity detection on data acquired via OCR process on documents dating from 1890. The resulting corpus is very noisy. We perform an analysis to find possible strategies to overcome errors introduced by the OCR process. We propose a preprocessing procedure in three steps to clean data and correct, at least in part, OCR mistakes. The task is made even harder by the complex tree-structure of named entities annotated on data, we solve this problem however by adopting an effective named entity detection system we proposed in previous work. We evaluate our procedure for preprocessing OCR-ized data in two ways: in terms of perplexity and OOV rate of a language model on development and evaluation data, and in terms of the performance of the named entity detection system on the preprocessed data. The preprocessing procedure results to be effective, allowing to improve by a large margin the system we proposed for the official evaluation campaign on Old Press, and allowing to outperform also the best performing system of the evaluation campaign.
This work describes the process of creation of a 70 billion word text corpus of English. We used an existing language resource, namely the ClueWeb09 dataset, as source for the corpus data. Processing such a vast amount of data presented several challenges, mainly associated with pre-processing (boilerplate cleaning, text de-duplication) and post-processing (indexing for efficient corpus querying using the CQL -- Corpus Query Language) steps. In this paper we explain how we tackled them: we describe the tools used for boilerplate cleaning (jusText) and for de-duplication (onion) that was performed not only on full (document-level) duplicates but also on the level of near-duplicate texts. Moreover we show the impact of each of the performed pre-processing steps on the final corpus size. Furthermore we show how effective parallelization of the corpus indexation procedure was employed within the Manatee corpus management system and during computation of word sketches (one-page, automatic, corpus-derived summaries of a word's grammatical and collocational behaviour) from the resulting corpus.
Domain adaptation is an important task in order for NLP systems to work well in real applications. There has been extensive research on this topic. In this paper, we address two issues that are related to domain adaptation. The first question is how much genre variation will affect NLP systems' performance. We investigate the effect of genre variation on the performance of three NLP tools, namely, word segmenter, POS tagger, and parser. We choose the Chinese Penn Treebank (CTB) as our corpus. The second question is how one can estimate NLP systems' performance when gold standard on the test data does not exist. To answer the question, we extend the prediction model in (Ravi et al., 2008) to provide prediction for word segmentation and POS tagging as well. Our experiments show that the predicted scores are close to the real scores when tested on the CTB data.
We give an overview of our approach to the extraction of interactions between pharmacogenomic entities like drugs, genes and diseases and suggest classes of interaction types driven by data from PharmGKB and partly following the top level ontology WordNet and biomedical types from BioNLP. Our text mining approach to the extraction of interactions is based on syntactic analysis. We use syntactic analyses to explore domain events and to suggest a set of interaction labels for the pharmacogenomics domain.
Since 2008, the Japanese FrameNet (JFN, http://jfn.st.hc.keio.ac.jp/) project has been annotating the Balanced Corpus of Contemporary Written Japanese (BCCWJ), the first such corpus, officially released in October 2011. This paper reports annotation results of the book genre of BCCWJ (Ohara 2011, Ohara, Saito, Fujii {\&} Sato 2011). Comparing the semantic frames needed to annotate BCCWJ with those that the FrameNet (FN) project (Fillmore and Baker 2009, Fillmore 2006) already has defined revealed that: 1) differences in the Japanese and English semantic frames often concern different perspectives and different lexical aspects exhibited by the two lexicons; and 2) in most of the cases where JFN defined new semantic frame for a word, the frame did not involve culture-specific scenes. We investigated the extent to which existing semantic frames originally defined for analyzing English words were used, annotating 810 sentences of the so-called core data of the book genre of BCCWJ. In the 810 sentences we were able to assign semantic frames to approximately 4000 words, although we could not assign any to 587 words. That is, of all the LUs in the sentences, we were able to identify semantic frames to about 87 per cent of them. In other words, the semantic frames already defined in FN for English could be used for 87 per cent of the Japanese LUs.
While a word in isolation has a high potential of expressing various senses, in certain phrases this potential is restricted up to the point that one and only one sense is possible. A phrase is called sense stable if the senses of all the words compounding it do not change their sense irrespective of the context which could be added to its left or to its right. By comparing sense stable phrases we can extract corpus patterns. These patterns have slots which are filled by semantic types that capture the relevant information for disambiguation. The relationship between slots is such that a chain like disambiguation process is possible. Annotating a corpus with these kinds of patterns is beneficial for NLP, because problems such as data sparseness, noise, learning complexity are alleviated. We evaluate the inter agreement of annotators on examples coming from BNC.
The analysis of a corpus of micro-blogs on the topic of the 2011 UK referendum about the Alternative Vote has been undertaken as a joint activity by text miners and social scientists. To facilitate the collaboration, the corpus and its analysis is managed in a Web-accessible framework that allows users to upload their own textual data for analysis and to manage their own text annotation resources used for analysis. The framework also allows annotations to be searched, and the analysis to be re-run after amending the analysis resources. The corpus is also doubly human-annotated stating both whether each tweet is overall positive or negative in sentiment and whether it is for or against the proposition of the referendum.
We present AWATIF, a multi-genre corpus of Modern Standard Arabic (MSA) labeled for subjectivity and sentiment analysis (SSA) at the sentence level. The corpus is labeled using both regular as well as crowd sourcing methods under three different conditions with two types of annotation guidelines. We describe the sub-corpora constituting the corpus and provide examples from the various SSA categories. In the process, we present our linguistically-motivated and genre-nuanced annotation guidelines and provide evidence showing their impact on the labeling task.
In this paper, we analyze the causes of task completion errors in spoken dialog systems, using a decision tree with N-gram features of the dialog to detect task-incomplete dialogs. The dialog for a music retrieval task is described by a sequence of tags related to user and system utterances and behaviors. The dialogs are manually classified into two classes: completed and uncompleted music retrieval tasks. Differences in tag classification performance between the two classes are discussed. We then construct decision trees which can detect if a dialog finished with the task completed or not, using information gain criterion. Decision trees using N-grams of manual tags and automatic tags achieved 74.2{\%} and 80.4{\%} classification accuracy, respectively, while the tree using interaction parameters achieved an accuracy rate of 65.7{\%}. We also discuss more details of the causality of task incompletion for spoken dialog systems using such trees.
The paper presents a procedure for generating prefixed verbs in Croatian comprising combinations of one, two or three prefixes. The result of this generation process is a pool of derivationally valid prefixed verbs, although not necessarily occuring in corpora. The statistics of occurences of generated verbs in Croatian National Corpus has been calculated. Further usage of such language resource with generated potential verbs is also suggested, namely, enrichment of Croatian Morphological Lexicon, Croatian Wordnet and CROVALLEX.
This paper presents a model to enrich an ontology with a thesaurus based on a domain corpus and WordNet. The model is applied to the data privacy domain and the initial domain resources comprise a data privacy ontology, a corpus of privacy laws, regulations and guidelines for projects. Based on these resources, a thesaurus is automatically generated. The thesaurus seeds are composed by the ontology concepts. For these seeds similar terms are extracted from the corpus using known thesaurus generation methods. A filtering process searches for semantic relations between seeds and similar terms within WordNet. As a result, these semantic relations are used to expand the ontology with relations between them and related terms in the corpus. The resulting resource is a hierarchical structure that can help on the ontology investigation and maintenance. The results allow the investigation of the domain knowledge with the support of semantic relations not present on the original ontology.
Researchers in the fields of psycholinguistics and neurolinguistics increasingly test their experimental hypotheses against probabilistic models of language. VALEX (Korhonen et al., 2006) is a large-scale verb lexicon that specifies verb usage as probability distributions over a set of 163 verb SUBCATEGORIZATION FRAMES (SCFs). VALEX has proved to be a popular computational linguistic resource and may also be used by psycho- and neurolinguists for experimental analysis and stimulus generation. However, a probabilistic model based upon a set of 163 SCFs often proves too fine grained for experimenters in these fields. Our goal is to simplify the classification by grouping the frames into genera―explainable clusters that may be used as experimental parameters. We adopted two methods for reclassification. One was a manual linguistic approach derived from verb argumentation and clause features; the other was an automatic, computational approach driven from a graphical representation of SCFs. The premise was not only to compare the results of two quite different methods for our own interest, but also to enable other researchers to choose whichever reclassification better suited their purpose (one being grounded purely in theoretical linguistics and the other in practical language engineering). The various classifications are available as an online resource to researchers.
Although the availability of the natural language processing tools and the development of metrics to evaluate them increases, there is a certain gap to fill in that field for the less-resourced languages, such as Polish. Therefore the projects which are designed to extend the existing tools for diverse languages are the best starting point for making these languages more and more covered. This paper presents the results of the first attempt of the co{\textbackslash}-re{\textbackslash}-fe{\textbackslash}-rence resolution for Polish using statistical methods. It presents the conclusions from the process of adapting the Beautiful Anaphora Resolution Toolkit (BART; a system primarily designed for the English language) for Polish and collates its evaluation results with those of the previously implemented rule-based system. Finally, we describe our plans for the future usage of the tool and highlight the upcoming research to be conducted, such as the experiments of a larger scale and the comparison with other machine learning tools.
This paper describes the LDC forced aligner which was designed to align audio and transcripts. Unlike existing forced aligners, LDC forced aligner can align partially transcribed audio files, and also audio files with large chunks of non-speech segments, such as noise, music, silence etc, by inserting optional wildcard phoneme sequences between sentence or paragraph boundaries. Based on the HTK tool kit, LDC forced aligner can align audio and transcript on sentence or word level. This paper also reports its usage on English and Mandarin Chinese data.
Comparable news texts are frequently proposed as a potential source of alignable subsentential fragments for use in statistical machine translation systems. But can we assess just how potentially useful they will be? In this paper we first discuss a scheme for classifying news text pairs according to the degree of relatedness of the events they report and investigate how robust this classification scheme is via a multi-lingual annotation exercise. We then propose an annotation methodology, similar to that used in summarization evaluation, to allow us to identify and quantify shared content at the subsentential level in news text pairs and report a preliminary exercise to assess this method. We conclude by discussing how this works fits into a broader programme of assessing the potential utility of comparable news texts for extracting paraphrases/translational equivalents for use in language processing applications.
This paper describes research on building text-to-speech synthesis systems (TTS) for resource poor languages using available resources from other languages and describes our general approach to building cross-linguistic polyglot TTS. Our approach involves three main steps: language clustering, grapheme to phoneme mapping and prosody modelling. We have tested the mapping of phonemes from German to English and from Indonesian to Spanish. We have also constructed three prosody representations for different language characteristics. For evaluation we have developed an English TTS based on German data, and a Spanish TTS based on Indonesian data and compared their performance against pre-existing monolingual TTSs. Since our motivation is to develop speech synthesis for resource poor languages, we have also developed three TTS for Iban, an Austronesian language with practically no available language resources, using Malay, Indonesian and Spanish resources.
ConceptNet is a knowledge representation project, providing a large semantic graph that describes general human knowledge and how it is expressed in natural language. This paper presents the latest iteration, ConceptNet 5, including its fundamental design decisions, ways to use it, and evaluations of its coverage and accuracy.
We report on several experiments on combining a rule-based tagger and a trigram tagger for Spanish. The results show that one can boost the accuracy of the best performing n-gram taggers by quickly developing a rough rule-based grammar to complement the statistically induced one and then combining the output of the two. The specific method of combination is crucial for achieving good results. The method provides particularly large gains in accuracy when only a small amount of tagged data is available for training a HMM, as may be the case for lesser-resourced and minority languages.
We describe in-progress work on the creation of a new lexical resource that contains a list of 486 verbs annotated with quantified temporal durations for the events that they describe. This resource is being compiled from more than 14 million tweets from the Twitter microblogging site. We are creating this lexicon of verbs and typical durations to address a gap in the available information that is represented in existing research. The data that is contained in this lexical resource is unlike any existing resources, which have been traditionally comprised from literature excerpts, news stories, and full-length weblogs. The kind of knowledge about how long an event lasts is crucial for natural language processing and is especially useful when the temporal duration of an event is implied. We are using data from Twitter because Twitter is a rich resource since people are publicly posting about real events and real durations of those events throughout the day.
This paper describes the development of a free/open-source finite-state morphological transducer for Kyrgyz. The transducer has been developed for morphological generation for use within a prototype Turkish{\^a}Kyrgyz machine translation system, but has also been extensively tested for analysis. The finite-state toolkit used for the work was the Helsinki Finite-State Toolkit (HFST). The paper describes some issues in Kyrgyz morphology, the development of the tool, some linguistic issues encountered and how they were dealt with, and which issues are left to resolve. An evaluation is presented which shows that the transducer has medium-level coverage, between 82{\%} and 87{\%} on two freely available corpora of Kyrgyz, and high precision and recall over a manually verified test set.
Deliberative, argumentative discourse is an important component of opinion formation, belief revision, and knowledge discovery; it is a cornerstone of modern civil society. Argumentation is productively studied in branches ranging from theoretical artificial intelligence to political rhetoric, but empirical analysis has suffered from a lack of freely available, unscripted argumentative dialogs. This paper presents the Internet Argument Corpus (IAC), a set of 390,704 posts in 11,800 discussions extracted from the online debate site 4forums.com. A 2866 thread/130,206 post extract of the corpus has been manually sided for topic of discussion, and subsets of this topic-labeled extract have been annotated for several dialogic and argumentative markers: degrees of agreement with a previous post, cordiality, audience-direction, combativeness, assertiveness, emotionality of argumentation, and sarcasm. As an application of this resource, the paper closes with a discussion of the relationship between discourse marker pragmatics, agreement, emotionality, and sarcasm in the IAC corpus.
With the constant increase in the amount of information available in online communities, the task of building an appropriate Recommender System to support the user in her decision making process is becoming more and more challenging. In addition to the classical collaborative filtering and content based approaches, taking into account ratings, preferences and demographic characteristics of the users, a new type of Recommender System, based on personality parameters, has been emerging recently. In this paper we describe the TWIN (Tell Me What I Need) Personality Based Recommender System, and report on our experiments and experiences of utilizing techniques which allow the extraction of the personality type from text (following the Big Five model popular in the psychological research). We estimate the possibility of constructing the personality-based Recommender System that does not require users to fill in personality questionnaires. We are applying the proposed system in the online travelling domain to perform TripAdvisor hotels recommendation by analysing the text of user generated reviews, which are freely accessible from the community website.
In this paper we present a framework to derive sentiment lexicons in a target language by using manually or automatically annotated data available in an electronic resource rich language, such as English. We show that bridging the language gap using the multilingual sense-level aligned WordNet structure allows us to generate a high accuracy (90{\%}) polarity lexicon comprising 1,347 entries, and a disjoint lower accuracy (74{\%}) one encompassing 2,496 words. By using an LSA-based vectorial expansion for the generated lexicons, we are able to obtain an average F-measure of 66{\%} in the target language. This implies that the lexicons could be used to bootstrap higher-coverage lexicons using in-language resources.
In this paper we investigate the role of multilingual features in improving word sense disambiguation. In particular, we explore the use of semantic clues derived from context translation to enrich the intended sense and therefore reduce ambiguity. Our experiments demonstrate up to 26{\%} increase in disambiguation accuracy by utilizing multilingual features as compared to the monolingual baseline.
Language resources have become a key factor in the development cycle of language technology. The current prevailing methodologies, the sheer number of languages and the vast volumes of digital content together with the wide palette of useful content processing applications, render new models for managing the underlying language resources indispensable. This paper presents META-SHARE, an open resource exchange infrastructure, which aims to boost visibility, documentation, identification, openness and sharing, collaboration, preservation and interoperability of language data and basic language processing tools. META-SHARE is implemented as a network of distributed repositories of language resources. It offers providers and consumers of resources the necessary functionalities for describing, storing, searching, licensing and downloading language resources in a single integrated technical platform. META-SHARE favours and aligns itself with the growing open data and open source tools movement. To this end, it has prepared the necessary underlying legal framework consisting of a Charter for language resource sharing, as well as a set of licensing templates aiming to act as recommended licence models in an attempt to facilitate the legal interoperability of language resources. In its current version, META-SHARE features 13 resource repositories, with over 1200 resource packages.
We present a set of stand-off annotations for the ninety thousand sentences in the spoken section of the British National Corpus (BNC) which feature a progressive aspect verb group. These annotations may be matched to the original BNC text using the supplied document and sentence identifiers. The annotated features mostly relate to linguistic form: subject type, subject person and number, form of auxiliary verb, and clause type, tense and polarity. In addition, the sentences are classified for register, the formality of recording context: three levels of `spontaneity' with genres such as sermons and scripted speech at the most formal level and casual conversation at the least formal. The resource has been designed so that it may easily be augmented with further stand-off annotations. Expert linguistic annotations of spoken data, such as these, are valuable for improving the performance of natural language processing tools in the spoken language domain and assist linguistic research in general.
A significant amount of spatial information in textual documents is hidden within the relationship between events. While humans have an intuitive understanding of these relationships that allow us to recover an object's or event's location, currently no annotated data exists to allow automatic discovery of spatial containment relations between events. We present our process for building such a corpus of manually annotated spatial relations between events. Events form complex predicate-argument structures that model the participants in the event, their roles, as well as the temporal and spatial grounding. In addition, events are not presented in isolation in text; there are explicit and implicit interactions between events that often participate in event structures. In this paper, we focus on five spatial containment relations that may exist between events: (1) SAME, (2) CONTAINS, (3) OVERLAPS, (4) NEAR, and (5) DIFFERENT. Using the transitive closure across these spatial relations, the implicit location of many events and their participants can be discovered. We discuss our annotation schema for spatial containment relations, placing it within the pre-existing theories of spatial representation. We also discuss our annotation guidelines for maintaining annotation quality as well as our process for augmenting SpatialML with spatial containment relations between events. Additionally, we outline some baseline experiments to evaluate the feasibility of developing supervised systems based on this corpus. These results indicate that although the task is challenging, automated methods are capable of discovering spatial containment relations between events.
We introduce a new corpus of sentence-level agreement and disagreement annotations over LiveJournal and Wikipedia threads. This is the first agreement corpus to offer full-document annotations for threaded discussions. We provide a methodology for coding responses as well as an implemented tool with an interface that facilitates annotation of a specific response while viewing the full context of the thread. Both the results of an annotator questionnaire and high inter-annotator agreement statistics indicate that the annotations collected are of high quality.
We present a new version of a Graphical User Interface (GUI) called DiCoInfo Visuel, mainly based on a graph visualization device and used for exploring and assessing lexical data found in DiCoInfo, a specialized e-dictionary of computing and the Internet. This new GUI version takes advantage of the fundamental nature of the lexical network encoded in the dictionary: it uses logic based methods from logic programming to explore relations between entries and find pieces of relevant information that may be not accessible by direct searches. The result is a more realistic and useful data coverage shown to end users.
It is well known that accuracies of statistical parsers trained over Penn Treebank on test sets drawn from the same corpus tend to be overestimates of their actual parsing performance. This gives rise to the need for evaluation of parsing performance on corpora from different domains. Evaluating multiple parsers on test sets from different domains can give a detailed picture about the relative strengths/weaknesses of different parsing approaches. Such information is also necessary to guide choice of parser in applications such as machine translation where text from multiple domains needs to be handled. In this paper, we report a benchmarking study of different state-of-art parsers for English, both constituency and dependency. The constituency parser output is converted into CoNLL-style dependency trees so that parsing performance can be compared across formalisms. Specifically, we train rerankers for Berkeley and Stanford parsers to study the usefulness of reranking for handling texts from different domains. The results of our experiments lead to interesting insights about the out-of-domain performance of different English parsers.
POS Taggers typically fail to correctly tag grammatical neologisms: for known words, a tagger will only take known tags into account, and hence discard any possibility that the word is used in a novel or deviant grammatical category in the text at hand. Grammatical neologisms are relatively rare, and therefore do not pose a significant problem for the overall performance of a tagger. But for studies on neologisms and grammaticalization processes, this makes traditional taggers rather unfit. This article describes a modified POS tagger that explicitly considers new tags for known words, hence making it better fit for neologism research. This tagger, called NeoTag, has an overall accuracy that is comparable to other taggers, but scores much better for grammatical neologisms. To achieve this, the tagger applies a system of {{\textbackslash}em lexical smoothing}, which adds new categories to known words based on known homographs. NeoTag also lemmatizes words as part of the tagging system, achieving a high accuracy on lemmatization for both known and unknown words, without the need for an external lexicon. The use of NeoTag is not restricted to grammatical neologism detection, and it can be used for other purposes as well.
This paper shows how interoperable dialogue act annotations, using the multidimensional annotation scheme and the markup language DiAML of ISO standard 24617-2, can conveniently be obtained using the newly implemented facility in the ANVIL annotation tool to produce XML-based output directly in the DiAML format. ANVIL offers the use of multiple user-defined `tiers' for annotating various kinds of information. This is shown to be convenient not only for multimodal information but also for dialogue act annotation according to ISO standard 24617-2 because of the latter's multidimensionality: functional dialogue segments are viewed as expressing one or more dialogue acts, and every dialogue act belongs to one of a number of dimensions of communication, defined in the standard, for each of which a different ANVIL tier can conveniently be used. Annotations made in the multi-tier interface can be exported in the ISO 24617-2 format, thus supporting the creation of interoperable annotated corpora of multimodal dialogue.
This paper presents two annotated corpora for word alignment between Japanese and English. We annotated on top of the IWSLT-2006 and the NTCIR-8 corpora. The IWSLT-2006 corpus is in the domain of travel conversation while the NTCIR-8 corpus is in the domain of patent. We annotated the first 500 sentence pairs from the IWSLT-2006 corpus and the first 100 sentence pairs from the NTCIR-8 corpus. After mentioned the annotation guideline, we present two evaluation algorithms how to use such hand-annotated corpora: although one is a well-known algorithm for word alignment researchers, one is novel which intends to evaluate a MAP-based word aligner of Okita et al. (2010b).
Sentiment Analysis (SA) and Opinion Mining (OM) have become a popular task in recent years in NLP with the development of language resources, corpora and annotation schemes. The possibility to discriminate between objective and subjective expressions contributes to the identification of a document's semantic orientation and to the detection of the opinions and sentiments expressed by the authors or attributed to other participants in the document. Subjectivity word sense disambiguation helps in this task, automatically determining which word senses in a corpus are being used subjectively and which are being used objectively. This paper reports on a methodology to assign in a semi-automatic way connotative values to eventive nouns usually labelled as neutral through syntagmatic patterns that express cause-effect relations between emotion cause events and emotion words. We have applied our method to nouns and we have been able reduce the number of OBJ polarity values associated to event noun.
Interactive story systems often involve dialogue with virtual dramatic characters. However, to date most character dialogue is written by hand. One way to ease the authoring process is to (semi-)automatically generate dialogue based on film characters. We extract features from dialogue of film characters in leading roles. Then we use these character-based features to drive our language generator to produce interesting utterances. This paper describes a corpus of film dialogue that we have collected from the IMSDb archive and annotated for linguistic structures and character archetypes. We extract different sets of features using external sources such as LIWC and SentiWordNet as well as using our own written scripts. The automation of feature extraction also eases the process of acquiring additional film scripts. We briefly show how film characters can be represented by models learned from the corpus, how the models can be distinguished based on different categories such as gender and film genre, and how they can be applied to a language generator to generate utterances that can be perceived as being similar to the intended character model.
SPPAS is a tool to produce automatic annotations which include utterance, word, syllabic and phonemic segmentations from a recorded speech sound and its transcription. SPPAS is distributed under the terms of the GNU Public License. It was successfully applied during the Evalita 2011 campaign, on Italian map-task dialogues. It can also deal with French, English and Chinese and there is an easy way to add other languages. The paper describes the development of resources and free tools, consisting of acoustic models, phonetic dictionaries, and libraries and programs to deal with these data. All of them are publicly available.
On the Linguistic Data Consortium's (LDC) 20th anniversary, this paper describes the changes to the language resource landscape over the past two decades, how LDC has adjusted its practice to adapt to them and how the business model continues to grow. Specifically, we will discuss LDC's evolving roles and changes in the sizes and types of LDC language resources (LR) as well as the data they include and the annotations of that data. We will also discuss adaptations of the LDC business model and the sponsored projects it supports.
Recent advances in modeling early language acquisition are due not only to the development of machine-learning techniques, but also to the increasing availability of data on child language and child-adult interaction. In the absence of recordings of child-directed speech, or when models explicitly require such a representation for training data, phonemic transcriptions are commonly used as input data. We present a novel (and to our knowledge, the first) phonemic corpus of Polish child-directed speech. It is derived from the Weist corpus of Polish, freely available from the seminal CHILDES database. For the sake of reproducibility, and to exemplify the typical trade-off between ecological validity and sample size, we report all preprocessing operations and transcription guidelines. Contributed linguistic resources include updated CHAT-formatted transcripts with phonemic transcriptions in a novel phonology tier, as well as by-product data, such as a phonemic lexicon of Polish. All resources are distributed under the LGPL-LR license.
Academic lectures offer valuable content, but often do not reach their full potential audience due to the language barrier. Human translations of lectures are too expensive to be widely used. Speech translation technology can be an affordable alternative in this case. State-of-the-art speech translation systems utilize statistical models that need to be trained on large amounts of in-domain data. In order to support the KIT lecture translation project in its effort to introduce speech translation technology in KIT's lecture halls, we have collected a corpus of German lectures at KIT. In this paper we describe how we recorded the lectures and how we annotated them. We further give detailed statistics on the types of lectures in the corpus and its size. We collected the corpus with the purpose in mind that it should not just be suited for training a spoken language translation system the traditional way, but should also enable us to research techniques that enable the translation system to automatically and autonomously adapt itself to the varying topics and speakers of lectures
This paper addresses the problem of the enrichment of transcriptions in the perspective of an automatic phonetization. Phonetization is the process of representing sounds with phonetic signs. There are two general ways to construct a phonetization process: rule based systems (with rules based on inference approaches or proposed by expert linguists) and dictionary based solutions which consist in storing a maximum of phonological knowledge in a lexicon. In both cases, phonetization is based on a manual transcription. Such a transcription is established on the basis of conventions that can differ depending on their working out context. This present study focuses on three different enrichments of such a transcription. Evaluations compare phonetizations obtained from automatic systems to a reference phonetized manually. The test corpus is made of three types of speech: conversational speech, read speech and political debate. A specific algorithm for the rule-based system is proposed to deal with enrichments. The final system obtained a phonetization of about 95.2{\%} correct (from 3.7{\%} to 5.6{\%} error rates depending on the corpus).
In this paper, we describe the methodology being used to develop certain aspects of ISO-Space, an annotation language for encoding spatial and spatiotemporal information as expressed in natural language text. After reviewing the requirements of a specification for capturing such knowledge from linguistic descriptions, we describe how ISO-Space has developed to meet the needs of the specification. ISO-Space is an emerging resource that is being developed in the context of an iterative effort to test the specification model with annotation, a methodology called MAMA (Model-Annotate-Model-Annotate) (Pustejovsky and Stubbs, 2012). We describe the genres of text that are being used in a pilot annotation study, in order to both refine and enrich the specification language by way of crowd sourcing simple annotation tasks with Amazon's Mechanical Turk Service.
Named entity recognition, which focuses on the identification of the span and type of named entity mentions in texts, has drawn the attention of the NLP community for a long time. However, many real-life applications need to know which real entity each mention refers to. For such a purpose, often refered to as entity resolution and linking, an inventory of entities is required in order to constitute a reference. In this paper, we describe how we extracted such a resource for French from freely available resources (the French Wikipedia and the GeoNames database). We describe the results of an instrinsic evaluation of the resulting entity database, named Aleda, as well as those of a task-based evaluation in the context of a named entity detection system. We also compare it with the NLGbAse database (Charton and Torres-Moreno, 2010), a resource with similar objectives.
We report here on the eighth evaluation campaign organized in 2011 by the IWSLT workshop series. That IWSLT 2011 evaluation focused on the automatic translation of public talks and included tracks for speech recognition, speech translation, text translation, and system combination. Unlike in previous years, all data supplied for the evaluation has been publicly released on the workshop website, and is at the disposal of researchers interested in working on our benchmarks and in comparing their results with those published at the workshop. This paper provides an overview of the IWSLT 2011 evaluation campaign, and describes the data supplied, the evaluation infrastructure made available to participants, and the subjective evaluation carried out.
Automatic approaches to creating and extending wordnets, which have become very popular in the past decade, inadvertently result in noisy synsets. This is why we propose an approach to detect synset outliers in order to eliminate the noise and improve accuracy of the developed wordnets, so that they become more useful lexico-semantic resources for natural language applications. The approach compares the words that appear in the synset and its surroundings with the contexts of the literals in question they are used in based on large monolingual corpora. By fine-tuning the outlier threshold we can influence how many outlier candidates will be eliminated. Although the proposed approach is language-independent we test it on Slovene and French that were created automatically from bilingual resources and contain plenty of disambiguation errors. Manual evaluation of the results shows that by applying a threshold similar to the estimated error rate in the respective wordnets, 67{\%} of the proposed outlier candidates are indeed incorrect for French and a 64{\%} for Slovene. This is a big improvement compared to the estimated overall error rates in the resources, which are 12{\%} for French and 15{\%} for Slovene.
One of the major issues dealing with any workflow management frameworks is the components interoperability. In this paper, we are concerned with the Apache UIMA framework. We address the problem by considering separately the development of new components and the integration of existing tools. For the former objective, we propose an API to generically handle TS objects by their name using reflexivity in order to make the components TS-independent. In the latter case, we distinguish the case of aggregating heterogeneous TS-dependent UIMA components from the case of integrating non UIMA-native third party tools. We propose a mapper component to aggregate TS-dependent UIMA components. And we propose a component to wrap command lines third party tools and a set of components to connect various markup languages with the UIMA data structure. Finally, we present two situations where these solutions were effectively used: Training a POS tagger system from a treebank, and embedding an external POS tagger in a workflow. Our approch aims at providing quick development solutions.
In this paper, we introduce a set of resources that we have derived from the EST R{\'E}PUBLICAIN CORPUS, a large, freely-available collection of regional newspaper articles in French, totaling 150 million words. Our resources are the result of a full NLP treatment of the EST R{\'E}PUBLICAIN CORPUS: handling of multi-word expressions, lemmatization, part-of-speech tagging, and syntactic parsing. Processing of the corpus is carried out using statistical machine-learning approaches - joint model of data driven lemmatization and part- of-speech tagging, PCFG-LA and dependency based models for parsing - that have been shown to achieve state-of-the-art performance when evaluated on the French Treebank. Our derived resources are made freely available, and released according to the original Creative Common license for the EST R{\'E}PUBLICAIN CORPUS. We additionally provide an overview of the use of these resources in various applications, in particular the use of generated word clusters from the corpus to alleviate lexical data sparseness for statistical parsing.
In this paper, we propose a simple methodology for building or extending wordnets using easily extractible lexical knowledge from Wiktionary and Wikipedia. This method relies on a large multilingual translation/synonym graph in many languages as well as synset-aligned wordnets. It guesses frequent and polysemous literals that are difficult to find using other methods by looking at back-translations in the graph, showing that the use of a heavily multilingual lexicon can be a way to mitigate the lack of wide coverage bilingual lexicon for wordnet creation or extension. We evaluate our approach on French by applying it for extending WOLF, a freely available French wordnet.
Abstract This paper describes the method and experiences of text and speech data collection in mobile communication in Indian English Hindi. The primary data collection is done in the form of large number of messages as part of Personal communication among natives of Hindi language and Indian speakers of English. To gather the versatility of mobile communication database among Hindi and English, 12 domains were identified for collection of text corpus from speaking population belonging to deferent age groups, sex and dialects. The text obtained in raw form based on slangs and unconventional grammar were cleaned using on language grammar rules and then tagged and expanded to explain context specific meaning of the words. Texts of 1163 participants from Hindi speaking regions and 1405 English users were taken for creating 13 prompt sheets; containing 630 phonetically rich sentences created using a special software. Each prompt sheet was recorded by at least 7 users simultaneously in three channels and recorded by a total of 100 speakers and annotated. The work is a step forward in the direction of development of standards for mobile text and speech data collection for Indian languages. Keywords - Speech data base, Text analysis, mobile communication, Hindi and Indian English Speech, multi-lingual speech processing.
∗ International Computer Science Institute, 1947 Center St. Ste. 600, Berkeley, CA 94611, USA. E-mail: ﬁllmore@icsi.berkeley.edu. I am especially indebted to the three directors of the International Computer Science Institute during the life of the FrameNet Project (Jerome Feldman, Nelson Morgan, and Roberto Pieraccini) and to Collin Baker, FrameNet Project Manager, for keeping the project alive during the recent years of my relative inactivity; to Mary Catherine O’Connor and Russell Lee Goldman for important assistance in the preparation of the present document; and to Lily Wong Fillmore, videographer, editor, and censor for the broadcast version of the acceptance speech. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 4  example was “With a haggard lift of the upper lip . . . ” I took the book home, cut sheets of typewriter paper into eight pieces to make ﬁle slips, chose phrases I thought I should memorize, and copied them onto these slips. I held them together with rubber bands, and I kept them in a secret place in my room. Thus supported with the early 1940s technologies of paper, scissors, pencil, and rubber bands, my earliest theory of language began to develop: Linguistic competence is having access to a large repertory of ready-made things to say. I added to the collection over the years, as I came upon clever or wise expressions, and consulted a selection of them every night, scheming to create situations in which I could use them, in speaking or writing. In later years I held on to the suspicion that much of ordinary conversation in real life involves calling on remembered phrases rather than creating novel expressions from rules. Much later I learned that in many Eastern European countries inﬂuenced by the Moscow School, the divisions of the ﬁeld of Linguistics were Phonology, Morphology, Lexicology, Phraseology, and Syntax. The study of phraseological units—phraseologisms—was seen as central, not peripheral, to linguistic inquiry. My ﬁrst exposure to the actual ﬁeld of Linguistics came a year later, around age 15, when a missionary lady on leave, living on my block in St. Paul, gave me a copy of Eugene Nida’s little book, Linguistic Interludes (Nida 1947). The text of this book takes the form of conversations in a college campus co-op between a clever and wise linguist and a caricatured collection of innocent and unsuspecting students and colleagues, among them a classicist who strongly defended the logical perfection of the classical languages Greek and Latin. This book succeeded in conveying simply many of the things that linguists believe: r Relevant linguistic generalizations are based on speech, not writing. r Almost all concepts of “correct grammar” are inventions, with no basis in the history of the language. r There may be primitive communities, but there are no primitive languages. The minor protagonists in the conversation contested each of these principles, and the linguist hero, from his vast knowledge of the most exotic of the world’s languages, kept showing them how wrong they were. I liked the idea of knowing things that most people, including college professors, had wrong opinions about. I also liked the idea of being able to help them change their wrong opinions, so I decided to study Linguistics. 2. Formal Studies Begin Before long I was enrolled in a fairly small linguistics program at the University of Minnesota. I could live at home, take a streetcar to Minneapolis for classes, and take another streetcar to Montgomery Wards in St. Paul, where I wrapped venetian blinds to support my studies. In those days there were no linguistics textbooks in the modern sense; we studied two books titled Language—one by Edward Sapir (1921) and the other by Leonard Bloomﬁeld (1933)—and we read grammars and treatises. I took two years of Arabic. I supplemented my training in linguistic methods through Summer Linguistic Institutes put on by the Linguistic Society of America, one in Michigan and one in Berkeley, where I learned about Thai, Sanskrit, and Navajo with Mary Haas, Franklin Edgerton, and Harry Hoijer.  702  Fillmore  Computational Linguistics  2.1 First Research Experience: Concordance-Building One of my professors at the University Minnesota was building concordances of some of the minor Late Latin texts, and he permitted the students in his class to work with him on these projects. For the advanced students this was a chance to get valuable hands-on research experience; for the less advanced students it was an opportunity to get “extra credit.” This was in a sense my ﬁrst exposure to corpus-based linguistics. For any given document, the professor would pass on the text to that year’s students. This “ﬁrst generation” of students copied word tokens onto separate index cards, together with each word’s “parse” in the classical sense, and its location in the document. Generation 2—the students in the next year’s class—alphabetized these cards and typed up the concordances. Generation 3, in which I participated, took this same stack of cards and reverse-alphabetized them, so they could be used for research on sufﬁxes. (Personal note: alphabetizing words from right to left is stressful at ﬁrst, but you get used to it.) So with the tools of pre-cut index cards, a pencil, and a typewriter, we students constructed a concordance—we physically experienced that concordance. So you can imagine my surprise when, thirty-some years later, I came upon UNIX commands like sort, sort -r, and grep. I don’t remember if I actually wept. And these were nothing compared to the marvels I experienced later still, with key-word-incontext extraction, lemmatizers, morphological parsers, part-of-speech tagging, sorting by right and left context, and the full toolkit of corpus processing tools that exist today. In those days it took a lot of patience and physical effort to build a concordance. But it also took a lot of patience and physical effort to use a concordance. A printed concordance to the Shakespeare corpus was a vast index in which, for each word, you could ﬁnd every line it occurred in, and you learned where that line appeared in Shakespeare’s writings. You would then go to the actual physical source text, look it up, and see it in its context. For example, if, when studying the phrasal verb take upon I want to ﬁnd the full context of This way will I take upon me to wash your liver I only need to open up As You Like It to Act 3, Scene 2, and hunt for it there. Compare that to the fully-searchable Shakespeare app you can use while sitting on a bus holding your iPad. 3. Encounters Beyond College President Truman’s Displaced Persons Act of 1948–1950 brought thousands of Eastern European immigrants to Minnesota, enabling me to ﬁnd work more satisfying than venetian-blind-wrapping. I began to teach English to Russians, Poles, Ukrainians, and Latvians. Depending on which of the daughters of the families in my classes I was trying to impress, I was motivated to learn something about Slavic and Baltic languages. Soon my student deferment would run out, and I had to decide between waiting for the draft (two years) or enlisting (three years). A persuasive recruiting ofﬁcer promised me one year at the Army Language School in Monterey, CA, (now the Defense Language Institute) for my ﬁrst year. Shortly after that, my head got shaved and I was suddenly a buck private. No one had any record of an offer to spend a year in sunny California learning Polish. I was not allowed to examine my ﬁle. So I took the U.S. Army Russian Language Proﬁciency Test instead. The questions were in spoken Russian, played on a record player, and the answers were multiple choice in English. In those days the art of designing guessproof multiple choice tests had not yet been perfected. There was kind of a student sport to see how well you could do in choosing answers without looking at the questions (you could usually at least get  703  Computational Linguistics  Volume 38, Number 4  a passing score); then you’d go back and read the questions to correct the choices that  weren’t obvious.  Although I didn’t fully understand any of the questions, my score came out as “high  ﬂuent” based in large part on acquired test-taking skills. After basic training, I was sent  to Arlington, VA, for a few months in radio training, after which I was assigned to  Kyoto, Japan, to a small ﬁeld station of the Army Security Agency. My duty: “listening  to Ivan.” The Ivans I listened to on short wave radio never had anything interesting to  say: They were Soviet Air Force men reading numbers, which I was supposed to write  down. Three days of the day shift, three days evening shift, three days night shift, three  days off. I quickly acquired an uncanny ability to detect Russian numbers against noise  and static. They were, of course, coded messages.  My job was to write the numbers down on the most modern typewriter of the day,  a model that had separate keys for zero and one! (The ordinary ofﬁce typewriter at that  time had separate keys for only the numbers 2 through 9, since lower-case L could be  used for 1 and upper-case O could be used for zero.) For this work I needed a very restricted vocabulary: the Russian long and short versions of the numbers 1–9,1 plus a  single version of zero, and the word for ‘mistake.’ If I had been permitted to say what  I was doing I would have said I was in cryptanalysis, but of course actually I was only  copying down the numbers I heard. Somebody smart, thousands of miles away, was  ﬁguring out what they meant.  The limited demands on my time and intellect allowed me to wander around in  Kyoto, with notebooks and dictionaries, trying to learn something about Japanese. The  linguistic methods I had learned back home stopped at morphology, the structure of  words. I hadn’t had any training in ways of representing the structure of a sentence,  but I worked out a do-it-yourself style of sentence diagrams, for both Japanese and  English, and I was fascinated when I found the occasional sentence in Japanese which  could be translated into English word by word backwards, going from the end to the  beginning.  When it was time to be discharged, I believed—wrongly—that I was close to mas-  tering the language, and I wanted to stay another year or two, because I knew I couldn’t  afford to come back to Japan on my own. I managed, with the help of Senator Hubert  Humphrey, to be the ﬁrst Army soldier to get a local discharge in Japan. As a civilian  there, I supported myself by teaching English. With two other visiting Americans I was  permitted to work at Kyoto University with the endlessly kind and patient Professor  Endo Yoshimoto (  ).  Professor Endo was the author of the main school grammar of Japanese and one  of the founders of an organization favoring Romanized spelling for Japanese. With his  help, my fellow students and I stumbled through old texts and became acquainted with  the categories and terminology of the Japanese grammatical tradition.  One of the themes weaving through this essay is the reality that it is not possible to  represent—in a writing system, in a parse, or in a grammar—every aspect of a language  worth noticing. My study of Japanese confronted me with the realization that for any  given representation system, it’s important to understand what it represents, and what  is missing. The Japanese kana syllabary presented me with an early experience of this.  The pronunciation of Japanese words is represented by the symbols of a syllabary, but  unfortunately the components of complex words in this language, in particular the  inﬂected verbs, are not segmented at syllable boundaries.  
Although there have been great advances in the statistical modeling of hierarchical syntactic structure over the past 15 years, exact inference with such models remains very costly and most rich syntactic modeling approaches resort to heavy pruning, pipelining, ∗ Center for Spoken Language Understanding, Oregon Health & Science University, Beaverton, Oregon, 97006 USA. E-mails: roarkbr@gmail.com, bodenstab@gmail.com. ∗∗ Some of the work in this paper was done while Kristy Hollingshead was at OHSU. She is currently at the University of Maryland Institute for Advanced Computer Studies, College Park, Maryland, 20740 USA. E-mail: hollingk@gmail.com. Submission received: 9 August 2011; revised submission received: 30 November 2011; accepted for publication: 4 January 2012. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 4  or both. Pipeline systems make use of simpler models with more efﬁcient inference to reduce the search space of the full model. For example, the well-known Ratnaparkhi (1999) parser used a part-of-speech (POS)-tagger and a ﬁnite-state noun phrase (NP) chunker as initial stages of a multi-stage Maximum Entropy parser. The Charniak (2000) parser uses a simple probalistic context-free grammar (PCFG) to sparsely populate a chart for a richer model, and Charniak and Johnson (2005) added a discriminatively trained reranker to the end of that pipeline. Finite-state pre-processing for context-free parsing is very common as a means of reducing the amount of search required in the later stage. As mentioned earlier, the Ratnaparkhi pipeline used a ﬁnite-state POS-tagger and a ﬁnite-state NP-chunker to reduce the search space at the parsing stage, and achieved linear observed-time performance. Other recent examples of the utility of ﬁnite-state constraints for parsing pipelines include Glaysher and Moldovan (2006), Djordjevic, Curran, and Clark (2007), and Hollingshead and Roark (2007). Similar hard constraints have been applied for dependency parsing, as will be outlined in Section 2. Note that by making use of preprocessing constraints, such approaches are no longer performing full exact inference— these are approximate inference methods, as are the methods presented in this article. Using ﬁnite-state chunkers early in a syntactic parsing pipeline has shown both an efﬁciency (Glaysher and Moldovan 2006) and an accuracy (Hollingshead and Roark 2007) beneﬁt for parsing systems. Glaysher and Moldovan (2006) demonstrated an efﬁciency gain by explicitly disallowing constituents that cross chunk boundaries. Hollingshead and Roark (2007) demonstrated that high-precision constraints on early stages of the Charniak and Johnson (2005) pipeline (in the form of base phrase constraints derived either from a chunker or from later stages of an earlier iteration of the same pipeline) achieved signiﬁcant accuracy improvements, by moving the pipeline search away from unlikely areas of the search space. All of these approaches (as with Ratnaparkhi earlier) achieve improvements by ruling out parts of the search space, and the gain can either be realized in efﬁciency (same accuracy, less time) and/or accuracy (same time, greater accuracy). Rather than extracting constraints from taggers or chunkers built for different purposes, in this study we have trained prediction models to more directly reduce the number of entries stored in cells of a dynamic programming chart during parsing—even to the point of “closing” chart cells to all entries. We demonstrate results using three ﬁnite-state taggers that assign each word position in the sequence with a binary class label. The ﬁrst tagger decides if the word can begin a constituent of span greater than one word; the second tagger decides if the word can end a constituent of span greater than one word; and the third tagger decides if a chart cell spanning a single word should contain phrase-level non-terminals, or only POS tags. Following the prediction of each word, chart cells spanning multiple words can be completely closed as follows: Given a chart cell (b, e) spanning words wb . . . we where b < e, we can “close” cell (b, e) if the ﬁrst tagger decides that wb cannot be the ﬁrst word of a multi-word constituent (MWC) or if the second tagger decides that we cannot be the last word in a MWC. Completely closing sufﬁcient chart cells allows us to impose worst-case complexity bounds on the overall pipeline, a bound that none of the other previously mentioned methods for ﬁnite-state preprocessing can guarantee. To complement closing multi-word constituent chart cells, our third tagger restricts the population of span-1 chart cells. We note that all span-1 chart cells must contain at least one POS tag and can therefore never be closed completely. Instead, our tagger restricts unary productions with POS tags on their right-hand side that span a single word. We term these single word constituents (SWCs). Disallowing SWCs alters span-1 720  Roark, Hollingshead, and Bodenstab  Chart Constraints for Reduced Complexity Parsing  cell population from potentially containing all non-terminals to just POS non-terminals. In practice, this decreases the number of entries in span-1 chart cells by 70% during exhaustive parsing, signiﬁcantly reducing the number of allowable constituents in larger spans (Bodenstab, Hollingshead, and Roark 2011). Span-1 chart cells are also the most frequently queried cells in the Cocke-Younger-Kasami (CKY) algorithm. The search over possible midpoints will always include two cells spanning a single word—one as the ﬁrst left child and one as the last right child. It is therefore beneﬁcial to minimize the number of entries in these span-1 cells. The pre-processing framework we have outlined is straightforward to incorporate into most existing context-free constituent parsers, a task we have already done for several state-of-the art parsers. In the following sections we formally deﬁne our approach to ﬁnite-state chart constraints and analyze the accuracy of each of the three taggers and their impact on parsing efﬁciency and accuracy when used to prune the search space of a constituent parser. We apply our methods to exhaustive CYK parsing with simple grammars, as well as to high-accuracy parsing approaches such as the Charniak and Johnson (2005) parsing pipeline and the Berkeley parser (Petrov and Klein 2007a, 2007b). Various methods for applying ﬁnite-state chart constraints are investigated, including methods that guarantee quadratic or linear complexity of the context-free parser.  2. Related Work Hard constraints are ubiquitous within parsing pipelines. One of the most basic and standard techniques is the use of a POS-tag dictionary, whereby words are only allowed to be assigned one of a subset of the POS-tag vocabulary, often based on what has been observed in the training data. This will overly constrain polysemous word types that happen not to have been observed with one of their possible tags; yet the large efﬁciency gain of so restricting the tags is typically seen as outweighing the loss in coverage. POStag preprocessing has been used for both context-free constituent parsing (Ratnaparkhi 1999) and dependency parsing (McDonald et al. 2005). Richer tag sets can also be used to further constrain the parser, such as supertags (Bangalore and Joshi 1999), which contain information about how the word will syntactically integrate with other words in the sequence. Supertagging has been widely used to make parsing algorithms efﬁcient, particularly those making use of context-sensitive grammars (Clark and Curran 2004). By applying ﬁnite-state chart constraints to constituent parsing, the approaches pursued in this article constrain the possible shapes of unlabeled trees, eliminating from consideration trees with constituents over speciﬁc spans. There is thus some similarity with other tagging approaches (e.g., supertagging) that dictate how words combine with the rest of the sentence via speciﬁc syntactic structures. Supertagging is generally used to enumerate which sorts of structures are licensed, whereas the constraints in this article indicate unlabeled tree structures that are proscribed. Along the same lines, there is a very general similarity with coarse-to-ﬁne search methods, such as those used in the Berkeley (Petrov and Klein 2007a) and Charniak (2000) parsers, and more general structured prediction cascades (Weiss, Sapp, and Taskar 2010; Weiss and Taskar 2010). Our approach also uses simpler models that reduce the search space for larger downstream models. Dependency parsing involves constructing a graph of head/dependent relations, and many methods for constraining the space of possible dependency graphs have been  721  Computational Linguistics  Volume 38, Number 4  investigated, such as requiring that each word have a single head or that the graph be acyclic. Nivre (2006) investigated the impact of such constraints on coverage and the number of candidate edges in the search space. Most interestingly, that paper found that constraining the degree of non-projectivity that is allowed can greatly reduce the number of arcs that must be considered during search, and, as long as some degree of non-projectivity is allowed, coverage is minimally impacted. Of course, the total absence of projectivity constraints allows for the use of spanning tree algorithms that can be quadratic complexity for certain classes of statistical models (McDonald et al. 2005), so the ultimate utility of such constraints varies depending on the model being used. Other hard constraints have been applied to dependency parsing, including constraints on the maximum length of dependencies (Eisner and Smith 2005; Dreyer, Smith, and Smith 2006), which is known as vine parsing. Such vine parsers can be further constrained using taggers to determine the directionality and distance of each word’s head in a way similar to our use of taggers (Sogaard and Kuhn 2009). More general arc ﬁltering approaches, using a variety of features (including some inspired by previously published results of methods presented in this article) have been proposed to reduce the number of arcs considered for the dependency graph (Bergsma and Cherry 2010; Cherry and Bergsma 2011), resulting in large parsing speedups. In a context-free constituent parsing pipeline, constraints on the ﬁnal parse structure can be made in stages preceding the CYK algorithm. For example, base phrase chunking (Hollingshead and Roark 2007) involves identifying a span as a base phrase of some category, often NP. A base phrase constituent has no children other than pre-terminal POS-tags, which all have a single terminal child (i.e., there is no internal structure in the base phrase involving non-POS non-terminals). This has a number of implications for the context-free parser. First, there is no need to build internal structure within the identiﬁed base phrase constituent. Second, constituents that cross brackets with the base phrase cannot be part of the ﬁnal tree structure. This second constraint on possible trees can be thought of as a constraint on chart cells, as pointed out in Glaysher and Moldovan (2006): No multi-word constituent can begin at a word falling within a base-phrase chunk, other than the ﬁrst word of that chunk. Similarly, no multi-word constituent can end at a word falling within a base-phrase chunk, other than the last word of that chunk. These constraints rule out many possible structures that the full context-free parser would have otherwise considered. These begin and end constraints can be extracted from the output of the chunker, but the chunker is most often trained to optimize chunking accuracy, not parsing accuracy (or parsing precision). Further, these constraints can apply even for words that fall outside of typical chunks. For example, in English, verbs and prepositions tend to occur before their arguments, hence are often unlikely to end constituents, yet verbs and prepositions are rarely inside a typically deﬁned base phrase. Instead of imposing parsing constraints from NLP pre-processing steps such as chunking, we propose that building speciﬁc prediction models to constrain the search space within the CYK chart will more directly optimize efﬁciency within a parsing pipeline. In this article, we focus on linear complexity ﬁnite-state methods for deriving constraints on the chart. Recent work has also examined methods for constraining each of the O(N2) chart cell independently (Bodenstab et al. 2011), permitting a ﬁner-grained pruning (e.g., not just “open” or “closed” but an actual beam width prediction) and the use of features beyond the scope of our tagger. We discuss this and other extensions of the current methods in our concluding remarks. 722  Roark, Hollingshead, and Bodenstab  Chart Constraints for Reduced Complexity Parsing  3. Preliminaries 3.1 Dynamic Programming Chart Dynamic programming for context-free inference generally makes use of a chart structure, as shown in Figure 1c for the left-binarized gold parse tree in Figure 1b. Each cell in the chart represents a collection of possible constituents covering a substring, which is identiﬁed by the indices of the ﬁrst and last words of the substring. Let w1 . . . wn be a string of n words to be parsed. The cell identiﬁed with (b, e) will contain possible constituents spanning the substring wb . . . we, where the span of a constituent or a chart cell is deﬁned as the number of words it covers, or e − b + 1. As an example, in this  Figure 1 Gold parse tree (a), left-binarized representation of the same tree (b), and corresponding dynamic programming chart (c) for the sentence As usual, the real-estate market had overreacted. (sentence 1094 in WSJ Section 24). Each cell in the chart spans a unique substring of the input sentence. Non-terminals preceded with the symbol “@” are created through binarization (see Section 3.3). 723  Computational Linguistics  Volume 38, Number 4  article we will occasionally refer to span-1 chart cells, meaning all chart cells covering a single word. Context-free inference using dynamic programming over a chart structure builds longer-span constituents by combining smaller span constituents, guided by rules in a context-free grammar. A context-free grammar G = (V, T, S†, P) consists of: a set of nonterminal symbols V, including a special start symbol S†; a set of terminal symbols T; and a set of rule productions P of the form A → β for A ∈ V and β ∈ (V ∪ T)∗, i.e., a single non-terminal on the left-hand side of the rule production, and a sequence of 0 or more terminals or non-terminals on the right-hand side of the rule. If we have a rule production A → B C ∈ P, a completed B entry in chart cell (b, m) and a completed C entry in chart cell (m+1, e), we can place a completed A entry in chart cell (b, e). Such a chart cell entry is sometimes called an “edge” and can be represented by the tuple (A → B C, b, m, e). Context-free inference has cubic complexity in the length of the string N, due to the O(N2) number of chart cells and O(N) possible child conﬁgurations at each cell. As an example, a cell spanning (b, e) must consider all possible conﬁgurations of two child constituents (child cells) that span a proper preﬁx (b, m) and a proper sufﬁx (m+1, e) where b ≤ m < e, leading to O(N) possible midpoints. 3.2 The CYK Algorithm Algorithm 1 contains pseudocode for the CYK algorithm (Kasami 1965; Younger 1967; Cocke and Schwartz 1970), where the context-free grammar G is assumed to be binarized. The function ρ maps each grammar production in P to a probability. Lines 1–3  Algorithm 1 CYK Pseudocode of the CYK algorithm using a binarized PCFG. Unary processing is simpliﬁed to allow only chains of length one (excluding lexical unary productions). Backpointer storage is omitted.  Input: w1 . . . wn: Input sentence G: Left-binarized PCFG Output: α: Viterbi-max score for all non-terminals over every span  CYK(w1 . . . wn, G = (V, T, S†, P, ρ)) 1: for s = 1 to n do  Span width: bottom-up traversal  2: for b = 1 to n − s + 1 do  Begin word position  3: e ← b+s−1  4:  for Ai ∈ V do  5:  if s = 1 then  Add lexical productions  6:  αi(b, e) ← ρ(Ai → wb )  7:  else  8:  αi(b, e) ← max max ρ(Ai → Aj Ak ) αj(b, m) αk(m + 1, e)  b≤m<e j,k  9:  for Ai ∈ V do  Add unary productions  10:  υi(b, e) ← max αi(b, e) , max ρ(Ai → Aj) αj(b, e)  j  11: α(b, e) ← υ(b, e)  12: return α  724  Roark, Hollingshead, and Bodenstab  Chart Constraints for Reduced Complexity Parsing  iterate over all O(N2) chart cells in a bottom–up traversal. Line 6 initializes span-1 cells with all possible part-of-speech tags, and line 8 introduces the cubic complexity of the algorithm: maximization over all midpoints m, which is O(N). The variable α stores the Viterbi-max score for each non-terminal Ai ∈ V at each cell (b, e), representing the span’s most probable derivation rooted in Ai. Backpointers indicating which argument(s) maximize each αi(b, e) can be optionally recorded to efﬁciently extract the maximum likelihood solution at the end of inference, but we omit these for clarity; they are easily recoverable by storing the argmax for each max in lines 8 and 10. We have also included pseudocode in Algorithm 1 to process unary productions in lines 9 and 10. Unary processing is a necessary step to recover the gold-standard trees of the Penn treebanks (Marcus, Marcinkiewicz, and Santorini 1993; Xue et al. 2005), but is often ignored in the presentation of the CYK algorithm. Because there is little discussion about unary processing in the literature, implementation details often differ from parser to parser. In Algorithm 1 we present a simpliﬁed version of unary processing that only allows unary chains of length 1 per span (excluding lexical productions). This approach is efﬁcient and can be iterated as needed to represent the length of observed unary chain in the treebank. Note that line 10 uses the temporary variable υ to store the accumulated Viterbi-max scores α. This temporary variable is necessary due to the iterative nature in which we update α. If we were to write the result of line 10 directly to αi(b, e), then the subsequent maximization of αi+1(b, e) would use an unstable version of α, some of which would already be updated with unary productions, and some which would not. 3.3 Incomplete Edges: Chomsky Normal Form and Dotted-Rules A key feature to the efﬁciency of the CYK algorithm is that all productions in the grammar G are assumed to have no more than two right-hand-side children. Rather than trying to combine an arbitrary number of smaller substrings (child cells), the CYK algorithm exploits shared structure between rules and only needs to consider pairwise combination. To conform to this requirement, incomplete edges are needed to represent that further combination is required to achieve a complete edge. This can either be performed in advance, for example, by transforming a grammar into Chomsky Normal Form resulting in “incomplete” non-terminals created by the transform, or incomplete edges can be represented through so-called dotted rules, as with the Earley (1970) algorithm, in which transformation is essentially performed on the ﬂy. For example, if we have a rule production A → B C D ∈ P, a completed B entry in chart cell (b, m1) and a completed C entry in chart cell (m1+1, m2), then we can place an incomplete edge A → B C · D in chart cell (b, m2). The dot signiﬁes the division between what has already been combined (left of the dot), and what remains to be combined. Then, if we have an incomplete edge A → B C · D in chart cell (b, m2) and a complete D in cell (m2+1, e), we can place a completed A entry in chart cell (b, e). Transforming a grammar into Chomsky Normal Form (CNF) is an off-line operation that converts rules with more than two children on the right-hand side into multiple binary rules. To do this, composite non-terminals are created during the transformation, which represent incomplete constituents (i.e., those edges that require further combination to be made complete).1 For example, if we have a rule production A → B C D in  
∗ 151 Engineer’s Way, University of Virginia, Charlottesville, VA 22904. E-mail: matt.gerber@virginia.edu. ∗∗ 3115 Engineering Building, Michigan State University, East Lansing, MI 48824. E-mail: jchai@cse.msu.edu. Submission received: 4 August 2011; revised version received: 23 December 2011; accepted for publication: 7 February 2012. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 4  and verbal SRL has stopped short of answering this question, opting instead for an approach that only labels local arguments and thus ignores predicates whose arguments are entirely non-local. This article directly addresses the issue of non-local (or implicit) argument identiﬁcation for nominal predicates. As an initial example, consider the following sentence, which is taken from the Penn TreeBank (Marcus, Santorini, and Marcinkiewicz 1993): (1) A SEC proposal to ease [arg1 reporting] [predicate requirements] [arg2 for some company executives] would undermine the usefulness of information on insider trades, professional money managers contend. The NomBank (Meyers 2007) role set for requirement is shown here: Frame for requirement, role set 1: arg0: the entity that is requiring something arg1: the entity that is required arg2: the entity of which something is being required In Example (1), the predicate has been annotated with the local argument labels provided by NomBank. As shown, NomBank does not annotate an arg0 for this instance of the requirement predicate; a reasonable interpretation of the sentence, however, is that SEC is the entity that is requiring something.1 This article refers to arguments such as SEC in Example (1) as implicit. In this work, the notion of implicit argument covers any argument that is not annotated by NomBank.2 Building on Example (1), consider the following sentence, which directly follows Example (1) in the corresponding TreeBank document: (2) Money managers make the argument in letters to the agency about [arg1 rule] [predicate changes] proposed this past summer. The NomBank role set for change is as follows: Frame for change, role set 1: arg0: the entity that initiates the change arg1: the entity that is changed arg2: the initial state of the changed entity arg3: the ﬁnal state of the changed entity Similarly to the previous example, Example (2) shows the local argument labels provided by NomBank. These labels only indicate that rules have been changed. For a full interpretation, Example (2) requires an understanding of Example (1). Without 
Statistical machine translation (MT) uses large target language models (LMs) to improve the ﬂuency of generated texts, and it is commonly assumed that for constructing language models, “more data is better data” (Brants and Xu 2009). Not all data, however, are created the same. In this work we explore the differences between language models compiled from texts originally written in the target language (O) and language models compiled from translated texts (T). This work is motivated by much research in Translation Studies that suggests that original texts are signiﬁcantly different from translated ones in various aspects (Gellerstam 1986). Recently, corpus-based computational analysis corroborated this observation, and Kurokawa, Goutte, and Isabelle (2009) apply it to statistical machine translation, showing that for an English-to-French MT system, a translation model trained on an English-translated-to-French parallel corpus is better than one trained on French-translated-to-English texts. The main research question we investigate here is whether a language model compiled from translated texts may similarly improve the results of machine translation. We test this hypothesis on several translation tasks, including translation from several languages to English, and two additional tasks where the target language is ∗ Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mails: glembers@campus.haifa.ac.il, noam.ordan@gmail.com, shuly@cs.haifa.ac.il. Submission received: 22 August 2011; revised submission received: 25 December 2011; accepted for publication: 31 January 2012 © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 4  not English. For each language pair we build two language models from two types of corpora: texts originally written in the target language, and human translations from the source language into the target language. We show that for each language pair, the latter language model better ﬁts a set of reference translations in terms of perplexity. We also demonstrate that the differences between the two LMs are not biased by content, but rather reﬂect differences on abstract linguistic features. Research in Translation Studies holds a dual view on translationese, the sublanguage of translated texts. On the one hand, there is a claim for so-called translation universals, traits of translationese which occur in any translated text irrespective of the source language. Others hold, on the other hand, that each source language “spills over” to the target text, and therefore creates a sub-translationese, the result of a pair-speciﬁc encounter between two speciﬁc languages. If both these claims are true then language models based on translations from the source language should best ﬁt target language reference sentences, and language models based on translations from other source languages should ﬁt reference sentences to a lesser extent yet outperform originally written texts. To test this hypothesis, we compile additional English LMs, this time using texts translated to English from languages other than the source. Again, we use perplexity to assess the ﬁt of these LMs to reference sets of translated-to-English sentences. We show that these LMs depend on the source language and differ from each other. Whereas they outperform O-based LMs, LMs compiled from texts that were translated from the source language still ﬁt the reference set best. Finally, we train phrase-based MT systems (Koehn, Och, and Marcu 2003) for each language pair. We use four types of LMs: original; translated from the source language; translated from other languages; and a mixture of translations from several languages. We show that the translated-from-source-language LMs provide a signiﬁcant improvement in the quality of the translation output over all other LMs, and that the mixture LMs always outperform the original LMs. This improvement persists even when the original LMs are up to ten times larger than the translated ones. In other words, one has to collect ten times more original material in order to reach the same quality as is provided with translated material. It is important to emphasize that translated texts abound: in fact, Pym and Chrupała (2005) show (quantitatively!) that the rate of translations into a language is inversely proportional to the number of books published in that language: So whereas in English only around 2% of texts published are translations, in languages such as Albanian, Arabic, Danish, Finnish, or Hebrew translated texts constitute between 20% and 25% of the total publications. Furthermore, such data can be automatically identiﬁed (see Section 2). The practical impact of our work on MT is therefore potentially dramatic. The main contributions of this work are thus a computational corroboration of the following hypotheses: 1. Original and translated texts exhibit signiﬁcant, measurable differences. 2. LMs compiled from translated texts better ﬁt translated references than LMs compiled from original texts of the same (and much larger) size (and, to a lesser extent, LMs compiled from texts translated from languages other than the source language). 3. MT systems that use LMs based on manually translated texts signiﬁcantly outperform LMs based on originally written texts.  800  Lembersky, Ordan, and Wintner  Language Models for Machine Translation  This article1 is organized as follows: Section 2 provides background and describes related work. We explain our experimental set-up, research methodology and resources in Section 3 and detail our experiments and results in Section 4. Section 5 discusses the results and their implications, and suggests directions for future research. 2. Background and Related Work Numerous studies suggest that translated texts are different from original ones. Gellerstam (1986) compares texts written originally in Swedish and texts translated from English into Swedish. He notes that the differences between them do not indicate poor translation but rather a statistical phenomenon, which he terms translationese. He focuses mainly on lexical differences, for example, less colloquialism in the translations, or foreign words used in the translations “with new shades of meaning taken from the English lexeme” (page 91). Only later studies consider grammatical differences (see, e.g., Santos 1995). The features of translationese were theoretically organized under the terms laws of translation and translation universals. Toury (1980, 1995) distinguishes between two laws: the law of interference and the law of growing standardization. The law of interference pertains to the ﬁngerprints of the source text that are left in the translation product. The law of standardization pertains to the effort to standardize the translation product according to existing norms in the target language (and culture). Interestingly, these two laws are in fact reﬂected in the architecture of statistical machine translation: Interference in the translation model and standardization in the language model. The combined effect of these laws creates a hybrid text that partly corresponds to the source text and partly to texts written originally in the target language, but in fact belongs to neither (Frawley 1984). Baker (1993, 1995, 1996) suggests several candidates for translation universals, which are claimed to appear in any translated text, regardless of the source language. These include simpliﬁcation, the tendency of translated texts to simplify the language, the message or both; and explicitation, their tendency to spell out implicit utterances that occur in the source text. During the 1990s, corpora were used extensively to study translationese. For example, Al-Shabab (1996) shows that translated texts exhibit lower lexical variety (type-totoken ratio) and Laviosa (1998) shows that their mean sentence length is lower, as is their lexical density (ratio of content to non-content words). These studies, although not conclusive, provide some evidence for the simpliﬁcation hypothesis. Baroni and Bernardini (2006) use machine learning techniques to distinguish between original and translated Italian texts, reporting 86.7% accuracy. They manage to abstract from content and perform the task using only morpho-syntactic cues. Ilisei et al. (2010) perform the same task for Spanish but enhance it theoretically in order to check the simpliﬁcation hypothesis. They ﬁrst use a set of features which seem to capture “general” characteristics of the text (ratio of grammatical words to content words); they then add another set of features, each of which relates to the simpliﬁcation hypothesis. Finally, they remove each “simpliﬁcation feature” in turn and evaluate its contribution to the classiﬁcation task. The most informative features are lexical variety, sentence length, and lexical density. 
∗∗ Nuance Communications, Inc., 1198 East Arques Avenue, Sunnyvale, CA 94085, USA. E-mail: Ronald.Kaplan@nuance.com. Submission received: 24 March 2011; revised submission received: 31 October 2011; accepted for publication: 28 December 2011. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 4  is typically identiﬁed with the complexity of the ﬁrst phase (e.g., the cubic bound for context-free parsing), because the collection of all parses can be delivered to a client application merely by presenting the compact representation. A client may be able to select a limited number of particularly desirable parses, perhaps the smallest or the most probable, without doing a full enumeration (Johnson and Riezler 2002; Kaplan et al. 2004). Lang (1994) gives a clear formal characterization of the ﬁrst phase of contextfree chart parsing.1 He observes that the recognition problem consists of ﬁnding the intersection of the language of the grammar with the input string, and then testing to see whether that intersection is empty. Many language classes are closed under intersection with a regular set, and the result of the intersection of a language L(G) with a regular language α is describable as a specialization Gα of G that assigns to all and only the strings in α effectively the same parse trees as G would assign. Lang argues that a chart for an input string s (a trivial regular language) and a context-free grammar G can be regarded as a specialization Gs of G that derives either the empty language (if s does not belong to L(G)) or a language consisting of just that input. In this view a parsing chart/grammar is a representation that makes it possible to enumerate all the derivation trees of the string, guaranteeing that each tree can be produced in a backtrack-free way in time proportional to its size. This guarantee holds even for an inﬁnitely ambiguous string: It would take forever to enumerate all valid derivations, but any particular one can be read out in linear time. The procedure for tree enumeration follows directly from the standard context-free generation algorithm applied to the grammar Gs. The generation problem for LFG and other description-based grammatical formalisms can also be viewed from this perspective. Several algorithms have been proposed for generation that avoid redundant recomputation by storing intermediate processing results in a chart-like auxiliary data structure (e.g., Shieber 1988; Kay 1996; Shemtov 1997; Neumann 1998; Carroll et al. 1999; Moore 2002; Carroll and Oepen 2005; Cahill and van Genabith 2006; White 2006; de Kok and van Noord 2010). Most of them can be construed as having a ﬁrst phase that provides a compact representation for alternative results, in this case for the strings that the grammar provides for a given functional or semantic input. The individual generated strings are then produced by an enumeration procedure operating on this compact representation. In this article we observe that the edges of a generation chart can be interpreted as rules of a specialized context-free grammar, just as in Lang’s (1994) characterization of parsing. We present a generation algorithm that specializes the context-free backbone of a given LFG grammar to a grammar that describes exactly the strings that the LFG grammar relates to a given acyclic f-structure. Derivations of the resulting grammar simulate all and only those derivations of the LFG grammar whose derived strings are assigned to that input.2 Thus the generated string set is a context-free language compactly represented by the specialized grammar, and the individual members of that language can be enumerated, just as for parsing, by using standard context-free generation algorithms. Our approach can be seen as a generalization and formalization of other chart-based generation algorithms, producing all and only correct outputs for larger combinations of grammars and inputs. It extends to uniﬁcation grammars with explicit context-free backbones, such as PATR (Shieber et al. 1983), and also to formalisms that permit a context-free skeleton to be extracted from richer speciﬁcations. But it does not extend 
© 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 4  This ﬁrst half of Chapter 2 focuses on the genre-based structure of scientiﬁc texts and of ﬁlm reviews. Here researchers have already shown that language technologies such as information extraction and sentiment analysis beneﬁt from taking such structure into account, so this is entirely appropriate for the book’s target audience. More on genre-based functional structure and its use in producing structured biomedical abstracts can be found in the recent survey of research on discourse structure and language technology by Webber, Egg, and Kordoni (2012). The second half of Chapter 2 discusses large-scale discourse structure associated with patterns of topics. Such structure is often found in expository writing such as encyclopedia articles and travel pieces. Here, changing patterns of content words correlate well with changes in topic, rendering them useful for the many approaches to text segmentation that are well-described in this half of the chapter. Because the discussion here of probabilistic models for topic segmentation is rather short, the reader who wants to know more should consult the excellent survey of topic segmentation methods by Purver (2011). Chapter 3 Chapter 3, entitled Coreference Resolution, addresses more than this, dealing with the resolution of other expressions whose reduction is licensed by the discourse context, such as bridging reference and “other” reference, which Halliday and Hasan (1976) call comparative reference because it occurs with comparative forms such as “larger ﬁsh” and “a more impressive poodle,” as well as with “other,” “another,” and “such.” Stede justiﬁes inclusion of this chapter for two reasons—the close connection between coreference resolution and topic segmentation and the beneﬁts to text analysis provided by having its pronouns resolved. But another reason must be the link mentioned earlier between text and context: Discourse creates the context in which context-reduced expressions make sense, so it falls naturally within the tasks of discourse processing to resolve them, either through modeling context explicitly or through the use of proxies. The chapter starts with an overview of coreference and anaphora that covers both their forms and their functions. This is followed by an important section on corpus annotation (Section 3.2), included because (as Stede notes) what has been annotated and why it has been annotated strongly determines what expressions are resolved and how. This section identiﬁes many of the problems in coreference annotation that have been raised in the literature, but recognizes that research has to make use of the resources that exist and not just the resources it wants. Several of these are indicated at the end of the section, reminding one that it would have been useful to have some pointers in Chapter 2 to corpora available for genre-based segmentation (such as Liakata’s ART corpus)1 or for topic-based segmentation. Stede then links the current chapter to the previous one through a discussion of entity-based coherence (Section 3.3) and then discusses how to identify when a pronoun or deﬁnite noun phrase should be treated as anaphoric (Section 3.4) as groundwork for discussion of anaphora resolution (Sections 3.5–3.7). Missing from the discussion of detecting non-anaphoric (pleonastic) pronouns is mention of Bergsma’s recent system NADA for doing this (Bergsma and Yarowsky 2011).2  
 discourse.cpp is a short collection of computer-generated poetry edited by computational linguist Aure´lie Herbelot. The poetry was produced by a program, named O.S. le Si, that was derived from Herbelot’s research on context-based ontology extraction from text (Herbelot and Copestake 2006; Herbelot 2009). In this case, Herbelot provided 200,000 pages from Wikipedia for the program to parse and output lists of items whose context is similar to words such as gender, love, family, and illness. For example, Herbelot explains that content in the opening piece titled “The Creation” was “selected out of a list of 10,000 entries. Each entry was produced by automatically looking for taxonomic relationships in Wikipedia”; and, for the piece titled “Gender,” she chose the “twentyﬁve best contexts for man and woman in original order. No further changes” (page 47). The collection is, then, as we are told on the back cover, “about things that people say about things. It was written by a computer.” Poets—or, for the sake of those still attached to the notion of an author who intentionally delivers well-crafted, expressive writing, “so-called poets”—have been experimenting with producing writing with the aid of digital computer algorithms since Max Bense and Theo Lutz ﬁrst experimented with computer-generated writing in 1959 (Funkhouser 2007). The best-known English-language example is the 1984 collection of poems The Policeman’s Beard is Half-Constructed by the artiﬁcial intelligence program Racter (a collection which, it was later discovered, was heavily edited by Racter creators William Chamberlain and Thomas Etter). discourse.cpp is yet another experiment in testing the capabilities of the computer and computer programmer to create not so much carefully and intentionally crafted, rhythmically or musically pleasing verse as broadly revealing poetry—poetry that is not meant to be close-read (most often to discover underlying authorial intent) but rather read as a collection of a kind of linguistic evidence. In this case, the collection provides evidence of trends in on-line human language usage which in turn, not surprisingly, provides evidence of certain prevailing cultural norms; for example, we can see quite clearly English-speaking, western culture’s continued attachment to heteronormative gender roles in “Gender” (page 18):  Woman man love — — marry man — give birth  Man — win title — love woman — claim be  One of the questions this collection raises is this, however: If the craft of poetry writing becomes more about programming and editing, and if the reader or critic does  © 2012 Association for Computational Linguistics  Computational Linguistics 
© 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 3  It was obvious that the output was poor, but nevertheless it appeared to be good enough for scientists with some knowledge of German grammar to read and extract relevant information. Yngve concluded that word-for-word translation could be taken as a ﬁrst approximation. But a major problem was that many words have more than one meaning out of context. Erwin Reiﬂer had suggested the use of pre-editors who would annotate texts before translation; and Victor Oswald proposed the use of microglossaries (dictionaries limited to one speciﬁc narrow ﬁeld) in order to reduce the number of homographs. But Yngve believed that the problem of multiple meanings could be resolved with syntactic analysis. Nonetheless, he rejected Bar-Hillel’s “operational syntax” put forward at the 1952 conference (and later known as categorical syntax) as too complex for long sentences and too remote from traditional grammatical distinctions. He had been impressed by the work of Bloomﬁeld and Fries, and had become convinced that linguistics could provide stable and repeatable methods akin to the procedures of cosmic ray physics. Therefore, over the following years, he appointed a number of linguists to the RLE staff, starting with Noam Chomsky in 1955, in order to undertake basic research. He was disappointed, however, that most of the linguists he hired were more interested in pursuing their theoretical studies than in tackling the practical problems of MT. Yngve’s approach to syntactic analysis was to begin with the identiﬁcation and isolation of the different possible grammatical functions of words. The aim was to set up mutually exclusive word-classes (i.e., one class for the noun function, another for the verb function, and so on) The approach was to set up substitution frames to isolate contexts. Thus walk may occur in the frame (1) The was enjoyable or in the frame (2) They home every day. Taking words from a corpus and testing in each frame produces a matrix of different contexts (substitution frames) and the words occurring in those contexts. These words form a word-class and their frames form context classes. As a result, each sentence sequence of word-classes would determine a unique sequence of context classes. An algorithm was proposed which searched left-to-right for the longest match for a sequence of word-classes. Sequences of word-classes formed phrases and clauses, so the algorithm was capable of looking also for phrase sequences. On this basis, a table-driven algorithm for syntactic analysis was designed and implemented (Yngve 1955b). This demonstrated for the ﬁrst time in MT the importance and practical value of separating descriptive linguistic information (in this case, words and contexts) and languageindependent algorithmic procedures (searching and matching). The practice was widely adopted by other groups in MT and in computational linguistics. At the same time, work was going on at the RLE on investigations of coding and switching theory and on Shannon’s information theory. Yngve decided to investigate error correcting properties of language (Yngve 1954). Sequences of letters and words are not random but constrained by codes specifying which sequences are legitimate. Testing a text for deviations from randomness would help to reveal its structure. A ﬁrst step would be the location of all occurrences of a given frequent word and the determination of its effect on the occurrence of other frequent words in the neighborhood by comparing their frequency of occurrence with what would be expected if they occurred at random. For this investigation of what was to be called “gap analysis” (Yngve 1956) 462  Hutchins  Obituary  a computational experiment was conducted on a corpus of just 9,490 words. First, the  most frequent words were identiﬁed: the (599 instances), to (252), of (241), a (221), and  (207), and in (162). Then, the gaps between these words were determined (in numbers  of intervening words). In the case of of and the the gaps were one (the of ), two  (the  of ), three (the  of ), and so on, with frequencies of 72, 31, and  6, respectively. These results pointed to syntactic constraints on constructions with of  and the. Further results indicated that “structures with the can be expected to have two  or three words, and constructions with and frequently involve at least ﬁfteen words.”  Similar observations were that “of is different from to in that it frequently follows a or the  with a gap of one or two.” And so forth, in a procedure which has now become familiar  in statistics-based computational linguistics. Yngve was a pioneer. Unfortunately, at the  time, these encouraging results could not be pursued because of the lack of large enough  corpora in machine-readable form.  The parallels between coding theory and linguistics suggested a two-stage model  for machine translation, where speakers encode a message that is decoded by a recipient  (Yngve 1955a). Between encoding and decoding there would be some representation  of the ‘message’ to be preserved in translation. Further consideration recognized that  structural representations of input would not be the same as those required for out-  put, however. Attention, therefore, focused on the transition stage where the meaning  or message of the input language would be expressed and where choices would be  made for producing output sentence structure. Thus the MIT group saw the need for  a three-stage model: syntactic analysis, structure transfer, and synthesis—a model for  many transfer-based MT systems in subsequent years. Yngve’s model was, however,  purely syntactic; no attempt was made at this time to include semantic information  (Yngve 1957).  At the same time as the development of the transfer model, Yngve and his col-  leagues tackled the problem of how linguists could be productive in MT research. They  decided that what was needed was a programming system that accepted notations  of a kind familiar to linguists, rather than systems such as FORTRAN and COBOL  designed for mathematicians and business applications. Thus, in collaboration with the  MIT Computation Center, they developed the ﬁrst programming language designed  speciﬁcally for string-handling and pattern-matching, COMIT (Yngve 1961a, 1967). It  was ready for testing in late 1957—thus antedating by two years LISP, another program-  ming language devised for linguistics applications. COMIT was later used as a basis for  the SNOBOL language.  The availability of COMIT meant that the MIT group could proceed further with the  three-stage model in the development of an algorithm for sentence production. Initially  the generative grammar of Chomsky seemed to be an ideal model. What was required in  MT, however, was not the generation of all grammatical sentences from a given source  but particular speciﬁable sentences in context. The rules of a grammar derived from  a simple children’s story were written in COMIT in 1959 and tested in a program of  random sentence production (Yngve 1961b). Its main objective, in which it succeeded,  was to test the validity of the grammar rules, particularly rules for discontinuous  constructions and for coordination.  One outcome of programming the sentence-production algorithm with COMIT was  the “depth hypothesis,” for which Yngve is probably now best known (Yngve 1960a)  both in linguistics and in computational linguistics. The transformational approach had  already been rejected because it required too much storage space. The next question was  how much storage (push-down store) was needed for discontinuous (left-branching)  expansion and for regular (right-branching) expansion. It was clear that right expansion  463  Computational Linguistics  Volume 38, Number 3  (or “progressive” application) was potentially inﬁnite: the dog that worried the cat that killed the rat that ate the malt . . . . On the other hand, left expansion (or ‘regressive’ application) was limited: the malt that the rat that the cat that the dog worried killed ate. Yngve calculated the maximum amount of extra temporary memory required to produce a sentence (i.e., the number of unexpanded constituents at any one time [its depth]). He found that in practice even very long sentences seldom had a depth of more than two or three. Sentences with depths of more than three, such as the “regressive” constructions, were often considered ungrammatical and/or unintelligible. Yngve noted the relationship of this linguistic feature to the restrictions on immediate memory and processing identiﬁed by Miller (1956). Most languages include mechanisms for restricting the depth of regressive constructions, such as agglutination and compounding. The depth hypothesis accounted for and predicted many syntactic features of English, including its historical changes, and also appeared to account for many features in other languages. Yngve recognized that it arose from MT research and not from linguistic theory, however. It was a hypothesis that needed to be tested against empirical evidence. Its signiﬁcance for linguistics was widely recognized from the beginning, but it did not conform to the preconceptions of Chomskyan theory. Although Chomsky had championed the rigorous statement of theory and its strict application to linguistic material without ad hoc adjustments (Chomsky 1957, page 5), he regarded the depth hypothesis not as a testable scientiﬁc hypothesis but as a rival linguistic theory. For Yngve this attitude was unscientiﬁc. Throughout his time at MIT Yngve stressed the need for a “basic, long-range approach to translation” and not to look for “short-cut methods that might yield partially adequate translations at an early date” (Yngve 1960b, page 183). No working MT system emerged from MIT, therefore, but the quality of the research is incontestable. Apart from Yngve’s own contributions to many aspects of syntactic analysis (the threestage transfer model, the depth hypothesis, and not least computer programming), his colleagues also made signiﬁcant contributions in a variety of areas: grammar theory (Gilbert Harman and Kenneth Knowlton), semantics (Elinor Charney), logic and language (Jared Darlington), transfer and interlingua (Edward Klima), computer storage (William S. Cooper), Arabic translation (Arnold Satterthwait), and French translation (David Dinneen). Citations for these contributions will be found in Yngve’s comprehensive survey of MT research at MIT (Yngve 1967; see also Yngve 2000). By 1964, Yngve had come to the conclusion that “work in mechanical translation has come up against what we will call the semantic barrier . . . we will only have adequate mechanical translations when the machine can ‘understand’ what it is translating and this will be a very difﬁcult task indeed” (Yngve 1964, page 279). Understanding involved the background knowledge that people bring to the comprehension of communication. He could see no solutions coming from linguistics. For many years, Yngve had grown increasingly doubtful about the health of linguistic science and consequently about the feasibility of good quality MT in general. By 1965, funding for the MIT research group had ceased—perhaps in anticipation of the ALPAC report which had a major impact on the funding of all US research groups—and in that year Yngve went back to the University of Chicago as head of the Department of Library Sciences, Linguistics, and Behavioral Sciences. By this time, the journal Mechanical Translation, which he had founded in 1954, was coming to an end. The aim had been to provide a public record of achievement in MT. His ambitions for the journal could not be fulﬁlled, however, for various reasons. There were relatively few outstanding contributions submitted for publication; many MT researchers were funded by government bodies, which required the submission of 464  Hutchins  Obituary  fairly frequent reports, and these reports were distributed widely to other researchers and public institutions. Researchers believed they had fulﬁlled their duties to publicize research, and did not see the need to submit shorter articles to a peer-reviewed journal which would probably not appear for several months. In addition the journal could not survive solely on subscription charges, and authors were asked to contribute a page charge towards publication costs, which they were reluctant to do. In June 1962, the Association for Machine Translation and Computational Linguistics was founded, with Yngve as its ﬁrst president. The inclusion of ‘computational linguistics’ in the title was indicative of the ever-expanding range of activities in the ﬁeld of natural language processing; MT was only part, and a diminishing proportion. The Association took over Yngve’s journal with a changed title, Mechanical Translation and Computational Linguistics, and Yngve remaining as editor. Even with the inclusion of many more articles in computational linguistics and signiﬁcantly fewer in MT, however, the journal became ever more irregular and it was wound up in 1970. In 1968 machine translation had already been dropped from the Association’s title.1 From this time on, Yngve turned away from his MT interests to questions of linguistic theory that had been his increasing concern since the publication of his “depth hypothesis.” From 1965, Yngve published a series of papers devoted to the foundations of linguistic theory, many at the conferences of LACUS (Linguistic Association of Canada and the United States). Just as when he had founded the Mechanical Translation journal in 1954, he was determined to put studies of language on a sound scientiﬁc footing. His recurrent theme was the unscientiﬁc nature of current linguistics. In this period, he set forth the framework of what he called “human linguistics” (later “hardscience linguistics”) where the units being analyzed are not the traditional properties of sentences, verbs, noun phrases, gender, tense, phonemes, and so on, which, as he demonstrated, are unfounded assumptions derived often from Greek philosophical speculations. Instead the basic participants are communicating individuals and physical observable “props” (abstractions of relevant physical objects, clocks, doors), “channels” of communication (sound waves, writing, signs), and “settings” (relevant aspects of the physical environment, ticket counters, rooms). Yngve’s ﬁrst attempt at summarizing and formulating his theory was published in his book, Linguistics as Science (Yngve 1986). Expansion and elaboration of his theoretical standpoint followed in further papers and were brought together in From Grammar to Science in 1996. The opening chapters are a cumulative rejection of all traditional and contemporary linguistics and philosophy of language (from the Greeks to Saussure, Bloomﬁeld, Fries, Harris, Chomsky, and many others), including a rejection of his own widely accepted “depth hypothesis.” The basic contention is that fundamental concepts of linguistics are intuitions and not based on the observable behavior of people communicating. Yngve describes a detailed comprehensive program for a new foundation of linguistics in which “we abandon the unsupported assumptions of signs, words, meanings, and sentences” and move completely into “the world of sound waves and the people who speak and understand where we can test our theories and hypotheses against observations of the physical reality all the way down to the basic assumptions of all science” (Yngve 1996, page 308). He had not lost sight of computational treatments, and included (chapters 19 and 20) an implementable notation for representing and testing hypotheses.  
University of Rochester Michael K. Tanenhaus§ University of Rochester We describe a novel domain, Fruit Carts, aimed at eliciting human language production for the twin purposes of (a) dialogue system research and development and (b) psycholinguistic research. Fruit Carts contains ﬁve tasks: choosing a cart, placing it on a map, painting the cart, rotating the cart, and ﬁlling the cart with fruit. Fruit Carts has been used for research in psycholinguistics and in dialogue systems. Based on these experiences, we discuss how well the Fruit Carts domain meets four desired features: unscripted, context-constrained, controllable difﬁculty, and separability into semi-independent subdialogues. We describe the domain in sufﬁcient detail to allow others to replicate it; researchers interested in using the corpora themselves are encouraged to contact the authors directly. 1. Introduction and Relation to Prior Work Dialogue system research, like much else in computational linguistics, has greatly beneﬁted from corpora of natural speech. With notable exceptions (e.g. the Edinburgh Maptask, Anderson et al. [1991]), these corpora consist of samples annotated with linguistic properties (e.g. POS, syntax, discourse status) setting aside the visual and ∗ 206 Ross Hall, Iowa State University, Ames, Iowa 50011. E-mail: gregory.aist@alumni.cmu.edu. ∗∗ 240c Matthews Center, Arizona State University, Tempe, Arizona 85287. E-mail: ellen.campana@asu.edu. † 721 CSB, University of Rochester, Rochester, New York 14627. Email: james@cs.rochester.edu. ‡ 732 CSB, University of Rochester, Rochester, New York 14627. E-mail: swift@cs.rochester.edu. § 420 Meliora, University of Rochester, Rochester, New York 14627. E-mail: mtan@bcs.rochester.edu. Submission received: 24 August 2009; revised submission received: 6 May 2010; accepted for publication: 20 September 2010. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 3  pragmatic aspects of the context in which they occurred. In recent years natural language processing (NLP) researchers have been working to incorporate visual and other context into their models and systems (DeVault and Stone 2004; Gabsdil and Lemon 2004; Schuler, Wu, and Schwartz 2009). This is consistent with the growing evidence in psycholinguistics that human language production crucially depends on such aspects of context. To take this NLP research further, there is a need for more corpora that include both variation in, and annotation of, visual and pragmatic context. There are still many open questions that span computational linguistics and psycholinguistics concerning how natural language and context are related. One core question at the intersection of these areas is how the inherent difﬁculty of describing an end-goal (i.e., its codability) will affect the structure and content of referring expressions and the referential strategy speakers adopt. Referential strategies are a topic of growing interest in natural language generation. In recent work, Viethen and Dale (2006) demonstrated that even when describing simple grid layouts, people adopt different referential strategies, due perhaps to proximity to landmarks (and hence codability): the orange drawer below the two yellow drawers, in contrast to the yellow drawer in the third column from the left second from the top. For systems to produce humanlike references in these situations, existing methods of reference generation will need to be modiﬁed or extended to include better models of the choice of referential strategies (Viethen and Dale 2006). Such models can also be expected to improve reference resolution: If better predictions can be made about what people will say in a given situation, automatic speech recognition language models can be tighter, NLP grammars can be smaller, and unlikely parses can be avoided, improving both speed and accuracy. Recent psycholinguistic research suggests that codability does play a role in human reference production (e.g., Cook, Jaeger, and Tanenhaus 2009). This work has largely focused on timing, signals of production difﬁculty (e.g., disﬂuency, gesture), and the content of referring expressions (e.g., adjectives, pronouns). There has been much less consideration of how entire referential strategies might systematically vary with codability. A corpus with the correct design and structure will allow for investigation of the more well-studied aspects as well as higher-level factors such as strategy choice, and possible interactions between them. With these considerations in mind, we designed a domain, Fruit Carts, and a set of corresponding tasks in order to elicit human language production for two purposes: 1) the testing of psycholinguistic hypotheses, speciﬁcally that object complexity modulates referential strategy, and more generally the exploration of the relationship between visual context and human–human dialogue, and 2) research and development of dialogue systems that understand language as it unfolds, taking pragmatic factors into account early in the recognition process. By designing with both ﬁelds in mind we hope to strengthen the long tradition of cross-fertilization between the disciplines (e.g., Brennan 1991), particularly for task- or game-oriented systems and domains, with a visual component. We identiﬁed four important features to build into the domain. First, the language produced should be completely unscripted: Participants should be able to perform the task with a general description of what to do (e.g., Give instructions on how to make the map on the screen look like the map in your hand) and zero prior examples of what to say. For psycholinguistics, this makes the language natural speech rather than speech that is restricted by the instructions or by prior examples. For dialogue systems, this makes the language “untrained” rather than the result of careful training, meaning that systems will be processing language that is representative of what speakers are likely to produce when they use the system, especially without extensive training. Second,  470  Aist et al.  Fruit Carts Domain and Corpus  the language should be fairly well constrained by context. For psycholinguistics, this makes the language more straightforward to analyze and also more directly tied to the visual context and thus amenable to “visual world” studies that use eye movements to examine real-time production (Grifﬁn and Bock 2000) and comprehension (Tanenhaus et al. 1995). For dialogue systems, this makes the language more amenable to automatic processing and also facilitates the integration of different types of knowledge into the recognition process. Third, it should be possible to vary the difﬁculty of the tasks. For psycholinguistics, this makes hypotheses about the effect of task difﬁculty on language production amenable to study. For dialogue systems, this allows the resulting corpora to have a combination of relatively easy tasks (“low-hanging fruit”) and more difﬁcult NLP challenges. Fourth, the domain should support the collection of dialogues that are separable into partially or semi-independent subdialogues, with limited need for reference to previous subdialogues. For psycholinguistics, this makes each subdialogue a separate trial, allowing for analyses where trials are treated as random effects in mixedeffect regression models or repeated measures in ANOVAs. For dialogue systems, this limits the likelihood that errors in processing one subdialogue will spill over and affect processing of subsequent subdialogues. For both research areas, this separability constraint enables within-subject experiments with each subdialogue as a trial. In purpose and approach, Fruit Carts is most similar to the Map Task (Anderson et al. 1991); both are simultaneously a set of experiments on language and a corpus used for developing language processing systems. Map Task dialogues “are unscripted [but] the corpus as a whole comprises a large, carefully controlled elicitation exercise” (Anderson et al. 1991, page 352) that has been used in many computational endeavors as well. Fruit Carts was guided by our twin goals of furthering the development of spoken language systems, and providing a psycholinguistic test bed in which to test speciﬁc hypotheses about human language production. Fruit Carts differs from Map Task in terms of dynamic object properties and in terms of the information available to the speaker and hearer. In the Map Task, objects have ﬁxed properties that differ between giver and follower, yet remain constant while the path is constructed. In Fruit Carts, objects have properties that can be changed: position, angle, and color. This allows for a wide variety of linguistic behavior which in turn supports detailed exploration of continuous understanding by humans and machines. In the Map Task, the participants’ screens differ, whereas in Fruit Carts the speaker and hearer share the same visual context, which simpliﬁes the analysis and interpretation of results (Figure 1).  2. Fruit Carts Domain and Tasks The Fruit Carts domain has three screen areas: a map, an object bin, and a controls panel. Each area was designed in part to elicit the types of expressions that require continuous understanding to approximate human behavior such as progressive restriction of a reference set throughout the utterance. The map contains named regions divided by solid lines, with three ﬂags as landmarks. The region names did not appear on the screen, to preclude use of spelling in referring expressions (the C in Central Park). Names were chosen to be phonetically distinct. To support progressive restriction of potential regions, regions whose initial portions overlap are adjacent (Morn identiﬁes Morningside and Morningside Heights) and some regions have ﬂags and others not (put the square on the ﬂag in... identiﬁes the regions with ﬂags.) No compass is displayed, in an attempt to limit the directions elicited to up, down, left, and right and not north, south, and so on.  471  Computational Linguistics  Volume 38, Number 3  Figure 1 Example initial and ﬁnal conﬁgurations for Fruit Carts domain and corpus. The region names were available to both director and actor (on paper) but were not shown on screen. The ﬁnal conﬁguration shown is the actual screen after the ﬁve dialogues from the participant whose third, fourth, and ﬁfth dialogues are shown in Appendix A. The object bin contains fruits and carts, by analogy with food vendor carts (e.g., hot dog stands). The fruits are avocados, bananas, cucumbers, grapefruits, and tomatoes, all botanically fruits. We chose fruits because they were nameable, especially with a label, and visually different from the carts. The carts are either squares or triangles, in two sizes, with an optional tag that for squares is either a diamond or a heart and for triangles is either a star or a circle. Adjectives (e.g., large, small) are commonly used in natural language descriptions and there is a growing body of psycholinguistic research, mostly with scripted utterances, that has used adjectives to investigate realtime language processing (Sedivy et al. 1999; Brown-Schmidt, Campana, and Tanenhaus 2005). Here, to support progressive restriction of potential carts, each component is easy to name but the entire shape requires a complex description rather than a prenominal modiﬁer—or at least strongly prefers one, as no examples to the contrary were observed in the Fruit Carts corpus described later in this article. That is, whereas a square with stripes could be either the square with stripes or the striped square, a square with a diamond on the corner is the square with a diamond on the corner but not *the corner-diamonded square. The controls panel contains left and right rotation arrows and six paint colors (black, brown, orange, blue, pink, and purple) chosen to be distinct from the colors of the fruit. Five tasks are included in Fruit Carts, all performed by using a mouse. To CHOOSE a cart, the user clicks on it. To PLACE it on the map, the user drags it there. To PAINT the cart, the user clicks on the desired color. Painting is a uniformly easy control task. To ROTATE the cart, the user presses and holds down the left or right rotation button. 472  Aist et al.  Fruit Carts Domain and Corpus  The goal of the rotation tool was to allow arbitrary rotations and to elicit utterances that were in response to visual feedback, such as rotate it a little to the right, more, stop. Finally, to FILL the cart, the user drags fruit to it. 3. Fruit Carts Corpus For the dual goals of gathering a corpus of utterances for dialogue system research, and testing the hypothesis that object complexity modulates referential strategy in human language production, we designed a set of goal maps that systematically manipulated: POSITION. Each cart was in a high-codability “easy” position, such as centered on a ﬂag or in a region; or a low-codability “hard” position, such as off-center. HEADING. Each cart was at an “easy” angle, an integer multiple of 45 degrees from its original orientation; or a “hard” angle, a non-multiple of 45 degrees. CONTENTS. Each cart contained an “easy” set of objects, fruit of the same type, such as three tomatoes; or a “hard” set of objects, such as two bananas and a grapefruit. COLOR. Each cart was painted a uniformly “easy” color to provide a control condition. One person (the director) gave directions to the other (the actor) on how to carry out the task. The director wore a headset microphone that collected speech data; the actor in this set-up wore a head-mounted eye-tracker that collected eye movements. The director (a subject) sat just behind the actor (a confederate); both viewed the same screen. Twelve subjects participated, each of whom speciﬁed twenty objects to place on the map; thus, a total of 240 dialogues were collected. The recordings were transcribed word-for-word by a professional transcription service that also provided sentence boundaries. The corpus has been labeled for referential strategy at the utterance level (Aist et al. 2005) and subsequently with referring expressions, spatial relations, and actions in order to support word-by-word incremental interpretation (Gallo et al. 2007); see Appendix A. 4. Analysis with Respect to Desired Features How well does the Fruit Carts domain meet the desired features described earlier? 1. Unscripted. Subjects were generally able to complete the task with only the instructions to make the screen match their paper map, and no prior examples of what to say, although one subject systematically did not issue instructions to paint the shapes. 2. Constrained. Generally speaking, subjects used the language we expected, such as square, triangle, and so forth, or high-frequency synonyms such as box for a square cart (from the ﬁrst dialogue of the participant in Appendix A, omitted for space) or dot for a circle tag (Appendix A, [D3]). There were examples of participants using unexpected expressions, such as calling an avocado a lime, despite the on-screen label. Yet overall the language was well constrained by the context. 3. Support for varying of task difﬁculty. As the Fruit Carts corpus showed, location, heading, and contents of carts can be systematically varied; later corpora, outside the scope of this article, have varied the number of carts placed together in order to construct simple or compound objects, in order to test the hypothesis that higher-level 473  Computational Linguistics  Volume 38, Number 3  task and goal knowledge (e.g. a tower is being built from several blocks) modulates language production, and to support further dialogue system research. 4. Support for collection of semi-independent subdialogues. Here the Fruit Carts domain excels. Due to the presence of multiple separate objects and regions, different subdialogues can make use of different objects, regions, properties, and so forth. By contrast, a domain revolving around construction of a single complex target, such as a landscaping plan, would have licensed substantial amounts of reference to previously placed objects including objects not in place at the time the dialogue began—making subdialogues dependent on each other in terms of accuracy, correctness, and so forth. As Appendix A illustrates, these Fruit Carts data contain relatively few such references. This is analogous to the difference between a math exercise set that contains several independent exercises, and a set where each exercise builds on previous answers. 5. Use in Research For dialogue systems research, the Fruit Carts domain has already been useful in developing dialogue systems that understand language continuously while taking pragmatics into account. For example, using Fruit Carts, incorporating pragmatic feedback about the visual world early in the parsing process was shown to substantially improve parsing efﬁciency as well as allowing parsing decisions to accurately reﬂect the visual world (Aist et al. 2006). Also using Fruit Carts, a dialogue system using continuous understanding was shown to be faster than, and preferred to, a counterpart that used a traditional pipeline architecture but was otherwise identical (Aist et al. 2007). For psycholinguistic research, Fruit Carts has also been used for studying the relationship between bi-clausal structure and theme complexity (Gallo et al. 2008) and testing hypotheses regarding the relationship of information in a message, resource limitations, and sentence production (Gallo, Jaeger, and Smyth 2008). 6. Discussion and Conclusions Fruit Carts also has a number of other advantages as well as some limitations. First, Fruit Carts provides ample temporary or local ambiguity in its utterances, a central challenge for continuous understanding systems and a classic target of research in psycholinguistics (for a review see Altmann [1998]). In a typical sequence such as okay take a ... small triangle with a dot on the corner (Appendix A, [D3]), most of the content words and some of the function words serve to resolve local ambiguity: okay take... – uniquely identiﬁes an action ...a ... small... – restricts (partially disambiguates) referential domain to half of the shapes ...triangle... – further restricts the referential domain to the triangles ...with... – further restricts the referential domain to carts with tags ...a dot... – further restricts the referential domain to carts with circles ...on the corner – uniquely identifes one of the twenty carts Likewise, ﬂag in right ... um ... side of the uh ... ﬂag in pine tree mountain [D5] restricts regions to ﬂagged regions.  474  Aist et al.  Fruit Carts Domain and Corpus  Second, Fruit Carts also elicits substantial variation in referential strategy. Some utterances could be grounded independent of context, up to pronominal reference. For example, the hypothetical utterance Move a large plain square to the ﬂag in Central Park has a fully speciﬁed action, object, and goal, as do rotate it about 45 degrees (Appendix A, [D4]), and and um make that orange [D5]. We labeled this category “all-at-once.” For other utterances, grounding relied on the surrounding context—dialogue and/or task. For example, um a little to the left [D4] contains a direction (left) but might rely on the last action to identify the intended action as rotation or movement, and on the selected shape on the screen to identify the object. We labeled this category “continuous.” Some utterances exhibited “both” all-at-once and continuous properties, or properties of “neither” category. The continuous utterances contained 21% fewer words (mean, 8.72 vs. 6.85) than the all-at-once and contained shorter words, too (mean, 3.95 letters vs. 3.74). About one-third of the utterances were labeled as “continuous”; speakers produced more continuous utterances as task experience increased (Aist et al. 2005). Finally, Fruit Carts is relatively abstract: The carts are basic shapes such as squares and triangles, and the fruit are chosen for language research purposes. On the one hand, this is desirable because it reduces the possibility of confounding effects from prior knowledge. On the other hand, it would be interesting for future work to extend Fruit Carts-style domains to more realistic object construction and placement tasks. Appendix A: Example Dialogues Referential strategy. These dialogues [D3]–[D5] are the third, fourth, and ﬁfth dialogues from one subject, screen one. For conciseness, “...” concatenates some adjacent utterances. All-at-once sections are marked in bold and continuous sections in italics. [D3] okay take a ... small triangle with a dot on the corner and ... um ... put it ... it should be in um ... kinda the uh ... center right side of morningside heights uh morningside heights ... oh ... um a little further in ... uh ... towards the um oh wait a little back sorry ... uh that’s good and then rotate it to the right until the l- hypotenuse is str- fa- yeah like that <laughter> and then make that blue and put a uh grapefruit in it so that it ... it’s touching the left side but sticking out of the top oh it should be inside the triangle and touching um a little ... over ... or down and over a little bit ... uh yeah that’s good um <breath> ... now ... uh [D4] take a square ... and put it in um ... oceanview terrace and pretty much in the center um i don’t know which one it i- i guess the s- try the smaller one um and then uh rotate it about 45 degrees um ... oh ... like one more turn ... yeah um and make that ... pink  475  Computational Linguistics  Volume 38, Number 3  and then put a uh tomato ... in the ... um a little to the left ...okay good ... um ... it ... i’m not sure if it should be a bigger one that triangle or not um you can try the bigger triangle ... i mean not the bigger triangle the bigger square ... i think maybe it should be the ... yeah i think it should be the bigger square <mumble> ... put the yeah right there [D5] and then um ... and put ... um <breath> ... <mumble> ... then put uh get a uh ... <mumble> take the uh large triangle with the star and um ... put that ... um to the ... right ... um ... side of the uh ... ﬂag in pine tree mountain er the right side and ... <laughter> um down a little um ... then rotate it so that ... the ... the hypotenuse is ... almost ... horizontal but ... tilted a little sli- like one more rotat- yeah and um make that orange um maybe a little closer to the ﬂag and down ... yeah that should be good kay um and then put a uh tomato in the right ... er in the left corner and then a cucumber in the right corner of it um ... the tomato should be a l- er um ... not ... quite ... in the corner th- yeah that’s good and the cucumber should be a little down a little more yeah um oh wait that’s a little too much ... uh that sh- um that’s good okay ... that’s it <laughter> ... <laughter> oh you wanna see this ... <laughter> i think that’s good ... okay <laughter> Incremental disambiguation. This example, adapted from Gallo et al. (2007), shows annotation to support disambiguation, here, in the small box in Morningside. These are word-level annotations in the smallest possible semantic units, marked at the point of disambiguation with no lookahead, and following the speaker’s intentions (Gallo et al. 2007). the anchor(A1) deﬁnite(A1) small size(A1, small) box objectType(A1, square) in anchor(A2) spatialRelation(A2, inside) location(A1, A2) 476  Aist et al.  Fruit Carts Domain and Corpus  Morningside anchor(A3) name(A3) objectReferent(A3, MorningsideRegion3) ground(A2, A3) Message structure. The following example, adapted from Gallo et al. (2008), shows annotation for the purpose of exploring the link between message structure and complexity of the theme. original: take a square with a ... square with a heart on the corner clean: take a square with a heart on the corner action: SELECT verb: take theme: a square with a heart on the corner theme disﬂuency: Yes theme pause: Yes  Acknowledgments This material is based upon work supported by the National Science Foundation under grant 0328810. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of the National Science Foundation. This publication was partially supported by grant HD 27206 from the NIH. The contents of this report are solely the responsibility of the authors and do not necessarily represent the ofﬁcial views of the NIH. References Aist, G. S., J. Allen, E. Campana, L. Galescu, C. Go´ mez Gallo, S. Stoness, M. Swift, and M. K. Tanenhaus. 2006. Software architectures for incremental understanding of human speech. In Proceedings of the 9th International Conference on Spoken Language Processing, pages 1922–1925, Pittsburgh, PA. Aist, G. S., J. Allen, E. Campana, C. Go´ mez Gallo, S. Stoness, M. Swift, and M. K. Tanenhaus. 2007. Incremental dialogue system faster than and preferred to its nonincremental counterpart. In Proceedings of the 29th Annual Meeting of the Cognitive Science Society, pages 761–766, Nashville, TN. Aist, G. S., E. Campana, J. Allen, M. Rotondo, M. Swift, and M. K. Tanenhaus. 2005.  Variations along the contextual continuum in task-oriented speech. In Proceedings of the 27th Annual Meeting of the Cognitive Science Society, pages 79–84, Stresa. Altmann, G. T. M. 1998. Ambiguity in sentence processing. Trends in Cognitive Sciences, 2(4):146–152. Anderson, A., M. Bader, E. Bard, E. Boyle, G. M. Doherty, G. M. Garrod, S. Isard, J. Kowtko, J. McAllister, J. Miller, C. Sotillo, H. S. Thompson, and R. Weinert. 1991. The HCRC map task corpus. Language and Speech, 34:351–366. Brennan, S. E. 1991. Conversation with and through computers. User Modeling and User-Adapted Interaction, 1:67–86. Brown-Schmidt, S., E. Campana, and M. K. Tanenhaus. 2005. Real-time reference resolution in a referential communication task. In J. C. Trueswell and M. K. Tanenhaus, editors, Processing World-situated Language: Bridging the Language-as-action and Language-as-product Traditions. MIT Press, Cambridge, MA, pages 153–171. Cook, S. W., T. F. Jaeger, and M. K. Tanenhaus. 2009. Producing less preferred structures: More gestures, less ﬂuency. In Proceedings of the 31st Annual Meeting of the Cognitive Science Society, pages 62–67, Amsterdam. DeVault, D. and M. Stone. 2004. Interpreting vague utterances in context. In Proceedings of COLING, pages 1247–1253, Geneva. Gabsdil, M. and O. Lemon. 2004. Combining acoustic and pragmatic features to predict  477  Computational Linguistics  Volume 38, Number 3  
∗ Department of Computer Science, Columbia University, New York, NY 10027, United States. E-mail: scohen@cs.columbia.edu. This research was completed while the ﬁrst author was at Carnegie Mellon University. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States. E-mail: nasmith@cs.cmu.edu. Submission received: 1 November 2010; revised submission received: 21 June 2011; accepted for publication: 3 August 2011. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 3  normalization of rule frequencies as they are observed in data. In the unsupervised case, on the other hand, algorithms such as expectation-maximization are available. MLE is attractive because it offers statistical consistency if some conditions are met (i.e., if the data are distributed according to a distribution in the family, then we will discover the correct parameters if sufﬁcient data is available). In addition, under some conditions it is also an unbiased estimator. An issue that has been far less explored in the computational linguistics literature is the sample complexity of MLE. Here, we are interested in quantifying the number of samples required to accurately learn a probabilistic grammar either in a supervised or in an unsupervised way. If bounds on the requisite number of samples (known as “sample complexity bounds”) are sufﬁciently tight, then they may offer guidance to learner performance, given various amounts of data and a wide range of parametric families. Being able to reason analytically about the amount of data to annotate, and the relative gains in moving to a more restricted parametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justiﬁed assumptions about the distributions that generate the data. Our framework uses and signiﬁcantly extends ideas that have been introduced for deriving sample complexity bounds for probabilistic graphical models (Dasgupta 1997). Maximum likelihood estimation is put in the empirical risk minimization framework (Vapnik 1998) with the loss function being the log-loss. Following that, we develop a set of learning theoretic tools to explore rates of estimation convergence for probabilistic grammars. We also develop algorithms for performing empirical risk minimization. Much research has been devoted to the problem of learning ﬁnite state automata (which can be thought of as a class of grammars) in the Probably Approximately Correct setting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 1989; Pitt 1989; Terwijn 2002). Typically, the setting in these cases is different from our setting: Error is measured as the probability mass of strings that are not identiﬁed correctly by the learned ﬁnite state automaton, instead of measuring KL divergence between the automaton and the true distribution. In addition, in many cases, there is also a focus on the distribution-free setting. To the best of our knowledge, it is still an open problem whether ﬁnite state automata are learnable in the distribution-dependent setting when measuring the error as the fraction of misidentiﬁed strings. Other work (Ron 1995; Ron, Singer, and Tishby 1998; Clark and Thollard 2004; Palmer and Goldberg 2007) also gives treatment to probabilistic automata with an error measure which is more suitable for the probabilistic setting, such as Kullback-Lielder (KL) divergence or variation distance.  480  Cohen and Smith  Empirical Risk Minimization for Probabilistic Grammars  These also focus on learning the structure of ﬁnite state machines. As mentioned earlier, in our setting we assume that the grammar is ﬁxed, and that our goal is to estimate its parameters. We note an important connection to an earlier study about the learnability of probabilistic automata and hidden Markov models by Abe and Warmuth (1992). In that study, the authors provided positive results for the sample complexity for learning probabilistic automata—they showed that a polynomial sample is sufﬁcient for MLE. We demonstrate positive results for the more general class of probabilistic grammars which goes beyond probabilistic automata. Abe and Warmuth also showed that the problem of ﬁnding or even approximating the maximum likelihood solution for a twostate probabilistic automaton with an alphabet of an arbitrary size is hard. Even though these results extend to probabilistic grammars to some extent, we provide a novel proof that illustrates the NP-hardness of identifying the maximum likelihood solution for probabilistic grammars in the speciﬁc framework of “proper approximations” that we deﬁne in this article. Whereas Abe and Warmuth show that the problem of maximum likelihood maximization for two-state HMMs is not approximable within a certain factor in time polynomial in the alphabet and the length of the observed sequence, we show that there is no polynomial algorithm (in the length of the observed strings) that identiﬁes the maximum likelihood estimator in our framework. In our reduction, from 3-SAT to the problem of maximum likelihood estimation, the alphabet used is binary and the grammar size is proportional to the length of the formula. In Abe and Warmuth, the alphabet size varies, and the number of states is two. This article proceeds as follows. In Section 2 we review the background necessary from Vapnik’s (1988) empirical risk minimization framework. This framework is reduced to maximum likelihood estimation when a speciﬁc loss function is used: the logloss.1 There are some shortcomings in using the empirical risk minimization framework in its simplest form. In its simplest form, the ERM framework is distribution-free, which means that we make no assumptions about the distribution that generated the data. Naively attempting to apply the ERM framework to probabilistic grammars in the distribution-free setting does not lead to the desired sample complexity bounds. The reason for this is that the log-loss diverges whenever small probabilities are allocated in the learned hypothesis to structures or strings that have a rather large probability in the probability distribution that generates the data. With a distribution-free assumption, therefore, we would have to give treatment to distributions that are unlikely to be true for natural language data (e.g., where some extremely long sentences are very probable). To correct for this, we move to an analysis in a distribution-dependent setting, by presenting a set of assumptions about the distribution that generates the data. In Section 3 we discuss probabilistic grammars in a general way and introduce assumptions about the true distribution that are reasonable when our data come from natural language examples. It is important to note that this distribution need not be a probabilistic grammar. The next step we take, in Section 4, is approximating the set of probabilistic grammars over which we maximize likelihood. This is again required in order to overcome the divergence of the log-loss for probabilities that are very small. Our approximations are 
Graphical representations are widely used to depict quantitative data and the relations among them (Friendly 2008). Although some graphics are constructed from raw data only for visualization purposes, the majority of information graphics (such as bar charts and line graphs) found in popular media (such as magazines and newspapers) are ∗ The Scientiﬁc and Technological Research Council of Turkey, Center of Research for Advanced Technologies of Informatics and Information Security, Gebze, Kocaeli, TURKEY, 41470. E-mail: senizd@uekae.tubitak.gov.tr. (This work was done while the author was a graduate student at the Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.) ∗∗ Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716. E-mail: carberry@cis.udel.edu. † Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716. E-mail: mccoy@cis.udel.edu. Submission received: 20 April 2010; revised submission received: 8 July 2011; accepted for publication: 6 September 2011. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 3  Figure 1 Graphic conveying a maximum bar. constructed to convey a message. For example, the graphic in Figure 1 ostensibly is intended to convey that “The United States has the highest number of hacker attacks among the countries listed.” The graphic designer made deliberate choices in order to bring that message out. For example, the bar representing the United States is highlighted with a different color from the other bars and the bars are sorted with respect to their values instead of their labels so that the bar with the highest value can be easily recognized. Such choices, we argue, are examples of communicative signals that graphic designers use. Under Clark’s deﬁnition (1996), language is not just text and utterances, but instead includes any deliberate signal (such as gestures and facial expressions) that is intended to convey a message; thus an information graphic is a form of language. In popular media, information graphics often appear as part of a multimodal document. Carberry, Elzer, and Demir (2006) conducted a corpus study of information graphics from popular media, where the extent to which the message of a graphic is also captured by the text of the accompanying document was analyzed. One hundred randomly selected graphics of different kinds (e.g., bar charts and line graphs) were collected from newspapers and magazines along with their articles. It was observed that in 26% of the instances, the text conveyed only a small portion of the graphic’s message and in 35% of the instances, the text didn’t capture the graphic’s message at all. Thus graphics, together with the textual segments, contribute to the overall purpose of a document (Grosz and Sidner 1986) and cannot be ignored. We argue that information graphics are an important knowledge resource that should be exploited, and understanding the intention of a graphic is the ﬁrst step towards exploiting it. This article presents our novel approach to identifying and textually conveying the high-level content of an information graphic (the message and knowledge that one would gain from viewing a graphic) from popular media. Our system summarizes this form of non-linguistic input data by utilizing the inferred intention of the graphic designer and the communicative signals present in the visual representation. Our overall goal is to generate a succinct coherent summary of a graphic that captures the intended message of the graphic and its visually salient features, which we hypothesize as being related to the intended message. Input to our system is the intention of the graphic inferred by the Bayesian Inference System (Elzer, Carberry, and Zukerman 2011), and an XML representation of the visual graphic (Chester and Elzer 2005) that speciﬁes the components of the graphic such as the number of bars and the heights of each bar. Our work focuses on the generation issues inherent in generating a textual summary of a graphic given this information. The current implementation of the system is applicable to only one kind of information graphic, simple bar charts, but we hypothesize that the overall summarization approach could be extended to other kinds of graphics. 528  Demir, Carberry, and McCoy  Summarizing Information Graphics Textually  In this article, we investigate answers to the following questions: (1) Among all possible information that could be conveyed about a bar chart, what should be included in its summary? (2) How should the content of a summary be organized into a coherent text? (3) How should the text structure be best realized in natural language? Given the intended message and the XML representation of a graphic, our system ﬁrst determines the content of the graphic’s summary (a list of propositions) by applying the content identiﬁcation rules constructed for that intended message category. Our system then produces a coherent organization of the selected content by applying a bottom–up approach which leverages a variety of considerations (such as the syntactic complexity of the realized sentences and clause embeddings) in choosing how to aggregate information into sentence-sized units. The system ﬁnally orders and realizes the sentence-sized units in natural language and generates referring expressions for graphical elements that are required in realization. The rest of this article is structured as follows. Section 2 discusses related work on summarization of non-linguistic input data and describes some natural language applications which could beneﬁt from summaries generated by our work. Section 3 outlines our summarization framework. Section 4 is concerned with identifying the propositional content of a summary and presents our content-identiﬁcation rules that specify what should be included in the summary of a graphic. Section 5 describes our bottom–up approach, which applies operators to relate propositions selected for inclusion, explores aggregating them into sentence-sized units, and selects the best organization via an evaluation metric. Section 6 presents our sentence-ordering mechanism, which incorporates centering theory to specify the order in which the sentence-sized units should be presented. Section 7 describes how our system realizes the selected content in natural language. Particular attention is devoted to our methodology for generating referring expressions for certain graphical elements such as a descriptor of what is being measured in the graphic. Section 8 presents a user study that was conducted to evaluate the effectiveness of the generated summaries for the purposes of this research by measuring readers’ comprehension. Section 9 concludes the article and outlines our future work. 2. Background 2.1 Related Work There has been a growing interest in language systems that generate textual summaries of non-linguistic input data (Reiter 2007). The overall goal of these systems, generally referred to as data-to-text systems, is to enable efﬁcient processing of large volumes of numeric data by supporting traditional visualisation modalities and to reduce the effort spent by human experts on analyzing the data. Various examples of datato-text systems in the literature include systems that summarize weather forecast data (Goldberg, Driedger, and Kittredge 1994; Coch 1998), stock market data (Kukich 1983), and georeferenced data (Turner, Sripada, and Reiter 2009). One of the most successful data-to-text generation research efforts is the SumTime project, which uses pattern recognition techniques to generate textual summaries of automatically generated time-series data in order to convey the signiﬁcant and interesting events (such as spikes and oscillations) that a domain expert would recognize by analyzing the data. The SumTime-Mousam (Somayajulu, Reiter, and Davy 2003) and SumTime-Turbine (Yu et al. 2007) systems were designed to summarize weather forecast data and the data from gas turbine engines, respectively. More recently, the  529  Computational Linguistics  Volume 38, Number 3  project was extended to the medical domain. The BabyTalk (Gatt et al. 2009) project produces textual summaries of clinical data collected for babies in a neonatal intensive care unit, where the summaries are intended to present key information to medical staff for decision support. The implemented prototype (BT-45) (Portet et al. 2009) generates multi-paragraph summaries from large quantities of heterogeneous data (e.g., time series sensor data and the records of actions taken by the medical staff). The overall goal of these systems (identifying and presenting signiﬁcant events) is similar to our goal of generating a summary that conveys what a person would get by viewing an information graphic, and these systems contend with each of the generation issues we must face with our system. Our generation methodology, however, is different from the approaches deployed in these systems in various respects. For example, BT-45 produces multi-paragraph summaries where each paragraph presents ﬁrst a key event (of highest importance), then events related to the key event (e.g., an event that causes the key event), and ﬁnally other co-temporal events. Our system, on the other hand, produces single-paragraph summaries where the selected propositions are grouped and ordered with respect to the kind of information they convey. In addition, BT-45 performs a limited amount of aggregation at the conceptual level, where the aggregation is used to express the relations between events with the use of temporal adverbials and cue phrases (such as as a result). Contrarily, our system syntactically aggregates the selected propositions with respect to the entities they share. There is also a growing literature on summarizing numeric data visualized via graphical representations. One of the recent studies, the iGRAPH-Lite (Ferres et al. 2007) system, provides visually impaired users access to the information in a graphic via keyboard commands. The system is speciﬁcally designed for the graphics that appear in “The Daily” (Statistics Canada’s main dissemination venue) and presents the user with a template-based textual summary of the graphic. Although this system is very useful for in depth analysis of statistical graphs and interpreting numeric data, it is not appropriate for graphics from popular media where the intended message of the graphic is important. In the iGRAPH-Lite system, the summary generated for a graphic conveys the same information (such as the title of the graphic, and the maximum and minimum values) no matter what the visual features of the graphic are. The content of the summaries that our system generates, however, is dependent on the intention and the visual features of the graphic. Moreover, that system does not consider many of the generation issues that we address in our work. Choosing an appropriate presentation for a large amount of quantitative data is a difﬁcult and time-consuming task (Foster 1999). A variety of systems were built to automatically generate presentations of statistical data—such as the PostGraphe system (Corio and Lapalme 1999; Fasciano and Lapalme 2000), which generates graphics and complementary text based on the information explicitly given by the user such as the intention to be conveyed in the graphic and the data of special interest to the user. The content of the accompanying text is determined according to the intention of the graphic and the features of the data. Moreover, the generated texts are intended to reinforce some important facts that are visually present in the graphic. In this respect, the generation in PostGraphe is similar to our work, although the output texts have a limited range and are heavily dependent on the information explicitly given by the user. 2.2 Role of Graphical Summaries in Natural Language Applications 2.2.1 Accessibility. Electronic documents that contain information graphics pose challenging problems for visually impaired individuals. The information residing in the  530  Demir, Carberry, and McCoy  Summarizing Information Graphics Textually  text can be delivered via screen reader programs but visually impaired individuals are generally stymied when they come across graphics. These individuals can only receive the ALT text (human-generated text that conveys the content of a graphic) associated with the graphic. Many electronic documents do not provide ALT texts and even in the cases where ALT text is present, it is often very general or inadequate for conveying the intended message of the graphic (Lazar, Kleinman, and Malarkey 2007). Researchers have explored different techniques for providing access to the informational content of graphics for visually impaired users, such as sound (Meijer 1992; Alty and Rigas 1998), touch (Ina 1996; Jayant et al. 2007), or a combination of the two (Kennel 1996; Ramloll et al. 2000). Unfortunately, these approaches have serious limitations such as requiring the use of special equipment (e.g., printers and touch panels) or preparation work done by sighted individuals. Research has also investigated language-based accessibility systems to provide access to graphics (Kurze 1995; Ferres et al. 2007). As mentioned in Section 2.1, these language-based systems are not appropriate for graphics in articles from popular media where the intended message of the graphic is important. We hypothesize that providing alternative access to what the graphic looks like is not enough and that the user should be provided with the message and knowledge that one would gain from viewing the graphic. We argue that the textual summaries generated by our approach could be associated with graphics as ALT texts so that individuals with sight impairments would be provided with the high-level content of graphics while reading electronic documents via screen readers. 2.2.2 Document Summarization. Research has extensively investigated various techniques for single (Hovy and Lin 1996; Baldwin and Morton 1998) and multi-document summarization (Goldstein et al. 2000; Schiffman, Nenkova, and McKeown 2002). The summary should provide the topic and an overview of the summarized documents by identifying the important and interesting aspects of these documents. Document summarizers generally evaluate and extract items of information from documents according to their relevance to a particular request (such as a request for a person or an event) and address discourse related issues such as removing redundancies (Radev et al. 2004) and ordering sentences (Barzilay, Elhadad, and McKeown 2002) in order to make the summary more coherent. It is widely accepted that to produce a good summary of a document, one must understand the document and recognize the communicative intentions of the author. Summarization work primarily focuses on the text of a document but, as mentioned earlier, information graphics are an important part of many multimodal documents that appear in popular media and these graphics contribute to the overall communicative intention of the document. We argue that document summarization should capture the high-level content of graphics that are included in the document, because information graphics often convey information that is not repeated elsewhere in the document. We believe that the summary of a graphic generated by our system, which provides the intended message of the graphic and the information that would be perceived with a casual look at the graphic, might help in summarizing multi-modal documents.1  
1. Introduction Adjectives are one of the most elusive parts of speech with respect to meaning. For example, it is very difﬁcult to establish a broad classiﬁcation of adjectives into semantic classes, analogous to a broad ontological classiﬁcation of nouns (Raskin and Nirenburg ∗ Department of Translation and Language Sciences, Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona, Spain. E-mail: gemma.boleda@upf.edu. ∗∗ E-mail: schulte@ims.uni-stuttgart.de. † E-mail: toni.badia@upf.edu. Submission received: 10 December 2008; revised submission received: 16 July 2011; accepted for publication: 5 September 2011. Part of the work reported in this article was done while the ﬁrst author was a postdoctoral scholar at U. Polite`cnica de Catalunya and a visiting researcher at U. Stuttgart. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 3  1998). This article tackles precisely this task, that is, the semantic classiﬁcation of adjectives, for Catalan. We aim at automatically inducing the semantic class for an adjective given its linguistic properties, as extracted from corpora and other resources. The acquisition of semantic classes has been widely studied for verbs (Dorr and Jones 1996; McCarthy 2000; Korhonen, Krymolowski, and Marx 2003; Lapata and Brew 2004; Schulte im Walde 2006; Joanis, Stevenson, and James 2008) and, to a lesser extent, for nouns (Hindle 1990; Pereira, Tishby, and Lee 1993), but, with very few exceptions (Bohnet, Klatt, and Wanner 2002; Carvalho and Ranchhod 2003), not for adjectives. Furthermore, we cannot rely on a well-established classiﬁcation for adjectives. The classes themselves are subject to experimentation. We will test two different classiﬁcations, analyzing the empirical properties of the classes and the problems in their deﬁnition. Another signiﬁcant challenge is posed by polysemy, or the fact that one and the same adjective can have multiple senses. Different senses may fall into different classes, such that it is no longer possible to identify one single semantic class per adjective. Moreover, many adjectives exhibit similar sense alternations, in a phenomenon known as regular or systematic polysemy (Apresjan 1974; Copestake and Briscoe 1995). A special focus of the research presented, therefore, is on modeling regular polysemy. As an example of regular polysemy, take for instance the sense alternation for the adjective econo`mic exempliﬁed in Example (1). Econo`mic, derived from economia (‘economy’), can be translated as ‘economic, of the economy’, as in Example (1a), or as ‘cheap’, as in Example (1b). As we will see, each of these senses corresponds to a different semantic class in our classiﬁcations.  (1) a. recuperacio´ econo` mica recovery economySUFFIX ‘recovery of the economy’ b. pantalons econo` mics trousers economySUFFIX ‘cheap trousers’  Other adjectives exhibit similar sense alternations; for example, familiar (derived from fam´ılia, ‘family’) and amoro´s (derived from amor, ‘love’), as shown in Example (2).  (2) a. reunio´ familiar  / cara  familiar  meeting familySUFFIX/ face familySUFFIX  ‘family meeting / familiar face’  b. problema amoro´ s / noi  amoro´ s  problem loveSUFFIX/ boy loveSUFFIX  ‘love problem / lovely boy’  The ﬁrst senses in Examples (1) and (2) have a transparent relation to the denotation of the deriving noun, as witnessed by the fact that they are translated as nouns in English (economy, family, love), whereas the other senses are translated as adjectives (cheap, familiar, lovely). For each of these adjectives, there is a relationship between the two senses, such that the sense alternations seem to correspond to a productive semantic process along the lines of Example (3) (Raskin and Nirenburg 1998, schema (43), page 173). (3) PERTAINING TO [noun meaning] → CHARACTERISTIC OF [noun meaning]  576  Boleda, Schulte im Walde, and Badia  Modeling Regular Polysemy in Catalan Adjectives  Because of the systematic semantic relationship between the two senses of these adjectives, they constitute an instance of regular polysemy. In this article, therefore, we not only address the acquisition of semantic classes, but also the acquisition of polysemy: Our goal is to determine, for a given adjective, whether it is monosemous or polysemous, and to which class(es) it belongs. Note that we are not dealing with individual sense alternations, as related work on sense induction does (Schu¨ tze 1998; McCarthy et al. 2004; Brody and Lapata 2009), but with sense alternation types, that systematically hold across different lemmata. Thus, the present research is at the crossroad between sense induction and lexical acquisition. Regularities in sense alternations are pervasive in human languages, and they are probably favored by the properties of human cognition (Murphy 2002). Regular polysemy has been studied in theoretical linguistics (Apresjan 1974; Pustejovsky 1995) and in symbolic approaches to computational semantics (Copestake and Briscoe 1995). It has received little attention in empirical computational semantics, however. This is surprising, given the amount of work devoted to sense-related tasks such as Word Sense Disambiguation (WSD). In WSD (see Navigli [2009] for an overview) sense ambiguities are almost exclusively modeled for each individual lemma, despite the ensuing sparsity problems (Ando [2006] is an exception). Properly modeling regular polysemy, therefore, promises to improve computational semantic tasks such as WSD and sense discrimination. This article has the goal of ﬁnding a computational model that responds to the theoretical and empirical properties of regular polysemy. In this direction, we test two alternative approaches. We ﬁrst model polysemy in terms of independent classes to be separately acquired (e.g., an adjective with two senses ai and bi belongs to a class AB deﬁned independently of classes A and B), and show that this model is not adequate. A second approach, which posits that polysemous adjectives simultaneously belong to more than one class (e.g., an adjective with two senses ai and bi belongs to both class A and class B), is more successful. Our best classiﬁer achieves 69.1% accuracy against a 51% baseline, which is satisfactory, considering that the estimated upper bound (human agreement) for this task is 68%. We discuss pros and cons of the two models described and ways to overcome their limitations. In the following, we ﬁrst review related work (Section 2) and linguistic aspects of adjective classiﬁcation (Section 3), then present the two acquisition experiments (Sections 4 and 5), and ﬁnish with a general discussion (Section 6) and some conclusions and directions for future research (Section 7).  2. Related Work As mentioned in the Introduction, there has been very little research in the semantic classiﬁcation of adjectives. We know of only two articles on speciﬁcally this topic: Carvalho and Ranchhod (2003) used adjective classes similar to the ones explored here to disambiguate between nominal and adjectival readings in Portuguese. Adjective information, manually coded, served to establish constraints in a ﬁnite-state transducer part-of-speech tagger. Actually, POS tagging was also the initial motivation for the present research, as adjective–noun and adjective–verb (participle) ambiguities cause most difﬁculties to both humans and machines in languages such as English, German, and Catalan (Marcus, Santorini, and Marcinkiewicz 1993; Brants 2000; Boleda 2007). Bohnet, Klatt, and Wanner (2002) also has similar goals to the present research, as it is aimed at automatically classifying German adjectives. However, the classiﬁcation 577  Computational Linguistics  Volume 38, Number 3  used is not purely semantic, polysemy is not taken into account, and the evidence and techniques used are more limited than the ones used here. Other research on adjectives within computational linguistics is oriented toward different goals than ours. Yallop, Korhonen, and Briscoe (2005) tackle syntactic, not semantic classiﬁcation, akin to the acquisition of subcategorization frames for verbs. Another relevant line of research pursues WSD. Justeson and Katz (1995) and Chao and Dyer (2000) showed that adjectives are a very useful cue for disambiguating the sense of the nouns they modify. Adjective classes could be further exploited in WSD in at least two respects: (1) to establish an inventory of adjective senses (if polysemous instances are correctly detected; this is where sense induction and our own work ﬁts in), and (2) to exploit class-based properties for the disambiguation, similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mining and Sentiment Analysis (Pang and Lee 2008), as adjectives are known to convey much of the evaluative and subjective information in language (Wiebe et al. 2004). The typical goal of this kind of study has been to identify subjective adjectives and their orientation (positive, neutral, negative). This type of research, from pioneering work by Hatzivassiloglou and colleages (Hatzivassiloglou and McKeown 1993, 1997; Hatzivassiloglou and Wiebe 2000) to current research (de Marneffe, Manning, and Potts 2010), has thus focused on scalar adjectives, that is, adjectives like good and bad, which can be translated into values that can be ordered along a scale. These adjectives typically enter into antonymy relations (the semantic relation between good and bad), and in fact antonymy is the main organizing criterion for adjectives in WordNet (Miller 1998), the most widely used semantic resource in NLP. However, when examining a large scale lexicon, it becomes immediately apparent that there are many other types of adjectives that do not easily ﬁt in a scale-based or antonymy-based view of adjectives (Alonge et al. 2000). Some examples are pulmonary, former, and foldable. It is not clear, for instance, whether it makes sense to ask for an antonym of pulmonary, or to establish a “foldability” scale for foldable. These adjectives need a different treatment, and they are treated in terms of different semantic classes in this article. The semantic properties of adjectives can also be exploited in advanced NLP tasks and applications such as Question Answering, Dialog Systems, Natural Language Generation, or Information Extraction. For instance, from a sentence like This maimai is round and sweet, we can quite safely infer that the (invented) object maimai is a physical object, probably edible. This type of process could be exploited in, for instance, Information Extraction and ontology population, although to our knowledge this possibility has received but little attention (Malouf 2000; Almuhareb and Poesio 2004). As for polysemy, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem, by biasing the experimental material to include monosemous words only, or by choosing an approach that ignores polysemy (Hindle 1990; Merlo and Stevenson 2001; Schulte im Walde 2006; Joanis, Stevenson, and James 2008). There are a few exceptions to this tradition, such as Pereira, Tishby, and Lee (1993), Rooth et al. (1999), and Korhonen, Krymolowski, and Marx (2003), who used soft clustering methods for multiple assignment to verb semantic classes (see Section 4.5). There is very little related work in empirical computational semantics in modeling regular polysemy. A pioneering piece of research is Buitelaar (1998), which tried to account for regular polysemy with the CoreLex resource. CoreLex, building on the Generative Lexicon theory (Pustejovsky 1995), groups WordNet senses into 39 “basic  578  Boleda, Schulte im Walde, and Badia  Modeling Regular Polysemy in Catalan Adjectives  types” (broad ontological categories). In CoreLex, each word is associated to a polysemy class, that is, the set of all basic types its synsets belong to. Some of these polysemy classes constitute instances of regular polysemy, as recently explored in Utt and Pado´ (2011). Lapata (2000, 2001) also addresses regular polysemy in the Generative Lexicon framework. This work attempts to establish all the possible meanings of adjective-noun combinations, and rank them using information gathered from the British National Corpus (Burnage and Dunlop 1992). This information should indicate that an easy problem is usually equivalent to problem that is easy to solve (as opposed to, for example, easy text, that is usually equivalent to text that is easy to read). Thus, the focus is on the meaning of adjective-noun combinations, not on that of adjectives alone as in the present research.  3. Basis for a Semantic Classiﬁcation of Adjectives Adjective classes in our deﬁnition are broad classes of lexical meaning. We will present lexical acquisition experiments in which, given the evidence found in corpora and other lexical resources, a semantic class can be assigned to a given adjective. For this purpose, two preconditions are required: (a) a classiﬁcation that establishes the number and characteristics of the target semantic classes; (b) a stable relation between observable features and each semantic class. There is no established semantic classiﬁcation for adjectives in computational linguistics that we can use and, therefore, one subgoal of the research is to establish the classiﬁcation in the ﬁrst place, addressing (a), and exploiting the morphology–semantics and syntax–semantics interfaces for acquisition, addressing (b). We are thus facing a highly exploratory endeavor, and we do not regard the classiﬁcations we use as ﬁnal. We test two different classiﬁcations: an initial classiﬁcation, based on the literature, for the experiments reported in Section 4, and an alternative classiﬁcation, for the experiments reported in Section 5. We next turn to presenting the two tested classiﬁcations. 3.1 Initial Classiﬁcation In the acquisition experiments reported in Section 4, we distinguish between qualitative, intensional, and relational adjectives, which have the following properties (Miller 1998; Raskin and Nirenburg 1998; Picallo 2002; Demonte 2011). Qualitative adjectives. These are prototypical adjectives like gran (‘big’) or dolc¸ (‘sweet’), including scalar adjectives, which denote attributes or properties of objects. Adjectives in this class tend to be gradable and comparable (see Examples (4a–4b)). They are characterized by exhibiting the greatest variability with respect to their syntactic behavior: In Catalan, they can act as predicates in copular sentences and other constructions (Examples (4c–4d)), and they can typically act as both pre- and post-nominal modiﬁers (Examples (4e–4f)). When an adjective modiﬁes a head noun in pre-nominal position, 579  Computational Linguistics  Volume 38, Number 3  the interpretation is usually nonrestrictive, as shown by the fact that they can modify proper nouns (Example (4e)).  (4) a. Taula molt gran / grand´ıssima Table very big / bigSUPERLATIVE ‘Very big table’  b. Aquesta taula e´s me´s gran que aquella This table is more big than that ‘This table is bigger than that one’  c. Aquesta taula e´s gran This table is big ‘This table is big’  d. Aquesta taula la  veig  massa gran  This table itOBJ-CL-FEM seepres−1stp−sg too big ‘This table seems to me to be too big’  e. La gran Diana va  seguir cantant  The great Diana PAST-AUX continue singing.  ‘Great Diana continued singing.’  f. Van  portar una taula gran  PAST-AUX bring a table big  ‘They brought in a big table’  Intensional adjectives. These are adjectives like presumpte (‘alleged’) or antic (‘former’), which according to formal semantics denote second-order properties (Montague 1974, and subsequent work). Most intensional adjectives modify nouns in pre-nominal position only (Example (5a)), and they cannot functionally act as predicates (Example (5b)). They are also typically not gradable (Example (5c)). (5) a. El Joan e´s el presumpte assass´ı The Joan is the alleged murderer ‘Joan is the alleged murderer’ b. #El Joan e´s presumpte The Joan is alleged ‘#Joan is alleged’ c. #Me´s presumpte assass´ı / #presumpt´ıssim assass´ı More alleged murderer / allegedSUPERLATIVE murderer ‘#More/very alleged murderer’ Intensional adjectives like presumpte may appear in any order with respect to qualitative adjectives, as in Example (6). The order, however, affects interpretation: Example (6a) entails that the referent of the noun phrase is young, whereas Example (6b) does not (McNally and Boleda 2004). (6) a. jove presumpte assass´ı ‘young alleged murderer’  580  Boleda, Schulte im Walde, and Badia  Modeling Regular Polysemy in Catalan Adjectives  b. presumpte jove assass´ı ‘alleged young murderer’  Relational adjectives. Adjectives such as pulmonar, estacional, bota`nic (‘pulmonary, seasonal, botanical’) denote a relationship to an object (in the mentioned examples, LUNG, SEASON, and PLANT objects). Most of them are denominal (e.g., pulmonar is derived from pulmo´, ‘lung’) and can only modify nouns post-nominally (see Example (7a)). Also, contrary to qualitative adjectives, they are not gradable (Example (7b)) and act as predicates only under very restricted circumstances (Example (7c) vs. (7d)). If other adjectives or modiﬁers co-occur with relational adjectives, these occur after the adjective (Example (7e)). We will say relational adjectives are adjacent to the head noun.  (7) a. Tenia una malaltia pulmonar / #pulmonar malaltia Had a disease pulmonary / pulmonary disease ‘He/she had a pulmonary disease’  b. #Malaltia molt pulmonar / pulmonar´ıssima Disease very pulmonary / pulmonarySUPERLATIVE #‘Very pulmonary disease’  c. La decisio´ europea → ??Aquesta decisio´ e´s europea  The decision European → This  decision is European  ‘The European decision → ??This decision is European’  d. La tuberculosi pot ser pulmonar The tuberculose can be pulmonary ‘Tuberculose can be pulmonary’  e. inﬂamacio´ pulmonar greu / #inﬂamacio´ greu pulmonar inﬂamation pulmonary serious / inﬂamation serious pulmonary ‘serious pulmonary inﬂammation’  Table 1 summarizes the properties just explained. Our goal is to use these properties to induce the semantic class of adjectives. For instance, if an adjective is denominal, appears almost exclusively in postnominal position, and is strictly adjacent to the head noun, we predict that it is relational. In the experiments reported in Sections 4 and 5, we  Table 1 Initial classiﬁcation: Linguistic properties of qualitative, intensional, and relational adjectives.  Qualitative  Intensional  Relational  gran (‘big’) presumpte (‘alleged’) pulmonar ‘pulmonary’  Property predicative gradable/comparable position with respect to head noun adjacent denominal  + + both − −  − − pre-nom. − −  restricted − post-nom. + +  581  Computational Linguistics  Volume 38, Number 3  extract data related to these and other properties of adjectives from linguistic resources, and use them as features in machine learning experiments. 3.2 Alternative Classiﬁcation In the acquisition experiments reported in Section 5, we distinguish between qualitative, relational, and event-related adjectives. The classiﬁcation presented in Section 3.1 is thus altered in two ways: (1) The intensional class is dropped. (2) A new class, that of event-related adjectives, is added to the classiﬁcation. The reasons for these changes will become clear in the discussion of the experiments in Section 4. Here, we describe the new class and provide a summary table of the alternative classiﬁcation. Event-related adjectives. Adjectives such as exportador, prome`s, resultant (‘exporting, promised, resulting’) denote a relationship to an event, in this case, EXPORT, PROMISE, and RESULT events, respectively. Most of them are deverbal. Like relational adjectives, they are typically nongradable (see Example (8a)) and prefer the postnominal position when modifying nouns (Example (8b)). Like qualitative adjectives, they typically can act as predicates (Example (8c)). (8) a. E´ s un pa´ıs {exportador / #molt exportador} de petroli Is a country {exporting / very exporting} of oil ‘It is an oil exporting / #very exporting country’ b. #exportador pa´ıs ‘exporting country’ c. Aquest pa´ıs e´s exportador This country is exporting ‘This is an exporting country’ Table 2 summarizes the properties of the alternative classiﬁcation (for a more thorough discussion of previous research on the semantics of adjectives and more motivation for the classiﬁcation, see Boleda [2007]). For comparison, we will brieﬂy outline the treatment of adjectives in WordNet (Miller 1998; Alonge et al. 2000). As  Table 2 Alternative classiﬁcation: Linguistic properties of qualitative, event-related, and relational adjectives.  Qualitative  Event-related  Relational  gran (‘big’) exportador (‘exporting’) pulmonar ‘pulmonary’  Property predicative gradable/comparable position with respect to head noun adjacent derivational type  + + both − non-derived  + typically not post-nom. − deverbal  restricted − post-nom. + denominal  582  Boleda, Schulte im Walde, and Badia  Modeling Regular Polysemy in Catalan Adjectives  mentioned in Section 2, the main semantic relation around which adjectives are organized in WordNet is antonymy. Also as explained, however, not all adjectives have antonyms. This is solved in WordNet by the use of indirect antonyms (e.g., swift and slow are indirect antonyms, through the semantic similarity between swift and fast). Still, indirect antonymy only applies to a small subset of the adjectives in WordNet (slightly over 20% in WordNet 1.5). Therefore, some kinds of adjectives receive a differentiated treatment. Speciﬁcally, two main kinds of adjectives are distinguished in WordNet: (1) Descriptive adjectives, akin to our qualitative adjectives, which are organized around antonymy (descriptive adjectives, however, include intensional adjectives). (2) Relational adjectives, as deﬁned in this article, for which two different solutions are adopted. If a suitable antonym can be found for a given relational adjective (antonym in a broad sense; in Miller [1998, page 60], physical and mental are considered antonyms), it is treated in the same way as a descriptive adjective. Otherwise, it is linked through a PERTAIN-TO pointer to the related noun. In addition, a subclass of descriptive adjectives, having the form of past or present participles, is distinguished, and also receives a hybrid treatment. Those that can be accommodated to antonymy are treated as descriptive adjectives (laughing–unhappy, through the similarity between laughing and happy). Those which cannot are linked to the source verb through a PRINCIPAL-PART-OF pointer. Our event-related class includes not only past and present participles, but other types of deverbal adjectives. Thus, most of the classes used in this article are to some extent backed up by the organization of adjectives in WordNet.  3.3 The Role of Polysemy  As explained in the Introduction, some adjectives are polysemous such that each sense falls into a different class of the classiﬁcations just presented. Consider for instance the adjective econo`mic in Example (1), repeated here as Example (9) for convenience. The two main senses of econo`mic instantiate the relational (sense in Example (9a)) and the qualitative class (sense in Example (9b)), respectively.  (9) a. ana`lisi econo` mica ‘economic analysis’ b. pantalons econo` mics ‘cheap trousers’  Crucially for our purposes, in each of the senses the adjective exhibits the properties of each of the associated classes. When used as a relational adjective, it is not gradable and cannot be used in a pre-nominal position (Example (10)). When used as a qualitative adjective, it is gradable and it can be used predicatively (see Example (11)). In the experiments that follow, we aim at capturing this hybrid behavior.  (10) a. #L’ana`lisi molt econo` mica de les dades The-analysis very economic of the data ‘#The very economic analysis of the data’  b. #Va  dur a terme una econo` mica ana`lisi  ‘PAST-AUX bring to term an economic analysis  ‘#He/she carried out an economic analysis’  583  Computational Linguistics  Volume 38, Number 3  (11) Aquests pantalons so´ n molt econo` mics! These trousers are very economic! ‘These trousers are very cheap!’  Cases of regular polysemy between the intensional and qualitative classes also exist, as illustrated in Examples (12) and (13). Antic has two major senses, a qualitative one (equivalent to ‘old, ancient’) and an intensional one (equivalent to ‘former’). Note again that, when used in the intensional sense, it exhibits properties of the intensional class: It appears pre-nominally (Example (13a)) and is not gradable (Example (13b)).  (12) a. ediﬁci antic building ancient ‘ancient building’ b. ediﬁci molt antic building very ancient ‘very ancient building’  (13) a. antic president ancient president ‘former president’ b. #molt antic president very ancient president ‘#very former president’  The new class in the alternative classiﬁcation, that of event-related adjectives, also introduces regular polysemy, speciﬁcally, between event-related and qualitative adjectives, as illustrated in Examples (14) and (15). The participial adjective sabut (‘known’) has an event-related sense, corresponding to the verb saber (‘know’), and a qualitative sense that can be translated as ‘wise’. Likewise, the deverbal adjective cridaner derived from cridar (‘to shout’) alternates between an event-related sense and a qualitative sense.  (14) problema sabut / home sabut problem known / man known ‘known problem / wise man’  (15) noi cridaner / camisa  cridanera  boy shoutSUFFIX/ shirt attention-gaining ‘boy who shouts a lot / attention-gaining shirt’  Examples (14) and (15) represent cases of regular polysemy because, as can be drawn from the translations, there is a systematic shift from a transparent relation with the event to a quality that bears a more distant relation to the event. In the case of sabut the relation is clear (if a man knows a lot, he is wise); in the case of cridaner, a shirt qualiﬁes for the adjective if it is for instance loud-colored or has an eccentric cut, such that it gains the attention of people, as shouting does. In this article, we only consider types of polysemy that cut across the classiﬁcation pursued. Other kinds of polysemy that have traditionally been tackled in the literature will not be considered. For instance, we will not be concerned with the polysemy illustrated in Example (16), which arguably has more to do with the semantics of the  584  Boleda, Schulte im Walde, and Badia  Modeling Regular Polysemy in Catalan Adjectives  modiﬁed noun than that of the adjective (Pustejovsky 1995). Both of the uses of trist (‘sad’) illustrated in Example (16) fall into the qualitative class, so, contrary to the work by Lapata (2000, 2001) cited previously, we do not treat the adjective as polysemous in the context of the present experiments. (16) noi trist / pel·l´ıcula trista boy sad / ﬁlm sad ‘sad boy / sad ﬁlm’  4. First Model: Polysemous Adjectives Constitute Independent Classes Given the hybrid behavior of polysemous adjectives explained in Section 3, we can expect that they behave differently from adjectives in the basic classes. For instance, adjectives polysemous between a qualitative and a relational use should exhibit more evidence for gradability than pure relational adjectives, but less than pure qualitative adjectives. In this view, polysemous adjectives belong to a class, for instance, the qualitative-relational class, that is distinct from both the qualitative and the relational classes, typically exhibiting feature values that are in between those of the basic classes. In this section, we report on experiments testing precisely this model for regular polysemy. We will therefore distinguish between ﬁve types of adjectives: qualitative, intensional, relational, polysemous between a qualitative and an intensional reading (intensional-qualitative), and polysemous between a qualitative and a relational reading (qualitative-relational). There is one polysemous class missing (intensional-relational). No cases of polysemy between intensional and relational adjectives were observed in our data. Recall from the previous sections that we cannot reuse an established classiﬁcation, and that there is virtually no previous work on the automatic semantic classiﬁcation of adjectives. The present experiments also aim at testing the overall enterprise of inducing semantic classes from distributional properties for adjectives. Given the exploratory nature of the experiment, we use clustering, an unsupervised technique, to uncover natural groupings of adjectives and test to what extent these correspond to the classes described in the literature.  4.1 Data and Gold Standard The experiments reported in this section are based on an eight million word fragment of the CTILC corpus (Corpus Informatitzat de la Llengua Catalana; Rafel 1994), developed at the Institut d’Estudis Catalans. Each word is associated with its lemma, part of speech, and inﬂectional features, as well as syntactic function. Lemma and morphological information have been manually checked. We automatically added syntactic information with CatCG (Alsina et al. 2002). CatCG is a shallow parser that assigns one or more syntactic functions to each word. In the case of the adjective, CatCG distinguishes between (1) predicate of a copular sentence; (2) predicate in another construction; (3) pre-nominal modiﬁer; (4) post-nominal modiﬁer. As no full dependencies are indicated, the head noun can only be identiﬁed with heuristics. In the experiments, we cluster all adjectives occurring more than ten times in the corpus (a total of 3,521 lemmata), and analyze the results using a subset of the data. This is a randomly chosen 101-lemma gold standard (available in the Appendix). Fifty  585  Computational Linguistics  Volume 38, Number 3  lemmata were chosen token-wise and 50 type-wise to balance high-frequency and lowfrequency adjectives (one lemma was chosen with both methods, so the repetition was removed). Two lemmata were added in a post-hoc fashion, as explained subsequently. The lemmata were annotated by four doctoral students in computational linguistics. The task of the judges was to assign each lemma to one of the ﬁve classes (qualitative, intensional, relational, qualitative-intensional, and qualitative-relational). The instructions for the judges included information about all linguistic characteristics discussed in Section 3, including syntactic and semantic characteristics. The judges had a moderate degree of agreement, comparable to that obtained in other tasks on semantics or discourse, inter-annotator scores ranging between κ = 0.54 and 0.64 (see Artstein and Poesio [2008] for a discussion of agreement measures for computational linguistics). For comparison, Ve´ronis (1998) reported a mean pair-wise weighted κ = 0.43 for a word sense tagging task in French; and Merlo and Stevenson (2001) obtained κ = 0.53–0.66 for the task of classifying English verbs as unergative, unaccusative, or object-drop. Poesio and Artstein (2005) report κ values of 0.63–0.66 (0.45–0.50 if a trivial category is dropped) for the tagging of anaphoric relations. Our judges reported difﬁculties in tagging particular kinds of adjectives, such as deverbal adjectives. This issue will be retaken in Section 4.5. No intensional adjectives were identiﬁed in the data by the judges, and only one intensional-qualitative adjective was identiﬁed. Two intensional lemmata were manually added to be able to minimally track the class. This is clearly insufﬁcient for a quantitative approach, however, so the intensional class is dropped in the alternative classiﬁcation. It is striking that intensional adjectives, which have traditionally been the focus of formal semantic approaches to the semantics of adjectives, constitute a very small class (less than a dozen lemmata are mentioned in the reviewed literature).  4.2 Features We use two sets of distributional features to model adjective behavior: on the one hand, theoretically motivated features (theoretical features for short); on the other hand, features that encode the part-of-speech distribution of a four-word window around the adjective (POS features). The former provide a theoretically informed model of adjectives, because they are cues to the properties of each class as described in the literature. The latter are meant to provide a theory-independent representation of adjectives, to test to what extent the structures obtained with theoretical and POS features are similar. Both sets of features take a narrow context into account (at most ﬁve words to each side of the adjective), because of the limited syntactic behavior of adjectives.  4.2.1 Theoretical Features. Theoretical features model the syntactic and semantic properties of the classes described in Section 3. The features used, together with their mean and standard deviation values (computed on all 3,521 adjectives), are summarized in Table 3. A feature value va,i for an adjective lemma a and a feature i corresponds to the proportion of the occurrences in which i is observed for adjective a over all occurrences of a (see Equation (1); f stands for absolute frequency).  f (a, i)  va,i = f (a)  (1)  586  Boleda, Schulte im Walde, and Badia  Modeling Regular Polysemy in Catalan Adjectives  Table 3 is the translation of Table 1 into shallow cues that can be extracted from a corpus. The mean values in Table 3 are very low, which points to the sparseness of theoretically deﬁned properties such as predicativity or gradability, at least in written texts (oral corpora would presumably yield different values). Also note that standard deviations are higher than mean values, which indicates a high variability in the feature values, something that will be exploited for classiﬁcation. From the discussion in Section 3, the following predictions with respect to the semantic features can be made. (1) In comparison with the other classes, qualitative adjectives should have higher values for features gradable, comparable, copular, predicative, middle values for feature prenominal, and low values for feature adjacent. (2) Relational adjectives should have an almost opposite distribution, with very low values for all features except for adjacent. (3) Intensional adjectives should exhibit very low values for all features except for pre-nominal, for which a very high value is expected. (4) With respect to polysemous adjectives, it can be foreseen that their feature values will be in between those of the basic classes. For instance, an adjective that is polysemous between a qualitative and a relational reading should exhibit a higher value for feature gradable than a monosemous relational adjective, but a lower value than a monosemous qualitative adjective. Figure 1 shows that the predictions just outlined are met to a large extent, showing that the empirical (corpus) data support the theoretical predictions. This graph represents the value distribution of each feature in the form of boxplots. In the boxplots, the rectangles have three horizontal lines, representing the ﬁrst quartile, the median, and the third quartile, respectively. The dotted line at each side of the rectangle stretches to the minimum and maximum values, at most 1.5 times the length of the rectangle. Values that are outside this range are represented as points and termed outliers (Verzani 2005). Note that the scale in Figure 1 does not range from 0 to 1; this is because the data are standardized, as will be explained subsequently.  Table 3 Theoretical features. The mean and SD values are computed on all clustered adjectives. Feature copular accounts for predicative constructions with the copula verbs ser, estar (‘be’). Feature predicative accounts for other predicative constructions, such as Example (4d).  Feature  Textual correlate  Mean SD  gradable degree adverbs, degree sufﬁxation  0.04 0.08  comparable comparative constructions  0.03 0.07  copular  copular predicate syntactic tag  0.06 0.10  predicative predicate syntactic tag  0.03 0.06  pre-nom  pre-nominal modiﬁer syntactic tag  0.04 0.08  adjacent  ﬁrst adjective in a series of two or more 0.03 0.05  587  Computational Linguistics  Volume 38, Number 3  Figure 1 Theoretical features: Feature value distribution in the gold standard. Class labels: I = intensional; IQ = polysemous between intensional and qualitative; Q = qualitative; QR = polysemous between qualitative and relational; R = relational. The differences in value distributions, although signiﬁcant,1 are not sharp, as most of the ranges in the boxes overlap. This affects mainly polysemous classes: Although they show the tendency predicted—exhibiting values that are in between those of the basic classes—they do not present clearly distinct values. The clustering results will be affected by this distribution, as will be discussed in Section 4.5. 4.2.2 POS Features. POS features encode the part-of-speech distribution of a four-word window around the adjective, providing a theory-independent representation of the linguistic behavior of adjectives. To avoid data sparseness, we encode possible POS for each position as a different feature. For instance, for an occurrence of alta (‘tall’) as in Example (17a), the representation would be as in Example (17b). In the example, the target adjective is in boldface, and the relevant word window is in italics. Negative numbers 
This paper presents an attempt at building a large scale distributed composite language model that is formed by seamlessly integrating an n-gram model, a structured language model, and probabilistic latent semantic analysis under a directed Markov random ﬁeld paradigm to simultaneously account for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves signiﬁcantly better translation quality measured by the Bleu score and “readability” of translations when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system. 1. Introduction The Markov chain (n-gram) source models, which predict each word on the basis of the previous n − 1 words, have been the workhorses of state-of-the-art speech recognizers and machine translators that help to resolve acoustic or foreign language ambiguities by placing higher probability on more likely original underlying word strings. Although the Markov chains are efﬁcient at encoding local word interactions, the n-gram model ∗ Kno.e.sis Center and Department of Computer Science and Engineering, Wright State University, Dayton OH 45435. E-mail: tan.6@wright.edu. ∗∗ Kno.e.sis Center and Department of Computer Science and Engineering, Wright State University, Dayton OH 45435. E-mail: zhou.23@wright.edu. † Kno.e.sis Center, Wright State University, Dayton OH 45435. E-mail: lei.zheng@wright.edu. ‡ Kno.e.sis Center and Department of Computer Science and Engineering, Wright State University, Dayton OH 45435. E-mail: shaojun.wang@wright.edu. Submission received: 10 October 2010; revised submission received: 17 October 2011; accepted for publication: 16 November 2011. No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law.  Computational Linguistics  Volume 38, Number 3  clearly ignores the rich syntactic and semantic structures that constrain natural languages. Attempting to increase the order of an n-gram to capture longer range dependencies in natural language immediately runs into the curse of dimensionality (Bengio et al. 2003). The performance of conventional n-gram technology has essentially reached a plateau (Rosenfeld 2000b; Zhang 2008), and it has proven remarkably difﬁcult to improve on n-grams (Jelinek 1991; Jelinek and Chelba 1999). Research groups (Och 2005; Zhang, Hildebrand, and Vogel 2006; Brants et al. 2007; Emami, Papineni, and Sorensen 2007) have shown that using an immense distributed computing paradigm, up to 6-grams, can be trained on up to billions and trillions of tokens, yielding consistent system improvements because of excellent n-gram hit ratios on unseen test data, but Zhang (2008) did not observe much improvement beyond 6-grams. As the machine translation (MT) working groups stated in their ﬁnal report (Lavie et al. 2006, page 3), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Over the past two decades, more sophisticated models have been developed that outperform n-grams; these are mainly the syntactic language models (Della Pietra et al. 1994; Chelba 2000; Chelba and Jelinek 2000; Charniak 2001; Roark 2001; Wang and Harper 2002; Jelinek 2004; Bened´ı and Sa´nchez 2005; Van Uytsel and Compernolle 2005) that effectively exploit sentence-level syntactic structure of natural language, and the topic language models (Saul and Pereira 1997; Gildea and Hofmann 1999; Bellegarda 2000; Wallach 2006) that exploit document-level semantic content. Unfortunately, each of these language models only targets some speciﬁc, distinct linguistic phenomena (Pereira 2000; Rosenfeld 2000a, 2000b); thus, each captures and exploits different aspects of natural language regularity. A natural question we should ask is whether/how we can construct more complex and powerful but computationally tractable language models by integrating many existing/emerging language model components, with each component focusing on speciﬁc linguistic phenomena like syntactic structure, semantic topic, morphology, and pragmatics in complementary, supplementary, and coherent ways (Bellegarda 2001, 2003). Several techniques for combining language models have been investigated. The most commonly used method is linear interpolation (Chen and Goodman 1999; Jelinek and Mercer 1980; Goodman 2001), where each individual model is trained separately and then combined by a weighted linear combination. All of the syntactic structurebased models have used linear interpolation to combine trigrams to achieve further improvement over using their own models alone (Charniak 2001; Chelba and Jelinek 2000; Chelba 2000; Roark 2001). The weights in this case are trained using held-out data. Even though this technique is simple and easy to implement, it does not generally yield very effective combinations (Rosenfeld 1996) because the linear additive form is a strong assumption in capturing subtleties in each of the component models (see more explanation and analysis in Section 6.2 and Appendix A). The second method is based on maximum entropy philosophy, which became very popular in machine learning and natural language processing communities due to the work in Berger, Della Pietra, and Della Pietra (1996), Della Pietra, Della Pietra, and Lafferty (1997), Lau et al. (1993) and Rosenfeld (1996). In fact, for a complete data case, maximum entropy is nothing but maximum likelihood estimation for undirected Markov random ﬁelds (MRFs) (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997). As stated in Wang et al. (2005b), however, there are two weaknesses with maximum entropy approach. The ﬁrst weakness is that this approach can only model distributions over explicitly observed features, but we know there is hidden  632  Tan et al.  A Scalable Distributed Syntactic, Semantic, and Lexical Language Model  information in natural language, such as syntactic structure and semantic topic. The second weakness is that if the statistical model is too complex it becomes intractable to estimate model parameters; computationally very expensive Markov chain Monte Carlo sampling methods (Mark, Miller, and Grenander 1996; Rosenfeld 2000b; Rosenfeld, Chen, and Zhu 2001) would have to be used. One way to overcome the ﬁrst hurdle is to use a preprocessing tool to extract hidden features (e.g., Rosenfeld [1996] used mutual information clustering method to ﬁnd word pair triggers) then combine these triggers with trigrams through a maximum conditional entropy approach to allow the discourse topic to inﬂuence word prediction; Khudanpur and Wu (2000) used Chelba and Jelinek’s structured language model and a word clustering model to extract relevant grammatical and semantic features, then to again combine these features with trigrams through a maximum conditional entropy approach to form a syntactic, semantic, and lexical language model. Wang and colleagues (Wang et al. 2005a; Wang, Schuurmans, and Zhao 2012) have proposed the latent maximum entropy (LME) principle, which extends standard maximum entropy estimation by incorporating hidden dependency structure, but still the LME wouldn’t overcome the second hurdle. The third method is directed Markov random ﬁeld (Wang et al. 2005b) that overcomes both weaknesses in the maximum entropy approach. Wang et al. used this approach to combine trigram, probabilistic context-free grammar (PCFG), and probabilistic latent semantic analysis (PLSA) models; a generalized inside–outside algorithm is derived that alters the wellknown inside–outside algorithm for PCFG (Baker 1979; Lari and Young 1990) with modular modiﬁcation to take into account the effect of n-gram and PLSA while remaining at the same cubic time complexity. When applying this to the Wall Street Journal corpus with 40 million tokens, they achieved moderate perplexity reduction. Because the probabilistic dependency structure in a structured language model (SLM) (Chelba 2000; Chelba and Jelinek 2000) is more complex and powerful than that in a PCFG, Wang et al. (2006) studied the stochastic properties for the composite language model that integrates n-gram, SLM, and PLSA under the directed MRF framework (Wang et al. 2005b) and derived another generalized inside–outside algorithm to train a composite ngram, SLM, and PLSA language model from a general expectation maximization (EM) (Dempster, Laird, and Rubin 1977) algorithm by following Jelinek’s ingenious deﬁnition of the inside and outside probabilities for SLM (Jelinek 2004). Again, the generalized inside–outside algorithm alters Jelinek’s inside–outside algorithm with modular modiﬁcation and has the same sixth order of sentence-length time complexity. Unfortunately, there are no experimental results reported. In this article, we study the same composite n-gram, SLM, and PLSA model under the directed MRF framework as in Wang et al. (2006). The composite n-gram/ SLM/PLSA language model under the directed MRF paradigm is ﬁrst introduced in Section 2. In Section 3, instead of using the sixth order generalized inside–outside algorithm proposed in Wang et al. (2006), we show how to train this composite model via an N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power. We prove the convergence of the N-best list approximate EM algorithm. To resolve the data sparseness problem, we generalize Jelinek and Mercer’s recursive mixing scheme for Markov source (Jelinek and Mercer 1980) to a mixture of Markov chains. To handle large-scale corpora up to a billion tokens, we demonstrate how to implement these algorithms under a distributed computing environment and how to store this language model on a supercomputer. In Section 4, we describe how to use the model for testing. Related works are then summarized and compared in Section 5. Because language modeling is a data-rich and featurerich density estimation problem, there is always a trade-off between approximate error  633  Computational Linguistics  Volume 38, Number 3  and estimation error, thus in Section 6 we conduct comprehensive experiments on corpora with 44 million tokens, 230 million tokens, and 1.3 billion tokens, and compare perplexity results with n-grams (n = 3, 4, 5 respectively) on these three corpora under various situations; drastic perplexity reductions are obtained. We explain why the composite language models lead to better predictive capacity than linear interpolation. The proposed composite language models are applied to the task of re-ranking the N-best list from Hiero (Chiang 2005, 2007), a state-of-the-art parsing-based machine translation system; we achieve signiﬁcantly better translation quality measured by the Bleu score and “readability” of translations. Finally, we draw our conclusions and propose future work in Section 7. The main theme of our approach is “to exploit information, be it syntactic structure or semantic fabric, which involves a fairly high degree of cognition. This is precisely the kind of knowledge that humans naturally and inherently use to process natural language, so it can be reasonably conjectured to represent a key ingredient for success” (Bellegarda 2003, p. 105). In that light, the directed MRF framework, “whose ultimate goal is to integrate all available knowledge sources, appears most likely to harbor a potential breakthrough. It is hoped that the on-going effort conducted in this work to leverage such latent synergies will lead, in the not-too-distant future, to more polyvalent, multi-faceted, effective and tractable solutions for language modeling – this is only beginning to scratch the surface in developing systems capable of deep understanding of natural language” (Bellegarda 2003, p. 105).  2. The Composite n-gram/SLM/PLSA Language Model  Let X denote a set of random variables (Xτ)τ∈Γ taking values in a (discrete) probability space (Xτ)τ∈Γ, where Γ is a ﬁnite set of states. We deﬁne a (discrete) directed Markov random ﬁeld to be a probability distribution P, which admits a recursive factorization if there exist non-negative functions, κτ(·, ·), τ ∈ Γ deﬁned on Xτ × Xpa(τ), such that xτ κτ(xτ, xpa(τ)) = 1 and P has density  p(x) = κτ(xτ, xpa(τ) )  (1)  τ∈Γ  Here pa(τ) denotes the set of parent states of τ. If the recursive factorization refers to a graph, then we have a Bayesian network (Lauritzen 1996). Broadly speaking, however, the recursive factorization can refer to a representation more complicated than a graph with a ﬁxed set of nodes and edges—for example, PCFG and SLM are examples of directed MRFs whose parse tree structure is a random object that can’t be described as a Bayesian network (McAllester, Collins, and Pereira 2004). A key difference between directed MRFs and undirected MRFs is that a directed MRF requires many local normalization constraints whereas an undirected MRF has a global normalization factor. The n-gram (Jelinek 1998; Jurafsky and Martin 2008) language model is essentially a WORD-PREDICTOR, that is, given its entire document history, it predicts the next word wk+1 ∈ V based on the last n–1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 = wk−n+2, · · · , wk and V denotes the vocabulary. The SLM proposed in Chelba and Jelinek (1998, 2000) and Chelba (2000) uses syntactic information beyond the regular n-gram models to capture sentence-level long-range  634  Tan et al.  A Scalable Distributed Syntactic, Semantic, and Lexical Language Model  dependencies. The SLM is based on statistical parsing techniques that allow syntactic analysis of sentences; it assigns a probability p(W, T) to every sentence W and every possible binary parse T. The terminals of T are the words of W with part of speech (POS) tags, and the nodes of T are annotated with phrase headwords and non-terminal labels. Let W be a sentence of length n words to which we have prepended the sentence beginning marker s and appended the sentence end marker /s so that w0 = s and wn+1 = /s . Let Wk = w0, · · · , wk be the word k-preﬁx of the sentence (the words from the beginning of the sentence up to the current position k) and WkTk be the word-parse k-preﬁx. A word-parse k-preﬁx has a set of exposed heads h−m, · · · , h−1 ∈ H, with each head being a pair (headword, non-terminal label), H = V × ONT where ONT denotes the set of non-terminal label (NTlabel), or in the case of a root-only tree (word, POS tag) H = V × O where O denotes the set of POS tags. The exposed heads at a given position k in the input sentence are a function of the word-parse k-preﬁx. The SLM operates left-to-right, building up the parse structure in a bottom–up manner. At any given stage of the word generation by the SLM, the exposed headwords are those headwords of the current partial parse which are not yet part of a higher phrase with a head of its own. An mth order SLM (m-SLM) has three operators to generate a sentence: r The WORD-PREDICTOR predicts the next word wk+1 ∈ V based on the m most recently exposed headwords h−−1m = h−m, · · · , h−1 in the word-parse k-preﬁx with probability p(wk+1|h−−1m), and then passes control to the TAGGER. r The TAGGER predicts the POS tag tk+1 ∈ O to the next word wk+1 based on the next word wk+1 and the POS tags of the m most recently exposed headwords h−−1m (denoted as h−−1m.tag = h−m.tag, · · · , h−1.tag) in the word-parse k-preﬁx with probability p(tk+1|wk+1, h−−1m.tag). r The CONSTRUCTOR builds the partial parse Tk+1 from Tk, wk+1, and tk+1 in a series of moves ending with NULL, where a parse move a is made with probability p(a|h−−1m); a ∈ A={(unary, NTlabel), (adjoin-left, NTlabel), (adjoin-right, NTlabel), NULL}. Depending on an action a = adjoin-right or adjoin-left, the headword h−1 or h−2 is percolated up by one tree level, the indices of the current exposed headwords h−3, h−4, · · · are increased by 1, and these headwords together with h−1 or h−2 become the new exposed headwords. Once the CONSTRUCTOR hits NULL, the headword indexing and current parse structure remain as they are, and the CONSTRUCTOR passes control to the WORD-PREDICTOR. SLM is thus essentially a generalization of a shift-reduce parser (Aho and Ullman 1972) with adjoin corresponding to reduce and predict to shift. (See a detailed description about SLM in Chelba and Jelinek [1998, 2000]; Chelba [2000]; Jelinek [2004]). As an example taken from Jelinek (2004), Figure 1 shows a complete parse where SB/SE is a distinguished POS tag for s / /s respectively, ( s ,TOP) is the only allowed head, and ( /s ,TOP’) is the head of any constituent that dominates /s but not s . In Figure 1, at the time just after the word as is generated, the exposed headwords are “ s SB, show np, has vbz.” The subsequent model actions are: “POStag as, null, predict its, POStag its, null, predict host, POStag host, adjoin-right-np, adjoin-left-pp, adjoin-leftpp, null, predict a, · · · .” 635  Computational Linguistics  Volume 38, Number 3  Figure 1 A complete parse tree by the structured language model. A PLSA model (Hofmann 2001) is a generative probabilistic model of worddocument co-occurrences using the bag-of-words assumption described as follows: r Choose a document d with probability p(d). r SEMANTIZER selects a semantic class g ∈ G with probability p(g|d) where G denotes the set of topics. r WORD-PREDICTOR picks a word w ∈ V with probability p(w|g). Because only one pair of (d, w) is being observed, the joint probability model is a mixture of log-linear models with the expression p(d, w) = p(d) g p(w|g)p(g|d). Typically, the number of documents and the vocabulary size are much larger than the size of latent semantic class variables. Latent semantic class variables therefore function as bottleneck variables to constrain word occurrences in documents. When combining n-gram, m-SLM, and PLSA together to build a composite generative language model under the directed MRF paradigm (Wang et al. 2005b, 2006), the composite language model is simply a complicated generative model that has four operators: WORD-PREDICTOR, TAGGER, CONSTRUCTOR, and SEMANTIZER. The TAGGER and CONSTRUCTOR in SLM and the SEMANTIZER in PLSA remain unchanged; the WORD-PREDICTORs in n-gram, m-SLM, and PLSA, however, are combined to form a stronger WORD-PREDICTOR that generates the next word, wk+1, not only depending on the m most recently exposed headwords h−−1m in the word-parse k-preﬁx but also its n-gram history wkk−n+2 and its semantic content gk+1. The parameter for WORD-PREDICTOR in the composite n-gram/m-SLM/PLSA language model becomes p(w|w−−1n+1h−−1mg). The resulting composite language model has an even more complex dependency structure but with more expressive power than the original SLM. Figure 2 illustrates the structure of a composite n-gram/m-SLM/PLSA language model. The composite n-gram/m-SLM/PLSA language model can be formulated as a rather complex chain-tree-table directed MRF model (Wang et al. 2006) with local 636  Tan et al.  A Scalable Distributed Syntactic, Semantic, and Lexical Language Model  Figure 2  A composite n-gram/m-SLM/PLSA language model where the hidden information is the parse  tree T and semantic content g. The n-gram encodes local word interactions, the m-SLM models  the sentence’s syntactic structure, and the PLSA captures the document’s semantic content;  all interact together to constrain the generation of natural language. The WORD-PREDICTOR  generates the next word wk+1 with probability p(wk+1|h−−1m ), and p(wk+1|gk+1 ), respectively.  p(wk+1 |wkk−n+2 h−−1m gk+1  )  instead  of  p(wk+1|wkk−n+2  ),  normalization constraints for the parameters of each model component, WORDPREDICTOR, TAGGER, CONSTRUCTOR, and SEMANTIZER. That is,  p(w|w−−1n+1h−−1mg) = 1  (2)  w∈V  p(t|wh−−1m.tag) = 1  (3)  t∈O  p(a|h−−1m) = 1  (4)  a∈A  p(g|d) = 1  (5)  g∈G  If we look at the example in Figure 1, for the composite n-gram/m-SLM/PLSA language model there exists a SEMANTIZER’s action to choose a topic g before any WORD-PREDICTOR’s action. Moreover, for m-SLM, its WORD-PREDICTOR predicts the next word, such as a, based on m most recently exposed headwords “ s -SB, show-np, has-vp,” but for the composite model, the WORD-PREDICTOR predicts the next word a based on m most recently exposed headwords “ s -SB, show-np, has-vp,” n-grams “as its host,” and a topic g. These are the only differences between SLM and our proposed composite language model.  3. Training Algorithm  For the composite n-gram/m-SLM/PLSA language model under the directed MRF  paradigm, the likelihood of a training corpus D, a collection of documents, can be  written as         Lˆ(D, p) =    Pp(Wl, Tl, Gl|d) p(d)  (6)  d∈D  l  Gl  Tl  637  Computational Linguistics  Volume 38, Number 3  where (Wl, Tl, Gl|d) denotes the joint sequence of the lth sentence Wl with its parse structure Tl and semantic annotation string Gl in document d. This sequence is produced by a unique sequence of model actions: WORD-PREDICTOR, TAGGER, CONSTRUCTOR, SEMANTIZER moves; its probability is obtained by chaining the probabilities of these moves    Pp(Wl, Tl, Gl|d) = p(g|d)#(g,Wl,Gl,d)  (7)  g∈G   h−1,··· ,h−m∈H    p(w|w−−1n+1 h−−1m g )#(w− −1n+1 wh− −1m g,W l ,Tl ,Gl ,d )  w,w−1,··· ,w−n+1∈V  p(t|wh−−1m.tag)#(t,wh− −1m.tag,Wl,Tl,d)  p(a|h−−1m )#(a,h− −1m,Wl,Tl,d)  t∈O  a∈A  where #(g, Wl, Gl, d) is the count of semantic content g in semantic annotation string Gl of the lth sentence Wl in document d; #(w−−1n+1wh−−1mg, Wl, Tl, Gl, d) is the count of n-grams, its m most recently exposed headwords, and semantic content g in parse Tl and semantic annotation string Gl of the lth sentence Wl in document d; #(twh−−1m.tag, Wl, Tl, d) is the count of tag t predicted by word w and the tags of m most recently exposed headwords in parse tree Tl of the lth sentence Wl in document d; and ﬁnally #(ah−−1m, Wl, Tl, d) is the count of constructor move a conditioning on m exposed headwords h−−1m in parse tree Tl of the lth sentence Wl in document d. Let       L(D, p) =    Pp(Wl, Tl, Gl|d)  (8)  d∈D l  Gl  Tl  then  Lˆ(D, p) = L(D, p) p(d)  (9)  d∈D  Clearly, when maximizing Lˆ(D, p) in Equation (6), p(d) is an ancillary term that is independent of all other data-generating parameters, it is not critical to anything that follows; moreover, when a language model is used to ﬁnd the most likely word sequence in machine translation and speech recognition, this term is useless. Thus, similar to an n-gram language model, we will generally ignore this term and concentrate on optimizing Equation (8) in the subsequent development. The objective of maximum likelihood estimation is to maximize the likelihood L(D, p) with respect to model parameters. For a given sentence, its parse tree and  638  Tan et al.  A Scalable Distributed Syntactic, Semantic, and Lexical Language Model  semantic content are hidden and the number of parse trees grows faster than exponentially with sentence length; Wang et al. (2006) have derived a generalized inside– outside algorithm by applying the standard EM algorithm and considering the auxiliary function  Q(p , p) =  Pp(Tl, Gl|Wl, d) log Pp (Wl, Tl, Gl|d)  (10)  d∈D l Gl Tl  The complexity of this algorithm is sixth order (sentence length), however; thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts (Jelinek and Chelba 1999; Jelinek 2004).  3.1 N-best List Approximate EM  Similar to SLM (Chelba and Jelinek 1998, 2000; Chelba 2000), we adopt an N-best list approximate EM re-estimation with modular modiﬁcations to seamlessly incorporate the effect of n-gram and PLSA components. Instead of maximizing the likelihood L(D, p), we maximize the N-best list likelihood,        max L(D, p, T N ) =   max    Pp(Wl, Tl, Gl|d) (11)  TN  d∈D  l  T  l N  ∈T  N  Gl  Tl ∈T  l N  ,||T  l N  ||=N  where  T  l N  is  a  set  of  N  parse  trees  for  sentence  Wl  in  document  d,  || · ||  denotes  the  cardinality, and T  N is a collection of T  l N  for  sentences  over  entire  corpus  D.  The N-best list approximate EM involves two steps:  1. N-best list search: For each sentence W in document d, ﬁnd N-best parse trees,  TNl = arg max  T  l N  Pp(Wl, Tl, Gl|d), ||T  l N  ||  =  N  Gl  Tl ∈T  l N  and denote TN as the collection of N-best list parse trees for sentences over entire corpus D under model parameter p. 2. EM update: Perform one iteration (or several iterations) of the EM algorithm to estimate model parameters that maximize N-best list likelihood of the training corpus D,       L˜(D, p, TN ) =     Pp(Wl, Tl, Gl|d)  d∈D l  Gl Tl∈TNl ∈TN  639  Computational Linguistics  Volume 38, Number 3  That is, (a) E-step: Compute the auxiliary function of the N-best list likelihood  Q˜ (p , p, TN ) =  Pp(Tl, Gl|Wl, d) log Pp (Wl, Tl, Gl|d)  d∈D l Gl Tl∈TNl ∈TN  (b) M-step: Maximize Q˜ (p , p, TN ) with respect to p to get the new update for p. Iterate steps (1) and (2) until the convergence of the N-best list likelihood. We use Zangwill’s global convergence theorem (Zangwill 1969) to analyze the behavior of convergence of the N-best list approximate EM. First, we deﬁne two concepts needed for Zangwill’s global convergence theorem. A map M is from points of Θ to subsets of Θ is called a point-to-set map on Θ. It is said to be closed at θ if θi → θ, θi ∈ Θ and λi → λ, λi ∈ M(θi) implies λ ∈ M(θ). For a point-to-point map, continuity implies closedness. Then the global convergence theorem (Zangwill 1969) states the following.  Theorem Let M be a point-to-set map (an algorithm) that, given a point θ0 ∈ Θ, generates a sequence {θ∞ i=0} through the iteration θi+1 = M(θi). Let Ω ∈ Θ be the set of ﬁxed points of M. Suppose (i) M is closed over the complement of Ω; (ii) there is a continuous function φ on Θ such that (a) if θ ∈/ Ω, φ(λ) > φ(θ) for all λ ∈ M(θ), and (b) if θ ∈ Ω, φ(λ) ≥ φ(θ) for all λ ∈ M(θ). Then all the limit points of {θi} are in Ω and φ(θi) converges monotonically to φ(θ) for some θ ∈ Ω.  Proof This theorem has been used by Wu (1983) to prove the convergence of a standard EM algorithm (Dempster, Laird, and Rubin 1977). We now use this theorem to show that the N-best list approximate EM algorithm globally converges to the stationary points of the N-best list likelihood. We encounter one difﬁculty at this point, however, due to the maximization operator in Equation (11); after each iteration the N-best list may have been changed, therefore the set of data presented for the estimation of model parameters may be different from the previous one. Nevertheless, we prove the convergence of the N-best list approximate EM algorithm by checking whether it satisﬁes two conditions in Zangwill’s global convergence theorem. Because the composite model is essentially a mixture model of a curved exponential family through a complex hierarchy, there is a closed form solution for the Q˜ (p , p, TN ) function irrespective of the N-best list parse trees, so the N-best list approximate EM algorithm is a one-to-one map. Because Q˜ (p , p, TN ) is continuous in both p and p, the map is closed, thus condition (i) is satisﬁed. To check condition (ii), we need to verify that the N-best list likelihood as a function of p satisﬁes the properties of φ(θ) in condition (ii). Let TˇN and T¯N be the two collections 640  Tan et al.  A Scalable Distributed Syntactic, Semantic, and Lexical Language Model  of N-best list parse trees for sentences over entire corpus D under two model parameters pˇ and p¯, respectively:  TˇN = arg max L(D, pˇ, T N )  (12)  TN  T¯N = arg max L(D, p¯, T N )  (13)  TN  and let p¯ be the closed form solution of maximizing Q˜ (p , pˇ, TˇN ) with respect to p , that is,  p¯ = arg max Q˜ (p , pˇ, TˇN )  (14)  p  Then  max L(D, p¯, T N ) ≥ L˜(D, p¯, TˇN )  (15)  TN  ≥ L˜(D, pˇ, TˇN )  (16)  ≥ max L(D, pˇ, T N )  (17)  TN  The inequality in Equation (15) is strict unless TˇN = T¯N, which results in p¯ ∈ M(p¯).  Using results proven by Wu (1983), we know that when pˇ is not a stationary point of the  N-best  list  likelihood  or  pˇ ∈/ M(pˇ),  ∂L˜ (D,pˇ,TN ) ∂pˇ  =  ∂Q˜ (p ,pˇ,TˇN ) ∂pˇ  = 0,  Q˜ (p¯, pˇ, TˇN ) > Q˜ (pˇ, pˇ, TˇN ),  thus the inequality in Equation (16) is strict. Finally, the inequality in Equation (17) is  strict unless pˇ ∈ M(pˇ). Thus condition (ii) is satisﬁed.  This completes the proof that the N-best list approximate EM algorithm mono-  tonically increases the N-best list likelihood and converges in the sense of Zangwill’s  global convergence.  In the following, we formally derive the N-best list approximate EM algorithm with  linear sentence length time complexity.  3.1.1 N-best List Search Strategy. For each sentence W in document d, instead of scanning all the hidden events (both allowed parse trees and semantic annotation strings) we restrict the algorithm to operate with N-best hidden events. We ﬁnd that, for each document, a large number of topics should be pruned and only a small set of allowed topics should be kept due to the considerations of both computational time and resource demand, otherwise we have to use many more machines to store WORD-PREDICTOR’s parameters. We can either ﬁnd both the N-best parses for each sentence and N-best topics for each document simultaneously or separately. The latter is much preferred, because the ﬁrst case is much more computationally expensive. To extract the N-best topics, we run an EM algorithm for a PLSA model on training corpus D, then keep the N most likely topics (denoted as Gd) according to the values of p(g|d); the rest of the topics are purged. To extract the N-best parse trees, we adopt a synchronous, multi-stack search strategy that is similar to the one in Chelba and Jelinek (1998, 2000) and Chelba (2000), which involves a set of stacks storing partial parses of the most likely ones for a given preﬁx Wk and the less probable parses are purged. Each stack contains  641  Computational Linguistics  Volume 38, Number 3  hypotheses (partial parses) that have been constructed by the same number of WORD- PREDICTOR and the same number of CONSTRUCTOR operations. The hypotheses in each stack are ranked according to the log(Pp(Wk, Tk|d)) score with the highest on top, where Pp(Wk, Tk|d) = Gk Pp(Wk, Tk, Gk|d) and the Wk, Tk, Gk denote the joint sequence of preﬁx Wk = w0, w1 · · · , wk with its parse structure Tk and semantic annotation string Gk = g1, · · · , gk, gi ∈ Gd, i = 1, · · · , k in document d. This sequence is produced by a unique sequence of model actions: WORD-PREDICTOR, TAGGER, CONSTRUCTOR, and SEMANTIZER moves. Its probability is obtained by chaining the probabilities of these moves. The value of Pp(Wk, Tk|d) is computed recursively from Pp(Wk−1, Tk−1|d) by the following formula:      Pp(Wk, Tk|d) = Pp(Wk−1, Tk−1|d)   p(wk|wkk−−1n+1h−−1mgk )  gk ∈Gd  p(gk|d)  gi∈Gd p(gi|d)  (18)  p(tk|wk, h−−1m.tag)p(Tk−1,k|Wk−1Tk−1, wk, tk )  where Wk−1Tk−1 is the word-parse (k − 1)-preﬁx; wk is the kth word predicted by WORD-PREDICTOR; tk is the tag assigned to wk by the TAGGER; Tk−1,k is the incremental parse structure that generates Tk = Tk−1||Tk−1,k when attached to Tk−1, (this is the parse structure built on top of Tk−1 and the newly predicted word wk); the || notation stands for concatenation. Finally, p(Tk−1,k|Wk−1Tk−1, wk, tk) is the product of the probabilities of a series of CONSTRUCTOR moves in Tk−1,k to form Tk. Because the topics are pruned to Gd, the probability of the SEMANTIZER is normalized to ensure a proper probability distribution. A stack vector consists of the ordered set of stacks containing partial parses with the same number of WORD-PREDICTOR operations but a different number of CONSTRUCTOR operations. In WORD-PREDICTOR and TAGGER operations, some hypotheses are discarded due to the maximum number of hypotheses that the stack can contain at any given time. In the CONSTRUCTOR operation, the resulting hypotheses are discarded due to either ﬁnite stack size or the log-probability threshold (the maximum tolerable difference between the log-probability score of the top-most hypothesis and the bottom-most hypothesis at any given state of the stack). The synchronous, multi-stack search strategy is a greedy best-ﬁrst search algorithm, one of the local heuristic search procedures that does not use future cost estimates to guide the search and thus does not guarantee that the N-best list parse trees are a global optimal solution (Russell and Norvig 2010). In practice, however, we ﬁnd that the N-best list approximate EM algorithm does converge within several iterations.  3.1.2 EM Update. Once we have both the N-best parse trees for each sentence in document d and the N-best topics for document d, we derive the EM algorithm to estimate model parameters. Maximizing Q˜ (p , p, TN ) with respect to p leads to re-estimated parameters of the composite model, which are nothing but the following normalized conditional expected counts:  p (w|w−−1n+1h−−1mg) ∝  Pp(Tl, Gl|Wl, d)#(w−−1n+1wh−−1mg, Wl, Tl, Gl, d) (19)  d∈D l Gl Tl∈TNl ∈TN  642  Tan et al.  A Scalable Distributed Syntactic, Semantic, and Lexical Language Model  p (t|wh−−1m.tag) ∝  Pp(Tl|Wl, d)#(twh−−1m.tag, Wl, Tl, d)  (20)  d∈D l Tl∈TNl ∈TN  p (a|h−−1m)) ∝  Pp(Tl|Wl, d)#(ah−−1m, Wl, Tl, d)  (21)  d∈D l Tl∈TNl ∈TN  p (g|d) ∝  Pp(Tl, Gl|Wl, d)#(g, Wl, Gl, d)  (22)  d∈D l Gl Tl∈TNl ∈TN  In the E-step, we use Equations (19)–(22) to compute the expected count of each model parameter over sentence Wl in document d in the training corpus D. In the full case where the number of parse trees grows faster than exponentially with sentence length, we use Jelinek-style recursive formulas in the generalized inside–outside algorithm (Jelinek 2004) to handle the tree structure and describe the weighted forest of possible derivations (Wang et al. 2006). In the N-best list case considered in this paper, however, we just enumerate each parse tree in the N-best list and compute the expected posterior count for each parse tree. For the WORD-PREDICTOR and the SEMANTIZER, we use Equations (19) and (22) and note that there is a sum over semantic annotation sequence Gl where the number of possible semantic annotation sequences is exponential. We use forward–backward recursive formulas reminiscent of those in hidden Markov models to compute the expected counts. To be more speciﬁc, for each parse Tl ∈ TNl , we deﬁne the forward vector αl(g|d) to be  αlk+1(g|d) = Pp(Wkl , Tkl , wkk−n+2wk+1h−−1mg, Glk|d)  (23)  Glk  = Pp(Wkl , Tkl , wkk−n+2wk+1h−−1mg|d)  = Pp(Wkl , Tkl |d)p(wk+1|wkk−n+2h−−1mg, d)  p(gk+1|d) gi∈Gd p(gi|d)  where Wkl is the word k-preﬁx for sentence Wl, and Tkl is the parse for k-preﬁx. It is easy to see that the forward vector αl(g|d) can be recursively computed in a forward manner using Equation (18) as      αlk+1(g|d) =   αlk(gk|d) p(tk|wk, h−−1m.tag)p(Tkl −1,k|Wkl−1Tkl −1, wk, tk ) (24)  gk ∈Gd  p(wk+1|wkk−n+2h−−1mg, d)  p(gk+1, d) gi∈Gd p(gi|d)  We deﬁne the backward vector βl(g|d) to be  βlk+1(g|d) =  Pp(Wkl +1,·, Tkl +1,·, Glk+1,·|wkk−n+2wk+1h−−1mg, d)  Glk+1,·  (25)  where Wkl+1,· = wlk+2, · · · , /s is the subsequence after word wlk+1 in sentence Wl, Tkl +1,· is the incremental parse structure after the parse structure Tkl +1 of word (k + 1)-preﬁx  643  Computational Linguistics  Volume 38, Number 3  Wkl+1 that generates parse tree Tl, Tl = Tkl +1||Tkl +1,·, and Glk+1,· = gk+2, · · · , is the semantic subsequence in Gl relevant to Wkl+1,·. Again it is easy to see that the backward vector βl(g|d) can be recursively computed in a backward manner as  βlk+1(g|d) = p(tk+1|wk+1, h−−1m.tag)p(Tkl ,k+1|Wkl Tkl , wk+1, tk+1 )  (26)  p(wk+2|wkk−n+3h−−1mgk+2, d) gk+2 ∈Gd  p(gk+2|d) gi∈Gd p(gi|d)  Pp(Wkl +2,·, Tkl +2,·, Glk+2,·|wkk+−1n+3wk+2h−−1mgk+2, d) Glk+2,·  = p(tk+1|wk+1, h−−1m.tag)p(Tkl ,k+1|Wkl Tkl , wk+1, tk+1 )  p(wk+2|wkk−n+3h−−1mgk+2, d) gk+2 ∈Gd  p(gk+2|d) gi∈Gd p(gi|d)  βlk+2  (gk+2  |d  )  Then, the expected count of w−−1n+1wh−−1mg for the WORD-PREDICTOR on sentence Wl in document d is  Pp(Tl, Gl|Wl, d)#(w−−1n+1wh−−1mg, Wl, Tl, Gl, d)  (27)  Gl Tl∈TNl ∈TN  =  Pp(Tl, Gl, Wl|d)#(w−−1n+1wh−−1mg, Wl, Tl, Gl, d)/Pp(Wl|d)  Gl Tl∈TNl ∈TN  =  αlk+1(g|d)βlk+1(g|d)δ(wkk−n+2wk+1h−−1mgk+1 = w−−1n+1wh−−1mg)/Pp(Wl|d)  lk  where Pp(Wl|d) = Gl Tl∈TNl ∈TN Pp(Tl, Gl, Wl|d) = Tl∈TNl ∈TN Pp(Tl, Wl|d), Pp(Tl, Wl|d) is recursively computed by Equation (18) through traversing the lth parse tree Tl ∈ TNl of sentence Wl from left to right, and δ(·) is an indicator function. The expected count of g for the SEMANTIZER on sentence Wl in document d is  Pp(Tl, Gl|Wl, d)#(g, Wl, Gl, d)  (28)  Gl Tl∈TNl ∈TN  =  αlk+1 (g|d )βlk+1 (g|d )p(wk+1 |wkk−n+2 h−−1m g )/Pp (Wl |d )  lk  For the TAGGER and the CONSTRUCTOR, we use Equations (20) and (21), and the expected count of each event of twh−−1m.tag and ah−−1m over parse Tl of sentence Wl in document d is the real count appearing in parse tree Tl of sentence Wl in document d times the conditional distribution Pp(Tl|Wl, d) = Pp(Tl, Wl|d)/ Tl∈T l Pp(Tl, Wl|d)—that is, Pp(Tl|Wl, d)#(twh−−1m.tag, Wl, Tl, d) and Pp(Tl|Wl, d)#(ah−−1m, Wl, Tl, d), respectively. When only SLM is considered, the expected count for each model component, WORD-PREDICTOR, TAGGER, and CONSTRUCTOR, over parse Tl of sentence Wl in document d is the real count that appeared in parse Tl of sentence Wl in document d  644  Tan et al.  A Scalable Distributed Syntactic, Semantic, and Lexical Language Model  times the posterior probability Pp(Tl|Wl, d), as is done in Chelba and Jelinek (1998, 2000) and Chelba (2000). In the M-step, the recursive linear interpolation scheme (Jelinek and Mercer 1980) is used to obtain a smooth probability estimate for each model component (WORDPREDICTOR, TAGGER, and CONSTRUCTOR). The TAGGER and CONSTRUCTOR are conditional probabilistic models of the type p(u|z1, · · · , zn) where u, z1, · · · , zn belong to a mixed set of words, POS tags, NTtags, and CONSTRUCTOR actions (u only); and z1, · · · , zn form a linear Markov chain. The recursive mixing scheme is the standard one among relative frequency estimates of different orders k = 0, · · · , n and has been explained in Chelba and Jelinek (1998, 2000) and Chelba (2000). The WORD-PREDICTOR kisi,nhdoswoefvceorn, taexcto,nwd−−it1nio+n1a, lh−−p1mro, baanbdiligs—ticemacohdfeolrpm(ws |aw−−lin1n+ea1hr−−M1mgar)kwovhecrheatihne.rTehaeremtohdreeel has a combinatorial number of relative frequency estimates of different orders among three linear Markov chains. We generalize Jelinek and Mercer’s (1980) original recursive mixing scheme to handle the situation where the context is a mixture of Markov chains. The factored language (FL) model (Bilmes and Kirchhoff 2003) is close to the smoothing technique we propose here, the major difference is that FL considers all possible combination of the context of conditional probability that can be concisely represented by a factor graph, whereas our approach strictly respects the order of Markov chains for word sequence and headword sequence because we believe natural language tightly follows these orders; moreover, where FL uses a backoff technique, we use linear interpolation. Consider a composite trigram/2-SLM/PLSA language model. Figure 3 illustrates a lattice formed of all possible conditional probabilistic models and relative frequency  Figure 3 Recursive linear interpolation lattice to estimate WORD-PREDICTOR p(w|w−2w−1h−2h−1g) of the composite trigram/2-SLM/PLSA language model, where U is the vocabulary in which the predicted random variable w takes values and p(U ) denotes uniform distribution of U. The lattice is formed by three linear Markov chains, w−2w−1, h−2h−1, and g. Starting from p(U ), each vertex is visited in a bottom–up, back to front, and right to left order. 645  Computational Linguistics  Volume 38, Number 3  estimates of different orders along each of the three linear Markov chains. Each vertex in the lattice represents a conditional probabilistic model that is a linear interpolation of vertices having directed arcs pointing to this vertex and its relative frequency estimate; the linear interpolation coefﬁcients are the weights of directed arcs. For example, the WORD-PREDICTOR p(w|w−2w−1h−2h−1g) is a linear interpolation of three conditional probabilistic models, p(w|w−1h−2h−1g), p(w|w−2w−1h−1g), p(w|w−2w−1h−2h−1), and their relative frequency estimate f (w|w−2w−1h−2h−1g),  p(w|w−2w−1h−2h−1g) = λw(w−2w−1h−2h−1g) · p(w|w−1h−2h−1g)  (29)  +λh(w−2w−1h−2h−1g) · p(w|w−2w−1h−1g)  +λg(w−2w−1h−2h−1g) · p(w|w−2w−1h−2h−1 )  +(1 − λw(w−2w−1h−2h−1g) − λh(w−2w−1h−2h−1g)  −λg(w−2w−1h−2h−1g)) · f (w|w−2w−1h−2h−1g)  where λw(w−2w−1h−2h−1g), λh(w−2w−1h−2h−1g), and λg(w−2w−1h−2h−1g) are non-  negative context-dependent interpolation coefﬁcients with a sum of less than 1;  f (w|w−2w−1h−2h−1g) =  ; C(w−2 w−1 wh−2 h−1 g ) C(w−2 w−1 h−2 h−1 g )  and  C(w−2w−1wh−2h−1g)  is  the  expected  count of the event w−2w−1wh−2h−1g that is extracted from the training cor-  pus by the E-step of the N-best approximate EM algorithm, C(w−2w−1h−2h−1g) = w∈U C(w−2w−2wh−2h−1g). The linear interpolation coefﬁcients are grouped into equivalence classes (tied) based on the range into which the count falls; the count ranges  for each equivalence class, “buckets,” are set such that a statistically sufﬁcient number  of events fall within that range. In our experiments, we set the count ranges to be the intervals of 2i, i = 0, 1, · · · , 10 (i.e., 0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, and ∞).  These “tied” interpolation weights are determined by the maximum likelihood estimate  from cross-validation data through the EM algorithm (Dempster, Laird, and Rubin 1977) where we use a public available parser in the openNLP software1 to parse sentences in  cross-validation data, and we run LSA to extract N most likely topics for each document  in cross-validation data, then we gather joint counts for each model component, WORD-  PREDICTOR, TAGGER, CONSTRUCTOR used to determine interpolation weights.  In the M-step, assuming that the count ranges and the corresponding interpolation  values for each order are kept ﬁxed to their initial values, the only parameters to be  re-estimated using the EM algorithm are the maximal order counts for each model  component. The interpolation scheme outlined here is then used to obtain a smooth  probability estimate for each model component.  3.2 Follow-up EM As explained in Chelba and Jelinek (2000) and Chelba (2000), for the SLM component a large fraction of the partial parse trees that can be used for assigning probability to the next word do not survive in the synchronous, multi-stack search strategy, thus they are not used in the N-best approximate EM algorithm for the estimation of WORDPREDICTOR to improve its predictive power. To remedy this weakness, we estimate a  
∗ CLiPS, University of Antwerp, Prinsstraat 13, B-2000 Antwerpen, Belgium. E-mail: roser.morante@ua.ac.be. ∗∗ Computational Linguistics, Saarland University, Postfach 15 11 50, D-66041 Saarbru¨ cken, Germany. E-mail: csporled@coli.uni-sb.de. Submission received: 5 April 2011; revised submission received: 18 January 2012; accepted for publication: 24 January 2012. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 2  illustrate this statement with the following examples, where the event LAY OFF(GM, workers) is presented with different extra-propositional meanings: (1) a. GM will lay off workers. b. A spokesman for GM said GM will lay off workers. c. GM may lay off workers. d. The politician claimed that GM will lay off workers. e. Some wish GM would lay of workers. f. Will GM lay off workers? g. Many wonder whether GM will lay off workers. Generally speaking, modality is a grammatical category that allows the expression of aspects related to the attitude of the speaker towards her statements in terms of degree of certainty, reliability, subjectivity, sources of information, and perspective. We understand modality in a broad sense, which involves related concepts like “subjectivity”, “hedging”, “evidentiality”, “uncertainty”, “committed belief,” and “factuality”. Negation is a grammatical category that allows the changing of the truth value of a proposition. A more detailed deﬁnition of these concepts with examples will be presented in Sections 2 and 3. Modality and negation are challenging phenomena not only from a theoretical perspective, but also from a computational point of view. So far two main tasks have been addressed in the computational linguistics community: (i) the detection of various forms of negation and modality and (ii) the resolution of the scope of modality and negation cues. Whereas modality and negation tend to be lexically marked, the class of markers is heterogeneous, especially in the case of modality. Determining whether a sentence is speculative or whether it contains negated concepts cannot be achieved by simple lexical look-up of words potentially indicating modality or negation. Modal verbs like might are prototypical modality markers, but they can be used in multiple senses. Multiword expressions can also express modality (e.g., this brings us to the largest of all mysteries or little was known). Modality and negation interact with mood and tense markers, and also with each other. Finally, discourse factors also add to the complexity of these phenomena. Incorporating information about modality and negation has been shown to be useful for a number of applications such as recognizing textual entailment (de Marneffe et al. 2006; Snow, Vanderwende, and Menezes 2006; Hickl and Bensley 2007), machine translation (Baker et al. 2010), trustworthiness detection (Su, Huang, and Chen 2010), classiﬁcation of citations (Di Marco, Kroon, and Mercer 2006), clinical and biomedical text processing (Friedman et al. 1994; Szarvas 2008), and identiﬁcation of text structure (Grabar and Hamon 2009). This overview is organized as follows: Sections 2 and 3 deﬁne modality and negation, respectively. Section 4 gives details of linguistic resources annotated with various aspects of negation and modality. We also discuss properties of the different annotation schemes that have been proposed. Having discussed the linguistic basis as well as the available resources, the remainder of the article then provides an overview of automated methods for dealing with modality and negation. Most of the work in this area has been carried out at the sentence or predicate level. Section 5 discusses various methods for detecting speculative sentences. This is only a ﬁrst step, however. For a more ﬁne-  224  Morante and Sporleder  Modality and Negation  grained analysis, it is necessary to deal with modality and negation on a sub-sentential (i.e., predicate) level. This is addressed in Section 6, which also discusses various methods for the important task of scope detection. Section 7 then moves on to work on detecting negation and modality at a discourse level, that is, in the context of recognizing contrasts and contradictions. Section 8 takes a closer look at dealing with positive and negative opinions and summarizes studies in the ﬁeld of sentiment analysis that have explicitly modeled modality and negation. Section 9 provides an overview of the articles in this special issue. Finally, Section 10 concludes this article by outlining some of the remaining challenges. Some notational conventions should be clariﬁed. In the literature, the afﬁxes, words or multiword expressions that express modality and negation have been referred to as triggers, signals, markers, and cues. Here, we will refer to them as cues and we will mark them in bold in the examples. The boundaries of their scope will be marked with square brackets.  2. Modality From a theoretical perspective, modality can be deﬁned as a philosophical concept, as a subject of the study of logic, or as a grammatical category. There are many definitions and classiﬁcations of modal phenomena. Even if we compiled an exhaustive and precise set of existing deﬁnitions, we would still be providing a limited view on what modality is, because, as Salkie, Busuttil, and van der Auwera (2009, page 7) put it: . . . modality is a big intrigue. Questions erstwhile considered solved become open questions again. New observations and hypotheses come to light, not least because the subject matter is changing. Deﬁning modality from a computational linguistics perspective for this special issue becomes even more difﬁcult because several concepts are used to refer to phenomena that are related to modality, depending on the task at hand and the speciﬁc phenomena that the authors address. To mention some examples, research focuses on categorizing modality, on committed belief tagging, on resolving the scope of hedge cues, on detecting speculative language, and on computing factuality. These concepts are related to the attitude of the speaker towards her statements in terms of degree of certainty, reliability, subjectivity, sources of information, and perspective. Because this special issue focuses on the computational treatment of modality, we will provide a general theoretical description of modality and the related concepts mentioned in the computational linguistics literature at the cost of offering a simpliﬁed view of these concepts. Jespersen (1924, page 329) attempts to place all moods in a logically consistent system, distinguishing between “categories containing an element of will” and “categories containing no element of will”—later named as propositional modality and event modality by Palmer (1986). Lyons (1977, page 793) describes epistemic modality as concerned with matters of knowledge and belief, “the speaker’s opinion or attitude towards the proposition that the sentence expresses or the situation that the proposition describes.” Palmer (1986, page 8) distinguishes propositional modality, which is “concerned with the speaker’s attitude to the truth-value or factual status of the proposition”  225  Computational Linguistics  Volume 38, Number 2  as in Example (2a), and event modality, which “refers to events that are not actualized, events that have not taken place but are merely potential” as in Example (2b): (2) a. Kate must be at home now. b. Kate must come in now. Within propositional modality, Palmer deﬁnes two types: epistemic, used by speakers “to express their judgement about the factual status of the proposition,” and evidential, used “to indicate the evidence that they have for its factual status” (Palmer 1986, 8– 9). He also deﬁnes two types of event modality: deontic, which relates to obligation or permission and to conditional factors “that are external to the relevant individual,” and dynamic, where the factors are internal to the individual (Palmer 1986, pages 9– 13). Additionally, Palmer indicates other categories that may be marked as irrealis and may be found in the mood system: future, negative, interrogative, imperative-jussive, presupposed, conditional, purposive, resultative, wishes, and fears. Palmer explains how modality relates to tense and aspect: The three categories are concerned with the event reported by the utterance, whereas tense is concerned with the time of the event and aspect is “concerned with the nature of the event in terms of its internal temporal constituency” (Palmer 1986, pages 13–16). From a philosophical standpoint, von Fintel (2006) deﬁnes modality as “a category of linguistic meaning having to do with the expression of possibility and necessity.” In this sense “a modalized sentence locates an underlying or prejacent proposition in the space of possibilities.” Von Fintel describes several types of modal meaning (alethic, epistemic, deontic, bouletic, circumstantial, and teleological), some of which are introduced by von Wright (1951), and shows that modal meaning can be expressed by means of several types of expressions, such as modal auxiliaries, semimodal verbs, adverbs, nouns, adjectives, and conditionals. Within the modal logic framework several authors provide a more technical approach to modality. Modal logic (von Wright 1951; Kripke 1963) attempts to represent formally the reasoning involved in expressions of the type it is necessary that ... and it is possible that ... starting from a weak logic called K (Garson 2009). Taken in a broader sense, modal logic also aims at providing an analysis for expressions of deontic, temporal, and doxastic logic. Within the modal logic framework, modality is analyzed in terms of possible worlds semantics (Kratzer 1981). The initial idea is that modal expressions are considered to express quantiﬁcation over possible worlds. Kratzer (1981, 1991), however, argues that modal expressions are more complex than quantiﬁers and that their meaning is context-dependent. Recent work on modality in the framework of modal logic is presented by Portner (2009, pages 2–8), who groups modal forms into three categories: sentential modality (“the expression of modal meaning at the level of the whole sentence”); sub-sentential modality (“the expression of modal meaning within constituents smaller than a full clause”); and discourse modality (“any contribution to meaning in discourse which cannot be accounted for in terms of a traditional semantic framework”). From a typological perspective, the study of modality seeks to describe how the languages of the world express different types of modality (Palmer 1986; van der Auwera and Plungian 1998). Knowing how modality is expressed across languages is relevant for the computational linguistics community, not only because it is essential for developing automated systems for languages other than English, but also because  226  Morante and Sporleder  Modality and Negation  it throws some light on the underlying phenomena that might be beneﬁcial for the development of novel methods for dealing with modality. Concepts related to modality that have been studied in computational linguistics are: hedging, evidentiality, uncertainty, factuality, and subjectivity. The term hedging is originally due to Lakoff (1972, page 195), who describes hedges as “words whose job is to make things more or less fuzzy.” Lakoff starts from the observation that “natural language concepts have vague boundaries and fuzzy edges and that, consequently, natural language sentences will very often be neither true, nor false, nor nonsensical, but rather true to a certain extent and false to a certain extent, true in certain aspects and false in certain aspects” (Lakoff 1972, page 183). In order to deal with this aspect of language, he extends the classical propositional and predicate logic to fuzzy logic and focuses on the study of hedges. Hyland (1998) studies hedging in scientiﬁc texts. He proposes a pragmatic classiﬁcation of hedge expressions based on an exhaustive analysis of a corpus. The catalogue of hedging cues includes modal auxiliaries, epistemic lexical verbs, epistemic adjectives, adverbs, nouns, and a variety of non-lexical cues. Evidentiality is related to the expression of the information source of a statement. As Aikhenvald (2004, page 1) puts it: In about a quarter of the world’s languages, every statement must specify the type of source on which it is based [...]. This grammatical category, whose primary meaning is information source, is called ‘evidentiality’. This grammatical category was already introduced by Boas (1938), and has been studied afterwards, although less than modality. There is no agreement on whether it should be a subcategory of modality (Palmer 1986; de Haan 1995) or a category by itself (de Haan 1999; Aikhenvald 2004). A broader deﬁnition relates evidentiality to the expression of the speaker’s attitude towards the information being presented (Chafe 1986). Ifantidou (2001, page 5) considers that the function of evidentials is to indicate the source of knowledge (observation, hearsay, inference, memory) on which a statement is based and the speaker’s degree of certainty about the proposition expressed. Certainty is a type of subjective information that can be conceived of as a variety of epistemic modality (Rubin, Liddy, and Kando 2005). Here we take their deﬁnition (page 65): . . . certainty is viewed as a type of subjective information available in texts and a form of epistemic modality expressed through explicitly-coded linguistic means. Such devices [...] explicitly signal presence of certainty information that covers a full continuum of writer’s conﬁdence, ranging from uncertain possibility and withholding full commitment to statements. Factuality involves polarity, epistemic modality, evidentiality, and mood. It is deﬁned by Saur´ı (2008, page 1) as: . . . the level of information expressing the commitment of relevant sources towards the factual nature of eventualities in text. That is, it is in charge of conveying whether eventualities are characterized as corresponding to a fact, to a possibility, or to a situation that does not hold in the world. Factuality can be expressed by several linguistic means: negative polarity particles, modality particles, event-selecting predicates which project factuality information on the events denoted by their arguments (claim, suggest, promise, etc.), and syntactic  227  Computational Linguistics  Volume 38, Number 2  constructions involving subordination. The factuality of a speciﬁc event can change during the unfolding of the text. As described in Saur´ı and Pustejovsky (2009), depending on the polarity, events are depicted as either facts or counterfacts. Depending on the level of uncertainty combined with polarity, events will be presented as possibly factual (3a) or possibly counterfactual (3b). (3) a. The United States may extend its naval quarantine to Jordan’s Red Sea port of Aqaba. b. They may not have enthused him for their particular brand of political idealism. The term subjectivity is introduced by Banﬁeld (1982). Work on subjectivity in computational linguistics is initially due to Wiebe, Wilson, and collaborators (Wiebe 1994; Wiebe et al. 2004; Wiebe, Wilson, and Cardie 2005; Wilson 2008; Wilson et al. 2005; Wilson, Wiebe, and Hwa 2006) and focuses on learning subjectivity from corpora. As Wiebe et al. (2004, page 279) put it: Subjective language is language used to express private states in the context of a text or conversation. Private state is a general covering term for opinions, evaluations, emotions, and speculations. Subjectivity is expressed by means of linguistic expressions of various types from words to syntactic devices that are called subjective elements. Subjective statements are presented from the point of view of someone, who is called the source. As Wiebe et al. (2004) highlight, subjective does not mean not true. For example, in Example (4a), criticized expresses subjectivity, but the events CRITICIZE and SMOKE are presented as being true. Not all events contained in subjective statements need to be true, however. Modal expressions can be used to express subjective language, as in Example (4b), where the modal cue perhaps combined with the future tense is used to present the event FORGIVE as non-factual. (4) a. John criticized Mary for smoking. b. Perhaps you’ll forgive me for reposting his response. Modality and evidentiality are grammatical categories, whereas certainty, hedging, and subjectivity are pragmatic positions, and event factuality is a level of information. In this special issue we will use the term modality in a broad sense, similar to the extended modality of Matsuyoshi et al. (2010), which they use to refer to “modality, polarity, and other associated information of an event mention.” Subjectivity in the general sense and opinion are beyond the scope of this special issue, however, because research in these areas focuses on different topics and already has a well deﬁned framework of reference. Modality-related phenomena are not rare. According to Light, Qiu, and Srinivasan (2004), 11% of sentences in MEDLINE contain speculative language. Vincze et al. (2008) report that around 18% of sentences occurring in biomedical abstracts are speculative. Nawaz, Thompson, and Ananiadou (2010) ﬁnd that around 20% of the events in a biomedical corpus belong to speculative sentences and that 7% of the events are expressed with some degree of speculation. Szarvas (2008) notes that a signiﬁcant proportion of the gene names mentioned in a corpus of biomedical articles appear in  228  Morante and Sporleder  Modality and Negation  speculative sentence (638 occurences out of a total of 1,968). This means that approximately 1 in every 3 genes should be excluded from the interaction detection process. Rubin (2006) reports that 59% of the sentences in a corpus of 80 articles from The New York Times were identiﬁed as epistemically modalized. 3. Negation Negation is a complex phenomenon that has been studied from many perspectives, including cognition, philosophy, and linguistics. As described by Lawler (2010, page 554), cognitively, negation “involves some comparison between a ‘real’ situation lacking some particular element and an ‘imaginal’ situation that does not lack it.” In the logic formalisms, “negation is the only signiﬁcant monadic functor,” whose behavior is described by the Law of Contradiction that asserts that no proposition can be both true and not true. In natural language, negation functions as an operator, like quantiﬁers and modals. A main characteristic of operators is that they have a scope, which means that their meaning affects other elements in the text. The affected elements can be located in the same clause (5a) or in a previous clause (5b). (5) a. We didn’t ﬁnd the book. b. We thought we would ﬁnd the book. This was not the case. The study of negation in philosophy started with Aristotle, but nowadays is still a topic that generates a considerable number of publications in the ﬁeld of philosophy, logic, psycholinguistics, and linguistics. Horn (1989) provides an extensive description of negation from a historic perspective and an analysis of negation in relation to semantic and pragmatic phenomena. Tottie (1991) studies negation as a grammatical category from a descriptive and quantitative point of view, based on the analysis of empirical material. She deﬁnes two main types of negation in natural language: rejections of suggestions and denials of assertions. Denials can be explicit and implicit. Languages have devices for negating entire propositions (clausal negation) or constituents of clauses (constituent negation). Most languages have several grammatical devices to express clausal negation, which are used with different purposes like negating existence, negating facts, or negating different aspects, modes, or speech acts (Payne 1997). As described by Payne (page 282): . . . a negative clause is one that asserts that some event, situation, or state of affairs does not hold. Negative clauses usually occur in the context of some presupposition, functioning to negate or counter-assert that presupposition. van der Wouden (1997) deﬁnes what a negative context is, showing that negation can be expressed by a variety of grammatical categories. We reproduce some of his examples in Example (6). (6) a. Verbs: We want to avoid doing any look-up, if possible. b. Nouns: The positive degree is expressed by the absence of any phonic sequence. c. Adjectives: It is pointless to teach any of the vernacular languages as a subject in schools.  229  Computational Linguistics  Volume 38, Number 2  d. Adverbs: I’ve never come across anyone quite as brainwashed as your student. e. Prepositions: You can exchange without any problem. f. Determiners: This fact has no direct implications for any of the two methods of font representation. g. Pronouns: Nobody walks anywhere in Tucson. h. Complementizers: Leave the door ajar, lest any latecomers should ﬁnd themselves shut out. i. Conjunctions: But neither this article nor any other similar review I have seen then had the methodological discipline to take the opposite point of view. Negation can also be expressed by afﬁxes, as in motionless or unhappy, and by changing the intonation or facial expression, and it can occur in a variety of syntactic constructions. Typical negation problems that persist in the study of negation are determining the scope when negation occurs with quantiﬁers (7a), neg-raising (7b), the use of polarity items (7c) (any, the faintest idea), double or multiple negation (7d), and afﬁxal negation (Tottie 1991). (7) a. All the boys didn’t leave. b. I don’t think he is coming. c. I didn’t see anything. d. I don’t know nothing no more. Like modality, negation is a frequent phenomenon in texts. Tottie reports that negation is twice as frequent in spoken text (27.6 per 1,000 words) as in written text (12.8 per 1,000 words). Elkin et al. (2005) ﬁnd that 1,823 out of 14,792 concepts in 41 Health Records from Johns Hopkins University are identiﬁed as negated by annotators. Nawaz, Thompson, and Ananiadou (2010) report that more than 3% of the biomedical events in 70 abstracts of the GENIA corpus are negated. Councill, McDonald, and Velikovich (2010) annotate a corpus of product reviews with negation information and they ﬁnd that 19% of the sentences contain negations (216 out of 1,135).  3.1 Negation versus Negative Polarity Negation and negative polarity are interrelated concepts, but it is important to notice that they are different. Negation has been deﬁned as a grammatical phenomenon used to state that some event, situation, or state of affairs does not hold, whereas polarity is a relation between semantic opposites. As Israel (2004, page 701) puts it, “as such polarity encompasses not just the logical relation between negative and afﬁrmative propositions, but also the conceptual relations deﬁning contrary pairs like hot–cold, long–short, and good–bad.” Israel deﬁnes three types of polar oppositions: contradiction, a relation in which one term must be true and the other false; contrariety, a relation in which only one term may be true, although both can be false; and reversal, which  230  Morante and Sporleder  Modality and Negation  involves an opposition between scales ( necessary, likely, possible impossible, unlikely, uncertain .). The relation between negation and polarity lies in the fact that negation can reverse the polarity of an expression. In this context, negative polarity items (NPIs) “are expressions with a limited distribution, part of which includes negative sentences” (Hoeksema 2000, page 115), like any in Example (8a) or ever in Example (8b). Lawler (2010, page 554) deﬁnes NPI as “a term applied to lexical items, ﬁxed phrases, or syntactic construction types that demonstrate unusual behavior around negation.” NPIs felicitously occur only in the scope of some negative element, such as didn’t in Example (8b). If this element is removed, the sentence becomes agrammatical, as shown in Example (8c). The presence of an NPI in a context does not guarantee that something is being negated, however, because NPIs can also occur in certain grammatical circumstances, like interrogatives as in Example (8d). (8) a. I didn’t read any book. b. He didn’t ever read the book. c. * He ever read the book. d. Do you think I could ever read this book? Polarity is a discrete category that can take two values: positive and negative. Determining the polarity of words, and phrases is a central task in sentiment analysis, in particular, disambiguating the contextual polarity of words (Wilson, Wiebe, and Hoffman 2009). Thus, in the context of sentiment analysis positive and negative polarity refers to positive and negative opinions, emotions, and evaluations. Negation is a topic of study in sentiment analysis because it is what Wilson, Wiebe, and Hoffman (2009, page 402) call a polarity inﬂuencer, an element that can change the polarity of an expression. As they put it, however, “many things besides negation can inﬂuence contextual polarity, and even negation is not always straightforward.” We discuss different ways of modeling negation in sentiment analysis in Section 8. The study of negative polarity is beyond the scope of this special issue, however.  4. Categorizing and Annotating Modality and Negation Over the last few years, several corpora of texts from various domains have been annotated at different levels (expression, event, relation, sentence) with information related to modality and negation. Compared to other phenomena like semantic argument structure, dialogue acts, or discourse relations, however, no comprehensive annotation standard has been deﬁned for modality and negation. In this section, we describe the categorization schemes that have been proposed and the corpora that have been annotated. In the framework of the OntoSem project (Nirenburg and Raskin 2004) a corpus has been annotated with modality categories and an analyzer has been developed that takes as input unrestricted raw text and carries out several levels of linguistic analysis, including modality at the semantic level (Nirenburg and McShane 2008). The output of the semantic analysis is represented as formal text-meaning representations. Modality information is encoded as part of the semantic module in the lexical entries of the modality cues. Four modality attributes are encoded: MODALITY TYPE, 231  Computational Linguistics  Volume 38, Number 2  VALUE, SCOPE, and ATTRIBUTED-TO. The MODALITY TYPES are: polarity, whether a proposition is positive or negated; volition, the extent to which someone wants or does not want the event/state to occur; obligation, the extent to which someone considers the event/state to be necessary; belief, the extent to which someone believes the content of the proposition; potential, the extent to which someone believes that the event/state is possible; permission, the extent to which someone believes that the event/state is permitted; and evaluative, the extent to which someone believes the event/state is a good thing. The SCALAR VALUE ranges from zero to one. The SCOPE attribute is the predicate that is affected by the modality and the ATTRIBUTEDTO attribute indicates to whom the modality is assigned, the default value being the speaker. In Example (9), should is identiﬁed as a modality cue and characterized with the type obligative, value 0.8, scope camouﬂage, and is attributed to the speaker. (9) Entrance to the tower should be totally camouﬂaged The publicly available MPQA Opinion Corpus1 (Wiebe, Wilson, and Cardie 2005) contains 10,657 sentences in 535 documents of English newswire annotated with information about private states at the word and phrase level. For every expression of private state a private state frame is deﬁned indicating the SOURCE of the private state, whose private state is being expressed; the TARGET, what the private state is about; and properties like INTENSITY, SIGNIFICANCE, and TYPE OF ATTITUDE. Three types of private state expressions are considered for the annotation: explicit mentions like fears in Example (10a), speech events like said in Example (10b), and expressive subjective elements, like full of absurdities in Example (10b). Apart from representing private states in private state frames, Wiebe, Wilson, and Cardie also deﬁne objective speech event frames that represent “material that is attributed to some source, but is presented as an objective fact” (page 171). Having two types of frames allows a distinction between opinion-oriented material (10a, 10b) and factual material (10c). (10) a. “The U.S. fears a spill-over,” said Xirao-Nima. b. “The report is full of absurdities,” Xirao-Nima said. c. Sergeant O’Leary said the incident took place at 2:00 pm. Rubin, Liddy, and Kando (2005) deﬁne a model for categorizing certainty. The model distinguishes four dimensions: LEVEL, which encodes the degree of certainty; PERSPECTIVE, which encodes whose certainty is involved; FOCUS, the object of certainty; and TIME, which encodes at what time the certainty is expressed. Each dimension is further subdivided into categories, resulting in 72 possible dimension–category combinations. The four certainty LEVELS are absolute (Example (11a)), high (Example (11b)), moderate (Example (11c)), and low (Example (11d)). PERSPECTIVE separates the writer’s point of view and the reported point of view. FOCUS is divided into abstract and factual information. TIME can be past, present, or future. The model is used to annotate certainty markers in 32 articles from The New York Times along these dimensions. Rubin  
1. Introduction A reader’s or listener’s understanding of an utterance depends heavily on assessing the extent to which the speaker (author) intends to convey that the events described did (or did not) happen. An unadorned declarative like The cancer has spread conveys ﬁrm speaker commitment, whereas qualiﬁed variants such as There are strong indicators that the cancer has spread or The cancer might have spread imbue the claim with uncertainty. We ∗ Linguistics Department, Margaret Jacks Hall Building 460, Stanford CA 94305, USA. E-mail: mcdm@stanford.edu. ∗∗ Linguistics Department & Computer Science Department, Gates Building 1A, 353 Serra Mall, Stanford CA 94305, USA. E-mail: manning@stanford.edu. † Linguistics Department, Margaret Jacks Hall Building 460, Stanford CA 94305, USA. E-mail: cgpotts@stanford.edu. Submission received: 4 April 2011; revised submission received: 1 October 2011; accepted for publication: 30 November 2011. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 2  call this event veridicality, building on logical, linguistic, and computational insights about the relationship between language and reader commitment (Montague 1969; Barwise 1981; Giannakidou 1994, 1995, 1999, 2001; Zwarts 1995; Asher and Lascarides 2003; Karttunen and Zaenen 2005; Rubin, Liddy, and Kando 2005; Rubin 2007; Saur´ı 2008). The central goal of this article is to begin to identify the linguistic and contextual factors that shape readers’ veridicality judgments.1 There is a long tradition of tracing veridicality to ﬁxed properties of lexical items (Kiparsky and Kiparsky 1970; Karttunen 1973). On this view, a lexical item L is veridical if the meaning of L applied to argument p entails the truth of p. For example, because both true and false things can be believed, one should not infer directly from A believes that S that S is true, making believe non-veridical. Conversely, realize appears to be veridical, because realizing S entails the truth of S. The prototypical anti-veridical operator is negation, because not S entails the falsity of S, but anti-veridicality is a characteristic of a wide range of words and constructions (e.g., have yet to, fail, without). These basic veridicality judgments can be further subdivided using modal or probabilistic notions. For example, although may is non-veridical by the basic classiﬁcations, we might classify may S as possible with regard to S.2 Lexical theories of this sort provide a basis for characterizing readers’ veridicality judgments, but they do not tell the whole story, because they neglect the pragmatic enrichment that is pervasive in human communication. In the lexical view, say can only be classiﬁed as non-veridical (both true and false things can be said), and yet, if a New York Times article contained the sentence United Widget said that its chairman resigned, readers would reliably infer that United Widget’s chairman resigned—the sentence is, in this context, veridical (at least to some degree) with respect to the event described by the embedded clause, with United Widget said functioning to mark the source of evidence (Simons 2007). Cognitive authority, as termed in information science (Rieh 2010), plays a crucial role in how people judge the veridicality of events. Here, the provenance of the document (the New York Times) and the source (United Widget) combine to reliably lead a reader to infer that the author intended to convey that the event really happened. Conversely, allege is lexically non-veridical, and yet this only begins to address the complex interplay of world knowledge and lexical meaning that will shape people’s inferences about the sentence FBI agents alleged in court documents today that Zazi had admitted receiving weapons and explosives training from al Qaeda operatives in Pakistan last year. We conclude from examples like this that veridicality judgments have an important pragmatic component, and, in turn, that veridicality should be assessed using information from the entire sentence as well as from the context. Lexical theories have a signiﬁcant role to play here, but we expect their classiﬁcations to be buffeted by other communicative pressures. For example, the lexical theory can tell us that, as a narrowly semantic fact, X alleges S is non-veridical with regard to S. Where X is a trustworthy source for S-type information, however, we might fairly conﬁdently conclude that S is true. Where X is known to spread disinformation, we might tentatively conclude that S is false. These pragmatic enrichments move us from uncertainty to some degree of 
© 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 2  Op den Akker). In addition to the corpora built in the project and the dialog manager developed on the basis of these corpora, the article also contains a very good discussion about how it is possible to integrate a dialog manager with a QA engine as a way of developing an interactive QA system. I am not aware of any other articles that contain all the information presented here in one place and in such detail. The second article in this part, “Multidimensional Dialogue Management” (Simon Keizer, Harry Bunt, and Volha Petukhova), is more theoretical and presents a dialog manager built using the framework of Dynamic Interpretation Theory (Bunt 2000) which is able to both interpret and generate utterances using dialog acts. The article also presents brieﬂy the way in which this dialog manager was integrated in the IMIX demonstrator. In my opinion, the editors of the book could have chosen a better title for the third part of the book: “Fusing Text, Speech, and Images.” Both articles in this part present work done in the IMOGEN (Interactive Multimodal Output GENeration) project,1 one of the subprojects embedded in the IMIX Programme that focused on producing multimodal presentations that combine text, speech, and graphics. Only the ﬁrst article focuses on the multimodal aspect of the project, however. The other one discusses only text processing. The article “Experiments in Multimodal Information Presentation” (Charlotte van Hooijdonk, Wauter Bosma, Emiel Krahmer, Alfons Maes, and Marie¨t Theune) presents three experiments for ﬁnding the appropriate way of combining text and images when answering questions from the medical domain. In one of these experiments, the multimodal answers are produced automatically. The other article, “Text-to-Text Generation for Question Answering” (Bosma, Erwin Marsi, Krahmer, and Theune), discusses sentence fusion and could ﬁt very well in a book dedicated to text summarization, as the method presented there is tested not only on data speciﬁc to IMIX, but also on the DUC 2005 data.2 The fourth and the largest part of the book is “Text Analysis for Question Answering.” It contains ﬁve articles, none of which describe a full QA system. Instead, as the title suggests, they focus on various ways of processing texts that can help with answering questions. One common feature of these articles is that they describe methods to extract entities or relations between entities from texts. Most of the articles also brieﬂy discuss how this information is used in QA systems. Most of the methods described in the fourth part of the book are now widely used in computational linguistics, but when they were proposed a few years ago many of them were rather innovative. For brevity, I give only a succinct indication of the methods presented in the articles. “Automatic Extraction of Medical Term Variants from Multilingual Parallel Translations” (Lonneke van der Plas, Jo¨ rg Tiedemann, and Ismail Fahmi) describes how to acquire medical terms and their variants from parallel corpora. “Relation Extraction for Open and Closed Domain Question Answering” (Bouma, Fahmi, and Jori Mur) shows how it is possible to extract relations between entities using dependency paths in a large collection of newspaper articles and in a much smaller and closed domain corpus of medical documents. A sequence labeling method for entity recognition is presented in the article “Constraint-Satisfaction Inference for Entity Recognition” (Sander Canisius, Antal van den Bosch, and Walter Daelemans). Large newspaper corpora and the Web are used in “Extraction of Hypernymy Information from Texts” (Erik Tjong Kim Sang, Katja Hofmann, and Maarten de Rijke) to determine hypernymy relations between entities. The last article in the fourth  
 Computational Linguistics  Volume 38, Number 2  modeling is discussed broadly in Chapter 2; the speciﬁc algorithms for HMMs are fully deﬁned in Chapter 3. This coarse-to-ﬁne introduction of material may challenge readers who are accustomed to more practical descriptions of material. Were I to teach a course based on this book, I would be tempted to present the third chapter before the second. Chapter 4 focuses on semisupervised, unsupervised, and hidden variable learning. With a good mix of theory and practical examples, Expectation-Maximization (EM) is introduced and grounded in several problems, then generalized with log-linear models and approximated with contrastive estimation. Hard EM is mentioned in the context of several examples, though a more detailed description of this potentially important technique (cf. Spitkovsky et al., 2010) would bridge the material of Chapters 2 and 4. The chapter then describes Bayesian approaches to NLP, working from theory into speciﬁc techniques and landing in models. Finally, a brief section is devoted to the related area of hidden variable learning. Chapter 5 begins by describing the partition function, as well as inference techniques for the partition function and decoding methods. I found it strange that this important section was postponed so late in the book; much of the material was forwardreferenced throughout Chapter 4. Regardless, the techniques are described in a unifying, generic manner. The book concludes with a discussion of minimum Bayes risk decoding, and a few other variants. Four appendices are devoted to optimization, experimental techniques, maximum entropy, and locally normalized conditional models. All of these sections provide some useful background. The section on hypothesis testing in Appendix B would be especially useful to students new to the area. It can be difﬁcult to pick the correct hypothesis testing method in general, and this problem is exacerbated in structure prediction. This material serves as a good guide for a researcher hoping to evaluate how effective these methods are. I have some concerns about the intended audience. Descriptions quickly descend into heavy notation and require knowledge of a broad range of mathematical concepts, from marginal polytopes to semirings. I suspect the average NLP graduate student would ﬁnd it difﬁcult to approach much of the material without a series of courses in probability, statistics, and machine learning. The book is also very theoretical: Few concrete algorithms are provided. Instead, the concepts are introduced using only mathematics and formalism. For readers already conversant in the mapping from mathematical descriptions into concrete algorithms and implementations, this will not be a signiﬁcant barrier. From the other direction, the structures used in NLP (e.g., dependency trees) are relatively well motivated, though a machine-learning researcher new to the area might beneﬁt from a fuller introduction to NLP. However, the text serves as an effective guide for introducing the machine-learning community into the NLP community, but I feel it would be challenging to use in the other direction. This may be a personal bias, but I was surprised by the avoidance of machinetranslation–related techniques, despite obvious inﬂuences. Why resort to the term “decoding” if not because of decipherment and translation? One of the most effective uses of hidden variable learning is in word alignment; it seems like a personal example. Of course, building an effective machine-translation system requires a huge amount of engineering in addition to the underlying theory, but I felt some discussion of the problem and effective techniques would be pertinent. 
CUE PHRASES are linguistic expressions that may be used to convey explicit information about the discourse or dialogue, or to convey a more literal, semantic contribution. They aid speakers and writers in organizing the discourse, and listeners and readers in processing it. In previous literature, these constructions have also been termed discourse markers, pragmatic connectives, discourse operators, and clue words. Examples of cue phrases include now, well, so, and, but, then, after all, furthermore, however, in consequence, as a matter of fact, in fact, actually, okay, alright, for example, and incidentally. The ability to correctly determine the function of cue phrases is critical for important natural language processing tasks, including anaphora resolution (Grosz and Sidner 1986), argument understanding (Cohen 1984), plan recognition (Grosz and Sidner 1986; Litman and Allen 1987), and discourse segmentation (Litman and Passonneau 1995). ∗ Departamento de Computación, FCEyN, Universidad de Buenos Aires, Pabellón I, Ciudad Universitaria, (C1428EGA) Buenos Aires, ARGENTINA. E-mail: gravano@dc.uba.ar. ∗∗ E-mail: julia@cs.columbia.edu. † E-mail: sbenus@ukf.sk. Submission received: 9 December 2009; revised submission received: 2 February 2011; accepted for publication: 13 March 2011. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 1  Furthermore, correctly determining the function of cue phrases using features of the surrounding text can be used to improve the naturalness of synthetic speech in text-tospeech systems (Hirschberg 1990). In this study, we focus on a subclass of cue phrases that we term afﬁrmative cue words (hereafter, ACWs), and that include alright, mm-hm, okay, right, and uh-huh, inter alia. These words are frequent in spontaneous conversation, especially in task-oriented dialogue, and are heavily overloaded: Their possible discourse/pragmatic functions include agreeing with what the interlocutor has said, displaying interest and continued attention, and cueing the start of a new topic. Some ACWs (e.g., alright, okay) are capable of conveying as many as ten different functions, as described in Section 3. Whereas ACWs thus form a subset of more general classes of utterances which have been studied in more general studies of cue words, cue phrases, discourse markers, feedback utterances, linguistic feedback, acknowledgments, grounding acts, our focus is on this particular subset of lexical items which may convey an afﬁrmative response—but which may also convey many different meanings. The disambiguation of these meanings we believe is critical to the success of spoken dialogue systems. In the studies presented here, our goal is to extend our understanding of ACWs, in particular by ﬁnding descriptions of the acoustic/prosodic characteristics of their different functions, and by assessing the predictive power of computational methods for their automatic disambiguation. This knowledge should be helpful in spoken language generation and understanding tasks, including interactive spoken dialogue systems and applications doing off-line analyses of conversational data, such as meeting segmentation and summarization. For example, spoken dialogue systems lacking a model of the appropriate realization of different uses of these words are likely to have difﬁculty in understanding and communicating with their users, either by producing cue phrases in a way that does not convey the intended meaning or by misunderstanding users’ productions. This article is organized as follows. Section 2 reviews previous literature. In Section 3 we describe the materials used in the present study from the Columbia Games Corpus. Section 4 presents a statistical description of the acoustic, prosodic, and contextual characteristics of the functions of ACWs in this corpus. In Section 5 we describe results from a number of machine learning experiments aimed at investigating how accurately ACWs may be automatically classiﬁed into their various functions. Finally, in Section 6 we summarize and discuss our main ﬁndings. 2. Previous Work Cue phrases have received extensive attention in the computational linguistics literature. Early work by Cohen (1984) presents a computational justiﬁcation for the importance of cue phrases in discourse processing. Using a simple propositional framework for analyzing discourse, Cohen claims that, in some cases, cue phrases decrease the number of operations required by the listener to process “coherent transmissions”; in other cases, cue phrases are necessary to allow the recognition of “transmissions which would be incoherent (too complex to reconstruct) in the absence of clues” (page 251). Reichman (1985) proposes a model of discourse structure in which discourse comprises a collection of basic constituents called context spaces, organized hierarchically according to semantic and logical relations called conversational moves. In Reichman’s model, cue phrases are portrayed as mechanisms that signal context space boundaries, specifying the kind of conversational move about to take place. Grosz and Sidner (1986) introduce an alternative model of discourse structure formed by three interrelated 2  Gravano, Hirschberg, and Benˇ uš  Afﬁrmative Cue Words in Task-Oriented Dialogue  components: a linguistic structure, an intentional structure, and an attentional state. In this model, cue phrases play a central role, allowing the speaker to provide information about all of the following to the listener: 1) that a change of attention is imminent; 2) whether the change returns to a previous focus space or creates a new one; 3) how the intention is related to other intentions; 4) what precedence relationships, if any, are relevant (page 196). In a corpus study of spontaneous conversations, Schiffrin (1987) describes cue phrases as syntactically detachable from a sentence, commonly used in initial position within utterances, capable of operating at both local and global levels of discourse, and having a range of prosodic contours. As other authors, Schiffrin observes that cue phrases provide contextual coordinates for an utterance in the discourse—that is, they indicate the discourse segment to which an utterance belongs. However, she suggests that cue phrases only display discourse structure relations; they do not create them. In a critique of Schiffrin’s work, Redeker (1991) proposes deﬁning cue phrases as phrases “uttered with the primary function of bringing to the listener’s attention a particular kind of linkage of the upcoming utterance with the immediate discourse context” (page 1169). Prior work on the automatic classiﬁcation of cue phrases includes a series of studies performed by Hirschberg and Litman (Hirschberg and Litman 1987, 1993; Litman and Hirschberg 1990), which focus on differentiating between the discourse and sentential senses of single-word cue phrases such as now, well, okay, say, and so in American English. When used in a discourse sense, a cue phrase explicitly conveys information about the discourse structure; when used in a sentential sense, a cue phrase instead conveys semantic information. Hirschberg and Litman present two manually developed classiﬁcation models, one based on prosodic features, and one based on textual features. This line of research is further pursued by Litman (1994, 1996), who incorporates machine learning techniques to derive classiﬁcation models automatically. Litman uses different combinations of prosodic and text-based features to train decision-tree and rule learners, and shows that machine learning constitutes a powerful tool for developing automatic classiﬁers of cue phrases into their sentential and discourse uses. Zufferey and Popescu-Belis (2004) present a similar study on the automatic classiﬁcation of like and well into discourse and sentential senses, achieving a performance close to that of human annotators. Besides the binary division of cue phrases into discourse vs. sentential meanings, the Conversational Analysis (CA) literature describes items it terms linguistic feedback or acknowledgments. These include not only the computational linguists’ cue phrases but also expressions such as I see or oh wow, which CA research describes in terms of attention, understanding, and acceptance by the speaker of a proposition uttered by another conversation participant (Kendon 1967; Yngve 1970; Duncan 1972; Schegloff 1982; Jefferson 1984). Such items typically occur at the second position in common adjacency pairs and include backchannels (also referred to as continuers), which “exhibit on the part of [their] producer an understanding that an extended unit of talk is underway by another, and that it is not yet, or may not be (even ought not yet be) complete; [they take] the stance that the speaker of that extended unit should continue talking” (Schegloff 1982, page 81), and agreements, which indicate the speaker’s agreement with a statement or opinion expressed by another speaker. Allwood, Nivre, and Ahlsen (1992) distinguish four basic communicative functions of linguistic feedback which enable conversational partners to exchange information: contact, perception, understanding, and attitudinal reactions. These correspond respectively 3  Computational Linguistics  Volume 38, Number 1  to whether the interlocutor is willing and able to continue the interaction, perceive the message, understand the message, and react and respond to the message. Allwood, Nivre, and Ahlsen posit that “simple feedback words, like yes, [...] involve a high degree of context dependence” (page 5), and suggest that their basic communicative function strongly depends on the type of speech act, factual polarity, and information status of the immediately preceding communicative act. Novick and Sutton (1994) propose an alternative categorization of linguistic feedback in task-oriented dialogue, which is based on the structural context of exchanges rather than on the characteristics of the preceding utterance. The three main classes in Novick and Sutton’s catalogue are: (i) other → ackn, where an acknowledgment immediately follows a contribution by other speaker; (ii) self → other → ackn, where self initiates an exchange, other eventually completes it, and self utters an acknowledgment; and (iii) self + ackn, where self includes an acknowledgment in an utterance independently of other’s previous contribution. Substantial attention has been paid to subsets and supersets of words we include in our class of ACWs in the psycholinguistic literature in studies of grounding— the process by which conversants obtain and maintain a common ground of mutual knowledge, mutual beliefs, and mutual assumptions over the course of a conversation (Clark and Schaefer 1989; Clark and Brennan 1991). Computational work on grounding has been pursued for a number of years by Traum and colleagues (e.g., Traum and Allen 1992; Traum 1994), who recently have described a corpus-based study of lexical and semantic evidence supporting different degrees of grounding (Roque and Traum 2009). Our ACWs often occur in the process of establishing such common ground. Prosodic characteristics of the responses involved in grounding have been studied in the Australian English Map Task corpus by Mushin et al. (2003), who ﬁnd that these utterances often consist of acknowledgment contributions such as okay or yeh produced with a “non-ﬁnal” intonational contour, and followed by speech by the same speaker which appears to continue the intonational phrase. Studies by Walker of informationally redundant utterances (IRUs) (Walker 1992, 1996), utterances which express “a proposition already entailed, presupposed or implicated by a previous utterance in the same discourse situation” (Walker 1993a, page 12), also include some of our ACWs, such as IRU prompts (e.g., uh-huh), which, according to Walker, “add no new propositional content to the common ground” (Walker 1993a, page 32). Walker adopts the term “continuer” from the Conversational Analysis school to further describe these prompts (Walker 1993a). Walker describes some intonational contours which are used to realize IRUs in generation in Walker (1993a) and in Walker (1993b), examining 63 IRU tokens and ﬁnding ﬁve different types of contour used among them. As part of a larger project on automatically detecting discourse structure for speech recognition and understanding tasks in American English, Jurafsky et al. (1998) present a study of four particular discourse/pragmatic functions, or dialog acts (Bunt 1989; Core and Allen 1997), closely related to ACWs: backchannel, agreement, incipient speakership (indicating an intention to take the ﬂoor), and yes-answer (afﬁrmative answer to a yes–no question). The authors examine 1,155 conversations from the Switchboard database (Godfrey, Holliman, and McDaniel 1992), and report that the vast majority of these four dialogue acts are realized with words like yeah, okay, or uh-huh. They ﬁnd that the lexical realization of the dialogue act is the strongest cue to its identity (e.g., backchannel is the preferred function for uh-huh and mm-hm), and report preliminary results on some prosodic differences across dialogue acts: Backchannels are shorter in duration, have lower pitch and intensity, and are more likely to end in a rising intonation than agreements. Two related studies, part of the same project, address the automatic classiﬁcation of dialogue acts in conversational speech (Shriberg et al. 1998; Stolcke 4  Gravano, Hirschberg, and Benˇ uš  Afﬁrmative Cue Words in Task-Oriented Dialogue  et al. 2000). The results of their machine learning experiments, conducted on the same subset of Switchboard used previously, indicate a high degree of confusion between agreements and backchannels, because both classes share words such as yeah and right. They also show that prosodic features (including duration, pause, and intensity) can aid the automatic disambiguation between these two classes: A classiﬁer trained using both lexical and prosodic features slightly yet signiﬁcantly outperforms one trained using just lexical features. There is also considerable evidence that linguistic feedback does not take place at arbitrary locations in conversation; rather, it mostly occurs at or near transitionrelevance places for turn-taking (Sacks, Schegloff, and Jefferson 1974; Goodwin 1981). Ward and Tsukahara (2000) describe, in both Japanese and American English, a region of low pitch lasting at least 110 msec which may function as a prosodic cue inviting the realization of a backchannel response from the interlocutor. In a corpus study of Japanese dialogues, Koiso et al. (1998) ﬁnd that both syntax and prosody play a central role in predicting the occurrence of backchannels. Cathcart, Carletta, and Klein (2003) propose a method for automatically predicting the placement of backchannels in Scottish English conversation, based on pause durations and part-of-speech tags, that outperforms a random baseline model. Recently, Gravano and Hirschberg (2009a, 2009b, 2011) describe six distinct prosodic, acoustic, and lexical events in American English speech that tend to precede the occurrence of a backchannel by the interlocutor. Despite their high frequency in spontaneous conversation, the set of ACWs we examine here have seldom, if ever, been an object of study in themselves, as a separate subclass of cue phrases or dialogue acts. Some have attempted to model other types of cue phrases (e.g., well, like) or cue phrases in general; others discuss discourse/ pragmatic functions that may be conveyed through ACWs, but which may also be conveyed through other types of expressions (e.g., agreements may be communicated by single words such as yes or longer cue phrases such as that’s correct). Subsets of ACWs have been studied in very small corpora, with some proposals about their prosodic and functional variations. For example, Hockey (1993) examines the prosodic variation of two ACWs, okay and uh-huh (66 and 77 data points, respectively) produced as full intonational phrases in two spontaneous task-oriented dialogues. She groups the F0 contours visually and auditorily, and shows that instances of okay produced with a high-rise contour are signiﬁcantly more likely to be followed by speech from the other speaker than from the same speaker. The results of a perception experiment conducted by Gravano et al. (2007) suggest that, in task-oriented American English dialogue, contextual information (e.g., duration of surrounding silence, number of surrounding words) as well as word-ﬁnal intonation ﬁgure as the most salient cues to disambiguation of the function of the word okay by human listeners. Also, in a study of the function of intonation in Scottish English task-oriented dialogue, Kowtko (1996) examines a corpus of 273 instances of single-word utterances, including afﬁrmative cue words such as mmhm, okay, right, uh-huh, and yes. Kowtko ﬁnds a signiﬁcant correlation between discourse function and intonational contour: The align function (which checks that the listener’s understanding aligns with that of the speaker) is shown to correlate with rising intonational contours; the ready function (which cues the speaker’s intention to begin a new task) and the reply-y function (which “has an afﬁrmative surface and usually indicates agreement”; Kowtko 1996, page 59) correlate with a non-rising intonation; and the acknowledge function (which indicates having heard and understood) presents all types of ﬁnal intonation. It is important to note, however, that different dialects and different languages have distinct ways of realizing different discourse/pragmatic functions, so it is unclear how useful these results are for American English.  5  Computational Linguistics  Volume 38, Number 1  Although broader studies focusing on the pragmatic function of cue phrases, discourse markers, linguistic feedback, and dialogue acts do shed light on the particular subset of utterances we are studying, and although there is some information on particular lexical items we include here in our study, the class of ACWs itself has received little attention. Particularly given the frequency of ACWs in dialogue, it is important to identify reliable and automatically extractable cues to their disambiguation, so that spoken dialogue systems can recognize the pragmatic function of ACWs in user input and can produce ACWs that are less likely to be misinterpreted in system output. 3. Materials The materials for all experiments in this study were taken from the Columbia Games Corpus, a collection of 12 spontaneous task-oriented dyadic conversations elicited from 13 native speakers (6 female, 7 male) of Standard American English (SAE). A detailed description of this corpus is given in Appendix A. In each session, two subjects were paid to play a series of computer games requiring verbal communication to achieve joint goals of identifying and moving images on the screen. Each subject used a separate laptop computer; they sat facing each other in a soundproof booth, with an opaque curtain hanging between to allow only verbal communication. Each session contains an average of 45 minutes of dialogue, totaling roughly 9 hours of dialogue in the corpus. Trained annotators orthographically transcribed the recordings and manually aligned the words to the speech signal, yielding a total of 70,259 words and 2,037 unique words in the corpus. Additionally, self repairs and certain non-word vocalizations were marked, including laughs, coughs, and breaths. For roughly two thirds of the corpus, intonational patterns and other aspects of the prosody were identiﬁed by trained annotators using the ToBI transcription framework (Beckman and Hirschberg 1994; Pitrelli, Beckman, and Hirschberg 1994). 3.1 Afﬁrmative Cue Words in the Games Corpus Throughout the Games Corpus, subjects made frequent use of afﬁrmative cue words: The 5,456 instances of afﬁrmative cue words alright, gotcha, huh, mm-hm, okay, right, uh-huh, yeah, yep, yes, and yup account for 7.8% of the total words in the corpus. Because the usage of these words seems to vary signiﬁcantly in meaning, we asked three labelers to independently classify all occurrences of these 11 words in the entire corpus into the ten discourse/pragmatic functions listed in Table 1. Among the distinctions we make in these pragmatic functions, we note particularly that our categories of Agr and BC differ primarily in that Agr is deﬁned as indicating belief in or agreement with the interlocutor (e.g., a response to a yes–no question), whereas BC indicates only continued attention.1  
Submission received: 29 October 2008; revised submission received: 28 April 2011; accepted for publication: 29 May 2011. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 1  no unifying theory of meaning to provide guidance to those making use of the new techniques. The problem appears to be a fundamental one in computational linguistics because the whole foundation of meaning seems to be in question. The older, logical theories often subscribe to a model-theoretic philosophy of meaning (Kamp and Reyle 1993; Blackburn and Bos 2005). According to this approach, sentences should be translated to a logical form that can be interpreted as a description of the state of the world. The new vector-based techniques, on the other hand, are often closer in spirit to the philosophy of meaning as context, the idea that the meaning of an expression is determined by how it is used. This is an old idea with origins in the philosophy of Wittgenstein (1953), who said that “meaning just is use,” Firth’s (1968) “You shall know a word by the company it keeps,” and the distributional hypothesis of Harris (1968), that words will occur in similar contexts if and only if they have similar meanings. This hypothesis is justiﬁed by the success of techniques such as latent semantic analysis as well as experimental evidence (Miller and Charles 1991). Although the two philosophies are not obviously incompatible—especially because the former applies mainly at the sentence level and the latter mainly at the word level—it is not clear how they relate to each other. The problem of how to compose vector representations of meanings of words has recently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell and Lapata 2008; Widdows 2008; Erk and Pado´ 2009; Baroni and Zamparelli 2010; Guevara 2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlier work (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998; Kintsch 2001). A solution to this problem would have practical as well as philosophical beneﬁts. Current techniques such as latent semantic analysis work well at the word level, but we cannot extend them much beyond this, to the phrase or sentence level, without quickly encountering the data-sparseness problem: There are not enough occurrences of strings of words to determine what their vectors should be merely by looking in corpora. If we knew how such vectors should compose then we would be able to extend the beneﬁts of the vector based techniques to the many applications that require reasoning about the meaning of phrases and sentences. This article describes the results of our own efforts to identify a theory that can unite these two paradigms, introduced in the author’s DPhil thesis (Clarke 2007). In addition, we also discuss the relationship between this theory and methods of composition that have recently been proposed in the literature, showing that many of them can be considered as falling within our framework. Our approach in identifying the framework is summarized in Figure 1: r Inspired by the philosophy of meaning as context and vector-based techniques we developed a mathematical model of meaning as context, in which the meaning of a string is a vector representing contexts in which that string occurs in a hypothetical inﬁnite corpus. r The theory on its own is not useful when applied to real-world corpora because of the problem of data sparseness. Instead we examine the mathematical properties of the model, and abstract them to form a framework which contains many of the properties of the model. Implementations of the framework are called context theories because they can be viewed as theories about the contexts in which strings occur. By analogy with the term “model-theoretic” we use the term “context-theoretic” for concepts relating to context theories, thus we call our framework the context-theoretic framework.  42  Clarke  A Context-Theoretic Framework for Distributional Semantics  Figure 1 Our approach in developing the context-theoretic framework. r In order to ensure that the framework was practically useful, context theories were developed in parallel with the framework itself. The aim was to be able to describe existing approaches to representing meaning within the framework as fully as possible. In developing the framework we were looking for speciﬁc properties; namely, we wanted it to: r provide some guidelines describing in what way the representation of a phrase or sentence should relate to the representations of the individual words as vectors; r require information about the probability of a string of words to be incorporated into the representation; r provide a way to measure the degree of entailment between strings based on the particular meaning representation; r be general enough to encompass logical representations of meaning; and r be able to incorporate the representation of ambiguity and uncertainty, including statistical information such as the probability of a parse or the probability that a word takes a particular sense. The framework we present is abstract, and hence does not subscribe to a particular method for obtaining word vectors: They may be raw frequency counts, or vectors obtained by a method such as latent semantic analysis. Nor does the framework provide a recipe for how to represent meaning in natural language; instead it provides restrictions on the set of possibilities. The advantage of the framework is in ensuring that techniques are used in a way that is well-founded in a theory of meaning. For example, given vector representations of words, there is not one single way of combining these to give vector representations of phrases and sentences, but in order to ﬁt within the framework there are certain properties of the representation that need to hold. Any method of combining 43  Computational Linguistics  Volume 38, Number 1  these vectors in which these properties hold can be considered within the framework and is thus justiﬁed according to the underlying theory; in addition the framework instructs us as to how to measure the degree of entailment between strings according to that particular method. The contribution of this article is as follows: r We deﬁne the context-theoretic framework and introduce the mathematics necessary to understand it. The description presented here is cleaner than that of Clarke (2007), and in addition we provide examples that should provide intuition for the concepts we describe. r We relate the framework to methods of composition that have been proposed in the literature, namely: – vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998); – the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008); – the multiplicative models of Mitchell and Lapata (2008); – matrix multiplication (Baroni and Zamparelli 2010; Rudolph and Giesbrecht 2010); and – the approach of Clark, Coecke, and Sadrzadeh (2008). It is important to note that the purpose of describing related work in terms of our framework is not merely to demonstrate the generality of our framework: In doing so, we identify previously ignored features of this work such as the lattice structure within the vector space. This allows any one of these approaches to be endowed with an entailment property deﬁned by this lattice structure, based on a philosophy of meaning as context. Although the examples described here show that existing approaches can be described within the framework and show some of its potential, they cannot demonstrate its full power. The mathematical structures we make use of are extremely general, and we hope that in the future many interesting discoveries will be made by exploring the realm we identify here. Our approach in deﬁning the framework may be perceived as overly abstract; however, we believe this approach has many potential beneﬁts, because approaches to composition which may have been considered unrelated (such as the tensor product and vector addition) are now shown to be related. This means that when studying such constructions, work can be avoided by considering the general case, for the same reason that class inheritance aids code reuse. For example, deﬁnitions given in terms of the framework can be applied to all instances, such as our deﬁnition of a degree of entailment. We also hope to motivate people to prove theorems in terms of the framework, having demonstrated its wide applicability. The remainder of the article is as follows: In Section 2 we deﬁne our framework, introducing the necessary deﬁnitions, and showing how related work ﬁts into the framework. In Section 3 we introduce our motivating example, showing that a simple mathematical deﬁnition of the notions of “corpus” and “context” leads to an instance of our framework. In Section 4, we describe speciﬁc instances of our framework in application to the task of recognizing textual entailment. In Section 5 we show how the  44  Clarke  A Context-Theoretic Framework for Distributional Semantics  sophisticated approach of Clark, Coecke, and Sadrzadeh (2008) can be described within our framework. Finally, in Section 6 we present our conclusions and plans for further work.  2. Context Theory In this section, we deﬁne the fundamental concept of our concern, a context theory, and discuss its properties. The deﬁnition is an abstraction of both the more commonly used methods of deﬁning composition in vector-based semantics and our motivating example of meaning as context, described in the next section. Because of its relation to this motivating example, a context theory can be thought of as a hypothesis describing in what contexts all strings occur.  Deﬁnition 1 (Context Theory) A context theory is a tuple A, A, ξ, V, ψ , where A is a set (the alphabet), A is a unital algebra over the real numbers, ξ is a function from A to A, V is an abstract Lebesgue space, and ψ is an injective linear map from A to V.  We will explain each part of this deﬁnition, introducing the necessary mathematics as we proceed. We assume the reader is familiar with linear algebra; see Halmos (1974) for deﬁnitions that are not included here.  2.1 Algebra over a Field We have identiﬁed an algebra over a ﬁeld (or simply algebra when there is no ambiguity) as an important construction because it generalizes nearly all the methods of vector-based composition that have been proposed. An algebra adds a multiplication operation to a vector space; the vector space is intended to describe meaning, and it is this multiplication operation that deﬁnes the composition of meaning in contexttheoretic semantics.  Deﬁnition 2 (Algebra over a Field) An algebra over a ﬁeld is a vector space A over a ﬁeld K together with a binary operation (a, b) → ab on A that is bilinear,  a(αb + βc) = αab + βac  (1)  (αa + βb)c = αac + βbc  (2)  and associative, (ab)c = a(bc) for all a, b, c ∈ A and all α, β ∈ K. Some authors do not place the requirement that an algebra is associative, in which case our deﬁnition would refer to an associative algebra. An algebra is called unital if it has a distinguished unity element 1 satisfying 1x = x1 = x for all x ∈ A. We are generally only interested in real algebras, where K is the ﬁeld of real numbers, R.  Example 1 The square real-valued matrices of order n form a real unital associative algebra under standard matrix multiplication. The vector operations are deﬁned entry-wise. The unity element of the algebra is the identity matrix.  45  Computational Linguistics  Volume 38, Number 1  This means that our proposal is more general than that of Rudolph and Giesbrecht (2010), who suggest using matrix multiplication as a framework for distributional semantic composition. The main differences in our proposal are as follows. r We allow dimensionality to be inﬁnite, instead of restricting ourselves to ﬁnite-dimensional matrices. r Matrix algebras form a ∗-algebra, whereas we do not currently impose this requirement. r Many of the vector spaces used in computational linguistics have an implicit lattice structure; we emphasize the importance of this structure and use the associated partial ordering to deﬁne entailment.  The purpose of ξ in the context theory is to associate elements of the algebra with strings of words. Considering only the multiplication of A (and ignoring the vector operations), A is a monoid, because we assumed that the multiplication on A is associative. Then ξ induces a monoid homomorphism a → aˆ from A∗ to A. We denote the mapped value of a ∈ A∗ by aˆ ∈ A, which is deﬁned as follows:  aˆ = ξ(a1)ξ(a2) . . . ξ(an)  (3)  where a = a1a2 . . . an for ai ∈ A, and we deﬁne ˆ = 1, where is the empty string. Thus, the mapping deﬁned by ˆ allows us to associate an element of the algebra with every string of words. The algebra is what tells us how meanings compose. A crucial part of our thesis is that meanings can be represented by elements of an algebra, and that the type of composition that can be deﬁned using an algebra is general enough to describe the composition of meaning in natural language. To go some way towards justifying this, we give several examples of algebras that describe methods of composition that have been proposed in the literature: namely, point-wise multiplication (Mitchell and Lapata 2008), vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998), and the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008).  Example 2 (Point-wise Multiplication) Consider the n-dimensional real vector space Rn. We describe a vector u ∈ Rn in terms of its components as (u1, u2, . . . un) with each ui ∈ R. We can deﬁne a multiplication · on this space by  (u1, u2, . . . , un) · (v1, v2, . . . , vn) = (u1v1, u2v2, . . . unvn)  (4)  It is easy to see that this satisﬁes the requirements for an algebra as speciﬁed earlier. Table 1 shows a simple example of possible occurrences for three terms in three different contexts, d1, d2, and d3, which may, for example, represent documents. We use this to deﬁne the mapping ξ from terms to vectors. Thus, in this example, we have ξ(cat) = (0, 2, 3) and ξ(big) = (1, 3, 0). Under point-wise multiplication, we would have  big cat = ξ(big) · ξ(cat) = (1, 3, 0) · (0, 2, 3) = (0, 6, 0)  (5)  46  Clarke  A Context-Theoretic Framework for Distributional Semantics  Table 1 Example of possible occurrences for three terms in three different contexts.  d1 d2 d3  cat  023  animal 2 1 2  big  130  One commonly used operation for composing vector-based representations of  meaning is vector addition. As noted by Rudolph and Giesbrecht (2010), this can be  described using matrix multiplication, by embedding an n-dimensional vector u into a  matrix of order n + 1:      α u1 u2 · · · un    0 0 ...  α 0 ...  0 α ...  ··· 0 ··· 0 ...    (6)  0 0 0 ··· α  where α = 1. The set of all such matrices, for all real values of α, forms a subalgebra of the algebra of matrices of order n + 1. A subalgebra of an algebra A is a sub-vector space of A which is closed under the multiplication of A. This subalgebra can be equivalently described as follows:  Example 3 (Additive Algebra) For two vectors u = (α, u1, u2, . . . un) and v = (β, v1, v2 . . . vn) in Rn+1, we deﬁne the additive product by  u v = (αβ, αv1 + βu1, αv2 + βu2, . . . αvn + βun)  (7)  To verify that this multiplication makes Rn+1 an algebra, we can directly verify the bilinear and associativity requirements, or check that it is isomorphic to the subalgebra of matrices discussed previously. Using Table 1, we deﬁne ξ+ so that it maps n-dimensional context vectors to Rn+1, where the ﬁrst component is 1, so ξ+(big) = (1, 1, 3, 0) and ξ+(cat) = (1, 0, 2, 3) and  big cat = ξ+(big) ξ+(cat) = (1, 1, 5, 3)  (8)  Point-wise multiplication and addition are not ideal as methods for composing meaning in natural language because they are commutative; although it is often useful to consider the simpler, commutative case, natural language itself is inherently noncommutative. One obvious method of composing vectors that is not commutative is the tensor product. This method of composition can be viewed as a product in an algebra by considering the tensor algebra, which is formed from direct sums of all tensor powers of a base vector space. We assume the reader is familiar with the tensor product and direct sum (see Halmos [1974] for deﬁnitions); we recall their basic properties here. Let Vn denote a vector space of dimensionality n (note that all vector spaces of a ﬁxed dimensionality  47  Computational Linguistics  Volume 38, Number 1  are isomorphic). Then the tensor product space Vn ⊗ Vm is isomorphic to a space Vnm of dimensionality nm; moreover, given orthonormal bases B = {b1, b2, . . . , bn} for Vn and C = {c1, c2, . . . , cm} for Vm there is an orthonormal basis for Vnm deﬁned by  {bi ⊗ cj : 1 ≤ i ≤ n and 1 ≤ j ≤ m}  (9)  Example 4 The multiplicative models of Mitchell and Lapata (2008) correspond to the class of ﬁnite dimensional algebras. Let A be a ﬁnite-dimensional vector space. Then every associative bilinear product on A can be described by a linear function T from A ⊗ A to A, as required in Mitchell and Lapata’s model. To see this, consider the action of the product · on two orthonormal basis vectors a and b of A. This is a vector in A, thus we can deﬁne T(a ⊗ b) = a · b. By considering all basis vectors, we can deﬁne the linear function T. If the tensor product can loosely be viewed as “multiplying” vector spaces, then the direct sum is like adding them; the space Vn ⊕ Vm has dimensionality n + m and has basis vectors  {bi ⊕ 0 : 1 ≤ i ≤ n} ∪ {0 ⊕ cj : 1 ≤ j ≤ m};  (10)  it is usual to write b ⊕ 0 as b and 0 ⊕ c as c.  Example 5 (Tensor Algebra) If V is a vector space, then we deﬁne T(V), the free algebra of tensor algebra generated by V, as:  T(V) = R ⊕ V ⊕ (V ⊗ V) ⊕ (V ⊗ V ⊗ V) ⊕ · · ·  (11)  where we assume that the direct sum is commutative. We can think of it as the direct sum of all tensor powers of V, with R representing the zeroth power. In order to make this space an algebra, we deﬁne the product on elements of these tensor powers, viewed as subspaces of the tensor algebra, as their tensor product. This is enough to deﬁne the product on the whole space, because every element can be written as a sum of tensor powers of elements of V. There is a natural embedding from V to T(V), where each element maps to an element in the ﬁrst tensor power. Thus for example we can think of u, u ⊗ v, and u ⊗ v + w as elements of T(V), for all u, v, w ∈ V. This product deﬁnes an algebra because the tensor product is a bilinear operation. Taking V = R3 and using ξ as the natural embedding from the context vector of a string T(V), our previous example becomes  big cat = ξ(big) ⊗ ξ(cat)  (12)  = (1, 3, 0) ⊗ (0, 2, 3)  (13)  ∼= (1(0, 2, 3), 3(0, 2, 3), 0(0, 2, 3))  (14)  ∼= (0, 2, 3, 0, 6, 9, 0, 0, 0)  (15)  where the last two lines demonstrate how a vector in R3 ⊗ R3 can be described in the isomorphic space R9.  48  Clarke  A Context-Theoretic Framework for Distributional Semantics  2.2 Vector Lattices  The next part of the deﬁnition speciﬁes an abstract Lebesgue space. This is a special kind of vector lattice, or even more generally, a partially ordered vector space. This lattice structure is implicit in most vector spaces used in computational linguistics, and an important part of our thesis is that the partial ordering can be interpreted as an entailment relation.  Deﬁnition 3 (Partially Ordered Vector Space) A partially ordered vector space V is a real vector space together with a partial ordering ≤ such that: if x ≤ y then x + z ≤ y + z if x ≤ y then αx ≤ αy for all x, y, z ∈ V, and for all α ≥ 0. Such a partial ordering is called a vector space order on V. An element u of V satisfying u ≥ 0 is called a positive element; the set of all positive elements of V is denoted V+. If ≤ deﬁnes a lattice on V then the space is called a vector lattice or Riesz space.  Example 6 (Lattice Operations on Rn) A vector lattice captures many properties that are inherent in real vector spaces when there is a distinguished basis. In Rn, given a speciﬁc basis, we can write two vectors u and v as sequences of numbers: u = (u1, u2, . . . un) and v = (v1, v2, . . . vn). This allows us to deﬁne the lattice operations of meet ∧ and join ∨ as  u ∧ v = (min(u1, v1), min(u2, v2), . . . min(un, vn))  (16)  u ∨ v = (max(u1, v1), max(u2, v2), . . . max(un, vn))  (17)  These are the component-wise minimum and maximum, respectively. The partial ordering is then given by u ≤ v if and only if u ∧ v = u, or equivalently un ≤ vn for all n. A graphical depiction of the meet operation is shown in Figure 2.  The vector operations of addition and multiplication by scalar, which can be deﬁned in a similar component-wise fashion, are nevertheless independent of the particular  Figure 2 Vector representations of the terms orange and fruit based on hypothetical occurrences in six documents and their vector lattice meet (the darker shaded area). 49  Computational Linguistics  Volume 38, Number 1  basis chosen. Conversely, the lattice operations depend on the choice of basis, so the operations as deﬁned herein would behave differently if the components were written using a different basis. We argue that it makes sense for us to consider these properties of vectors in the context of computational linguistics because we can often have a distinguished basis: namely, the one deﬁned by the contexts in which terms occur. Of course it is true that techniques such as latent semantic analysis introduce a new basis which does not have a clear interpretation in relation to contexts; nevertheless they nearly always identify a distinguished basis which we can use to deﬁne the lattice operations. Because our aim is a theory of meaning as context, we should include in our theory a description of the lattice structure which arises out of consideration of these contexts. We argue that the mere association of words with vectors is not enough to constitute a theory of meaning—a theory of meaning must allow us to interpret these vectors. In particular it should be able to tell us whether one meaning entails or implies another; indeed this is one meaning of the verb to mean. Entailment is an asymmetric relation: “x entails y” does not have the same meaning as “y entails x”. Vector representations allow the measurement of similarity or distance, through an inner product or metric; this is a symmetric relation, however, and so cannot be suitable for describing entailment. In propositional and ﬁrst order logic, the entailment relation is a partial ordering; in fact it is a Boolean algebra, which is a special kind of lattice. It seems natural to consider whether the lattice structure that is inherent in the vector representations used in computational linguistics can be used to model entailment. We believe our framework is suited to all vector-based representations of natural language meaning, however the vectors are obtained. Given this assumption, we can only justify our assumption that the partial order structure of the vector space is suitable to represent the entailment relation by observing that it has the right kind of properties we would expect from this relation. There may be more justiﬁcation for this assumption, however, based on the case where the vectors for terms are simply their frequencies of occurrences in n different contexts, so that they are vectors in Rn. In this case, the relation ξ(x) ≤ ξ(y) means that y occurs at least as frequently as x in every context. This means that y occurs in at least as wide a range of contexts as x, and occurs as least as frequently as x. Thus the statement “x entails y if and only if ξ(x) ≤ ξ(y)” can be viewed as a stronger form of the distributional hypothesis of Harris (1968). In fact, this idea can be related to the notion of distributional generality, introduced by Weeds, Weir, and McCarthy (2004) and developed by Geffet and Dagan (2005). A term x is distributionally more general than another term y if x occurs in a subset of the contexts that y occurs in. The idea is that distributional generality may be connected to semantic generality. An example of this is the hypernymy or is-a relation that is used to express generality of concepts in ontologies; for example, the term animal is a hypernym of dog because a dog is an animal. Weeds, Weir, and McCarthy (2004, p. 1019) explain the connection to distributional generality as follows: Although one can obviously think of counter-examples, we would generally expect that the more speciﬁc term dog can only be used in contexts where animal can be used and that the more general term animal might be used in all of the contexts where dog is used and possibly others. Thus, we might expect that distributional generality is correlated with semantic generality. . . Our proposal, in the case where words are represented by frequency vectors, can be considered a stronger version of distributional generality, where the additional 50  Clarke  A Context-Theoretic Framework for Distributional Semantics  requirement is on the frequency of occurrences. In practice, this assumption is unlikely to be compatible with the ontological view of entailment. For example the term entity is semantically more general than the term animal; however, entity is unlikely to occur more frequently in each context, because it is a rarer word. A more realistic foundation for this assumption might be if we were to consider the components for a word to represent the plausibility of observing the word in each context. The question then, of course, is how such vectors might be obtained. Another possibility is to attempt to weight components in such a way that entailment becomes a plausible interpretation for the partial ordering relation. Even if we allow for such alternatives, however, in general it is unlikely that the relation will hold between any two strings, because u ≤ v iff ui ≤ vi for each component, ui, vi, of the two vectors. Instead, we propose to allow for degrees of entailment. We take a Bayesian perspective on this, and suggest that the degree of entailment should take the form of a conditional probability. In order to deﬁne this, however, we need some additional structure on the vector lattice that allows it to be viewed as a description of probability, by requiring it to be an abstract Lebesgue space.  Deﬁnition 4 (Banach Lattice) A Banach lattice V is a vector lattice together with a norm · such that V is complete with respect to · .  Deﬁnition 5 (Abstract Lebesgue Space) An abstract Lebesgue (or AL) space is a Banach lattice V such that  u+v = u + v  (18)  for all u, v in V with u ≥ 0, v ≥ 0 and u ∧ v = 0. Example 7 ( p Spaces) Let u = (u1, u2, . . .) be an inﬁnite sequence of real numbers. We can view ui as components of the inﬁnite-dimensional vector u. We call the set of all such vectors the sequence space; it is a vector space where the operations are deﬁned component-wise. We deﬁne a set of norms, the p-norms, on the space of all such vectors by  1/p  u p=  |ui|p  (19)  i>0  The space of all vectors u for which u p is ﬁnite is called the p space. Considered as vector spaces, these are Banach spaces, because they are complete with respect to the associated norm, and under the component-wise lattice operations, they are Banach lattices. In particular, the 1 space is an abstract Lebesgue space under the 1 norm. The ﬁnite-dimensional real vector spaces Rn can be considered as special cases of the sequence spaces (consisting of vectors in which all but n components are zero) and, because they are ﬁnite-dimensional, we can use any of the p norms. Thus, our previous examples, in which ξ mapped terms to vectors in Rn, can be considered as mapping to abstract Lebesgue spaces if we adopt the 1 norm.  51  Computational Linguistics  Volume 38, Number 1  2.3 Degrees of Entailment  We propose that in vector-based semantics, a degree of entailment is more appropriate than a black-and-white observation of whether or not entailment holds. If we think of the vectors as describing “degrees of meaning,” it makes sense that we should then look for degrees of entailment. Conditional probability is closely connected to entailment: If A entails B, then P(B|A) = 1. Moreover, if A and B are mutually exclusive, then P(A|B) = P(B|A) = 0. It is thus natural to think of conditional probability as a degree of entailment. An abstract Lebesgue space has many of the properties of a probability space, where the set operations of a probability space are replaced by the lattice operations of the vector space. This means that we can think of an abstract Lebesgue space as a vectorbased probability space. Here, events correspond to positive elements with the norm less than or equal to 1; the probability of an event u is given by the norm (which we shall always assume is the 1 norm), and the joint probability of two events u and v is u ∧ v 1. Deﬁnition 6 (Degree of Entailment) We deﬁne the degree to which u entails v in the form of a conditional probability:  Ent(u, v) = u ∧ v 1  (20)  u1  If we are only interested in degrees of entailment (i.e., conditional probabilities) and not probabilities, then we can drop the requirement that the norm should be less than or equal to one, because conditional probabilities are automatically normalized. This deﬁnition, together with the multiplication of the algebra, allows us to compute the degree of entailment between any two strings according to the context theory.  Example 8 The vectors given in Table 1 give the following calculation for the degree to which cat entails animal:  ξ(cat) = (0, 2, 3)  (21)  ξ(animal) = (2, 1, 2)  (22)  ξ(cat) ∧ ξ(animal) = (0, 1, 2)  (23)  Ent(ξ(cat), ξ(animal)) = ξ(cat) ∧ ξ(animal) 1/ ξ(cat) 1 = 3/5  (24)  An important question is how this context-theoretic deﬁnition of the degree of entailment relates to more familiar notions of entailment. There are three main ways in which the term entailment is used: r the model-theoretic sense of entailment in which a theory A entails a theory B if every model of A is also a model of B. It was shown in Clarke (2007) that this type of entailment can be described using context theories, where sentences are represented as projections on a vector space. r entailment between terms (as expressed for example in the WordNet hierarchy), for example the hypernymy relation between the terms cat and animal encodes the fact that a cat is an animal. In Clarke (2007) we showed  52  Clarke  A Context-Theoretic Framework for Distributional Semantics  that such relations can be encoded in the partial order structure of a vector lattice. r Human common-sense judgments as to whether one sentence entails or implies another sentence, as used in the Recognising Textual Entailment Challenges (Dagan, Glickman, and Magnini 2005). Our context-theoretic notion of entailment is thus intended to generalize both the ﬁrst two senses of entailment given here. In addition, we hope that context theories will be useful in the practical application of recognizing textual entailment. Capturing this type of entailment is not our initial aim because we are interested in foundational issues, and doing well at this task poses major engineering challenges beyond the scope of our work. Nevertheless, we believe the ability to represent the preceding two types of entailment as well as standard distributional methods of composition bodes well for the possibility of using our framework for this task. In Section 4 we describe several basic approaches to textual entailment within the framework. Our deﬁnition is more general than the model-theoretic and hypernymy notions of entailment, however, as it allows the measurement of a degree of entailment between any two strings: As an extreme example, one may measure the degree to which not a entails in the. Although this may not be useful or philosophically meaningful, we view it as a practical consequence of the fact that every string has a vector representation in our model, which coincides with the current practice in vector-based compositionality techniques (Clark, Coecke, and Sadrzadeh 2008; Widdows 2008).  2.4 Lattice Ordered Algebras A lattice ordered algebra merges the lattice ordering of the vector space V with the product of A. This structure encapsulates the ordering properties that are familiar from multiplication in matrices and elementary arithmetic. For this reason, many proposed methods of composing vector-based representations of meaning can be viewed as lattice ordered algebras. The only reason we have not included it as a requirement of the framework is because our motivating example (described in the next section) is not guaranteed to have this property, although it does give us a partially ordered algebra. Deﬁnition 7 (Partially Ordered Algebra) A partially ordered algebra A is an algebra which is also a partially ordered vector space, which satisﬁes u · v ≥ 0 for all u, v ∈ A+. If the partial ordering is a lattice, then A is called a lattice-ordered algebra. Example 9 (Lattice-Ordered Algebra of Matrices) The matrices of order n form a lattice-ordered algebra under normal matrix multiplication, where the lattice operations are deﬁned as the entry-wise minimum and maximum. Example 10 (Operators on p Spaces) Matrices can be viewed as operators on ﬁnite-dimensional vector spaces; in fact this lattice property extends to operators on certain inﬁnite-dimensional spaces, the p spaces, 53  Computational Linguistics  Volume 38, Number 1  by the Riesz-Kantorovich theorem (Abramovich and Aliprantis 2002). The operations are deﬁned by:  (S ∨ T)(u) = sup{S(v) + T(w) : v, w ∈ U+ and v + w = u}  (25)  (S ∧ T)(u) = inf{S(v) + T(w) : v, w ∈ U+ and v + w = u}  (26)  If A is a lattice-ordered algebra which is also an abstract Lebesgue space, then A, A, ξ, A, 1 is a context theory. In this simpliﬁed situation, A plays the role of the vector lattice as well as the algebra; ξ maps from A to A as before, and 1 indicates the identity map on A. Many of the examples we discuss will be of this form, so we will use the shorthand notation, A, A, ξ . It is tempting to adopt this as the deﬁnition of context theory; as we will see in the next section, however, this is not supported by our prototypical example of a context theory as in this case the algebra is not necessarily lattice-ordered.  3. Context Algebras  In this section we describe the prototypical examples of a context theory, the context algebras. The deﬁnition of a context algebra originates in the idea that the notion of “meaning as context” can be extended beyond the word level to strings of arbitrary length. In fact, the notion of context algebra can be thought of as a generalization of the syntactic monoid of a formal language: Instead of a set of strings deﬁning the language, we have a fuzzy set of strings, or more generally, a real-valued function on a free monoid. We call such functions real-valued languages and they take the place of formal languages in our theory. We attach a real number to each string which is intended as an indication of its importance or likelihood of being observed; for example, those with a value of zero are considered not to occur.  Deﬁnition 8 (Real-Valued Language) Let A be a ﬁnite set of symbols. A real-valued language (or simply a language when there is no ambiguity) L on A is a function from A∗ to R. If the range of L is a subset of R+ then L is called a positive language. If the range of L is a subset of [0, 1] then L is called a fuzzy language. If L is a positive language such that x∈A∗ L(x) = 1 then L is a probability distribution over A∗, a distributional language.  One possible interpretation for L when it is a distributional language is that L(x) is the probability of observing the string x when selecting a document at random from an inﬁnite collection of documents. The following inclusion relation applies among these classes of language:  distributional =⇒ fuzzy =⇒ positive =⇒ real-valued  (27)  Because A∗ is a countable set, the set RA∗ of functions from A∗ to R is isomorphic to the sequence space, and we shall treat them equivalently. We denote by p(A∗) the set of functions with a ﬁnite p norm when considered as sequences. There is another hierarchy of spaces given by the inclusion of the p spaces: p(A∗) ⊆ q(A∗) if p ≤ q. In particular,  1(A∗ ) ⊆ 2(A∗) ⊆ ∞(A∗ ) ⊆ RA∗  (28)  54  Clarke  A Context-Theoretic Framework for Distributional Semantics  where the ∞ norm gives the maximum value of the function and ∞(A∗) is the space of all bounded real-valued functions on A∗. Recall that a linear operator T from one vector space U to another V is called bounded if there exists some α > 0 such that Tu ≤ α u for all u ∈ U, where the norm on the left hand side is the norm in V, and that on the right hand side is in U. Note that probability distributions are in 1(A∗) and fuzzy languages are in ∞(A∗). If L ∈ 1(A∗)+ (the space of positive functions on A∗ such that the sum of all values of the function is ﬁnite) then we can deﬁne a probability distribution pL over A∗ by pL(x) = L(x)/ L 1. Similarly, if L ∈ ∞(A∗)+ (the space of bounded positive functions on A∗) then we can deﬁne a fuzzy language fL by fL(x) = L(x)/ L ∞. Example 11 Given a ﬁnite set of strings C ⊂ A∗, which we may imagine to be a corpus of documents, deﬁne L(x) = 1/|C| if x ∈ C, or 0 otherwise. Then L is a probability distribution over A∗. In general, we think of a real-valued language as an abstraction of a corpus; in par- ticular, we think of a corpus as a ﬁnite sample of a distributional language representing all possible documents that could ever be written.  Example 12 Let L be a language such that L(x) = 0 for all but a ﬁnite subset of A∗. Then L ∈ p(A∗) for all p.  Example 13 Let L be the language deﬁned by L(x) = |x| where x is the length of (i.e., number of symbols in) string x. Then L is a positive language which is not bounded: For any string y there exists a z such that L(z) > L(y), for example z = ay for a ∈ A.  Example 14 Let L be the language deﬁned by L(x) = 1/2 for all x. Then L is a fuzzy language but L ∈/ 1(A∗) We will assume now that L is ﬁxed, and consider the properties of contexts of strings with respect to this language. As in a syntactic monoid, we consider the context to be everything surrounding the string, although in this case instead of a set of pairs of strings we have a function from pairs of strings to the real numbers. We emphasize the vector nature of these real-valued functions by calling them “context vectors.” Our thesis is centered around these vectors, and it is their properties that form the inspiration for the context-theoretic framework.  Deﬁnition 9 (Context Vectors) Let L be a language on A. For x ∈ A∗, we deﬁne the context of x as a vector xˆ ∈ RA∗×A∗ :  xˆ(y, z) = L(yxz)  (29)  In other words, xˆ is a function from pairs of strings to the real numbers, and the value of xˆ(y, z) is the value of x in the context (y, z), which is L(yxz). The question we are addressing is: Does there exist some algebra A containing the context vectors of strings in A∗ such that xˆ · yˆ = xy where x, y ∈ A∗ and · indicates multiplication in the algebra? As a ﬁrst try, consider the vector space L∞(A∗ × A∗) in which the context vectors live. Is it possible to deﬁne multiplication on the whole vector space such that the condition just speciﬁed holds?  55  Computational Linguistics  Volume 38, Number 1  Example 15  Consider the language C on the alphabet A = {a, b, c, d, e, f } deﬁned by C(abcd) =  C(aecd)  =  C(abfd)  =  
The Textual Entailment (TE) paradigm is a generic framework for applied semantic inference. The objective of TE is to recognize whether a target textual meaning can be inferred from another given text. For example, a question answering system has to recognize that alcohol affects blood pressure is inferred from the text alcohol reduces blood pressure to answer the question What affects blood pressure? In the TE framework, entailment is deﬁned as a directional relationship between pairs of text expressions, denoted by T, the entailing text, and H, the entailed hypothesis. The text T is said to entail the hypothesis H if, typically, a human reading T would infer that H is most likely true (Dagan et al. 2009). TE systems require extensive knowledge of entailment patterns, often captured as entailment rules—rules that specify a directional inference relation between two text fragments (when the rule is bidirectional this is known as paraphrasing). A common type of text fragment is a proposition, which is a simple natural language expression that contains a predicate and arguments (such as alcohol affects blood pressure), where the predicate denotes some semantic relation between the concepts that are expressed ∗ Tel-Aviv University, P.O. Box 39040, Tel-Aviv, 69978, Israel. E-mail: jonatha6@post.tau.ac.il. ∗∗ Bar-Ilan University, Ramat-Gan, 52900, Israel. E-mail: dagan@cs.biu.ac.il. † Bar-Ilan University, Ramat-Gan, 52900, Israel. E-mail: goldbej@eng.biu.ac.il. Submission received: 28 September 2010; revised submission received: 5 May 2011; accepted for publication: 5 July 2011. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 1  by the arguments. One important type of entailment rule speciﬁes entailment between propositional templates, that is, propositions where the arguments are possibly replaced by variables. A rule corresponding to the aforementioned example may be X reduce blood pressure → X affect blood pressure. Because facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task. This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010). Previous work has focused on learning each entailment rule in isolation. It is clear, however, that there are interactions between rules. A prominent phenomenon is that entailment is inherently a transitive relation, and thus the rules X → Y and Y → Z imply the rule X → Z.1 In this article we take advantage of these global interactions to improve entailment rule learning. After reviewing relevant background (Section 2), we describe a structure termed an entailment graph that models entailment relations between propositional templates (Section 3). Next, we motivate and discuss a speciﬁc type of entailment graph, termed a focused entailment graph, where a target concept instantiates one of the arguments of all propositional templates. For example, a focused entailment graph about the target concept nausea might specify the entailment relations between propositional templates like X induce nausea, X prevent nausea, and nausea is a symptom of X. In the core section of the article, we present an algorithm that uses a global approach to learn the entailment relations, which comprise the edges of focused entailment graphs (Section 4). We deﬁne a global objective function and look for the graph that maximizes that function given scores provided by a local entailment classiﬁer and a global transitivity constraint. The optimization problem is formulated as an Integer Linear Program (ILP) and is solved with an ILP solver, which leads to an optimal solution with respect to the global function. In Section 5 we demonstrate that this algorithm outperforms by 12–13% methods that utilize only local information as well as methods that employ a greedy optimization algorithm (Snow, Jurafsky, and Ng 2006) rather than an ILP solver. The article also includes a comprehensive investigation of the algorithm and its components. First, we perform manual comparison between our algorithm and the baselines and analyze the reasons for the improvement in performance (Sections 5.3.1 and 5.3.2). Then, we analyze the errors made by the algorithm against manually prepared gold-standard graphs and compare them to the baselines (Section 5.4). Last, we perform a series of experiments in which we investigate the local entailment classiﬁer and speciﬁcally experiment with various sets of features (Section 6). We conclude and suggest future research directions in Section 7. This article is based on previous work (Berant, Dagan, and Goldberger 2010), while substantially expanding upon it. From a theoretical point of view, we reformulate the two ILPs previously introduced by incorporating a prior. We show a theoretical relation between the two ILPs and prove that the optimization problem tackled is NP-hard. From an empirical point of view, we conduct many new experiments that examine both the local entailment classiﬁer as well as the global algorithm. Last, a rigorous analysis of the algorithm is performed and an extensive survey of previous work is provided.  
∗ Department of Computing, Open University, Milton Keynes MK7 6AA, UK. E-mail: r.power@open.ac.uk. ∗∗ Department of Computing, Open University, Milton Keynes MK7 6AA, UK. E-mail: s.h.williams@open.ac.uk. Submission received: 1 August 2009; revised submission received: 31 March 2011; accepted for publication: 25 May 2011. 
 Recent years have seen growing interest in the shallow semantic analysis of natural language text. The term is most commonly used to refer to the automatic identiﬁcation and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky 2002). Semantic roles themselves have a long-standing tradition in linguistic theory, dating back to the seminal work of Fillmore (1968). They describe the relations that hold between a predicate and its arguments, abstracting over surface syntactic conﬁgurations. Consider the following example sentences:  (1)  a. The burglar broke the window with a hammer.  b. A hammer broke the window.  c. The window broke.  ∗ Center for Computational Learning Systems, Columbia University, 475 Riverside Drive, Suite 850, New York, NY 10115, USA. E-mail: hagen@ccls.columbia.edu. (The work reported in this paper was carried out while the author was at Saarland University, Germany.) ∗∗ School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 30 August 2010; revised submission received: 29 April 2011; accepted for publication: 14 June 2011. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 1  Here, the phrase the window occupies different syntactic positions—it is the object of break in sentences (1a) and (1b), and the subject in (1c)—and yet bears the same semantic role denoting the affected physical object of the breaking event. Analogously, hammer is the instrument of break both when attested with a prepositional phrase in (1a) and as a subject in (1b). The examples represent diathesis alternations1 (Levin 1993), namely, regular variations in the syntactic expressions of semantic roles, and their computational treatment is one of the main challenges faced by automatic semantic role labelers. Several theories of semantic roles have been proposed in the literature, differing primarily in the number and type of roles they postulate. These range from Fillmore’s (1968) small set of universal roles (e.g., Agentive, Instrumental, Dative) to individual roles for each predicate (Palmer, Gildea, and Kingsbury 2005). Frame semantic theory (Fillmore, Johnson, and Petruck 2003) occupies the middle ground by postulating situations (or frames) that can be evoked by different predicates. In this case, roles are not speciﬁc to predicates but to frames, and therefore ought to generalize among semantically related predicates. As an example, consider the sentences in Example (2):  (2)  a. [Lee]Agent [punched]CAUSE HARM [John]Victim [in the eye]Body part.  b. [A falling rock]Cause [crushed]CAUSE HARM [my ankle]Body part.  c. [She]Agent [slapped]CAUSE HARM [him]Victim [hard]Degree [for his change of mood]Reason.  d. [Rachel]Agent [injured]CAUSE HARM [her friend]Victim [by closing the car door on his left hand]Means.  Here, the verbs punch, crush, slap, and injure are all frame evoking elements (FEEs), that is, they evoke the CAUSE HARM frame, which in turn exhibits the frame-speciﬁc (or “core”) roles Agent, Victim, Body part, and Cause, and the more general (“non-core”) roles Degree, Reason, and Means. A frame may be evoked by different lexical items, which may in turn inhabit several frames. For instance, the verb crush may also evoke the GRINDING frame, and slap the IMPACT frame. The creation of resources that document the realization of semantic roles in example sentences such as FrameNet (Fillmore, Johnson, and Petruck 2003) and PropBank (Palmer, Gildea, and Kingsbury 2005) has greatly facilitated the development of learning algorithms capable of automatically analyzing the role semantic structure of input sentences. Moreover, the shallow semantic analysis produced by existing systems has been shown to beneﬁt a wide spectrum of applications ranging from information extraction (Surdeanu et al. 2003) and question answering (Shen and Lapata 2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al. 2005). Most semantic role labeling (SRL) systems to date conceptualize the task as a supervised learning problem and rely on role-annotated data for model training. Supervised methods deliver reasonably good performance2 (F1 measures in the low 80s on standard test collections for English); however, the reliance on labeled training data, which is both difﬁcult and highly expensive to produce, presents a major obstacle to the widespread application of semantic role labeling across different languages and text genres. And although nowadays corpora with semantic role annotations exist in  
∗ Tilburg Center for Cognition and Communication (TiCC), Tilburg University, The Netherlands. E-mail: e.j.krahmer@uvt.nl. ∗∗ Computing Science Department, University of Aberdeen, Scotland, UK. E-mail: k.vdeemter@abdn.ac.uk. Submission received: 16 December 2009; revised submission received: 27 April 2011; accepted for publication: 15 June 2011. © 2012 Association for Computational Linguistics  Computational Linguistics  Volume 38, Number 1  Figure 1 A simple visual scene. from a database) into natural language text, which is useful for practical applications ranging from generating weather forecasts to summarizing medical information (Reiter and Dale 2000). Of all the subtasks of NLG, Referring Expression Generation (REG) is among those that have received most scholarly attention. A survey of implemented, practical NLG systems shows that virtually all of them, regardless of their purpose, contain an REG module of some sort (Mellish et al. 2006). This is hardly surprising in view of the central role that reference plays in communication. A system providing advice about air travel (White, Clark, and Moore 2010) needs to refer to ﬂights (“the cheapest ﬂight,” “the KLM direct ﬂight”), a pollen forecast system (Turner et al. 2008) needs to generate spatial descriptions for areas with low or high pollen levels (“the central belt and further North”), and a robot dialogue system that assembles construction toys together with a human user (Giuliani et al. 2010) needs to refer to the components (“insert the green bolt through the end of this red cube”). REG “is concerned with how we produce a description of an entity that enables the hearer to identify that entity in a given context” (Reiter and Dale 2000, page 55). Because this can often be done in many different ways, a REG algorithm needs to make a number of choices. According to Reiter and Dale (2000), the ﬁrst choice concerns what form of referring expression is to be used; should the target be referred to, for instance, using its proper name, a pronoun (“he”), or a description (“the man with the tie”). Proper names have limited applicability because many domain objects do not have a name that is in common usage. For pronoun generation, a simple but conservative rule is discussed by Reiter and Dale (2000), similar to one proposed by Dale (1989, pages 150–151): Use a pronoun if the target was mentioned in the previous sentence, and if this sentence contained no reference to any other entity of the same gender. Reiter and Dale (2000) concentrate mostly on the generation of descriptions. If the NLG system decides to generate a description, two choices need to be made: Which set of properties distinguishes the target (content selection), and how the selected properties are to be turned into natural language (linguistic realization). Content selection is a complex balancing act: We need to say enough to enable identiﬁcation of the intended referent, but not too much. A selection of information needs to be made, and this needs to be done quickly. Reiter and Dale discuss various strategies that try to manage this 174  Krahmer and van Deemter  Computational Generation of Referring Expressions  balancing act, based on Dale and Reiter (1995), an early survey article that summarizes and compares various inﬂuential algorithms for the generation of descriptions. Why a survey on REG, and how to read it? REG, like NLG in general, has changed considerably since the overviews presented in Dale and Reiter (1995) and Reiter and Dale (2000), owing largely to an increased use of empirical data, and a widening of the class of referring expressions studied. Moreover, a gradual shift has taken place towards extended application domains, different input and output formats, and more ﬂexible interactions with the user, and this shift is starting to necessitate the use of new REG techniques. Examples include recent systems in areas such as weather forecasting (Turner, Sripada, and Reiter 2009) and medical care (Portet et al. 2009), where complex references to spatial regions and time periods abound. The results of recent REG research are scattered over proceedings, books, and journals. The current survey offers a compact overview of the progress in this area and an assessment of the state of the art. The concept of reference is difﬁcult to pin down exactly (Searle 1969; Abbott 2010). Searle therefore suggests that the proper approach is “to examine those cases which constitute the center of variation of the concept of referring and then examine the borderline cases in light of similarities and differences from the paradigms” (Searle 1969, pages 26– 27). The “paradigms” of reference in Reiter and Dale (2000) are deﬁnite descriptions whose primary purpose it is to identify their referent. The vast majority of recent REG research subscribes to this view as well. Accordingly these paradigmatic cases will also be the main focus of this survey, although we shall often have occasion to discuss other types of expressions. However, to do full justice to indeﬁnite or attributive descriptions, proper names, and personal pronouns would, in our view, require a separate, additional survey. In Section 2 we offer a brief overview of REG research up to 2000, discussing some classic algorithms. Next, we zoom in on the new directions in which recent work has taken REG research: extension of the coverage of algorithms, to include, for example, vague, relational, and plural descriptions (Section 3), exploration of different computational frameworks, such as Graph Theory and Description Logic (Section 4), and collection of data and evaluation of REG algorithms (Section 5). Section 6 highlights open questions and avenues for future work. Section 7 summarizes our ﬁndings. 2. A Very Short History of Pre-2000 REG Research The current survey focuses primarily on the progress in REG research in the 21st century, but it is important to have a basic insight into pre-2000 REG research and how it laid the foundation for much of the current work. 2.1 First Beginnings REG can be traced back to the earliest days of Natural Language Processing; Winograd (1972) (Section 8.3.3, Naming Objects and Events), for example, sketches a primitive “incremental” REG algorithm, used in his SHRDLU program. In the 1980s, researchers such as Appelt and Kronfeld set themselves the ambitious task of modeling the human capacity for producing and understanding referring expressions in programs such as KAMP and BERTRAND (Appelt 1985; Appelt and Kronfeld 1987; Kronfeld 1990). They argued that referring expressions should be studied as part of a larger speech act. KAMP (Appelt 1985), for example, was conceived as a general utterance planning system, building on Cohen and Levesque’s (1985) formal speech act theory. It used logical 175  Computational Linguistics  Volume 38, Number 1  axioms and a theorem prover to simulate an agent planning instructions such as “use the wheelpuller to remove the ﬂywheel,” which contains two referring expressions, as part of a larger utterance. Like many of their contemporaries, Appelt and Kronfeld hoped to gain insight into the complexities of human communication. Doug Appelt (personal communication): “the research themes that originally motivated our work on generation were the outgrowth of the methodology in both linguistics and computational linguistics at the time that research progress was best made by investigating hard, anomalous cases that pose difﬁculties for conventional accounts.” Their broad focus allowed these researchers to recognize that although referring expressions may have identiﬁcation of the referent as their main goal, a referring expression can also add information about a target. By pointing to a tool on a table while saying “the wheelpuller,” the descriptive content of the referring expression may serve to inform the hearer about the function of the tool (Appelt and Kronfeld 1987). They also observed that referring expressions need to be sensitive to the communicative context in which they are used and that they should be consistent with the Gricean maxims (see subsequent discussion), which militate against overly elaborate referring expressions (Appelt 1985). It is remarkably difﬁcult, after 20 years, to ﬁnd out how these programs actually worked, because code was lost and much of what was written about them is pitched at a high level of abstraction. Appelt and Kronfeld were primarily interested in difﬁcult questions about human communication, but they were sometimes tantalizingly brief about humbler matters. Here, for instance, is how Appelt (1985, page 21) explains how KAMP would attempt to identify a referent: KAMP chooses a set of basic descriptors when planning a describe action to minimise both the number of descriptors chosen, and the amount of effort required to plan the description. Choosing a provably minimal description requires an inordinate amount of effort and contributes nothing to the success of the action. KAMP chooses a set of descriptors by ﬁrst choosing a basic category descriptor (see [Rosch 1978]) for the intended concept, and then adding descriptors from those facts about the object that are mutually known by the speaker and the hearer, subject to the constraint that they are all linguistically realizable in the current noun phrase, until the concept has been uniquely identiﬁed. . . . Some psychological evidence suggests the validity of the minimal description strategy; however, one does not have to examine very many dialogues to ﬁnd counter-examples to the hypothesis that people always produce minimal descriptions. This quote contains the seeds of much later work in REG, given its skepticism about the naturalness of minimal descriptions, its use of Rosch (1978)–style basic categories, and its acknowledgment of the role of computational complexity. Broadly speaking, it suggests an incremental generation strategy, compatible with the ones described subsequently, although it is uncertain what exactly was implemented. In recent years, the Appelt–Kronfeld line of research has largely given way to a new research tradition which focused away from the full complexity of human communication, with notable exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O’Donnell, Cheng and Hitzeman (1998), and Koller and Stone (2007). 2.2 Generating Distinguishing Descriptions In the early 1990s a new approach to REG started gaining currency, when Dale and Reiter re-focused on the problem of determining what properties a referring expression 176  Krahmer and van Deemter  Computational Generation of Referring Expressions  should use if identiﬁcation of the referent is the central goal (Dale 1989, 1992; Reiter 1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by Dale and Reiter (1995). Like Appelt (1985), Dale and Reiter are concerned with the link between the Gricean maxims and the generation of referring expressions. They discuss the following pair of examples: (1) Sit by the table. (2) Sit by the brown wooden table. In a situation where there is only one table, which happens to be brown and wooden, both the descriptions in (1) and (2) would successfully refer to their target. However, if you hear (2) you might make the additional inference that it is signiﬁcant to know that the table is brown and wooden; why else would the speaker mention these properties? If the speaker merely wanted to refer to the table, your inference would be an (incorrect) “conversational implicature,” caused by the speaker’s violation of Grice’s (1975, page 45) Maxim of Quantity (“Do not make your contribution more informative than is required”). Dale and Reiter (1995) ask how we can efﬁciently compute which properties to include in a description, such that it successfully identiﬁes the target while not triggering false conversational implicatures. For this, they zoom in on a relatively straightforward problem deﬁnition, and compare a number of concise, well-deﬁned algorithms solving the problem. Problem Deﬁnition. Dale and Reiter (1995) formulate the REG problem as follows. Assume we have a ﬁnite domain D of objects with attributes A. In our example scene (Figure 1), D = {d1, d2, d3} and A = {type, clothing, position, . . .}. The type attribute has a special status in Dale and Reiter (1995) because it represents the semantic content of the head noun. Alternatively, we could have deﬁned an attribute gender, stating that it should be realized as the head noun of a description. Typically, domains are represented in a knowledge base such as Table 1, where different values are clustered together because they are associated with the same attribute. Left, right, and middle, for example, belong to the attribute position, and are said to be three values that this attribute can take. The objects of which a given attribute–value combination (or “property”) is true are said to form its denotation. Sometimes we will drop the attribute, writing man, rather than type, man , for instance. The REG task is now deﬁned by Dale and Reiter (1995) through what may be called identiﬁcation of the target: given a target (or referent) object r ∈ D, ﬁnd a set of attribute–value pairs L whose conjunction is true of the target but not of any of the distractors (i.e., D − {r}, the domain objects different from the target). L is called a distinguishing description of the target. In our simple example, suppose that {d1} is the target (and hence {d2, d3} the set of distractors), then L could, for example, be  Table 1 Tabular representation of some information in our example scene.  Object type  clothing  position  d1  man  wearing suit  left  d2  woman wearing t-shirt middle  d3  man  wearing t-shirt right  177  Computational Linguistics  Volume 38, Number 1  either { type, man , clothing, wearing suit } or { type, man , position, left }, which could be realized as “the man wearing a suit” or “the man to the left.” If identiﬁcation were all that counted, a simple, fast, and fault-proof REG strategy would be to conjoin all the properties of the referent: This conjunction will identify the referent if it can be identiﬁed at all. In practice, Dale and Reiter, and others in their wake, include an additional constraint that is often left implicit: that the referring expressions generated should be as similar to human-produced ones as possible. In the Evaluation and Conclusion sections, we return to this “human-likeness” constraint (and to variations on the same theme). Full Brevity and Greedy Heuristic. Dale and Reiter (1995) discuss various algorithms which solve the REG task. One of these is the Full Brevity algorithm (Dale 1989) which deals with the problem of avoiding false conversational implicatures in a radical way, by always generating the shortest possible distinguishing description. Originally, the Full Brevity algorithm was meant to generate both initial and subsequent descriptions, by relying on a previous step that determines the distractor set based on which objects are currently salient. Given this set, it ﬁrst checks whether there is a single property of the target that rules out all distractors. If this fails, it considers all possible combinations of two properties, and so on: 1. Look for a description L that distinguishes target r using one property. If success then return L. Else go to 2. 2. Look for a description L that distinguishes target r using two properties. If success then return L. Else go to 3. 3. Etcetera Unfortunately, there are two problems with this approach. First, the problem of ﬁnding a shortest distinguishing description has a high complexity—it is NP hard (see, e.g., Garey and Johnson 1979)—and hence is computationally very expensive, making it prohibitively slow for large domains and descriptions. Second, Dale and Reiter note that human speakers routinely produce descriptions that are not minimal. This is conﬁrmed by a substantial body of psycholinguistic research (Olson 1970; Sonnenschein 1984; Pechmann 1989; Engelhardt, Bailey, and Ferreira 2006). An approximation of Full Brevity is the Greedy Heuristic algorithm (Dale 1989, 1992), which iteratively selects the property which rules out most of the distractors not previously ruled out, incrementally augmenting the description based on what property has most discriminatory power at each stage (as a result, it does not always generate descriptions of minimal size). The Greedy Heuristic algorithm is a more efﬁcient algorithm than the Full Brevity one, but it was soon eclipsed by another algorithm (Reiter and Dale 1992; Dale and Reiter 1995), which turned out to be the most inﬂuential algorithm of the pre-2000 era. It is this later algorithm that came to be known as “the” Incremental Algorithm (IA). The Incremental Algorithm. The basic idea underlying the IA is that speakers “prefer” certain properties over others when referring to objects, an intuition supported by the experimental work of, for instance, Pechmann (1989). Suppose you want to refer to a person 10 meters away from you. You might mention the person’s gender. If this is insufﬁcient to single out the referent, you might be more likely to make use of the color of the person’s coat than to the color of her eyes. Less preferred attributes, such as eye color, are only considered if other attributes do not sufﬁce. It is this intuition of a preference order between attributes that the IA exploits. By making this order  178  Krahmer and van Deemter  Computational Generation of Referring Expressions  a parameter of the algorithm, a distinction can be made between domain/genre dependent knowledge (the preferences), and a domain-independent search strategy. As in the Greedy Heuristic algorithm, descriptions are constructed incrementally; but unlike the Greedy Heuristic, the IA checks attributes in a ﬁxed order. By grouping properties into attributes, Dale and Reiter predict that all values of a given attribute have the same preference order. Ordering attributes rather than values may be disadvantageous, however. A simple shape (e.g., a circle), or a size that is unusual for its target (e.g., a tiny whale) might be preferred over a subtle color (purplish gray). Also, some values of a given attribute might be difﬁcult to express, and “dispreferred” for this reason (kind of like a ufo shape with a christmas tree sticking out the side). Figure 2 contains a sketch of the IA in pseudo code. It takes as input a target object r, a domain D consisting of a collection of domain objects, and a domain-speciﬁc list of preferred attributes Pref (1). Suppose we apply the IA to d1 of our example scene, assuming that Pref = type > clothing > position. The description L is initialized as the empty set (2), and the context set C of distractors (from which d1 needs to be distinguished) is initialized as D − {d1} (3). The algorithm then iterates through the list of preferred attributes (4), for each one looking up the target’s value (5), and checking whether this attribute–value pair rules out any of the distractors not ruled out so far (6). The function RulesOut ( Ai, V ) returns the set of objects which have a different value for attribute Ai than the target object has. If one or more distractors are ruled out, the attribute–value pair Ai, V is added to the description under construction (7) and a new set of distractors is computed (8). The ﬁrst attribute to be considered is type, for which d1 has the value man. This would rule out d2, the only woman in our domain, and hence the attribute–value pair type, man is added to L. The new set of distractors is C = {d3}, and the next attribute (clothing) is tried. Our target is wearing suit, and the remaining distractor is not, so the attribute–value pair clothing, wearing suit is included as well. At this point all distractors are ruled out (10), a set of properties has been found which uniquely characterize the target { type, man , clothing, wearing suit } (the man wearing a suit), and we are done (11). If we had reached the end of the list without ruling out all distractors, the algorithm would have failed (13): No distinguishing description for our target was found. The sketch in Figure 2 simpliﬁes the original algorithm in a number of respects. First, Dale and Reiter always include the type attribute, even if it does not rule out any  Figure 2 Sketch of the core Incremental Algorithm. 179  Computational Linguistics  Volume 38, Number 1  distractors, because speakers use type information in virtually all their descriptions. Second, the original algorithm checks, via a function called UserKnows, whether a given property is in the common ground, to prevent the selection of properties which the addressee might not understand. Unlike Appelt and Kronfeld, who discuss detailed examples that hinge on differences in common ground, Dale and Reiter (1995) treat UserKnows as a function that returns “true” for each true proposition, assuming that all relevant information is shared. Third, the IA can take some ontological information into account via subsumption hierarchies. For instance, in a dog-and-cat domain, a pet may be of the chihuahua type, but chihuahua is subsumed by dog, and dog in turn is subsumed by animal. A special value in such a subsumption hierarchy is reserved for the so-called basic level values (Rosch 1978); dog in this example. If an attribute comes with a subsumption hierarchy, the IA ﬁrst computes the best value for that attribute, which is deﬁned as the value closest to the basic level value, such that there is no more speciﬁc value that rules out more distractors. In other words, the IA favors dog over chihuahua, unless the latter rules out more distractors. The IA is conceptually straightforward and easy to implement. In addition, it is computationally efﬁcient, with polynomial complexity: Its worst-case run time is a constant function of the total number of attribute–value combinations available. This computational efﬁciency is due to the fact that the algorithm does not perform backtracking: once a property has been selected, it is included in the ﬁnal referring expression, even if later additions render it superﬂuous. As a result, the ﬁnal description may contain redundant properties. Far from seeing this as a weakness, Dale & Reiter (1995, page 19) point out that this makes the IA less “psycholinguistically implausible” than its competitors. It is interesting to observe that whereas Dale and Reiter (1995) discuss the theoretical complexity of the various algorithms in detail, later research has tended to attach more importance to empirical evaluation of the generated expressions (Section 5). 2.3 Discussion Appelt and Kronfeld’s work, founded on the assumption that REG should be seen as part of a comprehensive model of communication, started to lose some of its appeal in the early 1990s because it was at odds with the emerging research ethos in computational linguistics that stressed simple, well-deﬁned problems allowing for measurable results. The way current REG systems are shaped is largely due to developments summarized in Dale and Reiter (1995), which focuses on a speciﬁc aspect of REG, namely, determining which properties serve to identify some target referent. Dale and Reiter’s work aimed for generating human-like descriptions, but was not yet coupled with systematic investigation of data. REG as Search. The algorithms discussed by Dale and Reiter (1995) can be seen as different instantiations of a general search algorithm (Bohnet and Dale 2005; Gatt 2007). They all basically search through the same space of states, each consisting of three components: a description that is true of the target, a set of distractors, and a set of properties of the target that have not yet been considered. The initial state can be formalized as the triple ∅, C, P (no description for the target has been constructed, no distractors have been ruled out, and all properties P of the target are still available), and the goal state as L, ∅, P , for certain L and P : A description L has been found, which is distinguishing— the set of distractors is empty. All other states in the search space are intermediate ones, through which an algorithm might move depending on its search strategy. For instance, 180  Krahmer and van Deemter  Computational Generation of Referring Expressions  when searching for a distinguishing description for d1 in our example domain, an intermediate state could be s = { type, man }, {d3}, { clothing, wearing suit , position, left } . The algorithms discussed earlier differ in terms of their so-called expand-method, determining how new states are created, and their queue-method, which determines the order in which these states are visited (i.e., how states are inserted into the queue). Full Brevity, for example, uses an expand-method that creates a new state for each attribute of the target not checked before (as long as it rules out at least one distractor). Starting from the initial state and applied to our example domain, this expand-method would result in three new states, creating descriptions including type, clothing, and position information, respectively. These states would be checked using a queue-method which is breadth-ﬁrst. The IA, by contrast, uses a different expand-method, each time creating a single new state in accordance with the pre-determined preference order. Thus, in the initial state, and assuming (as before) that type is the most preferred attribute, the expand-method would create a single new state: s above. Because there is always only one new state, the queue-method is trivial. Limitations of pre-2000 REG. In the IA and related algorithms, the focus is on efﬁciently computing which properties to use in a distinguishing description. These algorithms rest on a number of implicit simpliﬁcations of the REG task, however. (1) The target is always just one object, not a larger set (hence, plural noun phrases are not generated). (2) The algorithms all assume a very simple kind of knowledge representation, consisting of a set of atomic propositions. Negated propositions are only represented indirectly, via the Closed World Assumption, so an atomic proposition that is not explicitly listed in the database is false. (3) Properties are always “crisp,” never vague. Vague properties such as small and large are treated as Boolean properties, which do not allow borderline cases and which keep the same denotation, regardless of the context in which they are used. (4) All objects in the domain are assumed to be equally salient, which implies that all distractors have to be removed, even those having a very low salience. (5) The full REG task includes ﬁrst determining which properties to include in a description, and then providing a surface realization in natural language of the selected properties. The second stage is not discussed, nor is the relation with the ﬁrst. A substantial part of recent REG research is dedicated to lifting one or more of these simplifying assumptions, although other limitations are still ﬁrmly in place (as we shall discuss in Section 6). 3. Extending the Coverage 3.1 Reference to Sets Until recently, REG algorithms aimed to produce references to a single object. But references to sets are ubiquitous in most text genres. In simple cases, it takes only a slight modiﬁcation to allow classic REG algorithms to refer to sets. The IA, for example, can be seen as referring to the singleton set {r} that contains the target r and nothing else. If in line 1 (Figure 2), {r} is replaced by an arbitrary set S, and line 3 is modiﬁed as saying C ← D − S instead of C ← D − {r}, then the algorithm produces a description that applies to all elements of S. Thus, it is easy enough to let these algorithms produce expressions like “the men” or “the t-shirt wearers,” to identify {d1, d3} and {d2, d3}, respectively. Unfortunately, things are not always so simple. What if we need to refer to the set {d1, d2}? Based on the properties in Table 1 alone this is not possible, because d1 and d2 have no properties in common. The natural solution is to treat the target set 181  Computational Linguistics  Volume 38, Number 1  as the union of two smaller sets, {d1} ∪ {d2}, and refer to both sets separately (e.g., “the man who wears a suit, and the woman”). Once unions are used, it becomes natural to allow set complementation as well, as in “the people who are not on the right.” Note that set complementation may also be useful for single referents. Consider a situation where all cats except one are owned by Mary, and the owner of the remaining one is unknown or non-existent. Complementation allows one to refer to “the cat not owned by Mary.” We shall call the resulting descriptions Boolean. As part of a more general logical analysis of the IA, van Deemter (2002) made a ﬁrst stab at producing Boolean descriptions, using a two-stage algorithm whose ﬁrst stage is a generalization of the IA, and whose second stage involves the optimization of the possibly lengthy expressions produced by the ﬁrst phase. The resulting algorithm is logically complete in the following sense: If a given set can be described at all using the properties available then this algorithm will ﬁnd such a description. With intersection as the only way to combine properties, REG cannot achieve logical completeness. The ﬁrst stage of the algorithm starts by conjoining properties (man, left) (omitting attributes for the sake of readability) in the familiar manner of the IA; if this does not sufﬁce for singling out the target set then the same incremental process continues with unions of two properties (e.g., man ∪ woman, middle ∪ left; that is, properties expressing that a referent is a man or a woman, in the middle or on the left), then with unions of three properties (e.g., man ∪ wearing suit ∪ woman), and so on. The algorithm terminates when the referent (individual or set) is identiﬁed (success) or when all combinations of properties have been considered (failure). Figure 3 depicts this in schematic form, where n represents the total number of properties in the domain, and P+/− denotes the set of all literals (atomic properties such as man, and their complements ¬man). Step (1) generalizes the original IA allowing for negated properties and target sets. As before, L is the description under construction. It will consist of intersections of unions of literals such as (woman ∪ man) ∩ (woman ∪ ¬wearing suit) (in other words, L is in Conjunctive Normal Form, CNF). Note that this ﬁrst stage is not only incremental at each of its n steps, but also as a whole: Once a property has been added to the description, later steps will not remove it. This can lead to redundancies, even more than in the original IA. The second stage removes the most blatant of these, but only where the redundancy exists as a matter of logic, rather than world knowledge. Suppose, for example, that Step 2 selects the properties P ∪ S and P ∪ R, ruling out all distractors. L now takes the form (P ∪ S) ∩ (P ∪ R) (e.g., “the things that are both (women or men) and (women or wearing suits)”). The second phase uses logic optimization techniques, originally designed for the minimization of digital circuits (McCluskey 1965), to simplify this to P ∪ (S ∩ R) (“the women, and the men wearing suits”).  Figure 3 Outline of the ﬁrst stage of van Deemter’s (2002) Boolean REG algorithm. 182  Krahmer and van Deemter  Computational Generation of Referring Expressions  Variations and Extensions. Gardent (2002) drew attention to situations where this proposal produces unacceptably lengthy descriptions; suppose, for example, the algorithm accumulates numerous properties during Steps 1 and 2, before ﬁnding one complex property (a union of three properties) during Step 3 which, on its own would have sufﬁced to identify the referent. This will make the description generated much lengthier than necessary, because the properties from Steps 1 and 2 are now superﬂuous. Gardent’s take on this problem amounts to a reinstatement of Full Brevity embedded in a reformulation of REG as a constraint satisfaction problem (see Section 4.2). The existence of fast implementations for constraint satisfaction alleviates the problems with computational tractability to a considerable extent. But by re-instating Full Brevity, algorithms like Gardent’s could run into the empirical problems noted by Dale and Reiter, given that human speakers frequently produce non-minimal descriptions (see Gatt [2007] for evidence pertaining to plurals). Horacek (2004) makes a case for descriptions in Disjunctive Normal Form (DNF; unions of intersections of literals). Horacek’s algorithm ﬁrst generates descriptions in CNF, then convert these into DNF, skipping superﬂuous disjuncts. Consider our example domain (Table 1). To refer to {d1, d2}, a CNF-oriented algorithm might generate (man ∪ woman) ∩ (left ∪ middle) (“the people who are on the left or middle”). Horacek converts this, ﬁrst, into DNF: (man ∩ left) ∪ (woman ∩ middle) ∪ (man ∩ middle) ∪ (woman ∩ left), after which the last two disjuncts are dropped, because there are no men in the middle, and no women on the left. The outcome could be worded as “the man on the left and the woman in the middle.” Later work has tended to agree with Horacek in opting for DNF instead of CNF (Gatt 2007; Khan, van Deemter, and Ritchie 2008). Perspective and Coherence. Recent studies have started to bring data-oriented methods to the generation of references to sets (Gatt 2007; Gatt and van Deemter 2007; Khan, van Deemter, and Ritchie 2008). One ﬁnding is that referring expressions beneﬁt from a “coherent” perspective. For example, “the Italian and the Greek” is normally a better way to refer to two people than “the Italian and the cook,” because the former is generated from one coherent perspective (i.e., nationalities). Two questions need to be addressed, however. First, how should coherence be deﬁned? Gatt (2007) opted for a deﬁnition that assesses the coherence of a combination of properties using corpus-based frequencies as deﬁned by Kilgarriff (2003), which in turn is based on Lin (1998). This choice was supported by a range of experiments (although the success of the approach is less well attested for descriptions that contain adjectives). Secondly, what if full coherence can only be achieved at the expense of brevity? Suppose a domain contains one Italian and two Greeks. One of the Greeks is a cook, whereas the other Greek and the Italian are both IT consultants. If this is all that is known, the generator faces a choice between either generating a description that is fully coherent but unnecessarily lengthy (“the Italian IT consultant and the Greek cook”), or brief but incoherent (“The Italian and the cook”). Simply saying “The Italian and the Greek” would not be distinguishing. In such cases, coherence becomes a tricky, and computationally complex, optimization problem (Gatt 2007; Gatt and van Deemter 2007). Collective Plurals. Reference to sets is a rich topic, where many issues on the borderline between theoretical, computational, and experimental linguistics are waiting to be explored. Most computational proposals, so far, use properties that apply to individual objects. To refer to a set, in this view, is to say things that are true of each member of the set. Such references may be contrasted with collective ones (e.g., “the lines that run parallel to each other,” “the group of four people”) which are more complicated from a 183  Computational Linguistics  Volume 38, Number 1  semantic point of view (Scha and Stallard 1988; Lønning 1997, among others). For initial ideas about the generation of collective plurals, we refer to Stone (2000). 3.2 Relational Descriptions Another important limitation of most early REG algorithms is that they are restricted to one-place predicates (e.g., “being a man”), instead of relations involving two or more arguments. Even a property like “wearing a suit” is modeled as if it were simply a one-place predicate without internal structure (instead of a relation between a person and a piece of clothing). This means that the algorithms in question are unable to identify one object via another, as when we say “the woman next to the man who wears a suit,” and so on. One early paper does discuss relational descriptions, making a number of important observations about them (Dale and Haddock 1991). First, it is possible to identify an object through its relations to other objects without identifying each of these objects separately. Consider a situation involving two cups and two tables, where one cup is on one of the tables. In this situation, neither “the cup” nor “the table” is distinguishing, but “the cup on the table” succeeds in identifying one of the two cups. Secondly, descriptions of this kind can have any level of ‘depth’: in a complex situation, one might say “the white cup on the red table in the kitchen,” and so on. To be avoided, however, are the kinds of repetitions that can arise from descriptive loops, because these do not add information. It would, for example, be useless to describe a cup as “the cup to the left of the saucer to the right of the cup to the left of the saucer . . . ” We shall return to this issue in Section 4, where we shall ask how suitable each of a number of representational frameworks is for the proper treatment of relational descriptions. Various researchers have attempted to extend the IA by allowing relational descriptions (Horacek 1996; Krahmer and Theune 2002; Kelleher and Kruijff 2006), often based on the assumption that relational properties (like “x is on y”) are less preferred than non-relational ones (like “x is white”). If a relation is required to distinguish the target x, the basic algorithm is applied iteratively to y. It seems, however, that these attempts were only partly successful. One of the basic problems is that relational descriptions— just like references to sets, but for different reasons—do not seem to ﬁt in well with an incremental generation strategy. In addition, it is far from clear that relational properties are always less preferred than non-relational ones (Viethen and Dale 2008). Viethen and Dale suggest that even in simple scenes, where objects can easily be distinguished without relations, participants still use relations frequently (in about one third of the trials). We return to this in Section 5. On balance, it appears that the place of relations in reference is only partly understood, with much of the iceberg still under water. If two-place relations can play a role in REG, then surely so can n-place relations for larger n, as when we say “the city that lies between the mountains and the sea” (n = 3). No existing proposal has addressed nplace relations in general, however. Moreover, human speakers can identify a man as the man who “kissed all women,” “only women,” or “two women.” The proposals discussed so far do not cover such quantiﬁed relations, but see Ren, van Deemter, and Pan (2010). 3.3 Context Dependency, Vagueness, and Gradability So far we have assumed that properties have a crisply deﬁned meaning that is ﬁxed, regardless of the context in which they are used. But many properties fail to ﬁt this mold. Consider the properties young and old, for example. In Figure 1, it is the leftmost 184  Krahmer and van Deemter  Computational Generation of Referring Expressions  male who looks the older of the two. But if we add an old-age pensioner to the scene then suddenly he is the most obvious target of expressions like “the older man” or “the old man.” Whether a man counts as old or not, in other words, depends on what other people he is compared to: being old is a context-dependent property. The concept of being “on the left” is context-dependent too: Suppose we add ﬁve people to the right of the young man in Figure 1; now all three characters originally depicted are suddenly on the left, including the man in the t-shirt who started out on the right. Concepts like “old” and “left” involve comparisons between objects. Therefore, if the knowledge base changes, the objects’ descriptions may change as well. But even if the knowledge base is kept constant, the referent may have to be compared against different objects, depending on the words in the expression. The word “short” in “John is a short basketball player,” for example, compares John’s height with that of the other basketball players, whereas “John is a short man” compares its referent with all the other men, resulting in different standards for what it means to be short. “Old” and “short” are not only context dependent but also gradable, meaning that you can be more or less of it (older, younger, shorter, taller) (Quirk et al. 1980). Gradable words are extremely frequent, and in many NLG systems they are of great importance, particularly in those that have numerical input, for example, in weather forecasting (Goldberg, Driedger, and Kittredge 1994) or medical decision support (Portet et al. 2009). In addition to being context-dependent, they are also vague, in the sense that they allow borderline cases. Some people may be clearly young, others clearly not, but there are borderline cases for whom it is not quite clear whether they were included. Context can help to diminish the problem, but it won’t go away: In the expression “short basketball player,” the noun gives additional information about the intended height range, but borderline cases still exist. Generating Vague References. REG, as we know it, lets generation start from a Knowledge Base (KB) whose facts do not change as a function of context. This means that context-dependent properties like a person’s height need to be stored in the KB in a manner that does not depend on other facts. It is possible to deal with size adjectives in a principled way, by letting one’s KB contain a height attribute with numerical values. Our running example can be augmented by giving each of the three people a precise height, for example: height(d1) = 170 cm, height(d2) = 180 cm and height(d3) = 180 cm (here the height of the woman d2 has been increased for illustrative purposes). Now imagine we want to refer to d3. This target can be distinguished by the set of two properties {man, height = 180 cm}. Descriptions of this kind can be produced by means of any of the classic REG algorithms. Given that type and height identify the referent uniquely, this set of properties can be realized simply as “the man who is 180 cm tall.” But other possibilities exist. Given that 180 cm is the greatest height of all men in this KB, the set of properties can be converted into {man, height = maximum}, where the exact height has been pruned away. The new description can be realized as “the tallest man” or simply as “the tall man” (provided the referent’s height exceeds a certain minimum value). The algorithm becomes more complicated when sets are referred to (because the elements of the target set may not all have the same heights), or when two or more gradable properties are combined (as in “the strong, tall man in the expensive car”) (van Deemter 2006). Variations and Extensions. Horacek (2005) integrates vagueness with other types of uncertainty. Horacek could be said to depict an REG algorithm as essentially a gambler who wants to maximize the chance of the referent being identiﬁed on the basis of 185  Computational Linguistics  Volume 38, Number 1  the generated expression. Other things being equal, for example, it may be safer to identify a dog as being “owned by John,” than as being “tall,” because the latter involves borderline cases. A similar approach can be applied to perceptual uncertainty (as when it is uncertain whether the hearer will be able to observe a certain property), or to the uncertainty associated with little-known words (e.g., will the hearer know what a basset hound is?) Quantifying all types of uncertainties could prove problematic in practice, yet by portraying a generator as a gambler, Horacek has highlighted an important aspect of reference generation which had so far been ignored. Crucially, his approach makes the success of a description a matter of degrees. The idea that referential success is a matter of degrees appears to be conﬁrmed by recent applications of REG to geo-spatial data. Here there tend to arise situations in which it is simply not feasible to produce a referring expression that identiﬁes its target with absolute precision (though good approximations may exist). Once again, the degree of success of a referring expression becomes gradable. Suppose you were asked to describe that area of Scotland where the temperature is expected to fall below zero on a given night, based on some computer forecast of the weather. Even if we assume that this is a well-deﬁned area with crisp boundaries, it is not feasible to identify the area precisely, because listing all the thousands of data points that make up the area separately is hardly an option. Various approximations are possible, including: (3) Roads above 500 meters will be icy. (4) Roads in the Highlands will be icy. Descriptions of this kind are generated by a system for road gritting, where the decision of which roads to treat with salt depends on the description generated by the system (Turner, Sripada, and Reiter 2009): Roads where temperatures are predicted to be icy should be treated with salt; others should not. These two descriptions are arguably only partially successful in singling out the target area. Generally speaking, one can distinguish between false positives and false negatives: The former are roads that are covered by the description but should not be (because the temperature there is not predicted to fall below zero); the latter are icy roads that will be left un-gritted. Turner and colleagues decided that it would be unacceptable to have even one false negative. In other situations, safety (from accidents) and environmental damage (through salt) might be traded off in different ways, for example, by associating a ﬁnite cost with each false positive and a possibly different cost with each false negative, and choosing the description that is associated with the lowest total cost (van Deemter 2010, pages 253–254). Again, a crucial and difﬁcult part is to come up with the right cost ﬁgures. 3.4 Degrees of Salience and the Generation of Pronouns When we speak about the world around us, we do not pay equal attention to all the objects in it. In a novel, for example, a sentence like “Smiley saw the man approaching” does not mean that Smiley saw the only man: It simply means that Smiley saw the man who is most salient at this stage of the novel. Passonneau (1996) and Jordan (2000) have shown how algorithms such as the IA may produce reasonable referring expressions “in context,” by limiting the set of salient objects in some sensible way—for example, to those objects mentioned in the previous utterance. Salience, in these works, was treated as a two-valued, “black-or-white” concept. But perhaps it is more natural to 186  Krahmer and van Deemter  Computational Generation of Referring Expressions  think of salience—just like height or age—as coming in degrees. Existing theories of linguistic salience do not merely separate what is salient from what is not. They assign referents to different salience bands, based on factors such as recency of mention and syntactic structure (Gundel, Hedberg, and Zacharski 1993; Hajic˘ova´ 1993; Grosz, Joshi, and Weinstein 1995).  Salience and Context-Sensitive REG. Early REG algorithms (Kronfeld 1990; Dale and Reiter 1995) assumed that salience could be modeled by means of a focus stack (Grosz and Sidner 1986): A referring expression is taken to refer to the highest element on the stack that matches its description (see also DeVault, Rich, and Sidner 2004). Krahmer and Theune (2002) argue that the focus stack approach is not ﬂexible enough for context-sensitive generation of descriptions. They propose to assign individual salience weights (sws) to the objects in the domain, and to reinterpret referring expressions like “the man” as referring to the currently most salient man. Once such a gradable notion of salience is adopted, we are back in the territory of Section 3.3. One simple way to generate context-sensitive referring expressions is to keep the algorithm of Figure 2 exactly as it is, but to limit the set of distractors to only those domain elements whose salience weight is at least as high as that of the target r. Line 3 (Figure 2) becomes:  3’.  C ← {x | sw(x) ≥ sw(r)} − {r}  To see how this works, consider the knowledge base of Table 1 once again, assuming that sw(d1) = sw(d2) = 10, and sw(d3) = 0 (d1 and d2 are salient, for example, because they were just talked about, and d3 was not). Suppose we keep the same domain and preference order as before. Now if d1 is the target, then, according to the new deﬁnition 3’, C = {d1, d2} − {d1} = {d2} (i.e., d2 is the only distractor which is at least as salient as the target, d1). The algorithm will select type, man , which rules out the sole distractor d2, leading to a successful reference (“The man”). If, however, d3 would be the target then C = {d1, d2, d3} − {d3} = {d1, d2}, and the algorithm would operate as normal, producing a description realizable as “the man in the t-shirt.” Krahmer and Theune chose to graft a variant of this idea onto the IA, but application to other algorithms is straightforward. Krahmer and Theune (2002) compare two theories of computing linguistic salience—one based on the hierarchical focus constraints of Hajic˘ova´ (1993), the other on the centering constraints of Grosz, Joshi, and Weinstein (1995). They argue that the centering constraints, combined with a gradual decrease in salience of non-mentioned objects (as in the hierarchical focus approach) yields the most natural results. Interestingly, the need to compute salience scores can affect the architecture of the REG module. In Centering Theory, for instance, the salience of a referent is co-determined by the syntactic structure of the sentence in which the reference is realized; it matters whether the reference is in subject, object, or another position. This suggests an architecture in which REG and syntactic realization should be interleaved, a point to which we return subsequently.  Variations and Extensions. Once salience of referring expressions is taken into account and they are no longer viewed as de-contextualized descriptions of their referent, a number of questions come up. When, for example, is it appropriate to use a demonstrative (“this man,” “that man”), or a pronoun (“he,” “she”)? As for demonstratives, it has proven remarkably difﬁcult to decide when these should be used, and even harder to choose between the different types of demonstratives (Piwek 2008). Concerning pronouns, Krahmer and Theune suggested that “he” abbreviates “the (most salient)  187  Computational Linguistics  Volume 38, Number 1  man,” and “she” “the (most salient) woman.” In this way, algorithms for generating distinguishing descriptions might also become algorithms for pronoun generation. Such an approach to pronoun generation is too simple, however, because additional factors are known to determine whether a pronoun is suitable or not (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000; Callaway and Lester 2002; Kibble and Power 2004). Based on analyses of naturally occurring texts, McCoy and Strube (1999), for example, emphasized the role of topics and discourse structure for pronoun generation, and pointed out that the changes in time scale are a reliable cue for this. In particular, they found that in certain places a deﬁnite description was used where a pronoun would have been unambiguous. This happened, for example, when the time frame of the current sentence differed from that of the sentence in which the previous mention occurred, as can be signaled, for example, by a change in tense or a cue-phrase such as “several months ago.” Kibble and Power (2004), in an alternative approach, use Centering Theory as their starting point in a constraint-based text generation framework, taking into account constraints such as salience, cohesion, and continuity for the choice of referring expressions. Many studies on contextual reference take text as their starting point (Poesio and Vieira 1998; Belz et al. 2010, among others), unlike the majority of REG research discussed so far, which uses standard knowledge representations of the kind exempliﬁed in Table 1 (or some more sophisticated frameworks, see Section 4). An interesting variant is presented by Siddharthan and Copestake (2004), who set themselves the task of generating a referring expression at a speciﬁc point in a discourse, without assuming that a knowledge base (in the normal sense of the word) is available: All their algorithm has to go by is text. For example, a text might start saying “The new president applauded the old president.” From this alone, the algorithm has to ﬁgure out whether, in the next sentence, it can talk about “the old president” (or some other suitable noun phrase) without risk of misinterpretation by the reader. The authors argue that standard REG methods can achieve reasonable results in such a setting, particularly (as we shall see next) with respect to the handling of lexical ambiguities that arise when a word can denote more than one property. Lexical issues such as these transcend the selection of semantic properties. Clearly, it is time for us to consider matters that lie beyond Content Determination. Before we do this, however, we would like to mention that differences in salience can also be caused by nonlinguistic factors: Some domain objects, for example, may be less salient because they are further removed from the hearer than others. Paraboni, van Deemter, and Masthoff (2007) demonstrated experimentally that such situations may require substantial deviations from existing algorithms to avoid causing unreasonable amounts of work to the reader. To see the idea, consider the way we refer to a (non-salient) address on a map: we probably don’t say “Go to house number 3012 in Aberdeen,” even if only one house in Aberdeen has that number and this description is thus perfectly distinguishing. It is more likely we say something like “Go to house number 3012 So-and-so Road, in the West End of Aberdeen,” adding logically redundant information speciﬁcally to aid the hearer’s search. 3.5 Beyond Content Determination In many early REG proposals, Lexical Choice and Surface Realization follow Content Determination, in the style of a pipeline, with most of the actual research focusing predominantly on Content Determination. One might have thought that good results are easy to achieve by sending the output of the Content Determination module to 188  Krahmer and van Deemter  Computational Generation of Referring Expressions  a generic realizer (i.e., a program converting meaning representations into natural language). With hindsight, any such expectations must probably count as naive. Some REG studies have taken a different approach, interleaving Content Determination and Surface Realization (Horacek 1997; Stone and Webber 1998; Krahmer and Theune 2002; Siddharthan and Copestake 2004), running counter to the pipeline architecture (Mellish et al. 2006). In this type of approach, syntactic structures are built up in tandem with semantic descriptions: when type, man has been added to the semantic description, a partial syntactic tree is constructed for a noun phrase, whose head noun is man. As more properties are added to the semantic description, appropriate modiﬁers are slotted into the syntax tree; ﬁnally, the noun phrase is completed by choosing an appropriate determiner. Even in these interleaved architectures, it is often assumed that there is a one-to-one correspondence between properties and words; but often a property can be expressed by different words, one of which may be more suitable than the other—for example, because it is unambiguous whereas the other is not (Siddharthan and Copestake 2004). One president may be “old” in the sense of former, whereas another is “old” in the sense of aged, in which case “the old president” can become ambiguous between the two people. To deal with the choice between “old” and “former,” Siddharthan and Copestake propose to look at discriminatory power, the idea being that in this case “former” rules out more distractors than “old” (both presidents are old). One wonders, however, to what extent readers interpret ambiguous words “charitably”: Suppose two presidents are aged, while only one is the former president. In this situation, “the old president” seems clear enough, because only one of its two interpretations justiﬁes the deﬁnite article (namely the one where “old” is to be understood as “former”). Clearly, people’s processing of ambiguous expressions is an area where there is still much to explore. If we turn away from Siddharthan and Copestake’s set-up, and return to the situation where generation starts from a non-textual knowledge base, similar problems with ambiguities may arise. In fact, the problem is not conﬁned to Lexical Choice: Ambiguities can arise during Surface Realization as well. To see this, suppose Content Determination has selected the properties man and with telescope to refer to a person, and the result after Surface Realization and Lexical Choice is “John saw the man with the telescope”; then, once again, the clarity of the semantic description can be compromised by putting the description in a larger context, causing an attachment ambiguity, which may sometimes leave it unclear what man is the intended referent of the description. The generator can save the day by choosing a different realization, generating “John saw the man who holds the telescope” instead. Similar ambiguities occur in conjoined references to plurals, as in “the old men and women,” where “old” may or may not pertain to the women. These issues have been studied in some detail as part of a systematic study of the ambiguities that arise in coordinated phrases of the form “the Adjective Noun and Noun,” asking when such phrases give rise to actual comprehension problems, and when they should be avoided by a generator (Chantree et al. 2005; Khan, van Deemter, and Ritchie 2008). When the generated referring expressions are realized in a medium richer than plain text, for instance, in the context of a virtual character (Gratch et al. 2002), another set of issues comes into play. It needs to be decided, then, which words should be emphasized in speech, possibly in combination with visual cues such as eyebrow movements and other gestures. Doing full justice to the expanding literature on multimodal reference is beyond the scope of this survey, but a few pointers may be useful. Various early studies looked at multimodal reference (Lester et al. 1999). One account, where pointing 189  Computational Linguistics  Volume 38, Number 1  gestures directly enter the Content Determination module of REG, is presented by van der Sluis and Krahmer (2007), who focus on the trade-off between gestures and words. Kopp, Bergmann, and Wachsmuth (2008) are more ambitious, modeling different kinds of pointing gestures and integrating their approach with the generation strategy of Stone et al. (2003). 3.6 Discussion Early REG research made a number of simplifying assumptions, and as a result the early REG algorithms could only generate a limited variety of referring expressions. When researchers started lifting some of these assumptions, this resulted in REG algorithms with an expanded repertoire, being able to generate, for instance, plural and relational descriptions. This move created a number of new challenges, however. For instance, the number of ways in which one can refer to a set of target objects increases, so choosing a good referring expression is more difﬁcult as well. Should we prefer, for example, “the men not wearing an overcoat,” “the young man and the old man,” or “the men left of the woman”? In addition, from a search perspective, the various proposals result in a larger search space, making computational issues more pressing. For some of the extensions (e.g., where Boolean combinations of properties are concerned), the complexity of the resulting algorithm is substantially higher than that of the base IA. Moreover, researchers have often zoomed in on one extension of the IA, developing a new version which lifts one particular limitation. Combining all the different extensions into one algorithm which is capable of, say, generating references to salient sets of objects, using negations and relations and possibly vague properties, is a non-trivial enterprise. To give just one example, consider what happens when we combine salience with (other) gradable properties (cf. Sections 3.4 and 3.3). Should “the old man” be interpreted as “the oldest of the men that are sufﬁciently salient” or “the most salient of the men that are sufﬁciently old”? Expressions that combine gradable properties can easily become unclear, and determining when such combinations are nevertheless acceptable is an interesting challenge. Some simplifying assumptions have only just begun to be lifted, through extensions that are only in their infancy, particularly in terms of their empirical validation. Other simplifying assumptions are still in place. For instance, there is a dearth of work that addresses functions of referring expressions other than mere identiﬁcation. Similarly, even recent proposals tend to assume that it is unproblematic to determine what information is shared between speaker and hearer. We return to these issues in Section 6.  4. REG Frameworks Most early REG algorithms represent knowledge in a very basic way, speciﬁcally designed for REG. This may have been justiﬁed at the time, but years of research in Knowledge Representation (KR) suggest that such a carefree attitude towards the modeling of knowledge may not be wise in the long run. For example, when wellestablished KR frameworks are used, it may become possible to re-use existing algorithms for these frameworks, which have often been optimized for speed, and whose computational properties are well understood. Depending on the choice of framework, many other advantages can ensue. Because research that couples REG with KR is relatively new, and technical properties of the frameworks themselves can be easily found elsewhere, we shall be comparatively brief. For each framework, we focus on three 190  Krahmer and van Deemter  Computational Generation of Referring Expressions  questions: (a) How is domain information represented? (b) How is the semantic content of a referring expression represented? and (c) How can distinguishing descriptions be found? 4.1 REG Using Graph Search One of the ﬁrst attempts to link REG with a more generic mathematical formalism was the proposal by Krahmer, van Erk, and Verleg (2003), who used labeled directed graphs for this purpose. In this approach, objects are represented as the nodes (vertices) in a graph, and the properties of and relations between these objects are represented as edges connecting the nodes. Figure 4 shows a graph representation of our example domain. One-place relations (i.e., properties) such as man are modeled as loops (edges beginning and ending in the same node), whereas two-place relations such as left of are modeled as edges between different nodes. Two kinds of graphs play a role: a scene graph representing the knowledge base, and referring graphs representing the content of referring expressions. The problem of ﬁnding a distinguishing referring expression can now be deﬁned as a comparison between graphs. More speciﬁcally, it is a graph search problem: Given a target object (i.e., a node in the scene graph), look for a distinguishing referring graph that is a subgraph of the scene graph and uniquely characterizes the target. Intuitively, such a distinguishing graph can be “placed over” the target node with its associated edges, and not over any other node in the scene graph. The informal notion of one graph being “placed over” another corresponds with a subgraph isomorphism (Read and Corneil 1977). Figure 5 shows a number of referring graphs which can be placed over our target object d1. The leftmost, which could be realized as “the man,” fails to distinguish our target, because it can be “placed over” the scene graph in two different ways (over nodes 1 and 3). Krahmer, van Erk, and Verleg (2003) use cost functions to guide the search process and to give preference to some solutions over others. They assume that these cost functions are monotonic, so extending a graph can never make it cheaper. Graphs are compatible with many different search algorithms, but Krahmer et al. employ a simple branch and bound algorithm for ﬁnding the cheapest distinguishing graph for a given target object. The algorithm starts from the graph containing only the node representing the target object and recursively tries to extend this graph by adding adjacent edges: edges starting from the target, or in any of the other vertices added later on to the  Figure 4 Representation of our example scene in Figure 1 as a labeled directed graph. 191  Computational Linguistics  Volume 38, Number 1  Figure 5 Some referring graphs for target d1. referring graph under construction. For each referring graph, the algorithm checks which objects in the scene graph it may refer to, other than the target; these are the distractors. As soon as this set is empty, a distinguishing referring graph has been found. At this point, only alternatives that are cheaper than this best solution found so far need to be inspected. In the end, the algorithm returns the cheapest distinguishing graph which refers to the target, if one exists; otherwise it returns the empty graph. One way to deﬁne the cost function would be to assign each edge a cost of one point. Then the algorithm will output the smallest graph that distinguishes a target (if one exists), just as the Full Brevity algorithm would. Alternatively, one could assign costs in accordance with the list of preferred attributes in the IA, making more preferred properties cheaper than less preferred ones. A third possibility is to compute the costs of an edge e in terms of the probability P(e) that e occurs in a distinguishing description (which can be estimated by counting occurrences in a corpus), making frequent properties cheap and rare ones expensive: cost(e) = −log2(P(e)) Experiments with stochastic cost functions have shown that these enable the graphbased algorithm to capture a lot of the ﬂexibility of human references (Krahmer et al. 2008; Viethen et al. 2008). In the graph-based perspective, relations are treated in the same way as individual properties, and there is no risk of running into inﬁnite loops (“the cup to the left of the saucer to the right of the cup . . . ”). Unlike Dale and Haddock (1991) and Kelleher and Kruijff (2006), no special measures are required, because a relational edge is either included in a referring graph or not: including it twice is not possible. Van Deemter and Krahmer (2007) show that many of the proposals discussed in Section 3 can be recast in terms of graphs. They argue, however, that the graph-based approach is ill-suited for representing disjunctive information. Here, the fact that directed graphs are not a fully ﬂedged KR formalism makes itself felt. Whenever an REG algorithm needs to reason with complex information, heavier machinery is required. 4.2 REG Using Constraint Satisfaction Constraint satisfaction is a computational paradigm that allows efﬁcient solving of NP– hard combinatoric problems such as scheduling (van Hentenryck 1989). It is among the earliest frameworks proposed for REG (Dale and Haddock 1991), but in later years, this approach has seldom been emphasized—with a few notable exceptions, such as Stone and Webber (1998)—until Gardent (2002) showed how constraint programming can be 192  Krahmer and van Deemter  Computational Generation of Referring Expressions  used to generate expressions that refer to sets. She proposed to represent a description L for a target set S as a pair of set variables: LS = P+S , P−S , where one variable (P+S ) ranges over sets of properties that are true of the elements in S and the other (P−S ) over properties that are false of the elements in S. The challenge— taken care of by existing constraint solving programs—is to ﬁnd suitable values (i.e., sets of properties) for these variables. To be “suitable,” values need to fulﬁl a number of REG-style constraints: 1. All the properties in P+S are true of all elements in S. 2. All the properties in P−S are false of all elements in S. 3. For each distractor d there is a property in P+S which is false of d, or there is a property in P−S which is true of d. The third clause says that every distractor is ruled out by either a positive property (i.e., a property in P+S ) or a negative property (i.e., a property in P−S ), or both. An example of a distinguishing description for the singleton target set {d1} in our example scene would be {man}, {right} , because d1 is the only object in the domain who is both a man and not on the right. The approach can be adapted to accommodate disjunctive properties to enable reference to sets (Gardent 2002). Constraint satisfaction is compatible with a variety of search strategies (Kumar 1992). Gardent opts for a “propagate-and-distribute” strategy, which means that solutions are searched for in increasing size, ﬁrst looking for single properties, next for combinations of two properties, and so forth. This amounts to the Full Brevity search strategy, of course. Accordingly, Gardent’s algorithm yields a minimal distinguishing description for a target, provided one exists. Given the empirical questions associated with Full Brevity, it may well be worthwhile to explore alternative search strategies. The constraint approach allows an elegant separation between the speciﬁcation of the REG problem and its implementation. Moreover, the handling of relations is straightforwardly applicable to relations with arbitrary numbers of arguments. Gardent’s approach does not run into the aforementioned problems with inﬁnite loops, because a set of properties (being a set) cannot contain duplicates. Yet, like the labeled graphs, the approach proposed by Gardent has signiﬁcant limitations, which stem from the fact that it does not rest on a fully developed KR system. General axioms cannot be expressed, and hence cannot play a role in logical deduction. We are forced to re-visit the question of what is the best way for REG to represent and reason with knowledge.  4.3 REG Using Modern Knowledge Representation To ﬁnd out what is missing, let us see what happens when domains scale up. Consider a furniture domain, and suppose every chair is in a room, that every room is in an apartment, and every apartment in a house. Listing all relevant relations between individual objects separately (“chair a is in room b,” “room b is in apartment c,” “chair a is in apartment c,” “apartment c is in house d”) is onerous, error prone, spaceconsuming, and messy. Modern KR systems solve this problem by employing axioms (e.g., expressing transitivity of the “in” relation; if x is in y, and y is in z, then x is in z). 193  Computational Linguistics  Volume 38, Number 1  Logical inference allows the KR system to derive implicit information. For example, from “chair a is in room b,” “room b is in apartment c,” and “apartment c is in house d,” the transitivity of “in” allows us to infer that “chair a is in house d”. This combination of basic facts and general axioms allows a succinct and insightful representation of facts. Modern KR comes in different ﬂavors. Recently, two different KR frameworks have been linked with REG, one based on Conceptual Graphs (Croitoru and van Deemter 2007), the other on Description Logics (Gardent and Striegnitz 2007; Areces, Koller, and Striegnitz 2008). The ﬁrst have their origin in Sowa (1984) and were greatly enhanced by Baget and Mugnier (2002). The latter grew out of work on KL-ONE (Brachman and Schmolze 1985) and became even more prominent in the wider world of computing when they came to be linked with the ontology language OWL, which underpins current work on the semantic Web (Baader et al. 2003). Both formalisms represent attempts to carve out computationally tractable fragments of First-Order Predicate Logic for deﬁning and reasoning about concepts, and are closely related (Kerdiles 2001). For reasons of space, we focus on Description Logic. The basic idea is that a referring expression can be modeled as a formula of Description Logic, and that REG can be viewed as the problem of ﬁnding a particular kind of formula, namely, one that denotes (i.e., refers to) the target set of individuals. Let us revisit our example domain, casting it as a logical model M, as follows: M = D, . , where D (the domain) is a ﬁnite set {d1, d2, d3} and . is an interpretation function which gives the denotation of the relevant predicates (thus: man = {d1, d3}, left-of = { d1, d2 , d2, d3 } etc.). Now the REG task can be formalized as: Given a model M and a target set S ⊆ D, look for a Description Logic formula ϕ such that ϕ = S. The following three expressions are the Description Logic counterparts of the referring graphs in Figure 5: (a) man (b) man wears suit (c) man ∃ left-of.(woman wears t-shirt) The ﬁrst, (a), would not be distinguishing for d1 (because its denotation includes d3), but (b) and (c) would. Note that represents the conjunction of properties, and ∃ represents existential restriction. Negations can be added straightforwardly, as in man ¬ wears suit, which denotes d3. Areces, Koller, and Striegnitz (2008) search for referring expressions in a somewhat non-standard way. In particular, their algorithm does not start with one particular target referent: It simply attempts to ﬁnd the different sets that can be referred to. They start from the observation that REG can be reduced to computing the similarity set of each domain object. The similarity set of an individual x is the set of those individuals that have all the properties that x has. Areces et al. present an algorithm, based on a proposal by Hopcroft (1971), which computes the similarity sets, along with a Description Logic formula associated with each set. The algorithm starts by partitioning the domain using atomic concepts such as man and woman, which splits the domain in two subsets ({d1, d3} and {d2} respectively). At the next stage, ﬁner partitions are made by making use of concepts of the form ∃R.AtomicConcept (e.g., men left of a woman), and so on, always using concepts established during one phase to construct more complex concepts during the next. All objects are considered in parallel, so there is no risk of inﬁnite loops. Control 194  Krahmer and van Deemter  Computational Generation of Referring Expressions  over the output formulae is achieved by specifying an incremental preference order over possible expressions, but alternative control strategies could have been chosen. 4.4 Discussion Even though the role of KR frameworks for REG has received a fair bit of attention in recent years, one can argue that this constitutes just the ﬁrst steps of a longer journey. The question of which KR framework suits REG best, for example, is still open; which framework has the best coverage, which allows all useful descriptions to be expressed? Moreover, can referring expressions be found quickly in a given framework, and is it feasible to convert these representations into adequate linguistic realizations? Given the wealth of possibilities offered by these frameworks, it is remarkable that much of their potential is often left unused. In Areces, Koller, and Striegnitz’s (2008) proposal, for example, generic axioms do not play a role, nor does logical inference. Ren, van Deemter, and Pan (2010) sketch how REG can beneﬁt if the full power of KR is brought to bear, using Description Logic as an example. They show how generic axioms can be exploited, as in the example of the furniture domain, where a simple transitivity axiom allows a more succinct and insightful representation of knowledge. Similarly, incomplete information can be used, as when we know that someone is either Dutch or Belgian, without knowing which of the two. Finally, by making use of more expressive fragments of Description Logic, it becomes possible to identify objects that previous REG algorithms were unable to identify, as when we say “the man who owns three dogs,” or “the man who only kisses women,” referring expressions that were typically not considered by previous REG algorithms. Extensions of this kind raise new empirical questions, as well. It is an open question, for instance, when human speakers would be inclined to use such complex descriptions. These problems existed even in the days of the classic REG algorithms (when it was already possible to generate lengthy descriptions) but they have become more acute now that it is possible to generate structurally complex expressions as well. There is a clear need for empirical work here, which might teach us how the power of these formalisms ought to be constrained. 5. Evaluating REG Pre-2000 REG research gave little or no attention to the empirical evaluation of algorithms. More recently, however, REG evaluation studies have started to be carried out more and more often. It appears that most of these were predicated on the assumption (debated in Section 7) that REG algorithms should try to generate expressions that are optimally similar to those produced by human speakers or writers, even though— importantly—this assumption was seldom made explicit. The dominant method at the moment is, accordingly, to measure the similarity between generated expressions and those in a suitable corpus of referring expressions. REG came late to corpus-based evaluation (compared to other parts of computational linguistics) because suitable data sets are hard to come by. In this section, we discuss what criteria a data set should meet to make it suitable for REG evaluation, and we survey which collections are currently available. In addition, we discuss how one is to determine the performance of an REG algorithm on a given data set. We shall see that although much work has been done in recent years, there are still signiﬁcant open questions, particularly regarding the relation between automatic metrics and human judgments. 195  Computational Linguistics  Volume 38, Number 1  5.1 Corpora for REG Evaluation Text corpora are full of referring expressions. For evaluating the realization of referring expressions, such corpora are very suitable, and various researchers have used them, for instance, to evaluate algorithms for modiﬁer orderings (Shaw and Hatzivassiloglou 1999; Malouf 2000; Mitchell 2009). Text corpora are also important for the study of anaphoric links between referring expressions. The texts that make up the GNOME corpus (Poesio et al. 2004), for instance, contain descriptions of museum objects and medical patient information leaﬂets, with each of the two subcorpora containing some 6,000 NPs. Much information is marked up, including anaphoric links. Yet, text corpora of this kind are of limited value for evaluating the content selection part of REG algorithms. For that, one needs a corpus that is fully “semantically transparent” (van Deemter, van der Sluis, and Gatt 2006): A corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target. Text corpora such as GNOME do not meet this requirement, and it is often difﬁcult or impossible to add all necessary information, because of the size and complexity of the relevant domains. For this reason, data sets for content selection evaluation are typically collected via experiments with human participants in simple and controlled settings. Broadly speaking, two kinds of experimental corpora can be distinguished: corpora speciﬁcally collected with reference in mind, and corpora collected wholly or partly for other purposes but which have nevertheless been analyzed for the referring expressions in them. We will brieﬂy sketch some corpora of the latter kind, after which we shall discuss the former in more detail. General-Purpose Corpora. One way to elicit “natural” references is to let participants perform a task for which they need to refer to objects. An example is the corpus of so-called pear stories of Chafe (1980), in which people were asked to describe a movie about a man harvesting pears, in a ﬂuent narrative. The resulting narratives featured such sequences as “And he ﬁlls his thing with pears, and comes down and there’s a basket he puts them in. . . . And then a boy comes by, on a bicycle, the man is in the tree, and the boy gets off his bicycle . . . ,” where a limited set of individuals come up several times. The referring expressions in a subset of these stories were analyzed by Passonneau (1996), who asked how the form of the re-descriptions (such as “he,” “them,” and “the man”) in these narratives might best be predicted, comparing “informational” considerations (which form the core of most algorithms in the tradition started by Dale and Reiter, as we have seen) with considerations based on Centering Theory (Grosz, Joshi, and Weinstein 1995). Passonneau, who tested her rules on 319 noun phrases, found support for an integrated model, where centering constraints take precedence over informational considerations. The well-known Map Task corpus (Anderson et al. 1991) is another example of a corpus in which reference plays an important role. It consists of dialogues between two participants; both have maps with landmarks indicated, but only one (the instruction giver) has a route on the map and he or she instructs the other (the follower) about this particular route. Referring expressions are routinely produced in this task to refer to the landmarks on the maps (“the cliff”). Participants use these not only for identiﬁcation purposes but also, for instance, to verify whether they understood their dialogue partner correctly. In the original Map Task corpus, the landmarks were labeled with proper names (“cliff”), making them less suitable for studying content determination. To facilitate the study of reference, the iMap corpus was created (Guhe and Bard 2008), a modiﬁed version of the Map Task corpus where landmarks are not labelled, and systematically differ along a number of dimensions, including type (owl, penguin, etc.), 196  Krahmer and van Deemter  Computational Generation of Referring Expressions  number (singular, plural) and color; a target may thus be referred to as “the two purple owls.” Because participants may refer to targets more than once, it becomes possible to study initial and subsequent reference (Viethen et al. 2010). Yet another example is the Coconut corpus (Di Eugenio et al. 2000), a set of taskoriented dialogues in which participants negotiate which furniture items they want to buy on a ﬁxed, shared budget. Referring expressions in this corpus (“a yellow rug for 150 dollars”) do not only contain information to identify a particular piece of furniture, but also include properties which directly refer to the task at hand (e.g., how much money is still available for a particular furniture item and what the state of agreement between the negotiators is). An attractive aspect of these corpora is that they represent fairly realistic communication, related to a more or less natural task. However, in these corpora, the identiﬁcation of objects tends to be mixed with other communicative tasks (veriﬁcation, negotiating). This does not mean that the corpora in question are unsuitable for the study of reference, of course. More speciﬁcally, they have been used for evaluating REG algorithms, to compare the performance of traditional algorithms with special-purpose algorithms that take dialogue context into account (Jordan and Walker 2005; Gupta and Stent 2005; Passonneau 1996). For example, when the speaker attempts to persuade the hearer to buy an item, Jordan’s Intentional Inﬂuences algorithm selects those properties of the item that make it a better solution than a previously discussed item. In yet other situations—for example, when a summarization is offered—all mutually known properties of the item are selected. Jordan’s algorithm outperforms traditional algorithms, which is perhaps not surprising given that the latter were not designed to deal with references in interactive settings (Jordan 2000). Dedicated Corpora. In recent years, a number of new corpora have been collected, speciﬁcally focusing on the types of referring expressions that we are focusing on in this survey. A number of such corpora are summarized in Table 2. In some ways, these corpora are remarkably similar. Reﬂecting the prevalent aims of research on REG, for example, they focus on descriptions that aim to identify their referent “in one shot,” disregarding the linguistic context of the expression (i.e., in the “null context,” as it is sometimes called [Viethen and Dale 2007]). In all these corpora, participants were asked to refer to targets in a visual scene also containing the distractors. This set-up means that the properties of target objects and their distractors are known, which makes it comparatively easy to make these corpora semantically transparent by annotating the references that were produced. In addition, most corpora are “pragmatically  Table 2 Overview of dedicated Referring Expression corpora (alphabetical), with for each corpus a representative reference, an indication of the domain, and the number of participants and collected distinguishing descriptions.  Corpus Name  Reference  Domain  Participants Descriptions  Bishop Gorniak & Roy (2004)  Colored cones in 3D scene  9  Drawer Viethen & Dale (2006)  Drawers in ﬁling cabinet  20  GRE3D3 Viethen & Dale (2008)  Spheres, Cubes in 3D scene  63  iMap Guhe & Bard (2008)  Various objects on a map  64  TUNA van Deemter et al. (in press) Furniture, People  60  447 140 630 9,567 2,280  197  Computational Linguistics  Volume 38, Number 1  transparent” as well, meaning that the communicative goals of the participants were known (typically identiﬁcation). An early example is the Bishop corpus (Gorniak and Roy 2004). For this data set, participants were asked to describe objects in various computer generated scenes. Each of these scenes contained up to 30 objects (“cones”) randomly positioned on a virtual surface. All objects had the same shape and size, and hence targets could only be distinguished using their color (either green or purple) and their location on the surface (“the green cone at the left bottom”). Each participant was asked to identify targets in one shot, and for the beneﬁt of an addressee who was physically present but did not interact with the participant. The Drawer corpus, collected by Viethen and Dale (2006), has a similar objective, but here targets are real, being one of 16 colored drawers in a ﬁling cabinet. On different occasions, participants were given a random number between 1 and 16 and asked to refer to the corresponding drawer for an onlooker. Naturally, they were asked not to use the number; instead they could refer to the target drawers using color, row, and column, or some combination of these. In this corpus, referring expressions (“the pink drawer in the ﬁrst row, third column”) once again solely serve an identiﬁcation purpose. Viethen and Dale (2008) also collected another corpus (GRE3D3) speciﬁcally looking at when participants use spatial relations. For this data collection, participants were presented with 3D scenes (made with Google SketchUp) containing three simple geometric objects (spheres and cubes of different colors and sizes, and in different conﬁgurations), of which one was the target. Viethen and Dale (2008) found that spatial relations were frequently used (“the ball in front of the cube”), even though they were never required for identiﬁcation. Whether this generalizes to other visual scenes (in which spatial relations are less immediately ‘available’) is an interesting question for future research. The TUNA corpus (Gatt, van der Sluis, and van Deemter 2007; van Deemter et al. in press) was collected via a Web-based experiment, in which singular and plural descriptions were gathered by showing participants one or two targets, where the plural targets could either be similar (same type) or dissimilar (different type). Targets were always displayed with six distractors, and the resulting domain objects were randomly positioned in a 3 x 5 grid, with targets surrounded by a red border. Example trials are shown in Figure 6. The corpus contains two different domains: a furniture domain and a people domain. The ﬁrst domain is based on pictures of furniture and household items, taken from the Object Databank (see http://www.tarrlab.org/). These were manipulated so that besides type (chair, desk, fan) also color, orientation, and size could systematically be varied. The number of possible attributes and values in the people domain is much  Figure 6 Example trials from the TUNA corpus, a singular trial for the furniture domain (“the small blue fan,” left) and a plural trial for the people domain (“the men with glasses,” right). 198  Krahmer and van Deemter  Computational Generation of Referring Expressions  larger (and more difﬁcult to pin down); this domain consists of a set of black and white photographs of people (all famous mathematicians) used in an earlier study of van der Sluis and Krahmer (2007). Properties of these photographs include gender, head orientation, age, beard, hair, glasses, suit, shirt, and tie. It is interesting to note that the TUNA corpus was designed to have one shortest description for each target, whereas in other data sets, such as Viethen and Dale’s (2006) drawer corpus, a single shortest description does not always exist. The TUNA corpus has formed the basis of three shared REG challenges, to which we turn now.  5.2 Evaluation Metrics  How should we compare human descriptions with those produced by a REG algorithm? When looking for measures that compute the content overlap, one source of inspiration may come from biology and information retrieval (van Rijsbergen 1979). One measure used in these ﬁelds is the Dice (1945) coefﬁcient, which was originally proposed to quantify ecologic association between species, and was ﬁrst applied to REG by Gatt, van der Sluis, and van Deemter (2007). The Dice coefﬁcient—which is not dissimilar to the “match” function used by Jordan (2000)—is computed by scaling the number of elements that two sets have in common, by the size of the two sets combined:  Dice(A, B)  =  2 × |A ∩ B| |A| + |B|  (1)  The Dice measure ranges from 0 (no agreement; i.e., no elements shared between A and B) to 1 (complete agreement; A and B share all elements). For REG, A and B can be understood as attributes (e.g., type) or as attribute–value pairs (properties; type, man ). The former option tends to be used in earlier work, but has the somewhat counterintuitive consequence that two descriptions which express different values of the same attribute (“the man” and “the woman,” say, or “the dog” and “the chihuahua,” in the earlier discussed cats-and-dogs example) have a Dice score of 1. Hence, in the following discussion we shall measure overlap in terms of properties. An alternative to Dice that is sometimes used is the MASI (Measuring Agreement on Set-valued Items) metric of Passonneau (2006):  MASI(A,  B)  =  δ  ×  |A |A  ∩ ∪  B| B|  (2)  This is basically an extension of the well-known Jaccard (1901) metric with a weighting function δ which biases the score in favor of similarity where one set is a sub- or a superset of the other:  ⎧  δ  =  ⎪⎪⎪⎨ ⎪⎪⎪⎩  1,  2 3  ,  0,  
Grew: a Graph Rewriting Tool for NLP We present a Graph Rewriting Tool dedicated to NLP applications. Graph nodes contain feature structures and edges describe relations between nodes. We explain the Graph Rewriting framework we use, the implemented system and some experiments. MOTS-CLÉS : réécriture de graphes, interface syntaxe-sémantique. KEYWORDS: graph rewriting, syntax-semantics interface. 
MOTS-CLES : Traitements multi-vues, navigation enrichie. KEYWORDS : Multi-view processing, enriched navigation. 
Automated generation of text with Syntox Syntox, which includes an online user interface at URL http://www.syntox.net, is an implementation of a model based on attribute grammars, in the context of automated generation of text. The sofware is intended as a platform for experimentation with an ergonomic interface. Syntox is usable with large vocabularies and grammars to produce ambiguous texts from an explicit description of linguistic phenomena. MOTS-CLÉS : Synthèse de texte, Grammaire attribuée, Syntaxe. KEYWORDS: Text generation, Attribute Grammars, Syntax. Introduction 
A Segmenter-POS Labeller and a Chunker for French We propose a demo of two softwares : a Segmenter-POS Labeller for French and a Chunker for texts treated by the ﬁrst program. Both have been learned from the French Tree Bank. MOTS-CLÉS : étiquetage POS, chunking, apprentissage automatique, French Tree Bank, CRF. KEYWORDS: POS tagging, chunking, Machine Learning, French Tree Bank, CRF. 
SPPAS : a tool to perform text/speech alignment SPPAS is a new tool dedicated to phonetic alignments, from the LPL laboratory. SPPAS produces automatically or semi-automatically annotations which include utterance, word, syllabic and phonemic segmentations from a recorded speech sound and its transcription. SPPAS is currently implemented for French, English, Italian and Chinese There is a very simple procedure to add other languages in SPPAS : it is just needed to add related resources in the appropriate directories. SPPAS can be used by a large community of users : accessibility and portability are importants aspects in its development. The tools and resources will all be distributed with a GPL license. MOTS-CLÉS : segmentation, phonétisation, alignement, syllabation. KEYWORDS: segmentation, phonetization, alignement, syllabiﬁcation. 
Proxem develops since 2007 the NLP platform, Antelope, with which one can quickly build vertical semantic applications (for e-reputation, business intelligence or consumer reviews analysis, for instance). Antelope was used to create a Human Resources solution, notably used by APEC, making it possible (1) to extract information from resumes and offers and (2) to find the most relevant jobs matching a given resume (or vice versa). We present here how to adapt Antelope to a particular area, namely HR. MOTS-CLES : entités nommées, extraction de relations, création d’ontologies, similarité KEYWORDS : named entities, information extraction, ontologies development, matching 
Nomao : a geolocalized search engine dedicated to place recommendation and ereputation This demonstration showcases NOMAO, a geolocalized search engine which recommends places (bars, shops...) based on the user’s and its friend’s tastes and on the web surfers’ recommendations. MOTS-CLÉS : recherche d’information, analyse d’opinion, génération de texte, fouille du web. KEYWORDS: information retrieval, opinion mining, text generation, web mining. 
ABSTRACT _________________________________________________________________________________________________________ ROCme!: software for the recording and management of oral corpora ROCme! has been designed to allow a sensible, autonomous, and dematerialized management of speech recordings. Users can create interfaces for metadata collection thanks to XML tags. Speakers autonomously fill in questionnaires, record, play, and save audio; and browse sentences (or other types of corpora). ROCme! can display text, optionally with HTML formatting, images, sounds, and video. MOTS-CLÉS : corpus, oral, linguistique, logiciel KEYWORDS : corpus, oral, linguistics, software 
Recent progress in translation technology has caused a real boost for research and technology deployment. At the same time, other areas of language technology also experience scientiﬁc advances and economic success stories. However, research in machine translation is still less affected by new developments in core areas of language processing than could be expected. One reason for the low level of interaction is certainly that the predominant research paradigm in MT has not started yet to systematically concentrate on high quality translation. Most of the research and nearly all of the application efforts have focused on solutions for informational inbound translation (assimilation MT). This focus has on the one hand enabled translation of information that normally is not translated at all. In this way MT has changed work and life of many people without ever infringing on the existing translation markets. In my talk I will present a new research approach dedicated to the analytical investigation of existing quality barriers. Such a systematic thrust can serve as the basis of scientiﬁcally guided combinations of technologies including hybrid approaches to transfer and the integration of advanced methods for syntactic and semantic processing into the translation process. Together with improved techniques for quality estimation, the expected results will drive translation technology into the direction badly needed by the multilingual European society. KEYWORDS : machine translation, hybrid approaches, syntactic processing, semantic processing, quality estimation. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 4: conférences invités, page 1, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 
Language and speech proﬁciency is considered to be an important factor to identify human beings. Though traditional studies on language and speech can reveal many aspects of their characteristics, we have not yet had a complete view of human’s language and speech ability. I believe that trans-disciplinary studies will enable us to have its scientiﬁc modeling. In this talk, I would like to introduce our research efforts on segmental duration control as an example of research towards computational human modeling. The computational modeling of segmental duration that we have been studying around three decades not only contributes to prosody control in speech synthesis technology but also gives an integrated view of individual timing characteristics studied in phonetic science. Together with duration control modeling, a series of perceptual studies on duration modiﬁcations needed for model evaluation have suggested us a uniﬁed view of scientiﬁc understanding on rhythm and timing. Through the introduction of our current efforts on the objective evaluation of 2nd language (L2) proﬁciency in speech timing control, we will see that these models and ﬁndings are useful for L2 learning and acquisition. To conclude my talk, I ﬁnally introduce research consortium called AESOP (Asian English Speech cOrpus Project) where researchers in different ﬁelds (speech science, informatics, phonetics, psychology and language education) have started to work together by collecting commonly sharable L2 language and speech data. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 4: conférences invités, page 5, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 5   
Simple measures of phonological complexity have been shown to frequently correlate with each other such that greater elaboration of, say, the size of a consonant inventory or of a vowel inventory co-occurs with greater numbers of tone contrasts when a large ensemble of languages is studied. Further, these measures correlate positively with elaboration of the syllable canon. Most linguists would probably approve the suggestion by Comrie (1992), that it is easier to imagine that “earlier stages of human language [differed] qualitatively from those spoken today in being less complex”. This talk will consider to what extent it is possible to usefully speculate about the origins of phonological complexity. One tool for such speculation is the examination of the patterns of distribution of measures of complexity around the world. If language originated with humans in Africa does a hypothesis of original simplicity suggest African languages should be the simplest ? In fact, African languages have a higher mean basic vowel inventory than languages elsewhere, and the highest proportion of tone languages. These factors enable a correlation to be found between on the one hand a measure of complexity weighing these factors equally with consonant inventory size and on the other hand the distance from a presumed origin in Africa (Atkinson 2011), with complexity declining with distance. Atkinson’s suggested analogy to a genetic ‘founder effect’ is unconvincing for multiple reasons. Deep phylogenetic relationships between languages mirroring the population genetic patterns have not (yet) proved recoverable and the logic of phoneme-loss-by-distance is unpersuasive. Various alternative hypotheses linking phonological patterns with external factors have been proposed. Those that will be explored include the effects of population size, social structure, language contact situation and environmental conditions. These ideas will be evaluated in light of data in a database assembling information on over 700 languages, as well as results from smaller projects addressing syllable structure and sonority patterns in selected language samples. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 4: conférences invités, page 7, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 7  
Unsupervized Word Segmentation In this paper, we present an unsupervised segmentation system tested on Mandarine Chinese. Following Harris’s Hypothesis in Kempe (1999) and Tanaka-Ishii (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin et Tanaka-Ishii, 2006) by adding normalization and Viterbi-decoding. This enables us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and deﬁne a more precise topline for the task using cross-trained supervised system available off-the-shelf (Zhang et Clark, 2010; Zhao et Kit, 2008; Huang et Zhao, 2007) MOTS-CLÉS : Apprentissage non-supervisé, segmentation, chinois, mandarin. KEYWORDS: Unsupervized machine learning, segmentation, Mandarin Chinese. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 1–13, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 
This paper evaluates a wide range of heterogeneous semantic similarity measures on the task of predicting semantic similarity scores and the task of predicting semantic relations that hold between two terms, and investigates ways to combine these measures. We present a large-scale benchmarking of 34 knowledge-, web-, corpus-, and deﬁnition-based similarity measures. The strengths and weaknesses of each approach regarding relation extraction are discussed. Finally, we describe and test two techniques for measure combination. These combined measures outperform all single measures, achieving a correlation of 0.887 and Precision(20) of 0.979. MOTS-CLÉS : Similarité sémantique, Relations sémantiques, Similarité distributionnelle. KEYWORDS: Semantic Similarity, Semantic Relations, Distributional Similarity. 
Confidence Estimation at word level is the task of predicting the correct and incorrect words in the target sentence generated by a MT system. It helps to conclude the reliability of a given translation as well as to filter out sentences that are not good enough for post-editing. This paper investigates various types of features to circumvent this issue, including lexical, syntactic and system-based features. A method to set training label for each word in the hypothesis is also presented. A classifier based on conditional random fields (CRF) is employed to integrate features and determine the word’s appropriate label. We conducted our preliminary experiment with all features, tracked precision, recall and F-score and we compared with our baseline system. Experimental results of the full combination of all features yield the very encouraging precision, recall and F-score for Good label (F-score: 86.7%), and acceptable scores for Bad label (F-score: 36.8%). MOTS-CLES : Système de traduction automatique, mesure de confiance, estimation de la confiance, champs aléatoires conditionnels KEYWORDS : Machine translation, confidence measure, confidence estimation, conditional random fields Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 43–56, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 43  
Applying a Statistical Machine Translation Algorithm to SMS Text Message Normalization We report on the application of a statistical machine translation algorithm to the problem of SMS text message normalization. The technique is based on a greedy search algorithm described in (Langlais et al., 2007). A ﬁrst normalization is generated, then a function that generates new hypotheses is applied iteratively to a current best guess, while maximizing a scoring function. This method leads to a drop in word error rate of 33% on a held-out test set, and a BLEU score gain of over 30%. We focus on the methods of generating the initial normalization and the operations that allow us to generate new hypotheses. MOTS-CLÉS : Traduction statistique, normalisation de textos, algorithme de recherche vorace, modèle de langue. KEYWORDS: Machine translation, SMS, text message, normalization, greedy search algorithm, language model. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 71–79, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 71  
Beginnings of a Transition-Based Parsing for Non-Projectives Dependency Structures This paper presents an extension of the traditional transition-based dependency parser adapted to discontinuous dependencies and the ﬁrst results of its training on a dependency tree corpus of French. The ﬁrst experimental results will be useful for the choice of parsing conﬁguration features well adapted to discontinuous dependencies in order to ameliorate learning of head dependencies. MOTS-CLÉS : analyse syntaxique par transitions, structure de dépendance non-projective, grammaire catégorielle de dépendance. KEYWORDS: transition-based parsing, non-projective dependency structure, dependency catego- rial grammar. 
Towards Automatic Spell-Checking of Noisy Texts : General Architecture and Language Identiﬁcation for Unknown Words This paper deals with the problem of spell checking on degraded-quality corpora such as blogs, review sites and social networks. We propose a ﬁrst architecture of correction which aims at reducing overcorrection, and we describe its implementation. We also report and discuss the results obtained thanks to the module that detects whether an unknown word from a sentence in a known language belongs to this language or not. MOTS-CLÉS : Correction automatique, détection de langue, données produite par l’utilisateur. KEYWORDS: Spelling correction, language identiﬁcation, User-Generated Content. 
Creating a Multi-Tree from a Tagged Text : Annotating Spoken French This study focuses on automatic analysis of annotated transcribed speech. The annotation system considered has been recently introduced to address the several limitations of classical syntactic annotations when faced to natural speech transcriptions. It introduces many different components such as embedding, piles, kernels, pre-kernels, discursive markers etc.. All those components are tightly coupled in a complex tree structure and can hardly be considered separately because of their close intrication. Hence, a joint analysis is required but no analysis tool to handle them all together was available yet. In this study, we introduce such an automatic parser of annotated transcriptions of speech and present the corresponding framework based on multi-trees. This framework permits to jointly handle separate aspects of speech such as macro and micro syntactic levels, which are traditionnaly considered separately. Several applications are proposed, including analysis of the transcribed speech by classical parsers designed for written language. MOTS-CLÉS : Arbres syntaxiques, unité illocutoire, unités rectionnelles, micro-syntaxe, macro- syntaxe, entassement. KEYWORDS: Syntactic trees, illocutionary unit, microsyntax, macrosyntax, piles. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 109–122, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 109  
Automatic Construction of a Contextual Valence Shifters Lexicon The research presented in this paper takes place in the ﬁeld of Opinion Mining, which is mainly devoted to assigning a positive or negative label to a text or a sentence. The context of a highly polarized word plays an essential role, as it can modify its original polarity. The present work addresses this issue and focuses on the detection of polarity shifters. In a previous study, we have automatically extracted adverbs impacting the polarity of the adjectives they are associated to and qualiﬁed their inﬂuence. The extraction system has then been improved to automatically build a lexicon of contextual valence shifters. This lexicon contains lexico-syntactic patterns combined with the type of inﬂuence they have on the valence of the polarized item. The purpose of this paper is to show how the current system works and to present the evaluation of the created lexicon. MOTS-CLÉS : fouille d’opinion, modiﬁeurs de valence affective, modiﬁeurs de polarité. KEYWORDS: opinion mining, contextual valence shifters. 1. avec le soutien de Wallonie-Bruxelles International Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 123–136, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 123  
An Open and Generic Framework for the Acquisition of Multiword Expressions In this paper, we present and evaluate an open and ﬂexible methodological framework for the automatic acquisition of multiword expressions (MWEs) from monolingual textual corpora. We start with a pratical motivation followed by a theoretical discussion of the behaviour and of the challenges that MWEs pose for NLP applications. Afterwards, we describe the modules of our framework, the overall pipeline and the design choices of the tool implementing the framework. The evaluation of the framework was performed extrinsically based on an application : computerassisted lexicography. This application can beneﬁt from MWE acquisition because the expressions acquired automatically from corpora can both speed up the creation and improve the quality and the coverage of the lexical resources. The promising results of previous and ongoing experiments encourage further investigation about the optimal way to integrate MWE treatment into NLP applications, and particularly into machine translation systems. MOTS-CLÉS : Expressions polylexicales, extraction lexicale, lexique, mesures d’association, corpus, lexicographie. KEYWORDS: Multiword expression, lexical extraction, lexicon, association measures, corpus, lexicography. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 137–149, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 137  
Adapting a French Named Entity Recognition System to English with Minimal Costs Cross-language portability of Named Entity Recognition systems requires linguistic expertise and needs human effort. Adapting symbolic systems suffers from the cost of developing new lexicons and updating grammar rules. Porting statistical systems on the other hand faces the problem of the high cost of annotation of new training corpus. This paper examines the cost of adapting a rule-based Named Entity Recognition system designed for well-formed text to another language. We present a low-cost method to adapt a French rule-based Named Entity Recognition system to English. We ﬁrst solve the problem of lexicon adaptation to English by simply translating the French lexical resources. We then get to the task of grammar adaptation by slightly modifying the grammar rules. Experimental results are compared to a state-of-the-art English system. MOTS-CLÉS : Reconnaissance d’entités nommées, approche symbolique, portabilité entre les langues. KEYWORDS: Named entity recognition, symbolic approache, cross-language portability. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 151–161, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 151  
State of the Art on the Acquisition of Semantic Relations between Terms : Contextualisation of the Synonymy Relations Accessing to the context of specialised texts is a crucial but difﬁcult task. It requires automatic or semi-automatic methods dedicated to the identiﬁcation of semantic relations between terms appearing in the texts. NLP approaches for acquiring semantic relations between terms can be distinguished according to the type of information : the internal structure of the terms and the term context. In order to improve the quality of the acquired synonymy relations and their reusability in other corpora, we aim at taking into account the context into an approach based on the internal structure of the terms. We present the results of a preliminary experiment taking into account the use of the terms in a English biomedical corpora. This experiment will be helpful to add semantic constraints to the already acquired synonymy relations. MOTS-CLÉS : Acquisition de relations, Synonymie, Relations sémantiques, Terminologie, Do- maine Biomédical, Corpus de spécialité. KEYWORDS: Relation Acquisition, Synonymy, Semantic Relations, Terminology, Biomedical Domain, Specialised corpora. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 163–175, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 163  
State of the Art : Inﬂuence of Domain on Opinion Classiﬁcation The interest in opinion mining has grown concurrently with blogs, forums, and others platforms where the internauts can freely write about their opinion on every topic. As the amounts of available data are increasingly huge, the use of automatic methods for opinion mining becomes imperative. However, sentiment is expressed differently in different domains : words distributions can indeed differ signiﬁcantly. An effective global opinion classiﬁer is therefore hard to develop. Moreover, a classiﬁer trained on a source domain can’t be used without adaptation on a target domain. This article aims to describe the state-of-the-art methods used to solve this difﬁcult task. MOTS-CLÉS : État de l’art, Fouille d’opinion, Multi-domaines, Cross-domaines. KEYWORDS: State of the art, Opinion mining, Multi-domain, Cross-domain. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 177–190, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 177  
Typology of Multiple Answer Questions for a Question-answering System The evaluation campaigns of question-answering systems are generally based on the validity of an individual answer supported by a passage (for a factual question) or a group of answers coming all from a same supporting passage (for a list question). This framework does not allow the possibily to answer with a set of answers, nor with answers gathered from several documents. This cross-checking can be needed for building an answer composed of several elements in order to be as accurate as possible. Besides a large majority of questions with a singular form seems to be answered with a single answer whereas they can be satisﬁed with many. We present here a typology of questions with multiple answers and an overview of problems encountered by a question-answering system with this kind of questions. MOTS-CLÉS : question-réponse, questions à réponses multiples, question liste. KEYWORDS: question-answering, multiple answer questions, list question. ∗Ces travaux ont été partiellement ﬁnancés par OSEO dans le cadre du programme QUAERO. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 191–204, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 191  
PCFG Extraction and Pre-typed Sentences Analysis This article explains the way we extract a PCFG from the Paris VII treebank. Firslty, we need to transform the syntactic trees of the corpus into derivation trees. The transformation is done with a generalized tree transducer, a variation of the usual top-down tree transducers, and gives as result some derivation trees for an AB grammar. Secondely, we have to extract a PCFG from the derivation trees. For this, we assume that the derivation trees are representative of the grammar. The extracted grammar is used, via the CYK algorithm, for sentence analysis. MOTS-CLÉS : Extraction de grammaire, grammaire de Lambek, PCFG, transducteur d’arbre, algorithme CYK. KEYWORDS: Grammar Extraction, Lambek grammar, PCFG, tree transducer, CYK Algorithm. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 205–218, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 205  
Automatic Analysis of Discourse in Sign Language : Signing Space Representation and Processing In sign language, signing space is used to locate and refer to entities whose locations are important for understanding the meaning. In this paper, we propose a computer-based representation of the signing space and their associated functions. It aims to analyze manual and non-manual gestures, that contribute to locating and referencing signs, and to make real their effect. For that, we propose an approach based on the analysis of motion capture data of entities’ assignment and activation events in the signing space. MOTS-CLÉS : Langue des signes, Espace de signation, gestes de pointage, capture de mouvement, suivi du regard. KEYWORDS: Sign language, Signing space, pointing gestures, motion capture, gaze tracker. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 219–232, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 219  
System of Customer Review Summarization using Twitter and SentiWordNet As E-commerce is becoming more and more popular, the number of customer reviews raises rapidly. Opinions on the Web affect our choices and decisions. Thus, it is more efficient to automatically process a mixture of reviews and prepare to the customer the required information in an appropriate form. In this paper, we present ResTS, a new system of feature-based opinion summarization. Our approach aims to turn the customer reviews into scores that measure the customer satisfaction for a given product and its features. These scores are between 0 and 1 and can be used for decision making and then help users in their choices. We investigated opinions extracted from nouns, adjectives, verbs and adverbs contrary to previous research which use only adjectives. Experimental results show that our method performs comparably to classic feature-based summarization methods. MOTS-CLES : Fouille d’opinion, Classification, Intensité de l’Opinion, Résumé de texte d’opinion, Popularité. KEYWORDS: Opinion mining, Sentiment Classification, Opinion Strength, Feature-based Opinion Summarization, Feature Buzz Summary. 
Analysis of Emotion in Health Fora Studies about emotion in fora are numerous in Linguistics and Psychology. This contribution approaches this subject from an Information and Communication Sciences point of view, and studies emotion as a criteron of pertinence for patients in a health forum. This paper introduces the empirical step of automatic language processing in order to answer this question, and uses data processing on the corpus of forum messages, semi-supervised categorisation of messages and use of software NooJ for Natural Language Processing. MOTS-CLÉS : émotion, forum de santé, traitement automatique de la langue, désambiguïsation lexicale. KEYWORDS: emotion, health forum, automatic language processing, lexical disambiguation. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 267–280, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 267  
Population of an Ontology Modeling the Behavior of an Intelligent Environment Guided by Instance Relation Extractions We present an approach for ontology population, which aims at modeling the behavior of software components, for enabling a transition from natural language requirements to formal speciﬁcations. The ontology was designed based on the knowledge of the domotic domain and is initialized from a description of a physical conﬁguration of an intelligent environment. Our method focuses on extracting relation instances which allows the extraction of concept instances linked by these relations. We built extraction rules using elements coming from syntactic analysis of user need descriptions, semantic and terminological resources linked to the knowledge contained in the ontology. Our approach for ontology population, distinguishes itself by its purpose, which is not to extract all instances describing a domain but to extract instances that can participate without any conﬂict to one of the mutiple operation decribed by users. MOTS-CLÉS : extraction de relations, peuplement d’ontologie, représentation des connaissances. KEYWORDS: relation extraction, ontology population, knowledge representation. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 281–294, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 281  
State of the art : Local Semantic Similarity Measures and Global Algorithmes for Knowledge-based Word Sense Disambiguation We present the main methods for unsupervised knowledge-based word sense disambiguation. On the one hand, at the local level, we present semantic similarity measures, which attempt to quantify the semantic proximity between two word senses. On the other hand, at the global level, we present algorithms which use local semantic similarity measures to assign the appropriate senses to words depending on their context, at the scale of a text or of a corpus. MOTS-CLÉS : désambiguïsation lexicale non-supervisée, mesures de similarité sémantique à base de connaissances, algorithmes globaux de propagation de mesures locales. KEYWORDS: unsupervised word sense disambiguation, knowledge-based semantic similarity measures, global algorithms for the propagation of local measures. 
ABSTRACT__________________________________________________________________________________________________________ Of the Use of Natural Dialogue to Hide MCQs in Serious Games A major weakness of serious games at the moment is that they often incorporate multiple choice questionnaires (MCQs). However, no study has demonstrated that MCQs can accurately assess the level of understanding of a learner. On the contrary, some studies have experimentally shown that allowing the learner to input a free-text answer in the program instead of just selecting one answer in an MCQ allows a much finer evaluation of the learner's skills. We therefore propose to design a conversational agent that can understand statements in natural language within a narrow semantic context corresponding to the area of competence on which we assess the learner. This feature is intended to allow a natural dialogue with the learner, especially in the context of serious games. Such interaction in natural language aims to hide the underlying MCQs. This paper presents our approach. MOTS-CLÉS : Agent conversationnel éducatif, intelligence artificielle, jeu sérieux, questionnaire à choix multiple, système d'évaluation de réponses libres. KEYWORDS : Educational conversational agent, artificial intelligence, serious game, multiple-choice questionnaire, automatic assessment of free-text answer. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 323–336, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 323  
Sentence simpliﬁcation for relation extraction Machine learning based relation extraction requires large annotated corpora to take into account the variability in the expression of relations. To deal with this problem, we propose a method for simplifying sentences, i.e. for reducing the syntactic variability of the relations. Simpliﬁcation requires the annotation of a small corpus, which will be automatically augmented. The process starts with the annotation of the simpliﬁcation thanks to a CRF classiﬁer, then the relation extraction, and lastly the automatic completion of the training corpus for the simpliﬁcation through the results of the relation extraction. The ﬁrst results we obtained for the task of relation extraction of the i2b2 2010 challenge are encouraging. MOTS-CLÉS : Extraction de relations, simpliﬁcation de phrases, apprentissage automatique. KEYWORDS: Relation extraction, sentence simpliﬁcation, machine learning. 
Automatic Information Extraction in the Medical Domain by Cross-Lingual Projection This research stems from our willingness to test new methods for automatic annotation or information extraction from one language L1 by exploiting resources and tools available to another language L2. This approach involves the use of a parallel corpus (L1-L2) aligned at the level of sentences and words. To address the lack of annotated medical French corpus, we focus on the French-English language pair to annotate automatically medical French texts. In particular, we focus in this article on medical entity recognition. We evaluate our medical entity recognition method on the English corpus and the projection of the annotations on the French corpus. We also discuss the problem of scalability since we use a parallel corpus extracted from the Web and propose a statistical method to handle heterogeneous corpora. MOTS-CLÉS : Extraction d’information, projection d’annotation, reconnaissance des entités médicales, apprentissage. KEYWORDS: Automatic Information Extraction, Annotation Projection, Medical Entity Recog- nition, Machine Learning. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 15–28, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 15  
A Graph-Based Method for Template Filling in Information Extraction In event-based Information Extraction systems, a major task is the automated ﬁlling from unstructured texts of a template gathering information related to a particular event. Such template ﬁlling may be a hard task when the information is scattered throughout the text and mixed with similar pieces of information relative to a different event. We propose in this paper a two-step approach for template ﬁlling : ﬁrst, an event-based segmentation is performed to select the parts of the text related to the target event ; then, a graph-based method is applied to choose the most relevant entities in these parts for characterizing the event. Using an evaluation of this model based on an annotated corpus for earthquake events, we achieve a 72% F-measure for the template-ﬁlling task. MOTS-CLÉS : Extraction d’information, segmentation de texte, remplissage de formulaires. KEYWORDS: Information Extraction, Text Segmentation, Template Filling. 
Processing of a Pyrenees travel novels corpus : a syntactical, semantical and temporal analysis. In this article, we present a type theoretical framework which we apply to the syntactic analysis and the computation of DRS semantics. Our tool, Grail, is used for the automatic treatment of French text and we use a Pyrenées travel novels corpus, Itipy, as a test case. We explain our use of categorial grammars and speciﬁcally the Lambek calculus and its connection to the simply typed λ-calculus in connection with DRT. Flexible typing has to be allowed in some cases and forbidden in others. Some linguistic phenomena presenting some kind of meaning shifts inducing typing conﬂicts will be introduced. We then present our motivations in the pragmatic ﬁeld to use a system with sorts and variable types in lexical semantics and then we present how we process events temporality, in the light of Verkuyl’s Binary Tense(Verkuyl, 2008) MOTS-CLÉS : compositionalité, interface syntaxe-sémantique, interface sémantique-pragmatique, grammaire catégorielle, théorie des types, récit de voyage. KEYWORDS: compositionality, syntax-semantics interface, semantics-pragmatics interface, cate- gorial grammar, type theory, travel novel. Ce travail de recherche a reçu un soutien ﬁnancier d’ INRIA et du Conseil Régional d’Aquitaine dans le cadre du projet Itipy Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 43–56, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 43  
Recognition of compound words tested against parsing and vice-versa : evaluation of two discriminative approaches We propose two discriminative strategies to integrate compound word recognition in a real parsing context : (i) state-of-the-art compound pregrouping with Conditional Random Fields before parsing, (ii) reranking parses with features dedicated to compounds after parsing. We show that these two approaches help reduce up to 18% of the gap between a baseline parser and parser with golden segmentation and up to 25% for compound recognition. MOTS-CLÉS : Mots composés, analyse syntaxique, champs markoviens aléatoires, réordon- nanceur. KEYWORDS: Multiword expressions, parsing, Conditional random Fields, reranker. 
On Computing Subcategorization Frames of French Deverbal Nouns (Case of Genitive) Fine dependency analysis needs exact information on the subcategoriziation frames of lexical units. These frames being well studied for the verbs, we are interested in this paper by the case of the noun deverbals. Our main goal is to calculate the subcategoriziation frame of deverbals in French from that of the source verb. However, this task needs a representative list of French deverbal nouns. To obtain such a list, we use a simpliﬁed algorithm detecting deverbal nouns in texts. The obtained list attested by linguists is compared with the existing list Verbaction of deverbals expressing the action/activity of French verbs. For these deverbal nouns, we analyse the origin of their subordinate prepositional phrases in different contexts relative to their source verbs. This analysis is carried out over the corpus Paris 7 and is limited to the most frequent cases of the genitive, i.e. to the prepositional phrases headed by the prepositions de, des, etc. MOTS-CLÉS : nom déverbal, cadre de sous-catégorisation, groupe prépositionnel, analyse en dépendances. KEYWORDS: Deverbal Noun, Subcategorization Frame, Prepositional Phrase, Dependency Tree. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 71–84, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 71  
Vectorization, Okapi and computing similarity for NLP : say goodbye to TF-IDF In this position paper, we review a problem very common for many NLP tasks: computing similarity (or distances) between texts. We aim at showing that what is often considered as a small component in a broader complex system is very often overlooked, leading to the use of sub-optimal solutions. Indeed, computing similarity with TF-IDF weighting and cosine is often presented as “state-of-theart”, while more effective alternatives are in the Information Retrieval (IR) community. Through some experiments on several tasks, we show how this simple calculation of similarity can inﬂuence system performance. We consider two particular alternatives. The ﬁrst is the weighting scheme Okapi-BM25, well known in IR and directly interchangeable with TF-IDF. The other, called vectorization, is a technique for calculating text similarities that we have developed which offers some interesting properties. MOTS-CLÉS : Calcul de similarité, modèle vectoriel, TF-IDF, Okapi BM-25, vectorisation. KEYWORDS: Calculating similarities, vector space model, TF-IDF, Okapi BM-25, vectoriza- tion. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 85–98, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 85  
TCOF-POS : A Freely Available POS-Tagged Corpus of Spoken French This article details the creation of TCOF-POS, the ﬁrst freely available corpus of spontaneous spoken French. We present here the methodology that was followed in order to obtain the best possible quality in the ﬁnal resource. This corpus already is freely available and can be used as a training/validation corpus for NLP tools, as well as a study corpus for linguistic research. We also present the results obtained by two POS-taggers trained on the corpus. MOTS-CLÉS : Etiquetage morpho-syntaxique, français parlé, ressources langagières. KEYWORDS: POS tagging, French, speech, language resources. 
Hierarchical sub-sentential alignment with Anymalign We present a sub-sentential alignment algorithm that aligns sentence pairs from an existing alignment matrix in a very easy way. This algorithm is inspired by previous work on alignment by recursive binary segmentation and on document clustering. We evaluate the alignments produced on machine translation tasks and show that we can obtain state-of-the-art results, with gains up to more than 4 BLEU points compared to our previous work, with a method that is very simple, independent of the size of the corpus to be aligned, and can directly produce symmetric alignments. When using this method as an extension of the translation extraction tool Anymalign, our experiments allow us to determine some of its limitations and to deﬁne possible leads for further improvements. MOTS-CLÉS : corpus parallèle ; alignement sous-phrastique ; traduction automatique statistique. KEYWORDS: parallel corpus ; sub-sentential alignment ; statistical machine translation. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 113–126, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 113  
Compositionality and Context for Bilingual Lexicon Extraction from Comparable Corpora In this article, we study the possibilities of improving the alignment of equivalent terms monolingually acquired from bilingual comparable corpora. Our overall objective is to identify and to translate highly specialised terminology. We applied a compositional approach enhanced with pre-processed context information. Our evaluation demonstrates that our alignment method outperforms the compositional approach for translationally equivalent term discovery from comparable corpora. MOTS-CLÉS : Corpus comparable, compositionnalité, information contextuelle, lexique bi- lingue. KEYWORDS: Comparable Corpora, compositionality, context information, bilingual lexicon. 
Resource Reﬁning : « Les Verbes Français » This paper introduce the impovements we made to the resource « Les Verbes Français » in order to make it more usable in the ﬁeld of natural language processing. Syntactic and semantic information is corrected, restructured, uniﬁed and then integrated to the XML version of this resource, in order to be used by a semantic role labelling system. MOTS-CLÉS : ressource, lexique, verbes, rafﬁnement, étiquetage de rôles sémantiques. KEYWORDS: resource, lexicon, verbs, reﬁnement, semantic roles labeling. 
Study of meronymy in a distribution-based lexical resource In this paper, we study the way meronymy behaves in a distribution-based lexical resource. We address the question of the evaluation of such resources through a semantic-based approach. Our method consists in collecting meronyms from a resource which we cross with a distributionbased lexical resource made from an encyclopedic corpus. Meronyms are then sub-categorized manually : ﬁrstly following the sub-relation they bear (STUFF/OBJECT, MEMBER/COLLECTION, etc.), then following the semantic class of their members. Results show that distributional analysis identiﬁes meronymic relations in different proportions according to the semantic classes of the words involved in the meronymic pairs. MOTS-CLÉS : analyse distributionnelle, sémantique lexicale, méronymie, évaluation. KEYWORDS: distributional analysis, lexical semantics, meronymy, evaluation. 
Topical Cohesion using Graph Random Walks In this article, we propose a novel metric to weight specialized lexicons terms according to their relevance to the underlying thematic. Our method is inspired by Web as corpus approaches and accumulates exogenous knowledge about a specialized lexicon from the web. Terms cooccurrences are modelled as a graph and a random walk algorithm is applied to compute terms relevance. Finally, we study the performance and stability of the metric and evaluate it in a bilingual lexicon creation context. MOTS-CLÉS : Cohésion thématique, graphe de cooccurrences, marche aléatoire. KEYWORDS: Thematic relevance, cooccurrence graph, random walk. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 183–195, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 183  
Assisted rephrasing for Wikipedia contributors through Web-based validation This works describes initial experiments on the validation of paraphrases in context. Wikipedia’s revisions are used : we assume that a set of possible rewritings are available for a given phrase that has been rewritten in the encyclopedia’s revision history, and we attempt to ﬁnd the subset of those rewritings that can be considered as valid paraphrases. We tackle this problem as a classication task which we provide with features obtained from Web data. Our experiments show that our system improves performance over a set of simple baselines. MOTS-CLÉS : paraphrase, Wikipédia, aide à la rédaction. KEYWORDS: paraphrasing, Wikipedia, authoring aids.  
Syntactic Simpliﬁcation for French Sentences This paper presents a method for the syntactic simpliﬁcation of French texts. Syntactic simpliﬁcation aims at making texts easier to understand by simplifying the elements that hinder reading. It is based on a corpus study that aimed at investigating the linguistic phenomena involved in the manual simpliﬁcation of French texts. We have ﬁrst gathered a parallel corpus of articles from Wikipedia and Vikidia, that we used to establish a typology of simpliﬁcations. In a second step, we implemented a system that carries out syntactic simpliﬁcations based on these corpus observations. We described simpliﬁcation rules in order to generate simpliﬁed sentences. A module subsequently selects the best subset of sentences. The evaluation of our system shows that about 80% of the sentences produced by our system are accurate. MOTS-CLÉS : simpliﬁcation automatique, lisibilité, analyse syntaxique. KEYWORDS: automatic simpliﬁcation, readability, syntactic analysis. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 211–224, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 211  
Semantic analysis of keywords and stable lexical markers in a technical corpus This article presents the results of a quantitative semantic analysis of typical lexical units in a specialised technical corpus of metalworking machinery in French. The study aims to find out whether and to what extent the keywords of the technical corpus are monosemous. A simple regression analysis, used to examine the correlation between typicality rank and monosemy rank of the keywords, points out some statistical and methodological problems, notably a frequency bias. In order to overcome these problems, we adopt an alternative approach for the identification of typical lexical units, called Stable Lexical Marker Analysis (SLMA). We discuss the quantitative and statistical results of this approach with respect to the correlation between typicality rank and monosemy rank. MOTS-CLES : unités lexicales spécifiques, analyse des mots-clés, analyse des marqueurs lexicaux stables, sémantique quantitative, analyse de régression. KEYWORDS : typical lexical units, Keyword Analysis, Stable Lexical Marker Analysis (SLMA), quantitative semantics, regression analysis. 
Graph Mining Under Linguistic Constraints to Explore Large Texts In this paper, we propose an approach to explore large texts by highlighting coherent sub-parts. The exploration method relies on a graph representation of the text according to the Hoey linguistic model which allows the selection and the binding of sentences in the graph. Our contribution relates to using graph mining techniques under constraints to extract relevant subparts of the text (i.e., collections of homogeneous sentence sub-networks). We have conducted some experiments on two large English texts to show the interest of the proposed approach. MOTS-CLÉS : Fouille de graphes, réseaux phrastiques, analyse textuelle, navigation textuelle. KEYWORDS: Graph Mining, sentence networks, textual analysis, textual navigation. 
A study of paraphrase along 3 dimensions : corpus types, languages and techniques In this paper, we report a detailed study of the impact of corpus type on the task of sub-sentential paraphrase acquisition. Our experiments are for 2 languages and 4 corpus types, and involve an efﬁcient machine learning-based combination of 4 paraphrase acquisition systems. We obtain relative improvements of more than 27% in F-measure over the best individual system on English and French, and obtain a relative improvement over the combination system of 22% for English and 5% for French when using all other corpus types as additional training data for our most readily available corpus type. MOTS-CLÉS : acquisition de paraphrases, constitution de corpus. KEYWORDS: paraphrase acquisition, corpus collection.  
Detecting and correcting POS annotation in the French TreeBank The quality of the Part-Of-Speech (POS) annotation in a corpus has a large impact on training and evaluating POS taggers. In this paper, we present a series of experiments that we have conducted on automatically detecting and correcting annotation errors in the French TreeBank. Two methods are used. The ﬁrst simply relies on identifying tokens with missing tags and correct them by assigning the tag the same token observed in the corpus. The second method uses n-gram variations to detect and correct conﬂicting annotations. The evaluation of the automatic correction is performed extrinsically by comparing the performance of different POS taggers in relation to the level of correction. Results show a statistically signiﬁcant improvement in precision and indicate that the POS annotation quality can be noticeably enhanced by using automatic correction methods. MOTS-CLÉS : Étiquetage morpho-syntaxique, correction automatique, qualité d’annotation. KEYWORDS: Part-Of-Speech tagging, automatic correction, annotation quality. 
Enriching the French Treebank with Properties We present in this paper the hybridation of the French Treebank with Property Grammars annotations. This process consists in acquiring a PG grammar from the source treebank and generating the new syntactic encoding on top of the original one. The result is a new resource for French, opening the way to new tools and descriptions. MOTS-CLÉS : Treebank hybride, French Treebank, Grammaires de Propriétés. KEYWORDS: Hybrid treebank, French Treebank, Property Grammars. 
The Sequoia corpus : syntactic annotation and use for a parser lexical domain adaptation method We present the building methodology and the properties of the Sequoia treebank, a freely available French corpus annotated following the French Treebank guidelines (Abeillé et Barrier, 2004). The Sequoia treebank comprises 3204 sentences (69246 tokens), from the French Europarl, the regional newspaper L’Est Républicain, the French Wikipedia and documents from the European Medicines Agency. We then provide a method for parser domain adaptation, that makes use of unsupervised word clusters. The method improves parsing performance on target domains (the domains of the Sequoia corpus), without degrading performance on source domain (the French treenbank test set), contrary to other domain adaptation techniques such as self-training. MOTS-CLÉS : Corpus arboré, analyse syntaxique statistique, adaptation de domaine. KEYWORDS: Treebank, statistical parsing, parser domain adaptation. 
This paper presents an open-source platform for collaborative editing dependency corpora. ACOLAD platform (Annotation of corpus linguistics for the analysis of dependencies) offers manual annotation services such as segmentation and multi-level annotation (segmentation into words and phrases minimum (chunks), morphosyntactic annotation of words, syntactic annotation chunks and annotating syntactic dependencies between words or chunks). In this paper, we present ACOLAD platform, then we detail the representation used to manage concurrent annotations, then we describe the mechanism for importing external linguistic resources. MOTS-CLES : annotation collaborative de corpus, annotations concurrentes, dépendances KEYWORDS : corpus collaborative annotation, concurrent annotations, dependencies 
Towards Preference Extraction From Negotiation Dialogues This paper presents an NLP based approach for preference expression extraction from negotiation dialogues. We propose a new annotation schema for preferences and dependencies among them and illustrate on two different corpus genres. We then suggest a learning approach that efﬁciently extracts preference expressions using a combination of local and discursive features and assess the reliability of our approach on each corpus genre. MOTS-CLÉS : Préférence, dialogue, apprentissage automatique. KEYWORDS: Preference, dialogue, machine learning. 
Conﬂicts detection in online epistemic communities Conﬂicts in online epistemic communities can be a blocking factor when producing knowledge. We present a way to automatically detect conﬂict in Wikipedia discussions, based on subjectivity and connotation marks. Two rules are evaluated : a local rule that uses the structure of the discussion threads, connotation and subjectivity marks and a global rule that takes the whole thread into account and only subjectivity. We show that the two rules produce similar results but that the simplicity of the global rule makes it a preferred approach to detect conﬂicts. MOTS-CLÉS : wikipedia, conﬂit, syntaxe, sémantique, interaction. KEYWORDS: wikipedia, conﬂict, syntax, semantics, interaction. 
What is the contribution of named entities detection for information extraction in restricted domain ? In the framework of general domain dialog corpora a particular focus is dedicated to Named Entities deﬁnition and recognition, which are mostly very generic (personal names, locations, etc.). Moreover, call-centre data mining is strategic for a company like EDF, the public opinion analysis playing a signiﬁcant role in EDF services quality evaluation and for marketing applications. In this purpose a domain dependant deﬁnition of entities of interest is essential. In this primary work we compare two types of entities models (generic and speciﬁc to the domain) in order to observe their respective coverage. We annotated manually a sub-corpus extracted from a large corpus of oral dialogs recorded in an EDF call-centre. The respective proportion of generic vs domain-speciﬁc Named Entities is then estimated. Impact for future work on building EDF domain-speciﬁc entities models is discussed. MOTS-CLÉS : entités nommées, concepts métier, extraction d’information, données conversa- tionnelles, annotation. KEYWORDS: named entities, business concept, information extraction, conversational data, annotation. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 359–366, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 359  
About the application of textometric methods for developing classi!cation criteria in Sentiment analysis Over the last ten years, NLP has contributed to applied research on subjectivity, especially in applications such as Opinion mining and Sentiment analysis. However, corpus linguistics and textometry have often addressed the issue of subjectivity in text. Our purpose is to show, !rst, what textometric analysis could bring to sentiment analysis, and second, the bene!ts of pooling linguistic/textometric analysis and automatic classi!cation methods based on supervised learning. By processing a corpus of posts from fora, we will show that the building of criteria from a textometric analysis could improve classi!cation results, compared to a purely lexical approach. MOTS-CLÉS : linguistique de corpus, textométrie, analyse de sentiments, classi!cation automatique supervisée. KEYWORDS : corpus linguistics, textometry, sentiment analysis, supervised learning. 
Methodology for corpus exploration and grammatical rule building in Sign Language This paper presents a methodology for Sign Language video observation to extract and then formalise observed linguistic structure. This methodology is relevant to all linguistic layers from sub-lexical to discourse as a whole. Relying on two examples, we apply this methodology and describe the AZee model, which integrates the required flexibility for synchronising articulators, hence enables a specification of any new systematic rule observed. MOTS-CLÉS : Langue des signes, analyse de corpus, modèle grammatical, synchronisation. KEYWORDS : .Sign Language, corpus analysis, grammatical models, synchronisation. 
Manual Annotation of Football Matches : Inter-annotator Agreement ! Gooooal ! We present here an annotation campaign of commentaries of football matches in French. The annotation was done from a very heterogeneous text corpus of both match minutes and video commentary transcripts. We show how the intra- and inter-annotator agreement can be used efﬁciently during the whole campaign by proposing a deﬁnition of the markables suited to our type of task, as well as emphasizing the importance of using it appropriately. We also show how some clues, collected through statistical analyses, could be used to help correcting the annotations. These statistical analyses are then used to assess the impact of the source modality (written or spoken) on the cost and quality of the annotation process. MOTS-CLÉS : annotation manuelle, accords inter-annotateurs. KEYWORDS: manual annotation, inter-annotator agreement. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 383–390, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 383  
Study of various strategies for adapting an opinion classiﬁer to a new domain The work presented in this article takes place in the ﬁeld of opinion mining and aims more particularly at ﬁnding the polarity of a text by relying on machine learning methods. In this context, it focuses on studying various strategies for adapting a statistical classiﬁer to a new domain when training data only exist for one or several other domains. This study shows more precisely that a self-training procedure consisting in enlarging the initial training corpus with texts from the target domain that were reliably classiﬁed by the classiﬁer is the most successful and stable strategy for the tested domains. Moreover, this strategy gets better results in most cases than (Blitzer et al., 2007)’s method on the same evaluation corpus while it is more simple. MOTS-CLÉS : fouille d’opinion, adaptation à un nouveau domaine, auto-apprentissage. KEYWORDS: opinion mining, domain adaptation, self-training. 
Analysis of contexts and candidates in term-translation spotting in comparable corpora The standard approach for identifying terminological equivalents from comparable corpora is based on the comparison of source and target language context words using a bilingual lexicon. We cary a manual analysis of the linguistic properties (parts of speech, speciﬁcity and semantic relations) of the context words and the inacurrate equivalents given by the standard approach applied to medical terminology, in order to suggest improvements based on the selection of context words. MOTS-CLÉS : équivalents terminologiques, vecteurs contextuels, corpus comparables, termino- logie médicale, étude qualitative. KEYWORDS: terminological equivalents, contextual vectors, comparable corpora, medical termi- nology, qualitative study. 
Automatic events extraction by combining multiple approaches In this paper, we present an automatic system for extracting events based on the combination of two existing information extraction approaches : the ﬁrst one is made of hand-crafted linguistic rules and the second one is based on an automatic learning of linguistic patterns. We have shown that this mixed approach leads to a signiﬁcant improvement of extraction performances. MOTS-CLÉS : Extraction d’information, événements, approche symbolique, apprentissage de patrons linguistiques. KEYWORDS: Text mining, events, symbolic extraction, linguistic pattern learning. 
Machine Learning of a chunker for French We describe in this paper how to automatically learn a chunker for French, from the French Tree Bank and CRFs (Conditional Random Fields). We did several experiments, either to recognize every possible kind of chunks, or to focus on simple nominal phrases only. We evaluate the obtained chunker on internal data (i.e. also extracted from the French Tree Bank) as well as on external (i.e from a distinct corpus) ones, to measure its robustness. MOTS-CLÉS : chunking, apprentissage automatique, French Tree Bank, CRF. KEYWORDS: chunking, Machine Learning, French Tree Bank, CRF. 
Deletion of dimensions of textual similarity for the exploration of collections of accident reports in aviation In this paper we study the relationship between external classiﬁcation and textual similarity in collections of incident reports. Our goal is to complement the existing classiﬁcation-based analysis strategies by automatically establishing similarity links between documents in such a way that they do not reﬂect the dominant organisation of the classiﬁcation schemas. In order to discover such transversal dimensions of similarity, we compute association scores between terms and classes and exlude the most correlated terms from the similarity calculation. We demonstrate on a 500 document corpus that by using this method, we can isolate topics that would otherwise have been masked by the dominant dimensions of similarity in the collection. MOTS-CLÉS : similarité textuelle, classiﬁcation de documents, corpus spécialisé. KEYWORDS: textual simliarity, document classiﬁcation, specialised corpora. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 439–446, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 439  
Automatic Translation from Comparable corpora : extracting parallel sentences from multimodal comparable corpora Statistical Machine Translation (SMT) systems depend on the availability of bilingual parallel text, also called bitext. However parallel corpora are a limited resource and are often not available for some domains or language pairs. We present an alternative method for extracting parallel sentences from multimodal comparable corpora. This work extends the use of comparable corpora, in using audio instead of text on the source side. The audio is transcribed by an automatic speech recognition system and translated with a base-line SMT system. We then use information retrieval in a large text corpus of the target language to extract parallel sentences. We have performed a series of experiments on data of the IWSLT’11 speech translation task (TED) that shows the feasibility of our approach. MOTS-CLÉS : Reconnaissance de la parole, traduction automatique statistique, corpus compa- rables multimodaux, extraction de phrases parallèles. KEYWORDS: Automatic speech recognition, statistical machine translation, multimodal compa- rable corpora, extraction of parallel sentences. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 447–454, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 447  
ABSTRACT _________________________________________________________________________________________________________ Automatic recognition of demonstrative pronouns function in Arabic Anaphora resolution is one of the most difficult tasks in NLP. Classifying pronouns before attempting a task of anaphora resolution is important because to handle the cataphoric pronoun, the system should determine the antecedent into the segment following the pronoun. Although, for the anaphoric pronoun, the system should look for the antecedent into the segment before the pronoun. In addition, the number of demonstrative pronouns is very important in Arabic. In this paper, we describe a machine learning method for classifying demonstrative pronouns in Arabic. We have evaluated our approach on a corpus of 365585 words which contain 14318 demonstrative pronouns and we have obtained encouraging results: 99.3% as F-Measure. MOTS-CLES : Pronoms démonstratifs, résolution des anaphores, traitement de la langue arabe. KEYWORDS: Demonstrative pronouns, anaphora resolution, ANLP. 
An Automatic Temporal Expression Annotator and its Evaluation on the French TimeBank In this article, we present a tool that extracts and normalises a subset of temporal expressions in French. This tool is being developed and used in the ANR (French National Research Agency) project Chronolines, applied to a corpus of provided by the Agence France Presse. The aim of the project is to semi-automatically construct event chronologies from this corpus. To do this, a detailed analysis of the temporal information conveyed by texts, is required. The system we present here is the ﬁrst version of a temporal annotator that we have developed for French. We describe it in this article and present the results of an evaluation. MOTS-CLÉS : Analyse temporelle, évaluation. KEYWORDS: Temporal processing, evaluation. 
Towards the FDTB : French Discourse Tree Bank We present the ﬁrst steps towards creating an annotated corpus for discourse in French : the French Discourse Treebank enriching the FTB. Our methodology is based on the Penn Discourse Treebank (PDTB), but it differs in at least two points of a theoretical nature. First, our goal is to provide full coverage of a text, while the PDTB provides only partial coverage, which can not be described as discourse analysis such as the one made in RST or SDRT, two major theories on discourse. Second, we were led to deﬁne a new hierarchy of discourse relations which is based on RST, SDRT and PDTB. MOTS-CLÉS : Discours, corpus annoté manuellement, analyse discursive, PDTB, RST, SDRT. KEYWORDS: Discourse, manually annotated corpus, discourse analysis, PDTB, RST, SDRT. 
Query Contextualization and Reformulation by Combining External Corpora Improving document retrieval using external sources of information has been extensively studied throughout the past. Improvements with either structured or large corpora have been reported. However, in these studies resources are often used separately and rarely combined together. We present an evaluation of the combination of four different scalable corpora over a web search task. An informative divergence measure is used to extract contextual features from the corpora and improve query representation. We use the ClueWeb09 collection along with TREC’s Web Track topics for the purpose of our evaluation. Best results are achieved when combining all four corpora, and are signiﬁcantly better than the results of other approaches. MOTS-CLÉS : Combinaison de ressources, RI contextuelle, recherche web. KEYWORDS: Resources combination, contextual IR, web search. 
Named Entity Recognition for Arabic : Unsupervised adaptation and Systems combination The recognition of Arabic Named Entities (NE) is a potentially useful preprocessing step for many Natural Language Processing Applications, such as Machine Translation. This task is however made very complex by some peculiarities of the Arabic language. In this paper, we present a summary of our recent efforts aimed at developing a statistical NE recognition system, with a speciﬁc focus on feature engineering aspects. We also report several approaches for adapting this system in an entirely unsupervised manner to a new domain. MOTS-CLÉS : Adaptation non supervisée, Repérage des entités nommées. KEYWORDS: Unsupervised domain adaptation, named entity recognition. 
Manual thematic annotation of a journalistic corpus : ﬁrst observations and evaluation. The work presented in this paper focuses on the creation of a corpus of journalistic texts annotated at dicourse level, more precisely on a topic level. The annotation model is a classic segmentation one, to which we add transition zones between topical units. We assume that in a well-structured text, the author provides information helping the reader to move from one topic to another, where an identiﬁcation of these clues is likely to improve automatic segmentation. The produced annotations have been subject of several quantitative analyses showing a set of linguistic properties of topical transitions. MOTS-CLÉS : Structure du discours, segments thématiques, transitions thématiques, annotation. KEYWORDS: Discourse structure, topical segments, topical transitions, annotation. 
ABSTRACT _________________________________________________________________________________________________________ Multi-extraction as a strategy of optimized extraction of terminological and lexical resources Based on the evaluation of terminological extractors, initially to find the best tool for building a controlled language lexicon, we propose a strategy of optimized extraction of terminological resources. Our work highlights that the cooperation of several extraction tools gives better results than the use of a single one. It both reduces silence and automatically filters noise thanks to a variable related to termhood. MOTS-CLÉS : terminologie, extraction, langue contrôlée, potentiel terminologique, filtrage de termes. KEYWORDS : terminology, extraction, controlled language, termhood, term filtering. 
Structured Information Retrieval Approach based on Indexing Time Error Correction In this paper, we focused on errors in the textual content of XML documents. We propose an approach to reduce the impact of these errors on Information Retrieval (IR) systems. Indeed, these systems rely on indexes associating each document to corresponding terms. Indexes quality is negatively affected by those misspellings. These errors makes it difﬁcult to later retrieve documents (or parts of them) in an effective way during the querying phase. In order to deal with this problem we propose to include an error correction mechanism during the indexing phase of documents. We achieved an implementation of this spelling aware information retrieval system which is currently evaluated over INEX evaluation campaign documents collection. MOTS-CLÉS : Recherche d’information, dysorthographie, correction d’erreurs, xml. KEYWORDS: Information retrieval, misspellings, error correction, xml. 
Statistical Post-Editing of Machine Translation for Domain Adaptation This paper presents a statistical approach to adapt generic machine translation systems to the medical domain through an unsupervised post-edition step. A statistical post-edition model is built on statistical machine translation outputs aligned with their translation references. Evaluations carried out to translate medical texts from French to English show that a generic machine translation system can be adapted a posteriori to a speciﬁc domain. Two systems are studied : a state-of-the-art phrase-based implementation and an online publicly available software. Our experiments also indicate that selecting sentences for post-edition leads to signiﬁcant improvements of translation quality and that more gains are still possible with respect to an oracle measure. MOTS-CLÉS : Traduction automatique statistique, post-édition, adaptation aux domaines de spécialité. KEYWORDS: Statistical Machine Translation, Post-editing, Domain Adaptation. 
Referential named entity annotation of the Paris 7 French TreeBank The French TreeBank developed at the University Paris 7 is the main source of morphosyntactic and syntactic annotations for French. However, it does not include explicit information related to named entities, which are among the most useful information for several natural language processing tasks and applications. Moreover, no large-scale French corpus with named entity annotations contain referential information, which complement the type and the span of each mention with an indication of the entity it refers to. We have manually annotated the French TreeBank with such information, after an automatic pre-annotation step. We sketch the underlying annotation guidelines and we provide a few ﬁgures about the resulting annotations. MOTS-CLÉS : Résolution d’entités nommées, Corpus annoté, Corpus arboré de Paris 7. KEYWORDS: Named entity resolution, Annotated corpus, French TreeBank. 
Feature calculation for Statistical Machine Translation by using belief functions In this paper, we consider the translation of texts within the framework of the 7th Workshop of Machine Translation evaluation task and the COSMAT corpus using a statistical machine translation approach. This work is focused on the translation features calculation of the phrase contained in a phrase table. The classical way to estimate these features are based on the direct computation counts or frequencies. In our approach, we propose to use the concept of belief masses to estimate the phrase probabilities. The Belief Function theory has proven to be suitable and adapted for the management of uncertainties in many domains. The experiments based on our approach are focused on the language pair English-French. MOTS-CLÉS : Traduction automatique statistique, fonctions de croyance, apprentissage automa- tique, estimation de paramètres. KEYWORDS: Statistical machine Translation, belief function, machine learning, feature estima- tion. 
ABSTRACT _________________________________________________________________________________________________________ Methodological, linguistic and computational challenges for processing written French of deaf people With the setup of a national emergency call-center for deaf people in France (CNRAU), some questions arise in linguistics and natural language processing about the written expression of deaf people. It is practiced by an heterogeneous population and shows morpho-syntactic, lexical and syntactic specificities which increase the difficulty, over the emergency situation, to successfully communicate between the deaf callers and the call-center operators. A first corpus (FAX-ESSU) of written French of deaf people was built with emergency conditions in order to provide linguistic and NLP solutions to the call center operators. On this corpus, we present a first study realized with the help of a natural language processing toolbox, in order to validation linguistic phenomenons described in the scientific literature and to enrich the knowledge of written French of deaf people. MOTS-CLES : Français écrit des sourds, TAL, Français Langue Etrangère, linguistique de corpus, lexique, syntaxe, méthodologie. KEYWORDS : Written French of deaf people, NLP, French as a foreign language, corpus linguistics, lexicon, syntax, methodology. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 559–566, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 559  
Context and nature of phonetic realizations in conversational speech Since a decade, research in phonetics has turned with interest to the description of large corpora of casual speech. This new field of research opens up many opportunities but asks also new questions for phoneticians. Firstly, this paper evaluates the lexical and phonological context in which phonetic realizations are produced. This context is noticeably different from lexical context in constructed corpora. Next, we address the question of phonetic annotation which is critical for phonetic analyses. Finally, we discuss some specific cases of phonetic reduction which offer new perspectives for our interpretations of speech production. MOTS-CLES : parole spontanée, grands corpus, données lexicales, annotation phonétique, alignement automatique, réduction phonétique. KEYWORDS : spontaneous speech, large corpora, lexical data, phonetic annotation, phonetic reduction. 
Prosodic Structuring and the Syntax-Prosody Relationship in Political Speech Studies on the prosodic structure of French recognize the existence of different types of prosodic constituents. These constituents differ from each other in the way in which they are constructed and realized. Generally speaking, prosodic structure is sensitive to syntactic structure and influenced by the syntax – despite the difference between prosodic and syntactic constituency. In listening to political speeches, one cannot help but notice the division of the stream of speech produced by its speakers; it differs markedly from what is expected, especially with respect to both the matching conditions with syntactic structure and the shape of the contours. The present paper is a systematic study of chunking produced in a political speech by Jacques Chirac. The goal is twofold: First, to uncover the factors intervening in the construction of the prosodic constituents and second, to determine the ways in which they resemble or differ from established assumptions pertaining to standard French. MOTS-CLES : structure prosodique, interface prosodie/syntaxe, variation et phonostyle. KEYWORDS : prosodic structure, prosody-syntax interface, variation and phono-style 
The prosodic structure of French involves a tight connection between the location of prominent syllables and locations where the flow of speech is divided into prosodic units. Researchers agree that the final (full) syllable of the smallest prosodic unit is prominent. The possibility remains that other syllables or words may be emphasized that do not precede a prosodic boundary. This paper examines cases from two radio broadcasts where emphasis is found without a boundary, and illustrates some prosodic modifications that speakers use to create emphasis with or without boundaries. The data demonstrate that emphasis is not tied to a specific phrasal position, even though words preceding a boundary are more prominent than other words. MOTS-CLÉS : syntagmes prosodiques, proéminence, perception de la prosodie, français. KEYWORDS : phrasing, prominence, perception of prosody, French. 
Do you hear my attitudes? Perception of Mandarin Chinese social affects’ prosody Social affects are, contrary to emotions, speech acts voluntarily controlled and socioculturally built. This work examines the perception of Chinese social affects by natives, in the aim of attitudinal prosody teaching. A speech corpus was designed, with variation of length, tone location and syntactic structures of utterances, and produced with 19 social affects. The perception test reveals that social affects were globally recognized, the expressions of “declaration” and “disappointment” received the best scores, and “confidence” and “irony” the lowest. The social affects were organized into seven conceptually coherent clusters. MOTS-CLES : perception de la prosodie, attitudes, affects sociaux, chinois mandarin KEYWORDS: perception of prosody, attitudes, social affects, Mandarin Chinese 
Recognition of desynchronized consonantics sounds with and without ﬁne spectral structure This paper reports two experiments in which the identiﬁcation of desynchronized VCV sequences was investigated with either both ﬁne spectral structure (TFS) and envelope information or envelope information alone. These experiments compare the decrease in intelligibility scores with respect to the degree of desynchronization applied between spectral channels. The data are ﬁrst analysed in terms of global performance intelligibility, then intelligibility of individual consonant sounds are investigated. Conﬁrming previous data obtained in sentence identiﬁcation tasks, it is shown that consonant identiﬁcation in a forced choice categorisation task occurs with relatively high levels of performance even for strong levels of desynchronization, but that performance is highly variable depending on individual consonants. Various explanations for differences between our results and preceding work are discussed. MOTS-CLÉS : Enveloppe d’amplitude, désynchronisation temporelle, structure spectrale ﬁne, parole vocodée. KEYWORDS: Temporal envelope, temporal desynchronization, ﬁne spectral structure, vocoded speech. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 33–40, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 33  
ABSTRACT _________________________________________________________________________________________________________ Reading and prosody in dyslexic children, pause patterns Dyslexia is widely associated with a deficit in phonological awareness. Only few works in suprasegmental phonology showed that prosody is involved in the processes of decoding and reading comprehension. We developed a corpus (DySpoLec) to examine various prosodic patterns in reading and spontaneous speech in dyslexic children. In this study, we propose an analysis of silent pauses (number, distribution and duration) in two conditions: reading aloud and spontaneous speech in dyslexic and control children. Results show differences in durations according to the group, the type of pause and the condition of production. Dyslexic children show longer duration which could mean subtle language deficit in planification of syntaxic and semantic units. MOTS-CLES : dyslexie, lecture, parole spontanée, prosodie, pauses KEYWORDS : dyslexia, reading, spontaneous speech, prosody, pauses Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 41–48, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 41  
Lexical-phonetic automata for spoken utterance indexing and retrieval This paper presents a method for indexing spoken utterances which combines lexical and phonetic hypotheses in a hybrid index built from automata. The retrieval is realised by a lexical-phonetic and semi-imperfect matching whose aim is to improve the recall. A feature vector, containing edit distance scores and a conﬁdence measure, weights each transition to help the ﬁltering of the candidate utterance list for a more precise search. Experiment results show that the lexical and phonetic representations are complementary and we compare the hybrid search with the state-of-the-art cascaded search to retrieve named entity queries. MOTS-CLÉS : recherche d’information, indexation de parole, représentations lexico-phonétiques, automates et transducteurs, mesures de conﬁance, distances d’édition, apprentissage supervisé. KEYWORDS: information retrieval, speech indexing, lexical-phonetic representations, automata and transducers, conﬁdence measures, edit distances, supervised learning. 
Acoustic characterization of phonologically voiced obstruents in Shanghai dialect In Shanghainese, phonologically voiced obstruents in word-initial, accented position are phonetically voiceless and are distinguished from the others (i.e., voiceless and/or aspirated) mainly by a low tone register. Slightly breathy voice is also reported in the relevant literature as an additional characteristic of these obstruents in accented position. In this study, we used both monosyllabic and disyllabic words to revisit the issue of breathy phonation as characterizing phonologically voiced Shanghainese obstruents not only in accented position but also in non-accented position, where sandhi may affect tone register. We found breathy phonation most clearly when tone register is not affected. We also found that systematic duration patterns robustly characterize phonologically voiced obstruents, whether or not in accented position. MOTS-CLES : dialectes Wu, shanghaïen, voix soufflée, voix slack, durée consonantique KEYWORDS : Wu dialects, Shanghainese, breathy/slack voice, consonant duration 
In this paper, we present a statistical method based on GMM modeling to map the acoustic speech spectral features to visual features of Cued Speech in the sense of least square error in a low signal level which is innovative and different with the classic text-tovisual approach. In comparison with the GMM based mapping modeling we first present the results with the use of a multi-linear model also at the low signal level and study the limitation of the approach. The experimental results demonstrate that the GMM based mapping method can significant improve the mapping performance compared with the multi-linear based mapping model especial in the sense of the weak linear correlation between the target and the predictor such as the hand positions of Cued Speech and the acoustic speech spectral features. MOTS-CLES : LPC, LSP, MFCC, PARAMETRES LABIAUX, CONVERSION, MODELE LINEAIRE, GMMS. KEYWORDS : Cued Speech, LSP, MFCC, Lips, Linear modeling, GMMs. 
Development and implementation of fiduciary markers for vocal tract MRI imaging and speech articulatory modelling MRI allows to characterize the shape and position of speech articulators, but not to track the evolution of flesh points, since there are no markers reliably associated with the highly deformable tissues of these articulators. This information is however interesting for the knowledge of the biomechanical properties of these organs as well as for modelling the relations between measurement modalities such as MRI or the electromagnetic articulography. We have therefore attached to a speaker’s articulators fiduciary markers made of non toxic polymers visible by MRI, and recorded a corpus of MRI midsagittal images. The articulators’ contours and the markers’ coordinates manually determined have been analysed. We have observed a departure from the hypothesis of uniformly distributed elasticity ranging from 0.6 to 1.5 cm. Besides, we have shown that the markers can predict the articulators’ contours with a variance explanation around 85 %, and an RMS error from 0.08 to 0.15 cm, compared to 74 à 95 %, and 0.07 à 0.14 cm for the original articulatory models. MOTS-CLES : Modèle articulatoire, point de chair, IRM, marqueur fiduciaire. KEYWORDS : Articulatory model, flesh point, MRI, fiduciary marker. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 81–88, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 81  
ABSTRACT _________________________________________________________________________________________________________ Rhoticity and derhoticisation in Ayrshire Scottish English Scottish English is typically described as a rhotic variety, whose rhotic variants are taps [ɾ] and approximants [ɹ] (Wells, 1982 : 411). Non-prevocalically, recent findings indicate not only that /r/ is extremely variable, but also that a process of derhoticisation might be ongoing in this accent, leading to r-loss or vocalisation in coda position (Romaine, 1978 ; Stuart-Smith, 2007 ; Stuart-Smith et al., 2007 ; Lawson et al., 2008 ; Llamas, 2010 ; Pukli & Jauriberry, 2011). The acoustic analysis of eight native speakers of Ayrshire reveals first, great variability in the realisation of /r/, in relation to internal and external factors, and second, that, according to the apparent time principle, two sound changes might be ongoing and led by young women: the lenition of taps [ɾ] towards approximants [ɹ], and the vocalisation or even loss of non-prevocalic /r/. MOTS-CLES : Rhotiques, Rhoticité, Anglais écossais, Variation, Changement KEYWORDS : Rhotics, Rhoticity, Scottish English, Variation, Change 
New approach for speaker clustering of broadcast news In this paper, we propose a new clustering model for speaker diarization. A major problem with using greedy agglomerative hierarchical clustering for speaker diarization is that they do not guarantee an optimal solution. We propose a new clustering model, by redeﬁning clustering as a problem of Integer Linear Programming (ILP). Thus an ILP solver can be used which searches the solution of speaker clustering over the whole problem. The experiments were conducted on the corpus of French broadcast news ESTER-2. With this new clustering, the DER decreases by 2.43 points. MOTS-CLÉS : segmentation et regroupement de locuteur, programmation linéaire en nombres entiers, i-vecteur. KEYWORDS: speaker diarization, integer linear programming, i-vector. 
Inﬂuence of the cheeks expansion during bilabial plosive production This experimental study highlights the inﬂuence of expansion of the cheeks during the production of bilabial plosives. The closure of the lips, before the plosive, causes an increase in intra-oral pressure. The assumption suggested in this paper is that the increase of oral cavity volume, as a result of the intra-oral pressure, has a non-negligible impact on the pressure inside the oral cavity. Fisrtly, this phenomenon is shown by in-vivo measurements for an /apa/ utterance, then reproduced in laboratory on a replica of human vocal apparatus. The upper part of vocal tract is represented by a ﬂexible tube, whose volume increases under the effect of the pressure. MOTS-CLÉS : Production de la parole, plosives bilabiales, aérodynamique, mesures in-vivo/in- vitro, pression intra-orale. KEYWORDS: Speech production, biliabial plosives, aerodynamic, in-vivo/in-vitro measurement, intra-oral pressure. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 113–120, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 113  
Acoustic analysis of atypical contrasts in Northern-Irish English This paper participates in the discussion about the problematic status of the so-called “derived contrasts”. Our study consists in an acoustic analysis of two vowel contrasts in Northern-Irish English. The first one arises from the Ulster variant of the Aitken law – according to which a vowel is appreciably longer when followed by the past morpheme /d/. A second part of this article deals with a process of vowel breaking which applies everywhere except at morpheme boundaries. We will thus describe the phonetic nature of the vowels under scrutiny and examine to what extent the pronunciation of pairs such as daze/days can be considered as phonemic contrasts. MOTS-CLES : phonétique, phonologie, anglais, contrastes dérivés, Irlande du Nord KEYWORDS : phonetics, phonology, English, derived contrasts, Ulster 
ABSTRACT _________________________________________________________________________________________________________ VisArtico : visualizing articulatory data acquired by an articulograph In this paper, we present VisArtico, visualization software of articulatory data acquired by an articulograph, AG500. The software allows displaying the positions of the sensors that are simultaneously played with the speech signal. It is possible to display the tongue contour and the lips contour. The software helps to find the midsagittal plane of the speaker and find the palate contour. In addition, VisArtico allows labeling phonetically the articulatory data. Our main goal is to provide an efficient tool to visualize articulatory data for researchers working in speech production field. MOTS-CLES : Articulographe, visualisation, production de la parole, conduit vocal, EMA. KEYWORDS : Articulograph, visualization, speech production, vocal tract, EMA. 
Emotions detection in the voice of patients interacting with an animated conversational agent The French ARMEN ANR-funded project aims at building an assistive robot for elderly and disabled people We focus in this paper on the emotion detection module for this robot. The interaction is almost entirely conducted in a natural, spoken fashion with a virtual agent. 77 patients have participated to the data collection. The specific difficulty in this project lies in the large variety of user voices (elderly, damaged) and affective behaviors of the patient. Our first results show 46% of good emotion detection on four classes (Anger, Joy, Neutral and Sadness). We first try to analyze the differences due to age and voice quality. MOTS-CLÉS : robot assistant, détection d'émotions spontanées, qualité vocale KEYWORDS : assistive robot, spontaneous emotions detection, vocal quality 1. Introduction 
Prosodic variations in unit-based speech synthesis: the example of interrogative sentences This paper proposes an automatic method to increase the number of possible prosodic variations in non-uniform unit-based speech synthesis. More speciﬁcally, we are interested in the production of interrogative sentences through the eLite text-to-speech synthesis system, which relies on the selection of non-uniform units, but does not have interrogative units in its speech database. The purpose of this work was to make the system able to synthesize interrogative sentences without having to record a new, interrogative database. After a study of the syntactic and prosodic phenomena involved in the production of interrogative sentences, we present our two-step method: an adapted pre-processing of the unit selection itself, and a post-processing of the whole speech signal built by the system. A perceptual evaluation of sentences synthesized by our approach is then described, which points out both pros and cons of the method and highlights some issues in the very principles of the eLite system. MOTS-CLÉS : synthèse NUU, phrases interrogatives, variations prosodiques. KEYWORDS: NUU text-to-speech synthesis, interrogative sentences, prosodic variations. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 161–168, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 161  
Toward an acoustic to articulatory inversion of a foreign speaker We present an extension of our acoustic-to-articulatory inversion method, based on unsupervised Hidden Markov Models. The articulatory vectors’ generation is based on the “GMM” approach. Considering the application of our method to the teaching of foreing languages, we study the performances of this approach in the case of missing data. MOTS-CLÉS : Inversion acoustico-articulatoire, HMM non supervisé, données manquantes. KEYWORDS: Acoustic-to-articulatory inversion, unsupervised HMM, missing data. 
Multimodal Prosody. The auction chant in the United States This paper proposes a multimodal prosodic analysis of the auction chant that is practiced in the US. Drawing upon a corpus that involves 6 auctioneers and relying upon the prosodic analysis of the chant by Kuiper, K. & Tillis F. (1985), we investigate how gestures, which are necessary in this type of interaction, align with speech despite a fast speech rate and the fact that verbal content is strongly constrained by the rhythmic structure of the chant. MOTS-CLES : Communication multimodale, prosodie, vente aux enchères chantée. KEYWORDS : Multimodal communication, prosody, auction chant. 
An experimental framework for speech sciences This article is a position paper in favor of the establishment of a theoretical and practical framework to bring out an empirical science of speech, based on the contribution of all the sciences whose object of study is the speech. Central to this re-convergence is the idea that automatic systems can be used as instruments to explore large amounts of data at our disposal and to derive new linguistic knowledge which, in turn, will allow to improve the models used in the automatic systems. Some crucial points are discussed, such as the deﬁnition of the observable, the study of the residual as a diagnostic of the gap between modeling and reality, and the development of instrumental centers for the sharing of development and maintenance of these complex instruments which are automatic speech processing systems. MOTS-CLÉS : analyse d’erreurs ; structuration de la recherche en parole. KEYWORDS: Epistemologic study ; error analysis ; structuration of speech sciences. 
Impact of the level of supervision on Web-based language model domain adaptation Domain adaptation of a language model aims at re-estimating word sequence probabilities in order to better match the peculiarities of a given broad topic of interest. To achieve this task, a common strategy consists in retrieving adaptation texts from the Internet based on a given domain-representative seed text. In this paper, we study the inﬂuence of the choice of this seed text on the adaptation process and on the performances of adapted language models in automatic speech recognition. More precisely, the goal of this original study is to analyze the differences between supervised adaptation, in which the seed text is manually generated, and unsupervised adaptation, where the seed text is an automatic transcript. Experiments carried out on videos from a real-world use case mainly show that differences vary according to adaptation scenarios and that the unsupervised approach is globally convincing, especially according to its low cost. MOTS-CLÉS : Modèle de langage, adaptation à un domaine, supervision, données du Web. KEYWORDS: Language model, domain adaptation, supervision, Web data. 
Speech clarity and coarticulatory effects in standard and dialectal Arabic This study deals with the co-variation of speech clarity and coarticulatory patterns. Two experiments were conducted to investigate the influence of two parameters, the speech style (formal vs. non formal) and the prosodic position (stressed vs. unstressed syllable). The speech material was composed of three word lists varying CV syllable contexts with pharyngealized /tܱ dܱ sܱ ᾩܱ/ vs. non- pharyngealized consonants /t d s ᾩ/ in Modern standard Arabic and dialectal Arabic. Acoustic materials revealed evident relationship between speech clarity and coarticulation: more coarticulation in formal speech and in strong prosodic position. Mots-clés : Arabe, effets coarticulatoires, clarté de la parole, équation de locus, pharyngalisation. Keywords: Arabic, coarticulatory effects, speech clarity, locus equation, pharyngealization. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 209–216, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 209 
Distortions of vocalic space: which measurements? An application to dysarthria. This paper presents several metrics derived from F1/F2 measurements for the description and quantification of the possible variations to be observed in a vocalic space. 8 metrics from the literature or adapted to our data are evaluated on productions of the vowels /a, e, i, u, o/, extracted from a text read by 78 dysarthric patients suffering from Parkinson disease, cerebellar syndrome or ASL, and by 26 healthy control speakers. The ability of the metrics to describe vocalic space alterations associated with the different dysarthria as compared with the control group is studied. The relations between the metrics and with perceived intelligibility of the patients are also discussed. Results outline the necessity to consider several metrics to account for the multidimensionality of the alteration possible in a vocalic space. MOTS-CLÉS : dysarthrie, espace vocalique acoustique, réduction, centralisation, Parkinson, Syndrome cérébelleux, Sclérose Latérale Amyotrophique KEYWORDS : dysarthria, acoustic vowel space, reduction, centralization, Parkinson, Cerebelar Syndrom, Lateral Amyotrophic Sclerosis. 
Spatio-temporal coordinations in Moroccan Arabic ab(bi) sequences In this study using 3-dimensional EMA (AG500 Carstens Medizinelektronik) we tried to characterize the temporal relations between consonant and vowels in [ab(b)i] contexts. We found that, [bb] has a consonantal gesture (LowerLip_y) whose total duration and plateau phase are longer and the vertical target higher compared to [b]. We also found an anticipation of this consonantal gesture in [abbi] correlated with the shortening of the acoustic duration of [a] in this context. The time interval between the vowels [a] and [i] is longer in [abbi] compared to [abi]. This result, combined with other observations, seems to support the models (ex. Articulatory Phonology) suggesting a temporal coordination between the oral gesture of a consonant with that of the adjacent vowel. MOTS-CLES : Gémination, coordinations temporelles, EMA, coarticulation, Arabe. KEYWORDS : Geminate, EMA, temporal coordination, coarticulation, Arabic. 
Optimization of a tutoring system from a ﬁxed set of data In this paper, we present a general method for optimizing a tutoring system with a target application in the domain of second language acquisition. More speciﬁcally, the optimisation process aims at learning the best sequencing strategy for switching between teaching and evaluation sessions so as to maximise the increase of knowledge of the learner in an adapted manner. The most important feature of the proposed method is that it is able to learn an optimal strategy from a ﬁxed set of data, collected with a hand-crafted strategy. This way, no model (neither cognitive nor probabilistic) of learners is required but only observations of their behavior when interacting with a simple (non-optimal) system. To do so, a particular batch-mode approximate dynamic programming algorithm is used, namely the Least Square Policy Iteration algorithm. Experiments on simulated data provide promising results. MOTS-CLÉS : Tuteurs intelligents, apprentissage par renforcement. KEYWORDS: Tutoring systems, reinforcement learning. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 241–248, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 241  
ABSTRACT__________________________________________________________________________________________________________ The prosody of questions in French as L2 This paper focuses on the acquisition of the tonal and prosodic structure of questions in French as a L2. Our study consists in a cross-comparison of utterances recorded in French and Spanish in various settings, and produced by 15 Mexican Spanish learners of French (L2), 10 French speakers and 10 Mexican speakers. In the yes-no questions as produced by the learners, some characteristics of their L1 are observed, which can be seen as a consequence of a transfer: (i) the nuclear contour consists in an extra-high F0 rise, and (ii) the internal prosodic structure at the level of the AP is not clearly marked. However, the tonal patterns observed in partial questions, where learners do use rising contours, cannot be attributed to a transfer. These findings prove that the acquisition of prosody in a L2 cannot be analyzed as a mere transfer from the learner’s first language. MOTS-CLÉS : Acquisition d’une L2, intonation, phrasé prosodique. KEYWORDS: L2 acquisition, intonation, prosodic phrasing. 
LDA-based tagging of Web videos This article presents a method for the automatic tagging of youtube videos. The proposed method combines an automatic speech recognition system, that extracts the spoken contents, and a keyword extraction system that aims at ﬁnding a small set of tags representing the video. In order to improve the robustness of the tagging system to the recognition errors, a video transcription is represented in a semantic space obtained by Latent Dirichlet Allocation (LDA), in which each dimension is automatically characterized by a list of weighted terms and chuncks. Our experiments demonstrate the interest of such a model to improve the robustness of the tagging system, especially when speech recognition (ASR) system produce highly errorneous transcript of spoken contents. MOTS-CLÉS : Reconnaissance de la parole, analyse des contenus, catégorisation audio, multi- média. KEYWORDS: Speech recognition, content analysis, audio categorization, multimedia. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 273–280, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 273  
Impact of the Social Behaviour of a Robot on the User’s Emotions: a Perceptive Experiment This study is carried out in the context of the French robotic project ROMEO. This project aims at designing a humanoid social robot which will assist disabled persons at home. So as to interact as naturally as possible with the user, the robotic system processes the paralinguistic cues (no semantics) extracted from speech. These cues allow building a dynamic interactional and emotional profile of the user, which is used to select the behaviour of the robot. Which behaviours are the most accepted by the user? We present the IDV-HR data collection featuring elderly people suffering from a loss of autonomy, and the coding of the robotic behaviours. In addition to the self-report questionnaire, we analyse the emotional state of the speaker in the course of the scenarios, according to the behaviour of the robot. MOTS-CLES : Traitement des signaux sociaux – Interaction humain-robot – Emotions KEYWORDS: Social signal processing – Human-robot interaction – Emotions 
Recent studies provide evidence for action goal coding of manual actions in premotor and posterior parietal cortices. To further extend these results, we used a repetition suppression paradigm while measuring neural activity with functional magnetic resonance imaging during repeated orofocial movements (lip protrusion, jaw lowering and tongue retraction movements). In the motor domain, this adaptation paradigm refers to decreased activity in specific neural populations due to repeated motor acts and has been proposed to reflect sensorimotor learning and reduced prediction errors by means of forward motor-to-sensory predictive processes. In the present study, orofacial movements activated a set of largely overlapping, common brain areas forming a core neural network classically involved in orofacial motor control. Crucially, suppressed neural responses during repeated orofacial actions were specifically observed in the left hemisphere, within the intraparietal sulcus and adjacent inferior parietal lobule, the superior parietal lobule and the ventral premotor cortex. These results provide evidence for action goal coding and forward motor-tosomatosensory predictive control of intransitive and silent orofacial actions in this frontoparietal circuit. MOTS-CLES : contrôle moteur orofacial, codage du but de l’action, modèle prédictif forward, IRMf, répétition suppression. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 289–296, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 289  KEYWORDS : orofacial motor control, action goal coding, forward predictive model, fMRI, repetition suppression. 
Principal Component Analysis for i-vector extraction in speaker veriﬁcation. In this work, we propose alternative algorithmic combinations for speaker veriﬁcation based on the Total Variability paradigm. Experiments presented in this paper show that replacing Factor Analysis (FA) by a Principal Component Analysis (PCA) for super-vector dimensionality reduction can lead to state-of-the-art performance. Extracting the i-vectors according to the Maximum Likelihood criteria when using an Eigen Vector matrix resulting from a PCA outperforms a state-of-the-art system based on Factor Analysis and Probabilistic Linear Discriminant Analysis in 3 conditions of the NIST-SRE08 evaluation over 8. Computation of i-vectors by an orthognonal projection on the PCA matrix is also shown to outperform the state-of-the-art conﬁguration in 2 of the 8 conditions. The results presented in this paper illustrate the potential of a Deterministic approach for speaker veriﬁcation. MOTS-CLÉS : Vériﬁcation du locuteur, I-vecteurs, Réduction de dimension. KEYWORDS: Speaker veriﬁcation, I-vectors, Dimension reduction.  Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 297–304, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 297  
Automatic measurement of prosodic accommodation in conversational interaction It has been observed in many studies that speakers, over the course of a conversation, adapt their verbal and non-verbal behaviour (lexicon, syntax, prosody, postures, gesture) to their interlocutor. This accommodation facilitates, on the one hand, the exchange of information, mutual understanding between interactants and the reaching of common ground. Moreover, it increases the social success of the interaction in terms of rapport (i.e. harmonious relation and mutual attention) and afﬁliation. While accommodation is a ubiquitous component of social interaction, few automatic systems and metrics have been developed to quantify it. In this paper, we present a model which provides metrics for the automatic measurement of prosodic accommodation and its dynamic manifestation in conversation. Based on this model, we discuss the different forms and the dynamics of prosodic accommodation, measured from conversations recorded over a period of several months. MOTS-CLÉS : Adaptation prosodique, dynamiques de la parole, interaction sociale. KEYWORDS: Prosodic adaptation, speech dynamics, social interaction. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 321–328, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 321  
F0-declination : a comparison between French and German journalistic speech The aim of the present study is to investigate F0-declination over the course of utterances in French and German journalistic speech by using large transcribed and automatically segmented corpora (a total of about 80,000 utterances of more than 1,000 speakers). Two different methods were applied : (i) regression-analysis in order to calculate the overall downtrend of F0 and (ii) convex-hull to detect local peaks and valleys in order to calculate the top- and bottom lines. The results show similar characteristics for both languages of the slope : there is an overall declining tendency for the F0 of about 2.5 st per second as well as the same predictors for the amplitude of the slope like utterance duration and the F0-value of the resetting, the intercept and the highest peak. Nevertheless we found language- speciﬁc parts of the slope in the mouvements of the topand bottom lines. MOTS-CLÉS : intonation, ligne de déclinaison, F0, régression, modelisation, inter-langue, resetting. KEYWORDS: intonation, declination line, F0, regression, modelling, crosslinguistic, resetting. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 329–336, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 329  
Pitch height in French: Gradual or categorical variations? This paper reports the results of an experiment on the question of whether the realizations of f0 variations at the end of the final IP are categorical or gradient. We conducted an imitation task with resynthesized stimuli where the final pitch height was varied in steps of one semi-tone. Results are ambivalent, since both strategies are possible. However, we argue that there is enough evidence for establishing at least three pitch categories. MOTS-CLES : intonation, tâche d’imitation, français, variations catégorielles ou continues, hauteurs mélodiques. KEYWORDS: intonation, imitation task, French, continuous, categorical, pitch level, range. 
Voicing contrast in whispered speech This paper presents analyses on the phonological voicing contrast in whispered speech, which is characterized by a semi-open configuration of the vocal folds preventing them from vibrating. In modal speech, in addition to vocal fold vibration, the contrast between voiced and unvoiced consonants is realized by other phonetic correlates: e.g. consonant and pre-consonantal vowel durations, intraoral pressure differences. Acoustic and aerodynamic analyzes show that these voicing correlates are preserved in whispered speech. These findings seem consistent with those showing that voiced contrast is maintained in perception despite the absence of vocal fold vibration. MOTS-CLÉS : phonétique, voisement, voix chuchotée, aérodynamique, durée segmentale KEYWORDS: phonetics, voicing, whisper, aerodynamics, segmental duration 
Abnormal Zone Detection in Dysarthric Speech Utterances according to Frequency Bands This paper proposes to join a speech processing-based system devoted to the automatic detection of abnormal zones in impaired speech utterances with an analysis in frequency subbands. This work aims to demonstrate that abnormal zones could be detected differently according to the frequency bands. The complementarity of the frequency subbands could be used afterwards to improve the robustness of the automatic detection. Experimental results, reported for a set of gender-balanced patients suffering from dysarthria, highlight a very interesting behavior of medium and high frequency subbands, different from male and female patients and supported by a comparison between vowel and consonant classes. The related observations open a large set of investigation perspectives, regarding the analysis of gains brought by individual subbands compared with the full frequency band, but also regarding potential subband combination strategy. MOTS-CLÉS : Troubles de la parole, dysarthrie, détection automatique, zones de déviance. KEYWORDS: Speech disorders, dysarthria, automatic detection, deviant speech zones. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 377–384, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 377  
ABSTRACT _________________________________________________________________________________________________________ The /y-u/ vowels in the IPFC project: a perceptual assessment of native, Spanish and Japanese learners’ productions. We present a perceptual study of the French vowels /y/ and /u/ produced by Spanish and Japanese learners, in the framework of the InterPhonology of Contemporary French (IPFC) project. To evaluate the quality of realization of their productions, we carried out two experiments in which 58 native listeners had to identify the vowel (/y-u/) of monosyllables produced by one group of native speakers, two categories of Spanish learners, and two categories of Japanese learners in a repetition and a reading task. The results globally show a population effect – a better identification for the natives – a task effect – a better identification in the repetition task – and, for the Spanish learners, a vowel effect – a higher identification rate for /u/ than for /y/. They also reveal differences between Spanish and Japanese learners, and between the two categories of learners within each group, which differ in terms of degree of L2 exposure. MOTS-CLES : français langue étrangère, apprenants espagnols, apprenants japonophones, évaluation perceptive, voyelles arrondies, phonologie L2. KEYWORDS : L2 French, Spanish learners, Japanese learners, perceptual assessment, rounded vowels, L2 phonology. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 385–392, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 385  
An original methodology is proposed for the study of the emergence of speech motor control in 4-years-old children. We combine the results of an analysis of an articulatory and acoustic speech corpus containing isolated vowels and vowel-consonant-vowel sequences with simulations obtained using a two-dimensional biomechanical model of the vocal tract. The analysis of the variability of vowels across repetitions in a single context provides information on the accuracy of the control. The analysis of the contextual variability gives insights into the planning process used in the production of anticipatory coarticulation. Finally, the linking of data and results obtained with the biomechanical model enables to infer patterns of motor commands for the main sounds and sequences of sounds. Preliminary results are presented. MOTS-CLES : Production de parole, développement, contrôle moteur, échographie linguale, modèle biomécanique de langue. KEYWORDS: Speech production, development, speech motor control, ultrasound tongue imaging, biomechanical tongue model. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 393–400, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 393  
Towards assessing phonic compliance The paper is focused on “phonic compliance”, the ability of the subject to accurately reproduce speech sounds presented to him/her as production models. Three mathematical techniques are proposed to perform objective assessment. A pilot study is carried out on previously collected and original data. Each speaker reproduces 6 times 94 synthetic vowels regularly spaced in a mel-scale F1*F2*F3 space. The aim is to check the feasibility of the assessment tools under development. Results suggest that each mathematical tool is sensitive to compliance; they emphasize the interest of further research on the specific properties of the tools and call for the refinement of the concept. MOTS-CLES : compliance phonique, talent, aptitude, capacité, adaptabilité KEYWORDS : phonic compliance, talent, aptitude, ability, adaptability 
Detection of incorrect transcriptions of non-native speech in the context of foreign language learning This article analyses the detection of incorrect transcriptions of non-native speech in the context of foreign language learning. The purpose is to detect and reject incorrect transcriptions (i.e. those for which the text does not correspond to the associated speech signal) while being tolerant to the pronunciation defects of non-native speech. The proposed approach exploits the comparison between two alignements : one constrained by the transcript which is being checked, with an other one unconstrained, corresponding to a phonetic decoding. Several criteria are described and combined via a logistic regression function. The article analyzes the inﬂuence of different settings, such as the impact of non-native pronunciation variants, the use of decision functions dependent on the length of the transcriptions, and the impact of learning decision functions on native or non-native speech. The performance evaluations are conducted both on native speech and non-native speech. MOTS-CLÉS : Apprentissage d’une langue étrangère, entrées incorrectes, parole non-native, variantes de prononciation, alignements contraint et non-contraint. KEYWORDS: Foreign language learning, incorrect transcriptions, non-native speech, pronuncia- tion variants, constrained and unconstrained alignements. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 409–416, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 409  
Forensic speaker identification: 20 years of scientific testimonies in courts of Justice. The case of LIPSADON “forensics independent laboratory” The Association Francophone de la Communication Parlée (AFCP) and the Société Française d’Acoustique (SFA) consider that “because of ethical concerns, it is incumbent upon any specialist to demonstrate his or her competence in speaker identification before assuming the authority of or operating as an expert.” For 20 years, the groups’ representatives have reiterated this principled position during legal proceedings in which an “expert” has identified a suspect using telephone recordings. Since its creation in 2008, LIPSADON, « laboratoire indépendant de police scientifique » [an “independent forensics laboratory”], has produced numerous reports of expert opinion. The signing director of these reports has never furnished proof of his scientific competence: the conclusions rendered in his reports are thus open to serious doubt. MOTS-CLES : identification juridique du locuteur, LIPSADON KEYWORDS : forensic speaker identification, LIPSADON laboratory. 
Speaker veriﬁcation : results variation Speaker veriﬁcation systems have shown signiﬁcant progress and have reached a level of performance that make their use in practical applications possible. Nevertheless, large differences in terms of performance are observed, depending on the speaker or the speech sample used. This context emphasizes the importance of a deeper analysis of the system’s performance over average error rate. In this paper, the effect of the training excerpt on performance is investigated. The results show that the performance are highly dependent on the voice samples used to train the speaker model for two state-of-art systems. A methods to observe the variation explained by the system him-self is investigated too. MOTS-CLÉS : Veriﬁcation du locuteur, Variation de la performance. KEYWORDS: Speaker Veriﬁcation, performance. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 425–432, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 425  
Cross-show speaker diarization We propose to study speaker diarization from a collection of audio documents. The goal is to detect speakers appearing in several shows. In our approach, shows are processed independently of each other before being processed collectively, to group speakers involved in several shows. Two clustering methods are studied for the overall treatment of the collection: one uses the NCLR metric and the other is inspired by techniques based on i-vectors, used in the speaker veriﬁcation ﬁeld, and is expressed as an ILP problem. Both methods were evaluated on two sets of 15 shows from ESTER 2. The method based on i-vectors achieves performance slightly lower than those obtained by the NCLR method, however, the computation time is on average 17 times faster. Therefore, this method is suitable for processing large volumes of data. MOTS-CLÉS : SRL, traitement de collection, i-vecteurs, regroupement PLNE. KEYWORDS: speaker diarization, cross-show diarization, i-vectors, ILP clustering. 
what is the impact of the transcription on the phonetization This paper aims at quantifying the impact of the transcription enrichments on the automatic phonetization of speech. Experiments were carried out on a 7 minutes French corpus including conversational speech, readed speech and a political discourse. Results showed the better the transcription the better the phonetization and that independently on the corpus. MOTS-CLÉS : transcription, oral, enrichissement, phonétisation. KEYWORDS: transription, phonetization, enrichment, speech.  
Study for improving the coded speech by tight framelet packet transform In this paper we propose to study the performance of a new time-frequency representation called the tight framelet packets transform in speech coding. For this, we performed a comparative study with the wavelet packets transform. Performance evaluation was done using various objective criteria : the coding gain, the normalized root mean square error, the peak signal to noise ratio, the segmental signal to noise ratio, the frequency weighted segmental signal to noise ratio and PESQ. The obtained results show that the speech coding by framelet packets transform provides a higher quality than that using wavelet packet transform. MOTS-CLÉS : Codage de la parole, frame d’ondelette, transformation en paquets de framelette, transformation en paquets d’ondelette. KEYWORDS: Speech coding, wavelet frame, framelet packets transform , wavelet packets transform. 
REPERE : preliminary results of a multimodal person recognition challenge The REPERE Challenge aims at supporting researches on people recognition in multimodal conditions. To estimate the technology progress, annual evaluation campaigns on multimodal recognition of people in videos will be organized from 2012 to 2014. In this context, the REPERE corpus, a French videos corpus with multimodal annotation has been developed. The systems which participated to the dry run have to answer the following questions : Who is speaking ? Who is present in the video ? What names are cited ? What names are displayed ? This paper describes the corpus used during the january 2012 dry run and presents the ﬁrst results. MOTS-CLÉS : Corpus, Parole multimodale, Reconnaissance du locuteur, Campagne d’évaluation. KEYWORDS: Corpora, Mutimodal conditions, Speaker recognition, Evaluation. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 497–504, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 497  
This paper proposes an efficient codebook design for tree-structured vector quantization (TSVQ) which is embedded in nature. We modify the speech coding standard G.729 by replacing its original quantizer for line spectral frequencies (LSF’s) with TSVQ-based quantizer. The modified coder is fine-granular bit-rate scalable with gradual change in quality for the synthetic speech. MOTS-CLES : Codage de la parole, quantification vectorielle arborescente, échelonnabilité. KEYWORDS : Speech coding, tree-structured vector quantization, scalability. 
ABSTRACT ________________________________________________________________________________________________________________________ Comparative study of the measurement accuracy of the 3D electromagnetic articulographs WAVE and AG500 We present a comparative study of the accuracy of the two most used 3D electromagnetic articulographs: NDI’s WAVE system and the Carstens AG500 system. To accurately judge their precision, we designed an experimental paradigm using the mkal setup, to allow the coils to rotate on a housing inside the magnetic field. We then evaluated the pairwise variation of the distances between the 6 coils in two ways: relative to the distance of the pairs from the magnetic center of the system and relative to rotational velocity. Results are similar for the 2 systems with somewhat better accuracy for the WAVE system, regardless of distance (0.0289 vs. 0.0347 cm) and regardless of coil velocity (0.0289 vs. 0.0401 cm). It also seems that the accuracy is relatively independent of the coil velocity, but decreases with the distance from magnetic center of the system. MOTS-CLES : Articulographe électromagnétique 3D, précision, production de la parole. KEYWORDS: 3D electromagnetic articulograph, accuracy, speech production. 
Dialectal Effect on Articulation Rate in French This paper compares the articulation rate of 3 distinct varieties of French: Parisian French (hereafter PA); Swiss French spoken in Neuchâtel (hereafter NE) and French spoken by Swiss German speakers (hereafter CH) who have been living in a French speaking environment (in Neuchâtel) for 20 years at least. The objective is twofold: to assess the existence of differences in articulation rate between native French speakers of a standard variety (PA) and native French speakers of a regional variety (NE); and to address whether the non-native speakers (CH) exhibit a different behaviour regarding articulation rate compared with the native speakers of the correspondent variety (NE). Besides the “dialectal” factor, this study takes into account further factors that may have an influence on articulation rate: age, gender, speech style (reading or conversation) and number of syllables within the Accentual Phrase. MOTS-CLES : français dialectal, français L2, vitesse d’articulation, syntagme accentuel. KEYWORDS: dialectal French, L2 French, articulation rate, accentual phrase. 
ABSTRACT ________________________________________________________________________________________________ Articulatory speaker normalisation based on MRI-data using three-way linear decomposition methods The aim of this study was to characterise, to model and to compare the different lingual articulatory strategies of a group of speakers. Individual principal component analysis (PCA) models and multi-linear decomposition methods have been applied to the tongue contours extracted from a magnetic resonance imaging (MRI) corpus of seven speakers articulating 63 French vowels and consonants. On the average over the seven speakers, using 4 components, the Root Mean Square prediction Error (RMSE) was 0.13 cm for the individual PCA models while the RMSE for the parallel factor model (PARAFAC) was 0.29 cm, accounting for a percentage of variance explanation of 91% and 62%, respectively. A multi-linear regression (MRL) model could predict, with 10 components, the tongue contour of a target subject from a given source subject, with about 65% of the variance explained and an RMSE of 0.38 cm. All the models have been assessed by a leave-one-out cross-validation procedure. MOTS-CLES: Modélisation articulatoire, normalisation du locuteur, analyse factorielle, IRM. KEYWORDS: Articulatory modelling, speaker normalisation, factor analysis, MRI. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 529–536, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 529  1. Introduction The Speech & Cognition Department at GIPSA-lab has developed acoustic-to-articulatory inversion methods to provide speakers with a visual articulatory feedback (Ben Youssef et al., 2011), based on a fairly complete orofacial clone. This clone is made of a set of models of articulators (jaw, tongue, velum, lips, etc.) based on articulatory data acquired on a single speaker (Badin & Serrurier, 2006). Therefore, the clone represents faithfully the characteristics of a specific speaker, but not necessarily those of other speakers that may have different morphologies and different articulatory control strategies. Thus, one important issue is the normalisation problem: how can the speaker-specific models of the orofacial clone be adapted to other speakers? This problem is particularly challenging as it implies discovering how different speakers with different morphologies can produce articulated sounds that are considered equivalent for speech communication purposes. Several studies based on measurements using Electromagnetic Articulography (EMA) and Magnetic Resonance Imaging (MRI) have been led in this field. Harshman et al. (1977) made a Parallel Factor analysis (PARAFAC) study on X-ray data of five American English speakers. The tongue postures were decomposed in two factors which explained 92.7% of the variance. In another study, Hoole (1998) provided a two factor PARAFAC solution for the German vowel system in three different consonant contexts /p t k/. Two-factor independent models were successfully extracted by Principal Component Analysis (PCA) for each consonant context. The explained variance amounted to about 92.3% and the Root Mean Square reconstruction Error (RMSE) to 1.24 mm for each model. On the other hand, the extracted two-factor PARAFAC solution for the complete dataset presented an increase of RMSE compared to the individual models, the explained variance now amounting to 80% and the RMSE to 1.9 mm. In another study, Hoole (1999) showed how the PARAFAC model error could be further analysed to extract an additional component. His approach consisted in examining the error of the two-factor PARAFAC model by subtracting the articulatory data predicted from the original data. Then, a PCA was employed to extract an extra-component. The final model explained over 90% of the variance. PARAFAC was performed by Hoole et al. (2000) on a set of MRI data of nine German speakers uttering seven German vowels in five different contexts. Two factors accounted for about 87 % of the variance with a RMSE of about 2.2 mm. Geng & Mooshammer (2000) provided a two factor PARAFAC solution. The speech material consisted of six German speakers uttering fifteen German vowels in /t/-context recorded by EMA. Two factors led to a variance explanation of about 96% and an RMSE of about 2 mm. A two-factor model resulted in a stable solution that explained about 70% of the variance in a study made by Zheng et al. (2003). The data consisted of MRI images of five American English speakers pronouncing nine English vowels. Hu (2006) presented a study on the Chinese dialect called Ningbo. Seven speakers pronouncing ten vowels were recorded by means of EMA. Two factors explained about 90% of the variance. More recently, Ananthakrishnan (2010) proposed a two factor PARAFAC model that accounted for 71% of the variance explanation for three French speakers articulating 13 vowels. The present study attempts to extend this type of modelling from vowels to consonants. We first describe the set of data acquired to perform the different experiments; then we describe the performance of individual speaker models and compare them in terms of variance explained, RMSE and individual articulatory strategies. Next, we present an 530  attempt to build a single model to drive the tongue contours of all the speakers based on multi-linear decomposition methods. We perform a PARAFAC solution up to 10 components and a more practical solution using Multiple Linear Regression (MLR) with a large number of components.  2. Data  In this study, midsagittal Magnetic Resonance Images (MRI) of seven French speakers (two males: PB, YL, and five females: HL, AA, MG, AK, MGO) have been collected. The subjects were asked to pronounce and sustain 63 different articulations for 16 seconds each. The corpus consisted of the 10 French oral vowels /i e ɛ a y ø œ u o ɔ/, the 3 nasal vowels /ã ɛ̃ ɔ/̃ and the 10 consonants /p t k f s ʃ m n ʁ l/ articulated in symmetric VCV context of five vowels /a e ɛ i u/. The contour of the tongue was manually traced. The present study is limited to the contour from the tongue tip to the base of the epiglottis, which is resampled with N = 150 equidistant points to model what we call Tongue upper contour.  3. Individual articulatory models (PCA)  PCA is a two-way factor analysis approach often used for dimensionality reduction and  analysis of data sets to summarize their main characteristics. Consider articulatory  measurements for the speaker s: 1  , which consists of Xs = , , … , , being  xa (1  ) a row vector of measurements for the articulation a: 1  . Such  that Xs is decomposed into a set of control parameters π  (set of Cmp components  that explain the variations in articulations) and the articulatory model C  (coefficients that explain the contribution of each articulator point to the components)  by the following equation: Xs =π C γ , where γ is the residual error.  percentage of variance explanation % RMSE in centimeters  LOOCV PCA Variance explained 1  0.9  0.8  0.7  0.6  0.5  0.4  Subject pb  0.3  Subject yl  Subject hl  0.2  Subject aa  Subject mg  0.1  Subject ak  Subject mgo  0  
[lol]: a preliminary study of laughter in 18- to 36- month old children Laughter is a non-verbal vocal behavior that has been extensively studied in philosophy and biology, but researches investigating the phonetics of laughter are pretty rare. The aim of this study is to see whether there is any kind of correlation between child language development and the acoustic characteristics of their mirth. To answer this question, we analyzed 120 laughter samples spontaneously produced by three 18- to 36month old children in natural conditions. Results show that among the 11 investigated acoustic cues, only the relative intensity increases significantly with age and that there is a great inter-individual variability between children. Moreover, laughters are characterized by a large majority of Rise-Fall contours and their amount increases with age. MOTS-CLES : Rire, développement, enfant, analyse acoustique, prosodie. KEYWORDS: Laughter, development, child, acoustic analysis, prosody. 
French Liaison in casual speech : automatic and manual investigations The realisation of the French Liaison is investigated in a large corpus of casual speech. Considering that casual speech gives rise to a large range of pronunciation variants and that overall temporal reduction increases (word and phone duration measurements) as compared to read and prepared speech, one may hypothesize that French liaison tends to be less productive in this kind of speaking style. We made use of automatic speech alignments to evaluate optional liaison realisations in potential liaison sites (word ending in a liaison consonant followed by a word-initial (semi)-vowel). Speech comes from the NCCFr corpus including 46 mostly young speakers with a total of more than 30 hours of speech. Realized liaisons were examined and measured for the most frequent liaison consonants (/z/, /n/ and /t/) as a function of a classiﬁcation of the sites as mandatory, optional or forbidden with respect to liaison realization. An original contribution investigates liaison realization as a function of a speaker-dependent speech rate measure. MOTS-CLÉS : Variantes de prononciation, liaison, parole familière, réduction temporelle. KEYWORDS: Pronunciation variants, liaison, French, casual speech, temporal reduction. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 545–552, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 545  
Percol0 - A multimodal person detection system in video documents The goal of the PERCOL project is to participate to the REPERE multimodal evaluation program by building a consortium combining different scientiﬁc ﬁelds (audio, text and video) in order to perform person recognition in video documents. The two main scientiﬁc challenges we are addressing are ﬁrstly multimodal fusion algorithms for automatic person recognition in video broadcast ; and secondly the improvement of information extraction from speech and images thanks to a combine decoding using both modalities to reduce decoding ambiguities. MOTS-CLÉS : Reconnaissance Automatique de la Parole, Segmentation en locuteurs, reconnais- sance de l’écriture, détection de visages. KEYWORDS: Automatic speech reconnaissance, speaker diarization, Optic Character Recognition, Face Detection. 
The aim of this acoustic study is to examine place of articulation during the production of voiced plosives by children with a cleft palate. The data are compared with those obtained from control children without speech pathological problems. Formant values of F2 and F3 are measured at burst-release of the plosives. Analyses are carried out for fifty-two children, divided into two groups, ranging from 9 to 18 years old. Each group of twenty-six kids comprised children with a cleft palate or cleft lip and palate, and children without any speech disorder. Results reveal differences in F2/F3 values between impaired children and unimpaired ones, and also between the different age groups, for impaired children. Differences in articulatory strategies are inferred from analyses of F2/F3 relations. MOTS-CLES : fente palatine, formants, occlusives, transition, enfant, phonétique clinique KEYWORDS : cleft palate, F2/F3 formants, plosives, transitions, children, clinical phonetics Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 561–568, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 561  
Segmental evaluation of HTS HTS is a parametric speech synthesis system based on the use of Hidden Markov Models (HMM). HTS is now widely used but very few studies have been conducted to analyze the inﬂuence of parameters on the quality of the synthetic speech. The aim of this paper is to provide an objective evaluation of the quality of the synthetic speech produced by HTS in order to assess the inﬂuence of multiple descriptions. Our study concerns only the speech spectrum analysis and is applied to French. We propose to use GMM to measure the degradations introduced by HTS compared to reference voice. Finally, we propose a listening test in order to calibrate our objective measure. This method indicates that using other descriptors than the previous-current-next phoneme sequence does not improve signiﬁcantly the modelisation of the spectrum. MOTS-CLÉS : HTS, qualité segmentale, évaluation, GMM. KEYWORDS: HTS, evaluation, segmental quality, GMM, spectral features. 
_______________________________________________________________________________________________    Read the tones on the lips : visual perception(s) of lexical tones in Mandarin Chinese   The aim of the present study is to verify whether the visual cues located on the neck, can contribute in Mandarin tones visual perception. However, in an unexpected way, this study shows that tones can be read on the lips, even when the syllable is pronounced in the back of the oral cavity. It seems indeed on the one hand that the labial reading is possible for Mandarin tones, on the other hand, that there could be various profiles of perception : some people seem to be more sensitive to the labial reading, other people could a priori use the neck’s cues, and they would be less suited to the labial reading.   MOTS-­‐CLES : chinois mandarin, tons, perception audiovisuelle, lecture labiale, multimodalité.  KEYWORDS : Mandarin Chinese, tones, audiovisual perception, labial reading, multimodality.   
Source separation by cepstral smoothing of binary masks In this paper, we propose a separation system of speech signals from two convolutive mixtures. The suggested system is based on the combination of blind source separation technique with a time-frequency masking procedure, followed by a smoothing cepstral. Indeed, after separation of signal sources, the estimated binary masks undergo a cepstral smoothing to reduce the ﬂuctuations artifacts which introduced by time-frequency masking operation. The evaluation results have shown the effectiveness of the proposed system even in the most unfavorable case. MOTS-CLÉS : Masque binaire idéal, Lissage cepstral, Séparation aveugle de sources. KEYWORDS: Ideal binary mask, Cepstral smoothing, Blind source separation . 
Since several years new tracks are explored to revisit speech production, emergence and development. Data bases and modeling concerning genetics (HOX and noHOX genes), biometrical data of head, hyoid bone and cervical vertebrae (C1-C7), muscle anatomy, developmental phonetics, vocal tract modeling (geometrical and biomechanical), and swallowing physiology have been interwoven in order to provide new insights on speech production. This research integrates, along all the steps, the realization of computerized dynamic graphics and video illustrations. They will provide help for speech researchers, physicians and speech therapists. MOTS-CLÉS : production et modélisation articulatoire, génétique, anatomie, biométrie, déglutition KEYWORDS : speech production and articulatory modeling, genetics, anatomy, biometry, swallowing 
Most of the existing prosodic transcription systems display some limitations: (i) they cannot account for the various prosodic phenomena (accentuation, phrasing and tonal patterns), as they focus on the variation of intonational patterns; and (ii) they usually rely on phonological knowledge concerning the language that has to be transcribed, which is problematic for unknown languages and dialects. To try to overcome these drawbacks, we are trying to develop a prosodic transcription tool (PROSTRAN), which automatically assigns to each utterance a multi-tiered transcription that symbolically represents how the three prosodic parameters (F0, duration & energy) vary over time. The goal of this paper is twofold: (i) providing a description of PROSOTRAN (ii) and evaluating different transcriptions obtained from distinct calculation procedures. MOTS-CLES : Système de transcription de la prosodie, outils d’annotation, prosodie, interface phonétique/phonologie. KEYWORDS : Prosodic annotation systems, automatic annotation tools, prosody, phonetic implementation and phonological analysis 
Vocalic system’s typology revisited from the functional load viewpoint Most studies in phonological typology implicitly assume that all segments in an inventory are equally important. We suggest here that the notion of functional load enriches this usual representation of phonological systems. Focusing on the vowel systems of 12 languages, we developed a corpus-based approach that reveals a very uneven use of the vowel contrasts available from their inventory. Furthermore, the crosslinguistic comparison reveals no uniform tendency to favor maximal contrasts (such as /a/~/i/ and /a/~/u/). It actually highlights that several languages with similar inventories may exhibit different contrast patterns, questioning some well-established universal tendencies. MOTS-CLÉS : système vocalique, typologie, opposition distinctive, charge fonctionnelle. KEYWORDS : vocalic system, typology, phonemic contrast, functional load. “The function of a phonemic system is to keep the utterances of a language apart. Some contrasts between the phonemes in a system apparently do more of this job than others.” Charles F. Hockett (1966) 
This paper investigates the lengthening of certain vowels in Belgian French and its influence on the perception of a Belgian accent. Based on recordings made in Belgium, two perceptual experiments were conducted, involving Belgian and French listeners: the first one enabled us, in a robust way, to identify lengthened vowels perceived as regionally marked by experts; the second one, using prosody modification/resynthesis, tested the impact of vowel lengthening with respect to the perception of a Belgian accent among naïve listeners. The first experiment showed that the vast majority of vowels perceived as lengthened are in word-penultimate syllable or belong to monosyllabic words and that these vowels are generally nasal or mid-closed vowels. The second experiment suggests that, all other things being equal, speech samples including vowel lengthening phenomena are rated as having a higher degree of accentedness than their counterparts without vowel lengthening. MOTS-CLÉS : variation régionale, accent belge, perception, (re)synthèse de prosodie. KEYWORDS: regional variation, Belgian accent, perception, prosody (re)synthesis. 
Developments of Swahili resources for an automatic speech recognition system This article describes our efforts to provide ASR resources for Swahili, a Bantu language spoken in a wide area of East Africa. We start with an introduction on the language situation. Then, we report the selected strategies to develop a text corpus, a pronunciation dictionary and a speech corpus for this under-resourced language. We explore methodologies as crowdsourcing or collaborative transcription process. Besides, we take advantage of some linguistic characteristics of the language such as rich morphology or shared vocabulary with English to improve performance of our baseline Swahili ASR system in a broadcast speech transcription task. MOTS-CLÉS : Swahili, langues peu dotées, reconnaissance automatique de la parole, ressources numériques. KEYWORDS: Swahili, under-resourced languages, automatic speech recognition, resources. 
Pronunciation generation for proper names using Conditional Random Fields We propose an approach to grapheme-to-phoneme conversion for proper names based on a probabilistic method: Conditional Random Fields (CRFs). CRFs give a long term prediction, assume a relafex state independence condition and allow a tag integration. In previous work, grapheme-to-phoneme conversion using CRF has been proposed for non proper names and different CRF features are studied. In this paper, we extend this work to proper names. Moreover, we propose an algorithm for origine detection of proper names of foreign origins. The proposed system is validated on two pronunciation dictionaries. Our approach compares favorably with the performance of the state-of-the-art Joint-Multigram Models and takes advantage of the knowledge of the origin of the proper name. MOTS-CLÉS : reconnaissance de la parole, noms propres, phonétisation du lexique, champs aléatoires conditionnels (CRF) KEYWORDS: automatic speech recognition, proper names, lexique phonetisation, conditional random field (CRF). 
In this study we compare the ESTER corpus of journalistic speech (Galliano et al., 2005) and the NCCF corpus of spontaneous speech (Torreira et al, 2010) in terms of duration, f0 and spectral reduction in productions automatically detected as sequences between, pauses. Continuation f0 rises are overall absent in spontaneous speech and sequences reveal a declination slope with less amplitude than in journalistic speech. For both corpora, lengthening starts around 60% of the sequence duration, but significantly less in spontaneous speech. Lengthening in the initial part of the sequence is observed in journalistic speech only. As expected we measure a faster speech rate in spontaneous speech with shorter vowel durations implying a more important vowel reduction. MOTS-CLES : parole journalistique, spontané, déclinaison, f0, durée, réduction spectrale. KEYWORDS : journalistic speech, spontaneous, declination line, f0, duration, spectral reduction. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 649–656, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 649  
Distant Speech Recognition in a Smart Home : Comparison of Several Multisource ASRs in Realistic Conditions While the smart home domain has become a major ﬁeld of application of ICT to improve support and wellness of people in loss of autonomy, speech technology in smart home has, comparatively to other ICTs, received limited attention. This paper presents the SWEET-HOME project whose aim is to make it possible for frail persons to control their domestic environment through voice interfaces. Several state-of-the-art and novel ASR techniques were evaluated on realistic data acquired in a multiroom smart home. This distant speech French corpus was recorded with 21 speakers playing scenarios including activities of daily living in a smart home equipped with several microphones. Techniques acting at the decoding stage and using a priori knowledge such as DDA give better results than the baseline and other approaches (Lecouteux et al., 2011). MOTS-CLÉS : domotique, parole distance, habitat intelligent, SRAP multisource. KEYWORDS: home automation, smart home, distant speech, multisource ASRs. 
Many neurocognitive aspects associated with the processing of speech were up to now studied by the analysis of event-related potentials. However, none of these cortical responses can be considered as a direct indicator of successful lexical access during speech comprehension. The aim of the present study is to develop an experimental paradigm and a statistical analysis on electrophysiological data, in order to identify timefrequency patterns in the oscillatory cortical activity that correlate with the intelligibility of degraded speech. For this purpose we used noise-vocoded speech that is very difficult to understand without prior exposure. Noise-vocoded words were presented before and after a short period of perceptual learning, and we compared the oscillatory activity following stimuli rated as “intelligible” or “unintelligible” by participants (N=12). Results show that we were able to identify three oscillatory activities with specific topology and latency resulting from a successful lexical access. MOTS-CLES : Intelligibilité, Noise-vocoded speech, EEG, Oscillations corticales KEYWORDS : Intelligibility, Noise-vocoded speech, EEG, Cortical oscillatory activity 
Distance encoding and speech/gesture cooperation : a developmental study on multimodal pointing The aim of this paper is to characterize, through the deictic process use, the speech/gesture interaction during language developement. Participants, both adults and children, had to designate with a deictic word and/or a deictic gesture a target, which could be either close or distant. The use of two vs one modality (speech and gesture) allowed us to attest a cooperation between the two systems, which take place in a complementary rather than redundant way. Furthermore, our results are attesting that distance can be encoded not only in articulatory cues of vocal pointing, but also in kinematic cues of manual pointing. MOTS-CLÉS : Interaction parole/geste ; Pointage déictique ; Encodage de la distance ; Dévelop- pement. KEYWORDS: Speech/gesture interaction ; Deictic pointing ; Distance encoding ; Development. 
Prediction of transcription indexability This paper presents a semantic conﬁdence measure that aims to predict the relevance of automatic transcripts for a task of Spoken Document Retrieval (SDR). The proposed predicting method relies on the combination of Automatic Speech Recognition conﬁdence measure and a Semantic Compacity Index, that estimates the relevance of the words considering the semantic context in which they occurred. Experiments are conducted on the French Broadcast news corpus ESTER 2, by simulating a classical SDR usage scenario : users submit text-queries to a search engine that is expected to return the most relevant documents regarding the query. Results demonstrate the interest of using semantic level information to predict the transcription indexability. MOTS-CLÉS : Reconnaissance de la parole, mesure de conﬁance, recherche d’information, document audio. KEYWORDS: Speech recognition, conﬁdence measures, spoken document retrieval. 
Assessment of the acoustic models performance in the ageing voice case for ASR system adaptation Our study concerns the integration of an automatic speech recognition system in a social inclusion product designed for elderly people. Due to voice change with age, speech recognition systems present higher word error rate when speech is uttered by elderly speakers compared to when non-aged voice is considered. To characterise these differences in speech recognition performance, we studied which phonemes lead to the lowest recognition rate in the elderly speakers with respect to the younger ones and we collected a speciﬁc corpus to make the adaptation of the acoustic models possible. The results show that some phonemes (such as plosives) are more speciﬁcally affected by age than others. Finally, the corpus was used to adapt the ASR to the elderly population which resulted in a 5% decrease of the word error rate. MOTS-CLÉS : reconnaissance automatique de parole, voix des personnes âgées, adaptation acoustique, régression linéaire du maximum de vraisemblance. KEYWORDS: automatic speech recognition, ageing voice, acoustic adaptation, maximum likeli- hood linear regression. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 707–714, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 707  
In order to validate an fMRI experiment, four participants were examined using functional magnetic resonance imaging while executing oro-facial movements, vowel and syllable production. This protocol will be used with patients who underwent oral resection. The study’s results should contribute to better understand cognitive processes associated with speech production. The three motor tasks activated a set of common brain areas classically involved in motor control and temporal areas involved in speech. These results support previous brain imaging studies and validate our protocol. MOTS-CLES : contrôle moteur oro-facial, production de la parole, IRMf, sparse sampling. KEYWORDS : oro-facial motor control, speech production, fMRI, sparse sampling. 
Towards Fully Automatic Annotation of Audio Books for Text-To-Speech (TTS) Synthesis Building speech corpora is a crucial step for every text-to-speech synthesis system. Nowadays, statistical models require enormous corpora that need to be recorded, transcribed, annotated and segmented to be usable. The variety of corpora necessary for recent applications (content, style, etc.) makes the use of existing audio resources very attractive. Taking the above considerations into account, a complete acquisition, segmentation and annotation chain for audio books, which tends to be fully automatic, is proposed. This process relies on a data structure, ROOTS, which establishes the relations between the different annotation levels. This methodology has been applied successfully on 11 hours of speech extracted from an audio book. A manual check, on a part of the corpus, has shown the efﬁciency of the process. MOTS-CLÉS : Livres audio, annotation, segmentation, synthèse de la parole. KEYWORDS: Audio books, annotation, segmentation, text-to-speech synthesis. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 731–738, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 731  
The Labial-Coronal effect in Italian Some studies have shown that there is a Labial-Coronal order preference both in children’s productions at the first-word stage and in the lexicons of various world languages. This Labial-Coronal effect (LC effect) has recently been studied in experimental phonetics. The goal of our study is to observe whether in Italian lexical stress disrupts this consonantal effect. First, we looked for the LC effect in a corpus of the 2000 most common words in Italian, which had been phonologically transcribed and syllabified. Then we tested the stability of stressed LC (Labial-Coronal) vs. CL (Coronal-Labial) patterns in an Italian native speaker by means of a speech production task using an accelerated repetition procedure. Mots-clés : Effet Labial-Coronal, universaux syllabiques, accent lexical, italien. Keywords : Labial-Coronal effect, syllable universals, lexical stress, Italian. 
ABSTRACT _________________________________________________________________________________________________________ When nasal is more than nasal: Oral articulation of French nasal vowels Lingual and labial articulations of oral and nasal vowels of three Metropolitan French (FM) speakers were recorded using an EMA system. Inter-speaker variation in these oral articulations suggest that the role of motor equivalence is important in the acoustic dispersion of this vowel system: the speakers have a similar acoustic output, but use different articulatory strategies to achieve this output. MOTS-CLES : Nasalisation vocalique, production vocalique, français, articulation, EMA KEYWORDS : Vowel nasalization, vowel production, French, articulation, EMA 
ABSTRACT _________________________________________________________________________________________________________ Acoustical and linguistic masking effects of different languages on the comprehension of French words The goal of our research is to explore linguistic interferences which occur during speechin-speech comprehension. Intensity, nature and language of the background noise were manipulated. Native French participants had to realize a lexical decision task on French target items inserted at 0 dB or -5 dB in background speech or fluctuating noise produced in French, Irish and Italian. At -5 dB, results showed that French and Italian had a stronger masking effect on target speech than Irish. A comparison of performances obtained with background speech and fluctuating noise revealed that for French, target speech was masked by acoustic and linguistic information whereas the degradation from Italian and Irish was only acoustic. MOTS-CLES : masqueur parolier, bruit fluctuant, compréhension de la parole, interlangue. KEYWORDS : babble noise, fluctuating noise, speech comprehension, interlanguage. 
Exploitation of a classification tolerance margin for improving the estimation of class-based acoustic models for speech recognition This paper presents the introduction of a classification tolerance margin in the classification of the training data for building class-based acoustic models for automatic speech transcription. Indeed, although automatic classification of speech data makes it possible to go beyond the traditional male / female partition, the number of usable classes is actually limited by the reliability of the associated acoustic models which, unfortunately, decreases when the number of classes increases. The reported experiments show that using a tolerance margin in the classification process increases the amount of training data associated to each class, and consequently increases the reliability of the acoustic models of the classes. The performance evaluation carried on the ESTER2 data have shown that it is possible with the proposed approach to build class-based acoustic models that lead to better speech recognition performance than with the usual genderbased acoustic models. MOTS-CLES :Reconnaissance de la parole, classification automatique, modèles acoustiques de classes, marge de tolérance de classification, KEYWORDS :Speech recognition, automatic classification, class-based acoustic models, classification tolerance margin Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 763–770, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 763  
It was shown in previous studies that Tokyo-Japanese speakers have difficulty in learning to produce the French /u/, characterized by a grouping of the first two formants below 1000 Hz, and that they tend to produce a vowel with an F2 higher than 1000 Hz instead, which is perceived rather as /ø/ by native listeners of French. The /u/ in Kansai Japanese is considered as rounded, with an F2 inferior to 1000 Hz, unlike that of Tokyo Japanese (Sugitô, 1995), which leads us to anticipate less difficulty in pronouncing the French /u/. Preliminary results based on 8 learners (including 4 from Kansai) show that one learner from Kansai produces /u/ with an F2 inferior to 1000 Hz, even if /ø/ is pronounced similarly. These findings suggest that the phonetic acquisition of /u/ is facilitated, but the phonemic opposition /u/-/ø/ remains a major difficulty. MOTS-CLES : français, acquisition L2, voyelles, production, dialecte natif, japonophones. KEYWORDS: French, L2 acquisition, vowels, production, native dialect, Japanese-speakers. 
Robustness and portability of spoken language understanding systems among languages and domains : the PORTMEDIA project The ANR PORTMEDIA projet aimed at complementing the MEDIA corpus so as to foster the development of new performing approaches, including statistical approaches, for the automatic spoken language understanding in the framework of human-machine spoken dialogue systems. The main topics for which work has been carried out are : robustness to speech recognition errors, language portability, domain portability and high-level semantic representation. Thus while elaborating some solutions to theses issues inside the projet itself, we focused our efforts towards collecting new data and metadata which could help other research groups to evaluate their own propositions in the best conditions possible. MOTS-CLÉS : corpus de dialogue oraux, compréhension de la parole, reconnaissance de la parole, multilinguisme, portabilité, représentation sémantique. KEYWORDS: spoken language understanding, dialogue systems, speech recognition system, multilingual, portability, semantic representation. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 779–786, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 779  
Improvements on driven decoding system combination This paper proposes an improved driven decoding method for speech recognition system combination. The combination method involves the use of auxiliary transcription as external information source included on primary system decoding process. Auxiliary transcriptions are used to modify search space exploration via linguistic score reevaluation. it was shown that DDA outperforms ROVER when the primary system is guided by a more accurate system. In this paper we propose a new method to manage auxiliary transcriptions which are presented as a bag-of-n-grams (BONG) without temporal matching. These modiﬁcations allow to make easier the combination of several hypotheses given by different auxiliary systems and improves primary system WER even with less accurate auxiliary systems. MOTS-CLÉS : Reconnaissance de la parole, combinaison de systèmes, décodage guidé. KEYWORDS: Speech recognition, systems combination, driven decoding. 
Given the multisensory nature of speech perception, one fundamental question is whether sensory signals are integrated early in the speech processing hierarchy and may reflect predictive, anticipatory, mechanisms. The present pilot EEG study aimed at investigating a possible modulation of auditory-evoked N1 component during audio-haptic compared to purely auditory speech perception. To this aim, we compared auditory-evoked N1 responses from five participants during auditory, audio-visual and audio-haptic perception of syllables. In line with previous studies, auditory-evoked N1 amplitude was attenuated during audiovisual compared to auditory speech perception. Crucially, similar results were observed for audio-haptic compared to auditory speech perception for parietal electrodes, with shortened latency. Altogether, these results suggest some early integrative mechanisms between auditory, visual and haptic modalities in speech perception as well as a predictive role of haptic information in auditory speech processing. MOTS-CLES : perception de la parole, multimodalité, interactions audio-haptique, EEG. KEYWORDS : speech perception, multimodality, audio-haptic interactions, EEG. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 803–810, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 803  
Error region detection and characterization in transcriptions of multimedia documents : application to person name search In this article, we propose to detect and characterize error regions in automatic transcriptions of multimedia documents. The simultaneous detection and characterization could be seen as a sequence labeling task where we compare sequential approaches (segmentation then classiﬁcation) and an integrated one. We compare our system performance on two different corpus by varying training data. We are particularly interested in person name errors, essential information in various information extraction applications. Results conﬁrm the interest for learning-based method using the apparition context of errors. MOTS-CLÉS : régions d’erreurs, caractérisation des erreurs, transcription automatique, classiﬁ- cation automatique, noms propres. KEYWORDS: error regions, error characterization, automatic transcription, automatic classiﬁca- tion, person names. 
French Cued Speech perception and expertise effect in hearing people Since French Cued Speech (CS) is multi-signal (lip movements and CS gestures), we conducted an eye tracking study to examine whether this perception involves integrative treatment and how expertise affects it in normally-hearing participants. Our paradigm consisted in a word/pseudowords (presented in video clip, without sound) identification task. It included three conditions: a CS condition, a meaningless gesture condition, and a lipreading condition. After each video, participants were presented three options (i.e. correct answer, labial distractor and manual distractor) and instructed to select the correct one. Behavioral and eye tracking data were collected on three groups of normally-hearing partcipants: experts in CS, beginners in CS, and completely naïve toward CS. The first results, very promising, suggest that only experts and beginners integrate CS gestural and labial information, and that the relative weight of labial and manual information seems to change with expertise. MOTS-CLES : Langue française Parlée Complétée, perception, effet d’expertise, intégration multi-signal, oculométrie KEYWORDS : French Cued Speech perception, effect of expertise, multi signal integration, eye tracking Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 819–826, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 819  
Combination of approaches for speaker role recognition In this article, we are particularly interested in recognizing speaker role inside broadcast news shows. Previous studies highlighted a link between speech spontaneity and speaker roles. An automatic spontaneous speech detection system has already been applied to recognize speaker roles, without any change in the method process (Dufour et al., 2011). We propose to improve this method by adding speciﬁc speaker role features. Thus, features from Social Network Analysis (SNA) are used in the method. This new information allowed to improve the two decision steps oh the method, with a gain of 3.2 and 1.9 points in absolute, respectively for the local then the global decision. Finally for a larger number of focused roles than usually retained, the proposed method allowed to associate the correct role to 76.3% of the speakers. MOTS-CLÉS : indexation automatique, analyse des réseaux sociaux, parole spontanée, rôle du locuteur. KEYWORDS: document indexing, social network analysis, spontaneous speech, speaker role recognition. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 1: JEP, pages 827–834, Grenoble, 4 au 8 juin 2012. c 2012 ATALA & AFCP 827  
In this experiment we explored the effect of the orientation of attention on speech processing in a categorization task. We oriented the subjects’ attention by providing information about the cues that were relevant for classifying the sounds and by giving feedback after each response. Our results tended to show a modified behavior when categorizing at the end of this 15-minute experiment. The feedback strongly contributed to these results confirming its prominent role in perceptual learning. The instructions also seemed to have a differential effect on performance. These exploratory data suggest that it is possible to influence subjects’ behavior by selectively orienting attention. Future research is needed to corroborate these results and to uncover the processes involved in perceptual learning. MOTS-CLES : apprentissage perceptuel, catégorisation, attention sélective KEYWORDS : perceptual learning, categorization, selective attention 
Two experiments were conducted to examine how the knowledge of the patient’s clinical state affects the results of perception of voice quality. This study involved 53 dysphonic speakers recorded twice in different circumstances. These pairs of voices were presented to seven listeners. The task was to perceptually compare the severity of the dysphonia between the 2 recordings of the pair. Stimuli were presented first in a blind test, then several weeks later with accompanying information about the patient (pre- or posttreatment). We balanced this artificial contextual information in order to reinforce the blind judgment or be inconsistent in a clinical point of view compared to the blind test. Results revealed that in the clinical-consistent context, the preference was amplified in a significant way. In clinical-inconsistent condition, we observed an inhibition effect or a change of decision. In this condition, the judgment was more dependent on the contextual information than on the auditory sensation obtained in blind condition. MOTS-CLES : perception, qualité vocale, dysphonie, processus descendant montant KEYWORDS : perception, voice quality, dysphonia, bottom-up top-down processes 
ABSTRACT _________________________________________________________________________________________________________ Robustness of fine acoustic cues and Speech variability: a Mismatch Negativity study We examined electrophysiological correlates of listener’s sensitivity to fine acoustic cues in intra-speaker variability conditions in order to test the relevance of such cues for the speech perception system. For this purpose, a modified oddball paradigm has been used with syllables such as French homophones la and l’a, and in a second experiment with longer sequences such as la mie and l’amie, both /lami/. The main result of this study was the observation of a mismatch negativity (MMN) for homophone deviants. Speech perception system is thus sensitive to subphonemic differences between homophone sequences despite the speech variability. Fine acoustic cues are robust enough to play a role in speech processing. MOTS-CLES : Indices acoustiques fins, Mismatch Negativity, traitement de la parole KEYWORDS: Fine acoustic cues, Mismatch Negativity, speech processing 
Machine Translation is a well–established ﬁeld, yet the majority of current systems translate sentences in isolation, losing valuable contextual information from previously translated sentences in the discourse. One important type of contextual information concerns who or what a coreferring pronoun corefers to (i.e., its antecedent). Languages differ signiﬁcantly in how they achieve coreference, and awareness of antecedents is important in choosing the correct pronoun. Disregarding a pronoun’s antecedent in translation can lead to inappropriate coreferring forms in the target text, seriously degrading a reader’s ability to understand it. This work assesses the extent to which source-language annotation of coreferring pronouns can improve English–Czech Statistical Machine Translation (SMT). As with previous attempts that use this method, the results show little improvement. This paper attempts to explain why and to provide insight into the factors affecting performance. 
Classifying text genres across languages can bring the beneﬁts of genre classiﬁcation to the target language without the costs of manual annotation. This article introduces the ﬁrst approach to this task, which exploits text features that can be considered stable genre predictors across languages. My experiments show this method to perform equally well or better than full text translation combined with monolingual classiﬁcation, while requiring fewer resources. 
Adaptive Dialogue Systems are rapidly becoming part of our everyday lives. As they progress and adopt new technologies they become more intelligent and able to adapt better and faster to their environment. Research in this ﬁeld is currently focused on how to achieve adaptation, and particularly on applying Reinforcement Learning (RL) techniques, so a comparative study of the related methods, such as this, is necessary. In this work we compare several standard and state of the art online RL algorithms that are used to train the dialogue manager in a dynamic environment, aiming to aid researchers / developers choose the appropriate RL algorithm for their system. This is the ﬁrst work, to the best of our knowledge, to evaluate online RL algorithms on the dialogue problem and in a dynamic environment. 
Myanmar language and script are unique and complex. Up to our knowledge, considerable amount of work has not yet been done in describing Myanmar script using formal language theory. This paper presents manually constructed context free grammar (CFG) with “111” productions to describe the Myanmar Syllable Structure. We make our CFG in conformity with the properties of LL(1) grammar so that we can apply conventional parsing technique called predictive top-down parsing to identify Myanmar syllables. We present Myanmar syllable structure according to orthographic rules. We also discuss the preprocessing step called contraction for vowels and consonant conjuncts. We make LL (1) grammar in which “1” does not mean exactly one character of lookahead for parsing because of the above mentioned contracted forms. We use five basic sub syllabic elements to construct CFG and found that all possible syllable combinations in Myanmar Orthography can be parsed correctly using the proposed grammar. 
There are lexical, syntactic, semantic and discourse variations amongst the languages used in various biomedical subdomains. It is important to recognise such differences and understand that biomedical tools that work well on some subdomains may not work as well on others. We report here on the semantic variations that occur in the sublanguages of two biomedical subdomains, i.e. cell biology and pharmacology, at the level of named entity information. By building a classiﬁer using ratios of named entities as features, we show that named entity information can discriminate between documents from each subdomain. More speciﬁcally, our classiﬁer can distinguish between documents belonging to each subdomain with an accuracy of 91.1% F-score. 
Language identiﬁcation of written text has been studied for several decades. Despite this fact, most of the research is focused on a few most spoken languages, whereas the minor ones are ignored. The identiﬁcation of a larger number of languages brings new difﬁculties that do not occur for a few languages. These difﬁculties are causing decreased accuracy. The objective of this paper is to investigate the sources of such degradation. In order to isolate the impact of individual factors, 5 different algorithms and 3 different number of languages are used. The Support Vector Machine algorithm achieved an accuracy of 98% for 90 languages and the YALI algorithm based on a scoring function had an accuracy of 95.4%. The YALI algorithm has slightly lower accuracy but classiﬁes around 17 times faster and its training is more than 4000 times faster. Three different data sets with various number of languages and sample sizes were prepared to overcome the lack of standardized data sets. These data sets are now publicly available. 
To cluster textual sequence types (discourse types/modes) in French texts, K-means algorithm with high-dimensional embeddings and fuzzy clustering algorithm were applied on clauses whose POS (part-ofspeech) n-gram proﬁles were previously extracted. Uni-, bi- and trigrams were used on four 19th century French short stories by Maupassant. For high-dimensional embeddings, power transformations on the chisquared distances between clauses were explored. Preliminary results show that highdimensional embeddings improve the quality of clustering, contrasting the use of biand trigrams whose performance is disappointing, possibly because of feature space sparsity. 
In this work I address the challenge of augmenting n-gram language models according to prior linguistic intuitions. I argue that the family of hierarchical Pitman-Yor language models is an attractive vehicle through which to address the problem, and demonstrate the approach by proposing a model for German compounds. In an empirical evaluation, the model outperforms the Kneser-Ney model in terms of perplexity, and achieves preliminary improvements in English-German translation. 
This paper is focused on one aspect of SOPMI, an unsupervised approach to sentiment vocabulary acquisition proposed by Turney (Turney and Littman, 2003). The method, originally applied and evaluated for English, is often used in bootstrapping sentiment lexicons for European languages where no such resources typically exist. In general, SO-PMI values are computed from word co-occurrence frequencies in the neighbourhoods of two small sets of paradigm words. The goal of this work is to investigate how lexeme selection affects the quality of obtained sentiment estimations. This has been achieved by comparing ad hoc random lexeme selection with two alternative heuristics, based on clustering and SVD decomposition of a word co-occurrence matrix, demonstrating superiority of the latter methods. The work can be also interpreted as sensitivity analysis on SO-PMI with regard to paradigm word selection. The experiments were carried out for Polish. 
Null subjects are non overtly expressed subject pronouns found in pro-drop languages such as Italian and Spanish. In this study we quantify and compare the occurrence of this phenomenon in these two languages. Next, we evaluate null subjects’ translation into French, a “non prodrop” language. We use the Europarl corpus to evaluate two MT systems on their performance regarding null subject translation: Its-2, a rule-based system developed at LATL, and a statistical system built using the Moses toolkit. Then we add a rule-based preprocessor and a statistical post-editor to the Its-2 translation pipeline. A second evaluation of the improved Its-2 system shows an average increase of 15.46% in correct pro-drop translations for Italian-French and 12.80% for Spanish-French. 
 2 Web Services and Workﬂows  This paper demonstrates a novel distributed architecture to facilitate the acquisition of Language Resources. We build a factory that automates the stages involved in the acquisition, production, updating and maintenance of these resources. The factory is designed as a platform where functionalities are deployed as web services, which can be combined in complex acquisition chains using workﬂows. We show a case study, which acquires a Translation Memory for a given pair of languages and a domain using web services for crawling, sentence alignment and conversion to TMX. 
 The emergence of the WWW as the main source of distributing content opened the floodgates of information. The sheer volume and diversity of this content necessitate an approach that will reinvent the way it is analysed. The quantitative route to processing information which relies on content management tools provides structural analysis. The challenge we address is to evolve from the process of streamlining data to a level of understanding that assigns value to content.  We present an open-source multilingual  platform ATALS that incorporates  human language technologies in the  process of multilingual web content  management. It complements a content  management  software-as-a-service  component i-Publisher, used for creating,  running and managing dynamic content-  driven websites with a linguistic  platform. The platform enriches the  content of these websites with revealing  details and reduces the manual work of  classification editors by automatically  categorising content. The platform ASSET supports six European languages. We expect ASSET to serve as a basis for future development of deep analysis tools capable of generating abstractive summaries and training models for decision making systems. Introduction The advent of the Web revolutionized the way in which content is manipulated and delivered. As a result, digital content in various languages has become widely available on the Internet and its sheer volume and language diversity have presented an opportunity for embracing new methods and tools for content creation and distribution. Although significant improvements have been made in the field of web content management lately, there is still a growing demand for online content services that incorporate language-based technology. Existing software solutions and services such as Google Docs, Slingshot and Amazon implement some of the linguistic mechanisms addressed in the platform. The most used opensource multilingual web content management  6 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 6–10, Avignon, France, April 23 - 27 2012. c 2012 Association for Computational Linguistics  systems (Joomla, Joom!Fish, TYPO3, Drupal)1 offer low level of multilingual content management, providing abilities for building multilingual sites. However, the available services are narrowly focused on meeting the needs of very specific target groups, thus leaving unmet the rising demand for a comprehensive solution for multilingual content management addressing the issues posed by the growing family of languages spoken within the EU. We are going to demonstrate the open-source content management platform ATLAS and as proof of concept, a multilingual library ilibrarian, driven by the platform. The demonstration aims to prove that people reading websites powered by ATLAS can easily find documents, kept in order via the automatic classification, find context-sensitive content, find similar documents in a massive multilingual data collection, and get short summaries in different languages that help the users to discern essential information with unparalleled clarity. The “Technologies behind the system” chapter describes the implementation and the integration approach of the core linguistic processing framework and its key sub-components – the categorisation, summarisation and machinetranslation engines. The chapter “i-Librarian – a case study” outlines the functionalities of an intelligent web application built with our system and the benefits of using it. The chapter “Evaluation” briefly discusses the user evaluation of the new system. The last chapter “Conclusion and Future Work” summarises the main achievements of the system and suggests improvements and extensions.  The processing of text in the system is split into three sequentially executed tasks. Firstly, the text is extracted from the input source (text or binary documents) in the “preprocessing” phase. Secondly, the text is annotated by several NLP tools, chained in a sequence in the “processing” phase. The language processing tools are integrated in a language processing chain (LPC), so that the output of a given NLP tool is used as an input for the next tool in the chain. The baseline LPC for each of the supported languages includes a sentence and paragraph splitter, tokenizer, part of speech tagger, lemmatizer, word sense disambiguation, noun phrase chunker and named entity extractor (Cristea and Pistiol, 2008). The annotations produced by each LPC along with additional statistical methods are subsequently used for detection of keywords and concepts, generation of summary of text, multilabel text categorisation and machine translation. Finally, the annotations are stored in a fusion data store, comprising of relational database and high-performance Lucene4 indexes. The architecture of the language processing framework is depicted in Figure 1.  Technologies behind the system  The linguistic framework ASSET employs diverse natural language processing (NLP) tools technologically and linguistically in a platform, based on UIMA 2 . The UIMA pluggable component architecture and software framework are designed to analyse content and to structure it. The ATLAS core annotation schema, as a uniform representation model, normalizes and harmonizes the heterogeneous nature of the NLP tools3. 
French researchers are required to frequently translate into French the description of their work published in English. At the same time, the need for French people to access articles in English, or to international researchers to access theses or papers in French, is incorrectly resolved via the use of generic translation tools. We propose the demonstration of an end-to-end tool integrated in the HAL open archive for enabling efﬁcient translation for scientiﬁc texts. This tool can give translation suggestions adapted to the scientiﬁc domain, improving by more than 10 points the BLEU score of a generic system. It also provides a post-edition service which captures user post-editing data that can be used to incrementally improve the translations engines. Thus it is helpful for users which need to translate or to access scientiﬁc texts. 
We introduce a method for learning to predict the following grammar and text of the ongoing translation given a source text. In our approach, predictions are offered aimed at reducing users’ burden on lexical and grammar choices, and improving productivity. The method involves learning syntactic phraseology and translation equivalents. At run-time, the source and its translation prefix are sliced into ngrams to generate subsequent grammar and translation predictions. We present a prototype writing assistant, TransAhead1, that applies the method to where computer-assisted translation and language learning meet. The preliminary results show that the method has great potentials in CAT and CALL (significant boost in translation quality is observed). 1. Introduction More and more language learners use the MT systems on the Web for language understanding or learning. However, web translation systems typically suggest a, usually far from perfect, onebest translation and hardly interact with the user. Language learning/sentence translation could be achieved more interactively and appropriately if a system recognized translation as a collaborative sequence of the user’s learning and choosing from the machine-generated predictions of the next-in-line grammar and text and the machine’s adapting to the user’s accepting /overriding the suggestions. Consider the source sentence “我們在結束這個 交易上扮演重要角色” (We play an important role in closing this deal). The best learning environment is probably not the one solely  providing the automated translation. A good learning environment might comprise a writing assistant that gives the user direct control over the target text and offers text and grammar predictions following the ongoing translations. We present a new system, TransAhead, that automatically learns to predict/suggest the grammatical constructs and lexical translations expected to immediately follow the current translation given a source text, and adapts to the user’s choices. Example TransAhead responses to the source “我們在結束這個交易上扮演重要角色” and the ongoing translation “we” and “we play an important role” are shown in Figure 12(a) and (b) respectively. TransAhead has determined the probable subsequent grammatical constructions with constituents lexically translated, shown in pop-up menus (e.g., Figure 1(b) shows a prediction “IN[in] VBG[close, end, …]” due to the history “play role” where lexical items in square brackets are lemmas of potential translations). TransAhead learns these constructs and translations during training. At run-time, TransAhead starts with a source sentence, and iteratively collaborates with the user: by making predictions on the successive grammar patterns and lexical translations, and by adapting to the user’s translation choices to reduce source ambiguities (e.g., word segmentation and senses). In our prototype, TransAhead mediates between users and automatic modules to boost users’ writing/ translation performance (e.g., productivity). 2. Related Work CAT has been an area of active research. Our work addresses an aspect of CAT focusing on language learning. Specifically, our goal is to build a human-computer collaborative writing assistant: helping the language learner with intext grammar and translation and at the same  1Available at http://140.114.214.80/theSite/TransAhead/ which, for the time being, only supports Chrome browsers.  2 Note that grammatical constituents (in all-capitalized words) are represented using Penn parts-of-speech and the history based on the user input is shown in shades.  16 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 16–19, Avignon, France, April 23 - 27 2012. c 2012 Association for Computational Linguistics  Input your source text and start to interact with TransAhead!  Source text: 
Difﬁculty of reading scholarly papers is signiﬁcantly reduced by reader-friendly writing principles. Writing reader-friendly text, however, is challenging due to difﬁculty in recognizing problems in one’s own writing. To help scholars identify and correct potential writing problems, we introduce SWAN (Scientiﬁc Writing AssistaNt) tool. SWAN is a rule-based system that gives feedback based on various quality metrics based on years of experience from scientiﬁc writing classes including 960 scientists of various backgrounds: life sciences, engineering sciences and economics. According to our ﬁrst experiences, users have perceived SWAN as helpful in identifying problematic sections in text and increasing overall clarity of manuscripts. 
We propose a real-time machine translation system that allows users to select a news category and to translate the related live news articles from Arabic, Czech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and Turkish into English. The Moses-based system was optimised for the news domain and differs from other available systems in four ways: (1) News items are automatically categorised on the source side, before translation; (2) Named entity translation is optimised by recognising and extracting them on the source side and by re-inserting their translation in the target language, making use of a separate entity repository; (3) News titles are translated with a separate translation system which is optimised for the speciﬁc style of news titles; (4) The system was optimised for speed in order to cope with the large volume of daily news articles. 
This paper deals with an application of automatic titling. The aim of such application is to attribute a title for a given text. So, our application relies on three very different automatic titling methods. The ﬁrst one extracts relevant noun phrases for their use as a heading, the second one automatically constructs headings by selecting words appearing in the text, and, ﬁnally, the third one uses nominalization in order to propose informative and catchy titles. Experiments based on 1048 titles have shown that our methods provide relevant titles. 
This paper presents Folheador, an online service for browsing through Portuguese semantic relations, acquired from different sources. Besides facilitating the exploration of Portuguese lexical knowledge bases, Folheador is connected to services that access Portuguese corpora, which provide authentic examples of the semantic relations in context. 
Current automatic speech transcription systems can achieve a high accuracy although they still make mistakes. In some scenarios, high quality transcriptions are needed and, therefore, fully automatic systems are not suitable for them. These high accuracy tasks require a human transcriber. However, we consider that automatic techniques could improve the transcriber’s efﬁciency. With this idea we present an interactive speech recognition system integrated with a word processor in order to assists users when transcribing speech. This system automatically recognizes speech while allowing the user to interactively modify the transcription. 
This paper presents the ﬁrst demonstration of a statistical spoken dialogue system that uses automatic belief compression to reason over complex user goal sets. Reasoning over the power set of possible user goals allows complex sets of user goals to be represented, which leads to more natural dialogues. The use of the power set results in a massive expansion in the number of belief states maintained by the Partially Observable Markov Decision Process (POMDP) spoken dialogue manager. A modiﬁed form of Value Directed Compression (VDC) is applied to the POMDP belief states producing a near-lossless compression which reduces the number of bases required to represent the belief distribution. 
The aim of our software presentation is to demonstrate that corpus-driven bilingual dictionaries generated fully by automatic means are suitable for human use. Previous experiments have proven that bilingual lexicons can be created by applying word alignment on parallel corpora. Such an approach, especially the corpus-driven nature of it, yields several advantages over more traditional approaches. Most importantly, automatically attained translation probabilities are able to guarantee that the most frequently used translations come ﬁrst within an entry. However, the proposed technique have to face some difﬁculties, as well. In particular, the scarce availability of parallel texts for medium density languages imposes limitations on the size of the resulting dictionary. Our objective is to design and implement a dictionary building workﬂow and a query system that is apt to exploit the additional beneﬁts of the method and overcome the disadvantages of it. 
Data-driven systems for natural language processing have the advantage that they can easily be ported to any language or domain for which appropriate training data can be found. However, many data-driven systems require careful tuning in order to achieve optimal performance, which may require specialized knowledge of the system. We present MaltOptimizer, a tool developed to facilitate optimization of parsers developed using MaltParser, a data-driven dependency parser generator. MaltOptimizer performs an analysis of the training data and guides the user through a three-phase optimization process, but it can also be used to perform completely automatic optimization. Experiments show that MaltOptimizer can improve parsing accuracy by up to 9 percent absolute (labeled attachment score) compared to default settings. During the demo session, we will run MaltOptimizer on different data sets (user-supplied if possible) and show how the user can interact with the system and track the improvement in parsing accuracy. 
Cognitive linguistics has reached a stage of maturity where many researchers are looking for an explicit formal grounding of their work. Unfortunately, most current models of deep language processing incorporate assumptions from generative grammar that are at odds with the cognitive movement in linguistics. This demonstration shows how Fluid Construction Grammar (FCG), a fully operational and bidirectional uniﬁcation-based grammar formalism, caters for this increasing demand. FCG features many of the tools that were pioneered in computational linguistics in the 70s-90s, but combines them in an innovative way. This demonstration highlights the main differences between FCG and related formalisms. 
This paper describes a system designed to support event detection over Twitter. The system operates by querying the data stream with a user-speciﬁed set of keywords, ﬁltering out non-English messages, and probabilistically geolocating each message. The user can dynamically set a probability threshold over the geolocation predictions, and also the time interval to present data for. 
Named Entity Extraction is a mature task in the NLP ﬁeld that has yielded numerous services gaining popularity in the Semantic Web community for extracting knowledge from web documents. These services are generally organized as pipelines, using dedicated APIs and different taxonomy for extracting, classifying and disambiguating named entities. Integrating one of these services in a particular application requires to implement an appropriate driver. Furthermore, the results of these services are not comparable due to different formats. This prevents the comparison of the performance of these services as well as their possible combination. We address this problem by proposing NERD, a framework which uniﬁes 10 popular named entity extractors available on the web, and the NERD ontology which provides a rich set of axioms aligning the taxonomies of these tools. 
This demo presents Information Extraction from discharge letters in Bulgarian language. The Patient history section is automatically split into episodes (clauses between two temporal markers); then drugs, diagnoses and conditions are recognised within the episodes with accuracy higher than 90%. The temporal markers, which refer to absolute or relative moments of time, are identiﬁed with precision 87% and recall 68%. The direction of time for the episode starting point: backwards or forward (with respect to certain moment orienting the episode) is recognised with precision 74.4%. 
We present a web tool that allows users to explore news stories concerning the 2012 US Presidential Elections via an interactive interface. The tool is based on concepts of “narrative analysis”, where the key actors of a narration are identiﬁed, along with their relations, in what are sometimes called “semantic triplets” (one example of a triplet of this kind is “Romney Criticised Obama”). The network of actors and their relations can be mined for insights about the structure of the narration, including the identiﬁcation of the key players, of the network of political support of each of them, a representation of the similarity of their political positions, and other information concerning their role in the media narration of events. The interactive interface allows the users to retrieve news report supporting the relations of interest. 
This article describes GALATEAS LangLog, a system performing Search Log Analysis. LangLog illustrates how NLP technologies can be a powerful support tool for market research even when the source of information is a collection of queries each one consisting of few words. We push the standard Search Log Analysis forward taking into account the semantics of the queries. The main innovation of LangLog is the implementation of two highly customizable components that cluster and classify the queries in the log. 
Data-driven approaches in computational semantics are not common because there are only few semantically annotated resources available. We are building a large corpus of public-domain English texts and annotate them semi-automatically with syntactic structures (derivations in Combinatory Categorial Grammar) and semantic representations (Discourse Representation Structures), including events, thematic roles, named entities, anaphora, scope, and rhetorical structure. We have created a wiki-like Web-based platform on which a crowd of expert annotators (i.e. linguists) can log in and adjust linguistic analyses in real time, at various levels of analysis, such as boundaries (tokens, sentences) and tags (part of speech, lexical categories). The demo will illustrate the different features of the platform, including navigation, visualization and editing. 
We propose a set of open-source software modules to perform structured Perceptron Training, Prediction and Evaluation within the Hadoop framework. Apache Hadoop is a freely available environment for running distributed applications on a computer cluster. The software is designed within the Map-Reduce paradigm. Thanks to distributed computing, the proposed software reduces substantially execution times while handling huge data-sets. The distributed Perceptron training algorithm preserves convergence properties, thus guaranties same accuracy performances as the serial Perceptron. The presented modules can be executed as stand-alone software or easily extended or integrated in complex systems. The execution of the modules applied to speciﬁc NLP tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the MapReduce jobs. 
 We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an opensource license from: http://brat.nlplab.org 
Much of what we know about speech perception comes from laboratory studies with clean, canonical speech, ideal listeners and artiﬁcial tasks. But how do interlocutors manage to communicate effectively in the seemingly less-than-ideal conditions of everyday listening, which frequently involve trying to make sense of speech while listening in a non-native language, or in the presence of competing sound sources, or while multitasking? In this talk I’ll examine the effect of real-world conditions on speech perception and quantify the contributions made by factors such as binaural hearing, visual information and prior knowledge to speech communication in noise. I’ll present a computational model which trades stimulus-related cues with information from learnt speech models, and examine how well it handles both energetic and informational masking in a two-sentence separation task. Speech communication also involves listening-while-talking. In the ﬁnal part of the talk I’ll describe some ways in which speakers might be making communication easier for their interlocutors, and demonstrate the application of these principles to improving the intelligibility of natural and synthetic speech in adverse conditions. 
We describe a novel method that extracts paraphrases from a bitext, for both the source and target languages. In order to reduce the search space, we decompose the phrase-table into sub-phrase-tables and construct separate clusters for source and target phrases. We convert the clusters into graphs, add smoothing/syntacticinformation-carrier vertices, and compute the similarity between phrases with a random walk-based measure, the commute time. The resulting phrase-paraphrase probabilities are built upon the conversion of the commute times into artiﬁcial cooccurrence counts with a novel technique. The co-occurrence count distribution belongs to the power-law family. 
We introduce two Bayesian models for unsupervised semantic role labeling (SRL) task. The models treat SRL as clustering of syntactic signatures of arguments with clusters corresponding to semantic roles. The ﬁrst model induces these clusterings independently for each predicate, exploiting the Chinese Restaurant Process (CRP) as a prior. In a more reﬁned hierarchical model, we inject the intuition that the clusterings are similar across different predicates, even though they are not necessarily identical. This intuition is encoded as a distance-dependent CRP with a distance between two syntactic signatures indicating how likely they are to correspond to a single semantic role. These distances are automatically induced within the model and shared across predicates. Both models achieve state-of-the-art results when evaluated on PropBank, with the coupled model consistently outperforming the factored counterpart in all experimental set-ups. 
We introduce two ways to detect entailment using distributional semantic representations of phrases. Our ﬁrst experiment shows that the entailment relation between adjective-noun constructions and their head nouns (big cat |= cat), once represented as semantic vector pairs, generalizes to lexical entailment among nouns (dog |= animal). Our second experiment shows that a classiﬁer fed semantic vector pairs can similarly generalize the entailment relation among quantiﬁer phrases (many dogs|=some dogs) to entailment involving unseen quantiﬁers (all cats|=several cats). Moreover, nominal and quantiﬁer phrase entailment appears to be cued by different distributional correlates, as predicted by the type-based view of entailment in formal semantics. 
A major focus of current work in distributional models of semantics is to construct phrase representations compositionally from word representations. However, the syntactic contexts which are modelled are usually severely limited, a fact which is reﬂected in the lexical-level WSD-like evaluation methods used. In this paper, we broaden the scope of these models to build sentence-level representations, and argue that phrase representations are best evaluated in terms of the inference decisions that they support, invariant to the particular syntactic constructions used to guide composition. We propose two evaluation methods in relation classiﬁcation and QA which reﬂect these goals, and apply several recent compositional distributional models to the tasks. We ﬁnd that the models outperform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference. 
A serious bottleneck of comparative parser evaluation is the fact that different parsers subscribe to different formal frameworks and theoretical assumptions. Converting outputs from one framework to another is less than optimal as it easily introduces noise into the process. Here we present a principled protocol for evaluating parsing results across frameworks based on function trees, tree generalization and edit distance metrics. This extends a previously proposed framework for cross-theory evaluation and allows us to compare a wider class of parsers. We demonstrate the usefulness and language independence of our procedure by evaluating constituency and dependency parsers on English and Swedish. 
Hungarian is a stereotype of morphologically rich and non-conﬁgurational languages. Here, we introduce results on dependency parsing of Hungarian that employ a 80K, multi-domain, fully manually annotated corpus, the Szeged Dependency Treebank. We show that the results achieved by state-of-the-art data-driven parsers on Hungarian and English (which is at the other end of the conﬁgurational-nonconﬁgurational spectrum) are quite similar to each other in terms of attachment scores. We reveal the reasons for this and present a systematic and comparative linguistically motivated error analysis on both languages. This analysis highlights that addressing the language-speciﬁc phenomena is required for a further remarkable error reduction. 
Transition-based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph conﬁguration is available. In this paper, we describe a model that takes into account complete structures as they become available to rescore the elements of a beam, combining the advantages of transition-based and graph-based approaches. We also propose an efﬁcient implementation that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores. 
In Information Retrieval (IR) in general and Question Answering (QA) in particular, queries and relevant textual content often signiﬁcantly differ in their properties and are therefore difﬁcult to relate with traditional IR methods, e.g. key-word matching. In this paper we describe an algorithm that addresses this problem, but rather than looking at it on a term matching/term reformulation level, we focus on the syntactic differences between questions and relevant text passages. To this end we propose a novel algorithm that analyzes dependency structures of queries and known relevant text passages and acquires transformational patterns that can be used to retrieve relevant textual content. We evaluate our algorithm in a QA setting, and show that it outperforms a baseline that uses only dependency information contained in the questions by 300% and that it also improves performance of a state of the art QA system signiﬁcantly. 
In this paper, we examined click patterns produced by users of Yahoo! search engine when prompting deﬁnition questions. Regularities across these click patterns are then utilized for constructing a large and heterogeneous training corpus for answer ranking. In a nutshell, answers are extracted from clicked web-snippets originating from any class of web-site, including Knowledge Bases (KBs). On the other hand, nonanswers are acquired from redundant pieces of text across web-snippets. The effectiveness of this corpus was assessed via training two state-of-the-art models, wherewith answers to unseen queries were distinguished. These testing queries were also submitted by search engine users, and their answer candidates were taken from their respective returned web-snippets. This corpus helped both techniques to ﬁnish with an accuracy higher than 70%, and to predict over 85% of the answers clicked by users. In particular, our results underline the importance of non-KB training data. 
This work proposes to adapt an existing general SMT model for the task of translating queries that are subsequently going to be used to retrieve information from a target language collection. In the scenario that we focus on access to the document collection itself is not available and changes to the IR model are not possible. We propose two ways to achieve the adaptation effect and both of them are aimed at tuning parameter weights on a set of parallel queries. The ﬁrst approach is via a standard tuning procedure optimizing for BLEU score and the second one is via a reranking approach optimizing for MAP score. We also extend the second approach by using syntax-based features. Our experiments show improvements of 1-2.5 in terms of MAP score over the retrieval with the non-adapted translation. We show that these improvements are due both to the integration of the adaptation and syntax-features for the query translation task. 
The search space of Phrase-Based Statistical Machine Translation (PBSMT) systems can be represented under the form of a directed acyclic graph (lattice). The quality of this search space can thus be evaluated by computing the best achievable hypothesis in the lattice, the so-called oracle hypothesis. For common SMT metrics, this problem is however NP-hard and can only be solved using heuristics. In this work, we present two new methods for efﬁciently computing BLEU oracles on lattices: the ﬁrst one is based on a linear approximation of the corpus BLEU score and is solved using the FST formalism; the second one relies on integer linear programming formulation and is solved directly and using the Lagrangian relaxation framework. These new decoders are positively evaluated and compared with several alternatives from the literature for three language pairs, using lattices produced by two PBSMT systems. 
We estimate the parameters of a phrasebased statistical machine translation system from monolingual corpora instead of a bilingual parallel corpus. We extend existing research on bilingual lexicon induction to estimate both lexical and phrasal translation probabilities for MT-scale phrasetables. We propose a novel algorithm to estimate reordering probabilities from monolingual data. We report translation results for an end-to-end translation system using these monolingual features alone. Our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights. In this paper, we examine an idealization where a phrase-table is given. We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone. We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features. 
In this paper we investigate the use of character-level translation models to support the translation from and to underresourced languages and textual domains via closely related pivot languages. Our experiments show that these low-level models can be successful even with tiny amounts of training data. We test the approach on movie subtitles for three language pairs and legal texts for another language pair in a domain adaptation task. Our pivot translations outperform the baselines by a large margin. 
Nowadays, there are large amounts of data available to train statistical machine translation systems. However, it is not clear whether all the training data actually help or not. A system trained on a subset of such huge bilingual corpora might outperform the use of all the bilingual data. This paper studies such issues by analysing two training data selection techniques: one based on approximating the probability of an indomain corpus; and another based on infrequent n-gram occurrence. Experimental results not only report signiﬁcant improvements over random sentence selection but also an improvement over a system trained with the whole available data. Surprisingly, the improvements are obtained with just a small fraction of the data that accounts for less than 0.5% of the sentences. Afterwards, we show that a much larger room for improvement exists, although this is done under non-realistic conditions. 
We consider the problem of NER in Arabic Wikipedia, a semisupervised domain adaptation setting for which we have no labeled training data in the target domain. To facilitate evaluation, we obtain annotations for articles in four topical groups, allowing annotators to identify domain-speciﬁc entity types in addition to standard categories. Standard supervised learning on newswire text leads to poor target-domain recall. We train a sequence model and show that a simple modiﬁcation to the online learner—a loss function encouraging it to “arrogantly” favor recall over precision— substantially improves recall and F1. We then adapt our model with self-training on unlabeled target-domain data; enforcing the same recall-oriented bias in the selftraining stage yields marginal gains.1 
In this paper we deal with Named Entity Recognition (NER) on transcriptions of French broadcast data. Two aspects make the task more difﬁcult with respect to previous NER tasks: i) named entities annotated used in this work have a tree structure, thus the task cannot be tackled as a sequence labelling task; ii) the data used are more noisy than data used for previous NER tasks. We approach the task in two steps, involving Conditional Random Fields and Probabilistic Context-Free Grammars, integrated in a single parsing algorithm. We analyse the effect of using several tree representations. Our system outperforms the best system of the evaluation campaign by a signiﬁcant margin. 
We present work on linking events and ﬂuents (i.e., relations that hold for certain periods of time) to temporal information in text, which is an important enabler for many applications such as timelines and reasoning. Previous research has mainly focused on temporal links for events, and we extend that work to include ﬂuents as well, presenting a common methodology for linking both events and relations to timestamps within the same sentence. Our approach combines tree kernels with classical feature-based learning to exploit context and achieves competitive F1-scores on event-time linking, and comparable F1scores for ﬂuents. Our best systems achieve F1-scores of 0.76 on events and 0.72 on ﬂuents. 
The well-studied supervised Relation Extraction algorithms require training data that is accurate and has good coverage. To obtain such a gold standard, the common practice is to do independent double annotation followed by adjudication. This takes significantly more human effort than annotation done by a single annotator. We do a detailed analysis on a snapshot of the ACE 2005 annotation files to understand the differences between single-pass annotation and the more expensive nearly three-pass process, and then propose an algorithm that learns from the much cheaper single-pass annotation and achieves a performance on a par with the extractor trained on multi-pass annotated data. Furthermore, we show that given the same amount of human labor, the better way to do relation annotation is not to annotate with high-cost quality assurance, but to annotate more. 1. Introduction Relation Extraction aims at detecting and categorizing semantic relations between pairs of entities in text. It is an important NLP task that has many practical applications such as answering factoid questions, building knowledge bases and improving web search. Supervised methods for relation extraction have been studied extensively since rich annotated linguistic resources, e.g. the Automatic Content Extraction1 (ACE) training corpus, were released. We will give a summary of related methods in section 2. Those methods rely on accurate and complete annotation. To obtain high quality annotation, the common wisdom is to let 
Topic models have great potential for helping users understand document corpora. This potential is stymied by their purely unsupervised nature, which often leads to topics that are neither entirely meaningful nor effective in extrinsic tasks (Chang et al., 2009). We propose a simple and effective way to guide topic models to learn topics of speciﬁc interest to a user. We achieve this by providing sets of seed words that a user believes are representative of the underlying topics in a corpus. Our model uses these seeds to improve both topicword distributions (by biasing topics to produce appropriate seed words) and to improve document-topic distributions (by biasing documents to select topics related to the seed words they contain). Extrinsic evaluation on a document clustering task reveals a signiﬁcant improvement when using seed information, even over other models that use seed information na¨ıvely. 
Update summarization is a new challenge in multi-document summarization focusing on summarizing a set of recent documents relatively to another set of earlier documents. We present an unsupervised probabilistic approach to model novelty in a document collection and apply it to the generation of update summaries. The new model, called DUALSUM, results in the second or third position in terms of the ROUGE metrics when tuned for previous TAC competitions and tested on TAC-2011, being statistically indistinguishable from the winning system. A manual evaluation of the generated summaries shows state-of-the art results for DUALSUM with respect to focus, coherence and overall responsiveness. 
In this paper, we present a supervised learning approach to training submodular scoring functions for extractive multidocument summarization. By taking a structured prediction approach, we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure. The learning method applies to all submodular summarization methods, and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets. Compared to state-of-theart functions that were tuned manually, our method signiﬁcantly improves performance and enables high-ﬁdelity models with number of parameters well beyond what could reasonably be tuned by hand. 
This paper presents an incremental probabilistic learner that models the acquistion of syntax and semantics from a corpus of child-directed utterances paired with possible representations of their meanings. These meaning representations approximate the contextual input available to the child; they do not specify the meanings of individual words or syntactic derivations. The learner then has to infer the meanings and syntactic properties of the words in the input along with a parsing model. We use the CCG grammatical framework and train a non-parametric Bayesian model of parse structure with online variational Bayesian expectation maximization. When tested on utterances from the CHILDES corpus, our learner outperforms a state-of-the-art semantic parser. In addition, it models such aspects of child acquisition as “fast mapping,” while also countering previous criticisms of statistical syntactic learners.  
Translation needs have greatly increased during the last years. In many situations, text to be translated constitutes an unbounded stream of data that grows continually with time. An effective approach to translate text documents is to follow an interactive-predictive paradigm in which both the system is guided by the user and the user is assisted by the system to generate error-free translations. Unfortunately, when processing such unbounded data streams even this approach requires an overwhelming amount of manpower. Is in this scenario where the use of active learning techniques is compelling. In this work, we propose different active learning techniques for interactive machine translation. Results show that for a given translation quality the use of active learning allows us to greatly reduce the human effort required to translate the sentences in the stream. 
Translation models used for statistical machine translation are compiled from parallel corpora; such corpora are manually translated, but the direction of translation is usually unknown, and is consequently ignored. However, much research in Translation Studies indicates that the direction of translation matters, as translated language (translationese) has many unique properties. Speciﬁcally, phrase tables constructed from parallel corpora translated in the same direction as the translation task perform better than ones constructed from corpora translated in the opposite direction. We reconﬁrm that this is indeed the case, but emphasize the importance of using also texts translated in the ‘wrong’ direction. We take advantage of information pertaining to the direction of translation in constructing phrase tables, by adapting the translation model to the special properties of translationese. We deﬁne entropybased measures that estimate the correspondence of target-language phrases to translationese, thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation. We show that incorporating these measures as features in the phrase tables of statistical machine translation systems results in consistent, statistically signiﬁcant improvement in the quality of the translation. 
In this paper we investigate the relevance of aspectual type for the problem of temporal information processing, i.e. the problems of the recent TempEval challenges. For a large list of verbs, we obtain several indicators about their lexical aspect by querying the web for expressions where these verbs occur in contexts associated with speciﬁc aspectual types. We then proceed to extend existing solutions for the problem of temporal information processing with the information extracted this way. The improved performance of the resulting models shows that (i) aspectual type can be data-mined with unsupervised methods with a level of noise that does not prevent this information from being useful and that (ii) temporal information processing can proﬁt from information about aspectual type. 
In this paper, we deﬁne a new type of summary for sentiment analysis: a singlesentence summary that consists of a supporting sentence that conveys the overall sentiment of a review as well as a convincing reason for this sentiment. We present a system for extracting supporting sentences from online product reviews, based on a simple and unsupervised method. We design a novel comparative evaluation method for summarization, using a crowdsourcing service. The evaluation shows that our sentence extraction method performs better than a baseline of taking the sentence with the strongest sentiment. 
Most event extraction systems are trained with supervised learning and rely on a collection of annotated documents. Due to the domain-speciﬁcity of this task, event extraction systems must be retrained with new annotated data for each domain. In this paper, we propose a bootstrapping solution for event role ﬁller extraction that requires minimal human supervision. We aim to rapidly train a state-of-the-art event extraction system using a small set of “seed nouns” for each event role, a collection of relevant (in-domain) and irrelevant (outof-domain) texts, and a semantic dictionary. The experimental results show that the bootstrapped system outperforms previous weakly supervised event extraction systems on the MUC-4 data set, and achieves performance levels comparable to supervised training with 700 manually annotated documents. 
In this paper, we describe a new approach to semi-supervised adaptive learning of event extraction from text. Given a set of examples and an un-annotated text corpus, the BEAR system (Bootstrapping Events And Relations) will automatically learn how to recognize and understand descriptions of complex semantic relationships in text, such as events involving multiple entities and their roles. For example, given a series of descriptions of bombing and shooting incidents (e.g., in newswire) the system will learn to extract, with a high degree of accuracy, other attack-type events mentioned elsewhere in text, irrespective of the form of description. A series of evaluations using the ACE data and event set show a significant performance improvement over our baseline system. 
Existing concept-color-emotion lexicons limit themselves to small sets of basic emotions and colors, which cannot capture the rich pallet of color terms that humans use in communication. In this paper we begin to address this problem by building a novel, color-emotion-concept association lexicon via crowdsourcing. This lexicon, which we call CLEX, has over 2,300 color terms, over 3,000 affect terms and almost 2,000 concepts. We investigate the relation between color and concept, and color and emotion, reinforcing results from previous studies, as well as discovering new associations. We also investigate cross-cultural differences in color-emotion associations between US and India-based annotators. 
We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more ﬁne-grained coherence preferences in training data. We associate multiple ranks with the set of permutations originating from the same source document, as opposed to the original pairwise rankings. We also study the eﬀect of the permutations used in training, and the eﬀect of the coreference component used in entity extraction. With no additional manual annotations required, our extended model is able to outperform the original model on two tasks: sentence ordering and summary coherence rating. 
In this paper, we compare three different generalization methods for in-domain and cross-domain opinion holder extraction being simple unsupervised word clustering, an induction method inspired by distant supervision and the usage of lexical resources. The generalization methods are incorporated into diverse classiﬁers. We show that generalization causes signiﬁcant improvements and that the impact of improvement depends on the type of classiﬁer and on how much training and test data differ from each other. We also address the less common case of opinion holders being realized in patient position and suggest approaches including a novel (linguisticallyinformed) extraction method how to detect those opinion holders without labeled training data as standard datasets contain too few instances of this type. 
In this paper, we extend current state-of-theart research on unsupervised acquisition of scripts, that is, stereotypical and frequently observed sequences of events. We design, evaluate and compare different methods for constructing models for script event prediction: given a partial chain of events in a script, predict other events that are likely to belong to the script. Our work aims to answer key questions about how best to (1) identify representative event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, deﬁning a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models. 
It is becoming clear that traditional evaluation measures used in Computational Linguistics (including Error Rates, Accuracy, Recall, Precision and F-measure) are of limited value for unbiased evaluation of systems, and are not meaningful for comparison of algorithms unless both the dataset and algorithm parameters are strictly controlled for skew (Prevalence and Bias). The use of techniques originally designed for other purposes, in particular Receiver Operating Characteristics Area Under Curve, plus variants of Kappa, have been proposed to fill the void. This paper aims to clear up some of the confusion relating to evaluation, by demonstrating that the usefulness of each evaluation method is highly dependent on the assumptions made about the distributions of the dataset and the underlying populations. The behaviour of a number of evaluation measures is compared under common assumptions. Deploying a system in a context which has the opposite skew from its validation set can be expected to approximately negate Fleiss Kappa and halve Cohen Kappa but leave Powers Kappa unchanged. For most performance evaluation purposes, the latter is thus most appropriate, whilst for comparison of behaviour, Matthews Correlation is recommended.  Introduction Research in Computational Linguistics usually requires some form of quantitative evaluation. A number of traditional measures borrowed from Information Retrieval (Manning & Schütze, 1999) are in common use but there has been considerable critical evaluation of these measures themselves over the last decade or so (Entwisle & Powers, 1998, Flach, 2003, Ben-David. 2008). Receiver Operating Analysis (ROC) has been advocated as an alternative by many, and in particular has been used by Fürnkranz and Flach (2005), Ben-David (2008) and Powers (2008) to better understand both learning algorithms relationship and the between the various measures, and the inherent biases that make many of them suspect. One of the key advantages of ROC is that it provides a clear indication of chance level performance as well as a less well known indication of the relative cost weighting of positive and negative cases for each possible system or parameterization represented. ROC Area Under the Curve (Fig. 1) has been also used as a performance measure but averages over the false positive rate (Fallout) and is thus a function of cost that is dependent on the classifier rather than the application. For this reason it has come into considerable criticism and a number of variants and alternatives have been proposed (e.g. AUK, Kaymak et. Al, 2010 and H-measure, Hand, 2009). An AUC curve that is at least as good as a second curve at all points, is said to dominate it and indicates that the first classifier is equal or better than the second for all plotted values of the parameters, and all cost ratios. However AUC being greater for one classifier than another does not have such a property – indeed deconvexities within or  
Document revision histories are a useful and abundant source of data for natural language processing, but selecting relevant data for the task at hand is not trivial. In this paper we introduce a scalable approach for automatically distinguishing between factual and ﬂuency edits in document revision histories. The approach is based on supervised machine learning using language model probabilities, string similarity measured over different representations of user edits, comparison of part-of-speech tags and named entities, and a set of adaptive features extracted from large amounts of unlabeled user edits. Applied to contiguous edit segments, our method achieves statistically signiﬁcant improvements over a simple yet effective edit-distance baseline. It reaches high classiﬁcation accuracy (88%) and is shown to generalize to additional sets of unseen data. 
Online community is an important source for latest news and information. Accurate prediction of a user’s interest can help provide better user experience. In this paper, we develop a recommendation system for online forums. There are a lot of differences between online forums and formal media. For example, content generated by users in online forums contains more noise compared to formal documents. Content topics in the same forum are more focused than sources like news websites. Some of these differences present challenges to traditional word-based user proﬁling and recommendation systems, but some also provide opportunities for better recommendation performance. In our recommendation system, we propose to (a) use latent topics to interpolate with content-based recommendation; (b) model latent user groups to utilize information from other users. We have collected three types of forum data sets. Our experimental results demonstrate that our proposed hybrid approach works well in all three types of forums. 
We present the PONG method to compute selectional preferences using part-of-speech (POS) N-grams. From a corpus labeled with grammatical dependencies, PONG learns the distribution of word relations for each POS N-gram. From the much larger but unlabeled Google N-grams corpus, PONG learns the distribution of POS N-grams for a given pair of words. We derive the probability that one word has a given grammatical relation to the other. PONG estimates this probability by combining both distributions, whether or not either word occurs in the labeled corpus. PONG achieves higher average precision on 16 relations than a state-of-the-art baseline in a pseudo-disambiguation task, but lower coverage and recall. 
This paper describes an automatic method for creating a domain-independent senseannotated corpus harvested from the web. As a proof of concept, this method has been applied to German, a language for which sense-annotated corpora are still in short supply. The sense inventory is taken from the German wordnet GermaNet. The web-harvesting relies on an existing mapping of GermaNet to the German version of the web-based dictionary Wiktionary. The data obtained by this method constitute WebCAGe (short for: Web-Harvested Corpus Annotated with GermaNet Senses), a resource which currently represents the largest sense-annotated corpus available for German. While the present paper focuses on one particular language, the method as such is language-independent.  Thus far, sense-annotated corpora have typically been constructed manually, making the creation of such resources expensive and the compilation of larger data sets difﬁcult, if not completely infeasible. It is therefore timely and appropriate to explore alternatives to manual annotation and to investigate automatic means of creating sense-annotated corpora. Ideally, any automatic method should satisfy the following criteria: (1) The method used should be language independent and should be applicable to as many languages as possible for which the necessary input resources are available. (2) The quality of the automatically generated data should be extremely high so as to be usable as is or with minimal amount of manual post-correction.  
Probabilistic accounts of language processing can be psychologically tested by comparing word-reading times (RT) to the conditional word probabilities estimated by language models. Using surprisal as a linking function, a signiﬁcant correlation between unlexicalized surprisal and RT has been reported (e.g., Demberg and Keller, 2008), but success using lexicalized models has been limited. In this study, phrase structure grammars and recurrent neural networks estimated both lexicalized and unlexicalized surprisal for words of independent sentences from narrative sources. These same sentences were used as stimuli in a self-paced reading experiment to obtain RTs. The results show that lexicalized surprisal according to both models is a significant predictor of RT, outperforming its unlexicalized counterparts. 
In this paper we study spectral learning methods for non-deterministic split headautomata grammars, a powerful hiddenstate formalism for dependency parsing. We present a learning algorithm that, like other spectral methods, is efﬁcient and nonsusceptible to local minima. We show how this algorithm can be formulated as a technique for inducing hidden structure from distributions computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 
Kernel based methods dominate the current trend for various relation extraction tasks including protein-protein interaction (PPI) extraction. PPI information is critical in understanding biological processes. Despite considerable efforts, previously reported PPI extraction results show that none of the approaches already known in the literature is consistently better than other approaches when evaluated on different benchmark PPI corpora. In this paper, we propose a novel hybrid kernel that combines (automatically collected) dependency patterns, trigger words, negative cues, walk features and regular expression patterns along with tree kernel and shallow linguistic kernel. The proposed kernel outperforms the exiting state-of-the-art approaches on the BioInfer corpus, the largest PPI benchmark corpus available. On the other four smaller benchmark corpora, it performs either better or almost as good as the existing approaches. Moreover, empirical results show that the proposed hybrid kernel attains considerably higher precision than the existing approaches, which indicates its capability of learning more accurate models. This also demonstrates that the different types of information that we use are able to complement each other for relation extraction. 
Coordination disambiguation remains a difﬁcult sub-problem in parsing despite the frequency and importance of coordination structures. We propose a method for disambiguating coordination structures. In this method, dual decomposition is used as a framework to take advantage of both HPSG parsing and coordinate structure analysis with alignment-based local features. We evaluate the performance of the proposed method on the Genia corpus and the Wall Street Journal portion of the Penn Treebank. Results show it increases the percentage of sentences in which coordination structures are detected correctly, compared with each of the two algorithms alone. 
In this paper, we address statistical machine translation of public conference talks. Modeling the style of this genre can be very challenging given the shortage of available in-domain training data. We investigate the use of a hybrid LM, where infrequent words are mapped into classes. Hybrid LMs are used to complement word-based LMs with statistics about the language style of the talks. Extensive experiments comparing different settings of the hybrid LM are reported on publicly available benchmarks based on TED talks, from Arabic to English and from English to French. The proposed models show to better exploit in-domain data than conventional word-based LMs for the target language modeling component of a phrase-based statistical machine translation system. 
In this paper, we extend the work on using latent cross-language topic models for identifying word translations across comparable corpora. We present a novel precisionoriented algorithm that relies on per-topic word distributions obtained by the bilingual LDA (BiLDA) latent topic model. The algorithm aims at harvesting only the most probable word translations across languages in a greedy fashion, without any prior knowledge about the language pair, relying on a symmetrization process and the one-to-one constraint. We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a signiﬁcant margin. In addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations. 
 SBARQ  Previous work on treebank parsing with discontinuous constituents using Linear Context-Free Rewriting systems (LCFRS) has been limited to sentences of up to 30 words, for reasons of computational complexity. There have been some results on binarizing an LCFRS in a manner that minimizes parsing complexity, but the present work shows that parsing long sentences with such an optimally binarized grammar remains infeasible. Instead, we introduce a technique which removes this length restriction, while maintaining a respectable accuracy. The resulting parser has been applied to a discontinuous treebank with favorable results. 
It is not always clear how the differences in intrinsic evaluation metrics for a parser or classiﬁer will affect the performance of the system that uses it. We investigate the relationship between the intrinsic evaluation scores of an interpretation component in a tutorial dialogue system and the learning outcomes in an experiment with human users. Following the PARADISE methodology, we use multiple linear regression to build predictive models of learning gain, an important objective outcome metric in tutorial dialogue. We show that standard intrinsic metrics such as F-score alone do not predict the outcomes well. However, we can build predictive performance functions that account for up to 50% of the variance in learning gain by combining features based on standard evaluation scores and on the confusion matrix entries. We argue that building such predictive models can help us better evaluate performance of NLP components that cannot be distinguished based on F-score alone, and illustrate our approach by comparing the current interpretation component in the system to a new classiﬁer trained on the evaluation data. 
We describe a set of experiments using automatically labelled data to train supervised classiﬁers for multi-class emotion detection in Twitter messages with no manual intervention. By cross-validating between models trained on different labellings for the same six basic emotion classes, and testing on manually labelled data, we conclude that the method is suitable for some emotions (happiness, sadness and anger) but less able to distinguish others; and that different labelling conventions are more suitable for some emotions than others. 
We present experiments with part-ofspeech tagging for Bulgarian, a Slavic language with rich inﬂectional and derivational morphology. Unlike most previous work, which has used a small number of grammatical categories, we work with 680 morpho-syntactic tags. We combine a large morphological lexicon with prior linguistic knowledge and guided learning from a POS-annotated corpus, achieving accuracy of 97.98%, which is a signiﬁcant improvement over the state-of-the-art for Bulgarian. 
Whether automatically extracted or human generated, open-domain factual knowledge is often available in the form of semantic annotations (e.g., composed-by) that take one or more speciﬁc instances (e.g., rhapsody in blue, george gershwin) as their arguments. This paper introduces a method for converting ﬂat sets of instance-level annotations into hierarchically organized, concept-level annotations, which capture not only the broad semantics of the desired arguments (e.g., ‘People’ rather than ‘Locations’), but also the correct level of generality (e.g., ‘Composers’ rather than ‘People’, or ‘Jazz Composers’). The method refrains from encoding features speciﬁc to a particular domain or annotation, to ensure immediate applicability to new, previously unseen annotations. Over a gold standard of semantic annotations and concepts that best capture their arguments, the method substantially outperforms three baselines, on average, computing concepts that are less than one step in the hierarchy away from the corresponding gold standard concepts. 
We present a model of semantic processing of spoken language that (a) is robust against ill-formed input, such as can be expected from automatic speech recognisers, (b) respects both syntactic and pragmatic constraints in the computation of most likely interpretations, (c) uses a principled, expressive semantic representation formalism (RMRS) with a well-deﬁned model theory, and (d) works continuously (producing meaning representations on a wordby-word basis, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far). We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component (around 10 % absolute, over a syntax-only baseline). 
In this paper we extend our work described in (Dinu et al., 2011) by adding more conjugational rules to the labelling system introduced there, in an attempt to capture the entire dataset of Romanian verbs extracted from (Barbu, 2007), and we employ machine learning techniques to predict a verb’s correct label (which says what conjugational pattern it follows) when only the inﬁnitive form is given. 
We evaluate measures of contextual ﬁtness on the task of detecting real-word spelling errors. For that purpose, we extract naturally occurring errors and their contexts from the Wikipedia revision history. We show that such natural errors are better suited for evaluation than the previously used artiﬁcially created errors. In particular, the precision of statistical methods has been largely over-estimated, while the precision of knowledge-based approaches has been under-estimated. Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations. Finally, we show that statistical and knowledgebased methods can be combined for increased performance. 
We investigate the problem of domain adaptation for parallel data in Statistical Machine Translation (SMT). While techniques for domain adaptation of monolingual data can be borrowed for parallel data, we explore conceptual differences between translation model and language model domain adaptation and their effect on performance, such as the fact that translation models typically consist of several features that have different characteristics and can be optimized separately. We also explore adapting multiple (4–10) data sets with no a priori distinction between in-domain and out-of-domain data except for an in-domain development set. 
This paper describes Subcat-LMF, an ISOLMF compliant lexicon representation format featuring a uniform representation of subcategorization frames (SCFs) for the two languages English and German. Subcat-LMF is able to represent SCFs at a very ﬁne-grained level. We utilized SubcatLMF to standardize lexicons with largescale SCF information: the English VerbNet and two German lexicons, i.e., a subset of IMSlex and GermaNet verbs. To evaluate our LMF-model, we performed a crosslingual comparison of SCF coverage and overlap for the standardized versions of the English and German lexicons. The SubcatLMF DTD, the conversion tools and the standardized versions of VerbNet and IMSlex subset are publicly available.1 
Text prediction is the task of suggesting text while the user is typing. Its main aim is to reduce the number of keystrokes that are needed to type a text. In this paper, we address the inﬂuence of text type and domain differences on text prediction quality. By training and testing our text prediction algorithm on four different text types (Wikipedia, Twitter, transcriptions of conversational speech and FAQ) with equal corpus sizes, we found that there is a clear effect of text type on text prediction quality: training and testing on the same text type gave percentages of saved keystrokes between 27 and 34%; training on a different text type caused the scores to drop to percentages between 16 and 28%. In our case study, we compared a number of training corpora for a speciﬁc data set for which training data is sparse: questions about neurological issues. We found that both text type and topic domain play a role in text prediction quality. The best performing training corpus was a set of medical pages from Wikipedia. The second-best result was obtained by leaveone-out experiments on the test questions, even though this training corpus was much smaller (2,672 words) than the other corpora (1.5 Million words). 
The search in patent databases is a risky business compared to the search in other domains. A single document that is relevant but overlooked during a patent search can turn into an expensive proposition. While recent research engages in specialized models and algorithms to improve the effectiveness of patent retrieval, we bring another aspect into focus: the detection and exploitation of patent inconsistencies. In particular, we analyze spelling errors in the assignee ﬁeld of patents granted by the United States Patent & Trademark Ofﬁce. We introduce technology in order to improve retrieval effectiveness despite the presence of typographical ambiguities. In this regard, we (1) quantify spelling errors in terms of edit distance and phonological dissimilarity and (2) render error detection as a learning problem that combines word dissimilarities with patent meta-features. For the task of ﬁnding all patents of a company, our approach improves recall from 96.7% (when using a state-of-the-art patent search engine) to 99.5%, while precision is compromised by only 3.7%. 
We present UBY, a large-scale lexicalsemantic resource combining a wide range of information from expert-constructed and collaboratively constructed resources for English and German. It currently contains nine resources in two languages: English WordNet, Wiktionary, Wikipedia, FrameNet and VerbNet, German Wikipedia, Wiktionary and GermaNet, and multilingual OmegaWiki modeled according to the LMF standard. For FrameNet, VerbNet and all collaboratively constructed resources, this is done for the ﬁrst time. Our LMF model captures lexical information at a ﬁne-grained level by employing a large number of Data Categories from ISOCat and is designed to be directly extensible by new languages and resources. All resources in UBY can be accessed with an easy to use publicly available API. 
We apply topic modelling to automatically induce word senses of a target word, and demonstrate that our word sense induction method can be used to automatically detect words with emergent novel senses, as well as token occurrences of those senses. We start by exploring the utility of standard topic models for word sense induction (WSI), with a pre-determined number of topics (=senses). We next demonstrate that a non-parametric formulation that learns an appropriate number of senses per word actually performs better at the WSI task. We go on to establish state-of-the-art results over two WSI datasets, and apply the proposed model to a novel sense detection task. 
Machine learning has become the dominant approach to building natural-language processing systems. However, current approaches generally require a great deal of laboriously constructed humanannotated training data. Ideally, a computer would be able to acquire language like a child by being exposed to linguistic input in the context of a relevant but ambiguous perceptual environment. As a step in this direction, we have developed systems that learn to sportscast simulated robot soccer games and to follow navigation instructions in virtual environments by simply observing sample human linguistic behavior in context. This work builds on our earlier work on supervised learning of semantic parsers that map natural language into a formal meaning representation. In order to apply such methods to learning from observation, we have developed methods that estimate the meaning of sentences given just their ambiguous perceptual context. 602 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, page 602, Avignon, France, April 23 - 27 2012. c 2012 Association for Computational Linguistics 
Microblogging websites such as Twitter offer a wealth of insight into a population’s current mood. Automated approaches to identify general sentiment toward a particular topic often perform two steps: Topic Identiﬁcation and Sentiment Analysis. Topic Identiﬁcation ﬁrst identiﬁes tweets that are relevant to a desired topic (e.g., a politician or event), and Sentiment Analysis extracts each tweet’s attitude toward the topic. Many techniques for Topic Identiﬁcation simply involve selecting tweets using a keyword search. Here, we present an approach that instead uses distant supervision to train a classiﬁer on the tweets returned by the search. We show that distant supervision leads to improved performance in the Topic Identiﬁcation task as well in the downstream Sentiment Analysis stage. We then use a system that incorporates distant supervision into both stages to analyze the sentiment toward President Obama expressed in a dataset of tweets. Our results better correlate with Gallup’s Presidential Job Approval polls than previous work. Finally, we discover a surprising baseline that outperforms previous work without a Topic Identiﬁcation stage. 
Open issue trackers are a type of social media that has received relatively little attention from the text-mining community. We investigate the problems inherent in learning to triage bug reports from time-varying data. We demonstrate that concept drift is an important consideration. We show the effectiveness of online learning algorithms by evaluating them on several bug report datasets collected from open issue trackers associated with large open-source projects. We make this collection of data publicly available. 
Informal and formal (“T/V”) address in dialogue is not distinguished overtly in modern English, e.g. by pronoun choice like in many other languages such as French (“tu”/“vous”). Our study investigates the status of the T/V distinction in English literary texts. Our main ﬁndings are: (a) human raters can label monolingual English utterances as T or V fairly well, given sufﬁcient context; (b), a bilingual corpus can be exploited to induce a supervised classiﬁer for T/V without human annotation. It assigns T/V at sentence level with up to 68% accuracy, relying mainly on lexical features; (c), there is a marked asymmetry between lexical features for formal speech (which are conventionalized and therefore general) and informal speech (which are text-speciﬁc). 
Better representations of plot structure could greatly improve computational methods for summarizing and generating stories. Current representations lack abstraction, focusing too closely on events. We present a kernel for comparing novelistic plots at a higher level, in terms of the cast of characters they depict and the social relationships between them. Our kernel compares the characters of different novels to one another by measuring their frequency of occurrence over time and the descriptive and emotional language associated with them. Given a corpus of 19thcentury novels as training data, our method can accurately distinguish held-out novels in their original form from artiﬁcially disordered or reversed surrogates, demonstrating its ability to robustly represent important aspects of plot structure. 
Morphological lexica are often implemented on top of morphological paradigms, corresponding to different ways of building the full inﬂection table of a word. Computationally precise lexica may use hundreds of paradigms, and it can be hard for a lexicographer to choose among them. To automate this task, this paper introduces the notion of a smart paradigm. It is a metaparadigm, which inspects the base form and tries to infer which low-level paradigm applies. If the result is uncertain, more forms are given for discrimination. The number of forms needed in average is a measure of predictability of an inﬂection system. The overall complexity of the system also has to take into account the code size of the paradigms deﬁnition itself. This paper evaluates the smart paradigms implemented in the open-source GF Resource Grammar Library. Predictability and complexity are estimated for four different languages: English, French, Swedish, and Finnish. The main result is that predictability does not decrease when the complexity of morphology grows, which means that smart paradigms provide an efﬁcient tool for the manual construction and/or automatically bootstrapping of lexica.  aiming for more precision, has 235 paradigms for Swedish. Mathematically, a paradigm is a function that produces inﬂection tables. Its argument is a word string (either a dictionary form or a stem), and its value is an n-tuple of strings (the word forms): P : String → Stringn We assume that the exponent n is determined by the language and the part of speech. For instance, English verbs might have n = 5 (for sing, sings, sang, sung, singing), whereas for French verbs in Bescherelle, n = 51. We assume the tuples to be ordered, so that for instance the French second person singular present subjunctive is always found at position 17. In this way, word-paradigm pairs can be easily converted to morphogical lexica and to transducers that map form descriptions to surface forms and back. A properly designed set of paradigms permits a compact representation of a lexicon and a user-friendly way to extend it. Different paradigm systems may have different numbers of paradigms. There are two reasons for this. One is that traditional paradigms often in fact require more arguments than one: P : Stringm → Stringn  
We propose a novel method for learning morphological paradigms that are structured within a hierarchy. The hierarchical structuring of paradigms groups morphologically similar words close to each other in a tree structure. This allows detecting morphological similarities easily leading to improved morphological segmentation. Our evaluation using (Kurimo et al., 2011a; Kurimo et al., 2011b) dataset shows that our method performs competitively when compared with current state-ofart systems. 
The current state-of-the-art in statistical machine translation (SMT) suffers from issues of sparsity and inadequate modeling power when translating into morphologically rich languages. We model both inﬂection and word-formation for the task of translating into German. We translate from English words to an underspeciﬁed German representation and then use linearchain CRFs to predict the fully speciﬁed German representation. We show that improved modeling of inﬂection and wordformation leads to improved SMT. 
Arabic morphology is complex, partly because of its richness, and partly because of common irregular word forms, such as broken plurals (which resemble singular nouns), and nouns with irregular gender (feminine nouns that look masculine and vice versa). In addition, Arabic morphosyntactic agreement interacts with the lexical semantic feature of rationality, which has no morphological realization. In this paper, we present a series of experiments on the automatic prediction of the latent linguistic features of functional gender and number, and rationality in Arabic. We compare two techniques, using simple maximum likelihood (MLE) with back-off and a support vector machine based sequence tagger (Yamcha). We study a number of orthographic, morphological and syntactic learning features. Our results show that the MLE technique is preferred for words seen in the training data, while the Yamcha technique is optimal for unseen words, which are our real target. Furthermore, we show that for unseen words, morphological features help beyond orthographic features and that syntactic features help even more. A combination of the two techniques improves overall performance even further. 
Widely accepted resources for semantic parsing, such as PropBank and FrameNet, are not perfect as a semantic role labeling framework. Their semantic roles are not strictly deﬁned; therefore, their meanings and semantic characteristics are unclear. In addition, it is presupposed that a single semantic role is assigned to each syntactic argument. This is not necessarily true when we consider internal structures of verb semantics. We propose a new framework for semantic role annotation which solves these problems by extending the theory of lexical conceptual structure (LCS). By comparing our framework with that of existing resources, including VerbNet and FrameNet, we demonstrate that our extended LCS framework can give a formal deﬁnition of semantic role labels, and that multiple roles of arguments can be represented strictly and naturally. 
We propose an unsupervised, iterative method for detecting downward-entailing operators (DEOs), which are important for deducing entailment relations between sentences. Like the distillation algorithm of Danescu-Niculescu-Mizil et al. (2009), the initialization of our method depends on the correlation between DEOs and negative polarity items (NPIs). However, our method trusts the initialization more and aggressively separates likely DEOs from spurious distractors and other words, unlike distillation, which we show to be equivalent to one iteration of EM prior re-estimation. Our method is also amenable to a bootstrapping method that co-learns DEOs and NPIs, and achieves the best results in identifying DEOs in two corpora. 
In pro-drop languages, the detection of explicit subjects, zero subjects and nonreferential impersonal constructions is crucial for anaphora and co-reference resolution. While the identiﬁcation of explicit and zero subjects has attracted the attention of researchers in the past, the automatic identiﬁcation of impersonal constructions in Spanish has not been addressed yet and this work is the ﬁrst such study. In this paper we present a corpus to underpin research on the automatic detection of these linguistic phenomena in Spanish and a novel machine learning-based methodology for their computational treatment. This study also provides an analysis of the features, discusses performance across two different genres and offers error analysis. The evaluation results show that our system performs better in detecting explicit subjects than alternative systems. 
The task of paraphrase acquisition from related sentences can be tackled by a variety of techniques making use of various types of knowledge. In this work, we make the hypothesis that their performance can be increased if candidate paraphrases can be validated using information that characterizes paraphrases independently of the set of techniques that proposed them. We implement this as a bi-class classiﬁcation problem (i.e. paraphrase vs. not paraphrase), allowing any paraphrase acquisition technique to be easily integrated into the combination system. We report experiments on two languages, English and French, with 5 individual techniques on parallel monolingual parallel corpora obtained via multiple translation, and a large set of classiﬁcation features including surface to contextual similarity measures. Relative improvements in F-measure close to 18% are obtained on both languages over the best performing techniques. 
When translating English to German, existing reordering models often cannot model the long-range reorderings needed to generate German translations with verbs in the correct position. We reorder English as a preprocessing step for English-to-German SMT. We use a sequence of hand-crafted reordering rules applied to English parse trees. The reordering rules place English verbal elements in the positions within the clause they will have in the German translation. This is a difﬁcult problem, as German verbal elements can appear in different positions within a clause (in contrast with English verbal elements, whose positions do not vary as much). We obtain a signiﬁcant improvement in translation performance. 
A fundamental problem in text generation is word ordering. Word ordering is a computationally difﬁcult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al., 2009; Zhang and Clark, 2011). By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task. One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve ﬂuency. We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying online large-margin training. Our system signiﬁcantly improved on the baseline by 3.7 BLEU points. 
This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator ﬁlters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date. 
We present a system for the real-time generation of car navigation instructions with landmarks. Our system relies exclusively on freely available map data from OpenStreetMap, organizes its output to ﬁt into the available time until the next driving maneuver, and reacts in real time to driving errors. We show that female users spend signiﬁcantly less time looking away from the road when using our system compared to a baseline system. 
We compare the impact of sentenceinternal vs. sentence-external features on word order prediction in two generation settings: starting out from a discriminative surface realisation ranking model for an LFG grammar of German, we enrich the feature set with lexical chain features from the discourse context which can be robustly detected and reﬂect rough grammatical correlates of notions from theoretical approaches to discourse coherence. In a more controlled setting, we develop a constituent ordering classiﬁer that is trained on a German treebank with gold coreference annotation. Surprisingly, in both settings, the sentence-external features perform poorly compared to the sentenceinternal ones, and do not improve over a baseline model capturing the syntactic functions of the constituents. 
In this paper, we propose an annotation schema for the discourse analysis of Wikipedia Talk pages aimed at the coordination efforts for article improvement. We apply the annotation schema to a corpus of 100 Talk pages from the Simple English Wikipedia and make the resulting dataset freely available for download1. Furthermore, we perform automatic dialog act classiﬁcation on Wikipedia discussions and achieve an average F1-score of 0.82 with our classiﬁcation pipeline. 
Speech style accommodation refers to shifts in style that are used to achieve strategic goals within interactions. Models of stylistic shift that focus on speciﬁc features are limited in terms of the contexts to which they can be applied if the goal of the analysis is to model socially motivated speech style accommodation. In this paper, we present an unsupervised Dynamic Bayesian Model that allows us to model stylistic style accommodation in a way that is agnostic to which speciﬁc speech style features will shift in a way that resembles socially motivated stylistic variation. This greatly expands the applicability of the model across contexts. Our hypothesis is that stylistic shifts that occur as a result of social processes are likely to display some consistency over time, and if we leverage this insight in our model,we will achieve a model that better captures inherent structure within speech. 
While information status (IS) plays a crucial role in discourse processing, there have only been a handful of attempts to automatically determine the IS of discourse entities. We examine a related but more challenging task, ﬁne-grained IS determination, which involves classifying a discourse entity as one of 16 IS subtypes. We investigate the use of rich knowledge sources for this task in combination with a rule-based approach and a learning-based approach. In experiments with a set of Switchboard dialogues, the learning-based approach achieves an accuracy of 78.7%, outperforming the rulebased approach by 21.3%. 
A composition procedure for linear and nondeleting extended top-down tree transducers is presented. It is demonstrated that the new procedure is more widely applicable than the existing methods. In general, the result of the composition is an extended top-down tree transducer that is no longer linear or nondeleting, but in a number of cases these properties can easily be recovered by a post-processing step. 
Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents. In this paper we analyze patents along the orthogonal dimensions of topic and textual structure. We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the inﬂuence of such tasks on machine translation performance. We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning. We ﬁnd small but signiﬁcant gains over task-speciﬁc training by techniques that model commonalities through shared parameters. A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs. 
 2 The Problem of German Case  German case syncretism is often assumed to be the accidental by-product of historical development. This paper contradicts this claim and argues that the evolution of German case is driven by the need to optimize the cognitive effort and memory required for processing and interpretation. This hypothesis is supported by a novel kind of computational experiments that reconstruct and compare attested variations of the German deﬁnite article paradigm. The experiments show how the intricate interaction between those variations and the rest of the German ‘linguistic landscape’ may direct language change. 
Low interannotator agreement (IAA) is a well-known issue in manual semantic tagging (sense tagging). IAA correlates with the granularity of word senses and they both correlate with the amount of information they give as well as with its reliability. We compare different approaches to semantic tagging in WordNet, FrameNet, PropBank and OntoNotes with a small tagged data sample based on the Corpus Pattern Analysis to present the reliable information gain (RG), a measure used to optimize the semantic granularity of a sense inventory with respect to its reliability indicated by the IAA in the given data set. RG can also be used as feedback for lexicographers, and as a supporting component of automatic semantic classiﬁers, especially when dealing with a very ﬁne-grained set of semantic categories. 
 We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language. In contrast to past work that used bitexts to transfer analyses of speciﬁc sentences at the token level, we instead use features to transfer the behavior of words at a type level. In a discriminative dependency parsing framework, our approach produces gains across a range of target languages, using two different lowresource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed). 
In this paper, we address the problem of building a multilingual transliteration system using an interlingual representation. Our approach uses international phonetic alphabet (IPA) to learn the interlingual representation and thus allows us to use any word and its IPA representation as a training example. Thus, our approach requires only monolingual resources: a phoneme dictionary that lists words and their IPA representations.1 By adding a phoneme dictionary of a new language, we can readily build a transliteration system into any of the existing previous languages, without the expense of all-pairs data or computation. We also propose a regularization framework for learning the interlingual representation, which accounts for language speciﬁc phonemic variability, and thus it can ﬁnd better mappings between languages. Experimental results on the name transliteration task in ﬁve diverse languages show a maximum improvement of 29% accuracy and an average improvement of 17% accuracy compared to a state-of-the-art baseline system. 
This paper proposes a novel method for lexicon extraction that extracts translation pairs from comparable corpora by using graphbased label propagation. In previous work, it was established that performance drastically decreases when the coverage of a seed lexicon is small. We resolve this problem by utilizing indirect relations with the bilingual seeds together with direct relations, in which each word is represented by a distribution of translated seeds. The seed distributions are propagated over a graph representing relations among words, and translation pairs are extracted by identifying word pairs with a high similarity in the seed distributions. We propose two types of the graphs: a co-occurrence graph, representing co-occurrence relations between words, and a similarity graph, representing context similarities between words. Evaluations using English and Japanese patent comparable corpora show that our proposed graph propagation method outperforms conventional methods. Further, the similarity graph achieved improved performance by clustering synonyms into the same translation. 
We present a system for automatic identiﬁcation of schizophrenic patients and healthy controls based on narratives the subjects recounted about emotional experiences in their own life. The focus of the study is to identify the lexical features that distinguish the two populations. We report the results of feature selection experiments that demonstrate that the classiﬁer can achieve accuracy on patient level prediction as high as 76.9% with only a small set of features. We provide an in-depth discussion of the lexical features that distinguish the two groups and the unexpected relationship between emotion types of the narratives and the accuracy of patient status prediction. 
Inferring attributes of discourse participants has been treated as a batch-processing task: data such as all tweets from a given author are gathered in bulk, processed, analyzed for a particular feature, then reported as a result of academic interest. Given the sources and scale of material used in these efforts, along with potential use cases of such analytic tools, discourse analysis should be reconsidered as a streaming challenge. We show that under certain common formulations, the batchprocessing analytic framework can be decomposed into a sequential series of updates, using as an example the task of gender classiﬁcation. Once in a streaming framework, and motivated by large data sets generated by social media services, we present novel results in approximate counting, showing its applicability to space efﬁcient streaming classiﬁcation. 
A mixture of positive (friendly) and negative (antagonistic) relations exist among users in most social media applications. However, many such applications do not allow users to explicitly express the polarity of their interactions. As a result most research has either ignored negative links or was limited to the few domains where such relations are explicitly expressed (e.g. Epinions trust/distrust). We study text exchanged between users in online communities. We ﬁnd that the polarity of the links between users can be predicted with high accuracy given the text they exchange. This allows us to build a signed network representation of discussions; where every edge has a sign: positive to denote a friendly relation, or negative to denote an antagonistic relation. We also connect our analysis to social psychology theories of balance. We show that the automatically predicted networks are consistent with those theories. Inspired by that, we present a technique for identifying subgroups in discussions by partitioning singed networks representing them. 
User simulation is frequently used to train statistical dialog managers for task-oriented domains. At present, goal-driven simulators (those that have a persistent notion of what they wish to achieve in the dialog) require some task-speciﬁc engineering, making them impossible to evaluate intrinsically. Instead, they have been evaluated extrinsically by means of the dialog managers they are intended to train, leading to circularity of argument. In this paper, we propose the ﬁrst fully generative goal-driven simulator that is fully induced from data, without hand-crafting or goal annotation. Our goals are latent, and take the form of topics in a topic model, clustering together semantically equivalent and phonetically confusable strings, implicitly modelling synonymy and speech recognition noise. We evaluate on two standard dialog resources, the Communicator and Let’s Go datasets, and demonstrate that our model has substantially better ﬁt to held out data than competing approaches. We also show that features derived from our model allow signiﬁcantly greater improvement over a baseline at distinguishing real from randomly permuted dialogs. 
Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less ﬂexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23%. 
Recent work has explored the use of hidden Markov models for unsupervised discourse and conversation modeling, where each segment or block of text such as a message in a conversation is associated with a hidden state in a sequence. We extend this approach to allow each block of text to be a mixture of multiple classes. Under our model, the probability of a class in a text block is a log-linear function of the classes in the previous block. We show that this model performs well at predictive tasks on two conversation data sets, improving thread reconstruction accuracy by up to 15 percentage points over a standard HMM. Additionally, we show quantitatively that the induced word clusters correspond to speech acts more closely than baseline models. 
Entity Linking (EL) has received considerable attention in recent years. Given many name mentions in a document, the goal of EL is to predict their referent entities in a knowledge base. Traditionally, there have been two distinct directions of EL research: one focusing on the effects of mention’s context compatibility, assuming that “the referent entity of a mention is reflected by its context”; the other dealing with the effects of document’s topic coherence, assuming that “a mention’s referent entity should be coherent with the document’s main topics”. In this paper, we propose a generative model – called entitytopic model, to effectively join the above two complementary directions together. By jointly modeling and exploiting the context compatibility, the topic coherence and the correlation between them, our model can accurately link all mentions in a document using both the local information (including the words and the mentions in a document) and the global knowledge (including the topic knowledge, the entity context knowledge and the entity name knowledge). Experimental results demonstrate the effectiveness of the proposed model. 
Existing techniques for disambiguating named entities in text mostly focus on Wikipedia as a target catalog of entities. Yet for many types of entities, such as restaurants and cult movies, relational databases exist that contain far more extensive information than Wikipedia. This paper introduces a new task, called Open-Database Named-Entity Disambiguation (Open-DB NED), in which a system must be able to resolve named entities to symbols in an arbitrary database, without requiring labeled data for each new database. We introduce two techniques for Open-DB NED, one based on distant supervision and the other based on domain adaptation. In experiments on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25% in accuracy. 
Generic rule-based systems for Information Extraction (IE) have been shown to work reasonably well out-of-the-box, and achieve state-of-the-art accuracy with further domain customization. However, it is generally recognized that manually building and customizing rules is a complex and labor intensive process. In this paper, we discuss an approach that facilitates the process of building customizable rules for Named-Entity Recognition (NER) tasks via rule induction, in the Annotation Query Language (AQL). Given a set of basic features and an annotated document collection, our goal is to generate an initial set of rules with reasonable accuracy, that are interpretable and thus can be easily reﬁned by a human developer. We present an efﬁcient rule induction process, modeled on a fourstage manual rule development process and present initial promising results with our system. We also propose a simple notion of extractor complexity as a ﬁrst step to quantify the interpretability of an extractor, and study the effect of induction bias and customization of basic features on the accuracy and complexity of induced rules. We demonstrate through experiments that the induced rules have good accuracy and low complexity according to our complexity measure. 
Active learning is a promising way for sentiment classification to reduce the annotation cost. In this paper, we focus on the imbalanced class distribution scenario for sentiment classification, wherein the number of positive samples is quite different from that of negative samples. This scenario posits new challenges to active learning. To address these challenges, we propose a novel active learning approach, named co-selecting, by taking both the imbalanced class distribution issue and uncertainty into account. Specifically, our co-selecting approach employs two feature subspace classifiers to collectively select most informative minority-class samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 
We propose the weakly supervised MultiExperts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews. In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such ﬁne-grained analysis helps to understand better why users like or dislike the entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of ﬁne-grained training data. We integrate these noisy indicators into a uniﬁed probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning. Our experiments indicate that MEM outperforms state-of-the-art methods by a signiﬁcant margin. 
This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, startup⟩), in which the sentiment orientation of the polarity word (“long”) changes along with different targets (“battery life” or “startup”). To disambiguate a collocation’s polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufﬁcient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective. 
Generating coherent discourse is an important aspect in natural language generation. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level. We present an important subtask for this overall goal, in which we align predicates across comparable texts, admitting partial argument structure correspondence. The contribution of this work is two-fold: We ﬁrst construct a large corpus resource of comparable texts, including an evaluation set with manual predicate alignments. Secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts. Our method signiﬁcantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1-score. 
Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classiﬁcation framework. We empirically demonstrate that our model signiﬁcantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task. 
Recently there has been substantial interest in using spectral methods to learn generative sequence models like HMMs. Spectral methods are attractive as they provide globally consistent estimates of the model parameters and are very fast and scalable, unlike EM methods, which can get stuck in local minima. In this paper, we present a novel extension of this class of spectral methods to learn dependency tree structures. We propose a simple yet powerful latent variable generative model for dependency parsing, and a spectral learning method to efﬁciently estimate it. As a pilot experimental evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-theart parser. Our approach gives us a moderate reduction in error of up to 4.6% over the baseline re-ranker. 
Topic models traditionally rely on the bagof-words assumption. In data mining applications, this often results in end-users being presented with inscrutable lists of topical unigrams, single words inferred as representative of their topics. In this article, we present a hierarchical generative probabilistic model of topical phrases. The model simultaneously infers the location, length, and topic of phrases within a corpus and relaxes the bagof-words assumption within phrases by using a hierarchy of Pitman-Yor processes. We use Markov chain Monte Carlo techniques for approximate inference in the model and perform slice sampling to learn its hyperparameters. We show via an experiment on human subjects that our model ﬁnds substantially better, more interpretable topical phrases than do competing models. 
We describe a nonparametric model and corresponding inference algorithm for learning Synchronous Context Free Grammar derivations for parallel text. The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules. Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences— including discontiguous, many-to-many alignments—and produces competitive translation results. Further, inference is efﬁcient and we present results on signiﬁcantly larger corpora than prior work. 
Multi-document summarization involves many aspects of content selection and surface realization. The summaries must be informative, succinct, grammatical, and obey stylistic writing conventions. We present a method where such individual aspects are learned separately from data (without any hand-engineering) but optimized jointly using an integer linear programme. The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints. Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and signiﬁcantly improves the informativeness of the summaries. 
Comprehension and corpus studies have found that the tendency to minimize dependency length has a strong inﬂuence on constituent ordering choices. In this paper, we investigate dependency length minimization in the context of discriminative realization ranking, focusing on its potential to eliminate egregious ordering errors as well as better match the distributional characteristics of sentence orderings in news text. We ﬁnd that with a stateof-the-art, comprehensive realization ranking model, dependency length minimization yields statistically signiﬁcant improvements in BLEU scores and signiﬁcantly reduces the number of heavy/light ordering errors. Through distributional analyses, we also show that with simpler ranking models, dependency length minimization can go overboard, too often sacriﬁcing canonical word order to shorten dependencies, while richer models manage to better counterbalance the dependency length minimization preference against (sometimes) competing canonical word order preferences. 
We present a new approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL) in this paper, which models the process of constructing a summary within the framework of reinforcement learning and attempts to optimize the given score function with the given feature representation of a summary. We demonstrate that the method of reinforcement learning can be adapted to automatic summarization problems naturally and simply, and other summarizing techniques, such as sentence compression, can be easily adapted as actions of the framework. The experimental results indicated ASRL was superior to the best performing method in DUC2004 and comparable to the state of the art ILP-style method, in terms of ROUGE scores. The results also revealed ASRL can search for sub-optimal solutions efﬁciently under conditions for effectively selecting features and the score function. 
We apply slice sampling to Bayesian decipherment and use our new decipherment framework to improve out-of-domain machine translation. Compared with the state of the art algorithm, our approach is highly scalable and produces better results, which allows us to decipher ciphertext with billions of tokens and hundreds of thousands of word types with high accuracy. We decipher a large amount of monolingual data to improve out-of-domain translation and achieve signiﬁcant gains of up to 3.8 BLEU points. 
Tense is a small element to a sentence, however, error tense can raise odd grammars and result in misunderstanding. Recently, tense has drawn attention in many natural language processing applications. However, most of current Statistical Machine Translation (SMT) systems mainly depend on translation model and language model. They never consider and make full use of tense information. In this paper, we propose n-gram-based tense models for SMT and successfully integrate them into a state-of-the-art phrase-based SMT system via two additional features. Experimental results on the NIST Chinese-English translation task show that our proposed tense models are very effective, contributing performance improvement by 0.62 BLUE points over a strong baseline. 
We propose a novel, language-independent approach for improving machine translation from a resource-poor language to X by adapting a large bi-text for a related resource-rich language and X (the same target language). We assume a small bi-text for the resourcepoor language to X pair, which we use to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the resource-rich and the resource-poor language; we then adapt the former to get closer to the latter. Our experiments for Indonesian/Malay–English translation show that using the large adapted resource-rich bitext yields 6.7 BLEU points of improvement over the unadapted one and 2.6 BLEU points over the original small bi-text. Moreover, combining the small bi-text with the adapted bi-text outperforms the corresponding combinations with the unadapted bi-text by 1.5– 3 BLEU points. We also demonstrate applicability to other languages and domains. 
The possibility of deleting a word from a sentence without violating its syntactic correctness belongs to traditionally known manifestations of syntactic dependency. We introduce a novel unsupervised parsing approach that is based on a new n-gram reducibility measure. We perform experiments across 18 languages available in CoNLL data and we show that our approach achieves better accuracy for the majority of the languages then previously reported results. 
State-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difﬁcult to generalize. On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant. In this paper, we attempt to address this imbalance for graph-based parsing by generalizing the Eisner (1996) algorithm to handle arbitrary features over higherorder dependencies. The generalization is at the cost of asymptotic efﬁciency. To account for this, cube pruning for decoding is utilized (Chiang, 2007). For the ﬁrst time, label tuple and structural features such as valencies can be scored efﬁciently with third-order features in a graph-based parser. Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al. (2010). 
We consider the problem of inducing grapheme-to-phoneme mappings for unknown languages written in a Latin alphabet. First, we collect a data-set of 107 languages with known grapheme-phoneme relationships, along with a short text in each language. We then cast our task in the framework of supervised learning, where each known language serves as a training example, and predictions are made on unknown languages. We induce an undirected graphical model that learns phonotactic regularities, thus relating textual patterns to plausible phonemic interpretations across the entire range of languages. Our model correctly predicts grapheme-phoneme pairs with over 88% F1-measure. 
Many linguistic and textual processes involve transduction of strings. We show how to learn a stochastic transducer from an unorganized collection of strings (rather than string pairs). The role of the transducer is to organize the collection. Our generative model explains similarities among the strings by supposing that some strings in the collection were not generated ab initio, but were instead derived by transduction from other, “similar” strings in the collection. Our variational EM learning algorithm alternately reestimates this phylogeny and the transducer parameters. The ﬁnal learned transducer can quickly link any test name into the ﬁnal phylogeny, thereby locating variants of the test name. We ﬁnd that our method can eﬀectively ﬁnd name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 
We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density. We provide the ﬁrst evidence for an association between syntactic surprisal and word duration in recorded speech. Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities. This result supports the uniform information density (UID) hypothesis and points a way to more realistic artiﬁcial speech generation. 
The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the question translation and the resulting query answering. 
 This paper proposes to generate appropriate answers for opinion questions about products by exploiting the hierarchical organization of consumer reviews. The hierarchy organizes product aspects as nodes following their parent-child relations. For each aspect, the reviews and corresponding opinions on this aspect are stored. We develop a new framework for opinion Questions Answering, which enables accurate question analysis and effective answer generation by making use the hierarchy. In particular, we ﬁrst identify the (explicit/implicit) product aspects asked in the questions and their sub-aspects by referring to the hierarchy. We then retrieve the corresponding review fragments relevant to the aspects from the hierarchy. In order to generate appropriate answers from the review fragments, we develop a multi-criteria optimization approach for answer generation by simultaneously taking into account review salience, coherence, diversity, and parent-child relations among the aspects. We conduct evaluations on 11 popular products in four domains. The evaluated corpus contains 70,359 consumer reviews and 220 questions on these products. Experimental results demonstrate the effectiveness of our approach. 
 In statistical machine translation, minimum error rate training (MERT) is a standard method for tuning a single weight with regard to a given development data. However, due to the diversity and uneven distribution of source sentences, there are two problems suffered by this method. First, its performance is highly dependent on the choice of a development set, which may lead to an unstable performance for testing. Second, translations become inconsistent at the sentence level since tuning is performed globally on a document level. In this paper, we propose a novel local training method to address these two problems. Unlike a global training method, such as MERT, in which a single weight is learned and used for all the input sentences, we perform training and testing in one step by learning a sentencewise weight for each input sentence. We propose efﬁcient incremental training methods to put the local training into practice. In NIST Chinese-to-English translation tasks, our local training method signiﬁcantly outperforms MERT with the maximal improvements up to 2.0 BLEU points, meanwhile its efﬁciency is comparable to that of the global method. 
In this paper we ﬁrst describe the technology of automatic annotation transformation, which is based on the annotation adaptation algorithm (Jiang et al., 2009). It can automatically transform a human-annotated corpus from one annotation guideline to another. We then propose two optimization strategies, iterative training and predict-self reestimation, to further improve the accuracy of annotation guideline transformation. Experiments on Chinese word segmentation show that, the iterative training strategy together with predictself reestimation brings signiﬁcant improvement over the simple annotation transformation baseline, and leads to classiﬁers with signiﬁcantly higher accuracy and several times faster processing than annotation adaptation does. On the Penn Chinese Treebank 5.0, it achieves an F-measure of 98.43%, significantly outperforms previous works although using a single classiﬁer with only local features. 
Microblog normalisation methods often utilise complex models and struggle to differentiate between correctly-spelled unknown words and lexical variants of known words. In this paper, we propose a method for constructing a dictionary of lexical variants of known words that facilitates lexical normalisation via simple string substitution (e.g. tomorrow for tmrw). We use context information to generate possible variant and normalisation pairs and then rank these by string similarity. Highlyranked pairs are selected to populate the dictionary. We show that a dictionary-based approach achieves state-of-the-art performance for both F-score and word error rate on a standard dataset. Compared with other methods, this approach offers a fast, lightweight and easy-to-use solution, and is thus suitable for high-volume microblog pre-processing. 
“Grounded” language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts. Bo¨rschinger et al. (2011) introduced an approach to grounded language learning based on unsupervised PCFG induction. Their approach works well when each sentence potentially refers to one of a small set of possible meanings, such as in the sportscasting task. However, it does not scale to problems with a large set of potential meanings for each sentence, such as the navigation instruction following task studied by Chen and Mooney (2011). This paper presents an enhancement of the PCFG approach that scales to such problems with highly-ambiguous supervision. Experimental results on the navigation task demonstrates the effectiveness of our approach. 
A forced derivation tree (FDT) of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. In this paper, we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation (SMT) systems. We ﬁrst describe how to generate different FDTs for each sentence pair in training corpus, and then present how to infer the optimal FDTs based on their derivation and alignment qualities. As the ﬁrst step in this line of research, we verify the effectiveness of our approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and signiﬁcant improvements are reported on both translation quality and alignment quality. 
Distant supervision for relation extraction (RE) – gathering training data by aligning a database of facts with text – is an efﬁcient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difﬁcult domains. 
This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the speciﬁcities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), signiﬁcantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a ﬁrst language. 
We introduce gap inheritance, a new structural property on trees, which provides a way to quantify the degree to which intervals of descendants can be nested. Based on this property, two new classes of trees are derived that provide a closer approximation to the set of plausible natural language dependency trees than some alternative classes of trees: unlike projective trees, a word can have descendants in more than one interval; unlike spanning trees, these intervals cannot be nested in arbitrary ways. The 1-Inherit class of trees has exactly the same empirical coverage of natural language sentences as the class of mildly nonprojective trees, yet the optimal scoring tree can be found in an order of magnitude less time. Gap-minding trees (the second class) have the property that all edges into an interval of descendants come from the same node, and thus an algorithm which uses only single intervals can produce trees in which a node has descendants in multiple intervals. 
We introduce a novel coreference resolution system that models entities and events jointly. Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations. As clusters are built, information ﬂows between entity and event clusters through features that model semantic role dependencies. Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa. In a cross-document domain with comparable documents, joint coreference resolution performs signiﬁcantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately. 
In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing. Previous work often used a pipeline method – Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components. In our approach, we train the three individual models separately during training, and incorporate them together in a uniﬁed framework during decoding. We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features. As far as we know, this is the ﬁrst work on joint Chinese word segmentation, POS tagging and parsing. Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system. 
In this paper, we propose a novel translation model (TM) based cross-lingual data selection model for language model (LM) adaptation in statistical machine translation (SMT), from word models to phrase models. Given a source sentence in the translation task, this model directly estimates the probability that a sentence in the target LM training corpus is similar. Compared with the traditional approaches which utilize the ﬁrst pass translation hypotheses, cross-lingual data selection model avoids the problem of noisy proliferation. Furthermore, phrase TM based cross-lingual data selection model is more effective than the traditional approaches based on bag-ofwords models and word-based TM, because it captures contextual information in modeling the selection of phrase as a whole. Experiments conducted on large-scale data sets demonstrate that our approach signiﬁcantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance. 
Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-speciﬁed vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, stateof-the-art Open IE systems such as REVERB and WOE share two important weaknesses – (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents OLLIE, a substantially improved Open IE system that addresses both these limitations. First, OLLIE achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. OLLIE obtains 2.7 times the area under precision-yield curve (AUC) compared to REVERB and 1.9 times the AUC of WOEparse. 
Topic models are increasingly being used for text analysis tasks, often times replacing earlier semantic techniques such as latent semantic analysis. In this paper, we develop a novel adaptive topic model with the ability to adapt topics from both the previous segment and the parent document. For this proposed model, a Gibbs sampler is developed for doing posterior inference. Experimental results show that with topic adaptation, our model signiﬁcantly improves over existing approaches in terms of perplexity, and is able to uncover clear sequential structure on, for example, Herman Melville’s book “Moby Dick”. 
In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We ﬁnd that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the ﬁt between the meaning representation and compositional method. 
Most existing systems solved the phrase chunking task with the sequence labeling approaches, in which the chunk candidates cannot be treated as a whole during parsing process so that the chunk-level features cannot be exploited in a natural way. In this paper, we formulate phrase chunking as a joint segmentation and labeling task. We propose an efficient dynamic programming algorithm with pruning for decoding, which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks. A relaxed, online maximum margin training algorithm is used for learning. Within this framework, we explored a variety of effective feature representations for Chinese phrase chunking. The experimental results show that the use of chunk-level features can lead to significant performance improvement, and that our approach achieves state-of-the-art performance. In particular, our approach is much better at recognizing long and complicated phrases. 
Evidence-based medicine is an approach whereby clinical decisions are supported by the best available ﬁndings gained from scientiﬁc research. This requires efﬁcient access to such evidence. To this end, abstracts in evidence-based medicine can be labeled using a set of predeﬁned medical categories, the socalled PICO criteria. This paper presents an approach to automatically annotate sentences in medical abstracts with these labels. Since both structural and sequential information are important for this classiﬁcation task, we use kLog, a new language for statistical relational learning with kernels. Our results show a clear improvement with respect to state-of-the-art systems. 
In this paper, we explore the classiﬁcation of emotions in songs, using the music and the lyrics representation of the songs. We introduce a novel corpus of music and lyrics, consisting of 100 songs annotated for emotions. We show that textual and musical features can both be successfully used for emotion recognition in songs. Moreover, through comparative experiments, we show that the joint use of lyrics and music brings signiﬁcant improvements over each of the individual textual and musical classiﬁers, with error rate reductions of up to 31%. 
This paper presents an integrated, end-to-end approach to online spelling correction for text input. Online spelling correction refers to the spelling correction as you type, as opposed to post-editing. The online scenario is particularly important for languages that routinely use transliteration-based text input methods, such as Chinese and Japanese, because the desired target characters cannot be input at all unless they are in the list of candidates provided by an input method, and spelling errors prevent them from appearing in the list. For example, a user might type suesheng by mistake to mean xuesheng 学生 'student' in Chinese; existing input methods fail to convert this misspelled input to the desired target Chinese characters. In this paper, we propose a unified approach to the problem of spelling correction and transliteration-based character conversion using an approach inspired by the phrasebased statistical machine translation framework. At the phrase (substring) level, k most probable pinyin (Romanized Chinese) corrections are generated using a monotone decoder; at the sentence level, input pinyin strings are directly transliterated into target Chinese characters by a decoder using a loglinear model that refer to the features of both levels. A new method of automatically deriving parallel training data from user keystroke logs is also presented. Experiments on Chinese pinyin conversion show that our integrated method reduces the character error rate by 20% (from 8.9% to 7.12%) over the previous state-of-the art based on a noisy channel model.  
We propose a new semantic orientation, Excitation, and its automatic acquisition method. Excitation is a semantic property of predicates that classiﬁes them into excitatory, inhibitory and neutral. We show that Excitation is useful for extracting contradiction pairs (e.g., destroy cancer ⊥ develop cancer) and causality pairs (e.g., increase in crime ⇒ heighten anxiety). Our experiments show that with automatically acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus. Furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus with reasonable precision. 
This paper presents a paraphrase acquisition method that uncovers and exploits generalities underlying paraphrases: paraphrase patterns are ﬁrst induced and then used to collect novel instances. Unlike existing methods, ours uses both bilingual parallel and monolingual corpora. While the former are regarded as a source of high-quality seed paraphrases, the latter are searched for paraphrases that match patterns learned from the seed paraphrases. We show how one can use monolingual corpora, which are far more numerous and larger than bilingual corpora, to obtain paraphrases that rival in quality those derived directly from bilingual corpora. In our experiments, the number of paraphrase pairs obtained in this way from monolingual corpora was a large multiple of the number of seed paraphrases. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrase pairs are of reasonable quality. Remaining noise can be further reduced by ﬁltering seed paraphrases. 
Learning the meaning of words from ambiguous and noisy context is a challenging task for language learners. It has been suggested that children draw on syntactic cues such as lexical categories of words to constrain potential referents of words in a complex scene. Although the acquisition of lexical categories should be interleaved with learning word meanings, it has not previously been modeled in that fashion. In this paper, we investigate the interplay of word learning and category induction by integrating an LDA-based word class learning module with a probabilistic word learning model. Our results show that the incrementally induced word classes signiﬁcantly improve word learning, and their contribution is comparable to that of manually assigned part of speech categories. 
This paper presents a comparative study of graph-based approaches for cross-domain sentiment classiﬁcation. In particular, the paper analyses two existing methods: an optimisation problem and a ranking algorithm. We compare these graph-based methods with each other and with the other state-ofthe-art approaches and conclude that graph domain representations offer a competitive solution to the domain adaptation problem. Analysis of the best parameters for graphbased algorithms reveals that there are no optimal values valid for all domain pairs and that these values are dependent on the characteristics of corresponding domains. 
This paper explores log-based query expansion (QE) models for Web search. Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries. These models are trained on pairs of user queries and titles of clicked documents. Evaluations on a real world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods. 
This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classiﬁers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model signiﬁcantly outperformed the local classiﬁers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system. 
We present a new family of models for unsupervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation. We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries — such as English determiners — resemble constituent edges. (ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences. (iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges. Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers. 
The task of inferring the native language of an author based on texts written in a second language has generally been tackled as a classiﬁcation problem, typically using as features a mix of n-grams over characters and part of speech tags (for small and ﬁxed n) and unigram function words. To capture arbitrarily long n-grams that syntax-based approaches have suggested are useful, adaptor grammars have some promise. In this work we investigate their extension to identifying n-gram collocations of arbitrary length over a mix of PoS tags and words, using both maxent and induced syntactic language model approaches to classiﬁcation. After presenting a new, simple baseline, we show that learned collocations used as features in a maxent model perform better still, but that the story is more mixed for the syntactic language model. 
We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections. As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information. We are particularly interested in revealing and exploiting relationships between documents. To this end, we focus on extracting diverse sets of threads—singlylinked, coherent chains of important documents. To illustrate, we extract research threads from citation graphs and construct timelines from news articles. Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model. Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges. 
This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efﬁcient system combination exploiting a large set of features for paraphrase recognition. A detailed quantiﬁed typology of subsentential paraphrases found in our corpus types is given. 
Graph-based dependency parsers suffer from the sheer number of higher order edges they need to (a) score and (b) consider during optimization. Here we show that when working with LP relaxations, large fractions of these edges can be pruned before they are fully scored—without any loss of optimality guarantees and, hence, accuracy. This is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best ﬁrst order edges. This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution. For second order grandparent models, our method considers, or scores, no more than 6–13% of the second order edges of the full model. This yields up to an eightfold parsing speedup, while providing the same empirical accuracy and certiﬁcates of optimality as working with the full LP relaxation. We also provide a tighter LP formulation for grandparent models that leads to a smaller integrality gap and higher speed. 
We propose an adaptive ensemble method to adapt coreference resolution across domains. This method has three features: (1) it can optimize for any user-speciﬁed objective measure; (2) it can make document-speciﬁc prediction rather than rely on a ﬁxed base model or a ﬁxed set of base models; (3) it can automatically adjust the active ensemble members during prediction. With simpliﬁcation, this method can be used in the traditional withindomain case, while still retaining the above features. To the best of our knowledge, this work is the ﬁrst to both (i) develop a domain adaptation algorithm for the coreference resolution problem and (ii) have the above features as an ensemble method. Empirically, we show the beneﬁts of (i) on the six domains of the ACE 2005 data set in domain adaptation setting, and of (ii) on both the MUC-6 and the ACE 2005 data sets in within-domain setting. 
We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences. We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation. This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments. We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase. On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form. 
This paper proposes cross-lingual language modeling for transcribing source resourcepoor languages and translating them into target resource-rich languages if necessary. Our focus is to improve the speech recognition performance of low-resource languages by leveraging the language model statistics from resource-rich languages. The most challenging work of cross-lingual language modeling is to solve the syntactic discrepancies between the source and target languages. We therefore propose syntactic reordering for cross-lingual language modeling, and present a ﬁrst result that compares inversion transduction grammar (ITG) reordering constraints to IBM and local constraints in an integrated speech transcription and translation system. Evaluations on resource-poor Cantonese speech transcription and Cantonese to resource-rich Mandarin translation tasks show that our proposed approach improves the system performance signiﬁcantly, up to 3.4% relative WER reduction in Cantonese transcription and 13.3% relative bilingual evaluation understudy (BLEU) score improvement in Mandarin transcription compared with the system without reordering. 
We examine the task of resolving complex cases of deﬁnite pronouns, speciﬁcally those for which traditional linguistic constraints on coreference (e.g., Binding Constraints, gender and number agreement) as well as commonly-used resolution heuristics (e.g., string-matching facilities, syntactic salience) are not useful. Being able to solve this task has broader implications in artiﬁcial intelligence: a restricted version of it, sometimes referred to as the Winograd Schema Challenge, has been suggested as a conceptually and practically appealing alternative to the Turing Test. We employ a knowledge-rich approach to this task, which yields a pronoun resolver that outperforms state-of-the-art resolvers by nearly 18 points in accuracy on our dataset. 
Quote extraction and attribution is the task of automatically extracting quotes from text and attributing each quote to its correct speaker. The present state-of-the-art system uses gold standard information from previous decisions in its features, which, when removed, results in a large drop in performance. We treat the problem as a sequence labelling task, which allows us to incorporate sequence features without using gold standard information. We present results on two new corpora and an augmented version of a third, achieving a new state-of-the-art for systems using only realistic features. 
Supervised hierarchical topic modeling and unsupervised hierarchical topic modeling are usually used to obtain hierarchical topics, such as hLLDA and hLDA. Supervised hierarchical topic modeling makes heavy use of the information from observed hierarchical labels, but cannot explore new topics; while unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space, but does not make use of any information from hierarchical labels. In this paper, we propose a semi-supervised hierarchical topic model which aims to explore new topics automatically in the data space while incorporating the information from observed hierarchical labels into the modeling process, called SemiSupervised Hierarchical Latent Dirichlet Allocation (SSHLDA). We also prove that hLDA and hLLDA are special cases of SSHLDA. We conduct experiments on Yahoo! Answers and ODP datasets, and assess the performance in terms of perplexity and clustering. The experimental results show that predictive ability of SSHLDA is better than that of baselines, and SSHLDA can also achieve signiﬁcant improvement over baselines for clustering on the FScore measure. 
Many NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable. For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task. We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task. Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random ﬁeld. At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem. Results show that this approach provides signiﬁcant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling. 
Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets. Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries. Taking the MINGREEDY algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics. We also deﬁne a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data. Altogether, our augmentations produce improvements to performance over the original MIN-GREEDY algorithm for both English and Italian data. 
In this paper, we investigate different usages of feature representations in the web person name disambiguation task which has been suffering from the mismatch of vocabulary and lack of clues in web environments. In literature, the latter receives less attention and remains more challenging. We explore the feature space in this task and argue that collecting person speciﬁc evidences from a corpus level can provide a more reasonable and robust estimation for evaluating a feature’s importance in a given web page. This can alleviate the lack of clues where discriminative features can be reasonably weighted by taking their corpus level importance into account, not just relying on the current local context. We therefore propose a topic-based model to exploit the person speciﬁc global importance and embed it into the person name similarity. The experimental results show that the corpus level topic information provides more stable evidences for discriminative features and our method outperforms the state-of-the-art systems on three WePS datasets. 
This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text. This is done by treating the parser’s derivation tree as a latent variable in a model that is trained to maximize reordering accuracy. We demonstrate that eﬃcient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree. Using this model in the pre-ordering framework results in signiﬁcant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 
The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser. In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules. In this paper, we propose two ways of re-training monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices. One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data. We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees. The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks. 
We describe a transformation-based learning method for learning a sequence of monolingual tree transformations that improve the agreement between constituent trees and word alignments in bilingual corpora. Using the manually annotated English Chinese Translation Treebank, we show how our method automatically discovers transformations that accommodate differences in English and Chinese syntax. Furthermore, when transformations are learned on automatically generated trees and alignments from the same domain as the training data for a syntactic MT system, the transformed trees achieve a 0.9 BLEU improvement over baseline trees. 
 We present a distantly supervised system for extracting the temporal bounds of ﬂuents (relations which only hold during certain times, such as attends school). Unlike previous pipelined approaches, our model does not assume independence between each ﬂuent or even between named entities with known connections (parent, spouse, employer, etc.). Instead, we model what makes timelines of ﬂuents consistent by learning cross-ﬂuent constraints, potentially spanning entities as well. For example, our model learns that someone is unlikely to start a job at age two or to marry someone who hasn’t been born yet. Our system achieves a 36% error reduction over a pipelined baseline. 
Because the real world evolves over time, numerous relations between entities written in presently available texts are already obsolete or will potentially evolve in the future. This study aims at resolving the intricacy in consistently compiling relations extracted from text, and presents a method for identifying constancy and uniqueness of the relations in the context of supervised learning. We exploit massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues. Experimental results conﬁrmed that the time-series frequency distributions contributed much to the recall of constancy identiﬁcation and the precision of the uniqueness identiﬁcation.  There exists, however, a great challenge to compile consistently relations extracted from text by these methods, because they assume a simplifying assumption that relations are time-invariant. In other words, they implicitly disregard the fact that statements in texts actually reﬂect the state of the world at the time when they were written, which follows that relations extracted from such texts eventually become outdated as the real world evolves over time. Let us consider that relations are extracted from the following sentences:1 (1) a. 1Q84 is written by Haruki Murakami. b. Moselle river ﬂows through Germany. c. U.S.’s president is George Bush. d. Pentax sells K-5, a digital SLR.  
Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and ﬁne-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24% greater accuracy than a Named Entity Recognition baseline, and our method for ﬁne-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering. 
We propose a complete probabilistic discriminative framework for performing sentencelevel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classiﬁer, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 
Previous work on paraphrase extraction using parallel or comparable corpora has generally not considered the documents’ discourse structure as a useful information source. We propose a novel method for collecting paraphrases relying on the sequential event order in the discourse, using multiple sequence alignment with a semantic similarity measure. We show that adding discourse information boosts the performance of sentence-level paraphrase acquisition, which consequently gives a tremendous advantage for extracting phraselevel paraphrase fragments from matched sentences. Our system beats an informed baseline by a margin of 50%. 
We propose a technique to generate nonprojective word orders in an efﬁcient statistical linearization system. Our approach predicts liftings of edges in an unordered syntactic tree by means of a classiﬁer, and uses a projective algorithm for tree linearization. We obtain statistically signiﬁcant improvements on six typologically different languages: English, German, Dutch, Danish, Hungarian, and Czech.  
We investigate paradigmatic representations of word context in the domain of unsupervised syntactic category acquisition. Paradigmatic representations of word context are based on potential substitutes of a word in contrast to syntagmatic representations based on properties of neighboring words. We compare a bigram based baseline model with several paradigmatic models and demonstrate significant gains in accuracy. Our best model based on Euclidean co-occurrence embedding combines the paradigmatic context representation with morphological and orthographic features and achieves 80% many-to-one accuracy on a 45-tag 1M word corpus. 
We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classiﬁcation. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation of documents and words in a corpus. 
Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation. However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy. In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the translation quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding. 
When trained on very large parallel corpora, the phrase table component of a machine translation system grows to consume vast computational resources. In this paper, we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation. Systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods. 
Accurate and robust metrics for automatic evaluation are key to the development of statistical machine translation (MT) systems. We ﬁrst introduce a new regression model that uses a probabilistic ﬁnite state machine (pFSM) to compute weighted edit distance as predictions of translation quality. We also propose a novel pushdown automaton extension of the pFSM model for modeling word swapping and cross alignments that cannot be captured by standard edit distance models. Our models can easily incorporate a rich set of linguistic features, and automatically learn their weights, eliminating the need for ad-hoc parameter tuning. Our methods achieve state-of-the-art correlation with human judgments on two different prediction tasks across a diverse set of standard evaluations (NIST OpenMT06,08; WMT0608). 
We investigate two aspects of the empirical behavior of paired signiﬁcance tests for NLP systems. First, when one system appears to outperform another, how does signiﬁcance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on? Is it true that for each task there is a gain which roughly implies signiﬁcance? We explore these issues across a range of NLP tasks using both large collections of past systems’ outputs and variants of single systems. Next, once signiﬁcance levels are computed, how well does the standard i.i.d. notion of signiﬁcance hold up in practical settings where future distributions are neither independent nor identically distributed, such as across domains? We explore this question using a range of test set variations for constituency parsing. 
Current Chinese event extraction systems suffer much from two problems in trigger identification: unknown triggers and word segmentation errors to known triggers. To resolve these problems, this paper proposes two novel inference mechanisms to explore special characteristics in Chinese via compositional semantics inside Chinese triggers and discourse consistency between Chinese trigger mentions. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 
We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base. Experiments show signiﬁcant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base. 
Discovering significant types of relations from the web is challenging because of its open nature. Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance, but most of them rely on tagging arguments of predefined types. Recently, a new algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input. However, it cannot handle polysemy of relation phrases and fails to group many similar (“synonymous”) relation instances because of the sparseness of features. In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems. The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction. Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones. While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method. It is also very efficient. Experiments on a realworld dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web. 
We propose the subtree ranking approach to parse forest reranking which is a generalization of current perceptron-based reranking methods. For the training of the reranker, we extract competing local subtrees, hence the training instances (candidate subtree sets) are very similar to those used during beamsearch parsing. This leads to better parameter optimization. Another chief advantage of the framework is that arbitrary learning to rank methods can be applied. We evaluated our reranking approach on German and English phrase structure parsing tasks and compared it to various state-of-the-art reranking approaches such as the perceptron-based forest reranker. The subtree ranking approach with a Maximum Entropy model signiﬁcantly outperformed the other approaches. 
Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors. We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together. We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difﬁcult for stateof-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text. 
This paper proposes the utilization of lexical cohesion to facilitate evaluation of machine translation at the document level. As a linguistic means to achieve text coherence, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meaning. A comparison between machine and human translation is conducted to illustrate one of their critical distinctions that human translators tend to use more cohesion devices than machine. Various ways to apply this feature to evaluate machinetranslated documents are presented, including one without reliance on reference translation. Experimental results show that incorporating this feature into sentence-level evaluation metrics can enhance their correlation with human judgements. 
Many natural language processing problems involve constructing large nearest-neighbor graphs. We propose a system called FLAG to construct such graphs approximately from large data sets. To handle the large amount of data, our algorithm maintains approximate counts based on sketching algorithms. To ﬁnd the approximate nearest neighbors, our algorithm pairs a new distributed online-PMI algorithm with novel fast approximate nearest neighbor search algorithms (variants of PLEB). These algorithms return the approximate nearest neighbors quickly. We show our system’s efﬁciency in both intrinsic and extrinsic experiments. We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications. 
Short listings such as classiﬁed ads or product listings abound on the web. If a computer can reliably extract information from them, it will greatly beneﬁt a variety of applications. Short listings are, however, challenging to process due to their informal styles. In this paper, we present an unsupervised information extraction system for short listings. Given a corpus of listings, the system builds a semantic model that represents typical objects and their attributes in the domain of the corpus, and then uses the model to extract information. Two key features in the system are a semantic parser that extracts objects and their attributes and a listing-focused clustering module that helps group together extracted tokens of same type. Our evaluation shows that the semantic model learned by these two modules is effective across multiple domains. 
Many NLP tasks rely on accurate statistics from large corpora. Tracking complete statistics is memory intensive, so recent work has proposed using compact approximate “sketches” of frequency distributions. We describe 10 sketch methods, including existing and novel variants. We compare and study the errors (over-estimation and underestimation) made by the sketches. We evaluate several sketches on three important NLP problems. Our experiments show that one sketch performs best for all the three tasks. 
Conditional random ﬁelds and other graphical models have achieved state of the art results in a variety of tasks such as coreference, relation extraction, data integration, and parsing. Increasingly, practitioners are using models with more complex structure—higher treewidth, larger fan-out, more features, and more data—rendering even approximate inference methods such as MCMC inefﬁcient. In this paper we propose an alternative MCMC sampling scheme in which transition probabilities are approximated by sampling from the set of relevant factors. We demonstrate that our method converges more quickly than a traditional MCMC sampler for both marginal and MAP inference. In an author coreference task with over 5 million mentions, we achieve a 13 times speedup over regular MCMC inference. 
This paper deals with the problem of predicting structures in the context of NLP. Typically, in structured prediction, an inference procedure is applied to each example independently of the others. In this paper, we seek to optimize the time complexity of inference over entire datasets, rather than individual examples. By considering the general inference representation provided by integer linear programs, we propose three exact inference theorems which allow us to re-use earlier solutions for certain instances, thereby completely avoiding possibly expensive calls to the inference procedure. We also identify several approximation schemes which can provide further speedup. We instantiate these ideas to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance. 
We present a method for exact optimization and sampling from high order Hidden Markov Models (HMMs), which are generally handled by approximation techniques. Motivated by adaptive rejection sampling and heuristic search, we propose a strategy based on sequentially reﬁning a lower-order language model that is an upper bound on the true model we wish to decode and sample from. This allows us to build tractable variable-order HMMs. The ARPA format for language models is extended to enable an efﬁcient use of the max-backoff quantities required to compute the upper bound. We evaluate our approach on two problems: a SMS-retrieval task and a POS tagging experiment using 5-gram models. Results show that the same approach can be used for exact optimization and sampling, while explicitly constructing only a fraction of the total implicit state-space. 
This paper presents PATTY: a large resource for textual patterns that denote binary relations between entities. The patterns are semantically typed and organized into a subsumption taxonomy. The PATTY system is based on efﬁcient algorithms for frequent itemset mining and can process Web-scale corpora. It harnesses the rich type system and entity population of large knowledge bases. The PATTY taxonomy comprises 350,569 pattern synsets. Random-sampling-based evaluation shows a pattern accuracy of 84.7%. PATTY has 8,162 subsumptions, with a random-sampling-based precision of 75%. The PATTY resource is freely available for interactive access and download. 
PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar. We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion. Our method works with linguisticallymotivated annotations, induced latent structure, lexicalization, or any mix of the three. We use a structured expectation propagation algorithm that makes use of the factored structure in two ways. First, by partitioning the factors, it speeds up parsing exponentially over the unfactored approach. Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach. Using purely latent variable annotations, we can efﬁciently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na¨ıve approach. Combining latent, lexicalized, and unlexicalized annotations, our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank. 
We introduce a model of coherence which captures the intentional discourse structure in text. Our work is based on the hypothesis that syntax provides a proxy for the communicative goal of a sentence and therefore the sequence of sentences in a coherent discourse should exhibit detectable structural patterns. Results show that our method has high discriminating power for separating out coherent and incoherent news articles reaching accuracies of up to 90%. We also show that our syntactic patterns are correlated with manual annotations of intentional structure for academic conference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles. 
Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments. We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant. Common practice uses lowerorder entries in an N -gram model to score the ﬁrst few words of a fragment; this violates assumptions made by common smoothing strategies, including Kneser-Ney. Instead, we use a unigram model to score the ﬁrst word, a bigram for the second, etc. This improves search at the expense of memory. Conversely, we show how to save memory by collapsing probability and backoff into a single value without changing sentence-level scores, at the expense of less accurate estimates for sentence fragments. These changes can be stacked, achieving better estimates with unchanged memory usage. In order to interpret changes in search accuracy, we adjust the pop limit so that accuracy is unchanged and report the change in CPU time. In a GermanEnglish Moses system with target-side syntax, improved estimates yielded a 63% reduction in CPU time; for a Hiero-style version, the reduction is 21%. The compressed language model uses 26% less RAM while equivalent search quality takes 27% more CPU. Source code is released as part of KenLM. 
Independence between sentences is an assumption deeply entrenched in the models and algorithms used for statistical machine translation (SMT), particularly in the popular dynamic programming beam search decoding algorithm. This restriction is an obstacle to research on more sophisticated discourse-level models for SMT. We propose a stochastic local search decoding method for phrase-based SMT, which permits free document-wide dependencies in the models. We explore the stability and the search parameters of this method and demonstrate that it can be successfully used to optimise a document-level semantic language model. 
Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efﬁciency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method. 
Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting ﬁne-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. 
Existing vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy. We introduce a new vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one. We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus – a word sense along with its synonyms and antonyms – is treated as a “document,” and the resulting document collection is subjected to LSA. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property. We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and ﬁnd that the method improves on the results of that study. Further improvements result from reﬁning the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11 points absolute in F measure. 
Adjectival modiﬁcation, particularly by expressions that have been treated as higherorder modiﬁers in the formal semantics tradition, raises interesting challenges for semantic composition in distributional semantic models. We contrast three types of adjectival modiﬁers – intersectively used color terms (as in white towel, clearly ﬁrst-order), subsectively used color terms (white wine, which have been modeled as both ﬁrst- and higher-order), and intensional adjectives (former bassist, clearly higher-order) – and test the ability of different composition strategies to model their behavior. In addition to opening up a new empirical domain for research on distributional semantics, our observations concerning the attested vectors for the different types of adjectives, the nouns they modify, and the resulting noun phrases yield insights into modiﬁcation that have been little evident in the formal semantics literature to date. 
We explore the interplay of knowledge and structure in co-reference resolution. To inject knowledge, we use a state-of-the-art system which cross-links (or “grounds”) expressions in free text to Wikipedia. We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system. To maximize the utility of the injected knowledge, we deploy a learningbased multi-sieve approach and develop novel entity-based features. Our end system outperforms the state-of-the-art baseline by 2 B3 F1 points on non-transcript portion of the ACE 2004 dataset. 
We annotate and resolve a particular case of abstract anaphora, namely, thisissue anaphora. We propose a candidate ranking model for this-issue anaphora resolution that explores different issuespeciﬁc and general abstract-anaphora features. The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text. Our results show that (a) the model outperforms the strong adjacent-sentence baseline; (b) general abstract-anaphora features, as distinguished from issue-speciﬁc features, play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other NPs such as this problem and this debate; and (c) it is possible to reduce the search space in order to improve performance. 
Bridging the lexical gap between the user’s question and the question-answer pairs in the Q&A archives has been a major challenge for Q&A retrieval. State-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models. While useful, the effectiveness of these models is highly dependant on the availability of quality corpus in the absence of which they are troubled by noise issues. Moreover these models perform word based expansion in a context agnostic manner resulting in translation that might be mixed and fairly general. This results in degraded retrieval performance. In this work we address the above issues by extending the lexical word based translation model to incorporate semantic concepts (entities). We explore strategies to learn the translation probabilities between words and the concepts using the Q&A archives and a popular entity catalog. Experiments conducted on a large scale real data show that the proposed techniques are promising. 
Taxonomies can serve as browsing tools for document collections. However, given an arbitrary collection, pre-constructed taxonomies could not easily adapt to the speciﬁc topic/task present in the collection. This paper explores techniques to quickly derive task-speciﬁc taxonomies supporting browsing in arbitrary document collections. The supervised approach directly learns semantic distances from users to propose meaningful task-speciﬁc taxonomies. The approach aims to produce globally optimized taxonomy structures by incorporating path consistency control and usergenerated task speciﬁcation into the general learning framework. A comparison to stateof-the-art systems and a user study jointly demonstrate that our techniques are highly effective. 
Cost-sensitive classiﬁcation, where the features used in machine learning tasks have a cost, has been explored as a means of balancing knowledge against the expense of incrementally obtaining new features. We introduce a setting where humans engage in classiﬁcation with incrementally revealed features: the collegiate trivia circuit. By providing the community with a web-based system to practice, we collected tens of thousands of implicit word-by-word ratings of how useful features are for eliciting correct answers. Observing humans’ classiﬁcation process, we improve the performance of a state-of-the art classiﬁer. We also use the dataset to evaluate a system to compete in the incremental classiﬁcation task through a reduction of reinforcement learning to classiﬁcation. Our system learns when to answer a question, performing better than baselines and most human players. 
We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multidomain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-speciﬁc class label biases. When multi-domain learning is applied to these settings, (2) are multidomain methods improving because they capture domain-speciﬁc class biases? An understanding of these two issues presents a clearer idea about where the ﬁeld has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art. 
Representation learning is a promising technique for discovering features that allow supervised classiﬁers to generalize from a source domain dataset to arbitrary new domains. We present a novel, formal statement of the representation learning task. We argue that because the task is computationally intractable in general, it is important for a representation learner to be able to incorporate expert knowledge during its search for helpful features. Leveraging the Posterior Regularization framework, we develop an architecture for incorporating biases into representation learning. We investigate three types of biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques. 
We introduce a novel approach named unambiguity regularization for unsupervised learning of probabilistic natural language grammars. The approach is based on the observation that natural language is remarkably unambiguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid. We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences. The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm. The softmax-EM algorithm can be implemented with a simple and computationally efﬁcient extension to standard EM. In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneﬁcial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art. 
Extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using Conditional Random Fields (CRFs). CRFs, however, do not readily model potentially useful segment-level information like syntactic constituent structure. Thus, we propose a semi-CRF-based approach to the task that can perform sequence labeling at the segment level. We extend the original semi-CRF model (Sarawagi and Cohen, 2004) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries. We evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segmentlevel syntactic parse features. Experimental results demonstrate that our approach outperforms state-of-the-art methods for both opinion expression tasks. 
This paper proposes a novel approach to extract opinion targets based on wordbased translation model (WTM). At first, we apply WTM in a monolingual scenario to mine the associations between opinion targets and opinion words. Then, a graphbased algorithm is exploited to extract opinion targets, where candidate opinion relevance estimated from the mined associations, is incorporated with candidate importance to generate a global measure. By using WTM, our method can capture opinion relations more precisely, especially for long-span relations. In particular, compared with previous syntax-based methods, our method can effectively avoid noises from parsing errors when dealing with informal texts in large Web corpora. By using graph-based algorithm, opinion targets are extracted in a global process, which can effectively alleviate the problem of error propagation in traditional bootstrap-based methods, such as Double Propagation. The experimental results on three real world datasets in different sizes and languages show that our approach is more effective and robust than state-of-art methods. 
We investigate the use of language in food writing, speciﬁcally on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external variables, such as restaurant menu prices. We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to describe food in restaurants. We also explore interactions in language use between menu prices and sentiment as expressed in user reviews. 
We present an automatic method for mapping language-speciﬁc part-of-speech tags to a set of universal tags. This uniﬁed representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. We encode this intuition in an objective function that captures a range of distributional and typological characteristics of the derived mapping. Given the exponential size of the mapping space, we propose a novel method for optimizing over soft mappings, and use entropy regularization to drive those towards hard mappings. Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing.1 
In modern Chinese articles or conversations, it is very popular to involve a few English words, especially in emails and Internet literature. Therefore, it becomes an important and challenging topic to analyze Chinese-English mixed texts. The underlying problem is how to tag part-of-speech (POS) for the English words involved. Due to the lack of specially annotated corpus, most of the English words are tagged as the oversimplified type, “foreign words”. In this paper, we present a method using dynamic features to tag POS of mixed texts. Experiments show that our method achieves higher performance than traditional sequence labeling methods. Meanwhile, our method also boosts the performance of POS tagging for pure Chinese texts. 
Despite signiﬁcant recent work, purely unsupervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks. Use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that signiﬁcantly improves accuracy. However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps. In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource, the Wiktionary. Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that signiﬁcantly exceeds best unsupervised and parallel text methods. We achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised Penn Treebank. 
We present a multilingual joint approach to Word Sense Disambiguation (WSD). Our method exploits BabelNet, a very large multilingual knowledge base, to perform graphbased WSD across different languages, and brings together empirical evidence from these languages using ensemble methods. The results show that, thanks to complementing wide-coverage multilingual lexical knowledge with robust graph-based algorithms and combination methods, we are able to achieve the state of the art in both monolingual and multilingual WSD settings. 
We present a new minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD). Glossaries for several domains are iteratively acquired from the Web by means of a bootstrapping technique. The acquired glosses are then used as the sense inventory for fullyunsupervised domain WSD. Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables highperformance results on dozens of domains at a coarse and ﬁne-grained level. 
A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action. In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data. Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word). The models differ in terms of the mechanisms by which they integrate the two modalities. Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by concatenating the two. 
State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of in-domain data. When training data is out-of-domain or limited, accuracy degrades. In this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences. We show how to augment sentencelevel models for parsing and POS tagging with inter-sentence consistency constraints. To deal with the resulting global objective, we present an efﬁcient and exact dual decomposition decoding algorithm. In experiments, we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate signiﬁcant error reduction in the domain adaptation and the lightly supervised settings across ﬁve languages. 
Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly deﬁned boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a uniﬁed dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the uniﬁed parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the uniﬁed model and algorithm in parsing structures of words, phrases and sentences simultaneously. 1 
Most current dependency parsers presuppose that input words have been morphologically disambiguated using a part-of-speech tagger before parsing begins. We present a transitionbased system for joint part-of-speech tagging and labeled dependency parsing with nonprojective trees. Experimental evaluation on Chinese, Czech, English and German shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system, which lead to improved state-of-theart results for all languages. 
Activities on social media increase at a dramatic rate. When an external event happens, there is a surge in the degree of activities related to the event. These activities may be temporally correlated with one another, but they may also capture different aspects of an event and therefore exhibit different bursty patterns. In this paper, we propose to identify event-related bursts via social media activities. We study how to correlate multiple types of activities to derive a global bursty pattern. To model smoothness of one state sequence, we propose a novel function which can capture the state context. The experiments on a large Twitter dataset shows our methods are very effective.  140 120 Noise 100  allïtweets retweets urlïembedded tweets  80  60  40  20  0 2  4  6  8  10  Time index  (a) Query=“Amazon.”  150 Noise 100  allïtweets retweets urlïembedded tweets  50  0 2  4  6  8  10  Time index  (b) Query=“Eclipse”.  Figure 1: The amount of activities within a 10-hour window for two queries. Three types of activities are considered: (1) posting a tweet (upward triangle), (2) retweet (downward triangle), (3) posting a URLembedded tweet (excluding retweet) (ﬁlled circle). As explained in Table 1, both bursts above are noisy.  
We consider the task of predicting the gender of the YouTube1 users and contrast two information sources: the comments they leave and the social environment induced from the afﬁliation graph of users and videos. We propagate gender information through the videos and show that a user’s gender can be predicted from her social environment with the accuracy above 90%. We also show that the gender can be predicted from language alone (89%). A surprising result of our study is that the latter predictions correlate more strongly with the gender predominant in the user’s environment than with the sex of the person as reported in the proﬁle. We also investigate how the two views (linguistic and social) can be combined and analyse how prediction accuracy changes over different age groups. 
 The question “how predictable is English?” has long fascinated researchers. While prior work has focused on formal English typically used in news articles, we turn to texts generated by users in online settings that are more informal in nature. We are motivated by a novel application scenario: given the difﬁculty of typing on mobile devices, can we help reduce typing effort with message completion, especially in conversational settings? We propose a method for automatic response completion. Our approach models both the language used in responses and the speciﬁc context provided by the original message. Our experimental results on a large-scale dataset show that both components help reduce typing effort. We also perform an information-theoretic study in this setting and examine the entropy of user-generated content, especially in conversational scenarios, to better understand predictability of user generated English. 
The geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text, often in the context of social media and online content. One common approach for geolocating texts is rooted in information retrieval. Given training documents labeled with latitude/longitude coordinates, a grid is overlaid on the Earth and pseudo-documents constructed by concatenating the documents within a given grid cell; then a location for a test document is chosen based on the most similar pseudo-document. Uniform grids are normally used, but they are sensitive to the dispersion of documents over the earth. We deﬁne an alternative grid construction using k-d trees that more robustly adapts to data, especially with larger training sets. We also provide a better way of choosing the locations for pseudo-documents. We evaluate these strategies on existing Wikipedia and Twitter corpora, as well as a new, larger Twitter corpus. The adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large Twitter corpus. The two grid constructions can also be combined to produce consistently strong results across all training sets. 
Discriminative training in query spelling correction is difﬁcult due to the complex internal structures of the data. Recent work on query spelling correction suggests a two stage approach a noisy channel model that is used to retrieve a number of candidate corrections, followed by discriminatively trained ranker applied to these candidates. The ranker, however, suffers from the fact the low recall of the ﬁrst, suboptimal, search stage. This paper proposes to directly optimize the search stage with a discriminative model based on latent structural SVM. In this model, we treat query spelling correction as a multiclass classiﬁcation problem with structured input and output. The latent structural information is used to model the alignment of words in the spelling correction process. Experiment results show that as a standalone speller, our model outperforms all the baseline systems. It also attains a higher recall compared with the noisy channel model, and can therefore serve as a better ﬁltering stage when combined with a ranker. 
Much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements. However, most previous research for computational stylometric analysis has relied on shallow lexico-syntactic patterns. Some very recent work has shown that PCFG models can detect distributional diﬀerence in syntactic styles, but without oﬀering much insights into exactly what constitute salient stylistic elements in sentence structure characterizing each authorship. In this paper, we present a comprehensive exploration of syntactic elements in writing styles, with particular emphasis on interpretable characterization of stylistic elements. We present analytic insights with respect to the authorship attribution task in two diﬀerent domains. 
Complex predicates raise the question of how to encode them in computational lexicons. Their computational implementation in South Asian languages is in its infancy. This paper examines in detail the variety of complex predicates in Telugu revealing the syntactic process of their composition and the constraints on their formation. The framework used is First Phase Syntax (Ramchand 2008). In this lexical semantic approach that ties together the constraints on the meaning and the argument structure of complex predicates, each verb breaks down into 3 sub-event heads which determine the nature of the verb. Complex predicates are formed by one verb subsuming the sub-event heads of another verb, and this is constrained in principled ways. The data analysed and the constraints developed in the paper are of use to linguists working on computational solutions for Telugu and other languages, for design and development of predicate structure functions in linguistic processors. KEYWORDS: Complex Predicates, Dravidian, First Phase Syntax, Argument Structure, Telugu. Proceedings of COLING 2012: Demonstration Papers, pages 1–8, COLING 2012, Mumbai, December 2012. 
This paper presents Improved Kit for Anaphora resolution (IKAR) – a hybrid system for anaphora resolution for Polish that combines machine learning methods with hand written rules. We give an overview of anaphora types annotated in the corpus and inner workings of the system. The preliminary experiments evaluating IKAR resolution performance are discussed. We have achieved promising results using standard measures employed in evaluation of anaphora and coreference resolution systems. KEYWORDS: Anaphora Resoulution, Coreference, Polish. Proceedings of COLING 2012: Demonstration Papers, pages 25–32, COLING 2012, Mumbai, December 2012. 25  
In recent years, social media has become a customer touch-point for the business functions of marketing, sales and customer service. We aim to show that intention analysis might be useful to these business functions and that it can be performed effectively on short texts (at the granularity level of a single sentence). We demonstrate a scheme of categorization of intentions that is amenable to automation using simple machine learning techniques that are language-independent. We discuss the grounding that this scheme of categorization has in speech act theory. In the demonstration we go over a number of usage scenarios in an attempt to show that the use of automatic intention detection tools would beneﬁt the business functions of sales, marketing and service. We also show that social media can be used not just to convey pleasure or displeasure (that is, to express sentiment) but also to discuss personal needs and to report problems (to express intentions). We evaluate methods for automatically discovering intentions in text, and establish that it is possible to perform intention analysis on social media with an accuracy of 66.97% ± 0.10%. KEYWORDS: intention analysis, intent analysis, social media, speech act theory, sentiment analysis, emotion analysis, intention. Proceedings of COLING 2012: Demonstration Papers, pages 33–40, COLING 2012, Mumbai, December 2012. 33  
Morphological segmentation of words is a subproblem of many natural language tasks, including handling out-of-vocabulary (OOV) words in machine translation, more effective information retrieval, and computer assisted vocabulary learning. Previous work typically relies on extensive statistical and semantic analyses to induce legitimate stems and afﬁxes. We introduce a new learning based method and a prototype implementation of a knowledge light system for learning to segment a given word into word parts, including preﬁxes, sufﬁxes, stems, and even roots. The method is based on the Conditional Random Fields (CRF) model. Evaluation results show that our method with a small set of seed training data and readily available resources can produce ﬁne-grained morphological segmentation results that rival previous work and systems. KEYWORDS: morphology, afﬁx, word root, CRF. Proceedings of COLING 2012: Demonstration Papers, pages 51–58, COLING 2012, Mumbai, December 2012. 51  
We describe a method of correcting noisy output of a machine translation system. Our idea is to consider dierent phrases of a given sentence, and nd appropriate replacements of some of these from the frequently occurring similar phrases in the monolingual corpus. The frequent phrases in the monolingual corpus are indexed by a search engine. When looking for similar phrases we consider phrases containing words that are spelling variations of or are similar in meaning to the words in the input phrase. We use a framework where we can consider dierent ways of splitting a sentence into short phrases and combining them so as to get the best replacement sentence that tries to preserve the meaning meant to be conveyed by the original sentence. 
Conversational agents that use reinforcement learning for policy optimization in large domains often face the problem of limited scalability. This problem can be addressed either by using function approximation techniques that estimate an approximate true value function, or by using a hierarchical decomposition of a learning task into subtasks. In this paper, we present a novel approach for dialogue policy optimization that combines the beneﬁts of hierarchical control with function approximation. The approach incorporates two concepts to allow ﬂexible switching between subdialogues, extending current hierarchical reinforcement learning methods. First, hierarchical treebased state representations initially represent a compact portion of the possible state space and are then dynamically extended in real time. Second, we allow state transitions across sub-dialogues to allow non-strict hierarchical control. Our approach is integrated, and tested with real users, in a robot dialogue system that learns to play Quiz games. Keywords: spoken dialogue systems, reinforcement learning, hierarchical control, function ap- proximation, user simulation, human-robot interaction, ﬂexible interaction. 
Proceedings of COLING 2012: Demonstration Papers, pages 111–118, COLING 2012, Mumbai, December 2012. 111 
Romanian has been traditionally seen as bearing three lexical genders: masculine, feminine, and neuter, although it has always been known to have only two agreement patterns (for masculine and feminine). Previous machine learning classiﬁers which have attempted to discriminate Romanian nouns according to gender have taken as input only the singular form, either presupposing the traditional tripartite analysis, or using additional information from case inﬂected forms. We present here a tool based on two parallel support vector machines using n-gram features from the singular and from the plural, which distinguish the neuter. KEYWORDS: Romanian morphology, neuter gender, noun class, SVM. Proceedings of COLING 2012: Demonstration Papers, pages 119–124, COLING 2012, Mumbai, December 2012. 119  
The purpose of this article is to propose a tool for measuring distances between different styles of one or more authors. The main study is focused on measuring and visualizing distances in a space induced by ranked lexical features. We investigate the case of Vladimir Nabokov, a bilingual Russian - English language author. KEYWORDS: stylometry, L1 distance, translationes, rankings, Nabokov. Proceedings of COLING 2012: Demonstration Papers, pages 125–130, COLING 2012, Mumbai, December 2012. 125  
The paper is focused on blogosphere research based on the TREC blog distillation task, and aims to explore unbiased and significant features automatically and efficiently. Feedback from faceted feeds is introduced to harvest relevant features and information gain is used to select discriminative features, including the unigrams as well as the patterns of unigram associations. Meanwhile facing the terabyte blog dataset, some flexible processing is adopted in our approach. The evaluation result shows that the selected feedback features can greatly improve the performance and adapt well to the terabyte data. KEYWORDS : Blog Distillation, Faceted Distillation, Feedback 1. Introduction With the accelerated growth of social networks, both organizations and individuals have shown great interest in conveying or exchanging ideas and opinions. The blogosphere provides an ideal platform for communication. According to the statistics of Blogpulse(blogpulse.com) in Jan. 2011, more than 152 million blogs have been published. One interesting issue related to the massive blogs is to automatically explore authors’ behaviours from their blog posts. Research related to blog posts mainly focuses on opinion retrieval to identify topic-based opinion posts, which means retrieved posts should not only be relevant to the targets, but also contain subjective opinions about given topics. Generally, topic-based opinion retrieval can be divided into two parts using a separated relevance and opinion model or a unified relevance model. In separated models, posts are first ranked by topical relevance only, then, the opinion scores can be acquired by either classifiers, such as SVM (Wu Zhang and Clement Yu, 2007), or external toolkits like OpinionFinder(www.cs.pitt.edu/mpqa/) (David Hannah et al. 2007). The precision of the opinion retrieval is highly dependent on the precision of relevance retrieval. Huang et al propose a unified relevance model by integrating a query-dependent and a query-independent method, which achieved high performance in topic-based opinion retrieval (Huang et al., 2009). Based on opinionated blogosphere identification, TREC introduces the faceted blog distillation track in 2009 with two subtasks: baseline distillation and faceted distillation. The former is to retrieve all the relevant feeds corresponding to given topics without any consideration of facets. The latter aims to re-rank the baseline feeds according to specific facets. For operational simplicity, TREC specifies three faceted inclination pairs (TREC Blog Wiki, 2009): Opinionated vs. Factual inclinations aim to differentiate feeds expressing opinions from those describing factual information; Personal vs. Official inclinations are to discriminate individual-authored feeds from organization-issued ones; 
While the popularity of Twitter brings a plethora of Twitter researches, short, plain and informal tweet texts limit the research progress. This paper aims to investigate whether hyperlinks in tweets and their linked pages can be used to discover rich information for Twitter applications. The statistical analysis on the analysed hyperlinks offers the evidence that tweets contain a large amount of hyperlinks and a high percentage of hyperlinks introduce substantial and informative information from external resources. The usage of hyperlinks is examined on a self-defined hyperlink recommendation task. The recommended hyperlinks can not only provide more descriptive or explanatory information for the corresponding trending topics, but also pave the way for further applications, such as Twitter summarization. KEYWORDS : Twitter, Hyperlink Usage, Hyperlink Recommendation 1. Introduction The shift of information center from the mainstream media to the general public drives a growth of social network sites among which Twitter is undoubtedly one of the popular applications now. In academia, Twitter researches have become a new hotspot (H. Kwak et al, 2010; D. Zhao and M. Rosson, 2009; M. Michael and N. Kouda, 2010). Existing researches mainly focus on tweet content and user communication, while ignoring external resources. However, the limitation is the plain and short tweet text, which contains 140 characters at most. Even worse, tweets are usually written with many informal expressions. It has been reported in (TechInfo, 2010) that about 25% of tweets contain hyperlinks and the proportion is increasing. Recently, even Twitter itself also provides an interface to allow people pay close attention to the tweets with hyperlinks. This move probably implies that (1) tweets contain a large amount of hyperlinks; and (2) hyperlinks in tweets may provide useful information for understanding topics. For instance, at the time when we write this paper, “William & Kate” is a popular topic of conversation. When we follow some arbitrary hyperlinks in the tweets under this topic, we are often directed to the Web pages containing the detailed information about their royal honeymoon (http://styleite.com/gjhao), the video of royal wedding (http://bit.ly/ld72jW) and the comments on their expensive wedding (http://ow.ly/1cKi99). Without length limit, a Web page tends to use a longer text describing the topics. Especially some of these hyperlinked pages are written by professional editors, and are much more regular than ordinary tweets. “William & Kate” is just an example here. But it motivates us to seek for the answers for the following questions: Q1: How popular are hyperlinks in tweets? Q2: Can these hyperlinks provide additional, useful and relevant information for understanding topics? Proceedings of COLING 2012: Demonstration Papers, pages 155–162, COLING 2012, Mumbai, December 2012. 
Proceedings of COLING 2012: Demonstration Papers, pages 163–174, COLING 2012, Mumbai, December 2012. 163  1. Introduction Natural language processing is a field of computer science, artificial intelligence (also called machine learning) and linguistics concerned with the interactions between computers and human (natural) languages. Specifically, it is the process of a computer extracting meaningful information from natural language input and/or producing natural language output. Part of Speech tagger is an important application of natural language processing. Part of speech tagging is the process of assigning a part of speech like noun, verb, preposition, pronoun, adverb, adjective or other lexical class marker to each word in a sentence. There are a number of approaches to implement part of speech tagger, i.e. Rule Based approach, Statistical approach and Hybrid approach. Rule-based tagger use linguistic rules to assign the correct tags to the words in the sentence or file. Statistical Part of Speech tagger is based on the probabilities of occurrences of words for a particular tag. Hybrid based Part of Speech tagger is combination of Rule based approach and Statistical approach. Part of Speech tagging is an important application of natural language processing. It is used in several Natural Languages processing based software implementation. Accuracy of all NLP tasks like grammar checker, phrase chunker, machine translation etc. depends upon the accuracy of the Part of Speech tagger. Tagger plays an important role in speech recognition, natural language parsing and information retrieval. 2. Related Work There have been many implementation of part of speech tagger using statistical approach, mainly for morphological rich languages like Hindi. Statistical techniques are easy to implement and require very less knowledge about the language. 
The efﬁciency and robustness of statistical parsers has made it possible to create very large treebanks. These serve as the starting point for further work including enrichment, extraction, and curation: semantic annotations are added, syntactic features are mined, erroneous analyses are corrected. In many such cases manual processing is required, and this must operate efﬁciently on the largest scale. We report on an efﬁcient web-based system for querying very large treebanks called Fangorn. It implements an XPath-like query language which is extended with a linguistic operator to capture proximity in the terminal sequence. Query results are displayed using scalable vector graphics and decorated with the original query, making it easy for queries to be modiﬁed and resubmitted. Fangorn is built on the Apache Lucene text search engine and is available under the Apache License. KEYWORDS: language resources, database query, annotation, syntax, parsing. Proceedings of COLING 2012: Demonstration Papers, pages 175–182, COLING 2012, Mumbai, December 2012. 175  
Given the rapid publication rate in many ﬁelds of science, it is important to develop technology that can help researchers locate different types of information in scientiﬁc literature. A number of approaches have been developed for automatic identiﬁcation of information structure of scientiﬁc papers. Such approaches can be useful for down-stream NLP tasks (e.g. summarization) and practical research tasks (e.g. scientiﬁc literature review), and can be realistically applied across domains when they involve light supervision. However, even light supervision requires some data annotation for new tasks. We introduce the CRAB Reader – a tool for the analysis and visualization of information structure (according to the Argumentative Zoning (AZ) scheme) in scientiﬁc literature which can facilitate efﬁcient and user-friendly expert-annotation. We investigate and demonstrate the use of our tool for this purpose and also discuss the beneﬁts of using the same tool to support practical tasks such as scientiﬁc literature review. TITLE AND ABSTRACT IN CHINESE CRAB Reader: 分析和查看科技文献论证结构的工具 随着各个学科领域出版量的迅速增长，新的文本挖掘技术的开发对研究人员从海量科技 文献中寻找有用信息至为重要。许多自动识别科技文献信息结构的技术的开发，对于自 然语言处理领域的下游工作（例如文本摘要） 以及实际的研究工作（例如科技文献的阅 读）有很大帮助，尤其是基于弱监督学习的技术更能够实际应用于不同的学科领域。然而 即使弱监督的学习依然需要一定量的人工标注数据以适应一项新任务。本文介绍了CRAB Reader： 一款分析和查看科技文献论证结构的工具。该工具有助于高效便捷地对文献进行 标注。本文研究并证实了该工具对于文献信息结构的自动分析和识别有重要作用，同时讨 论了该工具为实际科研工作例如科技文献的阅读带来的便利。 KEYWORDS: Information Structure, Argumentative Zoning (AZ), Interactive Annotation, Active Learning. KEYWORDS IN CHINESE: 信息结构，论证结构分析， 交互式标注，主动学习. Proceedings of COLING 2012: Demonstration Papers, pages 183–190, COLING 2012, Mumbai, December 2012. 183  
Proceedings of COLING 2012: Demonstration Papers, pages 191–198, COLING 2012, Mumbai, December 2012. 191  
 {lhart, hassana, amank}@bcltechnologies.com  As a critical language, there is huge potential for the usefulness of an Arabic Semantic Role Labeling (SRL) system. This task involves two subtasks: predicate argument boundary detection and argument classification. Based on the innovations of Diab, Moschitti, and Pighin (2007) in the field of Arabic Natural Language Processing (NLP), SRL in particular, we are currently developing a system for automatic SRL in Arabic.  KEYWORDS: Arabic, semantic role labeling, SRL, predicate argument, boundary detection, argument classification.  Proceedings of COLING 2012: Demonstration Papers, pages 207–214, COLING 2012, Mumbai, December 2012. 207  
輔助中文語篇語料庫開發的標記系統 標記詳細語篇資訊的語料庫，對於語篇研究有很大的幫助。在英文語言處理，目前已有公 眾可以取得的質量良好語篇語料庫。相較之下，中文領域尚未有這樣的公開資源。語篇標 記的工作需要投入相當的人力和時間，為了提高工作效率，我們開發了一套系統，透過網 頁介面，可以對中文語料標記詳細的語篇資訊。在本文中，我們首先回顧過去標記的成果， 指出根據中文的語言特性，需要特別考量的要點。針對這些要點，提出了一套高度彈性的 框架。在這套框架下，標記者將圈選出外顯或內隱、句內或跨句等各式各樣的語篇關係， 並且標上PDTB的三階語篇關係標籤。此外，每一個語篇實例的情緒資訊也一併標記，作 為將來進階研究之用。 KEYWORDS : Chinese Discourse Analysis, Corpus Annotation, Corpus Linguistics, Sentim ent Analysis KEYWORDS IN CHINESE : 中文語篇分析, 語料標記, 語料庫語言學, 情緒分析 Proceedings of COLING 2012: Demonstration Papers, pages 223–230, COLING 2012, Mumbai, December 2012. 223  
KEYWORDS : Sentiment Classification, Pollyanna Phenomena KEYWORDS IN MANDARIN : 情緒 析, 情緒 類, 波莉安娜效應 Proceedings of COLING 2012: Demonstration Papers, pages 231–238, COLING 2012, Mumbai, December 2012. 231  
Linked wordnets are invaluable linked lexical resources. Wordnet linking involves matching a particular synset (concept) in one wordnet to a synset in another wordnet. We have developed an automatic wordnet linking system that is divided into a number of stages. Starting with a synset in the ﬁrst language (also referred to as the source language), our algorithm generates a list of candidate synsets in the second language (also referred to as the target language). In consecutive stages, a heuristic is used to prune and rank this list. The winner synset is then chosen as the linkage for the source synset. The candidate synsets are generated using a bilingual dictionary (BiDict). Further, the earlier heuristics which we developed used BiDict to rank these candidate synsets. However, development of a BiDict is cumbersome and requires human labor. Furthermore, in several cases sparsity of the BiDict handicaps the ranking algorithm to a great extent. We have thus devised heuristics to eliminate the requirement of BiDict during the ranking process by using the already linked synsets. Once sufﬁcient number of linked synsets are available, these heuristics outperform our heuristics which use a BiDict. These heuristics are based on observations made from linking techniques applied by lexicographers. Our wordnet linking system can be used for any pair of languages, given either a BiDict or sufﬁcient number of already linked synsets. The interface of the system is easy to comprehend and use. In this paper, we present this interface along with the developed heuristics. KEYWORDS: Wordnet Linking, Bilingual Dictionary, Resource reuse for linking. Proceedings of COLING 2012: Demonstration Papers, pages 239–246, COLING 2012, Mumbai, December 2012. 239  
We have developed an online interface for running all the current state-of-the-art algorithms for WSD. This is motivated by the fact that exhaustive comparison of a new Word Sense Disambiguation (WSD) algorithm with existing state-of-the-art algorithms is a tedious task. This impediment is due to one of the following reasons: (1) the source code of the earlier approach is not available and there is a considerable overhead in implementing it or (2) the source code/binary is available but there is some overhead in using it due to system requirements, portability issues, customization issues and software dependencies. A simple tool which has no overhead for the user and has minimal system requirements would greatly beneﬁt the researchers. Our system currently supports 3 languages, viz., English, Hindi and Marathi, and requires only a web-browser to run. To demonstrate the usability of our system, we compare the performance of current state-of-the-art algorithms on 3 publicly available datasets. Keywords: WSD System. Proceedings of COLING 2012: Demonstration Papers, pages 247–254, COLING 2012, Mumbai, December 2012. 247  
Current state-of-the-art Word Sense Disambiguation (WSD) algorithms are mostly supervised and use the P (Sense|W ord) statistic for annotation. This P (Sense|W ord) statistic is obtained after training the model on an annotated corpus. The performance of WSD algorithms do not match the efﬁciency and quality of human annotation. It is therefore important to know the role of the contextual clues in WSD. Human beings in turn, actuate the task of disambiguating the sense of a word, by gathering hints from the context words in the neighbourhood of the word. Contextual clues thus form the basic building block for the human sense disambiguation task. The need was thus felt for a tool, which could help us get a deeper insight into the human mind, while disambiguating polysemous words. As mentioned earlier, in the human mind, sense disambiguation highly depends on ﬁnding clues in corpus text, which ﬁnally lead to a winner sense. In order to make WSD algorithms more efﬁcient, it is highly desirable to assimilate knowledge regarding contextual clues of words. In order to make WSD algorithms more efﬁcient, it is highly desirable to assimilate knowledge regarding contextual clues of words, which aid in ﬁnding correct senses of words in that context. Hence, we developed a tool which could help a lexicographer mark the clues for disambiguating a word in a context. In the current phase, this tool lets the lexicographer select the clues from the gloss and example ﬁelds in the synset, and adds them to a database. KEYWORDS: Sense discrimination, tool for generating discrimination-net. Proceedings of COLING 2012: Demonstration Papers, pages 261–266, COLING 2012, Mumbai, December 2012. 261  
 Joseph Max Kaufmann max.kaufmann@dac.us  Parallel corpora are an extremely useful tool in many natural language processing tasks, particularly statistical machine translation. Parallel corpora for certain language pairs, such as Spanish or French, are widely available, but for many language pairs, such as Bengali and Chinese, it is impossible to ﬁnd parallel corpora. Several tools have been developed to automatically extract parallel data from non–parallel corpora, but they use languagespeciﬁc techniques or require large amounts of training data. This paper demonstrates that maximum entropy classiﬁers can be used to detect parallel sentences between any language pairs with small amounts of training data. This paper is accompanied by JMaxAlign, a Java maxent classiﬁer which can detect parallel sentences. Keywords: Parallel Corpora, Comparable Corpora, Maximum Entropy Classiﬁers, Statis- tical Machine Translation.  Proceedings of COLING 2012: Demonstration Papers, pages 277–288, COLING 2012, Mumbai, December 2012. 277  
Social media, such as tweets on Twitter and Short Message Service (SMS) messages on cellular networks, are short-length textual documents (short texts or microblog posts) exchanged among users on the Web and/or their mobile devices. Automatic keyword extraction from short texts can be applied in online applications such as tag recommendation and contextual advertising. In this paper we present MIKE, a robust interactive system for keyword extraction from single microblog posts, which uses contextual semantic smoothing; a novel technique that considers term usage patterns in similar texts to improve term relevance information. We incorporate Phi coefﬁcient in our technique, which is based on corpus-based term-to-term relatedness information and successfully handles the shortlength challenge of short texts. Our experiments, conducted on multi-lingual SMS messages and English Twitter tweets, show that MIKE signiﬁcantly improves keyword extraction performance beyond that achieved by Term Frequency, Inverse Document Frequency (TFIDF). MIKE also integrates a rule-based vocabulary standardizer for multi-lingual short texts which independently improves keyword extraction performance by 14%. KEYWORDS: Keyword Extraction, Microblogs, Short texts, Semantic Smoothing, SMS, Romanized Urdu, MIKE. Proceedings of COLING 2012: Demonstration Papers, pages 289–296, COLING 2012, Mumbai, December 2012. 289  Type Message Tweet  Text aj friday hay is jaldi chutti hogayi, aur wassey mein mob lekare jata hun, tm ne kal program dekha tha kya? [It is Friday, therefore I got off early, otherwise I take mobile with me. Did you see the program yesterday?] Tweet 3x Lens Cap Keeper Holder with Elastic Band Loop Strap: US$6.93 End Date: Sunday Sep-05-2010 11:15:10 PDTBuy it N...http://bit.ly/cZXiSP  Table 1: Examples of short texts  
297  1. Introduction A review of the literature shows that no prior work has been done to classify the Punjabi documents. Therefore the objective of the work is to develop a system that takes Punjabi text documents as input and classify them into its corresponding classes using classification algorithm selected by user. These classes are: ਿਕਕਟ (krikaṭ) (Cricket), ਹਾਕੀ (hākī) (Hockey), ਕਬਡੀ (kabḍḍī) (Kabaddi), ਫੁਟਬਾਲ (phuṭbāl) (Football), ਟੈਿਨਸ (ṭainis) (Tennis), ਬੈਡਿਮੰ ਟਨ (baiḍmiṇṭan) (Badminton). These classes are defined by analyzing the Punjabi Corpus used for the classification task. And for classifying these documents, two new approaches are proposed for Punjabi language, Ontology Based Classification and Hybrid Approach. For Ontology Based Classification, Sports specific Ontology is manually created in Punjabi Language for the first time that consist of terms related to the class. E.g. Cricket class consists of following terms ਬਲੇਬਾਜ਼ੀ (batting), ਗਦਬਾਜ਼ੀ (bowling), ਫੀਲਿਡੰ ਗ (fielding), ਿਵਕਟ (wicket), Football class consist of ਗੋਲਕੀਪਰ (Goalkeeper), ਗੋਲ (Goal), ਫਾਰਵਰਡ (Forward), ਿਮਡਫੀਲਡਰ (Mid-fielder), ਿਡਫਡਰ (Defender) etc. An advantage of using Ontology is, there is no requirement of training set i.e. labeled documents and it can also be beneficial for developing other NLP applications in Punjabi. And to compare the efficiency of proposed algorithms, standard classification algorithms results, Naïve Bayes and Centroid Based Classification are compared using Fscore and Fallout.  2. Proposed algorithm for Punjabi Text Classification For Punjabi Text Classification, initials steps that need to do are following: • Prepare Training Set for Naïve Bayes and Centroid Based Classifier. The documents in the training set are tokenized and preprocessed. Stopwords, punctuations, special symbols, name entities are extracted from the document. • For each class, centroid vectors are created using training set. After initial steps, Punjabi Text Classification is implemented into three main phases: • Preprocessing Phase • Feature Extraction Phase • Processing Phase 2.1 Pre-processing Phase Each Unlabelled Punjabi Text Documents are represented as “Bag of Words”. Before classifying, stopwords, special symbols, punctuations (<,>, :,{,},[,],^,&,*,(,) etc.) are removed from the documents, as they are irrelevant to the classification task. Table 1 shows lists of some stopwords that are removed from the document. 
Open IE usually has been studied for English which of one of subject-verb-object(SVO) languages where a relation between two entities tends to occur in order of entity-relational phrase-entity within a sentence. However, in SOV languages, two entities occur before the relational phrase so that the subject and the relation have a long distance. The conventional methods for Open IE mostly dealing with SVO languages have difﬁculties of extracting relations from SOV style sentences. In this paper, we propose a new method of extracting relations from SOV languages. Our approach tries to solve long distance problems by identifying an entity-predicate pair and recognizing a relation within a predicate. Furthermore, we propose a post-processing approach using a language model, so that the system can detect more ﬂuent and precise relations. Experimental results on Korean corpus show that the proposed approach is effective in improving the performance of relation extraction. KEYWORDS: relation extraction, SOV language, predicate extraction. Proceedings of COLING 2012: Demonstration Papers, pages 305–312, COLING 2012, Mumbai, December 2012. 305  
We introduce a translation retrieval system THUTR, which casts translation as a retrieval problem. Translation retrieval aims at retrieving a list of target-language translation candidates that may be helpful to human translators in translating a given source-language input. While conventional translation retrieval methods mainly rely on parallel corpus that is difﬁcult and expensive to collect, we propose to retrieve translation candidates directly from target-language documents. Given a source-language query, we ﬁrst translate it into target-language queries and then retrieve translation candidates from target language documents. Experiments on Chinese-English data show that the proposed translation retrieval system achieves 95.32% and 92.00% in terms of P@10 at sentence level and phrase level tasks, respectively. Our system also outperforms a retrieval system that uses parallel corpus signiﬁcantly. TITLE AND ABSTRACT IN CHINESE THUTRµ˜‡È©u¢XÚ ·‚0 ˜‡È©u¢XÚTHUTR"TXÚò€ÈÀŠ•˜‡u¢¯K"È©u¢ ‘3•Ñ\ Šó© |¢˜|ÿÀ€È§•€È< Jø•Ï"DÚ È©u¢•{ Ì‡ÄuVŠŠ ¥§Ù¡ Ì‡¯K´ ïVŠŠ ¥“dp["•d§·‚JÑ¦ ^üŠŠ ¥¢yÈ©u¢"‰½ Šó Î§·‚ÄkòÙ€È¤8IŠó Î§, 2lüŠŠ ¥¥u¢ÿÀÈ©"3Ç=êâþ ¢ L²§·‚JÑ È©u¢XÚ© O3éf?ÚáŠ? 95.32%Ú92.00% P@10Š"·‚ XÚ•wÍ‡L ¦^²1 Š ¥ È©u¢XÚ" KEYWORDS: Translation retrieval, monolingual corpus, statistical machine translation. KEYWORDS IN CHINESE: È©u¢¶üŠŠ ¥¶ÚOÅì€È. ∗Chunyang Liu and Qi Liu have equal contribution to this work Proceedings of COLING 2012: Demonstration Papers, pages 321–328, COLING 2012, Mumbai, December 2012. 321  
Brazilian Portuguese needs a Wordnet that is open access, downloadable and changeable, so that it can be improved by the community interested in using it for knowledge representation and automated deduction. This kind of resource is also very valuable to linguists and computer scientists interested in extracting and representing knowledge obtained from texts. We discuss brieﬂy the reasons for a Brazilian Portuguese Wordnet and the process we used to get a preliminary version of such a resource. Then we discuss possible steps to improving our preliminary version.1 KEYWORDS: Wordnet, Portuguese, lexical resources, SUMO Ontology. KEYWORDS IN L2: Wordnet, Português, recursos léxicos, SUMO. 1Acknowledgments: We would like to thank Francis Bond for help in getting OpenWordNet-PT online and for general discussions. We also thank Adam Pease for introducing us and helping to start this project. Proceedings of COLING 2012: Demonstration Papers, pages 353–360, COLING 2012, Mumbai, December 2012. 353  
The WordNets for many ofﬁcial Indian languages are being developed by the members of the IndoWordNet Consortium in India. It was decided that all these WordNets be made open for public use and feedback to further improve their quality and usage. Hence each member of the IndoWordNet Consortium had to develop their own website and deploy their WordNets online. In this paper, the Content Management System (CMS) based approach used to speed up the WordNet website development and deployment activity is presented. The CMS approach is database driven and dynamically creates the websites with minimum input and effort from the website creator. This approach has been successfully used for the deployment of WordNet websites with friendly user interface and all desired functionalities in very short time for many Indian languages. KEYWORDS: WordNet, Content Management System, CMS, WordNet CMS, IndoWordNet, Application Programming Interface, API, WordNet Templates, IndoWordNet Database, Website. Proceedings of COLING 2012: Demonstration Papers, pages 361–368, COLING 2012, Mumbai, December 2012. 361  
Proceedings of COLING 2012: Demonstration Papers, pages 369–376, COLING 2012, Mumbai, December 2012. 369  
Proceedings of COLING 2012: Demonstration Papers, pages 377–384, COLING 2012, Mumbai, December 2012. 377  
Keywords: Dialectal Arabic, Arabic Natural Language Processing, Machine Translation, Rule- Based Machine Translation, Morphology. Keywords in L2: Proceedings of COLING 2012: Demonstration Papers, pages 385–392, COLING 2012, Mumbai, December 2012. 385  
Proceedings of COLING 2012: Demonstration Papers, pages 393–400, COLING 2012, Mumbai, December 2012. 393  1. Introduction The current study was undertaken specifically for clustering of text documents in Punjabi Language as no prior work has been done in this language as per review of literature done to carry out this study. It is an attempt in this direction to provide a solution for text clustering in Punjabi Language, by developing a new hybrid approach of text clustering, which will be immensely useful to the researchers who wish to undertake study and research in vernacular languages. The study proposed and implemented a new algorithm for clustering of Punjabi text documents by combining best features of the text clustering algorithms i.e. Clustering with frequent Item Sets, Clustering with Frequent Word Sequences, keeping in view the semantics of Punjabi language. Efficiency of the three algorithms for Punjabi Text Document Clustering was compared using Precision, Recall & F-Measure. 2. Proposed approach for Punjabi text clustering 
ABSTRACT The purpose of this demo is to introduce the linguistic development tool NooJ. The tool has been in development for a number of years and it has a solid community of computational linguists developing grammars in two dozen languages ranging from Arabic to Vietnamese1. Despite its manifest capabilities and reputation, its appeal within the wider HLT community was limited by the fact that it was confined to the .NET framework and it was not open source. However, under the auspices of the CESAR project it has recently been turned open source and a JAVA and a MONO version have been produced. In our view this significant development justifies a concise but thorough description of the system, demonstrating its potential for deployment in a wide variety of settings and purposes. The paper describes the history, the architecture, main functionalities and potential of the system for teaching, research and application development. KEYWORDS : NooJ, Finite-State NLP, Local Grammars, Linguistic Development Tools, INTEX, Information extraction, text mining 
Proceedings of COLING 2012: Demonstration Papers, pages 409–416, COLING 2012, Mumbai, December 2012. 409  
Proceedings of COLING 2012: Demonstration Papers, pages 417–422, COLING 2012, Mumbai, December 2012. 417  
The paper presents a web-based application of semantic networks to model Bulgarian inﬂectional morphology. It demonstrates the general ideas, principles, and problems of inﬂectional grammar knowledge representation used for encoding Bulgarian inﬂectional morphology in Universal Networking Language (UNL). The analysis of UNL formalism is outlined in terms of its expressive power to present inﬂection, and the principles and related programming encodings are explained and demonstrated. KEYWORDS: Morphology and POS tagging, Grammar and formalisms, Underresourced lan- guages. Proceedings of COLING 2012: Demonstration Papers, pages 423–430, COLING 2012, Mumbai, December 2012. 423  
KEYWORDS : language resources, language tools, language services, CESAR, META-NET, META-SHARE, Bulgarian, Croatian, Hungarian, Polish, Serbian, Slovakian Proceedings of COLING 2012: Demonstration Papers, pages 431–438, COLING 2012, Mumbai, December 2012. 431  
KEYWORDS: commonsense knowledge, knowledge-base construction. Proceedings of COLING 2012: Demonstration Papers, pages 439–446, COLING 2012, Mumbai, December 2012. 439  
This paper attempts to deal with a ranking problem with a collection of ﬁnancial reports. By using the text information in the reports, we apply learning-to-rank techniques to rank a set of companies to keep them in line with their relative risk levels. The experimental results show that our ranking approach signiﬁcantly outperforms the regression-based one. Furthermore, our ranking models not only identify some ﬁnancially meaningful words but suggest interesting relations between the text information in ﬁnancial reports and the risk levels among companies. Finally, we provide a visualization interface to demonstrate the relations between ﬁnancial risk and text information in the reports. This demonstration enables users to easily obtain useful information from a number of ﬁnancial reports. KEYWORDS: Text Ranking, Stock Return Volatility, Financial Report, 10-K Corpus. Proceedings of COLING 2012: Demonstration Papers, pages 447–452, COLING 2012, Mumbai, December 2012. 447  
This paper presents a tool for assisting users in composing texts in a language they do not know. While Machine Translation (MT) is pretty useful for understanding texts in an unfamiliar language, current MT technology has yet to reach the stage where it can be used reliably without a post-editing step. This work attempts to make a step towards achieving this goal. We propose a tool that provides suggestions for the continuation of the text in the source language (that the user knows), creating texts that can be translated to the target language (that the user does not know). In terms of functionality, our tool resembles text prediction applications. However , the target language, through a Statistical Machine Translation (SMT) model, drives the composition and not only the source language. We present the user interface and describe the considerations that underline the suggestion process. A simulation of user interaction shows that composition speed can be substantially reduced and provides initial positive feedback as to the ability to generate better translations. KEYWORDS: Statistical Machine Translation. Proceedings of COLING 2012: Demonstration Papers, pages 459–466, COLING 2012, Mumbai, December 2012. 459  
Large amounts of knowledge exist in the user-generated contents of web communities. Generating questions from such community contents to form the question-answer pairs is an effective way to collect and manage the knowledge in the web. The parser or rule based question generation (QG) methods have been widely studied and applied. Statistical QG aims to provide a strategy to handle the rapidly growing web data by alleviating the manual work. This paper proposes a deep belief network (DBN) based approach to address the statistical QG problem. This problem is considered as a three-step task: question type determination, concept selection and question construction. The DBNs are introduced to generate the essential words for question type determination and concept selection. Finally, a simple rule based method is used to construct questions with the generated words. The experimental results show that our approach is promising for the web community oriented question generation. KEYWORDS: statistical question generation, deep belief network, web community. Proceedings of COLING 2012: Demonstration Papers, pages 467–474, COLING 2012, Mumbai, December 2012. 467  
MT-­‐postediting, translation quality evaluation, parallel corpus production   WANG Ling Xiao, ZHANG Ying, Christian BOITET, Valerie BELLYNCK Lingxiao.Wang@imag.fr, Ying.Zhang@imag.fr, Christian.Boitet@imag.fr, Valerie.Bellynck@imag.fr ABSTRACT An interactive Multilingual Access Gateway (iMAG) dedicated to a web site S (iMAG-S) is a good tool to make S accessible in many languages immediately and without editorial responsibility. Visitors of S as well as paid or unpaid post-editors and moderators contribute to the continuous and incremental improvement of the most important textual segments, and eventually of all. Pre-translations are produced by one or more free MT systems. Continuous use since 2008 on many web sites and for several access languages shows that a quality comparable to that of a first draft by junior professional translators is obtained in about 40% of the (human) time, sometimes less. There are two interesting side effects obtainable without any added cost: iMAGs can be used to produce high-quality parallel corpora and to set up a permanent task-based evaluation of multiple MT systems. We will demonstrate (1) the multilingual access to a web site, with online postediting of MT results "à la Google", (2) postediting in "advanced mode", using SECTra_w as a back-end, enabling online comparison of MT systems, (3) task-oriented built-in evaluation (postediting time), and (4) application to a large web site to get a trilingual parallel corpus where each segment has a reliability level and a quality score. KEYWORDS: Online post-editing, interactive multilingual access gateway, free MT evaluation TITLE AND ABSTRACT IN CHINESE iMAG功 能 展 示 :   机 器 翻 译 后 编 辑 ， 翻 译 质 量 评 估 ， 平 行 语 料 生 成   简述 一个iMAG (interactive Multilingual Access Gateway, 多语言交互式网关) 是很好的面向一个 网站的工具，它可以提供对该网站的多语言访问，并且无需任何编辑。通过iMAG访问该 网站的用户，可以作为有偿或无偿的后编辑人员或是管理者，来对该网站的文本段进行可 持续的、增量的改进。该网站的预翻译是由一个或多个免费的MT系统提供的。自从2008 年以来，通过iMAG对多个网站进行多语言的持续访问结果表明，对于相对翻译质量，首 轮由初级翻译者提供的翻译，使用iMAG只占纯人工翻译40%的时间，或更少。iMAG有两 个非常吸引人的方面并且无需额外成本：iMAG能用于产生高质量的平行语料，而且可以 通过多个MT系统对其进行长久性的评估。我们将要展示：(1) 多语言访问目标网站，并对 Google提供的预翻译进行在线后编辑，(2) 后编辑的高级模式，SECTra作为后台模块，可 实现MT系统的在线比较，(3) 面向任务的评估 （后编辑时间），和 (4) 应用到大型网站， 可获得三种语言的平行语料，每个文字段都拥有可靠性和质量的评分。 关键词：在线后编辑，多语言交互网关，免费MT评估 Proceedings of COLING 2012: Demonstration Papers, pages 475–482, COLING 2012, Mumbai, December 2012. 475  
We present Jane 2, an open source toolkit supporting both the phrase-based and the hierarchical phrase-based paradigm for statistical machine translation. It is implemented in C++ and provides efﬁcient decoding algorithms and data structures. This work focuses on the description of its phrase-based functionality. In addition to the standard pipeline, including phrase extraction and parameter optimization, Jane 2 contains several state-of-the-art extensions and tools. Forced alignment phrase training can considerably reduce rule table size while learning the translation scores in a more principled manner. Word class language models can be used to integrate longer context with a reduced vocabulary size. Rule table interpolation is applicable for different tasks, e.g. domain adaptation. The decoder distinguishes between lexical and coverage pruning and applies reordering constraints for efﬁciency. KEYWORDS: statistical machine translation, open source toolkit, phrase-based translation, hierarchical translation. Proceedings of COLING 2012: Demonstration Papers, pages 483–492, COLING 2012, Mumbai, December 2012. 483  
In this paper, we propose a fully automatic system for acquisition of hypernym/hyponymy relations from large corpus in Turkish Language. The method relies on both lexico-syntactic pattern and semantic similarity. Once the model has extracted the seeds by using patterns, it applies similarity based expansion in order to increase recall. For the expansion, several scoring functions within a bootstrapping algorithm are applied and compared. We show that a model based on a particular lexico-syntactic pattern for Turkish Language can successfully retrieve many hypernym/hyponym relations with high precision. We further demonstrate that the model can statistically expand the hyponym list to go beyond the limitations of lexico-syntactic patterns and get better recall. During the expansion phase, the hypernym/hyponym pairs are automatically and incrementally extracted depending on their statistics by employing various association measures and graph-based scoring. In brief, the fully automatic model mines only a large corpus and produces is-a relations with promising precision and recall. To achieve this goal, several methods and approaches were designed, implemented, compared and evaluated. KEYWORDS: hypernym/hyponym, lexico-syntactic patterns. Proceedings of COLING 2012: Demonstration Papers, pages 493–500, COLING 2012, Mumbai, December 2012. 493  
The web provides a huge collection of web pages for researchers to study natural languages. However, processing web scale texts is not an easy task and needs many computational and linguistic resources. In this paper, we introduce two Chinese parts-of-speech tagged web-scale datasets and describe tools that make them easy to use for NLP applications. The first is a Chinese segmented and POS-tagged dataset, in which the materials are selected from the ClueWeb09 dataset. The second is a Chinese POS n-gram corpus extracted from the POS-tagged dataset. Tools to access the POS-tagged dataset and the POS n-gram corpus are presented. The two datasets will be released to the public along with their tools. 中文網路規模語言資料集和工具 簡網記集性料單際的集選標和的網網自記詞工路路於資性作提規料Clnu供模，集e-gW資而研中rae料是究擷mb0語集需人取9的料要員，中中庫大以巨文文的量及量詞語工的易網性料具計於頁n-，，g算進將ra這並和行這m兩經，語些自種過所言資然資中建資源語料文立源運言集斷的。處用連詞語在理於同和料本研自工詞庫文究然具性。中語，將標我言但，提記們處是我供。同理處們研第時理介應究二也網紹用人種提路兩的員資出規種工使料模具加搜用集的上尋。。是文詞第中由本一文性上不詞種標是述性資記件詞標料資  中文詞性 工具包 KEYWORDS : Chinese POS n-gram, ClueWeb09, TOOLKIT  KEYWORDS IN L2 :  n-gram, ClueWeb09,  Proceedings of COLING 2012: Demonstration Papers, pages 501–508, COLING 2012, Mumbai, December 2012. 501  
Current concatenative morphological analyzers consider preﬁx, sufﬁx and stem morphemes based on lexicons of morphemes, and morpheme concatenation rules that determine whether preﬁx-stem, stem-sufﬁx, and preﬁx-sufﬁx concatenations are allowed. Existing afﬁx lexicons contain extensive redundancy, suffer from inconsistencies, and require signiﬁcant manual work to augment with clitics and partial afﬁxes if needed. Unlike traditional work, our method considers Arabic afﬁxes as fusional and agglutinative, i.e. composed of one or more morphemes, introduces new compatibility rules for afﬁx-afﬁx concatenations, and reﬁnes the lexicons of the SAMA and BAMA analyzers to be smaller, less redundant, and more consistent. It also automatically and perfectly solves the correspondence problem between the segments of a word and the corresponding tags, e.g. part of speech and gloss tags. Title and Abstract in another language, L2 (optional, and on same page)    ©  ©        ©  ©    étk eÓ u d F  ét¯å   « dñ¯  È eÒªt eu F   étu ªË d F  ñtË  ú¯ åË d   ÉtÊjtË d   SAMA BAMA ÑðFeÉ  ø eèÒt©k j ek  îðtFF©hªeîf ©ª)Óh©uðªÓÓðtFuFdeø «t©ê0Fj r Ðdk tñðF FªFîed¯©uuFdhg ©úÓÉ « teÍDtË©f dédr j ÊFuªFdh îéf tFhÊÒá ©t©e¾©eÓ t»ÊuÓFm® &)9tÐe àÓËðî©dFiFk ñ®É  º r  k g uÓð ÖÊËg Ïeeª¿ddmðe&tËtF©9©tduFÉËfø ¯dg ñ¾edÓÐÓd ðÊðªde¬ tF)FÉ tËðé©ÒªúdÒg tÊu¯©Ò©ÊËÈdem¾©»&tåeË9ËáF d©dÓeÓîFf e)É ieFt©dàîj ©úd hFéñqËé©î®½ dehmdº % ªj ©ÓËus éÓðÊ©tFÖÉ»Ïud ðá©m©&kdr mÓ9&FDº9mø &eer ®t 90Ëè©td»Ê)eer åªu®Fddát©ÊËFeÓeá ªá edªuð©©©®ñtuÖÓÏÓñºdé0eº ttÒte©éº ¯éu©ëÊtt¯ð©ËËuËFåede 0á tªÈ©»Ë©©ø uFdË0deddð tñéuáú©Òé©t)k dº ÍtmÊd&Ë¾Fd9edÖ Ëßº ðdáé©é«)kt0ufdFËuñdeée©mÉ dmÌ¯k tÌeðk 9Fu9F¯éddFddtðeá ¯ÓÑÑ©á©È©0k k å0e®uFFg Fuª©©ÊeedËªª ªd ÖtÓr ÏúÓdr d¯© eFeê g ®eÊÑÒÊªÊ©ªtj m©ªj %«tF tFÖÖetË©ÏÏuËFddd  Keywords: morphology; lexicons; computational linguistics; Arabization; afﬁx.  Keywords in L : . 2   úÍ e) f d    jÊÖÏ d  Y  sF u   ªtË  d  Y úq emÌ 9d F  ©  ét(Ë f d  ÕÎ«  Y ÑkF eªÖÏ d  Y  © ú¯  åË  d     ÉtÊjtË d  Proceedings of COLING 2012: Demonstration Papers, pages 517–526, COLING 2012, Mumbai, December 2012. 517  Table 1: Partial preﬁx lexicon BAMA v1.2      étÓ eÓ f d  r    e®jÊÒÊË  ù u© kF  ÑjF ªÓ  ø  euF   ÊªtÓ  Preﬁx  © ¯   É¾ Ó Vocalized © ¯  u © t¯ t © t¯  u © t¯ t © t¯   © ét¯ Category Pref-Wa IVPref-hw-ya IVPref-hw-ya IVPref-hw-ya IVPref-hw-ya  ©    ø ñtªÓ   tÊªu  Gloss  and/so  he/it  and/so + he/it  will + he/it  and/so + will + he/it  Ð g¾Ë d  á© Ó   ©¯ñÓ  POS  fa/CONJ+  ya/IV3MS+  fa/CONJ+ya/IV3MS+  sa/FUT+ya/IV3MS+  fa/CONJ+sa/FUT+ya/IV3MS+  u  u  IVPref-hmA-ya  they (both)  ya/IV3MD+  © t¯  © t¯  IVPref-hmA-ya  and/so + they (both)  fa/CONJ+ya/IV3MD+  t  t  IVPref-hmA-ya  will + they (both)  sa/FUT+ya/IV3MD+  © t¯  © t¯  IVPref-hmA-ya and/so + will + they (both) fa/CONJ+sa/FUT+ya/IV3MD+  ð  ð  Pref-Wa  and  wa/CONJ+  u ð  u ð  IVPref-hw-ya  and + he/it  wa/CONJ+ya/IV3MS+  tð  tð  IVPref-hw-ya  and + will + he/it  wa/CONJ+sa/FUT+ya/IV3MS+  u ð  u ð  IVPref-hmA-ya  and + they (both)  wa/CONJ+ya/IV3MD+  tð  tð  IVPref-hmA-ya and + will + they (both) wa/CONJ+sa/FUT+ya/IV3MD+  
Proceedings of COLING 2012: Demonstration Papers, pages 527–534, COLING 2012, Mumbai, December 2012. 527  
Conversion between different grammar frameworks is of great importance to comparative performance analysis of the parsers developed based on them and to discover the essential nature of languages. This paper presents an approach that converts Combinatory Categorial Grammar (CCG) derivations to Penn Treebank (PTB) trees using a maximum entropy model. Compared with previous work, the presented technique makes the conversion practical by eliminating the need to develop mapping rules manually and achieves state-of-the-art results. Keywords: CCG, Grammar conversion. ∗ This work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119 and Grant No. 61170114), the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), and the European Union Seventh Framework Program (Grant No. 247619). † Corresponding author Proceedings of COLING 2012: Demonstration Papers, pages 535–542, COLING 2012, Mumbai, December 2012. 535  
This paper presents an efﬁcient method for approximate string matching against a lexicon. We deﬁne a ﬁlter that for each source word selects a small set of target lexical entries, from which the best match is then selected using generalized edit distance, where edit operations can be assigned an arbitrary weight. The ﬁlter combines a specialized hash function with best-ﬁrst search. Our work extends and improves upon a previously proposed hash-based ﬁlter, developed for matching with uniform-weight edit distance. We evaluate an approximate matching system implemented with the new best-ﬁrst ﬁlter, by conducting several experiments on a historical corpus and a set of weighted rules taken from the literature. We present running times and discuss how performance varies using different stopping criteria and target lexica. The results show that the ﬁlter is suitable for large rule sets and million word corpora, and encourage further development. KEYWORDS: Approximate string matching, generalized edit distance, anagram hash, spelling variation, historical corpora. Proceedings of COLING 2012: Posters, pages 13–22, COLING 2012, Mumbai, December 2012. 13  
In this work we present an approach for extracting parallel phrases from comparable news articles to improve statistical machine translation. This is particularly useful for under-resourced languages where parallel corpora are not readily available. Our approach consists of a phrase pair generator that automatically generates candidate parallel phrases and a binary SVM classiﬁer that classiﬁes the candidate phrase pairs as parallel or non-parallel. The phrase pair generator is also used to automatically create training and testing data for the SVM classiﬁer from parallel corpora. We evaluate our approach using English-German, English-Greek and English-Latvian language pairs. The performance of our classiﬁer on the test sets is above 80% precision and 97% accuracy for all language pairs. We also perform an SMT evaluation by measuring the impact of phrases extracted from comparable corpora on SMT quality using BLEU. For all language pairs we obtain signiﬁcantly better results compared to the baselines. KEYWORDS: SMT for under-resourced languages, phrase extraction from comparable corpora. Proceedings of COLING 2012: Posters, pages 23–32, COLING 2012, Mumbai, December 2012. 23  
Proceedings of COLING 2012: Posters, pages 33–42, COLING 2012, Mumbai, December 2012. 33  Introduction While languages differ greatly in their “surface structures”, they all share a common “deep structure”; hence came the idea of creating a universal representation capable of conveying this deep structure while enjoying the regularity and predictability natural languages lack. Although interlingua is a promising idea, the number of interlinguas created is still very limited. Examples of well-known interlinguas are DLT (Witkam 2006), UNITRAN (Dorr (1987, 1990) and (Dorr et al. (2004)), KANT (Nyberg and Mitamura (1992), Nyberg et al. (1997)) and UNL (Uchida 1996, Uchida and Zhu (1993, 2005), Alansary et al. (2010)). The first three of these interlinguas lack standardization, however, the fourth, UNL, has succeeded in standardizing its tools, tagset and methodology as well as rely on meaning as an intermediate representation (Alansary 2011). UNL is a kind of mark-up language which represents the core information of a text. The UNDL Foundation, the founder of UNL, has created a wrapper application for development of various UNL tools and applications (Martins 2012, Martins and Avatesyan 2009). All engines, resources and tools are available through the UNLweb (www.unlweb.net) that contains many tools designed for linguists, computational linguists as well as non-professionals. These tools are used in analysing and generating natural languages. IAN, the Interactive ANalyzer, it employs the analysis grammar rules to analyze input and finally generate its corresponding UNL expressions. It operates semi-automatically; word-sense disambiguation is still carried out by a language specialist, nevertheless, the system can filter the candidates using an optional set of disambiguation rules. EUGENE (the dEep-to-sUrface natural language GENErator) is a fully automatic engine, it simply uses the target language grammar rules in order to decode the incoming UNL document and generate it in natural languages. IAN and Eugene use two types of Natural language dictionaries; enumerative and generative. The enumerative dictionary of IAN contains all inflected word forms of a language together with their corresponding Universal Words (concepts) and a set of linguistic features covering different linguistic levels. The generative dictionary, on the other hand, is the same as the ‘enumerative’ one but it contains all lexemes of language as bases together with a morphological paradigm number that controls the generative morphological behaviour (e.g. agreement and inflected forms) of words in natural language (Martins and Avetisyan 2009). It might be a fact that all languages have classical reference grammars in grammar books. Such a reference grammar maybe defined as a description of the grammar of a language, with explanations of the principles governing the construction of words, phrases, clauses, and sentences. It is designed to give someone a reference tool for looking up specific details of the language. In Natural Language Processing, computers should also learn a language in order to give a comprehensive and objective test-bed that enables us to evaluate, compare and follow up the performance of different grammars. A formalized reference grammar is needed in order to synchronize different languages; the UNL is initiating this idea as it utilizes a standardized environment. The current paper is limited to English and Arabic only; it is organized as follows: Section 1 discusses the design and compilation of the reference corpus. Section 2 discusses the design and implementation of the analysis grammar. Section 3 discusses the design and implementation of the generation grammar. Section 4 evaluates the analysis and generation results in English and Arabic. And finally section 5 is a conclusion and future work. 
We propose an algorithm for the generation of referring expressions (REs) that adapts the approach of Areces et al. (2008, 2011) to include overspeciﬁcation and probabilities learned from corpora. After introducing the algorithm, we discuss how probabilities required as input can be computed for any given domain for which a suitable corpus of REs is available, and how the probabilities can be adjusted for new scenes in the domain using a machine learning approach. We exemplify how to compute probabilities over the GRE3D7 corpus of Viethen (2011). The resulting algorithm is able to generate different referring expressions for the same target with a frequency similar to that observed in corpora. We empirically evaluate the new algorithm over the GRE3D7 corpus, and show that the probability distribution of the generated referring expressions matches the one found in the corpus with high accuracy. KEYWORDS: Generation of referring expressions, reﬁnement algorithms, machine-learning. Proceedings of COLING 2012: Posters, pages 53–62, COLING 2012, Mumbai, December 2012. 53  
Cross-Lingual Sentiment Analysis (CLSA) is the task of predicting the polarity of the opinion expressed in a text in a language Ltest using a classiﬁer trained on the corpus of another language Ltrain. Popular approaches use Machine Translation (MT) to convert the test document in Ltest to Ltrain and use the classiﬁer of Ltrain. However, MT systems do not exist for most pairs of languages and even if they do, their translation accuracy is low. So we present an alternative approach to CLSA using WordNet senses as features for supervised sentiment classiﬁcation. A document in Ltest is tested for polarity through a classiﬁer trained on sense marked and polarity labeled corpora of Ltrain. The crux of the idea is to use the linked WordNets of two languages to bridge the language gap. We report our results on two widely spoken Indian languages, Hindi (450 million speakers) and Marathi (72 million speakers), which do not have an MT system between them. The sense-based approach gives a CLSA accuracy of 72% and 84% for Hindi and Marathi sentiment classiﬁcation respectively. This is an improvement of 14%-15% over an approach that uses a bilingual dictionary. KEYWORDS: Sentiment Analysis, Cross Lingual Sentiment Analysis, Linked Wordnets, Semantic Features, Sense Space. Proceedings of COLING 2012: Posters, pages 73–82, COLING 2012, Mumbai, December 2012. 73  
(3) Sholokhov Moscow State University for the Humanities, Moscow, 3-ya Vladimirskaya St. 5, room 35 (4) Institute for Linguistic Research of the Russian Academy of Sciences, St. Petersburg, Tuchkov per. 9 tarkhangelskiy@hse.ru, belyaev@iling-ran.ru, senjacom@gmail.com ABSTRACT This paper is devoted to the use of two tools for creating morphologically annotated linguistic corpora: UniParser and the EANC platform. The EANC platform is the database and search framework originally developed for the Eastern Armenian National Corpus (www.eanc.net) and later adopted for other languages. UniParser is an automated morphological analysis tool developed specifically for creating corpora of languages with relatively small numbers of native speakers for which the development of parsers from scratch is not feasible. It has been designed for use with the EANC platform and generates XML output in the EANC format. UniParser and the EANC platform have already been used for the creation of the corpora of several languages: Albanian, Kalmyk, Lezgian, Ossetic, of which the Ossetic corpus is the largest (5 million tokens, 10 million planned for 2013), and are currently being employed in construction of the corpora of Buryat and Modern Greek languages. This paper will describe the general architecture of the EANC platform and UniParser, providing the Ossetic corpus as an example of the advantages and disadvantages of the described approach. KEYWORDS : corpus linguistics, automated morphological analysis, language documentation, Iranian languages, Ossetic Proceedings of COLING 2012: Posters, pages 83–92, COLING 2012, Mumbai, December 2012. 83  
Proceedings of COLING 2012: Posters, pages 93–102, COLING 2012, Mumbai, December 2012. 93  
A spelling error detection and correction application is based on three main components: a dictionary (or reference word list), an error model and a language model. While most of the attention in the literature has been directed to the language model, we show how improvements in any of the three components can lead to significant cumulative improvements in the overall performance of the system. We semi-automatically develop a dictionary of 9.3 million fully inflected Arabic words using a morphological transducer and a large corpus. We improve the error model by analysing error types and creating an edit distance based re-ranker. We also improve the language model by analysing the level of noise in different sources of data and selecting the optimal subset to train the system on. Testing and evaluation experiments show that our system significantly outperforms Microsoft Word 2010, OpenOffice Ayaspell and Google Docs. TITLE IN ARABIC ‫تحسين اكتشاف وتصحيح الأخطاء الإملائية في اللغة العربية‬ ABSTRACT IN ARABIC ‫ ونموذج‬،)‫ قاموس (أو قائمة كلمات مرجعية‬:‫تقوم تطبيقات اكتشاف وتصحيح الأخطاء الإملائية على ثلاث مكونات رئيسية وهي‬ ‫ فإننا نبين أن التحسينات التي يتم إدخالها على‬،‫ وبينما ينصب الاهتمام في الأبحاث العلمية على نموذج اللغة‬.‫الخطأ ونموذج اللغة‬ ‫ وقد قمنا في هذا البحث بتطوير قاموس‬.‫المكونات الثلاثة يمكن أن تؤدي إلى تحسينات تراكمية في النتائج النهائية لأداء البرنامج‬ ‫ وقمنا بتحسين‬.‫ مليون مصرفة تصريفا كاملا كلمة بشكل شبه آلي باستخدام محلل صرفي وذخيرة نصوص كبيرة‬9.3 ‫يتكون من‬ ‫نموذج الخطأ عن طريق تحليل أنواع الأخطاء الإملائية وتطوير وسيلة لإعادة ترتيب المقترحات الناتجة عن خوارزمية مسافة‬ ‫ وقمنا كذلك بتحسين نموذج اللغة عن طريق تحليل نسبة الأخطاء في مصادر البيانات المختلفة واختيار الجزء المثالي‬.‫التحرير‬ ‫ وأوبن‬0202 ‫ وتبين تجارب الاختبار والتقييم أن البرنامج يتفوق بشكل كبير على مايكروسوفت أوفيس‬.‫لتدريب البرنامج عليه‬ .‫أوفيس وملفات جوجل‬ KEYWORDS : Arabic, spelling error detection and correction, finite state morphological generation, Arabic spell checker, spelling error model, Arabic word list KEYWORDS IN ARABIC: ‫ قائمة‬،‫ التدقيق الإملائي‬،‫ التوليد الصرفي باستخدام آلات الحالة المحدودة‬،‫ اكتشاف وتصحيح الأخطاء الإملائية‬،‫اللغة العربية‬ ‫كلمات اللغة العربية‬  103  Proceedings of COLING 2012: Posters, pages 103–112, COLING 2012, Mumbai, December 2012.  
Statistical machine translation has been remarkably successful for the world’s well-resourced languages, and much effort is focussed on creating and exploiting rich resources such as treebanks and wordnets. Machine translation can also support the urgent task of documenting the world’s endangered languages. The primary object of statistical translation models, bilingual aligned text, closely coincides with interlinear text, the primary artefact collected in documentary linguistics. It ought to be possible to exploit this similarity in order to improve the quantity and quality of documentation for a language. Yet there are many technical and logistical problems to be addressed, starting with the problem that – for most of the languages in question – no texts or lexicons exist. In this position paper, we examine these challenges, and report on a data collection effort involving 15 endangered languages spoken in the highlands of Papua New Guinea. KEYWORDS: endangered languages, documentary linguistics, language resources, bilingual texts, comparative lexicons.  125  Proceedings of COLING 2012: Posters, pages 125–134, COLING 2012, Mumbai, December 2012.  
We ﬁll a gap in the systematically analyzed space of available techniques for state-of-the-art dependency parsing by comparing non-projective strategies for graph-based parsers. Using three languages with varying frequency of non-projective constructions, we compare the non-projective approximation algorithm with pseudo-projective parsing. We also analyze the differences between different encoding schemes for pseudo-projective parsing. We ﬁnd only minor differences between the encoding schemes for pseudo-projective parsing, and that the non-projective approximation algorithm is superior to pseudo-projective parsing. KEYWORDS: Multilingual Dependency Parsing, Non-projective Parsing, Pseudo-projective Parsing.  135  Proceedings of COLING 2012: Posters, pages 135–144, COLING 2012, Mumbai, December 2012.  
In this paper, we investigate whether the social goals of an individual can be recognized through analysis of the social actions indicated by their use of language. Speciﬁcally, we focus on recognizing when someone is pursuing power within a web forum. Individuals pursue power in order to increase their control over the actions and goals of the group. We cast the problem as social conversational entailment where we determine if a dialogue entails a hypothesis which states a dialogue participant is in pursuit of power. In the social conversational entailment framework the hypothesis is decomposed into a series of social commitments which deﬁne series of actions and responses that are indicative of the hypothesis. The social commitments are modeled as social acts which are pragmatic speech acts. We identify nine culturally neutral psychologically-motivated social acts that can be detected in language and are indicative of whether an individual is pursuing power. Our best results using social conversational entailment achieve an overall F-measure of 79.7% for predicting pursuit of power for English speakers and 78.3% for Chinese speakers. Keywords: dialogue, power, social actions, entailment, online communication, culture, norms.  155  Proceedings of COLING 2012: Posters, pages 155–164, COLING 2012, Mumbai, December 2012.  
This paper tackles the problem of polar vocabulary ambiguity. While some opinionated words keep their polarity in any context and/or across any domain (except for the ironic style that goes beyond the present article), some other have an ambiguous polarity which is highly dependent of the context or the domain: in this case, the opinion is generally carried by complex expressions (“patterns”) rather than single words. In this paper, we propose and evaluate an original hybrid method, based on syntactic information extraction and clustering techniques, to learn automatically such patterns and integrate them into an opinion detection system. TITLE AND ABSTRACT IN FRENCH Apprentissage de patrons polarisés pour la détection contextuelle d’opinions Cet article se penche sur le problème de l’ambiguïté du vocabulaire de polarité. Alors que certains mots conservent la même polarité dans n’importe quel contexte ou domaine (à l’exception du registre ironique qui va au-delà du présent article), d’autres ont une polarité ambiguë dépendante du contexte ou du domaine : dans ce cas l’opinion est portée par des expressions complexes (patrons) et non des mots isolés. Dans cet article, nous proposons et évaluons une méthode hybride originale, utilisant de l’information syntaxique et des techniques de « clusterisation », pour apprendre automatiquement de tels patrons et les intégrer à un système de détection d’opinions. KEYWORDS: opinion detection, polar vocabulary ambiguity, hybrid method KEYWORDS IN FRENCH: détection d’opinions, ambiguïté du vocabulaire de polarité, méthode hybride  165  Proceedings of COLING 2012: Posters, pages 165–174, COLING 2012, Mumbai, December 2012.  Introduction A fundamental task in opinion mining is classifying the polarity of a given text, sentence or feature/aspect level to find out whether it is positive, negative or neutral. Different methodologies using NLP and machine learning techniques are used for this purpose. The most fine grained analysis model is the feature based sentiment mining method. Feature based opinion mining aims at to determining the sentiments or opinions that are expressed on different features or aspects of entities (e.g. [Bloom et al. 2007]). The context of this paper is the development of a feature-based opinion mining system, for French. One of the essential tasks in the course of this development is the acquisition of polar vocabulary, for which one encounters almost immediately the problem of polarity ambiguity. In the present paper, we try to address this particular problem: while some opinionated words keep their polarity in any context and/or across any domain (except for the ironic style that goes beyond the scope of the present article), some other have an ambiguous polarity and are highly dependent of the context or the domain. In this case, the opinion is generally carried by complex expressions rather than single words. Let’s illustrate this problem with some French examples: • An adjective like “hideux” (hideous) can be considered to have a negative polarity in any context and any domain; • An adjective like “merveilleux” (wonderful) can be considered to have a positive polarity in any context and any domain • On the contrary, an adjective like “frais” (fresh) in French might have different polarities depending on context and domain : o In the context “avoir le teint frais” (to have a healthy glow), “frais” has a positive connotation o In the context « un accueil plutôt frais » (a rather cool reception)… “frais” has a negative connotation o In the context un “poisson bien frais (a fresh fish) « frais » has a positive connotation • An adjective like “rapide » (rapid, fast) in French might also have different polarities depending on context and domain : o In the context “l’impression est rapide” (the printing is fast), “rapide” has a positive connotation o In the context “un résumé rapide” (a short summary), “rapide” is rather neutral. • Etc. When building an opinion detection system, it is necessary to be able to disambiguate these polar expressions and associate them the adequate polarity, i.e. positive or negative, according to the context. In this paper, we focus on the extraction of contextual patterns that carry a given polarity. In other terms, we try to automatically detect the polarity of a term according to the context, i.e. learn contextual polarity patterns, for ambiguous polar adjectives. 166  After a short review of the related work, we briefly describe our feature based opinion detection system, and then we present the methodology we propose to acquire opinionated patterns, which is based on syntactic information extraction combined with simple clustering techniques. We then show how we have integrated the learned patterns into our opinion detection system, and finally evaluate the benefits of this integration. Related Work In the literature about opinion mining, there is a considerable number of works aiming at associating polarity to single words. For example SentiWordnet (Baccianella at al. 2010) is a resource aiming at associating polarity scores to WordNet synsets. Many works try to classify polar adjectives, like for example (Vegnaduzzo 2004) who proposes a distributional method to classify polarity adjective using a small seed of polar adjectives. For French, (Vernier and Monceaux 2010) present a learning method relying on the indexing of Web documents by a search engine and large number of linguistically motivated requests automatically sent. There is considerably less attempts to address the problem of associating polarities to larger expressions, and in particular pairs of words in a given syntactic relation, as we propose here. (Wilson et al. 2005), noticed that polar vocabulary have a “prior polarity” that can change according to the context (negation, diminishers such as “little”, “less”,etc). They learn such contexts by performing classification using various features and an annotated corpus. In the present paper, we focus on different kind of patterns (noun-adj) and also use a different methodology since we only use the marks given to reviews by users and data automatically annotated with our rule-based system to perform the clustering step. (Riloff et al. 2003) propose a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. High-precision classifiers label are used on un-annotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. The learned patterns are then used to identify more subjective sentences. The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision. While it as some similarities with the work proposed in this paper, is also quite different since they try to learn opinionated syntactic patterns while we try to learn opinionated pairs of words, contextually dependent in a given syntactic relation. They also make use of annotated data, while we only use the marks given to reviews by users and data automatically annotated with our rule-based system in order to perform the clustering step. Our Opinion Detection System The opinion detection system we build relies on a robust deep syntactic parser, c.f. (AitMokhtar et al. 2002), as a fundamental component, from which semantic relations of opinion are calculated. Having syntactic relations already extracted by a general dependency grammar, we use the robust parser by combining lexical information about word polarities, sub categorization information and syntactic dependencies to extract the semantic relations. The polarity lexicon has been built using existing resources and also by applying classification techniques over large corpora, while the semantic extraction rules are handcrafted, see (Brun 2011) for the complete description of these 167  different components. At this step of development of the system for the French language, we have built generic rules for extracting opinion relations and a generic polar lexicon containing elements that can be considered as non ambiguous in terms of polarity. The work described in this paper aims at enriching this system with patterns that disambiguate ambiguous polar terms according to their context of appearance. Learning Opinionated Patterns As said in introduction, our goal is to try to automatically detect the polarity of a term according to the context, i.e. learn contextual polarity patterns, for ambiguous polar adjectives. We focus on NOUN-ADJ expressions, where the adjective is qualifying the noun and that can be mainly found in texts within two types of expressions, adjectives in modifier (1) or attribute (2) position: (1) « un accueil sympathique », … "a sympathetic reception”) (2) « la cuisine est inventive », le service est lent, … (« the cooking is inventive », « the service is slow ») To perform this task, we first collect a large corpus of customer reviews from the web, where such opinionated patterns can be found. We then use a robust syntactic parser to extract the candidate patterns, i.e. the modifier and attribute relationships presented above. We apply clustering techniques to group automatically the pattern according to their polarities. These different steps are detailed in the remaining of this section. Corpus Selection We have extracted a large corpus of online user’s reviews about restaurant in French, extracted from the web site (http://www.linternaute.com/restaurant/). The reviews in html format have been cleaned and converted into xml format. Here’s an example of such review, which contains a title (the name of the restaurant), and one or more user reviews containing the user rating of the restaurant and a free text comment: <review> <title> Brasserie André, restaurant gastronomique à Lille</title> <userreview> <rating>3</rating> <comment > Très bonne adresse, les salades sont copieuses, le coin retiré de la circulation, rapport qualité prix très correct. </comment> (Very good place, salads are substantial, the place is far from traffic, value for money quite correct.) </userreview > </review> The corpus we have collected contains 99364 user’s reviews about 15473 different restaurants, i.e. 260 082 sentences (3 337 678 words). The repartition of the reviews according to the rating given by the users is shown on table 1. We consider that reviews rated from 0 to 2 are negative and that reviews rated from 3 to 5 are positive. 168  User’s rating  0/5 1/5 2/5 3/5 4/5 5/5 total  Number of reviews 2508 8810 7511 14142 41382 25011 99364  TABLE 1 – Repartition of reviews according to user’s rating Pattern Extraction In order to extract the patterns we aim at classifying as positive or negative, we use the robust syntactic parser presented in section 2, which detects such relations (attribute modifier relations between noun and adjectives). Moreover, as this work aims at improving an opinion detection system, we also use the opinion detection component we have developed on top of this robust parser (see (anonymous_reference)). We filter out patterns that are already marked as positive and negative by the opinion detection system (because they contain single polar terms that are already encoded in the polar lexicon of the system) and keep only the patterns that do not carry any information about polarity. The parser outputs syntactic relations among which we select the nounadj modifiers and noun-adj attributes. We then count the number of occurrences of these relations within reviews rated 0, 1, 2, 3, 4 and 5. Moreover, we use the existing opinion detection system presented previously in order to also count the number of time a given pattern co-occurs with positive opinions and with negative opinions, on the whole corpus of reviews. Some examples of the results are shown in table 2:  Review rating Noun,adj patterns  0/5 1/5 2/5 3/5 4/5 5/5 Frequencies of patterns within reviews  Frequencies of co-occurring positive opinions  Frequencies of co-occurring negative opinions  addition, convenable  0  0  0  
Extracting question–answer pairs from social media discussions has garnered much attention in recent times. Several methods have been proposed in the past that pose this task as a post or sentence classiﬁcation problem, which label each entry as an answer or not. This paper makes the ﬁrst attempt at the following two–fold objectives: (a) In all classiﬁcation based approaches towards this direction, one of the foremost signals used to identify answers is their similarity to the question. We study the contribution of content similarity speciﬁcally in the context of technical problem–solving domain. (b) We introduce hitherto unexplored features that aid in high–precision extraction of answers, and present a thorough study of the contribution of all features to this task. Our results show that, it is possible to extract answers using these features with high accuracy, when their similarity to the question is unreliable. KEYWORDS: Question Answering, Information & Content Extraction, Text Mining.  175  Proceedings of COLING 2012: Posters, pages 175–184, COLING 2012, Mumbai, December 2012.  
This paper investigates the problem of distinguishing between original and rewritten text materials, with focus on the application of plagiarism detection. The hypothesis is that original texts and rewritten texts exhibit signiﬁcant and measurable differences, and that these can be captured through statistical and linguistic indicators. We propose and analyse a number of these indicators (including language models, syntactic trees, etc.) using machine learning algorithms in two main settings: (i) the classiﬁcation of individual text segments as original or rewritten, and (ii) the ranking of two or more versions of a text segment according to their “originality”, thus rendering the rewriting direction. Different from standard plagiarism detection approaches, our settings do not involve comparisons between supposedly rewritten text and (a large number of) original texts. Instead, our work focuses on the sub-problem of ﬁnding segments that exhibit rewriting traits. Identifying such segments has a number of potential applications, from a ﬁrst-stage ﬁltering for standard plagiarism detection approaches, to intrinsic plagiarism detection and authorship identiﬁcation. The corpus used in the experiments was extracted from the PAN-PC-10 plagiarism detection task, with two subsets containing manually and artiﬁcially generated plagiarism cases. The accuracies achieved are well above a by chance baseline across datasets and settings, with the statistical indicators being particularly effective. KEYWORDS: Text Reuse, Plagiarism Detection, Plagiarism Direction.  195  Proceedings of COLING 2012: Posters, pages 195–204, COLING 2012, Mumbai, December 2012.  
In this paper, we suggest a lattice rescoring architecture that has features of a Trie DB based language model (LM) server and a naïve parameter estimation (NPE) to integrate distributed language models. The Trie DB LM server supports an efficient computation of LM score to rerank the n-best sentences extracted from the lattice. In the case of NPE, it has a role of an integration of heterogeneous LM resources. Our approach distributes LM computations not only to distribute LM resources. This is simple and easy to implement and maintain the distributed lattice rescoring architecture. The experimental results show that the performance of the lattice rescoring has improved with the NPE algorithm that can find the optimal weights of the LM interpolation. In addition, we show that it is available to integrate n-gram LM and DIMI LM. KEYWORDS : lattice rescoring, distributed language model, large scale language model  217  Proceedings of COLING 2012: Posters, pages 217–224, COLING 2012, Mumbai, December 2012.  
In this paper we describe and evaluate a Finite State Machine (FSM) based Morphological Analyzer (MA) for Marathi, a highly inflectional language with agglutinative suffixes. Marathi belongs to the Indo-European family and is considerably influenced by Dravidian languages. Adroit handling of participial constructions and other derived forms (Krudantas and Taddhitas) in addition to inflected forms is crucial to NLP and MT of Marathi. We first describe Marathi morphological phenomena, detailing the complexities of inflectional and derivational morphology, and then go into the construction and working of the MA. The MA produces the root word and the features. A thorough evaluation against gold standard data establishes the efficacy of this MA. To the best of our knowledge, this work is the first of its kind on a systematic and exhaustive study of the Morphotactics of a suffix-stacking language, leading to high quality morph analyzer. The system forms part of a Marathi-Hindi transfer based machine translation system. The methodology delineated in the paper can be replicated for other languages showing similar suffix stacking behaviour as Marathi. KEYWORDS: Marathi, Morphology, Derivational, Inflectional, Architecture, Finite State Transducer, Two-Level, Indian Language Technology.  225  Proceedings of COLING 2012: Posters, pages 225–234, COLING 2012, Mumbai, December 2012.  1. Introduction The number of Marathi speakers all over the world is close to 72 million 1 . Marathi uses agglutinative, inflectional and analytic forms. It displays abundant amount of both derivational (wherein attachment of suffixes to a word form changes its grammatical category) and inflectional morphology. About 15% of the word forms are participial forms known as Krudantas, which result from the influence of Dravidian languages. Traditional grammars of Marathi classify the derived forms in Marathi into two categories- Krudantas and Taddhitas. Krudantas are the adjectives, adverbs and nouns derived from verbs, while Taddhitas are nouns, adjectives and adverbs derived from words of any category other than verb. This is also accompanied by inflectional processes which help lend the words features of gender, number, person, case, tense, aspect and modality (the latter 3 for verbs only). 1.1. Related work The first MA for Marathi used a very naïve suffix stripping approach propounded by Eryiğit and Adalı, (2004). This neither had the ability to handle the stacking of suffixes which might involve orthographic changes at morpheme boundaries, nor could it indicate spelling mistakes and thus was discarded. The need for a mechanism to handle both inflectional and derivational morphology was felt, and we adopted the Finite State Transducer (FST) based approach that allows specification of legal morpheme sequences of both inflectional and derivational kind. We thus used a two level morphological analysis model (Oflazer, 1993; Kim et al., 1994), including a Morphological Parser (Antworth, 1991). Dixit et al. (2006) implemented a Marathi spell-checker, which is an inherent part of our MA. Bapat et al. (2010) had developed a FST based MA which handled the derivational morphology of verbs, and Bhosale et al. (2011) showed that the inclusion of this MA helps improve the translation quality. We extended the work of Bapat et al. to other grammatical categories, thereby increasing the coverage of Marathi morphological phenomena. 2. Morphological phenomena in Marathi We first describe inflectional morphology. Nouns in Marathi are inflected for gender, number and case; adjectives are inflected for gender and number, pronouns for gender, number, case and person. The noun आंबा {aambaa} {mango} is masculine. Its direct singular and plural forms are: आबं ा and आंबे {aambe} {mangoes} respectively. Its oblique singular and plural forms are: आं्या {aambyaa} and आं्यां {aambyaan} respectively. Verbs in Marathi are inflected for person, number and gender of the subject alone or that of both the subject the object of the verb and also for tense, aspect and mood. Marathi has three genders (masculine, feminine and neuter), two numbers (singular and plural), eight cases (nominative, accusative, instrumental, dative, ablative, genitive, locative and vocative) and three persons (first, second and third). Different linguists give different typologies for the tenses, aspects and moods in Marathi. We have followed the typology given by Damle, M.K. (1970). We have also followed the linguistic analyses in the book of Dhongde and Wali (2009). 1http://en.wikipedia.org/wiki/List_of_Indian_languages_by_total_speakers 226  2.1. Derivational Morphology: In the derivational process, a derivational morpheme is affixed to the word stem (the form a root takes when a derivational morpheme is attached to it), in order to add meaning to it and thereby derive a new word. The resulting word may or may not be of the same grammatical category. For example, Marathi has a derivational morpheme- “पणा” {panaa}, which is attached to adjectives like “मू्”ख {moorkh} {foolish}, in order to derive nouns like “मू्पख णा” {moorkhapanaa} {foolishness}. Marathi has many such derivational morphemes. Another important feature of Marathi is its set of participles, which are derived by attaching derivational morphemes to verbs. These participles indicate tense, aspect, voice, mood in addition to gender and number features. For e.g. “येणारा” {yenaaraa} {coming} is a masculine, singular present participle form, while “गेऱेऱा” {gelelaa} {has gone} is a masculine, singular past participle. Most of these participles, in addition to infinitive forms are currently handled by our MA. It also handles the extraction of most of the derivational morphemes that attach to verbs and a few that attach to nouns, adjectives and adverbs. Handling Derivational Morphology is important as it requires only base forms to be stored thereby reducing the lexicon size. We now describe some of the morphological complexities and methods of handling them. 2.2. Complexities in handling Inflectional Morphology 1. When a genitive case marker is attached to a noun or a pronoun, the resulting form holds the gender and number information of both the base noun and the genitive case marker. For example, in the word “मुऱ ंचा” {muleenchaa} {of the girls}, the stem “मुऱ ”ं {muleen} has the features feminine, plural, while the suffix “चा” {chaa} has the features masculine, singular. Thus, the morphological analysis of this form should consist of two feature structures- one for the stem and the other for the genitive suffix. Currently, we obtain a selective combination of both. 2. Pronouns take all cases except the vocative. However in case of pronouns, all cases are not overtly marked. For example, the instrumental case is not overtly marked in case of first and second person pronouns (“मी” {mee} {I/me} and “तू” {tu} {you}). Marathi also has demonstrative pronouns which are same as third person pronouns. However, when these pronouns occur as demonstrative pronouns, they do not take case postpositions. Distinguishing between pronouns and demonstratives becomes difficult (a property of almost all Indo-Aryan languages). For instance, “्या मुऱान”े {tyaa mulaane} {that boy (did): ergative form}. We handle these by special entries in the repository of inflected forms (REPO) (see next section). 3. Spatial and temporal adverbs like “आता” {aata} {now}, which act as nouns, can take some case markers like “चा”{chaa} {of} to give “आताचा” {aataa-chaa} {now-of} for which the Marathi MA uses the type NST (Noun of Space and Time). We create special paradigms (Bapat et al., 2010) for NSTs. 4. There are morphemes that indicate a few features of the agent of the verb and a few features of the object of the verb. For example, the morpheme “ऱ स” {lees} in “्ा्ऱ स” {khallees} {eaten} in addition to indicating the perfective aspect, indicates that the agent of the verb is in singular and second person, while the object of the verb is feminine, singular and third 227  person. In such cases, the morphological output should ideally have two separate feature structures- one for the agent and the other for the object. 5. Stacking of two or more suffixes is very common. Consider the example, “जाणारयानेसुधा” {jaanaaryanesuddhaa} {the one going also (instrumental)} {जा + णारा + ने + सधु ा}. The root is the verb “जा” {jaa} {go} attached with three suffixes “णारया” {naarya}, ने {ne} and “सुधा” {suddha} {also} respectively. Here “णारया” has “न”े as suffix which in turn has “सुधा” as suffix. The finite state approach (next section) for morphological analysis helps in solving this. 6. There are a few pairs of morphemes that have similar orthographical shape, and the stems to which these morphemes are attached are orthographically similar too. Thus, the resulting inflected/derived forms are orthographically similar, but have two different meanings. For example, there are two morphemes represented by the letter “त” {ta}, one of which denotes habitual past and the other, imperfective aspect. Thus, attached to a verbal root like “फिर” {fir} {to wander}, these two suffixes produce two similar forms- “फिरत” {phirat} {(they) used to wander} and “फिरत” {phirat} {wandering}. In such cases, the Morphological Analyzer should be able to produce both the analyses. Once again, the finite state approach helps. 2.3. Complexities in handling Derivational Morphology 1. Base roots may have multiple forms (called stems) depending on which derivational morpheme is attached to them. For example, the cardinal “प्नास” {pannaas} {fifty}, when attached with the derivational morpheme “वा”, takes the stem “प्नासा” {pannaasaa}. However, when attached with the derivational morpheme “दा” {da}, the same cardinal takes the stem “प्नास” {pannaas}. In such cases, we need separate Suffix Replacement Rules (SRRs) (Bapat et al., 2010) for each derivational morpheme. 2. Some of the derivational morphemes like “पणा” {panaa}, “दा” {daa} are highly productive, as they are attached to all members of a particular grammatical category like nouns. However, some derivational morphemes are attached to only some particular semantic classes within a grammatical category. For instance “भर” {bhar} is attached to only nouns, and to only those nouns which indicate places or containers- “देश” {desh} {country- a place}, “वाट ” {vaati} {bowl - a container}. The resultant form for देश” is “देशभर” {deshbhar} {throughout the country}. For such nouns, we need to create special paradigms. 3. Architecture and Working of the Morphological Analyzer The Marathi Morphological Analyzer is fully rule-based and thus relies on string manipulation and file lookup. It requires two main resources, namely, a FST (Finite State Transducer) and a REPO (Repository of Inflected Forms), generated using an Inflector and SFST2 (Stuttgart Finite State Transducer) compiler, which are explained below. These are in turn generated by the basic resources; namely, the monolingual lexicon, the suffix replacement rules (SRRs), the special word forms repository, the verb suffix (for Krudantas) list (Bapat et al., 2010) and morphology rules (Morphotactics). 2http://www.ims.uni-stuttgart.de/projekte/gramotron/SOFTWARE/SFST.html 228  3.1. Tools and Resources 3.1.1. FST (Finite State Transducer) Figure 1 - FST for deriving Adverbs from Cardinal The rules which specify the legal sequences of word-forming morphemes in Marathi are called Morphotactics. These rules constitute a Finite State Transducer. This helps identify incorrectly written words efficiently and allow for easy word segmentation. An example of a rule would be: “$ADJ$ = $ADJ_OF$ $SSY$? ” This means that an adjective (ADJ) can be formed by a sequence of oblique form adjective (ADJ_OF) and an optional suffix (SSY). The question mark indicates optionality. This is a FSM rule. We thus work with parts instead of wholes. Here ADJ_OF and SSY are inflectional types. To understand this better consider the FST in figure 1 above. The FST describes the derivation of adverbs from cardinals. The adverb "वीसदा "{veesdaa} {twenty times} is derived by suffixing "दा "{da} {time(s)} (which comes under DA_CL) to the cardinal "वीस "{vees} {twenty} (which comes under STEM_DA), while the adverb "ववसावयांदा " {visaavyaandaa} {twentieth time} is derived by suffixing "दा "{da} {time(s)}to the stem “ववसावयां” {visaavyaan} of the ordinal "ववसावा" {visaavaa} {twentieth} where “वा” {vaa} becomes “वयां” {vyaan} (which comes under STEM_WA_DA). Here the ordinal "ववसावा " {visaavaa} {twentieth} is derived by suffixing "वा "to the cardinal "वीस "{vees} {twenty} (which comes under STEM_WA). DA_CL represents the derivational suffix “दा” which cannot be followed by any other suffix. STEM_DA is the cardinal stem and STEM_WA_DA is the ordinal stem deriving suffix “वयां” to which the suffix “दा” is attached. STEM_WA is the cardinal stem to which the suffix “वा” is attached. There are close to a 100 rules. We add more rules to handle more complex forms. We use Stuttgart University’s SFST (Stuttgart Finite State Transducer) compiler which takes the categorised inflected forms (in files) and the Morphotactics to give the transducer file, an augmented Finite Automaton (FA) transition table, called the Morphotact file. We chose SFST as it enjoys the ease of specifying Morphotactics. Alternatives like HFST (Helsinki Finite State Transducer) and FOMA also exist. 3.1.2. Repository of Inflected Forms (REPO) After undergoing inflection, using an Inflector, which applies SRR’s to the words in the lexicon, all inflectional forms with their root words and features (gender, number, etc.) are stored in a single flat file called as the REPO file. Separate files for each inflectional type containing the inflected morphemes of that type are also created which are used for the generation of the FST. The format of this file is: <inflectional type>; <inflected word>; <root word-1, feature list-1# 229  root word-2, feature list-2#...# root word-n, feature list-n>. An example for “महाबळेश्वर” {mahabaleshwar} {the god of great strength} is < DF> ; < महाबळेश्वर> ; < महाबळेश्वर,n,n,sg,,,,,d# महाबळेश्वर,n,m,sg,,,,,d# महाबळेश्वर,n,m,pl,,,,,d> . 3.2. Morphological Processing Figure 2 - Morphological Processing Flow The flow of processing is in figure 2 above. There are 3 main components: the FST interpreter/Stemmer (level 1), a lookup engine and a post processing unit (level 2). An auxiliary support list of suffixes is also used. 3.2.1. FST interpreter / Stemmer / Segmenter The interpreter (our Java equivalent of SFST interpreter) takes the input word and gives the morphemes it contains. As such this is a Stemmer or Segmenter. It uses the transition table of the FST and gives the output in the form: <input word>: morpheme-1 <category-1> morpheme-2 <category-2> ….. morpheme-n<category-n>. The first morpheme is the Stem. There is a possibility that a word may be stemmed in more than one way because it could be a direct form of a word or a morphologically complex word with a root and suffix(es). An example for “हऱवा” {ha la wa a }: 1. हऱवा: An inflected form with the imperative suffix “ाा” is attached to the verbal root “हऱव” {halaw} {to shake} . 2. हऱवा: A direct form of a noun referring to a dessert. 3.2.2. Lookup engine / Parsing This unit accepts stemmed results to give intermediate morphological analyses. This and the next stage constitute Morphological Parsing (MP). We currently perform MP for all inflectional morphemes and for the derivational morphemes that attach to verbs. First a hash table of the REPO file, by using the inflected form and the word form category as the joint index, is constructed. The first morpheme and its category are then used on this hash structure to obtain its root form and its features. This is followed by the most crucial- Krudanta processing. If the stemmer detects a Krudanta suffix, the lookup gets it from the hash table and modifies the features of the feature list using those of the Krudanta suffix. Otherwise the following suffixes (if 230  any) are either case markers or postpositions or non Krudanta derivational morphemes which we append to the feature list. Verbs have additional features namely “Krudanta Type” and “Krudanta Case Marker/Suffix”. An example for “धावणारा” {dhaavnara}{runner}, an adjective, would be: धावणारा< fs af (feature structure abbreviated form)= 'धाव,v,m,sg,,d,णारा,णारा' tense= '' aspect= '' mood= '' kridanta_type= 'nara' kridanta_cm= 'णारा'> . 3.2.3. Post Processing A word can be stemmed in multiple ways and hence the resulting duplication of features that happens is eliminated in this unit. Some Marathi specific cases which cannot be handled by rules are also handled here. The final part of this is handling unrecognised words. A word will not be stemmed if either it was not entered in the lexicon or there is a spelling mistake or there are no rules to handle it. It is important to identify the suffix as it shows relations between words and must be translated even if the word it is attached to is unknown or unidentifiable. This is mostly for foreign words. The word is matched against the list of suffixes and the one identified is extracted. There will be no linguistic features associated with it. Builders of Morphological Analyzers, especially, for Indian (and other similar) languages can use our framework effectively. Our Java based stemmer can completely stem/segment and parse around 50000 tokens in 8-10 seconds. The end result of all this processing is the minimally sufficient morphological analysis of the input word. In the next section we present the methods for evaluation of our MA and the results. 4. Evaluation We have two measures of quality, namely, accuracy and usability. We prepared Gold Standard Data of 101 sentences with a total of 1341 tokens/words. We compared the outputs of our MA with the gold standard data. For analysis, each word is put into one of 6 different categories. Table1 below describes these categories and also gives the results of our evaluation.  Analysis number 
In this paper we try to present psycholinguistically motivated computational model for the access and representation of Bangla polymorphemic words in the Mental Lexicon. We ﬁrst conduct a series of masked priming experiment on a set of Bangla polymorphemic words. Our analysis indicates a signiﬁcant number of words shows morphological decomposition during the processing stage. We further developed a computational model for the processing of Bangla polymorphemic words. The novelty of the new model over the existing ones are, the proposed model not only considers the frequency of the derived word but also considers the role of its constituent stem, sufﬁx and the degree of afﬁxation between the stem and the sufﬁx. We have evaluated the new model with the results obtained from the priming experiment and then compare it with the state of the art. The proposed model has been found to perform better than the existing models. KEYWORDS: Mental Lexicon, Morphology, Decomposition, Psycholinguistics, Masked Priming.  235  Proceedings of COLING 2012: Posters, pages 235–244, COLING 2012, Mumbai, December 2012.  
In this paper we describe a novel way of generating an optimal clustering for coreference resolution. Where usually heuristics are used to generate a document-level clustering, based on the output of local pairwise classiﬁers, we propose a method that calculates an exact solution. We cast the clustering problem as an Integer Linear Programming (ILP) problem, and solve this by using a column generation approach. Column generation is very suitable for ILP problems with a large amount of variables and few constraints, by exploiting structural information. Building on a state of the art framework for coreference resolution, we implement several strategies for clustering. We demonstrate a signiﬁcant speedup in time compared to state-ofthe-art approaches of solving the clustering problem with ILP, while maintaining transitivity of the coreference relation. Empirical evidence suggests a linear time complexity, compared to a cubic complexity of other methods. KEYWORDS: Coreference Resolution, Linear Programming, Column Generation.  245  Proceedings of COLING 2012: Posters, pages 245–254, COLING 2012, Mumbai, December 2012.  
In many domain adaption formulations, it is assumed to have large amount of unlabeled data from the domain of interest (target domain), some portion of it may be labeled, and large amount of labeled data from other domains, also known as source domain(s). Motivated by the fact that labeled data is hard to obtain in any domain, we design algorithms for the settings in which there exists large amount of unlabeled data from all domains, small portion of which may be labeled. We build on recent advances in graph-based semi-supervised learning and supervised metric learning. Given all instances, labeled and unlabeled, from all domains, we build a large similarity graph between them, where an edge exists between two instances if they are close according to some metric. Instead of using predeﬁned metric, as commonly performed, we feed the labeled instances into metric-learning algorithms and (re)construct a data-dependent metric, which is used to construct the graph. We employ different types of edges depending on the domain-identity of the two vertices touching it, and learn the weights of each edge. Experimental results show that our approach leads to signiﬁcant reduction in classiﬁcation error across domains, and performs better than two state-of-the-art models on the task of sentiment classiﬁcation. Keywords: Machine Learning, Domain Adaptation, Graph-based Semi-Supervised Learning, Sentiment Analysis.  255  Proceedings of COLING 2012: Posters, pages 255–264, COLING 2012, Mumbai, December 2012.  
Microblogging services continue to grow in popularity, users publish massive instant messages every day through them. Many tweets are marked with hashtags, which usually represent groups or topics of tweets. Hashtags may provide valuable information for lots of applications, such as retrieval, opinion mining, classiﬁcation, and so on. However, since hashtags should be manually annotated, only 14.6% tweets contain them (Wang et al., 2011). In this paper, we adopt topic-speciﬁc translation model(TSTM) to suggest hashtags for microblogs. It combines the advantages of both topic model and translation model. Experimental result on dataset crawled from real world microblogging service demonstrates that the proposed method can outperform some state-of-the-art methods. TITLE AND ABSTRACT IN CHINESE 基于特定话题下翻译模型的微博标签推荐 微博服务变得越来越流行，用户可以通过微博提交大量的及时信息。很多条微博被用户通 过标签标记，这些标签代表了微博的话题类别。标签可以为很多应用提供有价值的信息， 比如检索，情感分析，分类等等。微博的标签本应该由用户自行标记，然而，根据统计只 有14.6%的微博包含标签。在这篇论文中，我们提出了一种基于特定话题的翻译模型， 来 为每条微博自动推荐标签。此模型综合了话题模型和翻译模型的优点。在基于真实微博语 料的实验中，我们提出的方法超过了很多经典的方法。 KEYWORDS: Microblogs, Tag recommendation, Topic model. KEYWORDS IN CHINESE: 微博，标签推荐，话题模型.  265  Proceedings of COLING 2012: Posters, pages 265–274, COLING 2012, Mumbai, December 2012.  
Unsupervised clustering of documents is challenging because documents can conceivably be divided across multiple dimensions. Motivated by prior work incorporating expressive features into unsupervised generative models, this paper presents an unsupervised model for categorizing textual data which is capable of utilizing arbitrary features over a large context. Utilizing locally normalized log-linear models in the generative process, we offer straightforward extensions to the standard multinomial mixture model that allow us to effectively utilize automatically derived complex linguistic, statistical, and metadata features to inﬂuence the learned cluster structure for the desired task. We extensively evaluate and analyze the model’s capabilities over four distinct clustering tasks: topic, perspective, sentiment analysis, and Congressional bill survival, and show that this model outperforms strong baselines and state-of-the-art models. KEYWORDS: Unsupervised Learning, Text Clustering, Sentiment Analysis.  275  Proceedings of COLING 2012: Posters, pages 275–286, COLING 2012, Mumbai, December 2012.  
 Typically native speakers of Arabic mix dialectal Arabic and Modern Standard Arabic in the same  utterance. This phenomenon is known as linguistic code switching (LCS). It is a very challenging  task to identify these LCS points in written text where we don’t have an accompanying speech  signal. In this paper, we address automatic identiﬁcation of LCS points in Arabic social media text  by identifying token level dialectal words. We present an unsupervised approach that employs a set  of dictionaries, sound-change rules, and language models to tackle this problem. We tune and test  the performance of our approach against human-annotated Egyptian and Levantine discussion fora  datasets. Two types of annotations on the token level are obtained for each dataset: context sensitive  and context insensitive annotation. context-sensitive development and  We test  achieve a token level datasets, respectively.  FOβn=1thsecocroenotefx7t4i%nseannsdit7iv2e.4a%nnoontatthede  data, we achieve respectively.  a  token  level  Fβ =1  score  of  84.4%  and  84.9%  on  the  development  and  test  datasets,  Keywords: Linguistic Code Switching, Dialect Identiﬁcation, Modern Standard Arabic, Dialectal Arabic, Dictionaries, Language Models, Sound Change Rules.  Title and Abstract in Arabic:  ,  ,  ,  287  Proceedings of COLING 2012: Posters, pages 287–296, COLING 2012, Mumbai, December 2012.  
ABSTRACT One of the central problems of opinion mining is to extract aspects of entities or topics that have been evaluated in an opinion sentence or document. Much of the existing research focused on extracting explicit aspects which are nouns and nouns phrases that have ap‐ peared in sentences, e.g., price in The price of this bike is very high. (owever, in many cas‐ es, people do not explicitly mention an aspect in a sentence, but the aspect is implied, e.g., This bike is expensive, where expensive indicates the price aspect of the bike. Although there are some existing works dealing with the problem, they all used the corpus‐based approach, which has several shortcomings. )n this paper, we propose a dictionary‐based approach to address these shortcomings. We formulate the problem as collective classifica‐ tion. Experimental results show that the proposed approach is effective and produces sig‐ nificantly better results than strong baselines based on traditional supervised classification. KEYWORDS: )mplied Aspects or Topics, Opinion Mining, Sentiment Analysis  309  Proceedings of COLING 2012: Posters, pages 309–318, COLING 2012, Mumbai, December 2012.  
Standard word sense disambiguation (WSD) data sets annotate each word instance in context with exactly one sense of a predeﬁned inventory, and WSD systems are traditionally evaluated with regard to how good they are at picking this sense. Recently, the notion of graded word sense assignment (GWSA) has gained attention as a more natural view of the contextual speciﬁcation of word meaning; multiple senses may apply simultaneously to one instance of a word, and they may be applicable to different degrees. In this paper, we apply three different WSD algorithms to the task of GWSA. The three models belong to the class of knowledge-based models in the WSD terminology; they are unsupervised in the sense that they do not depend on annotated training material. We evaluate the models on two recently published GWSA data sets. We ﬁnd positive correlations with the human judgments for all models, and develop a metric based on the notion of accuracy that highlights differences in the behaviors of the models. KEYWORDS: lexical semantics, graded word senses, knowledge-based disambiguation.  329  Proceedings of COLING 2012: Posters, pages 329–338, COLING 2012, Mumbai, December 2012.  
This paper describes experiments with transliteration of out-of-vocabulary English terms into Bengali to improve the effectiveness of English-Bengali Cross-Language Information Retrieval. We use a statistical translation model as a basis for transliteration, and present evaluation results on the FIRE 2011 RISOT Bengali test collection. Incorporating transliteration is shown to substantially and statistically significantly improve Mean Average Precision for both the text and OCR conditions. Learning a distortion model for OCR errors and then using that model to improve recall is also shown to yield a further substantial and statistically significant improvement for the OCR condition. TITLE AND ABSTRACT IN BENGALI  OCRCLIR-এ  -  -  -এ  এ  -  এ  -  CLIR-এ  ৷  এ  এ FIRE  HFGG RISOT  -এ  ৷ এ  এ  -এ  -  CLIR-এ  ,  ৷  OCR  '  এ  ফ  ৷  , OCR  CLIR  OCR-এ  এ  এ  ৷  KEYWORDS: CLIR, OCR, English-Bengali, Dictionary based translation,  transliteration, OCR error modeling, Stemming, Evaluation, FIRE-RISOT 2011.  KEYWORDS IN BENGALI: ,  ,  /  , OCR-এ  ,  -  ,  ,  Statistical  ,  -  ৷  339  Proceedings of COLING 2012: Posters, pages 339–348, COLING 2012, Mumbai, December 2012.  
 The paper reports on the recent forum RU-EVAL ‒ a new initiative for evaluation of Russian NLP resources, methods and toolkits. It started in 2010 with evaluation of morphological parsers, and the second event RU-EVAL 2012 (2011-2012) focused on syntactic parsing. Eight participating IT companies and academic institutions submitted their results for corpus parsing. We discuss the results of this evaluation and describe the so-called “soПt” evaluation principles that allowed us to compare output dependency trees, which varied greatly depending on theoretical approaches, parsing methods, tag sets, and dependency orientations principles, adopted by the participants.  TITLE AND ABSTRACT IN RUSSIAN RU-EVAL-2012: Оц  а а я  аа  а  яы а  RU-EVAL – 2012)  ,  .  2010  (LвКsСОvskКвК Оt Кl. 2010),  (2011-  (TolНovК Оt Кl. 2012).  8  .  «  »  ,  ,  ,  .  ,  ,  ,  .  KEYWORDS : Parsing evaluation, dependency grammar, Russian, Russian treebank  KEYWORDS IN RUSSIAN :  ,  ,  ,  349  Proceedings of COLING 2012: Posters, pages 349–360, COLING 2012, Mumbai, December 2012.  
Producing annotated corpora for resource-poor languages can be prohibitively expensive, while obtaining parallel, unannotated corpora may be more easily achieved. We propose a method of augmenting a discriminative dependency parser using syntactic projection information. This modiﬁcation will allow the parser to take advantage of unannotated parallel corpora where high-quality automatic annotation tools exist for one of the languages. We use corpora of interlinear glossed text—short bitexts commonly found in linguistic papers on resource-poor languages with an additional gloss line that supports word alignment—and demonstrate this technique on eight different languages, including resource-poor languages such as Welsh, Yaqui, and Hausa. We ﬁnd that incorporating syntactic projection information in a discriminative parser generally outperforms deterministic syntactic projection. While this paper uses small IGT corpora for word alignment, our method can be adapted to larger parallel corpora by using statistical word alignment instead. KEYWORDS: Dependency Parsing, Syntactic Projection, Interlinear Glossed Text.  371  Proceedings of COLING 2012: Posters, pages 371–380, COLING 2012, Mumbai, December 2012.  
We present a method of ﬁnding and analyzing shifts in grammatical relations found in diachronic corpora. Inspired by the econometric technique of measuring return and volatility instead of relative frequencies, we propose them as a way to better characterize changes in grammatical patterns like nominalization, modiﬁcation and comparison. To exemplify the use of these techniques, we examine a corpus of NIPS papers and report trends which manifest at the token, part-of-speech and grammatical levels. Building up from frequency observations to a second-order analysis, we show that shifts in frequencies overlook deeper trends in language, even when part-of-speech information is included. Examining token, POS and grammatical levels of variation enables a summary view of diachronic text as a whole. We conclude with a discussion about how these methods can inform intuitions about specialist domains as well as changes in language use as a whole. KEYWORDS: Corpus Analysis, Diachronic Analysis, Language Variation, Text Classiﬁcation.  381  Proceedings of COLING 2012: Posters, pages 381–390, COLING 2012, Mumbai, December 2012.  
In this paper, we deﬁne models for automatically translating a factoid question in natural language to an SQL query that retrieves the correct answer from a target relational database (DB). We exploit the DB structure to generate a set of candidate SQL queries, which we rerank with an SVM-ranker based on tree kernels. In particular, in the generation phase, we use (i) lexical dependencies in the question and (ii) the DB metadata, to build a set of plausible SELECT, WHERE and FROM clauses enriched with meaningful joins. We combine the clauses by means of rules and a heuristic weighting scheme, which allows for generating a ranked list of candidate SQL queries. This approach can be recursively applied to deal with complex questions, requiring nested SELECT instructions. Finally, we apply the reranker to reorder the list of question and SQL candidate pairs, whose members are represented as syntactic trees. The F1 of our model derived on standard benchmarks, 87% on the ﬁrst question, is in line with the best models using external and expensive hand-crafted resources such as the question meaning interpretation. Moreover, our system shows a Recall of the correct answer of about 94% and 98% on the ﬁrst 2 and 5 candidates, respectively. This is an interesting outcome considering that we only need pairs of questions and answers concerning a target DB (no SQL query is needed) to train our model. KEYWORDS: Natural Language Interface to Databases, Semantic Parsing, Reranking.  401  Proceedings of COLING 2012: Posters, pages 401–410, COLING 2012, Mumbai, December 2012.  Figure 1: A DBMS catalog containing GEOQUERY database 
Tense of one sentence can indicate the time when an event takes place. Therefore, it is very useful for natural language processing tasks such as Machine Translation (MT). However, the mapping of tense in MT is a very challenging problem as the usage of tenses varies from one language to another. Aiming at translating one language (source) which lacks overt tense markers into another language (target) whose tense information is easily recognized, we propose to use a classiﬁer-based tense model to keep the main tense in target side consistent with the one in source side. Furthermore, we present a simple and effective way to help this model by expanding more phrase pairs with different tenses. Experimental results demonstrate our methods signiﬁcantly improve translation accuracy. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (CHINESE) Äu©a ž .3SMTp A^ ž é ˜ g , Š ó ? n ? Ö 5 ` A O - ‡, ' X Å ì € È. , , 3 € È L § ¥ ‡ ± !8Ià ž ˜—5´š~(J , cÙLy3é, vk²wž IP Š óéf?1€È žÿ. ©JÑ ˜«Äu©aEâ ž . •Ïùa€È. d , ©„æ^ ˜«{ük •{5¼ •õž áŠé. ¢ L²·‚ •{U w ÍJp€ÈŸþ. KEYWORDS: Statistical Machine Translation(SMT), Tense Model, Verb Phrase . KEYWORDS IN CHINESE: ÚOÅì€È, ž ., ÄcáŠ .  ∗*Corresponding author.  411  Proceedings of COLING 2012: Posters, pages 411–420, COLING 2012, Mumbai, December 2012.  
With the growing popularity of opinion-rich resources on the Web, new opportunities and challenges arise and aid people in actively using such information to understand the opinions of others. Opinion mining process currently focuses on extracting the sentiments of the users on products, social, political and economical issues. In many instances, users not only express their sentiments but also contribute their ideas, requests and suggestions through comments. Such comments are useful for domain experts and are referred to as actionable content. Extracting actionable knowledge from online social media has attracted a growing interest from both academia and the industry. We deﬁne a new problem in this line which is extracting entity-actionable knowledge from the users’ comments. The problem aims at extracting and normalizing the entity-action pairs. We propose a principled approach to solve this problem and detect exactly matched entities with 75.1% F-score and exactly matched actions with 76.43% F-score. We could achieve an average precision of 81.15% for entity-action normalization. KEYWORDS: Information Extraction, Normalization, Clustering, Conditional Random Fields.  421  Proceedings of COLING 2012: Posters, pages 421–430, COLING 2012, Mumbai, December 2012.  
Feature selection methods are essential for learning to rank (LTR) approaches as the number of features are directly proportional to computational cost and sometimes, might lead to the over-ﬁtting of the ranking model. We propose an expected divergence based approach to select a subset of highly discriminating features over relevance categories. The proposed method is evaluated in terms of performance of standard LTR algorithms when trained with reduced features over a set of standard LTR datasets. The proposed method leads to not signiﬁcantly worse, and in some cases, signiﬁcantly better performance compared to the baselines with as few features as less than 10%. The proposed method is scalable and can easily be parallelised. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, L2 (OPTIONAL, AND ON SAME PAGE)  a  к ea ш Ê a  шÊ  a  (learning to rank) r  к e шÊ  ,  yк  к  ,ш  aк к  ш  к  Ú к a -ú к r к  ।  шÊ к ea ш Ê  a  кr  к  rк  a  к шu  кÊ  к  ।r  к  a  a  ш rш ш  a  к  (algorithms) к Ú  кк  ।r  a  хa к  10% ш к u  not signiﬁcantly worse a  кк  signiﬁcantly º  rÚш Úх  ।r  кº  a  a  Êи  к।  KEYWORDS: feature selection, ranking.  KEYWORDS IN L2: ш Ê , к .  431  Proceedings of COLING 2012: Posters, pages 431–440, COLING 2012, Mumbai, December 2012.  
 In the conventional evaluation metrics of machine translation, considering less information about the translations usually makes the result not reasonable and low correlation with human judgments. On the other hand, using many external linguistic resources and tools (e.g. Part-ofspeech tagging, morpheme, stemming, and synonyms) makes the metrics complicated, timeconsuming and not universal due to that different languages have the different linguistic features. This paper proposes a novel evaluation metric employing rich and augmented factors without relying on any additional resource or tool. Experiments show that this novel metric yields the state-of-the-art correlation with human judgments compared with classic metrics BLEU, TER, Meteor-1.3 and two latest metrics (AMBER and MP4IBM1), which proves it a robust one by employing a feature-rich and model-independent approach.  KEYWORDS : Machine translation, Evaluation metric, Context-dependent n-gram alignment, Modified length penalty, Precision, Recall.  441  Proceedings of COLING 2012: Posters, pages 441–450, COLING 2012, Mumbai, December 2012.  
Debate stance classification, the task of classifying an author's stance in a two-sided debate, is a relatively new and challenging problem in opinion mining. One of its challenges stems from the fact that it is not uncommon to find words and phrases in a debate post that are indicative of the opposing stance, owing to the frequent need for an author to re-state other people's opinions so that she can refer to and contrast with them when establishing her own arguments. We propose a machine learning approach to debate stance classification that leverages two types of rich linguistic knowledge, one exploiting contextual information and the other involving the determination of the author's stances on topics. Experimental results on debate posts involving two popular debate domains demonstrate the effectiveness of our two types of linguistic knowledge when they are combined in an integer linear programming framework. Title and Abstract in Bengali উ ত ভাষািবদ ার সাহােয ভাবাদিশক িবতেকর প িনণয় িবতেকর প িনণয় তথা একিট ি পাি ক িবতেক একজন তািকক কান প িনে ন সিট িনধারণ করা ওিপিনয়ন মাইিনং-এ একিট অেপ াকৃত নতুন এবং জিটল সম া। এে ে একিট অ তম িতব ক হেলা একজন তািকেকর লখায় ায়ই িবপে র ব ব ত শ এবং বাক াংশ পাওয়া যায় যা ঐ তািকক অ পে র যুি পুন ে খ এবং খ ডেনর মাধ েম িনজ যুি উপ াপেনর জ ব বহার কেরন। িবতেকর প িনণেয়র জ আমরা একিট মিশন লািনং প িত াব করিছ যােত ই ধরেণর উ ত ভাষািবদ া েয়াগ করা হেয়েছ, থমিট াসংিগক তথ এবং অ িট িবিভ আেলাচ িবষেয়র ে তািকেকর অিভমেতর উপর িভি কের িতি ত। িট ব ল আেলািচত িবষেয়র পে -িবপে লখা রচনার উপর চালােনা পরী ার ফলাফল ইি টজার িলিনয়ার া ািমং-এর সােথ যু াব া এই ই ধরেণর উ ত ভাষািবদ ার কাযকািরতা মাণ কের। Keywords: debate stance classification, opinion mining, sentiment analysis. Keywords in Bengali: িবতেকর প িনণয়, ওিপিনয়ন মাইিনং, মতামত িবে ষণ।  451  Proceedings of COLING 2012: Posters, pages 451–460, COLING 2012, Mumbai, December 2012.  
We propose a simple and effective metric for automatically evaluating discourse coherence of a text using the outputs of a coreference resolution model. According to the idea that a writer tends to appropriately utilise coreference relations when writing a coherent text, we introduce a metric of discourse coherence based on automatically identiﬁed coreference relations. We empirically evaluated our metric by comparing it to the entity grid modelling by Barzilay and Lapata (2008) using Japanese newspaper articles as a target data set. The results indicate that our metric better reﬂects discourse coherence of texts than the existing model. KEYWORDS: discourse coherence, coreference resolution, evaluation metric.  483  Proceedings of COLING 2012: Posters, pages 483–494, COLING 2012, Mumbai, December 2012.  
Estimating word relatedness is essential in natural language processing (NLP), and in many other related areas. Corpus-based word relatedness has its advantages over knowledge-based supervised measures. There are many corpus-based measures in the literature that can not be compared to each other as they use a diﬀerent corpus. The purpose of this paper is to show how to evaluate diﬀerent corpus-based measures of word relatedness by calculating them over a common corpus (i.e., the Google n-grams) and then assessing their performance with respect to gold standard relatedness datasets. We evaluate six of these measures as a starting point, all of which are re-implemented using the Google n-gram corpus as their only resource, by comparing their performance in ﬁve diﬀerent data sets. We also show how a word relatedness measure based on a web search engine can be implemented using the Google n-gram corpus. Keywords: Word Relatedness, Similarity, Corpus, Unsupervised, Google n-grams, Tri- grams.  495  Proceedings of COLING 2012: Posters, pages 495–506, COLING 2012, Mumbai, December 2012.  
Abs tract: In this paper, we propose a two-stage bootstrapping approach to resolve various anaphora representing persons, places, plurals and events in Tamil text. The existing approaches dealt with only single pronoun type and not all anaphora using common approach. Moreover, most of the approaches concentrate on syntax- based algorith ms and semantics to some extent. Instead in our approach, we tackle various types of pronouns using a semi-supervised bootstrapping approach with uniform pattern representation and by exploring the semantic features in resolving anaphors. In order to aid the semantics, we use Un iversal Networking Language (UNL ), a deep semantic representation for resolving various types of pronouns. The two stages of our bootstrapping approach consists of identification of anaphora and its set of referring expressions in stage 1 and identification of correct antecedent of a pronoun in stage 2. In our approach two patterns are defined – one for anaphora and other for set of referring exp ressions. In addition, we introduce triggering tuples, which can be word based semantics or context based semantics, represented in the pattern of both anaphora and referring exp ressions so as to resolve the ambiguit ies during the identification of correct antecedent. The performance of our bootstrapping approach gives better results and proved. Keywords: Anaphora resolution, Uni versal Networking Language, Bootstrappi ng, Triggering Tuples  507  Proceedings of COLING 2012: Posters, pages 507–516, COLING 2012, Mumbai, December 2012. 
ABSTRACT The paper focuses on the interlocutors' self-evaluation in Finnish and Estonian first encounter dialogues. It studies affective and emotive impressions of the participants after they have met the partner for the first time, and presents comparison of the evaluation along the gender, age and education parameters. The results bring forward some statistically significant differences between the two groups, and point to different, culturally determined evaluation scales. The paper discusses the impact of the findings on the complex issues related to the evaluation of automatic interactive systems, and carries over to such applications as intelligent training and tutoring systems, and interactions with robots, encouraging further studies on the interlocutors' engagement in interaction and their evaluation of the success of the interaction. KEYWORDS : dialogue, conversational engagement, self-assessment, cross-cultural evaluation Kõnelejate suhtluskogemuse ja enesehinnangute uuringud KOKKUVÕTE Artikkel keskendub vestluskaaslaste enesehinnangutele esmakohtumisel peetud dialoogides soome ja eesti keeles. Uuritakse osalejate afektiivseid ja emotiivseid muljeid pärast seda, kui nad on kohtunud partneriga esmakordselt, ja esitatakse hinnangute võrdlus soo, vanuse ja hariduse parameetrite alusel. Tulemused toovad esile statistiliselt olulised erinevused kahe rühma vahel ja viitavad erinevatele, kultuuriliselt determineeritud hinnanguskaaladele. Artikkel analüüsib nende tulemuste mõju keerulistele probleemidele, mis on seotud automaatsete interaktiivsete süsteemide evalveerimisega, ja arendab edasi selliseid rakendusi nagu intelligentsed treenimis- ja õpetamissüsteemid ning suhtlus robotitega, pannes aluse edasistele uuringutele suhtlejate vestlusesse lülitumise ja vestluse edukuse hindamise kohta. VÕTMESÕNAD : dialoog, vestlusesse lülitumine, enesehinnang, kultuuridevaheline evalveerimine  517  Proceedings of COLING 2012: Posters, pages 517–526, COLING 2012, Mumbai, December 2012.  
We use crowdsourcing to disambiguate 1000 words from among coarse-grained senses, the most extensive investigation to date. Ten unique participants disambiguate each example, and, using regression, we ﬁnd surprising features which drive differential WSD accuracy: (a) the number of rephrasings within a sense deﬁnition is associated with higher accuracy; (b) as word frequency increases, accuracy decreases even if the number of senses is kept constant; and (c) spending more time is associated with a decrease in accuracy. We also observe that all participants are about equal in ability, practice (without feedback) does not seem to lead to improvement, and that having many participants label the same example provides a partial substitute for more expensive annotation. KEYWORDS: Word sense disambiguation, crowdsourcing.  539  Proceedings of COLING 2012: Posters, pages 539–548, COLING 2012, Mumbai, December 2012.  
This short paper summarizes a faithful implementation of the categorical framework of Coecke et al. (2010), the aim of which is to provide compositionality in distributional models of lexical semantics. Based on Frobenius Algebras, our method enable us to (1) have a unifying meaning space for phrases and sentences of different structure and word vectors, (2) stay faithful to the linguistic types suggested by the underlying type-logic, and (3) perform the concrete computations in lower dimensions by reducing the space complexity. We experiment with two different parameters of the model and apply the setting to a verb disambiguation and a term/deﬁnition classiﬁcation task with promising results. KEYWORDS: semantics, compositionality, distributional models, category theory, Frobenius algebra, vector space models, disambiguation, deﬁnition classiﬁcation.  ∗ Support by EPSRC grant EP/F042728/1 is gratefully acknowledged by the ﬁrst two authors.  549  Proceedings of COLING 2012: Posters, pages 549–558, COLING 2012, Mumbai, December 2012.  
An important problem in sentiment analysis are inconsistent words. We deﬁne an inconsistent word as a sentiment word whose dictionary polarity is reversed by the sentence context in which it occurs. We present a supervised machine learning approach to the problem of inconsistency classiﬁcation, the problem of automatically distinguishing inconsistent from consistent sentiment words in context. Our ﬁrst contribution to inconsistency classiﬁcation is that we take into account sentence structure and use syntactic constructions as features – in contrast to previous work that has only used word-level features. Our second contribution is a method for learning polarity reversing constructions from sentences annotated with polarity. We show that when we integrate inconsistency classiﬁcation results into sentence-level polarity classiﬁcation, performance is signiﬁcantly increased. KEYWORDS: sentiment analysis, polarity modiﬁers, polarity shifters, polarity reversers, nega- tion.  569  Proceedings of COLING 2012: Posters, pages 569–578, COLING 2012, Mumbai, December 2012.  
This paper introduces a cross-language information retrieval (CLIR) framework that combines the state-of-the-art keyword-based approach with a latent semantic-based retrieval model. To capture and analyze the hidden semantics in cross-lingual settings, we construct latent semantic models that map text in different languages into a shared semantic space. Our proposed framework consists of deep belief networks (DBN) for each language and we employ canonical correlation analysis (CCA) to construct a shared semantic space. We evaluated the proposed CLIR approach on a standard ad hoc CLIR dataset, and we show that the cross-lingual semantic analysis with DBN and CCA improves the state-of-the-art keyword-based CLIR performance. KEYWORDS: Cross-language Information Retrieval, Ad Hoc Retrieval, Deep Learning, Deep Belief Network, Canonical Correlation Analysis, Wikipedia, CLEF.  579  Proceedings of COLING 2012: Posters, pages 579–588, COLING 2012, Mumbai, December 2012.  
Acoustic-phonetic landmarks provide robust cues for speech recognition and are relatively invariant between speakers, speaking styles, noise conditions and sampling rates. The ability to detect acoustic-phonetic landmarks as a front-end for speech recognition has been shown to improve recognition accuracy. Biomimetic inter-spike intervals and average signal level have been shown to accurately convey information about acoustic-phonetic landmarks. This paper explores the use of inter-spike interval and average signal level as input features for landmark detectors trained and tested on mismatched conditions. These detectors are designed to serve as a front-end for speech recognition systems. Results indicate that landmark detectors trained using inter-spike intervals and signal level are relatively robust to both additive channel noise and changes in sampling rate. KEYWORDS: Auditory Modeling, Acoustic-Phonetic Landmark, Mismatched Conditions.  589  Proceedings of COLING 2012: Posters, pages 589–598, COLING 2012, Mumbai, December 2012.  
To answer the question “What are the duties of a medical doctor?”, one would require knowledge about verb-based relations. A lot of effort has been invested in developing relation learners, however to our knowledge there is no repository (or system) which can return all verb relations for a given term. This paper describes an automated procedure which can learn and produce such information with minimal effort. To evaluate the performance of our verb harvesting procedure, we have conducted two types of evaluations: (1) in the human based evaluation we found that the accuracy of the described algorithm is .95 at rank 100; (2) in the comparative study with existing relation learner and knowledge bases we found that our approach yields 12 times more verb relations. KEYWORDS: verb harvesting, relation learning, information extraction, knowledge acquisition.  599  Proceedings of COLING 2012: Posters, pages 599–610, COLING 2012, Mumbai, December 2012.  
In this paper, we propose a new method of training phrase segmentation model for phrasebased statistical machine translation(SMT). We deﬁne a good segmentation as the segmentation producing a good translation. According to this deﬁnition, we propose a method that can discriminate between a good segmentation and a bad segmentation based on the translation quality. The proposed approach constructs the phrase labeled data by using the SMT decoder, so that the phrase segmentations supporting good translations can be acquired. Furthermore, our iterative training algorithm of the segmentation model can gradually improve the performance of the SMT decoder. Experimental results show that the proposed method is effective in improving the translation quality of the phrase-based SMT system. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (KOREAN) 통계 기계번역을 위한 디코더 기반 구 분할 차별 학습 방 법 본 논문은 구 기반 통계 기계번역을 위한 구 분할 모델의 새로운 학습 방법을 제안한 다. 우리는 좋은 번역(good translation)을 생성하는 구 분할을 좋은 분할(good segmentation) 이라고 정의한다. 우리는 이 정의에 따라 번역 품질에 기반하여 좋은 분할과 좋지 않은 분할을 차별할 수 있는 방법을 제안한다. 제안하는 접근방법은 통계 기계번역(SMT) 디코더을 이용하여 구 부착 데이터를 구축함으로써, 좋은 번역을 만드는 구 분할을 얻을 수 있다. 또한 SMT 디코더의 성능을 점진적으로 개선시킬 수 있는 반복적인 학습 알고리즘을 제안한다. 실험을 통해, 제안 방법이 구 기반 SMT 시스템의 번역 품질 향상에 효과적이었음을 보인다. KEYWORDS: phrase-based SMT, phrase segmentation model, decoder-based approach. KEYWORDS IN KOREAN: 구 기반 통계 기계번역, 구 분할 모델, 디코더 기반 접근방법.  611  Proceedings of COLING 2012: Posters, pages 611–620, COLING 2012, Mumbai, December 2012.  
While our knowledge about ancient civilizations comes mostly from studies in archaeology and history books, much can also be learned or confirmed from literary texts. Using natural language processing techniques, we present aspects of ancient China as revealed by statistical textual analysis on the Complete Tang Poems, a 2.6-million-character corpus of all surviving poems from the Tang Dynasty (AD 618—907). Using an automatically created treebank of this corpus, we outline the semantic profiles of various poets, and discuss the role of seasons, geography, history, architecture, and colours, as observed through word selection and dependencies. KEYWORDS : Classical Chinese, poetry, dependency parsing, word selection, semantics.  621  Proceedings of COLING 2012: Posters, pages 621–632, COLING 2012, Mumbai, December 2012.  
This paper describes statistical techniques used for modelling transliteration systems between the scripts of Punjabi language. Punjabi is one of the unique languages, which are written in more than one script. In India, Punjabi is written in Gurmukhi script, while in Pakistan it is written in Shahmukhi (Perso-Arabic) script. Shahmukhi script has its origin in the ancient Phoenician script whereas Gurmukhi script has its origin in the ancient Brahmi script. Whilst in speech Punjabi spoken in the Eastern and the Western parts is mutually comprehensible, in the written form it is not so. This has created a script wedge as majority of Punjabi speaking people in Pakistan cannot read Gurmukhi script, and similarly the majority of Punjabi speaking people in India cannot comprehend Shahmukhi script. In this paper, we present an advanced and highly accurate transliteration system between Gurmukhi and Shahmukhi scripts of Punjabi language which addresses various challenges such as multiple/zero character mappings, missing vowels, word segmentation, variations in pronunciations and orthography and transliteration of proper nouns etc. by generating efficient algorithms along with special rules and using various lexical resources such as Gurmukhi spell checker, corpora of both scripts, Gurmukhi-Shahmukhi transliteration dictionaries, statistical language models etc. The proposed system attains more than 98.6% accuracy at word level while transliterating Gurmukhi text to Shahmukhi. The reverse part i.e. transliterating from Shahmukhi text to Gurmukhi is more complex and challenging but our system has achieved 97% accuracy at word level in this part too. KEYWORDS: n-gram language model, Shahmukhi, Gurmukhi, Punjabi, Machine Transliteration, Word disambiguation, HMM  633  Proceedings of COLING 2012: Posters, pages 633–642, COLING 2012, Mumbai, December 2012.  
Hindi and Urdu are variants of the same language, but while Hindi is written in the Devnagri script from left to right, Urdu is written in a script derived from a Persian modification of Arabic script written from right to left. The difference in the two scripts has created a script wedge as majority of Urdu speaking people in Pakistan cannot read Devnagri, and similarly the majority of Hindi speaking people in India cannot comprehend Urdu script. To break this script barrier, it becomes necessary to develop a high accuracy Urdu-Devnagri transliteration system. The major challenges in developing such system are handling missing diacritic marks and short vowels in Urdu, zero/multiple character mappings of Urdu in Hindi, absence of half characters in Urdu, multiple mappings of Urdu words in Hindi and word segmentation issues in Urdu including broken and merged words. Already a few Urdu-Hindi transliteration systems have developed but their accuracy is not very high and they have failed to address all the above issues. For the first time, we present a complete Urdu-Hindi transliteration system which takes care of all the above issues and has reported a transliteration accuracy of more than 97% at word level. KEYWORDS : Urdu, Hindi, Devnagri, Machine Transliteration, Language Models  643  Proceedings of COLING 2012: Posters, pages 643–652, COLING 2012, Mumbai, December 2012.  
Social tagging provides an efﬁcient way to manage online resources. In order to collect more social tags, many research efforts aim to automatically suggest tags to help users annotate tags. Many content-based methods assume tags are independent and suggest tags one by one independently. Although it makes suggestion easier, the independence assumption does not conﬁrm to reality, and the suggested tags are usually inconsistent and incoherent with each other. To address this problem, we propose to model contextaware relations of tags for suggestion: (1) By regarding resource content as context of tags, we propose Tag Context Model to identify speciﬁc context words in resource content for tags. (2) Given a new resource, we build a context-aware relation graph of candidate tags, and propose a random walk algorithm to rank tags for suggestion. Experiment results demonstrate our method outperforms other state-of-the-art methods.  TITLE AND ABSTRACT IN CHINESE  ¿  ³ á Ö½ Á Ë  ç­ÑÙÑÙ»çÁºàÐðßÞËèÁÆ³ÁÁÔÙ³¨ÝàǑÁ·¸Ñ½ÒÄ¿á¿Áùñè℄Öàç1­ßóßàßÆÁÙÆ2℄ËǑ­¨¢ÆǑªËËðªÆùøûàµèßǑÁǑèÁ℄¨ûÃ­´ÂÑóÁóÝðÂóÆÄìÁõÁüàÁßÐ  KEYWORDS: tag ranking, context-aware relation, random walk, social tag suggestion.  Á Ë KEYWORDS IN CHINESE:  ,  ³ , á Ö, Á .  ∗ indicates equal contributions from these authors.  653  Proceedings of COLING 2012: Posters, pages 653–662, COLING 2012, Mumbai, December 2012.  
This paper presents the utilization of chunk phrases to facilitate evaluation of machine translation. Since most of current researches on evaluation take great effects to evaluate translation quality on content relevance and readability, we further introduce high-level abstract information such as semantic similarity and topic model into this phrase-based evaluation metric. The proposed metric mainly involves three parts: calculating phrase similarity, determining weight to each phrase, and ﬁnding maximum similarity map. Experiments on MTC Part 2 (LDC2003T17) show our metric, compared with other popular metrics such as BLEU, MAXSIM and METEOR, achieves comparable correlation with human judgements at segment-level and signiﬁcant higher correlation at document-level. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (CHINESE) ÄuáŠ Åì€ÈgÄµd  © J Ñ Ä u á Š Å ì € È g Ä µ d • {, ¿ ò p g Ä – & E \ \ d • {  ¥, XŠÂƒqÝ, ÌK .. ©JÑ •{Ì‡kn‡Ü©: OŽáŠƒqÝ, •á  Š© -ÚÏé•ŒƒqÝš . 3MTC Part 2 (LDC2003T17)þ ¢ L², †BLEU,  MAXSIM, METEOR Ì6•{ƒ', © •{†<óµd ƒ'53éf?µdþ  ƒ  J, 3© ?µdþwÍ/puÙ¦•{.  KEYWORDS: phrase similarity, topic model, machine translation, automatic evaluation.  KEYWORDS IN CHINESE: áŠƒqÝ, ÌK ., Åì€È, gÄµd.  ∗*Corresponding author.  663  Proceedings of COLING 2012: Posters, pages 663–672, COLING 2012, Mumbai, December 2012.  
Inversion transduction grammar (ITG) provides a syntactically motivated solution to modeling the distortion of words between two languages. Although the Viterbi ITG alignments can be found in polynomial time using a bilingual parsing algorithm, the computational complexity is still too high to handle real-world data, especially for long sentences. Alternatively, we propose a simple and effective beam search algorithm. The algorithm starts with an empty alignment and keeps adding single promising links as early as possible until the model probability does not increase. Experiments on Chinese-English data show that our algorithm is one order of magnitude faster than the bilingual parsing algorithm with bitext cell pruning without loss in alignment and translation quality. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, CHINESE ˜«ITGcŠéà Î|¢Ž{ ‡•=¹Š{•ü«Šóm cŠNSJø ˜«é{°Ä )û•Y"•,VŠé{© ÛŽ{Œ±3õ‘ªžmS|¢ Viterbiéà§ÙOŽE,Ý•, p§J±?n•¹é õ•éf ý¢êâ"•d§·‚JÑ˜«{ük Î|¢Ž{"TŽ{±˜éà•å :§`kÀJ•Ð ë‚V\ éà¥§†–Ã{Jp .VÇ•Ž"3Ç=êâþ ¢ (JL²§·‚ Ž{'¦^}{Eâ VŠ©ÛŽ{¯˜‡êþ?§Óž ± éà Ú€È Ÿþ" KEYWORDS: word alignment, inversion transduction grammar, beam search. KEYWORDS IN L2: cŠéà, ‡•=¹Š{, Î|¢.  673  Proceedings of COLING 2012: Posters, pages 673–682, COLING 2012, Mumbai, December 2012.  
Currently, the best performing models for Chinese word segmentation (CWS) are extremely resource intensive in terms of annotation data quantity. One promising solution to minimize the cost of data acquisition is active learning, which aims to actively select the most useful instances to annotate for learning. Active learning on CWS, however, remains challenging due to its inherent nature. In this paper, we propose a Word Boundary Annotation (WBA) model to make effective active learning on CWS possible. This is achieved by annotating only those uncertain boundaries. In this way, the manual annotation cost is largely reduced, compared to annotating the whole character sequence. To further minimize the annotation effort, a diversity measurement among the instances is considered to avoid duplicate annotation. Experimental results show that employing the WBA model and the diversity measurement into active learning on CWS can save much annotation cost with little loss in the performance. KEYWORDS: Chinese Word Segmentation; Active Learning; Word Boundary Annotation  683  Proceedings of COLING 2012: Posters, pages 683–692, COLING 2012, Mumbai, December 2012.  
Due to the increase in the number of classes and the decrease in the semantic differences between classes, fine-grained classification of Named Entities is a more difficult task than classic classification of NEs. Using only simple local context features for this fine-grained task cannot yield a good classification performance. This paper proposes a method exploiting Multi-features for fine-grained classification of Named Entities. In addition to adopting the context features, we introduce three new features into our classification model: the cluster-based features, the entityrelated features and the class-specific features. We experiment on them separately and also fused with prior ones on the subcategorization of person names. Results show that our method achieves a significant improvement for the fine-grained classification task when the new features are fused with others. KEYWORDS : Named Entities, fine-grained classification, cluster-based features, entity-related features, class-specific features.   Corresponding author.  693  Proceedings of COLING 2012: Posters, pages 693–702, COLING 2012, Mumbai, December 2012.  
The growth of social media provides a convenient communication scheme for people, but at the same time it becomes a hotbed of misinformation. The wide spread of misinformation over social media is injurious to public interest. We design a framework, which integrates collective intelligence and machine intelligence, to help identify misinformation. The basic idea is: (1) automatically index the expertise of users according to their microblog contents; and (2) match the experts with given suspected misinformation. By sending the suspected misinformation to appropriate experts, we can collect the assessments of experts to judge the credibility of information, and help refute misinformation. In this paper, we focus on expert ﬁnding for misinformation identiﬁcation. We propose a tag-based method to index the expertise of microblog users with social tags. Experiments on a real world dataset demonstrate the effectiveness of our method for expert ﬁnding with respect to misinformation identiﬁcation in microblogs.  TITLE AND ABSTRACT IN CHINESE å ²Ç  ÙÝ  ¿Í²²ÖǑóÇ¿ÖìÂ²ÍÜ¿àªóÆòå  Ç¾¢ 2 þ²  ²àþ  Ç¨Ä  ²¡îÙó²Ýð²ñÃàÜëß1ÆñóǑ¯  ÔǑ àê ¨  ó½ÙÖÇÁ²ÂµÄË²ÒàþÆÙßÝ²Ã·ÙÙÆÜì  KEYWORDS: misinformation identiﬁcation, expert ﬁnding, microblog.  ² Ç ÙÝ KEYWORDS IN CHINESE:  ,  ,.  ∗ indicates equal contributions from these authors.  703  Proceedings of COLING 2012: Posters, pages 703–712, COLING 2012, Mumbai, December 2012.  
Relative Entropy-based pruning has been shown to be efﬁcient for pruning language models for more than a decade ago. Recently, this method has been applied to Phrase-based Machine Translation, and results suggest that this method is comparable the state-of-art pruning method based on signiﬁcance tests. In this work, we show that these 2 methods are effective in pruning different types of phrase pairs. On one hand, relative entropy pruning searches for phrase pairs that can be composed using smaller constituents with a small or no loss in probability. On the other hand, signiﬁcance pruning removes phrase pairs that are likely to be spurious. Then, we show that these methods can be combined in order to produce better results, over both metrics when used individually.  713  Proceedings of COLING 2012: Posters, pages 713–722, COLING 2012, Mumbai, December 2012.  
Minimum error rate training is a popular method for parameter tuning in statistical machine translation (SMT). However, the optimization objective function may change drastically at each optimization step, which may induce MERT instability. We propose an alternative tuning method based on an ultraconservative update, in which the combination of an expected task loss and the distance from the parameters in the previous round are minimized with a variant of gradient descent. Experiments on test datasets of both Chinese-to-English and Spanish-toEnglish translation show that our method can achieve improvements over MERT under the Moses system. KEYWORDS: statistical machine translation; tuning; minimum error rate training; ultraconser- vative update; expected BLEU.  723  Proceedings of COLING 2012: Posters, pages 723–732, COLING 2012, Mumbai, December 2012.  
In this work, we propose a novel approach to extract sentiment-bearing expression features derived from dependency structures. Rather than directly use dependency relations generated by a parser, we propose a set of heuristic rules to detect both explicit and implicit negations in the text. Then, three patterns are deﬁned to support generalized sentiment-bearing expressions. By altering existing dependency features with detected negations and generalized sentimentbearing expressions we are able to achieve more accurate sentiment polarity classiﬁcation. We evaluate the proposed approach on three labeled collections of different lengths, and measure the gain from the generalized dependency features when used in addition to the bag-of-words features. Our results demonstrate that generalized dependency-based features are more effective when compared to standard features. Using these we are able to surpass the state-of-the-art in sentiment classiﬁcation. KEYWORDS: Sentiment analysis, Natural Language Processing, Classiﬁcation.  733  Proceedings of COLING 2012: Posters, pages 733–744, COLING 2012, Mumbai, December 2012.  
Sophisticated models have been developed for joint word segmentation and part-of-speech tagging, with increasing accuracies reported on the Chinese Treebank data. These systems, which rely on supervised learning, typically perform worse on texts from a different domain, for which little annotation is available. We consider self-training and character clustering for domain adaptation. Both methods use only unannotated target-domain data, and are relatively straightforward to implement upon a baseline supervised system. Our results show that both methods can effectively improve target-domain performance. In addition, a combination of the two orthogonal methods leads to further improvement. TITLE AND ABSTRACT IN CHINESE 分词与词性标注联合模型的领域适应 分词与词性标注的联合模型是一个正在被广泛研究的问题，随着复杂模型的应用， 其在宾大中文数据库上的测试精度不断提升。这些方法通常使用有监督学习，致使在不同 领域下的效果不如单一领域满意。我们用自学习和字聚类实现领域适应。这两个方法使用 未标注领域训练数据，而且易于实现。我们的实验结果表明，这两种方法都可以提高领域 适应。同时，这两种方法可以结合使用达到更高性能。 KEYWORDS: Semi-supervised learning, domain adaptation, word segmentation, POS-tagging. KEYWORDS IN CHINESE: 分词，词性标注，领域适应，半监督学习，聚类，自学习  745  Proceedings of COLING 2012: Posters, pages 745–754, COLING 2012, Mumbai, December 2012.  
Microblog is a popular Web 2.0 service which reserves rich information about Web users. In a microblog service, it is a simple and effective way to annotate tags for users to represent their interests and attributes. The attributes and interests of a microblog user usually hide behind the text and network information of the user. In this paper, we propose a probabilistic model, Network-Regularized Tag Dispatch Model (NTDM), for microblog user tag suggestion. NTDM models the semantic relations between words in user descriptions and tags, and takes the social network structure as regularization. Experiments on a real-world dataset demonstrate the effectiveness and efﬁciency of NTDM compared to other baseline methods.  TITLE AND ABSTRACT IN CHINESE Ù ÖÁ  ÃÞ Á Ù º  ¿ÄðW/ eÖb2¸.0 þN³TD¨M Ä½  ½î NTDM  ß àÖ¿ßÁ  í¨ÂõªàèðÄÎè ³ ´¨N²T¨DM Ä  ºÖÃÖÞ²Ö¼à«  ¿ ÃÞ  ¸ÁÁ  ¿ǑÁÙÑðºè  KEYWORDS: user tag suggestion, microblog, tag dispatch model, random walks.  ÖÁ KEYWORDS IN CHINESE:  , , Á Ù º, á Ö.  755  Proceedings of COLING 2012: Posters, pages 755–764, COLING 2012, Mumbai, December 2012.  
We present a method for summarizing the collection of tweets related to a business. Our procedure aggregates tweets into subtopic clusters which are then ranked and summarized by a few representative tweets from each cluster. Central to our approach is the ability to group diverse tweets into clusters. The broad clustering is induced by ﬁrst learning a small set of business-related concepts automatically from free text and then subdividing the tweets into these concepts. Cluster ranking is performed using an importance score which combines topic coherence and sentiment value of the tweets. We also discuss alternative methods to summarize these tweets and evaluate the approaches using a small user study. Results show that the concept-based summaries are ranked favourably by the users. KEYWORDS: tweets, twitter, summarization, business, concepts, domain-speciﬁc.  765  Proceedings of COLING 2012: Posters, pages 765–774, COLING 2012, Mumbai, December 2012.  
Experiments on the detection of the source language of literary translations are described. Two feature types are exploited, n-gram based features and document-level statistics. Crossvalidation results on a corpus of twenty 19th-century texts including translations from Russian, French, German and texts written in English are promising: single feature classiﬁers yield signiﬁcant gains on the baseline, although classiﬁers containing a combination of feature types outperform these, bringing L1 detection accuracy to ~80% using ten-fold training set cross validation. Average test set results are slightly lower but still comparable to the crossvalidation results. Relative frequencies of a number of salient features are studied, including several English contractions (I’ll, that’s, etc.) and uncontracted forms; we articulate hypotheses, anchored in source languages, towards explaining differences. KEYWORDS: Computational stylometry, translation studies, source language detection, text classiﬁcation.  775  Proceedings of COLING 2012: Posters, pages 775–784, COLING 2012, Mumbai, December 2012.  
We present and implement a fourth-order projective dependency parsing algorithm that effectively utilizes both “grand-sibling” style and “tri-sibling” style interactions of third-order and “grand-tri-sibling” style interactions of forth-order factored parts for performance enhancement. This algorithm requires O(n5) time and O(n4) space. We implement and evaluate the parser on two languages—English and Chinese, both achieving state-of-the-art accuracy. This results show that a higher-order (≥4) dependency parser gives performance improvement over all previous lower-order parsers. KEYWORDS: Dependency Parsing, Fourth-order.  ∗This work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119 and Grant No. 61170114), the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No.20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), and the European Union Seventh Framework Program (Grant No. 247619). †Corresponding author  785  Proceedings of COLING 2012: Posters, pages 785–796, COLING 2012, Mumbai, December 2012.  
In this paper we propose SubSum, a subjective logic framework for sentence-based extractive multi-document summarization. Document summaries perceived by humans are subjective in nature as human judgements of sentence relevancy are inconsistent and laden with uncertainty. SubSum captures this uncertainty and extracts signiﬁcant sentences from a document cluster to generate extractive summaries. In particular, SubSum represents the sentences of a document cluster as propositions and computes opinions, a probability measure containing secondary uncertainty, for these propositions. Sentences with stronger opinions are considered more signiﬁcant and used as candidate sentences. The key advantage of SubSum over other techniques is its ability to quantify uncertainty. In addition, SubSum is a completely unsupervised approach and is highly portable across different domains and languages. KEYWORDS: multi-document summarization, subjective logic, belief measures, uncertainty.  797  Proceedings of COLING 2012: Posters, pages 797–808, COLING 2012, Mumbai, December 2012.  
Computing inter-annotator agreement measures on a manually annotated corpus is necessary to evaluate the reliability of its annotation. However, the interpretation of the obtained results is recognized as highly arbitrary. We describe in this article a method and a tool that we developed which “shufﬂes” a reference annotation according to different error paradigms, thereby creating artiﬁcial annotations with controlled errors. Agreement measures are computed on these corpora, and the obtained results are used to model the behavior of these measures and understand their actual meaning. KEYWORDS: inter-annotator agreement, manual corpus annotation, evaluation.  809  Proceedings of COLING 2012: Posters, pages 809–818, COLING 2012, Mumbai, December 2012.  
Chinese word segmentation (CWS) is a basic and important task for Chinese information processing. Standard approaches to CWS treat it as a sequence labelling task. Without manually annotated corpora, these approaches are ineffective. When a dictionary is available, dictionary maximum matching (DMM) is a good alternative. However, its performance is far from perfect due to the poor ability on out-of-vocabulary (OOV) words recognition. In this paper, we propose a novel approach that integrates the advantages of discriminative training and DMM, to build a high quality word segmenter with only a dictionary and a raw text. Experiments in CWS on different domains show that, compared with DMM, our approach brings signiﬁcant improvements in both the news domain and the Chinese medicine patent domain, with error reductions of 21.50% and 13.66%, respectively. Furthermore, our approach achieves recall rate increments of OOV words by 42.54% and 23.72%, respectively in both domains. KEYWORDS: discriminative model, word segmentation, dictionary maximum matching.  819  Proceedings of COLING 2012: Posters, pages 819–828, COLING 2012, Mumbai, December 2012.  
In this paper, we propose a simple yet efective approach to automatically building sentiment lexicons from English sentiment lexicons using publicly available online machine translation services. The method does not rely on any semantic resources or bilingual dictionaries, and can be applied to many languages. We propose to overcome the low coverage problem through putting each English sentiment word into diferent contexts to generate diferent phrases, which efectively prompts the machine translation engine to return diferent translations for the same English sentiment word. Experiment results on building a Chinese sentiment lexicon (available at https://github.com/fannix/ChineseSentiment-Lexicon) show that the proposed approach signiicantly improves the coverage of the sentiment lexicon while achieving relatively high precision. Keywords: Sentiment analysis, Multilingual, Dictionary.  ∗corresponding author  829  Proceedings of COLING 2012: Posters, pages 829–838, COLING 2012, Mumbai, December 2012.  
English as a Second Language (ESL) learners’ writings contain various grammatical errors. Previous research on automatic error correction for ESL learners’ grammatical errors deals with restricted types of learners’ errors. Some types of errors can be corrected by rules using heuristics, while others are difﬁcult to correct without statistical models using native corpora and/or learner corpora. Since adding error annotation to learners’ text is time-consuming, it was not until recently that large scale learner corpora became publicly available. However, little is known about the effect of learner corpus size in ESL grammatical error correction. Thus, in this paper, we investigate the effect of learner corpus size on various types of grammatical errors, using an error correction system based on phrase-based statistical machine translation (SMT) trained on a large scale errortagged learner corpus. We show that the phrase-based SMT approach is effective in correcting frequent errors that can be identiﬁed by local context, and that it is difﬁcult for phrase-based SMT to correct errors that need long range contextual information. KEYWORDS: ESL, grammatical error correction, statistical machine translation.  863  Proceedings of COLING 2012: Posters, pages 863–872, COLING 2012, Mumbai, December 2012.  
This paper describes the latest developments in the PeEn-SMT system, specifically covering experiments with Grafix, an APE component developed for PeEn-SMT. The success of well-designed SMT systems has made this approach one of the most popular MT approaches. However, MT output is often seriously grammatically incorrect. This is more prevalent in SMT since this approach is not language-specific. This system works with Persian, a morphologically rich language, so post-editing output is an important step in maintaining translation fluency. Grafix performs a range of corrections on sentences, from lexical transformation to complex syntactical rearrangement. It analyzes the target sentence (the SMT output in Persian language) and attempts to correct it by applying a number of rules which enforce consistency with Persian grammar. We show that the proposed system is able to improve the quality of the state-of-the-art EnglishPersian SMT systems, yielding promising results from both automatic and manual evaluation techniques. KEYWORDS : Machine Translation, Post-editing of Machine Translation, Evaluation of Machine Translation  873  Proceedings of COLING 2012: Posters, pages 873–882, COLING 2012, Mumbai, December 2012.  
Coreference resolution is the task of identifying the sets of mentions referring to the same entity. Although modern machine learning approaches to coreference resolution exploit a variety of semantic information, the literature on the effect of relational information on coreference is still very limited. In this paper, we discuss and compare two methods for incorporating relational information into a coreference resolver. One approach is to use a ﬁltering algorithm to rerank the output of coreference hypotheses. The ﬁlter is based on the relational structures between mentions and their corresponding relationships. The second approach is to use a joint model enriched with a set of relational features derived from semantic relations of each mention. Both methods have shown to improve the performance of a learning-based state-of-the-art coreference resolver. Keywords: coreference resolution, relation extraction, machine learning.  883  Proceedings of COLING 2012: Posters, pages 883–892, COLING 2012, Mumbai, December 2012.  
In this paper we propose a novel text summarization model, the redundancy-constrained knapsack model. We add to the Knapsack problem a constraint to curb redundancy in the summary. We also propose a fast decoding method based on the Lagrange heuristic. Experiments based on ROUGE evaluations show that our proposals outperform a state-of-the-art text summarization model, the maximum coverage model, in finding the optimal solution. We also show that our decoding method quickly finds a good approximate solution comparable to the optimal solution of the maximum coverage model. KEYWORDS: Text summarization, Knapsack problem, Maximum coverage problem, Lagrange heuristics.  893  Proceedings of COLING 2012: Posters, pages 893–902, COLING 2012, Mumbai, December 2012.  
We investigate the use of features expressing lexical generalizations over word forms when parsing web data and experiment with a range of web text samples, taken from the Ontonotes corpus, as well as the web 2.0 data sets described in Foster et al. (2011b). We obtain signiﬁcant improvements for a standard data-driven dependency parser when incorporating features expressing these lexical categories, and in fact ﬁnd that we may dispense with word form features altogether and still observe the same levels of improvement. KEYWORDS: Syntactic parsing, data-driven dependency parsing, web language, clustering, lemmatization, delexicalization.  903  Proceedings of COLING 2012: Posters, pages 903–912, COLING 2012, Mumbai, December 2012.  
Speech synthesis models are typically built from a corpus of speech that has accurate transcriptions. However, many of the languages of the world do not have a standardized writing system. This paper is an initial attempt at building synthetic voices for such languages. It may seem useless to develop a text-to-speech system when there is no text available. But we will discuss some well deﬁned use cases where we need these models. We will present our method to build synthetic voices from only speech data. We will present experimental results and oracle studies that show that we can automatically devise an artiﬁcial writing system for these languages, and build synthetic voices that are understandable and usable. TITLE AND ABSTRACT IN MARATHI अरपती नसलेा भाषासं ाठी वाणी सं ेषण विनमुत वाांया काेषापासून वाणी संले षणाची संगणकय ितपे बनवयासाठ या काेषाची अचूक लखत ितलपी उपलध असावी लागते. जगातील अनेक भाषा मा मानांकत अरपती वापरत नाहीत. तत काम हे अशा भाषांसाठ संले षत अावाज बनवयाचा एक पहला यास अाहे. मुळात अरपतीच नसताना या भाषेया लखत पाठ ाचे वाणी संले षण करयाचे तं हे यथ वाटू शकते. पण तत ले खात अाही या संले षण णालचे काही मुख उपयाेग सचवीत अाहाेत. के वळ विनमुत वाांचा काेष वापन संले षत अावाज बनवयाची अामची पत या ले खात अापण पा. अाही के ले ले याेग व वेषण असे दशवतात क अापण एखाद अरपती अापाेअाप शाेधू शकताे, जचा वापर कन के ले ले वाणी संले षण सगम व वापरयाजाेगे असते.  KEYWORDS: Speech Synthesis, Synthesis Without Text, Low Resource Languages, Languages without an Orthography. KEYWORDS IN L2: वाणी संले षण, पाठ ाशवाय संले षण, संसाधन-दुल भ भाषा, अरपती नसले या भाषा.  913  Proceedings of COLING 2012: Posters, pages 913–922, COLING 2012, Mumbai, December 2012.  
The Part of Speech (POS) tagging refers to the process of assigning appropriate lexical category to individual word in a sentence of a natural language. This paper describes the development of a POS tagger using rule based and supervised methods in Kokborok, a resource constrained and less computerized Indian language. In case of rule based POS tagging, we took the help of a morphological analyzer while for supervised methods, we employed two machine learning classifiers, Conditional Random Field (CRF) and Support Vector Machines (SVM). A total of 42,537 words were POS tagged. Manual checking achieves the accuracies of 70% and 84% in case of rule based and supervised POS tagging, respectively. KEYWORDS : Kokborok, POS Tagger, Suffix, Prefix, CRF, SVM, Morph analyser.  923  Proceedings of COLING 2012: Posters, pages 923–932, COLING 2012, Mumbai, December 2012.  
We present an efﬁcient framework to estimate the rule probabilities for a hierarchical phrasebased statistical machine translation system from parallel data. In previous work, this was done with bilingual parsing. We use a more efﬁcient approach splitting the bilingual parsing into two stages, which allows us to train a hierarchical translation model on larger tasks. Furthermore, we apply leave-one-out to counteract over-ﬁtting and use the expected count from the inside-outside algorithm to prune the rule set. On the WMT12 Europarl German→English and French→English tasks, we improve translation quality by up to 1.0 BLEU and 0.9 TER while simultaneously reducing the rule set to 5% of the original size. KEYWORDS: statistical machine translation, hierarchical decoding, translation model training, forced derivation.  933  Proceedings of COLING 2012: Posters, pages 933–942, COLING 2012, Mumbai, December 2012.  
This paper re-examines the widely held belief that the formalism underlying the rule system propounded by the ancient Indian grammarian, Pa¯n. ini (ca. 450–350 BCE), either anticipates or converges upon the same expressive power found in ﬁnite state control systems or the context-free languages that are used in programming language theory and computational linguistics. While there is indeed a striking but cosmetic resemblance to the contextualized rewriting systems used by modern morphologists and phonologists, a subtle difference in how rules are prevented from applying cyclically leads to a massive difference in generative capacity. The formalism behind Pa¯n. inian grammar, in fact, generates string languages not even contained within any of the multiple-component tree-adjoining languages, MCTAL(k), for any k. There is ample evidence, nevertheless, that Pa¯n. ini’s grammar itself judiciously avoided the potential pitfalls of this unconstrained formalism to articulate a large-coverage, but seemingly very tractable grammar of the Sanskrit language. Keywords: generative capacity, grammar formalisms, Pa¯n. ini, morphophonological rewrit- ing systems.  943  Proceedings of COLING 2012: Posters, pages 943–950, COLING 2012, Mumbai, December 2012.  
(2) Suntec Software (Shanghai) Co. Ltd. {xpqiu,fengji,11210240073,xjhuang}@fudan.edu.cn  A Segmentation and tagging task is the fundamental problem in natural language processing (NLP). Traditional methods solve this problem in either pipeline or joint cross-label ways, which suffer from error propagation and large number of labels respectively. In this paper, we present a novel joint model for segmentation and tagging, which integrates two dependent Markov chains. One chain is used for segmentation, and the other is for tagging. The model parameters can be estimated simultaneously. Besides, we can optimize the whole model by improving the single chain. The experiments show that our model could achieve higher performance over traditional models on both English shallow parsing and Chinese word segmentation and POS tagging tasks.  T  A  C  基于双链序列标注的联合切分和标注模型  在自然语言处理中， 序列标注模型是最常见的模型， 也有着广泛地应用。 针对常见 的可分解为分段和标注两个子任务的复杂序列标注问题，我们提出了双链序列标注模型。 该模型中存在着两条相互联系的马尔科夫链。为此我们提出了一个同时求解这两条链上最 优序列的解码算法。同时利用这两条链，针对不同的实际应用场景可以组合出不同的标注 模型，使用不同的解码算法完成实际的标注任务。为了能够适应不同的解码算法，我们还 提出了一个能够利用异构语料训练模型的参数学习算法。在多个语料上的实验表明，我们 提出的模型性能要优于其他模型，并能在同一个模型内完成多种标注任务。  K  : Coupled Sequences Labeling, Segmentation, Tagging.  K  C  : 双链序列标注, 切分, 标注.  951  Proceedings of COLING 2012: Posters, pages 951–964, COLING 2012, Mumbai, December 2012.  Figure 1: Coupled Sequences Labeling Figure 2: Factorial CRF Model (FCRF  Model (双链序列标注模型)  模型)  (a) segment by only using the linear chain s  (b) label a segmented sentence  (c) simultaneously segment and label a sentence  Figure 3: Coupled Sequences Labeling Model for Different Tasks (处理不同任务时的双链序列 标注模型变换), where gray nodes are observed nodes.  Table 1: Feature templates for shallow parsing (浅层句法分析特征模板)  Joint Cross-Product Model wi−2yi, wi−1yi, wiyi, wi+1yi, wi+2yi wi−1wiyi, wiwi+1yi pi−2yi, pi−1yi, piyi, pi+1yi, pi+2yi pi−2pi−1yi, pi−1piyi, pipi+1yi, pi+1pi+2yi pi−2pi−1piyi, pi−1pipi+1yi, pipi+1pi+2yi yi−1yi  Coupled Sequence Labeling Model wi−1si, wisi, wi+1si wi−2ti, wi−1ti, witi, wi+1ti, wi+2ti wi−1wisi, wiwi+1si wi−1witi, wiwi+1ti pi−1si, pisi, wi+1si pi−2ti, pi−1ti, piti, pi+1ti, pi+2ti pi−2pi−1si, pi−1pisi, pipi+1si, pi+1pi+2si pi−3pi−2ti, pi−2pi−1ti, pi−1piti, pipi+1ti, pi+1pi+2ti, pi+2pi+3ti, pi−1pi+1ti pi−2pi−1pisi, pi−1pipi+1si, pipi+1pi+2si wisiti wisi−1si wi−1ti−1ti, witi−1ti, pi−1ti−1ti, piti−1ti si−1ti−1si, ti−1siti  952  Table 2: Feature templates for Chinese S&T (中文分词、词性标注特征模板)  Joint Cross-Label Model ci−2yi, ci−1yi, ciyi, ci+1yi, ci+2yi ci−1ciyi, cici+1yi, ci−1ci+1yi yi−1yi  Coupled Sequence Labeling Model ci−2si, ci−1si, cisi, ci+1si, ci+2si ci−3ti, ci−2ti, ci−1ti, citi, ci+1ti, ci+2ti, ci+3ti ci−1cisi, cici+1si, ci−1ci+1si ci−3ci−2ti, ci−2ci−1ti, ci−1citi, cici+1ti, ci+1ci+2ti, ci+2ci+3ti, ci−2citi, cici+2ti cisiti ci−1ti−1ti, si−1si si−1ti−1si, ti−1siti  input : Taggging training dataset: (xi, si, ti), i = 1, · · · , M , ; input : Segmentation training dataset (optional): (xi, si), i = M + 1, · · · , M + N , ; input : Parameters: C, K. output: w Initialize: cw ← 0, w ← 0; for k = 0 · · · K − 1 do random select an integer number l ∈ (1, . . . , M + N ) with no repeat; if l ≤ M then receive an example (xl, sl, tl); predict (2nd Viterbi): (ˆsl, tˆl) = arg max ⟨w, Φst(xl, s, t)⟩; s,t if (ˆsl,ˆtl) ̸= (sl, tl) then update with w with Eq. 10, where (·) is (xl, sl, tl) and (∗) is (xl, ˆsl, tˆl); end else receive an example (xl, sl); predict (1st Viterbi): ˆsl = arg max ⟨w, Φs(xl, s)⟩; s if ˆsl ̸= si then update with w with Eq. 10, where (·) is (xl, sl) and (∗) is (xl, ˆsl); end end end w = cw/K ; Algorithm 1: Online Learning Algorithm for Coupled Sequences Labeling Model. (双链序列标 注在线学习算法)  953  
We discuss making syntactic annotation for learner language more precise, by clarifying the properties which the layers of annotation refer to. Building from previous proposals which split linguistic annotation into multiple layers to capture non-canonical properties of learner language, we lay out the questions which must be asked for grammatical annotation and provide some answers. Our investigation points to the layer of distributional syntax being based on properties of the target language (L2) and largely redundant with the other layers. We show, for example, that subcategorization seems to better be able to underspecify annotation for situations where no single correct solution can be found. While this paves the way for applying the annotation to larger corpus efforts, it also represents a signiﬁcant step in elucidating syntax for non-canonical language. KEYWORDS: syntactic annotation, dependency syntax, learner language.  965  Proceedings of COLING 2012: Posters, pages 965–974, COLING 2012, Mumbai, December 2012.  
The recent availability of typological databases such as World Atlas of Language Structures (WALS) has spurred investigations regarding their utility for language classiﬁcation, the stability of typological features in genetic linguistics and typological universals across the language families of the world. Existing work on building NLP resources such as parallel corpora, treebanks for under-resourced languages has a lot to gain by taking into consideration insights about inter-language relationships. Since Yarowsky et al. (2001), there have been a number of attempts to create resources for resource-poor languages by projecting information from resource-rich languages using comparable corpora. An important intuition in such work is that syntactic information can be transferred with higher accuracy between languages if they are similar. In this paper, we compare typological distances derived from ﬁfteen vector similarity measures with family internal classiﬁcations and also lexical divergence. These results are only a ﬁrst step towards the use of WALS database in the projection of NLP resources for typologically or genetically similar, yet resource-poor languages. KEYWORDS: WALS, ASJP, Vector similarity, Internal classiﬁcation, Typological features.  975  Proceedings of COLING 2012: Posters, pages 975–984, COLING 2012, Mumbai, December 2012.  
We review the state of the art in automated sentence boundary detection (SBD) for English and call for a renewed research interest in this foundational ﬁrst step in natural language processing. We observe severe limitations in comparability and reproducibility of earlier work and a general lack of knowledge about genre- and domain-speciﬁc variations. To overcome these barriers, we conduct a systematic empirical survey of a large number of extant approaches, across a broad range of diverse corpora. We further observe that much previous work interpreted the SBD task too narrowly, leading to overly optimistic estimates of SBD performance on running text. To better relate SBD to practical NLP use cases, we thus propose a generalized deﬁnition of the task, eliminating text- or language-speciﬁc assumptions about candidate boundary points. More speciﬁcally, we quantify degrees of variation across ‘standard’ corpora of edited, relatively formal language, as well as performance degradation when moving to less formal language, viz. various samples of user-generated Web content. For these latter types of text, we demonstrate how moderate interpretation of document structure (as is now often available more or less explicitly through mark-up) can substantially contribute to overall SBD performance. KEYWORDS: Sentence Boundary Detection, Segmentation, Comparability, Reproducibility.  985  Proceedings of COLING 2012: Posters, pages 985–994, COLING 2012, Mumbai, December 2012.  
There has been a lot of recent interest in Semantic Parsing, centering on using data-driven techniques for mapping natural language to full semantic representations (Mooney, 2007). One particular focus has been on learning with ambiguous supervision (Chen and Mooney, 2008; Kim and Mooney, 2012), where the goal is to model language learning within broader perceptual contexts (Mooney, 2008). We look at learning light inference patterns for Semantic Parsing within this paradigm, focusing on detecting speaker commitments about events under discussion (Nairn et al., 2006; Karttunen, 2012). We adapt PCFG induction techniques (Börschinger et al., 2011; Johnson et al., 2012) for learning inference using event polarity and context as supervision, and demonstrate the effectiveness of our approach on a modiﬁed portion of the Grounded World corpus (Bordes et al., 2010). KEYWORDS: Semantic Parsing, Computational Semantics, Detecting Textual Entailment, Gram- mar Induction.  1007  Proceedings of COLING 2012: Posters, pages 1007–1018, COLING 2012, Mumbai, December 2012.  
We proposed a hierarchical domain model (HDM)-based multi-domain selection framework (MDSF) for multi-domain dialog systems. The HDM-based MDSF statistically detects one or more candidate domains and heuristically determines one or more final domains from among the candidate domains. The HDM is used in both the candidate domain detection and final domain determination components. Multi-domain dialog systems that employ the HDM-based MDSF provide service to one or more domains at the same time, whereas traditional multi-domain dialog systems provide service to only one domain at a time. To validate the HDM-based MDSF, we developed a multi-domain dialog system for TV program, video-on-demand, and TV device domains. The experimental results show that the HDM-based MDSF correctly selects one or more domains and enables multi-domain dialog systems to provide more accurate and rapid dialog service than traditional multi-domain dialog systems. TITLE AND ABSTRACT IN KOREAN 다중 도메인 대화 시스템을 위한 계층적 도메인 모델 기반의 다중 도메인 선택 프레임워크 본 논문은 다중 도메인 대화 시스템을 위한 계층적 도메인 모델 기반의 다중 도메인 선택 프레임워크를 제안한다. 계층적 도메인 모델 기반의 다중 도메인 선택 프레임워크는 통계적 방법으로 한 개 이상의 후보 도메인을 검출하고, 규칙으로 후보 도메인 중 한 개 이상의 최종 도메인을 결정한다. 후보 도메인 검출 및 최종 도메인 결정 단계에서 계층적 도메인 모델이 사용된다. 기존의 다중 도메인 대화 시스템이 한 번에 한 개의 도메인에 대한 서비스만을 제공하는 반면, 계층적 도메인 모델 기반의 다중 도메인 선택 프레임워크를 적용한 다중 도메인 대화 시스템은 한 번에 한 개 이상의 도메인에 대한 서비스를 제공한다. TV 프로그램, 주문형 비디오, TV 장치에 대한 다중 도메인 대화 시스템에 대한 실험을 통해 계층적 도메인 모델 기반의 다중 도메인 선택 프레임워크는 한 번에 한 개 이상의 도메인을 정확하게 선택할 수 있고, 다중 도메인 대화 시스템이 기존 다중 도메인 대화 시스템에 비해 정확하고 신속한 대화 서비스를 제공할 수 있게 함을 확인할 수 있었다. KEYWORDS : Multi-domain dialog system; Multi-domain selection; Hierarchical domain model; Candidate domain detection; Final domain determination KEYWORDS IN KOREAN : 다중 도메인 대화 시스템; 다중 도메인 선택; 계층적 도메인 모델; 후보 도메인 검출; 최종 도메인 결정 Proceedings of COLING 2012: Posters, pages 1049–1058, COLING 2012, Mumbai, December 2012. 1049  
We describe a large coreference annotation task performed on a corpus of 266 papers from the ACL Anthology, a publicly, electronically available collection of scientiﬁc papers in the domain of computational linguistics and language technology. The annotation comprises mainly noun phrase coreference of the full textual content of each paper in the Anthology subset. It has been performed carefully and at least twice for each paper (initial annotation and secondary correction phase). The purpose of this paper is to summarize the comprehensive annotation schema and release the corpus publicly, along with this paper. The corpus is by far larger than the ACE coreference corpora. It can be used to train coreference resolution systems in the Computational Linguistics and Language Technology domain for semantic search, taxonomy extraction, question answering, citation analysis, scientiﬁc discourse analysis, etc. KEYWORDS: Coreference Resolution, Resource, Annotated Corpus, Scientiﬁc Papers, eScience. 
This paper presents a new approach to perform the estimation of the translation model probabilities of a phrase-based statistical machine translation system. We use neural networks to directly learn the translation probability of phrase pairs using continuous representations. The system can be easily trained on the same data used to build standard phrase-based systems. We provide experimental evidence that the approach seems to be able to infer meaningful translation probabilities for phrase pairs not seen in the training data, or even predict a list of the most likely translations given a source phrase. The approach can be used to rescore n-best lists, but we also discuss an integration into the Moses decoder. A preliminary evaluation on the English/French IWSLT task achieved improvements in the BLEU score and a human analysis showed that the new model often chooses semantically better translations. Several extensions of this work are discussed. KEYWORDS: Statistical machine translation, phrase probability estimation, continuous space models, neural network.  1071  Proceedings of COLING 2012: Posters, pages 1071–1080, COLING 2012, Mumbai, December 2012.  
Syntactic dependency structures are based on the assumption that there is exactly one node in the structure for each word in the sentence. However representing elliptical constructions (e.g. missing verbs) is problematic as the question where the dependents of the elided material should be attached to has to be solved. In this paper, we present an in-depth study into the challenges of introducing empty heads into dependency structures during automatic parsing. Structurally, empty heads provide an attachment site for the dependents of the non-overt material and thus preserve the linguistically plausible structure of the sentence. We compare three different (computational) approaches to the introduction of empty heads and evaluate them against German and Hungarian data. We then conduct a ﬁne-grained error analysis on the output of one of the approaches to highlight some of the difﬁculties of the task. We ﬁnd that while a clearly deﬁned part of the phenomena can be learned by the parser, more involved elliptical structures are still mostly out of reach of the automatic tools. KEYWORDS: Empty heads, statistical dependency parsing, German, Hungarian.  1081  Proceedings of COLING 2012: Posters, pages 1081–1090, COLING 2012, Mumbai, December 2012.  
Transductive SVM (TSVM) is a well known semi-supervised large margin learning method for binary text classiﬁcation. In this paper we extend this method to multi-class and hierarchical classiﬁcation problems. We point out that the determination of labels of unlabeled examples with ﬁxed classiﬁer weights is a linear programming problem. We devise an efﬁcient technique for solving it. The method is applicable to general loss functions. We demonstrate the value of the new method using large margin loss on a number of multi-class and hierarchical classiﬁcation datasets.  1091  Proceedings of COLING 2012: Posters, pages 1091–1100, COLING 2012, Mumbai, December 2012.  
In this paper, we consider a speciﬁc part of statistical machine translation: feature estimation for the translation model. The classical way to estimate these features is based on relative frequencies. In this new approach, we propose to use the concept of belief masses to estimate the phrase translation probabilities. The Belief Function theory has proven to be suitable and adapted for dealing with uncertainties in many domains. We have performed a series of experiments to translate from English into French and from Arabic into English showing that our approach performs, at least as well as and at times better than, the classical approach. KEYWORDS: Belief function, Statistical Machine Translation.  1101  Proceedings of COLING 2012: Posters, pages 1101–1110, COLING 2012, Mumbai, December 2012.  
Wikipedia articles are annotated by volunteer contributors with numerous links that connect words and phrases to relevant titles in Wikipedia. In this paper, we identify inconsistencies in the user annotation of links and show that they can have a substantial impact on the performance of word sense disambiguation systems that are trained on Wikipedia links. We describe two major types of link annotations – sense and reference – that are frequently used without being explicitly distinguished in Wikipedia, and present an approach to training sense and reference disambiguation systems in the presence of such annotation inconsistencies. Experimental results demonstrate that accounting for annotation ambiguity in Wikipedia links leads to signiﬁcant improvements in disambiguation accuracy. KEYWORDS: word sense disambiguation, Wikipedia. TITLE AND ABSTRACT IN ROMANIAN Dezambiguizare de Sensuri si Referinte in Wikipedia Articolele din Wikipedia sunt adnotate de editori voluntari cu numeroase link-uri ce conecteaza fraze din articol cu titluri relevante in Wikipedia. In acest articol descriem inconsecvente in adnotarile editorilor si aratam ca ele pot avea un impact substantial asupra performantei sistemelor de dezambiguizare care sunt antrenate cu link-uri din Wikipedia. Descriem doua tipuri majore de adnotari – sensuri si referinte – care sunt folosite frecvent fara a ﬁ diferentiate explicit in Wikipedia. Prezentam modele de invatare automata pentru dezambiguizare de sensuri si referinte care pot ﬁ antrenate in prezenta acestor ambiguitati de adnotare. Evaluarea experimentala a acestor modele conﬁrma o imbunatatire semniﬁcativa a performantei de dezambiguizare. KEYWORDS: dezambiguizare de sensuri, Wikipedia.  1111  Proceedings of COLING 2012: Posters, pages 1111–1120, COLING 2012, Mumbai, December 2012.  
We present the ﬁrst fully unsupervised approach to metaphor interpretation, and a system that produces literal paraphrases for metaphorical expressions. Such a form of interpretation is directly transferable to other NLP applications that can beneﬁt from a metaphor processing component. Our method is different from previous work in that it does not rely on any manually annotated data or lexical resources. First, our method computes candidate paraphrases according to the context in which the metaphor appears, using a vector space model. It then uses a selectional preference model to measure the degree of literalness of the paraphrases. The system identiﬁes correct paraphrases with a precision of 0.52 at top rank, which is a promising result for a fully unsupervised approach. KEYWORDS: Metaphor, paraphrasing, lexical substitution, vector space model.  1121  Proceedings of COLING 2012: Posters, pages 1121–1130, COLING 2012, Mumbai, December 2012.  
We have developed a cohesive extraction based single document summarizer (COHSUM) based on coreference links in a document. The sentences providing the most references to other sentences and that other sentences are referring to, are considered the most important and are therefore extracted. Additionally, before evaluations of summary quality, a corpus analysis was performed on the original documents in the dataset in order to investigate the distribution of coreferences. The quality of the summaries is evaluated in terms of content coverage and cohesion. Content coverage is measured by comparing the summaries to manually created gold standards and cohesion is measured by calculating the amount of broken and intact coreferences in the summary compared to the original texts. The summarizer is compared to the summarizers from DUC 2002 and a baseline consisting of the ﬁrst 100 words. The results show that COHSUM, aimed only at maintaining a cohesive text, performed better regarding text cohesion compared to the other summarizers and on par with the other summarizers and the baseline regarding content coverage. Keywords: Summarization, Coreference resolution, Cohesion.  1161  Proceedings of COLING 2012: Posters, pages 1161–1170, COLING 2012, Mumbai, December 2012.  
Inspired by work on robust optimization we introduce a subspace method for learning linear classiﬁers for natural language processing that are robust to out-of-vocabulary effects. The method is applicable in live-stream settings where new instances may be sampled from different and possibly also previously unseen domains. In text classiﬁcation and part-of-speech (POS) tagging, robust perceptrons and robust stochastic gradient descent (SGD) with hinge loss achieve average error reductions of up to 18% when evaluated on out-of-domain data. KEYWORDS: robust learning, regularization, document classiﬁcation, part-of-speech tagging.  1171  Proceedings of COLING 2012: Posters, pages 1171–1180, COLING 2012, Mumbai, December 2012.  
This work is about connotative aspects of words, often not carried over in translation, which depend on speciﬁc cultures. A cross-language computational study is presented, based on exploitation of similarity techniques on large corpora of news documents in English, Arabic, and Hebrew. In particular, focus of the exploration is on speciﬁc terms expressing emotion, negotiation and conﬂict. KEYWORDS: Multilinguality, Affective Language, Emotions in Language. KEYWORDS IN L2: Here a list of keywords in L2 (if option used).  1201  Proceedings of COLING 2012: Posters, pages 1201–1208, COLING 2012, Mumbai, December 2012.  
This paper presents a novel approach for inducing causal rules by using deverbal nouns as a clue for ﬁnding causal relations. We collect verbs and their deverbal forms from FrameNet, and extract pairs of sentences in which event verbs and their corresponding deverbal forms co-occur in documents. The most challenging part of this work is to generalize an instance of causal relation into a rule. This paper proposes a method to generalize and constrain causal rules so that the obtained rules have the high chance of applicability and reusability. In order to ﬁnd a suitable constraint for a causal rule, we utilize relation instances extracted by an open-information extractor, and build a classiﬁer to choose the most suitable constraint. We demonstrate that deverbal nouns provide a good clue for causal relations and that the proposed method can induce causal rules from deverbal noun constructions. KEYWORDS: causal relation, rules, pattern generalization, semantic inference, knowledge acquisition.  1209  Proceedings of COLING 2012: Posters, pages 1209–1218, COLING 2012, Mumbai, December 2012.  
Word alignment is a critical component of machine translation systems. Various methods for word alignment have been proposed, and different models can produce signiﬁcantly different outputs. To exploit the advantages of different models, we propose three ways to combine multiple alignments for machine translation: (1) alignment selection, a novel method to select an alignment with the least expected loss from multiple alignments within the minimum Bayes risk framework; (2) alignment reﬁnement, an improved algorithm to reﬁne multiple alignments into a new alignment that favors the consensus of various models; (3) alignment compaction, a compact representation that encodes all alignments generated by different methods (including (1) and (2) above) using a novel calculation of link probabilities. Experiments show that our approach not only improves the alignment quality, but also signiﬁcantly improves translation performance by up to 1.96 BLEU points over single best alignments, and 1.28 points over merging rules extracted from multiple alignments individually. KEYWORDS: alignment combination, minimum Bayes risk, alignment reﬁnement, weighted alignment matrix.  1249  Proceedings of COLING 2012: Posters, pages 1249–1260, COLING 2012, Mumbai, December 2012.  Alignments GIZA++ Berkeley Vigne  GIZA++ – 70.29% 75.17%  Berkeley 70.29% – 73.25%  Vigne 75.17% 73.25% –  Table 1: Agreement of alignment links between different alignment models. Here we use three different alignment models: GIZA++ (Och and Ney, 2003), the unsupervised Berkeley aligner (Liang et al., 2006), and a discriminative aligner Vigne (Liu et al., 2010).  
Most research on run-time efﬁciency in information extraction is of empirical nature. This paper analyzes the efﬁciency of information extraction pipelines from a theoretical point of view in order to explain empirical ﬁndings. We argue that information extraction can, at its heart, be viewed as a relevance ﬁltering task whose efﬁciency traces back to the run-times and selectivities of the employed algorithms. To better understand the intricate behavior of information extraction pipelines, we develop a sequence model for scheduling a pipeline’s algorithms. In theory, the most efﬁcient schedule corresponds to the Viterbi path through this model and can hence be found by dynamic programming. For real-time applications, it might be too expensive to compute all run-times and selectivities beforehand. However, our model implies the benchmarks of ﬁltering tasks and illustrates that the optimal schedule depends on the distribution of relevant information in the input texts. We give formal and experimental evidence where necessary. TITLE AND ABSTRACT IN GERMAN Optimales Scheduling von Information-Extraction-Verfahren Nahezu alle Forschung zur Laufzeitefﬁzienz in der Information Extraction ist empirischer Natur. Die vorliegende Arbeit analysiert die Efﬁzienz von Information-Extraction-Pipelines aus theoretischer Sicht, um empirische Erkenntnisse zu erklären. Wir sehen Information Extraction im Kern als Relevanz-Filteraufgabe an, deren Efﬁzienz auf die Laufzeiten und Selektivitäten der eingesetzten Algorithmen zurückgeht. Zum besseren Verständnis des komplexen Verhaltens von Information-Extraction-Pipelines entwickeln wir ein Sequenzmodell für das Scheduling der Algorithmen einer Pipeline. Theoretisch entspricht der efﬁzienteste Schedule dem Viterbi-Pfad durch dieses Modell und lässt sich daher mittels dynamischer Programmierung ﬁnden. Für Echtzeitanwendungen kann es zu teuer sein, alle Laufzeiten und Selektivitäten im Vorhinein zu berechnen. Unser Modell impliziert jedoch die Benchmarks von Filteraufgaben und zeigt, dass der optimale Schedule von der Verteilung relevanter Informationen in den Eingabetexten abhängt. Wo nötig, führen wir sowohl formale als auch experimentelle Belege an. KEYWORDS: information extraction, theory, efﬁciency, scheduling. KEYWORDS IN GERMAN: Information Extraction, Theorie, Efﬁzienz, Scheduling.  1281  Proceedings of COLING 2012: Posters, pages 1281–1290, COLING 2012, Mumbai, December 2012.  
Proceedings of COLING 2012: Posters, pages 1291–1300, COLING 2012, Mumbai, December 2012. 1291  
Sentence realization, as one of the important components in natural language generation, has taken a statistical swing in recent years. While most previous approaches make heavy usage of lexical information in terms of N -gram language models, we propose a novel method based on unlexicalized tree linearization grammars. We formally deﬁne the grammar representation and demonstrate learning from either treebanks with gold-standard annotations, or automatically parsed corpora. For the testing phase, we present a linear time deterministic algorithm to obtain the 1-best word order and further extend it to perform exact search for n-best linearizations. We carry out experiments on various languages and report state-of-the-art performance. In addition, we discuss the advantages of our method on both empirical aspects and its linguistic interpretability. Keywords: Tree Linearization Grammar, Sentence Realization, Dependency Tree.  1301  Proceedings of COLING 2012: Posters, pages 1301–1310, COLING 2012, Mumbai, December 2012.  
The overall sentiment of a text is critically affected by its discourse structure. By splitting a text into text spans with different discourse relations, we automatically train the weights of different relations in accordance with their importance, and then make use of discourse structure knowledge to improve sentiment classification. In this paper, we utilize explicit connectives to predict discourse relations, and then propose several methods to incorporate discourse relation knowledge to the task of sentiment analysis. All our methods integrating discourse relations perform better than the baseline methods, validating the effectiveness of using discourse relations in Chinese sentiment analysis. We also automatically find out the most influential discourse relations and connectives in sentiment analysis.  TITLE AND ABSTRACT IN CHINESE  基 际 系的情感 析方法  文档情感 篇章结构 相 。将一篇文档  有 同 际 系的文 语段，可  自动训练并获得表征 同 际 系重要性的权重，进而利用 篇章结构信 来提升情感  析的性能。 文利用显性 联标记来预测 际 系，继而提出了多种 同的方法利用  际 系来进行情感 析。实验结果表明，融合 际 系的方法均优 前人的情感 析方  法，证明了 际 系对 汉语篇章情感 析的重要作用。 文 自动发现了情感计算中显  要的 际 系类型和显要的 联标记。  KEYWORDS: sentiment analysis; discourse relation; connective. KEYWORDS IN CHINESE 情感 析 际 系 联标记  Proceedings of COLING 2012: Posters, pages 1311–1320, COLING 2012, Mumbai, December 2012. 1311  
We address the problem of matching jobs with workers when information about both elements is incomplete and in some cases inaccurate. Such a situation occurs, for example, when proﬁle information is generated from recorded audio, rather than typed or written sources. We present various methods of dealing with such post-processed voice information and show how it compares to human generated matches over the same data. Our analysis includes both SQL- and ontological-based methods that provide higher recall over a sparse data. A probabilistic weighted ontology model is proposed that enables assignment of realistic weights to different attributes and considers probabilistic conversion of audio to text. The evaluation is performed on real-life data from 1,100 candidates and 48 jobs spanning more than 3,000 vacancies. KEYWORDS: Spoken Web, job search, resume matching, ontology.  1321  Proceedings of COLING 2012: Posters, pages 1321–1330, COLING 2012, Mumbai, December 2012.  
This paper deals with Discourse Argument Identification (DAI) from both intra-sentence and inter-sentence perspectives. For intra-sentence cases, we approach it via a simplified shallow semantic parsing framework, which recasts the discourse connective as the predicate and its scope into several constituents as the argument of the predicate. Different from state-of-the-art chunking approaches, our parsing approach extends DAI from the chunking level to the parse tree level, where rich syntactic information is available, and focuses on determining whether a constituent, rather than a token, is an argument or not. For inter-sentence cases, we present a lightweight heuristic rule-based solution. Evaluation using Penn Discourse Treebank (PDTB) shows that the current research’s parsing approach significantly outperforms the state-of-the-art chunking alternatives. TITLE AND ABSTRACT IN CHINESE 基于浅层语义分析的篇章论元识别统一框架 本文从句内和句外两种角度处理篇章论元识别问题. 针对句内情况, 我们采用浅层语义分 析框架来处理, 将篇章连接词看作谓词, 并将谓词的论元映射成一些组块. 不同于现有的 基于组块方法, 我们的语义分析方法将组块层次提升为富于句法信息的句法树层次, 同时 将组块而不是单词作为处理单元. 针对句外情况, 我们提出了一种轻量级的规则解决方案. 通过宾州篇章树库上的实验, 说明我们提出的基于语义分析方法在性能上显著优于现有的 基于组块方法. KEYWORDS : Argument Pruning, Discourse Argument Identification, Shallow Semantic Parsing. KEYWORDS IN CHINESE : 论元过滤, 篇章论元识别, 浅层语义分析.  * Corresponding author  Proceedings of COLING 2012: Posters, pages 1331–1340, COLING 2012, Mumbai, December 2012. 1331  
While most recent work has focused on instances of opinion spam which are manually identiﬁable or deceptive opinion spam which are written by paid writers separately, in this work we study both of these interesting topics and propose an effective framework which has good performance on both datasets. Based on the golden-standard opinion spam dataset, we propose a novel model which integrates some deep linguistic features derived from a syntactic dependency parsing tree to discriminate deceptive opinions from normal ones. On a background of multiple language tasks, our model is evaluated on both English (gold-standard) and Chinese (non-gold) datasets. The experimental results show that our model produces state-of-the-art results on both of the topics. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (MANDARIN) Ð¢5-Ã&E¥ p?ŠóA &¢ •C§NõïÄóŠ©OX-ïÄ ŒÃÄ£O ¿„-ÃÚdÂ¤ Ã? Ð ¢5¿„-Ã" ©¥§·‚JÑ ˜‡1ƒk µe§¿3ùü‡ØÓ ÌK êâ 8þÑ¼ ûÐ ?n J"ÄuIO ¿„-Ãêâ8§ ©JÑ ˜‡8¤ •• é{ä p?ŠóA # .§^uÐ¢5-Ã¿„ £O"3õŠó?Ö µe§¤ JÑ .©O3=©êâ8£…’ Ã? ¤Ú¥©£ÃóI5 ¤êâ8þ?1 µ "¢ (JL²§¤JÑ .þU¼ 8c•Ž •`(J" KEYWORDS: Opinion Spam, Multi-Language, Deep Linguistic Features. KEYWORDS IN MANDARIN: ¿„-Ã, õŠó, ÝŠóA .  †Corresponding author ∗This work was partially supported by the National Natural Science Foundation of China (Grant No. 60903119 and Grant No. 61170114), the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 20110073120022, the National Basic Research Program of China (Grant No. 2009CB320901), and the European Union Seventh Framework Program (Grant No. 247619).  1341  Proceedings of COLING 2012: Posters, pages 1341–1350, COLING 2012, Mumbai, December 2012.  
Community structure is a common attribute of many social networks, which would give us a better understanding of the networks. However, as the social networks grow lager and lager nowadays, the Pareto Principle becomes more and more notable which makes traditional community discovery algorithms no longer suitable for them. This principle explains the unbalanced existence of two different types of network actors. Speciﬁcally, the core actors usually occupy only a small proportion of the population, but have a large inﬂuence. In this paper, we propose a novel algorithm LCDN (Latent Community Discovery with Network Structure) for dividing the core actors. This is a hierarchical probabilistic model based on statistical topic model and regularized by network structure in data. We had experiments on three large networks which show that this new model performs much better than the traditional statistical models and network partitioning algorithms. KEYWORDS: community discovery, statistical topic models, social networks, core actors, regu- larization.  1351  Proceedings of COLING 2012: Posters, pages 1351–1360, COLING 2012, Mumbai, December 2012.  
Inferring lexical type labels for entity mentions in texts is an important asset for NLP tasks like semantic role labeling and named entity disambiguation (NED). Prior work has focused on ﬂat and relatively small type systems where most entities belong to exactly one type. This paper addresses very ﬁne-grained types organized in a hierarchical taxonomy, with several hundreds of types at different levels. We present HYENA for multi-label hierarchical classiﬁcation. HYENA exploits gazetteer features and accounts for the joint evidence for types at different levels. Experiments and an extrinsic study on NED demonstrate the practical viability of HYENA. KEYWORDS: Fine-grained entity types, multi-labeling, hierarchical classiﬁcation, meta- classiﬁcation.  1361  Proceedings of COLING 2012: Posters, pages 1361–1370, COLING 2012, Mumbai, December 2012.  
我们曾开发了一个能跟用户进行虚拟戏剧 席创作交流的智能代理. 它能从有明显情感迹 象的单句话中检测情感. 然而，很多不具有情感词的句子也有很强的情感寓意 被认为是 中性. 在这里, 我们使用 Latent Semantic Analysis,摆脱语言特征的限制，用话题和目 标 众的检测去识别情感隐 在那些只具有微弱情感信号的输入中. 并讨论怎样从这样的 输入中, 使用基于神经网络的 文检测去识别情感. KEYWORDS : Affect detection, semantic interpretation, drama improvisation KEYWORDS IN CHINESE : 情感检测， 语义解析， 戏剧 席创作 Proceedings of COLING 2012: Posters, pages 1381–1390, COLING 2012, Mumbai, December 2012. 1381  
Beam-search and global models have been applied to transition-based dependency parsing, leading to state-of-the-art accuracies that are comparable to the best graph-based parsers. In this paper, we analyze the effects of global learning and beam-search on the overall accuracy and error distribution of a transition-based dependency parser. First, we show that global learning and beam-search must be jointly applied to give improvements over greedy, locally trained parsing. We then show that in addition to the reduction of error propagation, an important advantage of the combination of global learning and beam-search is that it accommodates more powerful parsing models without overﬁtting. Finally, we characterize the errors of a global, beam-search, transition-based parser, relating it to the classic contrast between “local, greedy, transition-based parsing” and “global, exhaustive, graph-based parsing”. TITLE AND ABSTRACT IN CHINESE 分析全局模型和柱搜索对基于转移依存分析器的影响 柱搜索和全局模型被应用于基于转移的依存分析，可以取得与最好的基于图的依存分析器 同一水平的精度。我们分析全局学习和柱搜索对基于转移的依存分析器的精度与错误分布 的影响。首先，全局学习和柱搜索需要同时使用才能达到显著优于局部学习和贪婪搜索的 效果。此外，全局学习和柱搜索的联合使用不仅可以减少错误蔓延，还可以支持更为复杂 的模型训练而不过拟合。最后，我们对应用了全局学习和柱搜索的基于转移的依存分析器 进行错误分析，且将此分析与对MaltParser与MSTParser的错误对比相比较。 KEYWORDS: Dependency parsing, error analysis, ZPar, MaltParser, MSTParser. KEYWORDS IN CHINESE: 依存分析, 错误分析, ZPar, MaltParser, MSTParser  1391  Proceedings of COLING 2012: Posters, pages 1391–1400, COLING 2012, Mumbai, December 2012.  
In this paper, we introduce new features for question-answering systems. These features are inspired by the fact that justiﬁcation of the correct answer (out of many candidate answers) may be present in multiple passages. Our features attempt to combine evidence from multiple passages retrieved for a candidate answer. We present results on two data-sets: Jeopardy! and Doctor’s Dilemma. In both data-sets, our features are ranked highest in correlation with gold class (in the training data) and signiﬁcantly improve the performance of our existing QA system, Watson. Keywords: Question Answering, multi-dimensional feature merger, Watson. Proceedings of COLING 2012: Technical Papers, pages 1–16, COLING 2012, Mumbai, December 2012. 
Unsupervised Relation Extraction (URE) is the task of extracting relations of a priori unknown semantic types using clustering methods on a vector space model of entity pairs and patterns. In this paper, we show that an informed feature generation technique based on dependency trees signiﬁcantly improves clustering quality, as measured by the F-score, and therefore the ability of the URE method to discover relations in text. Furthermore, we extend URE to produce a set of weighted patterns for each identiﬁed relation that can be used by an information extraction system to ﬁnd further instances of this relation. Each pattern is assigned to one or multiple relations with different conﬁdence strengths, indicating how reliably a pattern evokes a relation, using the theory of Discriminative Category Matching. We evaluate our ﬁndings in two tasks against strong baselines and show signiﬁcant improvements both in relation discovery and information extraction. KEYWORDS: Unsupervised Relation Extraction, Clustering, Vector Space Models. Proceedings of COLING 2012: Technical Papers, pages 17–32, COLING 2012, Mumbai, December 2012. 17  
We investigate differences in point of view (POV) between two objective documents, where one is describing the subject matter in a more positive/negative way than the other, and present an automatic method for detecting such POV differences. We use Amazon Mechanical Turk (AMT) to annotate sentences as positive, negative or neutral based on their POV towards a given target. A statistical classiﬁer is trained to predict the POV score of a document, which reﬂects how positive/negative the document’s POV towards its target is. The results of our experiments on a set of articles in the Arabic and English Wikipedias from the people category show that our method successfully detects POV differences. KEYWORDS: sentiment analysis, content analysis, natural language processing. Proceedings of COLING 2012: Technical Papers, pages 33–50, COLING 2012, Mumbai, December 2012. 33  
Online content analysis employs algorithmic methods to identify entities in unstructured text. Both machine learning and knowledge-base approaches lie at the foundation of contemporary named entities extraction systems. However, the progress in deploying these approaches on web-scale has been been hampered by the computational cost of NLP over massive text corpora. We present SpeedRead (SR), a named entity recognition pipeline that runs at least 10 times faster than Stanford NLP pipeline. This pipeline consists of a high performance Penn Treebankcompliant tokenizer, close to state-of-art part-of-speech (POS) tagger and knowledge-based named entity recognizer. KEYWORDS: Tokenization, Part Of Speech, Named Entity Recognition, NLP pipelines. Proceedings of COLING 2012: Technical Papers, pages 51–66, COLING 2012, Mumbai, December 2012. 51  
In this article we investigate the translation of ﬁnancial terms from English into German in the isolation of an ontology vocabulary. For this study we automatically built new domain-speciﬁc resources from the translation search engine Linguee and from the online encyclopaedia Wikipedia. Due to the fact that we performed the translation approach on a monolingual ontology, we ran several sub-experiments to ﬁnd the most appropriate model to translate the ﬁnancial vocabulary. The ﬁndings from these experiments lead to the conclusion that a hybrid translation system, a combination of bilingual terminological resources and statistical machine translation, can help to improve translation of domain-speciﬁc terms. Finally we undertook a manual cross-lingual evaluation on the monolingual ontology to get a better understanding on this speciﬁc short text translation task. Keywords: Ontologies and terminology, Empirical machine translation. Proceedings of COLING 2012: Technical Papers, pages 67–82, COLING 2012, Mumbai, December 2012. 67  
In this study, we explore the impact of complex lexical information to solve syntactic ambiguity, including verbal subcategorization in the form of verbal transitivity and verb-noun-case or verb-noun-case-auxiliary relations. The information was obtained from different sources, including a subcategorization dictionary extracted from a Basque corpus, the web as a corpus, an English corpus and a Basque dictionary. Functional ambiguity between subject and object is a widespread problem in Basque, where 22% of subjects and objects are ambiguous, and this ambiguity surfaces in 33% of the sentences. This problem is comparable to PP attachment ambiguities in other languages. Our results show that, using complex lexical information, our results are better than a state-of-the-art statistical parser, obtaining a statistically signiﬁcant error reduction of 20%. The disambiguation system is independent on the actual parsing algorithm used. The analysis revealed that the most relevant information are the case carried by the noun and the transitivity of the verb. TITLE AND ABSTRACT IN BASQUE Informazio lexikal konplexuaren ekarpena euskarazko anbiguotasun sintaktikoen ebazpenean Lan honetan informazio lexikal konplexua erabiltzearen garrantzia aztertzen dugu euskarazko anbiguotasun sintaktikoen ebazpenean. Aditzen iragankortasuna erakusten duen azpikategorizazioaren ekarpena aztertu dugu, baita aditz-izen-kasu eta aditz-izen-kasu-laguntzaile erlazioena ere. Informazio horiek hainbat iturritatik jaso ditugu: euskarazko corpus batetik, webetik berau corpus gisa hartuta, ingelesezko corpus batetik eta euskarazko hiztegi batetik. Subjektu eta objektuaren arteko anbiguotasun funtzionala maiz aurkitzen dugu euskarazko testuetan; subjektua edo objektua bereiztea kasuen %22an ambiguoa da, eta hori gertatzen da perpausen %33an. Horrela, arazo horren garrantzi handia konparagarria da beste hizkuntza batzuek duten PP attachment arazoarenarekin. Gure sistemaren emaitzak hobeak dira artearen egoerako analizatzaile sintaktiko estatistiko batenak baino, estatistikoki esanguratsua den %20ko errore-murrizketa lortzen baitu. Analisi sintaktikoa egiteko edozein algoritmorekin erabil daiteke desanbiguazio-sistema hau. KEYWORDS: Syntactic ambiguity resolution, subcategorization, web as a corpus. KEYWORDS IN BASQUE: Anbiguotasun sintaktikoaren ebazpena, azpikategorizazioa, ama- rauna corpus gisa. Proceedings of COLING 2012: Technical Papers, pages 97–114, COLING 2012, Mumbai, December 2012. 97  
A machine learning mechanism is learned from human annotations in order to perform preference ranking. The mechanism operates on a sentence level and ranks the alternative machine translations of each source sentence. Rankings are decomposed into pairwise comparisons so that binary classiﬁers can be trained using black-box features of automatic linguistic analysis. In order to re-compose the pairwise decisions of the classiﬁer, this work introduces weighing the decisions with their classiﬁcation probabilities, which eliminates ranking ties and increases the coefﬁcient of the correlation with the human rankings up to 80%. The authors also demonstrate several conﬁgurations of successful automatic ranking models; the best conﬁguration achieves acceptable correlation with human judgments (tau=0.30), which is higher than that of state-of-the-art reference-aware automatic MT evaluation metrics such as METEOR and Levenshtein distance. KEYWORDS: Machine Translation, Quality Estimation, Ranking. Proceedings of COLING 2012: Technical Papers, pages 115–132, COLING 2012, Mumbai, December 2012. 115  
Supplementary data selection from out-of-domain or related-domain data is a well established technique in domain adaptation of statistical machine translation. The selection criteria for such data are mostly based on measures of similarity with available in-domain data, but not directly in terms of translation quality. In this paper, we present a technique for selecting supplementary data to improve translation performance, directly in terms of translation quality, measured by automatic evaluation metric scores. Batches of data selected from out-of-domain corpora are incrementally added to an existing baseline system and evaluated in terms of translation quality on a development set. A batch is selected only if its inclusion improves translation quality. To assist the process, we present a novel translation model merging technique that allows rapid retraining of the translation models with incremental data. When incorporated into the ‘in-domain’ translation models, the ﬁnal cumulatively selected datasets are found to provide statistically signiﬁcant improvements for a number of different supplementary datasets. Furthermore, the translation model merging technique is found to perform on a par with state-of-the-art methods of phrase-table combination. KEYWORDS: Statistical Machine Translation, Domain Adaptation, Supplementary Data Selec- tion, Model Merging, Incremental Update. Proceedings of COLING 2012: Technical Papers, pages 149–166, COLING 2012, Mumbai, December 2012. 149  
Detecting text reuse is a fundamental requirement for a variety of tasks and applications, ranging from journalistic text reuse to plagiarism detection. Text reuse is traditionally detected by computing similarity between a source text and a possibly reused text. However, existing text similarity measures exhibit a major limitation: They compute similarity only on features which can be derived from the content of the given texts, thereby inherently implying that any other text characteristics are negligible. In this paper, we overcome this traditional limitation and compute similarity along three characteristic dimensions inherent to texts: content, structure, and style. We explore and discuss possible combinations of measures along these dimensions, and our results demonstrate that the composition consistently outperforms previous approaches on three standard evaluation datasets, and that text reuse detection greatly beneﬁts from incorporating a diverse feature set that reﬂects a wide variety of text characteristics. TITLE AND ABSTRACT IN GERMAN Erkennung von Textwiederverwendung durch Komposition von Textähnlichkeitsmaßen Die Frage, ob und in welcher Weise Texte in abgewandelter Form wiederverwendet werden, ist ein zentraler Aspekt bei einer Reihe von Problemstellungen, etwa im Rahmen journalistischer Tätigkeit oder als Mittel zur Plagiatserkennung. Textwiederverwendung wird traditionell ermittelt durch Berechnen von Textähnlichkeit zwischen einem Ursprungstext und einem potentiell wiederverwendeten Text. Bestehende Textähnlichkeitsmaße haben jedoch die starke Einschränkung, dass sie Ähnlichkeit nur anhand von Eigenschaften berechnen, die vom Inhalt der gegebenen Texte abgeleitet werden können, und somit implizieren, dass jegliche andere Textcharacteristika vernächlässigbar sind. In dieser Arbeit berechnen wir Textähnlichkeit anhand von drei Dimensionen: Inhalt, Struktur und Stil. Wir untersuchen mögliche Kombinationen von Maßen entlang dieser Dimensionen, und zeigen deutlich anhand der Ergebnisse auf drei etablierten Evaluationsdatensätzen, dass die Komposition generell bessere Ergebnisse liefert als bestehende Ansätze, und dass die Bestimmung von Textwiederverwendung stark von einem breiten Spektrum an Textcharacteristika proﬁtiert. KEYWORDS: text similarity, text reuse, plagiarism, paraphrase. KEYWORDS IN GERMAN: Textähnlichkeit, Textwiederverwendung, Plagiat, Paraphrase. Proceedings of COLING 2012: Technical Papers, pages 167–184, COLING 2012, Mumbai, December 2012. 167  
from Comparable Documents  Kfir BAR Nachum DERSHOWITZ School of Computer Science, Tel Aviv University, Ramat Aviv, Israel kfirbar@post.tau.ac.il, nachumd@tau.ac.il ABSTRACT We describe an automatic paraphrase-inference procedure for a highly inflected language like Arabic. Paraphrases are derived from comparable documents, that is, distinct documents dealing with the same topic. A co-training approach is taken, with two classifiers, one designed to model the contexts surrounding occurrences of paraphrases, and the other trained to identify significant features of the words within paraphrases. In particular, we use morpho-syntactic features calculated for both classifiers, as is to be expected when working with highly inflected languages. We provide some experimental results for Arabic, and for the simpler English, which we find to be encouraging. Our immediate interest is to incorporate such paraphrases within an Arabic-toEnglish translation system. KEYWORDS : Paraphrases, highly inflected languages, morphologically rich languages, cotraining, comparable documents, Arabic Proceedings of COLING 2012: Technical Papers, pages 185–200, COLING 2012, Mumbai, December 2012. 185  
The Web is an ever increasing, dynamically changing, multilingual repository of text. There have been several approaches to harvest this repository for bootstrapping, supplementing and adapting data needed for training models in speech and language applications. In this paper, we present semi-supervised and unsupervised approaches to harvesting multilingual text that rely on a key observation of link collocation. We demonstrate the eﬀectiveness of our approach in the context of statistical machine translation by harvesting parallel texts and training translation models in 20 diﬀerent languages. Furthermore, by exploiting the DOM trees of parallel webpages, we extend our harvesting technique to create parallel data for resource limited languages in an unsupervised manner. We also present some interesting observations concerning the socio-economic factors that the multilingual Web reﬂects. Keywords: web crawling, parallel text, document model object tree, machine translation. 
Statistical post-editing (SPE) of the output produced by rule-based MT (RBMT) systems has been reported to produce extraordinary BLEU (and other automatic evaluation) score improvements. SPE has also been applied to the output of statistical MT (SMT) systems, albeit with more mixed results. We present a statistical post-editing pipeline and evaluate the outputs using automatic and human evaluation techniques, comparing the two SPE pipeline systems (RBMT + SPE and SMT + SPE) with the pure RBMT and SMT system, in an SPE scenario that uses independently existing bitext data, rather than manually corrected ﬁrst stage MT output, as its training data. Our results show that although automatic evaluation metrics favour the pure SMT system, human evaluators prefer the output provided by the statistically post-edited RBMT system. KEYWORDS: Rule-Based Machine Translation, Statistical Machine Translation, Statistical Post- Editing. Proceedings of COLING 2012: Technical Papers, pages 215–230, COLING 2012, Mumbai, December 2012. 215  
Though it is generally accepted that language models do not capture all aspects of real language, no adequate measures to quantify their shortcomings have been proposed until now. We will use n-gram models as workhorses to demonstrate that the differences between natural and generated language are indeed quantiﬁable. More speciﬁcally, for two algorithmic approaches, we demonstrate that each of them can be used to distinguish real text from generated text accurately and to quantify the difference. Therefore, we obtain a coherent indication how far a language model is from naturalness. Both methods are based on the analysis of co-occurrence networks: a speciﬁc graph cluster measure, the transitivity, and a speciﬁc kind of motif analysis, where the frequencies of selected motifs are compared. In our study, artiﬁcial texts are generated by n-gram models, for n = 2, 3, 4. We found that, the larger n is chosen, the narrower the distance between generated and natural text is. However, even for n = 4, the distance is still large enough to allow an accurate distinction. The motif approach even allows a deeper insight into those semantic properties of natural language that evidently cause these differences: polysemy and synonymy. To complete the picture, we show that another motif-based approach by Milo et al. (2004) does not allow such a distinction. Using our method, it becomes possible for the ﬁrst time to measure generative language models deﬁciencies with regard to semantics of natural language. KEYWORDS: quantitative linguistics, network analysis, motif analysis, co-occurrence networks, language models. Proceedings of COLING 2012: Technical Papers, pages 263–278, COLING 2012, Mumbai, December 2012. 263  
Many parsers learn sparse class distributions over trees to model natural language. Recursive Neural Networks (RNN) use much denser representations, yet can still achieve an F-score of 92.06% for right binarized sentences up to 15 words long. We examine an RNN model by comparing it with an abstract generative probabilistic model using a Deep Belief Network (DBN). The DBN provides both an upwards and downwards pointing conditional model, drawing a connection between RNN and Charniak type parsers, while analytically predicting average scoring parameters in the RNN. In addition, we apply the RNN to longer sentences and develop two methods which, while having negligible effect on short sentence parsing, are able to improve the parsing F-Score by 0.83% on longer sentences. KEYWORDS: Parsing Recurrent Neural Network Restricted Boltzmann Machine. Proceedings of COLING 2012: Technical Papers, pages 279–294, COLING 2012, Mumbai, December 2012. 279  
Subjectivity analysis has been actively used in various applications such as opinion mining of customer reviews in online review sites, question-answering in CQA sites, multi-document summarization, etc. However, there has been very little focus on subjectivity analysis in the domain of online forums. Online forums contain huge amounts of user-generated data in the form of discussions between forum members on speciﬁc topics and are a valuable source of information. In this work, we perform subjectivity analysis of online forum threads. We model the task as a binary classiﬁcation of threads in one of the two classes: subjective and nonsubjective. Unlike previous works on subjectivity analysis, we use several non-lexical threadspeciﬁc features for identifying subjectivity orientation of threads. We evaluate our methods by comparing them with several state-of-the-art subjectivity analysis techniques. Experimental results on two popular online forums demonstrate that our methods outperform strong baselines in most of the cases. KEYWORDS: Online Forums, subjecitivity, dialogue act. Proceedings of COLING 2012: Technical Papers, pages 295–310, COLING 2012, Mumbai, December 2012. 295  
This paper explores the use of Natural Language Generation (NLG) for facilitating the provision of feedback to citizen scientists in the context of a nature conservation programme, BEEWATCH. BEEWATCH aims to capture the distribution of bumblebees, an ecologically and economically important species group in decline, across the UK and beyond. The NLG module described here uses comparisons of visual features of bumblebee species as well as contextual information to improve the citizen scientists’ identiﬁcation skills and to keep them motivated. We report studies that show a positive effect of NLG feedback on accuracy of bumblebee identiﬁcation and on volunteer retention, along with a positive appraisal of the generated feedback. KEYWORDS: NLG, Natural Language Generation, Educational Application, Bumblebee Conser- vation, Citizen Science, Generating Feedback. Proceedings of COLING 2012: Technical Papers, pages 311–324, COLING 2012, Mumbai, December 2012. 311  
Studies of computational models of language acquisition depend to a large part on the input available for experiments. In this paper, we study the effect that input size has on the performance of word segmentation models embodying different kinds of linguistic assumptions. Because currently available corpora for word segmentation are not suited for addressing this question, we perform our study on a novel corpus based on the Providence Corpus (Demuth et al., 2006). We ﬁnd that input size can have dramatic effects on segmentation performance and that, somewhat surprisingly, models performing well on smaller amounts of data can show a marked decrease in performance when exposed to larger amounts of data. We also present the data-set on which we perform our experiments comprising longitudinal data for six children. This corpus makes it possible to ask more speciﬁc questions about computational models of word segmentation, in particular about intra-language variability and about how the performance of different models can change over time.1 KEYWORDS: word segmentation, language acquisition, resources, Bayesian modelling, unsu- pervised learning. 1The corpus and the code to run the experiments is available at http://web.science.mq.edu.au/ ~bborschi/. Proceedings of COLING 2012: Technical Papers, pages 325–340, COLING 2012, Mumbai, December 2012. 325  
In this work we address the challenge of augmenting n-gram language models according to prior linguistic intuitions. We argue that the family of hierarchical Pitman-Yor language models is an attractive vehicle through which to address the problem, and demonstrate the approach by proposing a model for German compounds. In our empirical evaluation the model outperforms a modiﬁed Kneser-Ney n-gram model in test set perplexity. When used as part of a translation system, the proposed language model matches the baseline BLEU score for English→German while improving the precision with which compounds are output. We ﬁnd that an approximate inference technique inspired by the Bayesian interpretation of Kneser-Ney smoothing (Teh, 2006) offers a way to drastically reduce model training time with negligible impact on translation quality. TITLE AND ABSTRACT IN AFRIKAANS Bayes-modellering van saamgestelde woorde in Duits Hierdie werk neem uitdagings rondom die uitbreiding van n-gramtaalmodelle volgens voorafgaande linguistieke intuïsie onder die loep. Ons voer aan dat die familie van hiërargiese Pitman-Yor taalmodelle ’n wenslike stuk gereedskap is om hierdie probleem mee aan te pak en formuleer ’n model van Duitse saamgestelde woorde om die benadering te demonstreer. Met behulp van ’n empiriese evaluering bevind ons dat die model in terme van toetsdataperpleksiteit beter vaar as die aangepaste Kneser-Ney n-grammodel. As onderdeel van ’n Engels→Duitsvertalingstelsel behaal die model in terme van die BLEU-metriek dieselfde vertaalafvoerkwaliteit as die kontrole stelsel en genereer saamgestelde woorde teen ’n hoër presisie. Verder stel ons vas dat ’n benaderde inferensietegniek, geïnspireer deur die Bayes-interpretasie van Kneser-Ney-gladstryking (Teh, 2006), gebruik kan word om die modelberamingtyd drasties te verminder sonder wesenlike impak op die vertaalafvoerkwaliteit. KEYWORDS: language model; Bayesian methods; machine translation; compounding; ngram model; approximate inference. KEYWORDS IN L2: taalmodel; Bayes-metodes; masjienvertaling; samestellings; ngrammodel; benaderde inferensie. Proceedings of COLING 2012: Technical Papers, pages 341–356, COLING 2012, Mumbai, December 2012. 341  
The emergence of dialogue on social medial neccessitates the development of new dialogue processing models. We argue that to address coherence and to infer the implicatures of social dialogue it is vital to understand the social aspirations of the dialogue participants. One key aspect of understanding social dialogue is to understand the intentions and goals of participants. In this paper, we present 11 social acts that capture a broad number of social intentions and goals. We deﬁne social acts as pragmatic speech acts designed to give insight into the socio-cognitive processes that individuals unconsciously go through when communicating in dialogue. Identiﬁcation of the social acts is done using a combination of a generative model in which utterances are generated from gappy patterns, which deﬁne a given social act, and a series of binary classiﬁers. Our experimentation shows that we can capture these social acts with an overall F-measure of 50.4%. Keywords: dialogue, social actions, online communication, speech acts, social goals, social implicature. Proceedings of COLING 2012: Technical Papers, pages 375–390, COLING 2012, Mumbai, December 2012. 375  
Previous approaches to the task of native language identiﬁcation (Koppel et al., 2005) have been limited to small, within-corpus evaluations. Because these are restrictive and unreliable, we apply cross-corpus evaluation to the task. We demonstrate the efﬁcacy of lexical features, which had previously been avoided due to the within-corpus topic confounds, and provide a detailed evaluation of various options, including a simple bias adaptation technique and a number of classiﬁer algorithms. Using a new web corpus as a training set, we reach high classiﬁcation accuracy for a 7-language task, performance which is robust across two independent test sets. Although we show that even higher accuracy is possible using crossvalidation, we present strong evidence calling into question the validity of cross-validation evaluation using the standard dataset. KEYWORDS: Native language identiﬁcation, text classiﬁcation, evaluation. Proceedings of COLING 2012: Technical Papers, pages 391–408, COLING 2012, Mumbai, December 2012. 391  
A problem that crops up repeatedly in shallow and deep syntactic parsing approaches to South Asian languages like Urdu/Hindi is the proper treatment of complex predications. Problems for the NLP of complex predications are posed by their productiveness and the ill understood nature of the range of their combinatorial possibilities. This paper presents an investigation into whether ﬁne-grained information about the distributional properties of nouns in N+V CPs can be identiﬁed by the comparatively simple process of extracting bigrams from a large “raw” corpus of Urdu. In gathering the relevant properties, we were aided by visual analytics in that we coupled our computational data analysis with interactive visual components in the analysis of the large data sets. The visualization component proved to be an essential part of our data analysis, particular for the easy visual identiﬁcation of outliers and false positives. Another essential component turned out to be our language-particular knowledge and access to existing language-particular resources. Overall, we were indeed able to identify high frequency N-V complex predications as well as pick out combinations we had not been aware of before. However, a manual inspection of our results also pointed to a problem of data sparsity, despite the use of a large corpus. KEYWORDS: Urdu, complex predicates, visualization, bigrams, corpus. Proceedings of COLING 2012: Technical Papers, pages 409–424, COLING 2012, Mumbai, December 2012. 409  
Native Language Identiﬁcation tackles the problem of determining the native language of an author based on a text the author has written in a second language. In this paper, we discuss the systematic use of recurring n-grams of any length as features for training a native language classiﬁer. Starting with surface n-grams, we investigate two degrees of abstraction incorporating parts-of-speech. The approach outperforms previous work employing a comparable data setup, reaching 89.71% accuracy for a task with seven native languages using data from the International Corpus of Learner English (ICLE). We then investigate the claim by Brooke and Hirst (2011) that a content bias in ICLE seems to result in an easy classiﬁcation by topic instead of by native language characteristics. We show that training our model on ICLE and testing it on three other, independently compiled learner corpora dealing with other topics still results in high accuracy classiﬁcation. TITLE AND ABSTRACT IN GERMAN Muttersprachenerkennung mittels rekurrenter N-Gramme – Untersuchungen zur Abstraktion und Domänenabhängigkeit Die Muttersprachenerkennung befasst sich mir der Erkennung der Muttersprache eines Autors auf der Basis eines Textes, der von diesem Autor in einer Zweitsprache verfasst worden ist. In der vorliegenden Arbeit diskutieren wir die systematische Verwendung rekurrenter N-Gramme aller Längen als Features für das Trainieren eines Muttersprachen-Klassiﬁzierers. Beginnend mit oberﬂächenbasierten N-Grammen, untersuchen wir zwei Stufen der Abstraktion unter Verwendung von Wortarten. Unser Ansatz liefert eine Klassiﬁkationsgenauigkeit von 89.71% für Texte aus dem International Corpus of Learner English (ICLE) mit sieben unterschiedlichen Muttersprachen und übertrifft somit die bisherigen Ergebnisse auf vergleichbaren Daten. Ferner untersuchen wir die Behauptung von Brooke und Hirst (2011), dass inhaltliche Aspekte des ICLE zu einer einfacheren Klassiﬁkation der Texte nach dem Thema anstatt nach der Muttersprache führen könnten. Wir zeigen, dass ein auf ICLE Daten trainiertes Modell auch bei Tests auf drei unabhängig erstellten Lernerkorpora eine hohe Klassiﬁkationsgenauigkeit ermöglicht. KEYWORDS: Native Language Identiﬁcation, Author Proﬁling, Text Classiﬁcation, Second Language Acquisition, Learner Corpora. KEYWORDS IN GERMAN: Muttersprachenerkennung, Autoren-Proﬁling, Textklassifkation, Zweitspracherwerb, Lernerkorpora. Proceedings of COLING 2012: Technical Papers, pages 425–440, COLING 2012, Mumbai, December 2012. 425  
Disambiguation to Wikipedia (D2W) is the task of linking mentions of concepts in text to their corresponding Wikipedia entries. Most previous work has focused on linking terms in formal texts (e.g. newswire) to Wikipedia. Linking terms in short informal texts (e.g. tweets) is difﬁcult for systems and humans alike as they lack a rich disambiguation context. We ﬁrst evaluate an existing Twitter dataset as well as the D2W task in general. We then test the effects of two tweet context expansion methods, based on tweet authorship and topic-based clustering, on a state-of-the-art D2W system and evaluate the results. TITLE AND ABSTRACT IN BASQUE Testuinguruaren Hedapenaren Analisia eta Hobekuntza Mikroblogak Wikiﬁkatzeko Esanahia Wikipediarekiko Argitzea (D2W) deritzo testuetan aurkitutako kontzeptuen aipamenak Wikipedian dagozkien sarrerei lotzeari. Aurreko lan gehienek testu formalak (newswire, esate baterako) lotu dituzte Wikipediarekin. Testu informalak (tweet-ak, esate baterako) lotzea, ordea, zaila da bai sistementzat eta baita gizakiontzat ere, argipena erraztuko luketen testuingururik ez dutelako. Lehenik eta behin, Twitter-en gainean sortutako datu-sorta bat, eta D2W ataza bera ebaluatzen ditugu. Ondoren, egungo D2W sistema baten gainean testuingurua hedatzeko bi teknika aztertu eta ebaluatzen ditugu. Bi teknika hauek tweet-aren egilean eta gaikako multzokatze metodo batean oinarritzen dira. KEYWORDS: Disambiguation to Wikipedia (D2W), Twitter, disambiguation context. KEYWORDS IN BASQUE: Wikipediarekiko Argitzea (D2W), Twitter, argipen testuingurua. Proceedings of COLING 2012: Technical Papers, pages 441–456, COLING 2012, Mumbai, December 2012. 441  
This paper applies sentence compression models for the task of query-focused multi-document summarization in order to investigate if sentence compression improves the overall summarization performance. Both compression and summarization are considered as global optimization problems and solved using integer linear programming (ILP). Three different models are built depending on the order in which compression and summarization are performed: 1) ComFirst (where compression is performed ﬁrst), 2) SumFirst (where important sentence extraction is performed ﬁrst), and 3) Combined (where compression and extraction are performed jointly via optimizing a combined objective function). Sentence compression models include lexical, syntactic and semantic constraints while summarization models include relevance, redundancy and length constraints. A comprehensive set of query-related and importance-oriented measures are used to deﬁne the relevance constraint whereas four alternative redundancy constraints are employed based on different sentence similarity measures using a) cosine similarity, b) syntactic similarity, c) semantic similarity, and d) extended string subsequence kernel (ESSK). Empirical evaluation on the DUC benchmark datasets demonstrates that the overall summary quality can be improved signiﬁcantly using global optimization with semantically motivated models. KEYWORDS: Sentence compression, query-focused multi-document summarization, integer linear programming (ILP). Proceedings of COLING 2012: Technical Papers, pages 457–474, COLING 2012, Mumbai, December 2012. 457  
We address the challenge of automatically generating questions from topics. We consider that each topic is associated with a body of texts containing useful information about the topic. Questions are generated by exploiting the named entity information and the predicate argument structures of the sentences present in the body of texts. To measure the importance of the generated questions, we use Latent Dirichlet Allocation (LDA) to identify the sub-topics (which are closely related to the original topic) in the given body of texts and apply the Extended String Subsequence Kernel (ESSK) to calculate their similarity with the questions. We also propose the use of syntactic tree kernels for computing the syntactic correctness of the questions. The questions are ranked by considering their importance (in the context of the given body of texts) and syntactic correctness. To the best of our knowledge, no other study has accomplished this task in our setting before. Experiments show that our approach can signiﬁcantly outperform the state-of-the-art results. KEYWORDS: Question generation, named entity information, predicate argument structures, latent dirichlet allocation (LDA), extended string subsequence kernel (ESSK), syntactic tree kernel. Proceedings of COLING 2012: Technical Papers, pages 475–492, COLING 2012, Mumbai, December 2012. 475  
This paper describes two methods for checking the acceptability of adjective deletion in noun phrases. The ﬁrst method uses the Google n-gram corpus to check the ﬂuency of the remaining context after an adjective is removed. The second method trains an SVM model using n-gram counts and other measures to classify deletable and undeletable adjectives in context. Both methods are evaluated against human judgements of sentence naturalness. The application motivating our interest in adjective deletion is data hiding, in particular linguistic steganography. We demonstrate the proposed adjective deletion technique can be integrated into an existing stegosystem, and in addition we propose a novel secret sharing scheme based on adjective deletion. KEYWORDS: linguistic steganography, adjective deletion. Proceedings of COLING 2012: Technical Papers, pages 493–510, COLING 2012, Mumbai, December 2012. 493  
Linguistic steganography is a form of covert communication using natural language to conceal the existence of the hidden message, which is usually achieved by systematically making changes to a cover text. This paper proposes a linguistic steganography method using word ordering as the linguistic transformation. We show that the word ordering technique can be used in conjunction with existing translation-based embedding algorithms. Since unnatural word orderings would arouse the suspicion of third parties and diminish the security of the hidden message, we develop a method using a maximum entropy classiﬁer to determine the naturalness of sentence permutations. The classiﬁer is evaluated by human judgements and compared with a baseline method using the Google n-gram corpus. The results show that our proposed system can achieve a satisfactory security level and embedding capacity for the linguistic steganography application. KEYWORDS: linguistic steganography, word ordering, surface realisation. Proceedings of COLING 2012: Technical Papers, pages 511–528, COLING 2012, Mumbai, December 2012. 511  
Compared to the amount of research that has been done on English event extraction, there exists relatively little work on Chinese event extraction. We seek to push the frontiers of supervised Chinese event extraction research by proposing two extension to Li et al.'s (2012) state-of-the-art event extraction system. First, we employ a joint modeling approach to event extraction, aiming to address the error propagation problem inherent in Li et al.'s pipeline system architecture. Second, we investigate a variety of rich knowledge sources for Chinese event extraction that encode knowledge ranging from the character level to the discourse level. Experimental results on the ACE 2005 dataset show that our joint-modeling, knowledge-rich approach significantly outperforms Li et al.'s approach.  Title and Abstract in Chinese 运用丰富语言学特征的中文事件抽取联合模型  文的事件抽取 学 的事件抽取 用 中文 抽取 的  中文的事件抽取  联合模型  Li et al.  文  运用丰富语言学特征的联合模型  Li et al.(2012) 的 中文事件抽取的 中的 的特征 ACE2005 Li et al. 的  Keywords: event extraction, Chinese language processing. Keywords in Chinese: 抽取, 中文 语言 .  Proceedings of COLING 2012: Technical Papers, pages 529–544, COLING 2012, Mumbai, December 2012. 529  
Microblogging services have brought users to a new era of knowledge dissemination and information seeking. However, the large volume and multi-aspect of messages hinder the ability of users to conveniently locate the speciﬁc messages that they are interested in. While many researchers wish to employ traditional text classiﬁcation approaches to effectively understand messages on microblogging services, the limited length of the messages prevents these approaches from being employed to their full potential. To tackle this problem, we propose a novel semi-supervised learning scheme to seamlessly integrate the external web resources to compensate for the limited message length. Our approach ﬁrst trains a classiﬁer based on the available labeled data as well as some auxiliary cues mined from the web, and probabilistically predicts the categories for all unlabeled data. It then trains a new classiﬁer using the labels for all messages and the auxiliary cues, and iterates the process to convergence. Our approach not only greatly reduces the time-consuming and labor-intensive labeling process, but also deeply exploits the hidden information from unlabeled data and related text resources. We conducted extensive experiments on two real-world microblogging datasets. The results demonstrate the effectiveness of the proposed approaches which produce promising performance as compared to state-of-the-art methods.  Title and abstract in Chinese  Ù ã º Ãº  Ã Ä  ¢Ã³Ã℄­ǑǑêýèè¨Æ¡Â´´ÂÂºªûàÄèß¨ ¨ÜÍÆè Ãª¨éãÁàß³ ³þ²ºÁÝåÄÂûóÃÃ©àǑ­ßª℄¡Í­©¨ªªÍààÐÙßßé³TwÛÁittÖerÂÍæÔÁ℄Ä¢­  ¸ ð  Keywords: Semi-supervised algorithm, microblog classiﬁcation, probabilistic graph model.  Keywords in L2:  ß ßÃ Î º.  Proceedings of COLING 2012: Technical Papers, pages 561–576, COLING 2012, Mumbai, December 2012. 561  
Classifying documents according to the sentiment they convey (whether positive or negative) is an important problem in computational linguistics. There has not been much work done in this area on general techniques that can be applied eﬀectively to multiple languages, nor have very large data sets been used in empirical studies of sentiment classiﬁers. We present an empirical study of the eﬀectiveness of several sentiment classiﬁcation algorithms when applied to nine languages (including Germanic, Romance, and East Asian languages). The algorithms are implemented as part of a system that can be applied to multilingual data. We trained and tested the system on a data set that is substantially larger than that typically encountered in the literature. We also consider a generalization of the n-gram model and a variant that reduces memory consumption, and evaluate their eﬀectiveness. Keywords: sentiment, classiﬁcation, multilingual, empirical veriﬁcation. Proceedings of COLING 2012: Technical Papers, pages 577–592, COLING 2012, Mumbai, December 2012. 577  
Many evaluation issues for grammatical error detection have previously been overlooked, making it hard to draw meaningful comparisons between different approaches, even when they are evaluated on the same corpus. To begin with, the three-way contingency between a writer’s sentence, the annotator’s correction, and the system’s output makes evaluation more complex than in some other NLP tasks, which we address by presenting an intuitive evaluation scheme. Of particular importance to error detection is the skew of the data – the low frequency of errors as compared to non-errors – which distorts some traditional measures of performance and limits their usefulness, leading us to recommend the reporting of raw measurements (true positives, false negatives, false positives, true negatives). Other issues that are particularly vexing for error detection focus on deﬁning these raw measurements: specifying the size or scope of an error, properly treating errors as graded rather than discrete phenomena, and counting non-errors. We discuss recommendations for best practices with regard to reporting the results of system evaluation for these cases, recommendations which depend upon making clear one’s assumptions and applications for error detection. By highlighting the problems with current error detection evaluation, the ﬁeld will be better able to move forward. KEYWORDS: grammatical error detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611  
In the biomedical ﬁeld, the key to access information is the use of specialized terms. However, in most of Indo-European languages, these terms are complex morphological structures. The aim of the presented work is to identify the various meaningful components of these terms and use this analysis to improve biomedical Information Retrieval. We present an approach combining an automatic alignment using a pivot language, and an analogical learning that allows an accurate morphological analysis of terms. These morphological analysis are used to improve the indexing of medical documents. The experiments reported in this paper show the validity of this approach with a 10% improvement in MAP over a standard IR system. Keywords: Morphology, biomedical terminology, alignment, analogical learning, morpho- semantic indexing, biomedical information retrieval. Proceedings of COLING 2012: Technical Papers, pages 629–646, COLING 2012, Mumbai, December 2012. 629  
Named entity recognition (NER) has been extensively studied for the names of genes and gene products but there are few proposed solutions for phenotypes. Phenotype terms are expected to play a key role in inferring gene function in complex heritable diseases but are intrinsically difﬁcult to analyse due to their complex semantics and scale. In contrast to previous approaches we evaluate state-of-the-art techniques involving the fusion of machine learning on a rich feature set with evidence from extant domain knowledge-sources. The techniques are validated on two gold standard collections including a novel annotated collection of 112 abstracts derived from a systematic search of the Online Mendelian Inheritance of Man database for auto-immune diseases. Encouragingly the hybrid model outperforms a HMM, a CRF and a pure knowledge-based method to achieve an F1 of 77.07. Disagreement analysis points to further improvements on this emerging NE task. The annotated corpus and guidelines are available on request. KEYWORDS: conditional random ﬁelds, biomedicine, machine learning, genetic disorders, text mining. Proceedings of COLING 2012: Technical Papers, pages 647–662, COLING 2012, Mumbai, December 2012. 647  
Information structure, i.e the way speakers construct sentences to present new information in the context of old, can capture rich linguistic information about the discourse structure of scientiﬁc documents. Information structure has been found useful for important Natural Language Processing (NLP) tasks, such as information retrieval and extraction. Since scientiﬁc articles typically follow a certain discourse structure describing the prior work, problem being solved, methods used, and so forth, it could also be useful for summarization of these articles. In this work we focus on a scheme of information structure called Argumentative Zoning (AZ), and investigate whether its categories could support extractive text summarization in a scientiﬁc domain. We develop a summarization system that uses AZ categories (i) as features and (ii) in the ﬁnal sentence selection process. We evaluate the system directly as well as using task-based evaluation. The results show that AZ can support both full document and customized summarization. We report a statistically signiﬁcant improvement in summarization performance against a competitive baseline that uses journal section labels instead of AZ information. TITLE AND ABSTRACT IN MANDARIN 一种根据“论证结构”自动摘录科技文献的方法 信息结构是指作者组织语句陈述信息的方式。信息结构例如科技文献的篇章结构包含丰富 的语言信息，有助于解决自然语言处理领域的一些重要问题例如信息检索和信息提取等。 科技文献通常使用特定的篇章结构来陈述以往的研究，阐述研究问题以及研究方法等等， 这些篇章结构可以被用于文献的自动摘录。本文着眼于一类特定的信息结构 —“论证结构 ”，研究其是否有助于更好地摘录科技文献。在本文开发的摘录系统中，“论证结构”有 两种用途：一是作为特征供机器学习，二是用于最终的语句筛选过程。本文对该系统进行 了直接和间接的评测，测试结果显示“论证结构”有助于更好地对全文或指定信息进行摘 录。基于“论证结构”的摘录系统显著性优于基于章节标题的摘录系统。 KEYWORDS: discourse, information structure, argumentative zones, summarization, document summarization, information access. KEYWORDS IN MANDARIN: 篇章，信息结构，论证结构，文本摘要，信息获取. Proceedings of COLING 2012: Technical Papers, pages 663–678, COLING 2012, Mumbai, December 2012. 663  
Text-to-scene conversion requires knowledge about how actions and locations are expressed in language and realized in the world. To provide this knowlege, we are creating a lexical resource (VigNet) that extends FrameNet by creating a set of intermediate frames (vignettes) that bridge between the high-level semantics of FrameNet frames and a new set of low-level primitive graphical frames. Vignettes can be thought of as a link between function and form – between what a scene means and what it looks like. In this paper, we describe the set of primitive graphical frames and the functional properties of 3D objects (affordances) we use in this decomposition. We examine the methods and tools we have developed to populate VigNet with a large number of action and location vignettes. KEYWORDS: text-to-scene conversion, world knowledge, frame semantics, visual semantics, linguistic annotation, crowdsourcing. Proceedings of COLING 2012: Technical Papers, pages 679–694, COLING 2012, Mumbai, December 2012. 679  Figure 1: Mocked-up scenes using the WASH-FRUIT-IN-SINK vignette (“John washes the apple”) and WASH-FLOOR-W-SPONGE vignette (“John washes the ﬂoor”). 
In this paper, we propose a rule-based method to improve efﬁciency in bottom-up chart generation with GG, an open-source reversible large-scale HPSG for German. Following an indepth analysis of efﬁciency problems in the baseline system, we show that costly combinatorial explosion in brute force bottom-up search can be largely avoided using information already contained implicitly in the input semantics: either (i) information is globally present, but needs to be made locally available to a particular elementary predication, or (ii) semantic conﬁgurations in the input have a clear translation to syntactic constraints, provided some knowledge of the grammar. We propose several performance features targeting inﬂection and extraction, as well as more language-speciﬁc features, relating to verb movement and discontinuous complex predicates. In a series of experiments on three different test suites we show that 7 out of 8 features are consistently effective in reducing generation times, both in isolation and in combination. Combining all efﬁciency measures, we observe a speedup factor of 4.5 for our less complex test suites, increasing to almost 28 for the more complex one: the fact that performance beneﬁts drastically increase with input length suggests that our method scales up well in the sense that it effectively heads off the problem with exponential growth. The present approach of using a generator-internal transfer grammar has the added advantage that it locates performance-related issues close to the grammar, thereby keeping the external semantic interface as general as possible. TITLE AND ABSTRACT IN GERMAN Efﬁziente HPSG-Generierung für das Deutsche Wir stellen eine regelbasierte Methode vor, zur automatischen Anreicherung der semantischen Eingabe einer reversiblen HPSG des Deutschen, die es erlaubt, teure uninformierte Suche bei der Bottom-Up-Chart-Generierung weitgehend zu vermeiden, indem (i) globale Information, die implizit in der Eingabe vorhanden ist, explizit und lokal verfügbar gemacht wird, und (ii) syntaktische Constraints aus semantischen Konﬁgurationen abgeleitet werden. Wir schlagen Performanzfeatures für verschiedene Phänomene vor, wie Flexion, Extraktion, Verbbewegung und diskontuierliche komplexe Prädikate. Unsere Experimente zeigen erhebliche Efﬁzienzsteigerungen (Faktor 4.5–Faktor 27.8), deren Zunahme mit steigender Eingabekomplexität korreliert, was die gute Skalierbarkeit unserer Methode belegt. Der generator-interne Transferansatz zeichnet sich weiterhin dadurch aus, daß Performanzaspekte grammatik-nah behandelt werden, wodurch die externe Semantikschnittstelle so allgemein wie möglich bleibt. KEYWORDS: Surface generation, HPSG, German. Proceedings of COLING 2012: Technical Papers, pages 695–710, COLING 2012, Mumbai, December 2012. 695  
In this paper, we present a study of the collaborative writing process in Wikipedia. Our work is based on a corpus of 1,995 edits obtained from 891 article revisions in the English Wikipedia. We propose a 21-category classiﬁcation scheme for edits based on Faigley and Witte’s (1981) model. Example edit categories include spelling error corrections and vandalism. In a manual multi-label annotation study with 3 annotators, we obtain an inter-annotator agreement of α = 0.67. We further analyze the distribution of edit categories for distinct stages in the revision history of 10 featured and 10 non-featured articles. Our results show that the information content in featured articles tends to become more stable after their promotion. On the opposite, this is not true for non-featured articles. We make the resulting corpus and the annotation guidelines freely available.1 TITLE AND ABSTRACT IN GERMAN Eine Korpusbasierte Studie von Änderungstypen in Exzellenten und Nicht-Exzellenten Wikipedia-Artikeln In dieser Arbeit stellen wir eine Studie über den kollaborativen Schreibprozess in Wikipedia vor. Unsere Studie basiert auf einem Korpus aus 1.995 Änderungen in 891 Artikelrevisionen der englischen Wikipedia. Wir schlagen ein Klassiﬁkationsschema mit 21 Änderungenstypen vor, basierend auf dem Modell von Faigley and Witte (1981). Unter den Änderungenstypen beﬁnden sich beispielsweise Rechtschreibkorrekturen und Vandalismus. In einer manuellen multi-label Annotationsstudie mit 3 Annotatoren erzielen wir eine Interrater-Reliabilität von α = 0.67. Wir analysieren außerdem die Verteilung von Änderungstypen zu unterschiedlichen Stadien in der Revisionsgeschichte von 10 exzellenten und 10 nicht-exzellenten Artikeln. Unsere Ergebnisse zeigen, dass der Informationsgehalt in exzellenten Artikeln nach ihrer Auszeichnung tendenziell stabiler wird. Im Gegensatz dazu ist das bei nicht-exzellenten Artikeln nicht der Fall. Das dabei entstandene Korpus und die Annotationsrichtlinien stellen wir zur freien Verfügung. KEYWORDS: Wikipedia, Revision History, Collaborative Writing, Quality Assessment. KEYWORDS IN GERMAN: Wikipedia, Revisionsgeschichte, Kollaboratives Schreiben, Quali- tätsbewertung. 1http://www.ukp.tu-darmstadt.de/data/wiki-edits/ Proceedings of COLING 2012: Technical Papers, pages 711–726, COLING 2012, Mumbai, December 2012. 711  
KEYWORDS: COMPUTER-AIDED TRANSLATION, MACHINE TRANSLATION, COMPARABLE CORPORA, LEARNING-TORANK, COMPOSITIONALITY, TERMINOLOGY MOTS-CLÉS : TRADUCTION ASSISTÉE PAR ORDINATEUR, TRADUCTION AUTOMATIQUE, CORPUS COMPARABLES, LEARNING-TO-RANK, COMPOSITIONNALITÉ, TERMINOLOGIE Proceedings of COLING 2012: Technical Papers, pages 745–762, COLING 2012, Mumbai, December 2012. 745  Introduction Comparable corpora are composed of texts in different languages which are not translations but deal with the same subject matter and were produced in similar situations of communication. They are used in Computer-Aided Translation to provide technical translators with domainspecific bilingual lexicons when there is no parallel data available (e.g. translation memories, multilingual terminologies). This situation happens when translators have to translate texts which deal with emerging technical domains or when the translation is done from/to an under-resourced language. Comparable corpora also have the advantage of containing more idiomatic expressions than parallel corpora do because the target texts do not bear the influence of the source language. Indeed, Baker (1996) observed that translated texts tend to bear features like explicitation, simplification, normalization and levelling out. As a consequence, one of the difficulties with comparable corpora is that the translation of a source term may not be present in its “normalized” or “canonical” form but rather in the form of a morphological or paraphrastic variant (e.g. postmenopausal translates to après la ménopause 'after the menopause' instead of postménopausique). Another limitation is that algorithms output, for each source term, a set of candidate translations instead of just one target term. This state of affairs makes it very challenging for translators to use lexicons extracted from comparable corpora in real-life situations (Delpech, 2011). The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung & Cheung, 2004; Rauf & Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo & Gaussier (2010) show that they improve the quality of the extracted lexicon if they improve the comparability of the corpus by selecting a smaller – but more comparable – corpus from an initial set of documents. This paper proposes methods for ranking and extracting canonical translations as well as translation variants, with a special focus on the extraction of fertile translations. In parallel texts processing, the notion of fertility has been defined by Brown et al. (1993). They defined the fertility of a source word e as the number of target words to which e is connected in a randomly selected alignment. Similarly, we call a fertile translation a translation pair in which the target term has more words than the source term. The identification of fertile translations is useful because (i) they frequentlty correspond to non-canonical translations, e.g. paraphrastic variants and (ii) they tend to correspond to vulgarized forms of technical terms (e.g. « cytotoxic » vs. « toxic to the cells ») which are useful when the translator translates lay science texts. Up to now, fertility has received little attention in the field of comparable corpora processing. To our knowledge, only Daille & Morin (2005) and Weller et al. (2011) tried to extract translation pairs of different lengths from comparable corpora. Daille & Morin (2005) focus on the specific case of multi-word terms whose meaning is not compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach. Weller 746  et al. (2011) concentrate on translating noun compounds as noun phrases. Similar to the approach presented here, Claveau & Kijak (2011) use translation equivalences between morphemes to generate translations and can handle fertility. However it is not suited for comparable corpora since it requires domain-specific parallel data (in their case, a multilingual terminology) to learn alignment probabilities. Our method is based on compositional translation. We chose this approach because: (i) according to Namer & Baud (2007), compositional terms form a major part of the new terms found in technical and scientific domains, this is not restricted to the field of biomedicine as it is generally believed ; (ii) compositionality-based methods have been shown to clearly outperform contextbased ones for the translation of terms with compositional meaning, both in terms of translation accuracy and rank of the correct candidate translation (Morin & Daille, 2010) ; (iii) we believe that compositionality-based methods offer the opportunity to generate fertile translations if combined with a morphology-based approach. This method, which we call morphocompositional translation, consists in: (i) decomposing the source term into morphemes: postmenopause is split into post- + menopause1 ; (ii) translating the morphemes to bound morphemes or fully autonomous words: post- becomes post- or après, menopause becomes ménopause ; (iii) recomposing the translated elements into a target term: post-ménopause 'postmenopause', après la ménopause 'after the menopause'. Fertile translations can be generated because we allow bound morphemes to be translated to autonomous lexical items (e.g. prefix post- → preposition après). The proposed ranking methods exploit various corpus-based and translation-based features. This paper falls into 4 sections. Section 1 outlines recent research in compositional approaches to bilingual lexicon extraction. Section 2 explains the methods we designed for translation generation and ranking. Section 3 describes our experimental data. Section 4 presents and discusses the results of our experimentations. 
In this paper, we propose a time-line based framework for topic summarization in Twitter. We summarize topics by sub-topics along time line to fully capture rapid topic evolution in Twitter. Speciﬁcally, we rank and select salient and diversiﬁed tweets as a summary of each sub-topic. We have observed that ranking tweets is signiﬁcantly different from ranking sentences in traditional extractive document summarization. We model and formulate the tweet ranking in a uniﬁed mutual reinforcement graph, where the social inﬂuence of users and the content quality of tweets are taken into consideration simultaneously in a mutually reinforcing manner. Extensive experiments are conducted on 3.9 million tweets. The results show that the proposed approach outperforms previous approaches by 14% improvement on average ROUGE-1. Moreover, we show how the content quality of tweets and the social inﬂuence of users effectively improve the performance of measuring the salience of tweets. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (CHINESE)  úŽ(7> qÍ›Œ‡,(Ï„¨yÝ˜X•  ,‡Ðú† *úŽöôt„¨yÝ˜ê¨X•F¶ Ý˜ göôz• PÝ˜-9n‡,„Í•¦Œ 7'ù¨y‡,ÛL’•Œ½Öå X• ’ž: þ!‹ ö Q‡,…¹ \ > qÍ›Œ‡,(ÏÛL’• 1 ,‡Ðú„!‹(úÆ!‹ ROUGE-1sGÐØ†14% 2 \ > ,(Ï H09Û†‡,Í•¦„¦Ï  PÝ˜ ì•(ø žŒÓœh qÍ›Œ‡  KEYWORDS: Topic summarization, Twitter, Timeline summarization, Content quality, Social inﬂuence.  KEYWORDS IN CHINESE: Ý˜X• ¨y öôt „X• ‡,(Ï (7> qÍ ›.  ∗ This work was done when the ﬁrst author was an intern at Microsoft Research Asia. Proceedings of COLING 2012: Technical Papers, pages 763–780, COLING 2012, Mumbai, December 2012. 763  
We present a simple and straightforward alignment algorithm for monotone many-to-many alignments in grapheme-to-phoneme conversion and related ﬁelds such as morphology, and discuss a few noteworthy extensions. Moreover, we specify combinatorial formulas for monotone many-to-many alignments and decoding in G2P which indicate that exhaustive enumeration is generally possible, so that some limitations of our approach can easily be overcome. Finally, we present a decoding scheme, within the monotone many-to-many alignment paradigm, that relates the decoding problem to restricted integer compositions and that is, putatively, superior to alternatives suggested in the literature. Title and Abstract in German S-beschränkte monotone Alignierungen: Algorithmus, Suchraum und Anwendungen Wir präsentieren einen einfachen Alignment-Algorithmus für monotone ‘many-tomany’ Alignierungen im Bereich Graphem-zu-Phonem-Konversion und verwandten Gebieten wie z.B. Morphologie, und besprechen sinnvolle Erweiterungen. Darüber hinaus geben wir kombinatorische Formeln im Bereich der monotonen ‘many-to-many’ Alignierungen und im Bereich des Decoding in G2P an, die suggerieren, dass vollständige Enumeration hier im Allgemeinen möglich ist, sodass ein paar Einschränkungen unseres Ansatzes leicht behoben werden können. Schließlich präsentieren wir ein Decoding-Schema, innerhalb des Paradigmas von ‘many-to-many’ Alignierungen, dass das Decoding-Problem mit beschränkten Zahlenkompositionen in Beziehung setzt und das in der Literatur vorgeschlagenen Alternativen vermeintlich überlegen ist. Keywords: many-to-many alignments, monotone alignments, string transduction, restricted integer compositions, grapheme-to-phoneme conversion. Keywords in German: many-to-many Alignierungen, monotone Alignierungen, String- Überführungen, beschränkte Zahlenkompositionen, Graphem-zu-Phonem-Konversion. Proceedings of COLING 2012: Technical Papers, pages 781–798, COLING 2012, Mumbai, December 2012. 781  
While there have been many studies on measuring the size of learners’ vocabulary or the vocabulary they should learn, there have been few studies on what kind of words learners actually know. Therefore, we investigated theoretically and practically important models for predicting second language learners’ vocabulary and propose another model for this vocabulary prediction task. With the current models, the same word difﬁculty measure is shared by all learners. This is unrealistic because some learners have special interests. A learner interested in music may know special music-related terms regardless of their difﬁculty. To solve this problem, our model can deﬁne a learner-speciﬁc word difﬁculty measure. Our model is also an extension of these current models in the sense that these models are special cases of our model. In a qualitative evaluation, we deﬁned a measure for how learner-speciﬁc a word is. Interestingly, the word with the highest learner-speciﬁcity was “twitter”. Although “twitter” is a difﬁcult English word, some low-ability learners presumably knew this word through the famous micro-blogging service. Our qualitative evaluation successfully extracted such interesting and suggestive examples. Our model achieved an accuracy competitive with the current models. KEYWORDS: Learner-speciﬁcity, vocabulary prediction, Rasch model. Proceedings of COLING 2012: Technical Papers, pages 799–814, COLING 2012, Mumbai, December 2012. 799  
We present a novel approach for jointly disambiguating and clustering known and unknown concepts and entities with Markov Logic. Concept and entity disambiguation is the task of identifying the correct concept or entity in a knowledge base for a single- or multi-word noun (mention) given its context. Concept and entity clustering is the task of clustering mentions so that all mentions in one cluster refer to the same concept or entity. The proposed model (1) is global, i.e. a group of mentions in a text is disambiguated in one single step combining various global and local features, and (2) performs disambiguation, unknown concept and entity detection and clustering jointly. The disambiguation is performed with respect to Wikipedia. The model is trained once on Wikipedia articles and then applied to and evaluated on different data sets originating from news papers, audio transcripts and internet sources. KEYWORDS: Word Sense Disambiguation. Proceedings of COLING 2012: Technical Papers, pages 815–832, COLING 2012, Mumbai, December 2012. 815  
We present a new method for directly working with typed uniﬁcation grammars in which type uniﬁcation is not well-deﬁned. This is often the case, as large-scale HPSG grammars now usually have type systems for which many pairs do not have least upper bounds. Our method yields a uniﬁcation algorithm that compiles quickly and yet is nearly as fast during parsing as one that requires least upper bounds. The method also provides a natural naming convention for uniﬁcation results in cases where no user-deﬁned type exists. Keywords: HPSG, typed feature structures, uniﬁcation-based grammar. Proceedings of COLING 2012: Technical Papers, pages 833–848, COLING 2012, Mumbai, December 2012. 833  
We investigate the stacking of dependency and phrase structure parsers, i.e. we deﬁne features from the output of a phrase structure parser for a dependency parser and vice versa. Our features are based on the original form of the external parses and we also compare this approach to converting phrase structures to dependencies then applying standard stacking on the converted output. The proposed method provides high accuracy gains for both phrase structure and dependency parsing. With the features derived from the phrase structures, we achieved a gain of 0.89 percentage points over a state-of-the-art parser and reach 93.95 UAS, which is the highest reported accuracy score on dependency parsing of the Penn Treebank. The phrase structure parser obtains 91.72 F-score with the features derived from the dependency trees, and this is also competitive with the best reported PARSEVAL scores for the Penn Treebank. KEYWORDS: parsing, stacking, dependency parsing, phrase structure parsing. Proceedings of COLING 2012: Technical Papers, pages 849–866, COLING 2012, Mumbai, December 2012. 849  
In this paper, we propose a novel semantic cohesion model. Our model utilizes the predicateargument structures as soft constraints and plays the role as a reordering model in the phrasebased statistical machine translation system. We build a translation system with GALE data. Experimental results on the NIST02, NIST03, NIST04, NIST05 and NIST08 Chinese-English tasks show that our model improves the baseline system by 0.93 BLEU 0.98 TER on average. We also compare our method with a syntax-augmented model (Cherry, 2008), and demonstrate the importance of predicate-argument semantics in machine translation. KEYWORDS: statistical machine translation, semantic role labeling. Proceedings of COLING 2012: Technical Papers, pages 867–878, COLING 2012, Mumbai, December 2012. 867  
There is a demand for taxonomies to organise large collections of documents into categories for browsing and exploration. This paper examines four existing taxonomies that have been manually created, along with two methods for deriving taxonomies automatically from data items. We use these taxonomies to organise items from a large online cultural heritage collection. We then present two human evaluations of the taxonomies. The ﬁrst measures the cohesion of the taxonomies to determine how well they group together similar items under the same concept node. The second analyses the concept relations in the taxonomies. The results show that the manual taxonomies have high quality well deﬁned relations. However the novel automatic method is found to generate very high cohesion. TITLE AND ABSTRACT IN BASQUE Dokumentu bildumak antolatzeko taxonomien arteko alderaketa Dokumentu bildumak kategorietan sailkatzea oso erabilgarria da, dokumentuak arakatzeko eta aztertzeko aukera berriak eskaintzen duen heinean. Hori horrela izanik, dokumentu bilduma handiak sailkatzeko taxonomien behar handia dago. Artikulu honetan eskuz sortutako lau taxonomia aztertzen dira, taxonomiak automatikoki sortzen dituzten bi metodorekin batera. Taxonomia hauek ondare kulturaleko bilduma handiak antolatzeko erabili ditugu. Taxonomien ebaluazioa egin dugu galdetegietan oinarritutako bi metodo erabiliaz. Lehenbizikoak taxonomiaren kohesioa neurtzen du, hau da, antzeko itemak kontzeptu beraren azpian zein ondo taldekatzen diren. Bigarrenak taxonomiako kontzeptuen arteko erlazioak aztertzen ditu. Emaitzek erakusten dute eskuzko taxonomien erlazioen kalitatea, baina metodo automatiko berri batek lortzen du kohesio handiena. KEYWORDS: Semantic network, taxonomy, hierarchy, Wikipedia, WordNet, ontology. KEYWORDS IN BASQUE: Sare semantiko, taxonomia, hierarkia, Wikipedia, WordNet, Ontolo- gia. Proceedings of COLING 2012: Technical Papers, pages 879–894, COLING 2012, Mumbai, December 2012. 879  
Manual corpus annotation is getting widely used in Natural Language Processing (NLP). While being recognized as a difﬁcult task, no in-depth analysis of its complexity has been performed yet. We provide in this article a grid of analysis of the different complexity dimensions of an annotation task, which helps estimating beforehand the difﬁculties and cost of annotation campaigns. We observe the applicability of this grid on existing annotation campaigns and detail its application on a real-world example. KEYWORDS: manual corpus annotation, annotation campaign management, annotation cam- paign cost estimate. Proceedings of COLING 2012: Technical Papers, pages 895–910, COLING 2012, Mumbai, December 2012. 895  
We present a new method to generate extractive multi-document summaries. The method uses Integer Linear Programming to jointly maximize the importance of the sentences it includes in the summary and their diversity, without exceeding a maximum allowed summary length. To obtain an importance score for each sentence, it uses a Support Vector Regression model trained on human-authored summaries, whereas the diversity of the selected sentences is measured as the number of distinct word bigrams in the resulting summary. Experimental results on widely used benchmarks show that our method achieves state of the art results, when compared to competitive extractive summarizers, while being computationally efﬁcient as well. KEYWORDS: Text Summarization; Integer Linear Programming; Support Vector Regression. Proceedings of COLING 2012: Technical Papers, pages 911–926, COLING 2012, Mumbai, December 2012. 911  
Cross-lingual relevance modelling (CLRLM) is a state-of-the-art technique for cross-lingual information retrieval (CLIR) which integrates query term disambiguation and expansion in a uniﬁed framework, to directly estimate a model of relevant documents in the target language starting with a query in the source language. However, CLRLM involves integrating a translation model either on the document side if a parallel corpus is available, or on the query side if a bilingual dictionary is available. For low resourced language pairs, large parallel corpora do not exist and the vocabulary coverage of dictionaries is small, as a result of which RLM-based CLIR fails to obtain satisfactory results. Despite the lack of parallel resources for a majority of language pairs, the availability of comparable corpora for many languages has grown considerably in the recent years. Existing CLIR techniques such as cross-lingual relevance models, cannot effectively utilize these comparable corpora, since they do not use information from documents in the source language. We overcome this limitation by using information from retrieved documents in the source language to improve the retrieval quality of the target language documents. More precisely speaking, our model involves a two step approach of ﬁrst retrieving documents both in the source language and the target language (using query translation), and then improving on the retrieval quality of target language documents by expanding the query with translations of words extracted from the top ranked documents retrieved in the source language which are thematically related (i.e. share the same concept) to the words in the top ranked target language documents. Our key hypothesis is that the query in the source language and its equivalent target language translation retrieve documents which share topics. The ovelapping topics of these top ranked documents in both languages are then used to improve the ranking of the target language documents. Since the model relies on the alignment of topics between language pairs, we call it the cross-lingual topical relevance model (CLTRLM). Experimental results show that the CLTRLM signiﬁcantly outperforms the standard CLRLM by upto 37% on English-Bengali CLIR, achieving mean average precision (MAP) of up to 60.27% of the Bengali monolingual IR MAP. Keywords: Cross-lingual Information Retrieval, Relevance Model, Topic Model, Pseudo- Relevance Feedback, Latent Dirichlet Allocation. Proceedings of COLING 2012: Technical Papers, pages 927–942, COLING 2012, Mumbai, December 2012. 927  
In this work we present a novel approach to bootstrap domain speciﬁc terminology, namely Structured Term Recognition, and we apply it to the medical domain. In contrast to previous approaches, based on observing distributional properties of terminology with respect to their contexts, our method analyzes the “internal structure” of multi-word terms by learning patterns of word clusters. Evaluation shows that our method can be used to boost the performances of term recognition systems based on dictionary lookup while extending the coverage of large ontologies like UMLS. KEYWORDS: terminology recognition, bootstrapping, medical terminology, named entity recognition, joint inference. Proceedings of COLING 2012: Technical Papers, pages 943–958, COLING 2012, Mumbai, December 2012. 943  
The standard training regime for transition-based dependency parsers makes use of an oracle, which predicts an optimal transition sequence for a sentence and its gold tree. We present an improved oracle for the arc-eager transition system, which provides a set of optimal transitions for every valid parser conﬁguration, including conﬁgurations from which the gold tree is not reachable. In such cases, the oracle provides transitions that will lead to the best reachable tree from the given conﬁguration. The oracle is efﬁcient to implement and provably correct. We use the oracle to train a deterministic left-to-right dependency parser that is less sensitive to error propagation, using an online training procedure that also explores parser conﬁgurations resulting from non-optimal sequences of transitions. This new parser outperforms greedy parsers trained using conventional oracles on a range of data sets, with an average improvement of over 1.2 LAS points and up to almost 3 LAS points on some data sets. KEYWORDS: dependency parsing, transition system, oracle. Proceedings of COLING 2012: Technical Papers, pages 959–976, COLING 2012, Mumbai, December 2012. 959  
Many of the state-of-the-art methods for constructing a polarity lexicon rely on the propagation of polarity on the lexical network. In one of those methods, where the Ising spin model is employed as a probabilistic model, it is reported that the system exhibits the phase transition in the vicinity of the optimal temperature parameter. We provide an analysis of this phenomenon from the viewpoint of statistical mechanics and clarify the underlying mechanism. On the basis of this analysis, we propose a scheme for improving the extraction performance, i.e., by removing the largest eigenvalue component from the weight matrix. Experimental results show that the scheme signiﬁcantly improves the accuracy of the extraction of the semantic orientations at negligible additional computational cost, outperforming the state-of-the-art algorithms. We also explore the origin of the high classiﬁcation performance by analyzing eigenvalues of the weight matrix and a linearized model. KEYWORDS: sentiment analysis, polarity lexicon, spin model, label propagation. ∗ Takuma Goto now works for ACCESS CO., LTD. Proceedings of COLING 2012: Technical Papers, pages 977–994, COLING 2012, Mumbai, December 2012. 977  
Online user comments contain valuable user opinions. Comments vary greatly in quality and detecting high quality comments is a subtask of opinion mining and summarization research. Finding attentive comments that provide some reasoning is highly valuable in understanding the user’s opinion particularly in sociopolitical opinion mining and aids policy makers, social organizations or government sectors in decision making. In this paper we study the problem of detecting thoughtful comments. We empirically study various textual features, discourse relations and relevance features to predict thoughtful comments. We use logistic regression model and test on the datasets related to sociopolitical content. We found that the most useful features include the discourse relations and relevance features along with basic textual features to predict the comment quality in terms of thoughtfulness. In our experiments on two different datasets, we could achieve a prediction score of 79.37% and 73.47% in terms of F-measure on the two data sets, respectively. KEYWORDS: Opinion mining, Information Extraction, Text Classiﬁcation. Proceedings of COLING 2012: Technical Papers, pages 995–1010, COLING 2012, Mumbai, December 2012. 995  
Sanskrit, the classical language of India, presents speciﬁc challenges for computational linguistics: exact phonetic transcription in writing that obscures word boundaries, rich morphology and an enormous corpus, among others. Recent international cooperation has developed innovative solutions to these problems and signiﬁcant resources for linguistic research. Solutions include efﬁcient segmenting and tagging algorithms and dependency parsers based on constraint programming. The integration of lexical resources, text archives and linguistic software is achieved by distributed interoperable Web services. Resources include a morphological tagger and tagged corpus. Keywords: Indian language technology, Resources and annotation, Morphology and POS tagging, Parsing. Proceedings of COLING 2012: Technical Papers, pages 1011–1028, COLING 2012, Mumbai, December 2012. 1011  
We present a framework for the analysis of Machine Translation performance. We use multivariate linear models to determine the impact of a wide range of features on translation performance. Our assumption is that variables that most contribute to predict translation performance are the key to understand the differences between good and bad translations. During training, we learn the regression parameters that better predict translation quality using a wide range of input features based on the translation model and the ﬁrst-best translation hypotheses. We use a linear regression with regularization. Our results indicate that with regularized linear regression, we can achieve higher levels of correlation between our predicted values and the actual values of the quality metrics. Our analysis shows that the performance for in-domain data is largely dependent on the characteristics of the translation model. On the other hand, out-of domain data can beneﬁt from better reordering strategies. TITLE AND ABSTRACT IN ANOTHER LANGUAGE Modelos Lineales para el Análisis del Desempeño de la Traducción Automática En este documento presentamos una metodología para el análisis del desempeño de los sistemas de traducción automática. Utilizamos modelos lineales multivariados para determinar el impacto que diversas variables tienen en la calidad de las traducciones. En este estudio se asume que las variables que más contribuyen a predecir la calidad de las traducciones, son determinantes para entender las diferencias entre buenas y malas traducciones. Nuestros resultados demuestran que usando regresión lineal penalizada, se pueden obtener altos índices de predicción de calidad de traducción. Un análisis detallado revela que el desempeño de los sistemas de traducción frente a datos in-domain dependen en gran medida de las características de nuestros modelos de traducción. En contraste, la traducción de documentos out-of-domain está fuertemente ligada a las estrategias de reodenamiento que se utilicen. KEYWORDS: Statistical machine translation, translation quality prediction, system performance analysis. KEYWORDS IN L2: Traducción automática estadística, calidad de traducción, Análisis de desempeño. Proceedings of COLING 2012: Technical Papers, pages 1029–1044, COLING 2012, Mumbai, December 2012. 1029  
Geolocation prediction is vital to geospatial applications like localised search and local event detection. Predominately, social media geolocation models are based on full text data, including common words with no geospatial dimension (e.g. today) and noisy strings (tmrw), potentially hampering prediction and leading to slower/more memory-intensive models. In this paper, we focus on ﬁnding location indicative words (LIWs) via feature selection, and establishing whether the reduced feature set boosts geolocation accuracy. Our results show that an information gain ratiobased approach surpasses other methods at LIW selection, outperforming state-of-the-art geolocation prediction methods by 10.6% in accuracy and reducing the mean and median of prediction error distance by 45km and 209km, respectively, on a public dataset. We further formulate notions of prediction conﬁdence, and demonstrate that performance is even higher in cases where our model is more conﬁdent, striking a trade-off between accuracy and coverage. Finally, the identiﬁed LIWs reveal regional language differences, which could be potentially useful for lexicographers. Keywords: Social Media, Geolocation, Feature Selection. Proceedings of COLING 2012: Technical Papers, pages 1045–1062, COLING 2012, Mumbai, December 2012. 1045  
We propose a new measure of semantic similarity between words in context, which exploits the syntactic/semantic structure of the context surrounding each target word. For a given pair of target words and their sentential contexts, labeled directed graphs are made from the output of a semantic parser on these sentences. Nodes in these graphs represent words in the sentences, and labeled edges represent syntactic/semantic relations between them. The similarity between the target words is then computed as the sum of the similarity of walks starting from the target words (nodes) in the two graphs. The proposed measure is tested on word sense disambiguation and paraphrase ranking tasks, and the results are promising: The proposed measure outperforms existing methods which completely ignore or do not fully exploit syntactic/semantic structural co-occurrences between a target word and its neighbors. KEYWORDS: contextual word similarity, word sense disambiguation, paraphrase, kernel method. Proceedings of COLING 2012: Technical Papers, pages 1081–1096, COLING 2012, Mumbai, December 2012. 1081  
Sentence compression is important in a wide range of applications in natural language processing. Previous approaches of Japanese sentence compression can be divided into two groups. Word-based methods extract a subset of words from a sentence to shorten it, while bunsetsubased methods extract a subset of bunsetsu (where a bunsetsu is a text unit that consists of content words and following function words). Basically, bunsetsu-based methods perform better than word-based methods. However, bunsetsu-based methods have the disadvantage that they cannot drop unimportant words from each bunsetsu because they have to follow constraints under which each bunsetsu is treated as a unit. In this paper, we propose a novel compression method to overcome this disadvantage. Our method relaxes the constraints using Lagrangian relaxation and shortens each bunsetsu if it contains unimportant words. Experimental results show that our method effectively compresses a sentence while preserving its important information and grammaticality. TITLE AND ABSTRACT IN JAPANESE Ϣχοτ੍໿ͷ؇࿨ʹΑΔॊೈͳ೔ຊ‫ޠ‬จѹॖ จѹॖ͸ɼࣗવ‫ॲޠݴ‬ཧͷ༷ʑͳΞϓϦέʔγϣϯʹ͓͍ͯॏཁͰ͋Δɽ೔ຊ‫ޠ‬จʹର͢Δ ‫ط‬ଘͷѹॖख๏͸ೋछྨʹ෼͚ΒΕΔɽ୯‫ޠ‬ϕʔεͷख๏͸จ͔Β୯‫߹ूޠ‬Λબग़͠ɼѹॖ จͱ͢ΔɽҰํɼจઅϕʔεͷख๏͸จ͔Βจઅू߹Λબग़͠ɼѹॖจͱ͢Δɽ‫ج‬ຊతʹ͸ ‫ऀޙ‬ͷํ͕ྑ͘‫ػ‬ೳ͢Δɽ͔͠͠ɼจઅϕʔεͷख๏͸ɼจઅΛϢχοτͱͯ͠ѻ͏ͱ͍͏ ੍໿͕͋ΔͨΊɼ‫ݸ‬ʑͷจઅΛѹॖͰ͖ͳ͍ɽຊߘͰ͸ɼ͜ͷܽ఺Λࠀ෰͢Δ৽͍͠ѹॖख ๏ΛఏҊ͢ΔɽఏҊख๏͸ϥάϥϯδϡ؇࿨Λ༻্͍ͯͷ੍໿Λ؇࿨͠ɼ֤จઅΛѹॖ͢Δɽ ࣮‫ݧ‬ͷ݁ՌɼఏҊख๏ʹΑͬͯ‫ݪ‬จͷ৘ใΛଟ͘อ࣋͢Δจ๏తͳѹॖจΛੜ੒Ͱ͖Δ͜ͱ ͕෼͔ͬͨɽ KEYWORDS: sentence compression, Lagrangian relaxation. KEYWORDS IN JAPANESE: จѹॖ, ϥάϥϯδϡ؇࿨. Proceedings of COLING 2012: Technical Papers, pages 1097–1112, COLING 2012, Mumbai, December 2012. 1097  
Testing a theory against real world data can sometimes be helpful in ﬁguring out the shortcomings of your current theory. In this paper, we test a theory about the syntax-semantics interface of German nach-particle verbs against data from a web corpus in order to see if we can use our automatic NLP machinery to corroborate the predictions of the theory. We use state-of-the-art parsers to automatically annotate our data with the features predicted by the theory and then apply a standard clustering approach to approximate the nach-particle verb classes of the theory. The results of our experiment not only help us highlighting the more problematic parts of the theory but also teach us about the strengths and weaknesses of our automatic analysis tools. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, L2 (OPTIONAL, AND ON SAME PAGE) Corpusbasierte Überprüfung einer semantischen Klassiﬁkation deutscher nach-Partikelverben Um Unzulänglichkeiten einer Theorie auszumachen ist es mitunter vonnöten, Hypothesen gegen echtes Textmaterial abzugleichen. In diesem Beitrag soll diskutiert werden, wie Vorhersagen einer Theorie zum syntaktischen und semantischen Verhalten deutscher nach-Partikelverben gegen Netztexte abgeglichen werden können und wie dabei eine automatische Textverarbeitung unterstützend zum Tragen kommt. Es werden Parser des letzten Stands der Forschung verwendet um die Daten mit den von der Theorie vorhergesagten Merkmalen zu annotieren bevor ein standardisiertes Clustering-Verfahren angewandt wird um die theoretischen nach-Partikel-Verb-Klassen nachzubilden. Die Resultate des Experiments unterstreichen nicht nur Problemfälle der Theorie sondern zeigen auch die Stärken und Schwächen der automatischen Analyse. KEYWORDS: particle verbs, syntax-semantics interface, German, web data, parser combination. KEYWORDS IN L2: Partikelverben, Syntax-Semantik-Schnittstelle, Deutsch, Netztexte, Parser- Kombination. Proceedings of COLING 2012: Technical Papers, pages 1113–1128, COLING 2012, Mumbai, December 2012. 1113  
Snippet generation plays an important role in a search engine. Good snippets provide users a good indication on the main content of a search result related to the query and on whether one can ﬁnd relevant information in it. Previous studies on snippet generation focused on selecting sentences that are related to the query and to the document. However, resulting snippet may look highly relevant while the document itself is not. A missing factor that has not been considered is the consistency between the perceived relevance by the user in reading the snippet and the intrinsic relevance of the document. This factor is important to avoid generating a seemingly relevant snippet for an irrelevant document and vice versa. In this paper, we incorporate this factor in a snippet generation method that imposes the constraint that the snippet of a more relevant document should also be more relevant to the query. We derive a set of pairwise preferences between sentences from relevance judgments. We then use this set to train a gradient boosting decision tree to model a sentence scoring function used in snippet generation. Compared to the existing snippet generation methods and to the snippets generated by a commercial search engine, our snippets are more consistent with the true relevance of the documents. When the snippets are incorporated into a document ranking function, we also observe a signiﬁcant improvement in retrieval effectiveness. This study shows the importance to generate snippets indicating the right level of relevance to the search results. KEYWORDS: Search Engine Snippet, Query-biased Summarization, Document Retrieval. Proceedings of COLING 2012: Technical Papers, pages 1129–1146, COLING 2012, Mumbai, December 2012. 1129  
Sequence labeling models like conditional random ﬁelds have been successfully applied in a variety of NLP tasks. However, as the size of label set and dataset grows, the learning speed of batch algorithms like L-BFGS quickly becomes computationally unacceptable. Several online learning methods have been proposed in large scale setting, yet little effort has been made to compare the performance of these algorithms. Comparison is often carried out on a few datasets with ﬁne tuned parameters for speciﬁc algorithm. In this paper, we investigate and compare several online learning algorithms for sequence labeling with datasets varying in scale, feature design and label set. We ﬁnd that Dual Coordinate Ascent (DCA) is robust across datasets even without careful tuning of parameter. Furthermore, a recently proposed variant of Stochastic Gradient Descent (SGD), Adaptive online gradient Descent based on feature Frequency information (ADF), has very fast training speed compared with plain SGD, but fails to converge under certain conditions. Finally, We propose a simple modiﬁcation of ADF, which bears comparable convergence speed with ADF, and is consistently better than plain SGD. TITLE AND ABSTRACT IN CHINESE 序列标注在线学习算法的比较和改进 序列标注模型，如条件随机场，已广泛用于自然语言处理的很多任务中。但随着数 据规模和标记集的增大，批量学习算法（如L-BFGS）的训练时间复杂性变得越来越不可接 受；于是，出现了多个大规模环境下的在线学习算法。这些算法有各自的特点，但对这些 算法的比较研究很少有报道。通常情况下都是针对几个数据集，通过对特定算法细致调 整参数来比较最后的测试结果。本文针对几个典型的序列标注在线学习算法，在不同规 模、特征设计和标记集合的数据集上进行了比较研究。结果发现，即使没有特别对参数 调优，对偶坐标上升算法（DCA）在不同数据集上也能有很好的表现；而随机梯度下降 算法（SGD）的一个变种算法——基于特征频度的适应性在线梯度下降法（ADF）与普通 的SGD相比，训练速度更快，但不能保证总收敛。最后，本文还对ADF提出了一种简单的 改进，改进后的算法好于普通的SGD，与ADF 收敛速度接近。 KEYWORDS: Sequence labeling, online learning, stochastic gradient descent, named entity recognition. KEYWORDS IN CHINESE: 序列标注模型，在线学习，随机梯度下降，命名实体识别 ∗Corresponding author Proceedings of COLING 2012: Technical Papers, pages 1147–1162, COLING 2012, Mumbai, December 2012. 1147  
Automatic methods to create entity dictionaries or gazetteers have used only a small number of entity types (18 at maximum), which could pose a limitation for ﬁne-grained information extraction. This paper aims to create a dictionary of 200 extended named entity (ENE) types. Using Wikipedia as a basic resource, we classify Wikipedia titles into ENE types to create an ENE dictionary. In our method, we derive a large number of features for Wikipedia titles and train a multiclass classiﬁer by supervised learning. We devise an extensive list of features for the accurate classiﬁcation into the ENE types, such as those related to the surface string of a title, the content of the article, and the meta data provided with Wikipedia. By experiments, we successfully show that it is possible to classify Wikipedia titles into ENE types with 79.63% accuracy. We applied our classiﬁer to all Wikipedia titles and, by discarding low-conﬁdence classiﬁcation results, created an ENE dictionary of over one million entities covering 182 ENE types with an estimated accuracy of 89.48%. This is the ﬁrst large scale ENE dictionary. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (JAPANESE) Wikipedia Λ༻͍֦ͨு‫ݻ‬༗ද‫ॻࣙݱ‬ͷߏங ैདྷͷ‫ݻ‬༗ද‫Ͱॻࣙݱ‬͸ɼগͳ͍਺ʢ࠷େͰ 18ʣͷ‫ݻ‬༗ද‫ݱ‬λΠϓ͕༻͍ΒΕ͖ͯͨͨΊɼ ϐϯϙΠϯτͳ৘ใநग़ʹద༻͢Δ͜ͱ͕೉͍͠ͱ͍͏໰୊͕͋ͬͨɽͦ͜ͰɼຊߘͰ͸ɼ 200 ͷ֦ு‫ݻ‬༗ද‫ݱ‬λΠϓΛ༻͍ͨ‫ݻ‬༗ද‫ॻࣙݱ‬ͷߏஙΛ໨ࢦ͢ɽ۩ମతʹ͸ɼ‫͋ࢣڭ‬Γֶ शʹΑΔଟΫϥε෼ྨ‫ث‬Λ༻͍ɼWikipedia ͷ‫ݟ‬ग़͠‫ޠ‬Λ֦ு‫ݻ‬༗ද‫ݱ‬λΠϓʹ෼ྨ͢Δ͜ ͱͰࣙॻΛߏங͢Δɽಛ௃ྔͱͯ͠ɼ‫ݟ‬ग़͠‫ͦޠ‬ͷ΋ͷɼຊจɼͦͯ͠ɼΧςΰϦ౳ͷϝλ σʔλʹؔ͢Δ΋ͷΛ਺ଟ͘ྻ‫͍ͨ༻͠ڍ‬ɽ݁Ռͱͯ͠ɼ‫ݟ‬ग़͠‫ޠ‬Λɼ79.63%ͷਫ਼౓Ͱɼ֦ ு‫ݻ‬༗ද‫ݱ‬λΠϓʹ෼ྨͰ͖Δ͜ͱ͕෼͔ͬͨɽֶश͞ΕͨଟΫϥε෼ྨ‫ث‬ΛɼWikipedia ͷ ͢΂ͯͷ‫ݟ‬ग़͠‫ʹޠ‬ద༻͠ɼ·ͨɼ৴པ౓ͷ௿͍෼ྨ݁Ռʹ͍ͭͯ͸আ֎͢ΔΑ͏ʹͨ͠ͱ ͜Ζɼਪఆ෼ྨਫ਼౓͕ 89.48%Ͱɼ·ͨɼ182 ͷ֦ு‫ݻ‬༗ද‫ݱ‬λΠϓΛΧόʔ͢ΔɼඦສҎ ্ͷΤϯτϦΛ֦࣋ͭு‫ݻ‬༗ද‫ॻࣙݱ‬Λߏங͢Δ͜ͱ͕Ͱ͖ͨɽ͜ͷࣙॻ͸ɼॳͷେ‫ن‬໛ͳ ֦ு‫ݻ‬༗ද‫͋Ͱॻࣙݱ‬Δɽ KEYWORDS: Extended Named Entity, Dictionary, Wikipedia. KEYWORDS IN JAPANESE: ֦ு‫ݻ‬༗ද‫ݱ‬ɼࣙॻɼWikipedia. Proceedings of COLING 2012: Technical Papers, pages 1163–1178, COLING 2012, Mumbai, December 2012. 1163  
This paper develops a new statistical method of building language models (LMs) of Japanese dialects for automatic speech recognition (ASR). One possible application is to recognize a variety of utterances in our daily lives. The most crucial problem in training language models for dialects is the shortage of linguistic corpora in dialects. Our solution is to transform linguistic corpora into dialects at a level of pronunciations of words. We develop phonemesequence transducers based on weighted ﬁnite-state transducers (WFSTs). Each word in common language (CL) corpora is automatically labelled as dialect word pronunciations. For example, anta (Kansai dialect) is labelled anata (the most common representation of ‘you’ in Japanese). Phoneme-sequence transducers are trained from parallel corpora of a dialect and CL. We evaluate the word recognition accuracy of our ASR system. Our method outperforms the ASR system with LMs trained from untransformed corpora in written language by 9.9 points. KEYWORDS: spoken language, dialect, language model, weighted ﬁnite-state transducer (WFST). Proceedings of COLING 2012: Technical Papers, pages 1179–1194, COLING 2012, Mumbai, December 2012. 1179  
Scientiﬁc publications contain many references to method terminologies used during scientiﬁc experiments. New terms are constantly created within the research community, especially in the biomedical domain where thousands of papers are published each week. In this study we report our attempt to automatically extract such method terminologies from scientiﬁc research papers, using rule-based and machine learning techniques. We ﬁrst used some linguistic features to extract ﬁne-grained method sentences from a large biomedical corpus and then applied well established methodologies to extract the method terminologies. We focus the present study on the extraction of method phrases that contain an explicit mention of method keywords such as (algorithm, technique, analysis, approach and method) and other less explicit method terms such as Multiplex Ligation dependent Probe Ampliﬁcation. Our initial results show an average F-score of 91.89 for the rule-based system and 78.26 for the Conditional Random Field-based machine learning system. KEYWORDS: terminology extraction, rule-based, machine learning, corpus, linguistic features. Proceedings of COLING 2012: Technical Papers, pages 1211–1222, COLING 2012, Mumbai, December 2012. 1211  
Proceedings of COLING 2012: Technical Papers, pages 1223–1238, COLING 2012, Mumbai, December 2012. 1223  
Ranking tweets is a fundamental task to make it easier to distill the vast amounts of information shared by users. In this paper, we explore the novel idea of ranking tweets on a topic using heterogeneous networks. We construct heterogeneous networks by harnessing cross-genre linkages between tweets and semantically-related web documents from formal genres, and inferring implicit links between tweets and users. To rank tweets effectively by capturing the semantics and importance of different linkages, we introduce Tri-HITS, a model to iteratively propagate ranking scores across heterogeneous networks. We show that integrating both formal genre and inferred social networks with tweet networks produces a higher-quality ranking than the tweet networks alone. 1 Title and Abstract in Chinese 基于异构网络的微信息排序 微信息排序是一个可以过滤由用户分享的大量信息的根本任务。 在这篇文章中， 我们探索利用异构网络来排序微信息。我们利用来源于正规类型并且与微信息语义相关的 网络文本的跨类型联接, 和推理微信息和用户之间的潜在联系来构造异构网络。为了有效 地捕获不同联接的语义和重要性，我们提出了Tri-HITS, 一个能够跨网络循环传播排序分数 的模型。我们证明了结合来自正规类型的信息，和推理隐含的社交网络可以取得比仅靠微 信息网络本身更高的排序质量。2 Keywords: Tweet Ranking, Heterogeneous Networks, Iterative Propagation Model. Keywords in Chinese: 微信息排序，异构网络，循环传播模型. 1Related resources and software are freely available for research purposes at http://nlp.cs.qc.cuny.edu/tweetranking.zip; the system demo is at http://nlp.cs.qc.cuny.edu/tweet_summary/ground-truth-demo.xhtml. 2有关的资源和程序公布在如下地址和研究相关的应用分享: http://nlp.cs.qc.cuny.edu/tweetranking.zip; 系统演示 在如下地址： http://nlp.cs.qc.cuny.edu/tweet_summary/ground-truth-demo.xhtml. Proceedings of COLING 2012: Technical Papers, pages 1239–1256, COLING 2012, Mumbai, December 2012. 1239  
Combinatory Categorial Grammar (CCG) is an expressive grammar formalism which is able to capture long-range dependencies. However, building large and wide-coverage treebanks for CCG is expensive and time-consuming. In this paper, we focus on the problem of unsupervised CCG induction from plain texts. Based on the baseline model in (Bisk and Hockenmaier, 2012), we propose following two improvements: (1) we utilize boundary part-of-speech (POS) tags to capture lexical information; (2) we perform nonparametric Bayesian inference based on the Pitman-Yor process to learn compact grammars. Experiments on English Penn treebank demonstrate the effectiveness of our boundary model and Bayesian learning.  TITLE AND ABSTRACT IN ANOTHER LANGUAGE (CHINESE) 基于边界词和贝叶斯模型改进的组合范畴语法推导  组 合 范 畴 语 法(CCG)是 一 种 具 有 丰 富 表 达 能 力 的 语 法 形 式 ， 它 可 以 捕 获 长 距 离 的 依 赖关系。但是，构建大规模、覆盖面广的组合范畴语法语料库既昂贵又耗时。在本 文中，我们研究如何从普通文本中无监督地推导出组合范畴语法。基于现有的工 作(Bisk and Hockenmaier, 2012)，我们提出以下两个改进：(1) 我们使用边界词的词性标 记把词汇化信息引入模型中；(2) 我们使用基于Pitman-Yor过程的非参数贝叶斯模型来学习 简洁的文法。在英语宾州树库上的实验结果显示了我们提出的边界词模型和贝叶斯模型的 有效性。 KEYWORDS: Grammar Induction, Combinatory Categorial Grammar, Boundary Words, Bayesian Model. KEYWORDS IN CHINESE: 语法推导, 组合范畴语法, 边界词, 贝叶斯模型.  Proceedings of COLING 2012: Technical Papers, pages 1257–1274, COLING 2012, Mumbai, December 2012. 1257  
Recently, methods for mining graph sequences have attracted considerable interest in datamining research. A graph sequence is a data structure used to represent changing networks. The aim of graph sequence mining is to enumerate common changing patterns appearing more frequently than a given threshold in graph sequences. Dependency analysis is recognized as a basic process in natural language processing. In transition-based parsers for dependency analysis, a transition sequence can be represented by a graph sequence, where each graph, vertex, and edge corresponds to a state, word, and dependency, respectively. In this paper, we propose a method for mining rules to rewrite states reaching incorrect ﬁnal states to those reaching correct ﬁnal states, from transition sequences of a dependency parser using a beam search. The proposed method is evaluated using an English corpus, and we demonstrate the design of effective feature templates based on knowledge obtained from the mined rules. KEYWORDS: Dependency Parsing, Graph Sequence, Frequent Pattern Mining. Proceedings of COLING 2012: Technical Papers, pages 1275–1290, COLING 2012, Mumbai, December 2012. 1275  
Traditional general readability methods tend to underperform in domain-speciﬁc document retrieval because they fail to effectively differentiate the reading difﬁculty of the individual domain-speciﬁc terms and the semantic associations between the textual units in a document. On the other hand, recently proposed domain-speciﬁc readability methods have relied upon an external knowledge base which may be unavailable in some domains. We develop a novel unsupervised framework for computing domain-speciﬁc document readability. Our model does not require an ontology or a knowledge base to capture domain-speciﬁc terms in a document. The sequential ﬂow of terms in a document is modeled as a connected sequence of n-gram fragments in the latent concept space. We investigate an automatic sequential n-gram determination scheme that aids in capturing appropriate n-gram fragments which are semantically associated with the document’s theme and cohesive with the context. The domain-speciﬁc readability cost of a document is computed based on n-gram cohesion and n-gram speciﬁcity guided by the latent concepts. The cost can be employed to re-rank the search results generated from an information retrieval (IR) engine. The experimental results demonstrate that our framework achieves signiﬁcant improvement in ranking documents against the state-of-the-art unsupervised comparative methods. KEYWORDS: Term Sequence, LSI, IR, Domain-speciﬁc Readability, Ranking, Dynamic program- ming. Proceedings of COLING 2012: Technical Papers, pages 1309–1326, COLING 2012, Mumbai, December 2012. 1309  
Coreference resolution is the problem of clustering mentions into entities and is very critical for natural language understanding. This paper studies the problem of coreference resolution in the context of the newly emerging domain of Electronic Health Records (EHRs). The commonly used “best-link” model for coreference resolution considers only the scores from a pairwise classiﬁer in selecting the best antecedent. In this paper, we extend this model to include several constraints derived from surface-form of the mentions and the context in which they appear. Another major contribution of this paper is to show the use of domain-speciﬁc knowledge sources, mention parsing and clinical descriptors in deriving features which contribute to improved coreference resolution performance. We present experiments on 4 different clinical datasets illustrating that our approach outperforms a strong baseline and a state-of-the-art system by a wide margin. Keywords: Natural Language Processing, Information Extraction, Coreference Resolution, Elec- tronic Health Records, Knowledge Based Systems. Proceedings of COLING 2012: Technical Papers, pages 1327–1342, COLING 2012, Mumbai, December 2012. 1327  
Citations are a valuable resource for characterizing scientiﬁc publications that has already been used in applications such as summarization and information retrieval. These applications could be even better served by expanding citation information. We aim to achieve this by extracting and classifying citation information from the text, so that subsequent applications may make use of it. We make three contributions to the advancement of ﬁne-grained citation classiﬁcation. First, our work uses a standard classiﬁcation scheme for citations that was developed independently of automatic classiﬁcation and therefore is not bound to any particular citation application. Second, to address the lack of available annotated corpora and reproducible results for citation classiﬁcation, we are making available a manually-annotated corpus as a benchmark for further citation classiﬁcation research. Third, we introduce new features designed for citation classiﬁcation and compare them experimentally with previously proposed citation features, showing that these new features improve classiﬁcation accuracy. KEYWORDS: citation classiﬁcation, feature extraction. Proceedings of COLING 2012: Technical Papers, pages 1343–1358, COLING 2012, Mumbai, December 2012. 1343  
We present an approach to semantics-based statistical machine translation that uses synchronous hyperedge replacement grammars to translate into and from graph-shaped intermediate meaning representations, to our knowledge the ﬁrst work in NLP to make use of synchronous context free graph grammars. We present algorithms for each step of the semantics-based translation pipeline, including a novel graph-to-word alignment algorithm and two algorithms for synchronous grammar rule extraction. We investigate the inﬂuence of syntactic annotations on semantics-based translation by presenting two alternative rule extraction algorithms, one that requires only semantic annotations and another that additionally relies on syntactic annotations, and explore the effect of syntax and language bias in meaning representation structures by running experiments with two different meaning representations, one biased toward an English syntax-like structure and another that is language neutral. While preliminary work, these experiments show promise for semantically-informed machine translation. TITLE AND ABSTRACT IN GERMAN Semantikbasierte Maschinelle Übersetzung mit Hyperkantenersetzungsgrammatiken Wir beschreiben einen Ansatz zur semantikbasierten statistischen maschinellen Übersetzung, der synchrone Hyperkantenersetzungsgrammatiken benutzt um in und aus graphgeformten Zwischenrepräsentationen zu übersetzen. Unseres Wissens ist dies die erste Arbeit in der natürlichen Sprachverarbeitung die synchrone kontextfreie Graphgrammatiken verwendet. Wir beschreiben Algorithmen für jeden Schritt der semantikbasierten Übersetzungskette, inklusive einem neuen Graph-zu-Wort Alinierungsalgorithmus und automatische Regelextraktionsalgorithmen für synchrone Grammatiken. Wir untersuchen den Effekt der syntaktischen Annotation auf semantikbasierte Übersetzung, indem wir zwei verschiedene Regelextraktionsalgorithmen vorstellen, einen, der lediglich semantische Annotationen erfordert und einen, der zusätzlich syntaktische Informationen verwendet. Wir untersuchen ausserdem den Einﬂuss von semantischen Repräsentationen die auf bestimmte Syntax und Sprache ausgerichted sind indem wir mit zwei verschiedenen Repräsentationen experimentieren: mit einer englischausgerichteten syntaxartigen Struktur und mit einer sprachneutralen Struktur. Unsere Arbeit zeigt dass semantikbasierte maschinelle Übersetzung vielversprechend ist. ∗ The authors contributed equally to this work and are listed in randomized order. Proceedings of COLING 2012: Technical Papers, pages 1359–1376, COLING 2012, Mumbai, December 2012. 1359  
This paper investigates a solution to yes/no question answering, which can be mapped to the task of determining the correctness of a given proposition. Generally it is hard to obtain explicit evidence to conclude a proposition is false from an information source, so we convert this task to a set of factoid-style questions and use an existing question answering system as a subsystem. By aggregating the answers and conﬁdence values from a factoid-style question answering system we can determine the correctness of the entire proposition or the substitutions that make the proposition false. We evaluated the system on multiple-choice questions from a university admission test on world history, and found it to be highly accurate. Keywords: question answering, facts validation, yes/no question, question inversion. Proceedings of COLING 2012: Technical Papers, pages 1377–1392, COLING 2012, Mumbai, December 2012. 1377  
The phonetic alphabet enables people to dictate letters of the alphabet accurately by using representative words, i.e., A for Alpha. Japanese kanji (idiographic Chinese characters) vastly outnumber the letters of the Roman alphabet, and thus Japanese requires an explanatory reading like a phonetic alphabet. We call the explanatory reading of a kanji a “distinctive explanation.” Most kanji characters have their homophones, and the role of the distinctive explanations is to enable users to identify a speciﬁc kanji character only by listening to the explanation. In this paper, we propose a corpus-based method for automatically generating distinctive explanations for a kanji, in which information about familiarity and homophones of kanji are taken into consideration. Through the kanji-identiﬁcation experiments, we show that the quality of the explanations generated by the proposed method is higher than that of the manually crafted distinctive explanations. KEYWORDS: phonetic alphabet, distinctive explanation for a kanji. Proceedings of COLING 2012: Technical Papers, pages 1411–1424, COLING 2012, Mumbai, December 2012. 1411  
Extracting biomedical named entities is one of the major challenges in automatic processing of biomedical literature. This paper proposes a machine learning approach for ﬁnding phenotype names in text. Features are included in a machine learning infrastructure to implement the rules found in our previously developed rule-based system. The system also uses two available resources: MetaMap and HPO. As we are not aware of any available corpus for phenotype names, a corpus has been constructed. Since manual tagging of the corpus was not possible for us, we started tagging only HPO phenotypes in the corpus and then using a semi-supervised learning method, the tagging process improved. The evaluation results (F-Score 92.25) suggest that the system achieved good performance and it outperforms the rule-based system. KEYWORDS: Phenotype, Named Entity Recognition, MetaMap, Human Phenotype Ontology. Proceedings of COLING 2012: Technical Papers, pages 1425–1440, COLING 2012, Mumbai, December 2012. 1425  
This paper presents a novel method of improving Combinatory Categorial Grammar (CCG) parsing using features generated from Dependency Grammar (DG) parses and combined using reranking. Different grammar formalisms have different strengths and different parsing models have consequently divergent views of the data. More speciﬁcally, dependency parsers are sensitive to linguistic generalisations that differ from the generalisations that the CCG parser is sensitive to, and which the reranker exploits to identify the parse most likely to be correct. We propose DG-derived reranking features, which are obtained by comparing dependencies from the CCG parser with DG dependencies, and demonstrate how they improve the performance of a CCG parser and reranker in a variety of settings. We record a ﬁnal labeled F-score of 87.93% on section 23 of CCGbank, 0.5% and 0.35% improvements over the base parser (87.43%) and reranker (87.58%), respectively. KEYWORDS: Combinatory Categorial Grammar (CCG), Dependency Grammar (DG), Rerank- ing, Dependency Grammar-derived features, parsing, syntax. Proceedings of COLING 2012: Technical Papers, pages 1441–1458, COLING 2012, Mumbai, December 2012. 1441  
Distributed representations of words have proven extremely useful in numerous natural language processing tasks. Their appeal is that they can help alleviate data sparsity problems common to supervised learning. Methods for inducing these representations require only unlabeled language data, which are plentiful for many natural languages. In this work, we induce distributed representations for a pair of languages jointly. We treat it as a multitask learning problem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data. These representations can be used for a number of crosslingual learning tasks, where a learner can be trained on annotations present in one language and applied to test data in another. We show that our representations are informative by using them for crosslingual document classiﬁcation, where classiﬁers trained on these representations substantially outperform strong baselines (e.g. machine translation) when applied to a new language. KEYWORDS: distributed representations, multilingual learning, direct transfer of annotation. Proceedings of COLING 2012: Technical Papers, pages 1459–1474, COLING 2012, Mumbai, December 2012. 1459  
ABSTRACT Event anaphora resolution plays a critical role in discourse analysis. This paper focuses on improving event pronoun resolution using both local and global semantic information. In particular, a predicate-argument structure is proposed to represent the local semantic information about an event while the global semantic information is represented by the entity coreference chains related with various arguments in the predicate-argument structure to complement its locality. Evaluation on the OntoNotes English corpus shows the effectiveness of local and global semantic information for event pronoun resolution. KEYWORDS: event pronoun, semantic information, corefernce resolution, predicate-argument structure  *Corresponding author  Proceedings of COLING 2012: Technical Papers, pages 1475–1488, COLING 2012, Mumbai, December 2012. 1475  
Compounds occur very frequently in Indian Languages. There are no strict orthographic conventions for compounds in modern Indian Languages. In this paper, Sanskrit compounding system is examined thoroughly and the insight gained from the Sanskrit grammar is applied for the analysis of compounds in Hindi and Marathi. It is interesting to note that compounding in Hindi deviates from that in Sanskrit in two aspects. The data analysed for Hindi does not contain any instance of Bahuvr¯ıhi (exo-centric) compound. Second, Hindi data presents many cases where quite a lot of compounds require a verb as well as vibhakti(a case marker) for its paraphrasing. Compounds requiring a verb for paraphrasing are termed as madhyama-pada-lop¯ı in Sanskrit, and they are found to be rare in Sanskrit. KEYWORDS: Multi Word Expression, Compounds, Pa¯n. ini, As.t.a¯dhya¯y¯ı, semantics. Proceedings of COLING 2012: Technical Papers, pages 1489–1502, COLING 2012, Mumbai, December 2012. 1489  
We present a novel topic modelling-based methodology to track emerging events in microblogs such as Twitter. Our topic model has an in-built update mechanism based on time slices and implements a dynamic vocabulary. We ﬁrst show that the method is robust in detecting events using a range of datasets with injected novel events, and then demonstrate its application in identifying trending topics in Twitter. KEYWORDS: Topic Model, Twitter, Trend Detection, Topic Evolution, Online Processing. Proceedings of COLING 2012: Technical Papers, pages 1519–1534, COLING 2012, Mumbai, December 2012. 1519  
Approximate sentence matching (ASM) is an important technique for tasks in machine translation (MT) such as example-based MT (EBMT) which inﬂuences the translation time and the quality of translation output. We investigate different approaches to ﬁnd similar sentences in an example base and evaluate their efﬁciency (runtime), effectiveness, and the resulting quality of translation output. A comparison of approaches demonstrates that i) a sequential computation of the edit distance between an input sentence and all sentences in the example base is not feasible, even when efﬁcient algorithms to compute the edit distance are employed; ii) in-memory data structures such as tries and ternary search trees are more efﬁcient in terms of runtime, but are not scalable for large example bases; iii) standard IR models which only cover material similarity (e.g. term overlap), do not perform well in ﬁnding the approximate matches, due to their lack of handling word order and word positions. We propose a new retrieval model derived from language modeling (LM), named LM-ASM, to include positional and ordinal similarities in the matching process, in addition to material similarity. Our IR based retrieval experiments involve reranking the top-ranked documents based on their true edit distance score. Experimental results show that i) IR based approaches result in about 100 times faster translation; ii) LM-ASM approximates edit distance better than standard LM by about 10%; and iii) surprisingly, LM-ASM even improves MT quality by 1.52% in comparison to sequential edit distance computation. KEYWORDS: Example-Based Machine Translation, Information Retrieval, Approximate Sen- tence Matching, Edit Distance. Proceedings of COLING 2012: Technical Papers, pages 1571–1586, COLING 2012, Mumbai, December 2012. 1571  
There are many abbreviation and non-standard tokens in SMS and Twitter messages. Normalizing these non-standard tokens will ease natural language processing modules for these domains. Recently, character-level machine translation (MT) and sequence labeling methods have been used for this normalization task, and demonstrated competitive performance. In this paper, we propose an approach to segment words into blocks of characters according to their phonetic symbols, and apply MT and sequence labeling models on such block-level. We also propose to combine these methods, as well as with other existing methods, in order to leverage their different strengths. Our experiments show our proposed approach achieved high precision and broad coverage. TITLE AND ABSTRACT IN CHINESE 使用字符块模型和系统合成提高文字纠错率 在 手 机 短 信 和Twitter消 息 中 存 在 许 多 缩 写 和 非 规 范 的 单 词 。 纠 正 这 些 非 规 范 的 单 词 会给后续的自然语言处理模型带来便利。近年来，基于字符层面的机器翻译方法和序列标 注方法被广泛应用到这个任务中，并且大大提高了纠错率。本文介绍了一种根据单词发音 来切割单词的方法，然后把上述的两种模型利用在切割后的字符块层面进行纠错尝试。另 外，我们还尝试了将现有的方法和提出的方法进行各种合成以期利用各自的优点来提高最 终的效果。最后我们的实验表明我们的方法在对非规范词的纠错召回率和准确率上都有了 明显的提高。 KEYWORDS: text normalization, machine translation, CRF, NLP application. KEYWORDS IN CHINESE: 文本纠错，机器翻译，条件随机场，自然语言处理应用. Proceedings of COLING 2012: Technical Papers, pages 1587–1602, COLING 2012, Mumbai, December 2012. 1587  
Currently, Chinese event extraction systems suffer much from the low quality of annotated event corpora and the high ratio of pseudo trigger mentions to true ones. To resolve these two issues, this paper proposes a joint model of trigger identification and event type determination. Besides, several trigger filtering schemas are introduced to filter out those pseudo trigger mentions as many as possible. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline. 一个应用于中文事件抽取的事件触发词识别和类型判别联合模 型 当前，有2个问题困扰着中文事件抽取系统：低质量的事件标记语料库和假事件触发词相 对于真事件触发词的高比例。为了解决以上2个问题，本文提出了一个结合事件触发词识 别和事件类型判别的联合模型。另外，几个触发词过滤模式同样被引入本系统用于过滤掉 尽可能多的假触发词实例。在ACE2005中文语料上的测试结果表明，本文的方法和基准系 统相比具有更高的性能。 KEYWORDS: Joint modeling, Event type determination, Trigger identification, Trigger filtering. KEYWORDS IN L2: 联合模型, 事件类型判别, 触发词识别, 触发词过滤  Corresponding author  Proceedings of COLING 2012: Technical Papers, pages 1635–1652, COLING 2012, Mumbai, December 2012. 1635  
Proceedings of COLING 2012: Technical Papers, pages 1653–1670, COLING 2012, Mumbai, December 2012. 1653  
We propose a ﬁrst ever code-switch language model for mixed language speech recognition that incorporates syntactic constraints by a code-switch boundary prediction model, a code-switch translation model, and a reconstruction model. A WFST-based decoder then recognizes speech by combining an acoustic model, a pronunciation model and the code-switch language model in an integrated approach. Our proposed approach avoids making early decisions on code-switch boundaries and is therefore more robust than previous approaches. Our proposed system using the code-switch language model outperforms a baseline of interpolated language models by a statistically signiﬁcant 0.91% on a mixed language lecture speech corpus, and 1.25% on a mixed language lunch conversation corpus. Our method also outperforms a language model that permits code-switch at all word boundaries by a statistically signiﬁcant 1.35% on the lecture speech corpus and 1.69% on the lunch conversation corpus. KEYWORDS: Code-switch, mixed language, language modeling. Proceedings of COLING 2012: Technical Papers, pages 1671–1680, COLING 2012, Mumbai, December 2012. 1671  
Recent study shows that parsing accuracy can be largely improved by the joint optimization of part-of-speech (POS) tagging and dependency parsing. However, the POS tagging task does not beneﬁt much from the joint framework. We argue that the fundamental reason behind is because the POS features are overwhelmed by the syntactic features during the joint optimization, and the joint models only prefer such POS tags that are favourable solely from the parsing viewpoint. To solve this issue, we propose a separately passive-aggressive learning algorithm (SPA), which is designed to separately update the POS features weights and the syntactic feature weights under the joint optimization framework. The proposed SPA is able to take advantage of previous joint optimization strategies to signiﬁcantly improve the parsing accuracy, but also overcome their shortages to signiﬁcantly boost the tagging accuracy by effectively solving the syntax-insensitive POS ambiguity issues. Experiments on the Chinese Penn Treebank 5.1 (CTB5) and the English Penn Treebank (PTB) demonstrate the effectiveness of our proposed methodology and empirically verify our observations as discussed above. We achieve the best tagging and parsing accuracies on both datasets, 94.60% in tagging accuracy and 81.67% in parsing accuracy on CTB5, and 97.62% and 93.52% on PTB. KEYWORDS: Part-of-speech Tagging, Dependency Parsing, Joint Models, Separately Passive- aggressive Algorithm.  ∗ Corresponding author  Proceedings of COLING 2012: Technical Papers, pages 1681–1698, COLING 2012, Mumbai, December 2012. 1681  
We study the multi-tweet summarization task, which aims to ﬁnd representative tweets from a given set of tweets. Multi-tweet summarization allows people to quickly grasp the essential meaning of a large number of tweets. It can also be used as a pre-processing component for information extraction tasks on tweets. The challenge of this task lies in computing a tweet’s salience score with little information in a single tweet. We propose a graph-based multi-tweet summarization system that incorporates social network features, which make up for the information shortage in a tweet. Another distinguished feature of our system is that tweet readability and user diversity are considered. We evaluate our system on a manually annotated dataset, and show that our system outperforms the stateof-the-art baseline. We further evaluate our method in a real scenario of summarization of Twitter search results and demonstrate its effectiveness. Title and Abstract in another language (Chinese) 基于图模型和社会关系特征的推特消息摘要方法 本文考察多推特消息摘要任务。其目的是帮助用户快速了解大量推特消息的基本意思，或 在信息抽取前对推特消息做预处理。 该任务的主要挑战是：单条推特消息往往不能提供足 够的信息来计算它的显著度。本文提出基于图模型的方法，考虑社会关系网相关的特征、 可读性及用户的多样性来克服单条推特消息的不足。 在人工标注的数据集以及推特搜索 上，该方法均展示了其有效性。 Keywords: Graph, Summarization, Social signals, Tweets. Keywords in Chinese: 图模型，摘要，社会关系网特征，推特消息 . Proceedings of COLING 2012: Technical Papers, pages 1699–1714, COLING 2012, Mumbai, December 2012. 1699  
Keyphrase extraction aims to ﬁnd representative phrases for a document. Keyphrases are expected to cover main themes of a document. Meanwhile, keyphrases do not necessarily occur frequently in the document, which is known as the vocabulary gap between the words in a document and its keyphrases. In this paper, we propose Topical Word Trigger Model (TWTM) for keyphrase extraction. TWTM assumes the content and keyphrases of a document are talking about the same themes but written in different languages. Under the assumption, keyphrase extraction is modeled as a translation process from document content to keyphrases. Moreover, in order to better cover document themes, TWTM sets trigger probabilities to be topic-speciﬁc, and hence the trigger process can be inﬂuenced by the document themes. On one hand, TWTM uses latent topics to model document themes and takes the coverage of document themes into consideration; on the other hand, TWTM uses topic-speciﬁc word trigger to bridge the vocabulary gap between the words in document and keyphrases. Experiment results on real world dataset reveal that TWTM outperforms existing state-of-the-art methods under various evaluation metrics. TITLE AND ABSTRACT IN CHINESE Ù Ã Ù º ½³  »³ÎÃ º³³ß ð¬ Ã ¤ß¼¬ÃÃÙáËî¿àÃÆǑ³½ÙßÐ³Ý³ª¬Ð ¬Â ­Ð¬ûð³­è¿³è ÙãùÄ¬ Ä¿ÙÃÜ¬ÄÃ  ù  ÙÝ  ¬ǑªÃ  ¿  ¿Ãºß»ËǑÃòTWð¸èT¿èà¨M³¬à´ßÂ ½ÃÌÄ¬³Ì¨Ìº³ ºÌº³ º¬Ñ õÌÙÃÙ  KEYWORDS: keyphrase extraction, latent topic model, word trigger model.  ³ õ Ã º Ù º KEYWORDS IN CHINESE:  ,  ,  .  Proceedings of COLING 2012: Technical Papers, pages 1715–1730, COLING 2012, Mumbai, December 2012. 1715  
In the present work we raise the hypothesis that eye-movements when reading texts reveal task performance, as measured by the level of understanding of the reader. With the objective of testing that hypothesis, we introduce a framework to integrate geometric information of eye-movements and text layout into natural language processing models via image processing techniques. We evidence the patterns in reading behavior between subjects with similar task performance using principal component analysis and quantify the likelihood of our hypothesis using the concept of linear separability. Finally, we point to potential applications that could beneﬁt from these ﬁndings. KEYWORDS: eye-tracking, natural language processing, image recognition. TITLE AND ABSTRACT IN JAPANESE 視線の動きとテキスト素性を用いた読み手の個性認識 本研究では、テキストを読む際の視線の動きから、テキストの理解度によって測定され るような読み手のタスクパフォーマンスが予測可能である、という仮説を立てる。この 仮説を検証するため、我々はまず、画像処理技術を介して、視線の動きとテキスト配置 に関する位置情報を、自然言語処理のモデルとして統合する枠組を導入する。次に我々 は、近いタスクパフォーマンスの被験者間に共通した読解行動のパターンを主成分分析 によって同定し、この線形分離可能性を求めることで我々の仮説の蓋然性を定量的に示 す。最後に、我々はこれらの発見から恩恵を受け得る応用例について述べる。 KEYWORDS: 視線追跡、自然言語処理、画像認識. Proceedings of COLING 2012: Technical Papers, pages 1747–1762, COLING 2012, Mumbai, December 2012. 1747  
We explore the contribution of distributional information for purely knowledge-based word sense disambiguation. Speciﬁcally, we use a distributional thesaurus, computed from a large parsed corpus, for lexical expansion of context and sense information. This bridges the lexical gap that is seen as the major obstacle for word overlap–based approaches. We apply this mechanism to two traditional knowledge-based methods and show that distributional information signiﬁcantly improves disambiguation results across several data sets. This improvement exceeds the state of the art for disambiguation without sense frequency information—a situation which is especially encountered with new domains or languages for which no sense-annotated corpus is available. TITLE AND ABSTRACT IN GERMAN Über die Bestimmung lexikalischer Expansionen mittels distributioneller Ähnlichkeit und deren Einsatz in der wissensbasierten Lesartendisambiguierung Wir untersuchen den Einﬂuss distributioneller Informationen auf die rein wissensbasierte Lesartendisambiguierung. Basierend auf einem distributionellen Thesaurus, den wir aus einem großen geparsten Korpus erzeugen, erweitern wir die Deﬁnition der Lesart und deren Kontext mit lexikalischen Expansionen. Dadurch schließen wir die ‘lexikalische Lücke’, die sich als Haupthindernis für Ansätze basierend auf Wortgemeinsamkeiten herausgestellt hat. Wir erweitern zwei klassische wissensbasierte Ansätze um lexikalische Expansionen und zeigen, dass dadurch die Qualität der Lesartendisambiguierung deutlich erhöht wird. Wir erzielen die bisher besten veröffentlichten Ergebnisse für Disambiguierung ohne Nutzung der Lesartenhäuﬁgkeiten, was besonders für Domänen oder Sprachen relevant ist, für die keine Lesarten-annotierten Korpora zur Verfügung stehen. KEYWORDS: word sense disambiguation, distributional thesaurus, lexical expansion. KEYWORDS IN GERMAN: Lesartendisambiguierung, distributioneller Thesaurus, lexikalische Expansion. Proceedings of COLING 2012: Technical Papers, pages 1781–1796, COLING 2012, Mumbai, December 2012. 1781  
In this paper, we present a new method that improves the alignment of equivalent terms monolingually acquired from bilingual comparable corpora: the Compositional Method with Context-Based Projection (CMCBP). Our overall objective is to identify and to translate high specialized terminology made up of multi-word terms acquired from comparable corpora. Our evaluation in the medical domain and for two pairs of languages demonstrates that CMCBP outperforms the state-of-art compositional approach commonly used for translationally equivalent multi-word term discovery from comparable corpora. KEYWORDS: Comparable corpora, bilingual lexicon, compositionality, multi-word term, con- text information. 
This paper presents a novel approach to document-based discourse analysis by performing a global A* search over the space of possible structures while optimizing a global criterion over the set of potential coherence relations. Existing approaches to discourse analysis have so far relied on greedy search strategies or restricted themselves to sentence-level discourse parsing. Another advantage of our approach, over other global alternatives (like Maximum Spanning Tree decoding algorithms), is its ﬂexibility in being able to integrate constraints (including linguistically motivated ones like the Right Frontier Constraint). Finally, our paper provides the ﬁrst discourse parsing system for French; our evaluation is carried out on the Annodis corpus. While using a lot less training data than earlier approaches than previous work on English, our system manages to achieve state-of-the-art results, with F1-scores of 66.2 and 46.8 when compared to unlabeled and labeled reference structures.  Keywords: Discourse Structure, Discourse Parsing, Dependency Structures, Constrained Decoding, A*.  Proceedings of COLING 2012: Technical Papers, pages 1883–1900, COLING 2012, Mumbai, December 2012. 1883  
In this paper, we propose the use of spans in addition to edges in noun compound analysis. A span is a sequence of words that can represent a noun compound. Compared with edges, spans have good properties in terms of semi-supervised parsing. They can be reliably extracted from a huge amount of unannotated text. In addition, while the combinations of edges such as sibling and grandparent interactions are, in general, difﬁcult to handle in parsing, it is quite easy to utilize spans with arbitrary width. We show that spans can be incorporated straightforwardly into the standard chart-based parsing algorithm. We create a semi-supervised discriminative parser that combines edge and span features. Experiments show that span features improve accuracy and that further gain is obtained when they are combined with edge features. TITLE AND ABSTRACT IN JAPANESE スパンとエッジ特徴量を用いた 半教師あり名詞句解析 名詞句解析において，エッジだけでなくスパンを手がかりとして使うことを提案する． スパンは名詞句を表しうる単語列であり，エッジと比べて半教師あり学習に適した性質 を持っている．すなわち，大量の生テキストから高い信頼性をもって抽出可能である． さらに，エッジは解析時に組み合わせ (兄弟や孫の関係など) を考えることが一般に難 しいのに対して，スパンは任意の長さの組み合わせを自明に利用できる．この論文で は，スパンが動的計画法による標準的な構文解析手法に簡単に組み込めることを示し， エッジとスパン特徴量を組み合わせた半教師ありの識別型構文解析器を提案する．実験 により，スパン特徴量が解析精度を改善し，エッジと組み合わせることでさらに精度が 向上することが示された． KEYWORDS: noun compound analysis, semi-supervised learning, parsing, span. KEYWORDS IN JAPANESE: 名詞句解析, 半教師あり学習, 構文解析, スパン. Proceedings of COLING 2012: Technical Papers, pages 1915–1932, COLING 2012, Mumbai, December 2012. 1915  
In this paper, we introduce an application of matrix factorization to produce corpus-derived, distributional models of semantics that demonstrate cognitive plausibility. We ﬁnd that word representations learned by Non-Negative Sparse Embedding (NNSE), a variant of matrix factorization, are sparse, effective, and highly interpretable. To the best of our knowledge, this is the ﬁrst approach which yields semantic representation of words satisfying these three desirable properties. Though extensive experimental evaluations on multiple real-world tasks and datasets, we demonstrate the superiority of semantic models learned by NNSE over other state-of-the-art baselines. Keywords: distributional semantics, sparse coding, neuro-semantics, vector-space models, inter- pretability, word embeddings. Proceedings of COLING 2012: Technical Papers, pages 1933–1950, COLING 2012, Mumbai, December 2012. 1933  
The paper presents results of clustering terms extracted from economic articles in Polish Wikipedia. First, we describe the method of automatic term extraction supported by linguistic knowledge. Then, we deﬁne different types of term similarities used in the clustering experiment. Term similarities are based on Polish Wordnet and morphosyntactic analysis of data. The latter takes into account: term contexts, coordinated sequences of terms, syntactic patterns in which terms appear and words that are parts of terms (such as their heads and modiﬁers). Then we performed several experiments with hierarchical clustering of the 400 most frequent terms. We present the results of clustering when different groups of similarity coefﬁcients are applied. Finally, we present an evaluation that compares the results with manually obtained groups. Our results prove that morphosyntactic information can help or even serve themselves for initial clustering of terms in semantically coherent groups. KEYWORDS: terminology extraction, terminology clustering, Polish. Proceedings of COLING 2012: Technical Papers, pages 1951–1962, COLING 2012, Mumbai, December 2012. 1951  
One of the main issues in a word alignment task is the difﬁculty of handling function words that do not have direct translations which we call unique function words. They are often aligned to some words in the other language incorrectly. This is prominent in language pairs with very different sentence structures. In this paper, we propose a novel approach for handling unique function words. The proposed model monolingually derives unique function words from bilingually generated treelet pairs. The monolingual derivation prevents incorrect alignments for unique function words. The derivation probabilities are estimated from a large monolingual corpus, which is much easier to acquire than a parallel corpus. Also, the proposed alignment model uses semantic-head dependency trees where dependency relations between words become similar in each language. Experimental results on an English-Japanese corpus show that the proposed model achieves better alignment and translation quality compared with the baseline models. TITLE AND ABSTRACT IN JAPANESE ೋ‫ޠݴ‬ͷੜ੒ͱ୯‫ޠݴ‬ͷ೿ੜʹΑΔΞϥΠϝϯτ ୯‫ޠ‬ΞϥΠϝϯτλεΫʹ͓͚Δओͳ໰୊ͷҰͭ͸ɺ‫ػ‬ೳ‫ޠ‬ͷதͰ΋૬ख‫ʹޠݴ‬ରԠ͢Δ‫ޠ‬ ͕ଘࡏ͠ͳ͍‫ػ‬ೳ‫ޠ‬ͷѻ͍ͷࠔ೉͞Ͱ͋Δɻզʑ͸͜ͷΑ͏ͳ‫ޠ‬Λ‫ػཱݽ‬ೳ‫Ϳݺͱޠ‬ɻ‫ཱݽ‬ ‫ػ‬ೳ‫ޠ‬͸ɺ૬ख‫ޠݴ‬ͷԿΒ͔ͷ୯‫ʹޠ‬ෆద੾ʹରԠ෇͚ΒΕΔ͜ͱ͕ଟ͘ɺ͜Ε͸ಛʹจߏ ଄͕େ͖͘ҟͳΔ‫ޠݴ‬ରʹ͓͍ͯ‫ݦ‬ஶͰ͋Δɻຊ࿦จͰ͸ɺ‫ػཱݽ‬ೳ‫ޠ‬Λѻ͏ͨΊͷ৽͍͠ ख๏ΛఏҊ͢ΔɻఏҊϞσϧ͸ɺೋ‫Ͱޠݴ‬ੜ੒͞Εͨ෦෼໦ϖΞ͔Βɺ‫ػཱݽ‬ೳ‫ޠ‬ΛͦΕͧ Ε୯‫Ͱޠݴ‬೿ੜ͢Δ͜ͱʹΑΓɺ‫ػཱݽ‬ೳ‫ͯͬޡ͕ޠ‬ରԠ෇͚ΒΕΔ͜ͱΛ๷͙ɻ೿ੜ֬཰ ͸ɺର༁ίʔύεʹൺ΂ͯೖख͕༰қͰ͋Δେ‫ن‬໛୯‫ޠݴ‬ίʔύε͔Βਪఆ͢Δɻ·ͨఏҊ Ϟσϧ͸ɺ୯‫ޠ‬ಉ࢜ͷґଘؔ܎͕֤‫ͳۙ͘Ͱޠݴ‬ΔΑ͏ʹɺҙຯओࣙґଘߏ଄໦Λ༻͍Δɻ ӳ೔ίʔύεͰͷ࣮‫݁ݧ‬Ռ͔ΒɺఏҊϞσϧ͸ϕʔεϥΠϯϞσϧͱൺ΂ͯΑΓྑ͍ΞϥΠ ϝϯτ͓Αͼ຋༁ਫ਼౓Λ࣮‫ͨ͠ݱ‬ɻ KEYWORDS: monolingual derivation, semantic-head dependency tree, treelet alignment. KEYWORDS IN JAPANESE: ୯‫ޠݴ‬ͷ೿ੜ, ҙຯओࣙґଘߏ଄໦, ໦ߏ଄ΞϥΠϝϯτ. Proceedings of COLING 2012: Technical Papers, pages 1963–1978, COLING 2012, Mumbai, December 2012. 1963  
We study a problem with pairwise ranking optimization (PRO): that it tends to yield too short translations. We ﬁnd that this is partially due to the inadequate smoothing in PRO’s BLEU+1, which boosts the precision component of BLEU but leaves the brevity penalty unchanged, thus destroying the balance between the two, compared to BLEU. It is also partially due to PRO optimizing for a sentence-level score without a global view on the overall length, which introducing a bias towards short translations; we show that letting PRO optimize a corpus-level BLEU yields a perfect length. Finally, we ﬁnd some residual bias due to the interaction of PRO with BLEU+1: such a bias does not exist for a version of MIRA with sentence-level BLEU+1. We propose several ways to ﬁx the length problem of PRO, including smoothing the brevity penalty, scaling the effective reference length, grounding the precision component, and unclipping the brevity penalty, which yield sizable improvements in test BLEU on two Arabic-English datasets: IWSLT (+0.65) and NIST (+0.37). KEYWORDS: Statistical machine translation, parameter optimization, MERT, PRO, MIRA. Proceedings of COLING 2012: Technical Papers, pages 1979–1994, COLING 2012, Mumbai, December 2012. 1979  
Many NLP tasks interact with syntax. The presence of a named entity span, for example, is often a clear indicator of a noun phrase in the parse tree, while a span in the syntax can help indicate the lack of a named entity in the spans that cross it. For these types of problems joint inference offers a better solution than a pipelined approach, and yet large joint models are rarely pursued. In this paper we argue this is due in part to the absence of a general framework for joint inference which can efﬁciently represent syntactic structure. We propose an alternative and novel method in which constituency parse constraints are imposed on the model via combinatorial factors in a Markov random ﬁeld, guaranteeing that a variable conﬁguration forms a valid tree. We apply this approach to jointly predicting parse and named entity structure, for which we introduce a zero-order semi-CRF named entity recognizer which also relies on a combinatorial factor. At the junction between these two models, soft constraints coordinate between syntactic constituents and named entity spans, providing an additional layer of ﬂexibility on how these models interact. With this architecture we achieve the best-reported results on both CRF-based parsing and named entity recognition on sections of the OntoNotes corpus, and outperform state-of-the-art parsers on an NP-identiﬁcation task, while remaining asymptotically faster than traditional grammar-based parsers. 
In recent years, error mining approaches have been proposed to identify the most likely sources of errors in symbolic parsers and generators. However the techniques used generate a ﬂat list of suspicious forms ranked by decreasing order of suspicion. We introduce a novel algorithm that structures the output of error mining into a tree (called, suspicion tree) highlighting the relationships between suspicious forms. We illustrate the impact of our approach by applying it to detect and analyse the most likely sources of failure in surface realisation; and we show how the suspicion tree built by our algorithm helps presenting the errors identiﬁed by error mining in a linguistically meaningful way thus providing better support for error analysis. The right frontier of the tree highlights the relative importance of the main error cases while the subtrees of a node indicate how a given error case divides into smaller more speciﬁc cases.  Title and Abstract in Hindi  Ú  к  х  кх  к  ,  к к Ú - хк  Ú - u Úк к  к  к Êк  ки  х  кк  к  ।  к  к к Ú к uÚ  к eк  Ê uк  Úк  к।  eк  кк  к  хк  к eк  (Ú  )к  к Ú к uÚ  к º Ê к º к Úх  ।  a к кк u  Ú - u Úк к  a  aк  к  кк  к।  Úх  кк  кк  Ú  х  Ê  к  ш кÚ к  кa  к  к  -  к  ÚÚ  º  ।Ú  кÚ  uÚ  к uк  к  к  к  ºк Ú  к ш х Úх  к к eк  uÚ  -  ш uÚ  º  ।  Keywords: Error Mining, Generation. Keywords in Hindi: х , Ú - u Úк.  Proceedings of COLING 2012: Technical Papers, pages 2011–2026, COLING 2012, Mumbai, December 2012. 2011  
We present a novel algorithm for surface realisation with lexicalist grammars. In this algorithm, the structure of the input is used both top-down to constrain the selection of applicable rules and bottom-up to ﬁlter the initial search space associated with local input trees. In addition, parallelism is used to recursively pursue the realisation of each daughter node in the input tree. We evaluate the algorithm on the input data provided by the Generation Challenge Surface Realisation Task and show that it drastically reduce processing time when compared with a simpler, top-down driven, lexicalist approach. Title and Abstract in Hindi  Ê-  ш Ú-  uÚ  ш Ú-  кк  u  к  u  a  к кк  Úх  к кº u  к к ÚÚ  к  ।i к к  кÊ  “Ê  к  к  к шх к  к “u Ú  к  кÚ  Ú  кa к  к  Úк  a  Ê кu  Ú“  ”a  Ê  Ú  к  । iк  ,  Ú  кш  кк  :  Ú ” к aк  к  “  Ê”  ш Ú-  Ú।  eк Ê” Úк ।a к  Keywords: Generation, Tree Adjoining Grammar, Surface Realization.  Keywords in Hindi: u Ú , - - к ,  Ú.  Proceedings of COLING 2012: Technical Papers, pages 2027–2042, COLING 2012, Mumbai, December 2012. 2027  
We describe two methods for syntactic source reordering developed for English-German SMT. Both methods learn from bilingual data accompanied by automatic word alignments to reorder the source such that it resembles that of the target. While the ﬁrst method is an extension of a parse-based algorithm and accommodates contextual triggers in the parse, the second method uses a linear feature-based cost model along with a Traveling Salesman Problem (TSP) solver to perform the reordering. Our results indicate that both methods lead to improvements in BLEU scores in both directions, English→German and German→English. Signiﬁcant gains in human translation quality assessment are observed for German→English, however, no signiﬁcant changes are observed in the human assessment for English→German. KEYWORDS: Parse-based reordering, TSP, English-German SMT. Proceedings of COLING 2012: Technical Papers, pages 2043–2058, COLING 2012, Mumbai, December 2012. 2043  
We take up the challenge of learning a grounded model of language when our agent has a body of machine learning algorithms and no prior knowledge of either the physical domain or language, in the sense of "least commitment". Based on a 2D video and co-occurring raw text, we demonstrate how this cognitively inspired model segments the world to obtain a meaning space, and combines words into hierarchical patterns for a linguistic pattern space. By associating these two spaces under temporal co-occurrence constraints, we demonstrate the acquisition of term-meaning pairs for names, actions and relations. We next map physical arguments for actions and relations to syntactical constructions resembling a cognitive grammar framework. Thus the system is able to bootstrap a rudimentary lexicon and syntax. While experiments are primarily in English, we present partial results for Hindi obtained without any change in the methods, to indicate its potential application to other languages. KEYWORDS: Cognitive grammar, image schema, ADIOS. Proceedings of COLING 2012: Technical Papers, pages 2059–2076, COLING 2012, Mumbai, December 2012. 2059  
º Ù, Ò Ó Ð Ù º Ù, Ð Ù ×× ºÙÒ Ñ Ð º Ùº Ù, Ø Ð Û ÒºÒ Ø ABSTRACT Automatically extracting terminology and index terms from scientiﬁc literature is useful for a variety of digital library, indexing and search applications. This task is non-trivial, complicated by domain-speciﬁc terminology and a steady introduction of new terminology. Correctly identifying nested terminology further adds to the challenge. We present a Dirichlet Process (DP) model of word segmentation where multiword segments are either retrieved from a cache or newly generated. We show how this DP-Segmentation model can be used to successfully extract nested terminology, outperforming previous methods for solving this problem. KEYWORDS: Dirichlet Process, segmentation, Bayesian learning, index term, keyphrase ex- traction. Proceedings of COLING 2012: Technical Papers, pages 2077–2092, COLING 2012, Mumbai, December 2012. 2077  
We show that by making use of information common to document sets belonging to a common category, we can improve the quality of automatically extracted content in multi-document summaries. This simple property is widely applicable in multi-document summarization tasks, and can be encapsulated by the concept of category-speciﬁc importance (CSI). Our experiments show that CSI is a valuable metric to aid sentence selection in extractive summarization tasks. We operationalize the computation CSI of sentences through the introduction of two new features that can be computed without needing any external knowledge. We also generalize this approach, showing that when manually-curated document-to-category mappings are unavailable, performing automatic categorization of document sets also improves summarization performance. We have incorporated these features into a simple, freely available, open-source extractive summarization system, called SWING. In the recent TAC-2011 guided summarization task, SWING outperformed all other participant summarization systems as measured by automated ROUGE measures. KEYWORDS: text summarization, csi, guided summarization, tac. Proceedings of COLING 2012: Technical Papers, pages 2093–2108, COLING 2012, Mumbai, December 2012. 2093  
We study the problem of classifying the temporal relationship between events and time expressions in text. In contrast to previous methods that require extensive feature engineering, our approach is simple, relying only on a measure of parse tree similarity. Our method generates such tree similarity values using dependency parses as input to a convolution kernel. The resulting system outperforms the current state-of-the-art. To further improve classiﬁer performance, we can obtain more annotated data. Rather than rely on expert annotation, we assess the feasibility of acquiring annotations through crowdsourcing. We show that quality temporal relationship annotation can be crowdsourced from novices. By leveraging the problem structure of temporal relation classiﬁcation, we can selectively acquire annotations on problem instances that we assess as more difﬁcult. Employing this annotation strategy allows us to achieve a classiﬁcation accuracy of 73.2%, a statistically signiﬁcant improvement of 8.6% over the previous state-of-the-art, while trimming annotation efforts by up to 37%. Finally, as we believe that access to sufﬁcient training data is a signiﬁcant barrier to current temporal relationship classiﬁcation, we plan to share our collected data with the research community to promote benchmarking and comparative studies. KEYWORDS: temporal relations, information extraction, crowdsourcing, convolution kernels, clause structure, dependency parsing. Proceedings of COLING 2012: Technical Papers, pages 2109–2124, COLING 2012, Mumbai, December 2012. 2109  
Accurate recovery of predicate-argument dependencies is vital for interpretation tasks like information extraction and question answering, and unbounded dependencies may account for a signiﬁcant portion of the dependencies in any given text. This paper describes a categorial grammar which, like other categorial grammars, imposes a small, uniform, and easily learnable set of semantic composition operations based on functor-argument relations, but like HPSG, is generalized to limit the number of categories used to those needed to enforce grammatical constraints. The paper also describes a novel reannotation system used to map existing resources based on Government and Binding Theory, like the Penn Treebank, into this categorial representation. This grammar is evaluated on an existing unbounded dependency recovery task (Rimell et al., 2009; Nivre et al., 2010). KEYWORDS: Grammar and formalisms, Semantics. Proceedings of COLING 2012: Technical Papers, pages 2125–2140, COLING 2012, Mumbai, December 2012. 2125  
基于汉藏句子对齐语料的藏文BaseNP识别框架 本文提出藏文BaseNP识别框架，它分两步完成。先通过句法分析得到汉语BaseNP。再为 这些汉语BaseNP从汉藏句子对齐语料中识别出藏文对应短语。我们应用四种方法识别藏 文BaseNP，分别是词对齐、迭代重估算法、词典和词对齐相结合的方法以及基于序列相 交的方法。评价实验表明，没有藏文词性标注和树库的前提下，基于序列相交的方法性能 最好。本文提出的框架可以用于藏文动词短语识别任务中。 KEYWORDS : Tibetan information processing; base noun phrase; head-phrase; CHINESE KEYWORDS :藏文信息处理,基本名词短语,中心语块 Proceedings of COLING 2012: Technical Papers, pages 2141–2158, COLING 2012, Mumbai, December 2012. 2141  
Most Arabic Named Entity Recognition (NER) systems have been developed using either of two approaches: a rule-based or Machine Learning (ML) based approach, with their strengths and weaknesses. In this paper, the problem of Arabic NER is tackled through integrating the two approaches together in a pipelined process to create a hybrid system with the aim of enhancing the overall performance of NER tasks. The proposed system is capable of recognizing 11 different types of named entities (NEs): Person, Location, Organization, Date, Time, Price, Measurement, Percent, Phone Number, ISBN and File Name. Extensive experiments are conducted using three different ML classifiers to evaluate the overall performance of the hybrid system. The empirical results indicate that the hybrid approach outperforms both the rule-based and the ML-based approaches. Moreover, our system outperforms the state-of-the-art of Arabic NER in terms of accuracy when applied to ANERcorp dataset, with f-measures 94.4% for Person, 90.1% for Location, and 88.2% for Organization.  KEYWORDS : Natural Language Processing, Named Entity Recognition, Machine Learning.  Title in Arabic ‫تنسي ٌق متتا ٍل في التعرف على أنماط ا سماء العربية من خ ل استخدام المنھج الھجين‬  Abstract in Arabic  ،‫تم بناء معظم أنظمة التعرف على أنماط ا سماء العربية من خ ل تبني منھجية القواعد أو تبني المنھجية المبنية على تعلم ا لة‬ ‫ عملية التعرف على أنماط ا سماء في اللغة العربية يتم معالجتھا من خ ل دمج‬،‫ في ھذه الورقة‬.‫بما فيھما من نقاط قوة وضعف‬ ‫ النظام ال ُمقترح‬.‫المنھجيتين م ًعا في تنسي ٍق متتا ٍل لتشكيل المنھج الھجين في محاولة لتحسين أداء مھام التعرف على أنماط ا سماء‬ ،‫ والتواريخ‬،‫ والمنظمات‬،‫ وا ماكن‬،‫ نو ًعا مختلفًا من أنماط ا سماء بما في ذلك أسماء ا شخاص‬11 ‫قادر على التعرف على‬ ‫ وردمك )الرقم الدولي‬،‫ وأرقام الھواتف‬،‫ والنسب المئوية‬،(‫ والمقاييس )المقادير القياسية‬،(‫ وا سعار )ا موال‬،‫وا وقات‬ ‫ وقد تم إجراء تجارب مكثفة باستخدام ث ث مصنفات مختلفة تُطَبّق تعلم ا لة لتقييم أداء‬.‫ وأسماء الملفات‬،(‫المعياري للكتاب‬ ‫ تُظھر النتائج التجريبية تفوق المنھج الھجين على ك ٍل من المنھج المبني على القواعد والمنھج المبني على تعلم‬.‫النظام الھجين‬ ‫ يتفوق نظامنا الھجين على أفضل ا نظمة المنشورة في الدوريات العلمية في مجال التعرف على أنماط ا سماء العربية من‬.‫ا لة‬ ‫ في حالة أسماء‬%94.4 :‫حيث الدقة عند تطبيق نظامنا على مجموعة البيانات "أنيركورب" بنتيجة معد ت توافقية قدرھا‬ .‫ في حالة أسماء المنظمات‬%88.2 ‫ و‬،‫ في حالة أسماء ا ماكن‬%90.1 ،‫ا شخاص‬  KEYWORDS in Arabic  ‫ تعلم ا لة‬،‫ التعرف على أنماط ا سماء‬،‫معالجة اللغات الطبيعية‬  Proceedings of COLING 2012: Technical Papers, pages 2159–2176, COLING 2012, Mumbai, December 2012. 2159  
 Examples of Attributes Available in or Extracted from Various Sources, for a Sample of Classes Food ingredients: W: energy, dietary ﬁber, solubility in water F: energy per 100g, availability, scientiﬁc name, solubility in water D1: species, pounds, cup, kinds, lbs, bowl D2: quality, part, taste, value, portion Q1: nutritional value, health beneﬁts, glycemic index, varieties, calories QC : gluten free?, safe?, healthy?, vegan?, halal?, fattening?, acidic?, good for skin? Astronomical objects: W: constellation, right ascension, spectral type, rotational velocity, orbital period, mean radius F: category, constellation, age, periapsis, orbital period, mean radius D1: observations, spectrum, planet, spectra, conjunction, transit, temple, surface D2: surface, orbit, bars, history, atmosphere Q1: atmosphere, surface, gravity, diameter, mass, rotation, revolution, moons, radius QC : bigger than earth?, close to the sun?, circumpolar?, capable of supporting life? Religions: W: fundamentals, texts, deities, sacred sites, schools, people F: founding ﬁgures, beliefs, practices, texts, deities, sacred sites D1: teachings, practice, beliefs, religion spread, principles, emergence, doctrines D2: basis, inﬂuence, name, truths, symbols, principles, strength, practice, origin, god, defence Q1: basic beliefs, teachings, holy book, practices, rise, branches, spread, sects QC : monotheistic?, a religion or a way of life?, peaceful?, older than hinduism? Table 1: Examples of attributes already explicitly encoded in human-compiled knowledge resources (W=Wikipedia; F=Freebase) or extracted by various methods from text. Some of the entries in the table are also listed in (Van Durme et al., 2008) (D1=from documents (Pas¸ca et al., 2007), D2=from documents (Van Durme et al., 2008), Q1=from queries (Pas¸ca, 2007), QC =from conjectural queries (this method)) 
In most summarization approaches, sentence ranking plays a vital role. Most previous work explored different features and combined them into uniﬁed ranking methods. However, it would be imprecise to rank sentences from a single point of view because contributions from the features are onefold in these methods. In this paper, a novel supervised aggregation approach for summarization is proposed which combines different summarization methods including LexPageRank, LexHITS, manifold-ranking method and DivRank. Human labeled data are used to train an optimization model which combines these multiple summarizers and then the weights assigned to each individual summarizer are learned. Experiments are conducted on DUC2004 data set and the results demonstrate the effectiveness of the supervised aggregation method compared with typical ensemble approaches. In addition, we also investigate the inﬂuence of training data construction and component diversity on the summarization results. KEYWORDS: Multi-document summarization, supervised aggregation framework. Proceedings of COLING 2012: Technical Papers, pages 2225–2242, COLING 2012, Mumbai, December 2012. 2225  
Name ambiguity is a major problem in information retrieval: The name "Metropolis" may refer to a movie, a physicist, or Superman’s hometown. Recent work resolves ambiguity in natural language text by linking name mentions against the corresponding Wikipedia concept (Wikiﬁcation). Standard methods comparing a single mention with the corresponding Wikipedia concept can potentially be improved by simultaneously considering all mentions in the input document. We propose a novel multiple assignment process based on a collective search over an inverted index that exploits the coherence of Wikipedia concepts. Based on this coherence, we compute the best ﬁtting candidate concept for each mention and combine it with context information in a second search step. Using additional attributes an SVM then re-ranks the result of this search and estimates if a concept is not covered in Wikipedia. We give a uniﬁed view over the different performance measures used in other state-of-the art approaches and evaluate our approach on ﬁve benchmark corpora. On these corpora, our method has the most stable performance yielding similar or better results compared to other approaches. KEYWORDS: Concept and Entity Disambiguation, Wikiﬁcation, Natural Language Processing, Search and Ranking. Proceedings of COLING 2012: Technical Papers, pages 2243–2258, COLING 2012, Mumbai, December 2012. 2243  
We study the perception of situational power in written dialogs in the context of organizational emails and contrast it to the power attributed by organizational hierarchy. We analyze various correlates of the perception of power in the dialog structure and language use by participants in the dialog. We also present an SVM-based machine learning system using dialog structure and lexical features to predict persons with situational power in a given communication thread. Keywords: Computational Sociolinguistics, Social Networks, Power Analysis, Dialog Acts, Dialog. Title and Abstract in German Wer ist (wirklich) der Chef? Situative Macht in schriftlichen Dialogen Wir untersuchen, wie situative Macht in schriftlichen Dialogen wahrgenommen wird. Situative Macht ist Macht, die nicht beständig ist, sondern nur zielbedingt während einer (vielleicht längeren) Interaktion existiert. Unsere Studie beruht auf Geschäftsemails. Wir kontrastieren situative Macht mit der Macht, die durch die organisatorische Hierarchie entsteht. Wir identiﬁzieren verschiedene Korrelate der Wahrnehmung situativer Macht in der Dialogstruktur und im Sprachgebrauch der Dialogteilnehmer. Wir stellen ausserdem einen SVM-basierten maschinellen Lernalgorithmus vor, der Dialogstruktur und Wörter in den Emails benutzt, um Dialogteilnehmer mit situativer Macht zu identiﬁzieren. Keywords in German: Komputationalle Soziolinguistik, Soziale Netzwerke, Macht, Dialogakte, Dialog. Proceedings of COLING 2012: Technical Papers, pages 2259–2274, COLING 2012, Mumbai, December 2012. 2259  
Bilingual lexicon construction (BLC) from comparable corpora is based on the idea that bilingual similar words tend to occur in similar contexts, usually of words. This, however, introduces noise and leads to low performance. This paper proposes a bilingual dependency mapping model for BLC which encodes a word’s context as a combination of its dependent words and their relationships. This combination can provide more reliable clues than mere context words for bilingual translation words. We further demonstrate that this kind of bilingual dependency mappings can be successfully generated and maximally exploited without human intervention. The experiments on BLC from English to Chinese show that, by mapping context words and their dependency relationships simultaneously when calculating the similarity between bilingual words, our approach significantly outperforms a state-of-the-art one by ~14 units in accuracy for frequently occurring noun pairs and similarly, though in a less degree, for nouns and verbs in a wide frequency range. This justifies the effectiveness of our dependency mapping model for BLC. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, CHINESE 应用依存映射从可比较语料库中抽取双语词表 从可比较语料库中抽取双语词表的基本思想是，双语相似的词语出现在相同的语词上下文 中。不过，这种方法引入了噪声，从而导致了低的抽取性能。本文提出了一种用于双语词 表抽取的双语依存映射模型，在该模型中一个词语的上下文结合了依存词语及其依存关 系。这种结合方法为双语词表构建提供了比单一的词语上下文更为可靠的信息。我们还进 一步展示了在没有人工干预的情况下可以产生和利用这种双语依存关系。从英文到中文的 双语词表构建实验表明，通过在计算双语词语相似度时同时映射词语及其依存关系，同目 前性能最好的系统相比，我们的方法显著提高了精度。对于经常出现的名词，精度提高了 14个百分点；对于较大频率范围内的名词和动词，性能也提高了，尽管程度较小。这说明 了依存映射模型对双语词表构建的有效性。 KEYWORDS: Bilingual Lexicon Construction, Comparable Corpora, Dependency Mapping KEYWORDS IN CHINESE: 双语词表构建, 可比较语料库, 依存映射  * Corresponding author  Proceedings of COLING 2012: Technical Papers, pages 2275–2290, COLING 2012, Mumbai, December 2012. 2275  
This paper describes the development of a web-service tool for the automatic extraction of Multi-word expressions lexicons, which has been integrated in a distributed platform for the automatic creation of linguistic resources. The main purpose of the work described is thus to provide a (computationally “light”) tool that produces a full lexical resource: multi-word terms/items with relevant and useful attached information that can be used for more complex processing tasks and applications (e.g. parsing, MT, IE, query expansion, etc.). The output of our tool is a MW lexicon formatted and encoded in XML according to the Lexical Mark-up Framework. The tool is already functional and available as a service. Evaluation experiments show that the tool precision is of about 80%. KEYWORDS: Multiword extraction, lexical resources, LMF, web services. Proceedings of COLING 2012: Technical Papers, pages 2291–2306, COLING 2012, Mumbai, December 2012. 2291  
We address the problem of unsupervised tagging of phrase structure trees with phrase categories (parse tree nonterminals). Motivated by the inability of a range of direct clustering approaches to improve over the current leading algorithm, we propose a mixture of experts approach. In particular, we tackle the difﬁcult challenge of producing a diverse collection of useful tagging experts, which can then be aggregated into a ﬁnal high-quality tagging. To do so, we use the particular properties of the Dirichlet Process mixture model. We evaluate on English, German and Chinese corpora and demonstrate both a substantial and consistent improvement in overall performance over previous work, as well as empirical justiﬁcation of our algorithmic choices. KEYWORDS: Unsupervised parsing, Grammar induction, Non terminals, Dirichlet Process, Ensemble learning. Proceedings of COLING 2012: Technical Papers, pages 2307–2324, COLING 2012, Mumbai, December 2012. 2307  
We report a wide range of comparative experiments establishing for the ﬁrst time contrastive foundations for a completely unsupervised approach to bilingual grammar induction that is cognitively oriented toward early category formation and phrasal chunking in the bootstrapping process up the expressiveness hierarchy from ﬁnite-state to linear to inversion transduction grammars. We show a consistent improvement in terms of cross-entropy throughout the bootstrapping process, as well as promising decoding experiments using the learned grammars. Rather than relying on external resources such as parses, POS tags or dictionaries, our method is fully unsupervised (in the way this term is typically understood in the machine translation community). This means that the bootstrapping can only rely on information gathered during the previous step, which necessitates some strategy for expanding the expressiveness of the grammars. We present principled approaches for moving from ﬁnite-state to linear transduction grammars as well as from linear to inversion transduction grammars. It is our belief that early, integrated category formation and phrasal chunking in this unsupervised bootstrapping process is better aligned to child language acquisition. Finally, we also report exploratory decoding results using some of the learned grammars. This is the ﬁrst step towards an end-to-end grammar-based statistical machine translation system. KEYWORDS: Grammar & Formalisms, Empirical machine translation, Multilinguality and Bilingual grammar induction. Proceedings of COLING 2012: Technical Papers, pages 2325–2340, COLING 2012, Mumbai, December 2012. 2325  Figure1: Bootstrapping paths through the grammar hierarchy along with biparsing time complexities of grammar formalisms. 
Underspeciﬁed queries are common in vertical search engines, leading to large result sets that are difﬁcult for users to navigate. In this paper, we show that we can automatically guide users to their target results by engaging them in a dialog consisting of well-formed binary questions mined from unstructured data. We propose a system that extracts candidate attribute-value question terms from unstructured descriptions of records in a database. These terms are then ﬁltered using a Maximum Entropy classiﬁer to identify those that are suitable for question formation given a user query. We then select question terms via a novel ranking function that aims to minimize the number of question turns necessary for a user to ﬁnd her target result. We evaluate the quality of system-generated questions for grammaticality and reﬁnement effectiveness. Our ﬁnal system shows best results in effectiveness, percentage of well-formed questions, and percentage of answerable questions over three baseline systems. KEYWORDS: Query reﬁnement, question generation, search as a dialog. ∗This work was conducted at Microsoft Research. Proceedings of COLING 2012: Technical Papers, pages 2341–2356, COLING 2012, Mumbai, December 2012. 2341  
Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357  
ABSTRACT Psychological disorders are frequently under-diagnosed and consequently have an irreversible impact on individuals and society. The stigma associated with such disorders makes face-to-face discussions with family members and clinicians difficult for many individuals. In contrast, people openly relate experiences on Internet forums. This paper describes a novel system that analyses forum posts to: (1) detect distress indicators that directly map to the Diagnostic and Statistical Manual of Mental Disorders (DSM) IV constructs, and (2) assess the severity of distress for prioritizing individuals who should seek clinical help (i.e. triage). For distress indicator detection, we use support vector machines (SVMs) trained on a suite of innovative intra- and inter-message features. We show significant improvements in multi-label classification accuracy using humangenerated rationales in support of annotated distress labels. For triage assessment, we demonstrate the effectiveness of Markov Logic Networks (MLNs) in dealing with noisy distress label detections and encoding expert rules. KEYWORDS: Psychological Distress, Web forums, Text classification, Annotator rationales, Support Vector Machines, Probabilistic Logic, Markov Logic Networks. Proceedings of COLING 2012: Technical Papers, pages 2375–2388, COLING 2012, Mumbai, December 2012. 2375  
There is often more than one way to represent syntactic structures, even within a given formalism. Selecting one representation over another may affect parsing performance. Therefore, selecting between alternative syntactic representations (henceforth, syntactic selection) is an essential step in designing an annotation scheme. We present a methodology for syntactic selection and apply it to six central dependency structures. Our methodology compares pairs of annotation schemes that differ in the annotation of a single structure. It selects the more learnable scheme, namely the one that can be better learned using statistical parsers. We ﬁnd that in three of the structures, one annotation is unequivocally better than the alternatives. Our results are consistent over various settings involving ﬁve parsers and two deﬁnitions of learnability. Furthermore, we show that the learnability gains incurred by our selections are both considerable (error reductions of up to 19.8%) and additive. The contribution of this work is in demonstrating that syntactic selection has a substantial and predictable effect on parsing performance, and showing that this effect can be effectively used in designing syntactic annotation schemes. Keywords: Syntactic annotation design, Learnability, Parsing. Proceedings of COLING 2012: Technical Papers, pages 2405–2422, COLING 2012, Mumbai, December 2012. 2405  
This paper introduces a method to improve supervised word sense disambiguation performance by including a new class of features which leverage contextual information from large unannotated corpora. This new feature class, selectors, contains words that appear in other corpora with the same local context as a given lexical instance. We show that support vector sense classiﬁers trained with selectors achieve higher accuracy than those trained only with standard features, producing error reductions of 15.4% and 6.9% on standard coarse-grained and ﬁne-grained disambiguation tasks respectively. Furthermore, we ﬁnd an error reduction of 9.3% when including selectors for the classiﬁcation step of named-entity recognition over a representative sample of OntoNotes. These signiﬁcant improvements come free of any human annotation cost, only requiring unlabeled Web-Scale corpora. KEYWORDS: word sense disambiguation, lexical semantics, semi-supervised learning. Proceedings of COLING 2012: Technical Papers, pages 2423–2440, COLING 2012, Mumbai, December 2012. 2423  
In recent years, statistical parsers have reached high performance levels on well-edited texts. Domain adaptation techniques have improved parsing results on text genres differing from the journalistic data most parsers are trained on. However, such corpora usually comply with standard linguistic, spelling and typographic conventions. In the meantime, the emergence of Web 2.0 communication media has caused the apparition of new types of online textual data. Although valuable, e.g., in terms of data mining and sentiment analysis, such user-generated content rarely complies with standard conventions: they are noisy. This prevents most NLP tools, especially treebank based parsers, from performing well on such data. For this reason, we have developed the French Social Media Bank, the ﬁrst user-generated content treebank for French, a morphologically rich language (MRL). The ﬁrst release of this resource contains 1,700 sentences from various Web 2.0 sources, including data speciﬁcally chosen for their high noisiness. We describe here how we created this treebank and expose the methodology we used for fully annotating it. We also provide baseline POS tagging and statistical constituency parsing results, which are lower by far than usual results on edited texts. This highlights the high difﬁculty of automatically processing such noisy data in a MRL. KEYWORDS: Treebanking, User Generated Content, Parsing, Social Media. Proceedings of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441  
This paper reports the highest results (95% in MUC and 92% in CoNLL metric) in the literature for Turkish named entity recognition; more speciﬁcally for the task of detecting person, location and organization entities in general news texts. We give an in depth analysis of the previous reported results and make comparisons with them whenever possible. We use conditional random ﬁelds (CRFs) as our statistical model. The paper presents initial explorations on the usage of rich morphological structure of the Turkish language as features to CRFs together with the use of some basic and generative gazetteers. KEYWORDS: Named Entity Recognition , Turkish , Conditional Random Fields , ENAMEX. Proceedings of COLING 2012: Technical Papers, pages 2459–2474, COLING 2012, Mumbai, December 2012. 2459  
In this paper, we propose a differential evolution (DE) based two-stage evolutionary approach for named entity recognition (NER). The ﬁrst stage concerns with the problem of relevant feature selection for NER within the frameworks of two popular machine learning algorithms, namely Conditional Random Field (CRF) and Support Vector Machine (SVM). The solutions of the ﬁnal best population provides different diverse set of classiﬁers; some are effective with respect to recall whereas some are effective with respect to precision. In the second stage we propose a novel technique for classiﬁer ensemble for combining these classiﬁers. The approach is very general and can be applied for any classiﬁcation problem. Currently we evaluate the proposed algorithm for NER in three popular Indian languages, namely Bengali, Hindi and Telugu. In order to maintain the domain-independence property the features are selected and developed mostly without using any deep domain knowledge and/or language dependent resources. Experimental results show that the proposed two stage technique attains the ﬁnal F-measure values of 88.89%, 88.09% and 76.63% for Bengali, Hindi and Telugu, respectively. The key contributions of this work are two-fold, viz. (i). proposal of differential evolution (DE) based feature selection and classiﬁer ensemble methods that can be applied to any classiﬁcation problem; and (ii). scope of the development of language independent NER systems in a resource-poor scenario. KEYWORDS: Named Entity Recognition, Differential Evolution, Feature Selection, Classiﬁer Ensemble. Authors equaly contributed for this paper. * corresponding authors Proceedings of COLING 2012: Technical Papers, pages 2475–2490, COLING 2012, Mumbai, December 2012. 2475  
KEYWORDS: coreference resolution, discourse processing, supervised clustering, greedy ap- proaches. KEYWORDS IN L2: resolución de correferencia. Proceedings of COLING 2012: Technical Papers, pages 2519–2534, COLING 2012, Mumbai, December 2012. 2519  
In this article, we present a novel approach towards the detection and modeling of complex social phenomena in multi-party discourse, including leadership, influence, pursuit of power and group cohesion. We have developed a two-tier approach that relies on observable and computable linguistic features of conversational text to make predictions about sociolinguistic behaviors such as Topic Control and Disagreement, that speakers deploy in order to achieve and maintain certain positions and roles in a group. These sociolinguistic behaviors are then used to infer higher-level social phenomena such as Leadership and Influence, which is the focus of this paper. We show robust performance results by comparing our automatically computed results to participants’ own perceptions and rankings. We use weights learnt from correlations with training examples known leadership and influence rankings of participants to optimize our models and to show performance significantly above baseline for two different languages – English and Mandarin Chinese. KEYWORDS : computational sociolinguistics, online dialogues, social phenomena, linguistic behavior, influence, multi-disciplinary artificial intelligence, social computing 
In this paper we present work on the task of Native Language Identiﬁcation (NLI). We present an alternative corpus to the ICLE which has been used in most work up until now. We believe that our corpus, TOEFL11, is more suitable for the task of NLI and will allow researchers to better compare systems and results. We show that many of the features that have been commonly used in this task generalize to new and larger corpora. In addition, we examine possible ways of increasing current system performance (e.g., additional features and feature combination methods), and achieve overall state-of-the-art results (accuracy of 90.1%) on the ICLE corpus using an ensemble classiﬁer that includes previously examined features and a novel feature (n-gram language models). We also show that training on a large corpus and testing on a smaller one works well, but not vice versa. Finally, we show that system performance varies across proﬁciency scores. KEYWORDS: Native Language Identiﬁcation, Text Classiﬁcation, Corpora.  
The advent and recent popularity of location-enabled social media services like Twitter and Foursquare has brought a dataset of immense value to researchers in several domains ranging from theory validation in computational sociology, over market analysis, to situation awareness in disaster management. Many of these applications, however, require evaluating the a priori relevance of trends, topics and terms in given regions of interest. Inspired by the well-known notion of the tf-idf weight combined with kernel density methods we present a smooth measure that utilizes large corpora of social media data to facilitate scalable, real-time and highly interactive analysis of geolocated text. We describe the implementation speciﬁcs of our measure, which are grounded in aggregation and preprocessing strategies, and we demonstrate its practical usefulness with two case studies within a sophisticated visual analysis system. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, L2 (OPTIONAL, AND ON SAME PAGE) Inverse Dokumentdichte: Ein glattes Maß für ortsbezogene Termnutzungsunregelmäßigkeiten Das Aufkommen und die derzeitige Beliebtheit von ortsbezogenen Diensten der Sozialen Medien wie Twitter und Foursquare haben einen Datensatz von immensem Wert für Forscher verschiedener Domänen, von der Aussagenvalidierung in der Soziologie, über Marktanalysen, bis zur Situationseinschätzung im Katastrophenschutz, geschaffen. Viele dieser Anwendungen erfordern jedoch eine Einschätzung der a priori Relevanz von Trends, Themen und Termen für gegebene geographische Regionen. Basierend auf der Idee hinter dem bekannten Tf-idf-Maß, präsentieren wir eine geglättetes Maß, welches, durch die Ausnutzung großer Korpora bestehend aus Daten der Sozialen Medien, die skalierbare, echtzeitfähige und voll interaktive Analyse von geokodierten Texten ermöglicht. Wir beschreiben die Details der Umsetzung unseres Maßes, welche auf Aggregations- und Vorverarbeitungsstrategien beruht, und wir zeigen seine praktische Anwendbarkeit durch zwei Fallbeispiele mit Hilfe eines elaborierten Systems zur visuellen Analyse. KEYWORDS: tf-idf, term density, geolocated corpora, Visual Analytics, Twitter, social media. KEYWORDS IN L2: TD-IDF, Termdichte, geokodierte Korpora, Visual Analytics, Twitter, Soziale Medien. Proceedings of COLING 2012: Technical Papers, pages 2603–2618, COLING 2012, Mumbai, December 2012. 2603  Figure 1: Left: The ScatterBlogs system for visual social media analysis. Interactive exploration techniques are used to examine aggregated message contents. Middle: content lens — a circle which can be interactively moved over the map to show the most frequent terms within a region and timeframe. Right: Cluster analysis is used to detect spatiotemporal clusters of similar topic usage to display them on a map. 
In this paper, we revisit the problem of language identiﬁcation with the focus on proper discrimination between closely related languages. Strong similarities between certain languages make it very hard to classify them correctly using standard methods that have been proposed in the literature. Dedicated models that focus on speciﬁc discrimination tasks help to improve the accuracy of general-purpose language identiﬁcation tools. We propose and compare methods based on simple document classiﬁcation techniques trained on parallel corpora of closely related languages and methods that emphasize discriminating features in terms of blacklisted words. Our experiments demonstrate that these techniques are highly accurate for the difﬁcult task of discriminating between Bosnian, Croatian and Serbian. The best setup yields an absolute improvement of over 9% in accuracy over the best performing baseline using a state-of-the-art language identiﬁcation tool. KEYWORDS: language identiﬁcation, language discrimination, closely related languages. Proceedings of COLING 2012: Technical Papers, pages 2619–2634, COLING 2012, Mumbai, December 2012. 2619  
Reducing the reliance of semantic role labeling (SRL) methods on human-annotated data has become an active area of research. However, the prior work has largely focused on either (1) looking into ways to improve supervised SRL systems by producing surrogate annotated data and reducing sparsity of lexical features or (2) considering completely unsupervised semantic role induction settings. In this work, we aim to link these two veins of research by studying how unsupervised techniques can be improved by exploiting small amounts of labeled data. We extend a state-of-the-art Bayesian model for unsupervised semantic role induction to better accommodate for annotated sentences. Our semi-supervised method outperforms a strong supervised baseline when only a small amount of labeled data is available. KEYWORDS: semantic role labeling, semi-supervised learning, shallow semantics, Bayesian model. Proceedings of COLING 2012: Technical Papers, pages 2635–2652, COLING 2012, Mumbai, December 2012. 2635  
The annotations of explicit and implicit discourse connectives in the Penn Discourse Treebank make it possible to investigate on a large scale how diﬀerent types of discourse relations are expressed. Assuming an account of the Uniform Information Density hypothesis, we expect that discourse relations should be expressed explicitly with a discourse connector when they are unexpected, but may be implicit when the discourse relation can be anticipated. We investigate whether discourse relations which have been argued to be expected by the comprehender exhibit a higher ratio of implicit connectors. We ﬁnd support for two hypotheses put forth in previous research which suggest that continuous and causal relations are presupposed by language users when processing consecutive sentences in a text. We then proceed to analyze the eﬀect of Implicit Causality (IC) verbs (which have been argued to raise an expectation for an explanation) as a local cue for an upcoming causal relation. Keywords: Causality, Continuity, Discourse relations, Discourse cues, Implicit discourse relations, Corpus study, Uniform Information Density. Proceedings of COLING 2012: Technical Papers, pages 2669–2684, COLING 2012, Mumbai, December 2012. 2669  
Cross-language information retrieval today is dominated by techniques that rely principally on context-independent token-to-token mappings despite the fact that state-of-the-art statistical machine translation systems now have far richer translation models available in their internal representations. This paper explores combination-of-evidence techniques using three types of statistical translation models: context-independent token translation, token translation using phrase-dependent contexts, and token translation using sentence-dependent contexts. Context-independent translation is performed using statistically-aligned tokens in parallel text, phrase-dependent translation is performed using aligned statistical phrases, and sentence-dependent translation is performed using those same aligned phrases together with an n-gram language model. Experiments on retrieval of Arabic, Chinese, and French documents using English queries show that no one technique is optimal for all queries, but that statistically signiﬁcant improvements in mean average precision over strong baselines can be achieved by combining translation evidence from all three techniques. The optimal combination is, however, found to be resource-dependent, indicating a need for future work on robust tuning to the characteristics of individual collections. KEYWORDS: cross-language information retrieval, machine translation, context. Proceedings of COLING 2012: Technical Papers, pages 2685–2702, COLING 2012, Mumbai, December 2012. 2685  
Online discussion forums are a valuable means for users to resolve speciﬁc information needs, both interactively for the participants and statically for users who search/browse over historical thread data. However, the complex structure of forum threads can make it difﬁcult for users to extract relevant information. Automatically identifying whether the problem in a thread has been solved or not can help direct users to threads where the original problem has been solved, hence enhancing their prospects of solving their particular problem. In this paper, we investigate the task of Solvedness classiﬁcation by exploiting the discourse structure of forum threads. Experimental results show that simple features derived from thread discourse structure can greatly boost the accuracy of Solvedness classiﬁcation, which has been shown to be very difﬁcult in previous research. KEYWORDS: Discourse Structure, Web User Forum, Social Media, Dialogue Act. Proceedings of COLING 2012: Technical Papers, pages 2739–2756, COLING 2012, Mumbai, December 2012. 2739  
Implicit discourse relation recognition is a challenging task in the natural language processing field, but important to many applications such as qu estion answering, summarizat ion and so on. Previous research used either art ificially created imp licit discourse relat ions with connectives removed fro m exp licit relations or annotated implicit relat ions as training data to detect the possible implicit relations, and do not further discern which examples are fit to be training data. This paper is the first time to apply a d ifferent typical/atypical perspective to select the most suitable discourse relation examp les as training data. To differentiate typical and atypical examples for each discourse relation, a novel single centroid clustering algorithm is proposed. With this typical/atypical distinction, we aim to recognize those easily identified discourse relations more p recisely so as to promote the performance of the implicit relation recognition. The experimental results verify that the proposed new method outperforms the state -of-the-art methods. KEYWORDS : Discourse relation recognition, single centroid clustering, implicit discourse relation.   Corresponding author.  Proceedings of COLING 2012: Technical Papers, pages 2757–2772, COLING 2012, Mumbai, December 2012. 2757  
Together with the ever-growing amount of Chinese web data, the number of opinions voiced by Chinese users is rapidly increasing, and analyzing them is an important task. This paper introduces a Chinese Evaluative Information Analyzer (CEIA) and proposes a method to improve its performance. We use evaluative information as a unifying term for the information about attitudes, opinions, sentiments and so on. This paper makes three contributions: (i) CEIA can identify and analyze a more diverse and richer set of evaluative information than previous studies for Chinese; (ii) to implement the system, we constructed an original annotated corpus for Chinese evaluative information and built a large sentiment dictionary; (iii) we introduce syntactic dependency, semantic class and distance features to improve the evaluative information extraction. The performance of the system and the effectiveness of the newly introduced features are evaluated in a series of experiments on our Chinese evaluative information corpus. Title and Abstract in Chinese  i ii iii  CEIA CEIA  CEIA  Keywords: Chinese Evaluative Information, Opinion Mining, Sentiment Analysis.  Keywords in Chinese:  .  Proceedings of COLING 2012: Technical Papers, pages 2773–2788, COLING 2012, Mumbai, December 2012. 2773  
We describe a domain-speciﬁc method of adapting conditional random ﬁelds (CRFs) to morphosyntactic tagging of highly-inﬂectional languages. The solution involves extending CRFs with additional, position-wise restrictions on the output domain, which are used to impose consistency between the modeled label sequences and morphosyntactic analysis results both at the level of decoding and, more importantly, in parameters estimation process. We decompose the problem of morphosyntactic disambiguation into two consecutive stages of the context-sensitive morphosyntactic guessing and the disambiguation proper. The division helps in designing well-adjusted, CRF-based methods for both tasks, which in combination constitute Concraft, a highly accurate tagging system for the Polish language available under the 2-clause BSD license. Evaluation on the National Corpus of Polish shows that our solution signiﬁcantly outperforms other state-of-the-art taggers for Polish – Pantera, WMBT and WCRFT – especially in terms of the accuracy measured with respect to unknown words. KEYWORDS: morphosyntactic tagging, conditional random ﬁelds, Polish. Proceedings of COLING 2012: Technical Papers, pages 2789–2804, COLING 2012, Mumbai, December 2012. 2789  
Recognizing semantic relations between sentences, such as entailment and contradiction, is a challenging task that requires detailed analysis of the interaction between diverse linguistic phenomena. In this paper, we propose a latent discriminative model that uniﬁes a statistical framework and a theory of Natural Logic to capture complex interactions between linguistic phenomena. The proposed approach jointly models alignments, their local semantic relations, and a sentence-level semantic relation, and has hidden variables including alignment edits between sentences and their semantic relations, only requires sentences pairs annotated with sentence-level semantic relations as training data to learn appropriate alignments. In evaluation on a dataset including diverse linguistic phenomena, our proposed method achieved a competitive results on alignment prediction, and signiﬁcant improvements on a sentence-level semantic relation recognition task compared to an alignment supervised model. Our analysis did not provide evidence that directly learning alignments and their labels using gold standard alignments contributed to semantic relation recognition performance and instead suggests that they can be detrimental to performance if used in a manner that prevents the learning of globally optimal alignments. KEYWORDS: Recognizing Textual Entailment, Natural Logic, Latent Variable Model. KEYWORDS IN L2: . Proceedings of COLING 2012: Technical Papers, pages 2805–2820, COLING 2012, Mumbai, December 2012. 2805  
One of the biggest bottlenecks for conversational systems is large-scale provision of suitable content. In this paper, we present the use of content mined from online question-and-answer forums to automatically construct system utterances. Although this content is mined in the form of question-answer pairs, our system is able to use it to formulate utterances that drive a conversation, not just for answering user questions as has been done in previous work. We use a collection of strategies that specify how and when the question-answer pairs can be used and augmented with a small number of generic hand-crafted text snippets to generate natural and coherent system utterances. Our experiments involving 11 human participants demonstrated that this approach can indeed produce relatively natural and coherent interaction. KEYWORDS: conversational system; question-answer pairs; conversational strategies. 
Among various neural network language models (NNLMs), recurrent neural network-based language models (RNNLMs) are very competitive in many cases. Most current RNNLMs only use one single feature stream, i.e., surface words. However, previous studies proved that language models with additional linguistic information achieve better performance. In this study, we extend RNNLM by explicitly integrating additional linguistic information, including morphological, syntactic, or semantic factors. Our proposed RNNLM is called a factored RNNLM that is expected to enhance RNNLMs. A number of experiments are carried out that show the factored RNNLM improves the performance for all considered tasks: consistent perplexity and word error rate (WER) reductions. In the Penn Treebank corpus, the relative improvements over n-gram LM and RNNLM are 29.0% and 13.0%, respectively. In the IWSLT-2011 TED ASR test set, absolute WER reductions over RNNLM and n-gram LM reach 0.63 and 0.73 points. Title and Abstract in another language (Chinese)  NNLM  RNNLM  RNNLM  fRNNLM  RNNLM  fRNNLM  IWSLT-2011 TED  n  0.73  RNNLM n fRNNLM RNNLM  RNNLM  fRNNLM WER 29.0% 13.0% 0.63  Keywords: Factored Language Model, Recurrent Neural Network, Large Vocabulary Continuous Speech Recognition.  Keywords in Chinese:  ,  ,  .  Proceedings of COLING 2012: Technical Papers, pages 2835–2850, COLING 2012, Mumbai, December 2012. 2835  
Subjectivity analysis has received increasing attention in natural language processing ﬁeld. Most of the subjectivity analysis works however are conducted on single languages. In this paper, we propose to perform multilingual subjectivity analysis by combining multi-view learning and AdaBoost techniques. We aim to show that by boosting multi-view classiﬁers we can develop more effective multilingual subjectivity analysis tools for new languages as well as increase the classiﬁcation performance for English data. We empirically evaluate our two multi-view AdaBoost approaches on the multilingual MPQA dataset. The experimental results show the multi-view AdaBoost approaches signiﬁcantly outperform existing monolingual and multilingual methods. KEYWORDS: Multi-view learning, AdaBoost, Multilingual subjectivity analysis. Proceedings of COLING 2012: Technical Papers, pages 2851–2866, COLING 2012, Mumbai, December 2012. 2851  
Recently, various unsupervised representation learning approaches have been investigated to produce augmenting features for natural language processing systems in the open-domain learning scenarios. In this paper, we propose a dynamic dependency network model to conduct semi-supervised representation learning. It exploits existing task-speciﬁc labels in the source domain in addition to the large amount of unlabeled data from both the source and target domains to produce informative features for NLP tasks. We empirically evaluate the proposed learning technique on the part-of-speech tagging task using Wall Street Journal and MEDLINE sentences and on the syntactic chunking task using Wall Street Journal corpus and Brown corpus. Our experimental results show that the proposed semi-supervised learning model can produce more effective features than unsupervised representation learning methods for opendomain part-of-speech taggers and syntactic chunkers. KEYWORDS: Domain Adaptation, Representation Learning, POS Tagging, Syntactic Chunking. Proceedings of COLING 2012: Technical Papers, pages 2867–2882, COLING 2012, Mumbai, December 2012. 2867  
We present a global log-linear model for synchronous grammar induction, which is capable of incorporating arbitrary features. The parameters in the model are trained in an unsupervised fashion from parallel sentences without word alignments. To make parameter training tractable, we also propose a novel and efﬁcient cube pruning based synchronous parsing algorithm. Using learned synchronous grammar rules with millions of features that contain rule level, word level and translation boundary information, we signiﬁcantly outperform a competitive hierarchical phrased-based baseline system by +1.4 BLEU on average on three NIST test sets. KEYWORDS: synchronous grammar induction, discriminative model, unsupervised learning, machine translation. Proceedings of COLING 2012: Technical Papers, pages 2883–2898, COLING 2012, Mumbai, December 2012. 2883  
We present initial investigation into the task of paraphrasing language while targeting a particular writing style. The plays of William Shakespeare and their modern translations are used as a testbed for evaluating paraphrase systems targeting a speciﬁc style of writing. We show that even with a relatively small amount of parallel training data, it is possible to learn paraphrase models which capture stylistic phenomena, and these models outperform baselines based on dictionaries and out-of-domain parallel text. In addition we present an initial investigation into automatic evaluation metrics for paraphrasing writing style. To the best of our knowledge this is the ﬁrst work to investigate the task of paraphrasing text with the goal of targeting a speciﬁc style of writing. KEYWORDS: Paraphrase, Writing Style. Proceedings of COLING 2012: Technical Papers, pages 2899–2914, COLING 2012, Mumbai, December 2012. 2899  
Automatic error correction systems for English as a Second Language(ESL) speakers often rely on the use of a confusion set to limit the choices of possible correction candidates. Typically, the confusion sets are either manually constructed or extracted from a corpus of manually corrected ESL writings. Both options require the involvement of English teachers. This paper proposes a method to automatically construct confusion sets for commonly used prepositions from non-ESL corpus without manual intervention. The proposed method simulates how ESL learners learn both the intensions and extensions of English words from standard English text. Our experimental results suggest that the automatically constructed confusion sets based on the similarities between the learned words’ intensions is competitive with those directly learned from an ESL corpus containing about 150K preposition usages. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, L2 (OPTIONAL, AND ON SAME PAGE) 通过分析单词的内涵和外延来对用词混淆建模 针对把英语作为第二语言的人群的自动语法纠错系统，通常会需要使用“混淆集” 来限制系统纠错的种类。一般来说，这些混淆集或者是由专家总结经验得出，或者是从被 专家纠错过的文字当中提取的。这两种方法都需要英语专家的介入。在这篇论文当中，我 们提出了一种无需专家介入，自动建立常用介词混淆集的方法。在此方法中，我们对英语 单词的内涵和外延建模，并且模拟了英语学习者们学习单词内涵和外延的过程。实验表 明，使用单词内涵之间的相似度来创建的混淆集，与从含15万介词的标注语料当中提取的 混淆集质量是相当的。 KEYWORDS: Confusion Sets, Lexical Semantics, Grammatical Error Correction, Distance Metric Learning, English as Second Language, Second Language Acquisition. KEYWORDS IN L2: 选词混淆、混淆集、词汇语义学、语法纠错、距离度量学习、英语作 为第二语言、第二语言学习. Proceedings of COLING 2012: Technical Papers, pages 2915–2930, COLING 2012, Mumbai, December 2012. 2915  
KEYWORDS : Measuring similarity, Semantic relation, Recommendation system, Graph structure, Random walk Proceedings of COLING 2012: Technical Papers, pages 2945–2960, COLING 2012, Mumbai, December 2012. 2945  the number of selected programs  
Most existing learning to rank based summarization methods only used content relevance of sentences with respect to queries to rank or estimate sentences, while neglecting sentence relationships. In our work, we propose a novel model, RelationListwise, by integrating relation information among all the estimated sentences into listMLE-Top K, a basic listwise learning to rank model, to improve the quality of top-ranked sentences. In addition, we present some unique sentence features as well as a novel measure of sentence semantic relation, aiming to enhance the performance of training model. Experimental results on DUC2005-2007 standard summarization data sets demonstrate the effectiveness of our proposed method. KEYWORDS: Query-focused multi-document summarization, Listwise, Sentence relation. Proceedings of COLING 2012: Technical Papers, pages 2961–2976, COLING 2012, Mumbai, December 2012. 2961  
Extractive multi-document summarization is mostly treated as a sentence ranking problem. Existing graph-based ranking methods for key-sentence extraction usually attempt to compute a global importance score for each sentence under a single relation. Motivated by the fact that both documents and sentences can be presented by a mixture of semantic topics detected by Latent Dirichlet Allocation (LDA), we propose SentTopic-MultiRank, a novel ranking model for multi-document summarization. It assumes various topics to be heterogeneous relations, then treats sentence connections in multiple topics as a heterogeneous network, where sentences and topics/relations are effectively linked together. Next, the iterative algorithm of MultiRank is carried out to determine the importance of sentences and topics simultaneously. Experimental results demonstrate the effectiveness of our model in promoting the performance of both generic and query-biased multi-document summarization tasks. KEYWORDS: Multi-document summarization, Topic decomposition, Heterogeneous network. Proceedings of COLING 2012: Technical Papers, pages 2977–2992, COLING 2012, Mumbai, December 2012. 2977  
We present a novel scheme of language modeling for a spoken dialogue system by effectively ﬁltering query sentences collected via a Web site of wisdom of crowds. Our goal is a speechbased information navigation system by retrieving from backend documents such as Web news. Then, we expect that users make queries that are relevant to the backend documents. The relevance measure can be deﬁned with cross-entropy or perplexity by the language model generated from the documents in a conventional manner. In this article, we propose a novel criteria that considers semantic-level information. It is based on predicate-argument (P-A) pairs and their relevance to the documents (or topic) is deﬁned by a naive Bayes score. Experimental evaluations demonstrate that the proposed relevance measure effectively selects relevant sentences used for a language model, resulting in signiﬁcant reduction of the word error rate of speech recognition as well as the semantic-level error rate. KEYWORDS: Language Modeling, Predicate Argument Structure, Spoken Dialogue System. Proceedings of COLING 2012: Technical Papers, pages 2993–3002, COLING 2012, Mumbai, December 2012. 2993  
Automatic detection of sentence errors is an important NLP task and is valuable to assist foreign language learners. In this paper, we investigate the problem of word ordering errors in Chinese sentences and propose classifiers to detect this type of errors. Word n-gram features in Google Chinese Web 5-gram corpus and ClueWeb09 corpus, and POS features in the Chinese POStagged ClueWeb09 corpus are adopted in the classifiers. The experimental results show that integrating syntactic features, web corpus features and perturbation features are useful for word ordering error detection, and the proposed classifier achieves 71.64% accuracy in the experimental datasets. 協助非中文母語學習者偵測中文句子語序錯誤 自這的在徵所動篇分、得中類的偵論及文器分測文擾詞中類句中動彙我器子特，n-們效錯徵我gr使能誤對們am用可是偵研s及的達自測究中特然中中71文徵語文文.6詞包4言語句%性括處序子。標：理錯語注G研誤序o特究有錯og徵幫誤一le。助的項中實問重。文驗題要在網結議，實路果題並驗5顯-提所，gr示出用對am，分的於語整類資協料合器料助庫語來集外、法偵中語與特測學，徵C這習合lu、種者併eW網類很使e路型有用b0語的價這9 料錯值些語庫誤特。料特徵在。庫  KEYWORDS : ClueWeb09, computer-aided language learning, HSK corpus, word ordering error  電腦輔助語言學習 語料庫 語序錯誤偵測 detection KEYWORDS IN L2 : ClueWeb09,  , HSK  ,  Proceedings of COLING 2012: Technical Papers, pages 3003–3018, COLING 2012, Mumbai, December 2012. 3003  
Abbreviation is a common linguistic phenomenon with wide popularity and high rate of growth. Correctly linking full forms to their abbreviations will be helpful in many applications. For example, it can improve the recall of information retrieval systems. An intuition to solve this is to build an abbreviation dictionary in advance. This paper investigates an automatic abbreviation generation method, which uses a stacked approach for Chinese abbreviation generation. We tackle this problem in two stages. First we use a sequence labeling method to generate a list of candidate abbreviations. Then, we try to use search engine to incorporate web data to re-rank the candidates, and ﬁnally get the best candidate. We use a Chinese abbreviation corpus which contains 8015 abbreviation pairs to evaluate the performance. Experiments revealed that our method gave better performance than the baseline methods. KEYWORDS: Chinese Abbreviation Generation, Abbreviation Mining.  ∗Corresponding author  Proceedings of COLING 2012: Technical Papers, pages 3055–3070, COLING 2012, Mumbai, December 2012. 3055  
Previous joint models of Chinese part-of-speech (POS) tagging and dependency parsing are extended from either graph- or transition-based dependency models. Our analysis shows that the two models have different error distributions. In addition, integration of graph- and transition-based dependency parsers by stacked learning (stacking) has achieved signiﬁcant improvements. These motivate us to study the problem of stacking graph- and transition-based joint models. We conduct experiments on Chinese Penn Treebank 5.1 (CTB5.1). The results demonstrate that the guided transition-based joint model obtains better performance than the guided graph-based joint model. Further, we introduce a constituent-based joint model which derives the POS tag sequence and dependency tree from the output of PCFG parsers, and then integrate it into the guided transition-based joint model. Finally, we achieve the best performance on CTB5.1, 94.95% in tagging accuracy and 83.98% in parsing accuracy respectively. TITLE AND ABSTRACT IN CHINESE 采用堆方法融合异种的中文词性和依存句法联合模型 过去的中文词性和依存句法联合模型基本上都根据基于图的依存句法分析模型或者 基于转移的依存句法分析模型进行拓展而形成的。我们的分析结果表明这两种不同的模型 错误分布并不一样，而且在依存句法中，将基于图的模型和基于转移的模型使用堆方法融 合之后，能够显著的提升依存句法的性能，这些促使我们进一步研究采用堆方法去融合基 于图的和基于转移的词性依存句法联合模型。我们在中文宾州树库5.1版本（CTB5.1）上 进行试验，实验结果表明，相比使用基于图的联合模型为被指导模型，采用转移的联合模 型为被指导模型能取得较好的性能。更进一步，我们介绍了基于短语句法结构的联合模 型，它从一个句子的概率短语文法分析器输出结果中提取句子的词性序列以及依存树结 果，然后我们采用基于短语句法结构的联合模型更进一步指导基于转移的联合模型，最终 我们在CTB5.1的数据上取得了最好结果，词性标注准确率达到94.95%，同时，依存句法 准确率达到83.98%。 KEYWORDS: Chinese POS Tagging, Dependency Parsing, Joint Model, Stacked Learning. KEYWORDS IN CHINESE: 中文词性标注,依存分析,联合模型,堆方法融合学习.  ∗Corresponding author  Proceedings of COLING 2012: Technical Papers, pages 3071–3088, COLING 2012, Mumbai, December 2012. 3071  
Entity linking disambiguates a mention of an entity in text to a Knowledge Base (KB). Most previous studies disambiguate a mention of a name (e.g.“AZ”) based on the distribution knowledge learned from labeled instances, which are related to other names (e.g.“Hoffman”,“Chad Johnson”, etc.). The gaps among the distributions of the instances related to different names hinder the further improvement of the previous approaches. This paper proposes a lazy learning model, which allows us to improve the learning process with the distribution information speciﬁc to the queried name (e.g.“AZ”). To obtain this distribution information, we automatically label some relevant instances for the queried name leveraging its unambiguous synonyms. Besides, another advantage is that our approach still can beneﬁt from the labeled data related to other names (e.g.“Hoffman”,“Chad Johnson”, etc.), because our model is trained on both the labeled data sets of queried and other names by mining their shared predictive structure. Keywords: Entity Linking, Lazy Learning, Query-Speciﬁc Information. Proceedings of COLING 2012: Technical Papers, pages 3089–3104, COLING 2012, Mumbai, December 2012. 3089  
With the emergence of community-based question answering (cQA) services, question retrieval has become an integral part of information and knowledge acquisition. Though existing information retrieval (IR) technologies have been found to be successful for document retrieval, they are less effective for question retrieval due to the inherent characteristics of questions, which have shorter texts. One of the major common drawbacks for the term weightingbased question retrieval models is that they overlook the relations between term pairs when computing their weights. To tackle this problem, we propose a novel term weighting scheme by incorporating the dependency relation cues between term pairs. Given a question, we ﬁrst construct a dependency graph and compute the relation strength between each term pairs. Next, based on the dependency relation scores, we reﬁne the initial term weights estimated by conventional term weighting approaches. We demonstrate that the proposed term weighting scheme can be seamlessly integrated with popular question retrieval models. Comprehensive experiments well validate our proposed scheme and show that it achieves promising performance as compared to the state-of-the-art methods. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (CHINESE) |^••é{'XãU?¯éu¢¥ c‘D ‘X «.¯‰ÑÖ Ñy§¯éu¢¤• &E9•£¼ -‡å»"¦+® k &Eu¢ .3© u¢•¡ ¤õ§ ´du¯éu¢ á© A5§¦ ®k u¢ .éJ·^u¯éu¢" éu®k Äuc‘D ¯éu¢ . ó§˜‡Ì‡ ¯K´3OŽc‘ -ž Ñ c‘ƒm 'X"• )ûù‡¯K§·‚JÑ ˜« # |^c‘m ••é{'XŠ•‚¢ c‘D Å›"éu‰½¯é§·‚Äk ï ••'Xã5OŽz‡c‘é 'érÝ§? ·‚Šâ••'éÝ5•#~5 c‘ -"·‚ y ¤JÑ c‘D Å›U k / Ü yk ¯éu¢ .¥§…¢ (Jƒ'u c•`¯éu¢ .k ŒJ," KEYWORDS: cQA, Question Retrieval, Dependency Relations, Term Weighting. KEYWORDS IN CHINESE: «.¯‰§¯éu¢§••é{'X§c‘D . ∗ This work was done when the ﬁrst author was an intern in National University of Singapore. † Corresponding author Proceedings of COLING 2012: Technical Papers, pages 3105–3120, COLING 2012, Mumbai, December 2012. 3105  
In previous work on unsupervised learning of morphology, the long-tail pattern in the rank-frequency distribution of words, as well as of morphological units, is usually considered as following Zipf’s law (power-law). We argue that these long-tail distributions can also be considered as lognormal. Since we know the conjugate prior distribution for a lognormal likelihood, we propose to generate morphology data from lognormal distributions. When the performance is evaluated by a tokenbased criterion, giving more weights to the results of frequent words, the proposed model preforms signiﬁcantly better than other models in discussion. Moreover, we capture the statistical properties of morphological units with a Bayesian approach, other than a rule-based approach as studied in (Chan, 2008) and (Zhao and Marcus, 2011). Given the multiplicative property of lognormal distributions, we can directly capture the long-tail distribution of word frequency, without the need of an additional generative process as studied in (Goldwater et al., 2006). Keywords: Morphological Learning, Zipf’s law, Lognormal distribution, Long tail distribution, Gibbs Sampling, Bayesian approach. Proceedings of COLING 2012: Technical Papers, pages 3121–3136, COLING 2012, Mumbai, December 2012. 3121  
Search log sessions contain a large number of paraphrases contributed by users during query rewriting. However, it is a big challenge to distinguish paraphrases from the simply related queries in the sessions. This paper addresses this problem by making innovative use of user behavior information embodied in query sessions. Speciﬁcally, we learn paraphrase patterns from the search log sessions with a classiﬁcation framework, in which three types of user behavior features are exploited besides the conventional features. We evaluate the method using a query log of a commercial search engine. Experimental results demonstrate the effectiveness of our method, especially the signiﬁcant contribution of the user behavior features. We extract over 250,000 pairs of paraphrase patterns from the used search log, with a precision over 76%. TITLE AND ABSTRACT IN ANOTHER LANGUAGE (CHINESE) Äu^r1•A l|¢¬{¥ ÷Eã Î † |¢Ú™^r3|¢ž²~¬é Î?1ÓÂU §±¼ •Ð |¢(J§Ïd |¢F“ Î¬{¥•¹Œþ Eã] ", ˜‡(J ¯K´XÛ«© Î¬{¥ = Î´Eã'X§= K==´ŠÂƒ' ®" ©ÏL•Ð/|^ Î¬{¥Û¹ ^r1•A 5)ûù˜¯K"äN/§·‚Äu©a •{l Î¬{] ¥ ÷E ã Î †"3©a .¥§·‚Ø=¦^ DÚ Eã£OA §„}Á n«^r1 •A "·‚¦^˜‡û’|¢Ú™ ^r|¢F“5µ ©JÑ •{"µ (Jy ² ©•{ k 5§cÙ´^r1•A å ²wŠ^"·‚l¤¦^ ^rF“ ¥ Ä Ñ ‡L250,000é Eã †§ÙO(Ç‡L76%" KEYWORDS: paraphrase, pattern, query, search log. KEYWORDS IN CHINESE: Eã, †, Î, |¢F“.  ∗ Corresponding author  Proceedings of COLING 2012: Technical Papers, pages 3137–3152, COLING 2012, Mumbai, December 2012. 3137  
Community-based question answering (CQA) has become an important issue due to the popularity of CQA archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in CQA archives aims to ﬁnd historical questions that are semantically equivalent or relevant to the queried questions. However, question retrieval is challenging partly due to the word ambiguity and lexical gap between the queried questions and the historical questions in the archives. To deal with these problems, we propose the use of translated words to enrich the question representation, going beyond the words in the original language to represent a question. In this paper, each original language question (e.g., English) is automatically translated into an foreign language (e.g., Chinese) by machine translation services, and the resulting translated questions serves as a semantically enhanced representation for supplementing the original bag of words. Experiments conducted on real CQA data set demonstrate that our proposed approach signiﬁcantly outperforms several baseline methods and achieves the state-of-the-art performance. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, L2 (OPTIONAL, AND ON SAME PAGE) 利用双语翻译对社区问答进行问题检索 由于互联网上社区问答数据集的流行，使得社区问答的研究变得越来越流行。本文 关注的是问题检索。 问题检索的目的是从历史问题数据集中查找与查询问题语义等价或相 关的历史问题。然而，问题检索的挑战主要是词汇歧义和查询问题与历史问题 之间的词汇 鸿沟。为了解决这些问题，我们提出利用翻译词来丰富问题的表示，而不单纯利用原始语 言的词来表示问题。 在本文中，通过机器翻译，每个原始语言（例如：英语）的问题都被 自动翻译成另一种外国语言（例如：汉语）， 经过翻译后的问题可以作为一种增强的语义 表示来辅助原始的基于词袋的表示方法。在真实社区问答数据集上的实验表明，我们的方 法可以极大提升基线系统的方法 并取得了最好的性能。 KEYWORDS: Community Question Answering, Question Retrieval, Bilingual Translation. KEYWORDS IN L2: 社区问答，问题检索，双语翻译 Proceedings of COLING 2012: Technical Papers, pages 3153–3170, COLING 2012, Mumbai, December 2012. 3153  
This paper proposes a method to improve shift-reduce constituency parsing by using lexical dependencies. The lexical dependency information is obtained from a large amount of auto-parsed data that is generated by a baseline shift-reduce parser on unlabeled data. We then incorporate a set of novel features deﬁned on this information into the shift-reduce parsing model. The features can help to disambiguate action conﬂicts during decoding. Experimental results show that the new features achieve absolute improvements over a strong baseline by 0.9% and 1.1% on English and Chinese respectively. Moreover, the improved parser outperforms all previously reported shift-reduce constituency parsers. Title and Abstract in Chinese 利用大规模数据词汇依存关系改进移进－归约成分句法分析 本文提出了一种利用词汇依存关系改进移进－归约成分句法分析的方法。首先，我们利用 基准系统在大规模无标注数据上进行自动句法分析并从分析结果中抽取词汇依存关系。其 后，我们在词汇依存信息的基础上定义了一组新特征并将这些特征整合到移进－归约句法 分析模型 中。新特征用于帮助消除移进－归约过程中的动作歧义。实验结果表明，新特征 在英文和中文数据上分别取得了0.9% 和1.1%的性能改进。最终得到的句法分析器的性能 优于相关研究工作中所报告的移进－归约句法分析器的性能。 Keywords: Shift-reduce Constituency Parsing, Lexical Dependencies, Large-scale Data. Keywords in Chinese: 移进－归约成分句法分析，词汇依存，大规模数据. Proceedings of COLING 2012: Technical Papers, pages 3171–3186, COLING 2012, Mumbai, December 2012. 3171  
Increasingly, both commercial and non-commercial translation rely on highly collaborative activity. Thus, we contend, students aiming for a career in translation gain from early exposure to such a working model. On analysing a range of commercial and not-for-profit translation platforms, we identified, from our social-constructivist pedagogical perspective, a major defect. By allowing neither for preserving a trace of interactions nor for relating these interactions to the intermediate products generated during the workflow, they deny participants the chance to later reflect on them. To scaffold the trainee experience, we therefore extended an existing platform – Minna no Hon’yaku (Translation of/by/for All) – with four functions. First, each participant is assigned one or more roles which map into various workflows. Second, communication between role-players is structured by a menu of dialogue acts, each act linked to an entity in the translation project. Third, a menu of revision categories is used to motivate individual edits. Fourth, these events are recorded using an extended TMX notation and can be visualised graphically via a dashboard to answer such questions as: Where within the workflow are the peaks in interaction? Do these correlate with significant modifications to the translation product? 
Input methods vary depending on the Translation Service and are correlated to demographics. In general, younger translators or translators who have worked in the private sector before joining the UN tend to favour keyboarding and computer-assisted translation (CAT) tools, while senior staff who have spent most of their careers at the UN or other international organizations usually prefer dictation. However, dictation is gradually being phased-out for budgetary reasons and voice recognition is not available for all the UN official languages. Use of machine translation at UNHQ UN translators in New York have been exposed to machine translation through Google Translate and Bing Translator (either directly or through CAT tools) and have found that the output quality, for the purposes of the translation of some categories of UN documents, is good enough for post-editing. However, the quality of machine translation, especially in Google Translate, has decreased over the years as documents from other organizations were added to the system. For this reason, the UN was interested in developing its own tool with its own documents. Regarding confidentiality, it is important to keep in mind that the overwhelming majority of documents translated at the UN are for general distribution and are available on the Internet through the Official Document System. In automatic evaluations for all the language combinations, TAPTA4UN has consistenly obtained better BLEU scores than Google Translate, using as reference 1.000 segments of human translations not included in the training. The same goes for human evaluation. In this respect, the only system that was the object of a structured, blind evaluation was the EnglishSpanish system, which rated roughly 4/5 for fluency and 4/5 for accuracy. Scores vary widely depending on the categories of documents processed with machine translation. For some cyclic documents produced by the UN Secretariat, the scores and the quality are very high, while for documents submitted by Members States the scores and quality are less good. This variation is correlated with the categories of documents contained in the training set (if a similar document was included in the training, the machine translation system will produce better results). In some categories of documents, the output of TAPTA4UN allows translators to speed-up the translation process. However, no productivity analysis has been conducted so far as to measure efficiency gains. Currently, TAPTA4UN is available and translators are free to decide when they want to use it and how they want to use it (through its web interfaces or through Studio 2011). An aspect that is unique about the implementation of machine translation at UNHQ is that translators, and in particular senior revisers (P5), have been  enthusiastic about adopting TAPTA4UN as an additional tool1. Some revisers are not used to touch-typing and they appreciate the fact that machine translation gives them a typed draft. As UN translators have been very systematic over the years in the use of terminology and style guidelines, the output of TAPTA4UN is very consistent. Senior revised are also used to correct and evaluate the work of junior translators, so they are comfortable with post-editing. It is interesting to note that most revisers report that post-editing requires a different intellectual effort than translation, an effort that is similar to revision but that is still more challenging, as in some cases the system might deliver sentences with high fluency but low accuracy (sentences that are gramatically correct, but where the meaning has not been fully reflected or is totally wrong). Given the positive user acceptance of TAPTA4UN among DD translators, the scope of gText, a current global project to develop an integrated and web-based suite of terminology, referencing and CAT tools for all UN duty stations, was expanded to include also machine translation. As a first step, the existing language combinations of TAPTA4UN will be integratedto the gText CAT tool. Then, new language combinations will be developed and new services will be created (for instance, to translate MS-Word documents). Also, we would like to conduct structured, blind human evaluations for all language combinations besides English-Spanish.Translators are closely collaborating with WIPO and UN developers and have proposed to add to the system a terminology verification feature, where the output of TAPTA4UN will be automatically checked against the UN main termbases to ensure compliance with official terminology. As today, there are no plans to open the system to other UN departments for gist translation, so TAPTA4UN remains a machine translation tool for professional UN translators. For technical information on TAPTA4UN, please contact Bruno Pouliquen (bruno.pouliquen@wipo.int) and Christophe Mazenc (christophe.mazenc@wipo.int) (WIPO). For information on the TAPTA4UN plug-in for Studio 2011, please contact Michal Ziemski (ziemski@un.org) (UNHQ). For information on usage of TAPTA4UN by UN translators, please contact Cecilia Elizalde (elizalde@un.org) and José García-Verdugo (garcia-verdugo@un.org) (UNHQ) 
The first task carried out by the MTUG was to assess the quality (defined as usefulness for translators) of the system’s output. This so-called “maturity check” kicked off by giving translation staff with an interest in machine translation access to a first generation of engines from English into their languages. These volunteers used the output from the engines as a translation aid, then assessed the usefulness of these baseline engines for translation work. This test was completed in May 2011. As was to be expected, results varied across the various language pairs. Nonetheless, translators deemed 10 of the language pairs (EN into BG, DA, ES, FR, IT, NL, PL, PT, RO and SV) to be sufficiently useful to include them in the next stage: the “real-life trial”, where machine translation is integrated into existing workflows through the automatic provision of tmx files for use with the CAT tool (Trados Translator’s Workbench 7) in day-to-day production. One year on, MT as a tool has gained greater acceptance. Even prior to the introduction of the second generation of engines, a further seven language departments (CS, FI, LT, LV, MT, SL, followed by EL) elected to have machine translation provided as a matter of course. A new generation of engines was built for English into the remaining 22 languages, and vice-versa and became available in June. BLEU scores are used as an automatic metric, and, as was to be expected, the scores improved simply through the addition of an additional year’s data. Nonetheless, although the incremental change according to BLEU was always positive, it ranged from a low of 0.98% (EN-RO) to a high of 4.25% (EN-GA). The latter is an outlier, as Irish has much less data available for building engines and a very low score to begin with, so a larger amount of data would be expected to produce significant improvements. The next highest improvement was that of EN-EL, at 3.79%. Although the engines team felt confident that the gains were significant, confirmation was sought through a second round of testing. Apart from the obvious benefit of obtaining human confirmation of the trends evinced by the flawed but useful BLEU score, additional testing would afford an opportunity for the engines team to devise and test a modular tool for comparing the output of two engines.  ASLIB - Translating and the Computer Conference 29 & 30 November 2012  MF 1 
Machine translation (MT) post-editing is now a popular practice in the translation industry. It has been shown to allow translations to be produced at lower cost without a decrease in quality. The post-editing of automatic translations can provide useful information for MT researchers and developers to evaluate translations and systems. We describe a standalone tool that has two main purposes: allow the post-editing of translations from any MT system and collect sub-segment and segment level information from the post-editing process, e.g., detailed keystroke and time statistics. 
There is an increasing need for a vendor-independent standard regarding the packaging of translation-related projects to increase interoperability among translation tools worldwide. The Language Interoperability Portfolio (Linport) project, a result of collaboration among various organizations, presents a viable option for such a standard. Implementing Structured Translation Specifications (STS), Linport uses a design framework wherein projects can be documented in detail in a “portfolio” and then split out into discrete translation task packages, called "TIPPs", which can then be merged back into a revised portfolio after the tasks are completed. Linport will greatly enhance the cooperative efforts of myriad individuals involved in translation, serving to reduce costs, increase efficiency, and promote quality. Introduction The modern world sees increasing opportunities for connection and communication with others. Thanks to modern technologies such as satellites, cell phones, and the Internet, instant global communication is possible, a phenomenon unimaginable even one hundred years ago. This level of global interoperability, the capacity to work with others to accomplish tasks quickly and easily, is a defining achievement of our era. The great advancement of interoperability in the age of globalization will continue to make life easier. However increased interconnectivity does not necessarily bring interoperability. Specifically, it often requires the development of new standards to promote interoperability. A classic example of interoperability breakdown comes from the shipping industry. For thousands of years countries and companies would ship products internationally. Eventually, there was a focus on determining means of cutting costs and travel time in order to maximize trade profits. During attempts to streamline these processes, it was realized that containers were being unloaded from cargo ships which then did not fit onto trains and boats that would then carry them to their final destinations. Further, there was great inconsistency among container 
Anna De Meo is Associate Professor of Linguistics at the University of Naples “L’Orientale”, where she teaches Specialized Translation, and Multiculturality and Language Acquisition. She is director of the Language Center of the same Institution (CILA), and has been a researcher also at RWTH Aachen University (Germany) and at Georgetown University (Washington D.C.). Her main fields of interest are second language acquisition, L2 speech perception and production, interlanguage pragmatics, and translation. PAPER What else can databases do to assist translators? Illustrating a rated inventory of Web dictionaries Data storage is one of the computer facilities that translators frequently rely on. The so-called ‘translation memories’ allow the creation of multilingual lexical archives which function as customized dictionaries, and are particularly suitable for highly specialized translations in which interlinguistic (Jakobson, 1971) lexical equivalence is more stable, as specialist words are almost monosemous. As a matter of fact, not every translator is a one-field specialist, and the need for a sporadic expertise could be of crucial importance for professionals working with journals, press agencies and media in general. In these contexts, but also in many others where the promptness of information is as important as its accuracy, the Internet becomes the most useful encyclopedia, offering a large amount of inexpensive data, also to be found in the form of dictionaries. The question is thus how to get to the best source available as quickly as possible, since the overproduction of information on the Web eventually turns into “information death”, as Tarp [2010: 41] calls it. Search engines are too generic to be of any assistance to users with these tasks, and metalexicographical resources have started to appear. The quickest searches are offered by metadictionaries1, which show definitions taken from different vocabularies on one page, a system that doesn’t seem to be completely effective, because terminology archives - and hence the number of definitions provided - are either too small to cope with users’ needs or too big to solve the problem of quick access to information. 
 The European Commission’s Directorate-General for Translation (DGT) is currently building a new machine translation system, dubbed MT@EC, to be used by the Commission’s Directorates-General and ultimately by other EU institutions, some public-facing services, and public authorities in the EU Member States. The project is being built using statistical MT technology and the open-source MOSES engine in combination with DGT’s vast database of translated documents covering the 23 official languages of the EU (Euramis), and the expertise and effort of its 1750 translators. Work is ongoing in three strands: data, focussing on cleaning the data available and adding other sources, engines, building and improving the MT system itself, and services, building future portals for other Commission users and other Institutions. 
In the last few years, an ever-increasing number of translation resources have been moved into the cloud where shared repositories of language resources, such as Translation Memories, can be found. Access to these external TMs is generally granted through bilingual and multilingual concordancers. Concordancing tools enable users to be fully in control of the search process because searches are performed manually. This form of external support is used both by translators to solve translation problems and by non-translators to satisfy other information needs. This study analyzes concordance searches in terms of user behavior by drawing a comparison with Web search logs. It aims to identify the most common search strategies and types of interactions, recurrent search patterns, as well as possible issues that can negatively affect search efficiency. A large corpus of queries submitted by translators working at the European institutions will be analyzed across the EU official languages. Inferring user behavior directly from large volumes of authentic data can be used to gain further insights into translators' and general users’ needs to improve currently available tools as well as develop new ones. 
Without high-quality and standards-based terminologies, it is impossible to reach precision, efficiency, and transparency within and across any services, processes or systems especially in legal and administrative domain. The cost of collecting, elaborating, maintaining and modifying terminological resources is enormous in terms of financial, human and time resources. Since terminological work is an on-going process and does not end with the creation of the resource, especially in the legal domain, it is not enough only to store terminological resources. Already existing resources could and should be (re)used and processed to create more complete, higher quality resources. Thus, it is required to add languages to existing terminological resources or to merge different terminological resources. This is a time consuming task and often the reason for keeping the resources separate. The lack of interoperability between different terminological data is another crucial issue that limits the reuse. A semi-automatic method would facilitate the merging of separate resources and increase the reuse of terminological databases. Terminology management systems are mainly used for data storage, but many other activities, such as collecting information and the majority of the decision-making processes, such as expert consultation, discussion and revision, take place outside the terminology management system and can thus remain undocumented and non-traceable. 2. LISE project In this paper, we present the LISE project (Legal Language Interoperability Services)1, financed 
This paper aims to describe ProTermino, a comprehensive web-based terminological management system that is been recently developed.1 This new system is a prototype that intends to fulfill translators’ terminological needs regarding multilingual terminological resources and fill the existing gap of comprehensive terminological management tool. In this paper, we present the main functionalities of ProTermino and briefly outline the needs for such a tool in the terminology and translation context. Subsequently, we discuss the advantages offered by ProTermino comparing it with other similar systems. Introduction Terminology management is been subjected to a number of changes during the last decade which have prompted a radical turn from the traditional assumptions and methodologies. At present the influence of computational and corpus linguistics is observed in almost every terminological project, and terminographers’ tasks have been facilitated thanks to the introduction of computational linguistic technologies. However, they do not cover all terminographers’ needs, and thus they are obliged to combine several tools and technologies to carry out a terminological project. For example, they need a terminological management tool to create a terminology database, along with term extractors, concordancers, concept map editors among other tools, so as to accomplish the different phases and steps of any terminological project. This situation provokes a mushrooming of these tools which hampers terminographers’ tasks. As a consequence, technology, on the one hand, facilitates these tasks by avoiding timeconsuming manual processing but, on the other, the great number of it hinders their tasks by requiring the employment of different tools for different purposes. As a matter of fact, it is urged to investigate on new resources addressed to terminographers that provides them with an efficient comprehensive tool integrating all the tools that are required for a terminological project. ProTermino is a solution to this. 
We describe a novel two-way speech-to-speech (S2S) translation system that actively detects a wide variety of common error types and resolves them through user-friendly dialog with the user(s). We present algorithms for detecting out-of-vocabulary (OOV) named entities and terms, sense ambiguities, homophones, idioms, ill-formed input, etc. and discuss novel, interactive strategies for recovering from such errors. We also describe our approach for prioritizing different error types and an extensible architecture for implementing these decisions. We demonstrate the efficacy of our system by presenting analysis on live interactions in the English-to-Iraqi Arabic direction that are designed to invoke different error types for spoken language translation. Our analysis shows that the system can successfully resolve 47{\%} of the errors, resulting in a dramatic improvement in the transfer of problematic concepts.
This paper is concerned with speech-to-speech translation that is sensitive to paralinguistic information. From the many different possible paralinguistic features to handle, in this paper we chose duration and power as a first step, proposing a method that can translate these features from input speech to the output speech in continuous space. This is done in a simple and language-independent fashion by training a regression model that maps source language duration and power information into the target language. We evaluate the proposed method on a digit translation task and show that paralinguistic information in input speech appears in output speech, and that this information can be used by target language speakers to detect emphasis.
We present a novel approach for continuous space language models in statistical machine translation by using Restricted Boltzmann Machines (RBMs). The probability of an n-gram is calculated by the free energy of the RBM instead of a feedforward neural net. Therefore, the calculation is much faster and can be integrated into the translation process instead of using the language model only in a re-ranking step. Furthermore, it is straightforward to introduce additional word factors into the language model. We observed a faster convergence in training if we include automatically generated word classes as an additional word factor. We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures. Instead of replacing the conventional n-gram-based language model, we trained the RBM-based language model on the more important but smaller in-domain data and combined them in a log-linear way. With this approach we could show improvements of about half a BLEU point on the translation task.
This paper describes a method for selecting text data from a corpus with the aim of training auxiliary Language Models (LMs) for an Automatic Speech Recognition (ASR) system. A novel similarity score function is proposed, which allows to score each document belonging to the corpus in order to select those with the highest scores for training auxiliary LMs which are linearly interpolated with the baseline one. The similarity score function makes use of {''}similarity models{''} built from the automatic transcriptions furnished by earlier stages of the ASR system, while the documents selected for training auxiliary LMs are drawn from the same set of data used to train the baseline LM used in the ASR system. In this way, the resulting interpolated LMs are {''}focused{''} towards the output of the recognizer itself. The approach allows to improve word error rate, measured on a task of spontaneous speech, of about 3{\%} relative. It is important to note that a similar improvement has been obtained using an {''}in-domain{''} set of texts data not contained in the sources used to train the baseline LM. In addition, we compared the proposed similarity score function with two other ones based on perplexity (PP) and on TFxIDF (Term Frequency x Inverse Document Frequency) vector space model. The proposed approach provides about the same performance as that based on TFxIDF model but requires both lower computation and occupation memory.
We present a Monte Carlo model to simulate human judgments in machine translation evaluation campaigns, such as WMT or IWSLT. We use the model to compare different ranking methods and to give guidance on the number of judgments that need to be collected to obtain sufficiently significant distinctions between systems.
Transliteration is the process of writing a word (mainly proper noun) from one language in the alphabet of another language. This process requires mapping the pronunciation of the word from the source language to the closest possible pronunciation in the target language. In this paper we introduce a new semi-supervised transliteration mining method for parallel and comparable corpora. The method is mainly based on a new suggested Three Levels of Similarity (TLS) scores to extract the transliteration pairs. The first level calculates the similarity of of all vowel letters and consonants letters. The second level calculates the similarity of long vowels and vowel letters at beginning and end position of the words and consonants letters. The third level calculates the similarity consonants letters only. We applied our method on Arabic-English parallel and comparable corpora. We evaluated the extracted transliteration pairs using a statistical based transliteration system. This system is built using letters instead or words as tokens. The transliteration system achieves an accuracy of 0.50 and a mean F-score 0.8958 when trained on transliteration pairs extracted from a parallel corpus. The accuracy is 0.30 and the mean F-score 0.84 when we used instead a comparable corpus to automatically extract the transliteration pairs. This shows that the proposed semi-supervised transliteration mining algorithm is effective and can be applied to other language pairs. We also evaluated two segmentation techniques and reported the impact on the transliteration performance.
The task of domain-adaptation attempts to exploit data mainly drawn from one domain (e.g. news) to maximize the performance on the test domain (e.g. weblogs). In previous work, weighting the training instances was used for filtering dissimilar data. We extend this by incorporating the weights directly into the standard phrase training procedure of statistical machine translation (SMT). This allows the SMT system to make the decision whether to use a phrase translation pair or not, a more methodological way than discarding phrase pairs completely when using filtering. Furthermore, we suggest a combined filtering and weighting procedure to achieve better results while reducing the phrase table size. The proposed methods are evaluated in the context of Arabicto-English translation on various conditions, where significant improvements are reported when using the suggested weighted phrase training. The weighting method also improves over filtering, and the combined filtering and weighting is better than a standalone filtering method. Finally, we experiment with mixture modeling, where additional improvements are reported when using weighted phrase extraction over a variety of baselines.
We broaden the application of data selection methods for domain adaptation to a larger number of languages, data, and decoders than shown in previous work, and explore comparable applications for both monolingual and bilingual cross-entropy difference methods. We compare domain adapted systems against very large general-purpose systems for the same languages, and do so without a bias to a particular direction. We present results against real-world generalpurpose systems tuned on domain-specific data, which are substantially harder to beat than standard research baseline systems. We show better performance for nearly all domain adapted systems, despite the fact that the domainadapted systems are trained on a fraction of the content of their general domain counterparts. The high performance of these methods suggest applicability to a wide variety of contexts, particularly in scenarios where only small supplies of unambiguously domain-specific data are available, yet it is believed that additional similar data is included in larger heterogenous-content general-domain corpora.
Although statistical machine translation (SMT) has made great progress since it came into being, the translation of numerical and time expressions is still far from satisfactory. Generally speaking, numbers are likely to be out-of-vocabulary (OOV) words due to their non-exhaustive characteristics even when the size of training data is very large, so it is difficult to obtain accurate translation results for the infinite set of numbers only depending on traditional statistical methods. We propose a language-independent framework to recognize and translate numbers more precisely by using a rule-based method. Through designing operators, we succeed to make rules educible and totally separate from codes, thus, we can extend rules to various language-pairs without re-coding, which contributes a lot to the efficient development of an SMT system with good portability. We classify numbers and time expressions into seven types, which are Arabic number, cardinal numbers, ordinal numbers, date, time of day, day of week and figures. A greedy algorithm is developed to deal with rule conflicts. Experiments have shown that our approach can significantly improve the translation performance.
In this work, we present and evaluate the usage of an interactive web interface for browsing and correcting lecture transcripts. An experiment performed with potential users without transcription experience provides us with a set of example corrections. On German lecture data, user corrections greatly improve the comprehensibility of the transcripts, yet only reduce the WER to 22{\%}. The precision of user edits is relatively low at 77{\%} and errors in inflection, case and compounds were rarely corrected. Nevertheless, characteristic lecture data errors, such as highly specific terms, were typically corrected, providing valuable additional information.
In this study, we extend recurrent neural network-based language models (RNNLMs) by explicitly integrating morphological and syntactic factors (or features). Our proposed RNNLM is called a factored RNNLM that is expected to enhance RNNLMs. A number of experiments are carried out on top of state-of-the-art LVCSR system that show the factored RNNLM improves the performance measured by perplexity and word error rate. In the IWSLT TED test data sets, absolute word error rate reductions over RNNLM and n-gram LM are 0.4∼0.8 points.
It is well known that statistical machine translation systems perform best when they are adapted to the task. In this paper we propose new methods to quickly perform incremental adaptation without the need to obtain word-by-word alignments from GIZA or similar tools. The main idea is to use an automatic translation as pivot to infer alignments between the source sentence and the reference translation, or user correction. We compared our approach to the standard method to perform incremental re-training. We achieve similar results in the BLEU score using less computational resources. Fast retraining is particularly interesting when we want to almost instantly integrate user feed-back, for instance in a post-editing context or machine translation assisted CAT tool. We also explore several methods to combine the translation models.
In this paper, we study the incorporation of statistical machine translation models to automatic speech recognition models in the framework of computer-assisted translation. The system is given a source language text to be translated and it shows the source text to the human translator to translate it orally. The system captures the user speech which is the dictation of the target language sentence. Then, the human translator uses an interactive-predictive process to correct the system generated errors. We show the efficiency of this method by higher human productivity gain compared to the baseline systems: pure ASR system and integrated ASR and MT systems.
This paper provides a fast alternative to Minimum Discrimination Information-based language model adaptation for statistical machine translation. We provide an alternative to computing a normalization term that requires computing full model probabilities (including back-off probabilities) for all n-grams. Rather than re-estimating an entire language model, our Lazy MDI approach leverages a smoothed unigram ratio between an adaptation text and the background language model to scale only the n-gram probabilities corresponding to translation options gathered by the SMT decoder. The effects of the unigram ratio are scaled by adding an additional feature weight to the log-linear discriminative model. We present results on the IWSLT 2012 TED talk translation task and show that Lazy MDI provides comparable language model adaptation performance to classic MDI.
In spoken language translation (SLT), finding proper segmentation and reconstructing punctuation marks are not only significant but also challenging tasks. In this paper we present our recent work on speech translation quality analysis for German-English by improving sentence segmentation and punctuation. From oracle experiments, we show an upper bound of translation quality if we had human-generated segmentation and punctuation on the output stream of speech recognition systems. In our oracle experiments we gain 1.78 BLEU points of improvements on the lecture test set. We build a monolingual translation system from German to German implementing segmentation and punctuation prediction as a machine translation task. Using the monolingual translation system we get an improvement of 1.53 BLEU points on the lecture test set, which is a comparable performance against the upper bound drawn by the oracle experiments.
For current statistical machine translation system, reordering is still a major problem for language pairs like Chinese-English, where the source and target language have significant word order differences. In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. For the given source sentence, we assign each source token a label which contains the reordering information for that token. We also design an unaligned word tag so that the unaligned word phenomenon is automatically implanted in the proposed model. Our reordering model is conditioned on the whole source sentence. Hence it is able to catch the long dependency in the source sentence. Although the learning on large scale task requests notably amounts of computational resources, the decoder makes use of the tagging information as soft constraints. Therefore, the training procedure of our model is computationally expensive for large task while in the test phase (during translation) our model is very efficient. We carried out experiments on five Chinese-English NIST tasks trained with BOLT data. Results show that our model improves the baseline system by 1.32 BLEU 1.53 TER on average.
We present a new approach to domain adaptation for SMT that enriches standard phrase-based models with lexicalised word and phrase pair features to help the model select appropriate translations for the target domain (TED talks). In addition, we show how source-side sentence-level topics can be incorporated to make the features differentiate between more fine-grained topics within the target domain (topic adaptation). We compare tuning our sparse features on a development set versus on the entire in-domain corpus and introduce a new method of porting them to larger mixed-domain models. Experimental results show that our features improve performance over a MIRA baseline and that in some cases we can get additional improvements with topic features. We evaluate our methods on two language pairs, English-French and German-English, showing promising results.
In spoken language translation a machine translation system takes speech as input and translates it into another language. A standard machine translation system is trained on written language data and expects written language as input. In this paper we propose an approach to close the gap between the output of automatic speech recognition and the input of machine translation by training the translation system on automatically transcribed speech. In our experiments we show improvements of up to 0.9 BLEU points on the IWSLT 2012 English-to-French speech translation task.
We describe several experiments to better understand the usefulness of statistical post-edition (SPE) to improve phrase-based statistical MT (PBMT) systems raw outputs. Whatever the size of the training corpus, we show that SPE systems trained on general domain data offers no breakthrough to our baseline general domain PBMT system. However, using manually post-edited system outputs to train the SPE led to a slight improvement in the translations quality compared with the use of professional reference translations. We also show that SPE is far more effective for domain adaptation, mainly because it recovers a lot of specific terms unknown to our general PBMT system. Finally, we compare two domain adaptation techniques, post-editing a general domain PBMT system vs building a new domain-adapted PBMT system with two different techniques, and show that the latter outperforms the first one. Yet, when the PBMT is a {``}black box{''}, SPE trained with post-edited system outputs remains an interesting option for domain adaptation.
Adaptation for Machine Translation has been studied in a variety of ways, using an ideal scenario where the training data can be split into {''}out-of-domain{''} and {''}in-domain{''} corpora, on which the adaptation is based. In this paper, we consider a more realistic setting which does not assume the availability of any kind of {''}in-domain{''} data, hence the name {''}any-text translation{''}. In this context, we present a new approach to contextually adapt a translation model onthe-fly, and present several experimental results where this approach outperforms conventionaly trained baselines. We also present a document-level contrastive evaluation whose results can be easily interpreted, even by non-specialists.
We report on the ninth evaluation campaign organized by the IWSLT workshop. This year, the evaluation offered multiple tracks on lecture translation based on the TED corpus, and one track on dialog translation from Chinese to English based on the Olympic trilingual corpus. In particular, the TED tracks included a speech transcription track in English, a speech translation track from English to French, and text translation tracks from English to French and from Arabic to English. In addition to the official tracks, ten unofficial MT tracks were offered that required translating TED talks into English from either Chinese, Dutch, German, Polish, Portuguese (Brazilian), Romanian, Russian, Slovak, Slovene, or Turkish. 16 teams participated in the evaluation and submitted a total of 48 primary runs. All runs were evaluated with objective metrics, while runs of the official translation tracks were also ranked by crowd-sourced judges. In particular, subjective ranking for the TED task was performed on a progress test which permitted direct comparison of the results from this year against the best results from the 2011 round of the evaluation campaign.
This paper describes our automatic speech recognition (ASR) system for the IWSLT 2012 evaluation campaign. The target data of the campaign is selected from the TED talks, a collection of public speeches on a variety of topics spoken in English. Our ASR system is based on weighted finite-state transducers and exploits an combination of acoustic models for spontaneous speech, language models based on n-gram and factored recurrent neural network trained with effectively selected corpora, and unsupervised topic adaptation framework utilizing ASR results. Accordingly, the system achieved 10.6{\%} and 12.0{\%} word error rate for the tst2011 and tst2012 evaluation set, respectively.
In this paper, we present the KIT systems participating in the English-French TED Translation tasks in the framework of the IWSLT 2012 machine translation evaluation. We also present several additional experiments on the English-German, English-Chinese and English-Arabic translation pairs. Our system is a phrase-based statistical machine translation system, extended with many additional models which were proven to enhance the translation quality. For instance, it uses the part-of-speech (POS)-based reordering, translation and language model adaptation, bilingual language model, word-cluster language model, discriminative word lexica (DWL), and continuous space language model. In addition to this, the system incorporates special steps in the preprocessing and in the post-processing step. In the preprocessing the noisy corpora are filtered by removing the noisy sentence pairs, whereas in the postprocessing the agreement between a noun and its surrounding words in the French translation is corrected based on POS tags with morphological information. Our system deals with speech transcription input by removing case information and punctuation except periods from the text translation model.
This paper describes the University of Edinburgh (UEDIN) systems for the IWSLT 2012 Evaluation. We participated in the ASR (English), MT (English-French, German-English) and SLT (English-French) tracks.
This paper describes the NAIST statistical machine translation system for the IWSLT2012 Evaluation Campaign. We participated in all TED Talk tasks, for a total of 11 language-pairs. For all tasks, we use the Moses phrase-based decoder and its experiment management system as a common base for building translation systems. The focus of our work is on performing a comprehensive comparison of a multitude of existing techniques for the TED task, exploring issues such as out-of-domain data filtering, minimum Bayes risk decoding, MERT vs. PRO tuning, word alignment combination, and morphology.
This paper reports on FBK{'}s Machine Translation (MT) submissions at the IWSLT 2012 Evaluation on the TED talk translation tasks. We participated in the English-French and the Arabic-, Dutch-, German-, and Turkish-English translation tasks. Several improvements are reported over our last year baselines. In addition to using fill-up combinations of phrase-tables for domain adaptation, we explore the use of corpora filtering based on cross-entropy to produce concise and accurate translation and language models. We describe challenges encountered in under-resourced languages (Turkish) and language-specific preprocessing needs.
In this paper, the automatic speech recognition (ASR) and statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2012 are presented. We participated in the ASR (English), MT (English-French, Arabic-English, Chinese-English, German-English) and SLT (English-French) tracks. For the MT track both hierarchical and phrase-based SMT decoders are applied. A number of different techniques are evaluated in the MT and SLT tracks, including domain adaptation via data selection, translation model interpolation, phrase training for hierarchical and phrase-based systems, additional reordering model, word class language model, various Arabic and Chinese segmentation methods, postprocessing of speech recognition output with an SMT system, and system combination. By application of these methods we can show considerable improvements over the respective baseline systems.
In this paper, we describe HIT-LTRC's participation in the IWSLT 2012 evaluation campaign. In this year, we took part in the Olympics Task which required the participants to translate Chinese to English with limited data. Our system is based on Moses[1], which is an open source machine translation system. We mainly used the phrase-based models to carry out our experiments, and factored-based models were also performed in comparison. All the involved tools are freely available. In the evaluation campaign, we focus on data selection, phrase extraction method comparison and phrase table combination.
This paper reports on the participation of FBK at the IWSLT2012 evaluation campaign on automatic speech recognition: namely in the English ASR track. Both primary and contrastive submissions have been sent for evaluation. The ASR system features acoustic models trained on a portion of the TED talk recordings that was automatically selected according to the fidelity of the provided transcriptions. Three decoding steps are performed interleaved by acoustic feature normalization and acoustic model adaptation. A final rescoring step, based on the usage of an interpolated language model, is applied to word graphs generated in the third decoding step. For the primary submission, language models entering the interpolation are trained on both out-of-domain and in-domain text data, instead the contrastive submission uses both {''}general purpose{''} and auxiliary language models trained only on out-of-domain text data. Despite this fact, similar performance are obtained with the two submissions.
This paper describes our English Speech-to-Text (STT) systems for the 2012 IWSLT TED ASR track evaluation. The systems consist of 10 subsystems that are combinations of different front-ends, e.g. MVDR based and MFCC based ones, and two different phone sets. The outputs of the subsystems are combined via confusion network combination. Decoding is done in two stages, where the systems of the second stage are adapted in an unsupervised manner on the combination of the first stage outputs using VTLN, MLLR, and cM-LLR.
This paper describes the KIT-NAIST (Contrastive) English speech recognition system for the IWSLT 2012 Evaluation Campaign. In particular, we participated in the ASR track of the IWSLT TED task. The system was developed by Karlsruhe Institute of Technology (KIT) and Nara Institute of Science and Technology (NAIST) teams in collaboration within the interACT project. We employ single system decoding with fully continuous and semi-continuous models, as well as a three-stage, multipass system combination framework built with the Janus Recognition Toolkit. On the IWSLT 2010 test set our single system introduced in this work achieves a WER of 17.6{\%}, and our final combination achieves a WER of 14.4{\%}.
This paper describes the EBMT system of Kyoto University that participated in the OLYMPICS task at IWSLT 2012. When translating very different language pairs such as Chinese-English, it is very important to handle sentences in tree structures to overcome the difference. Many recent studies incorporate tree structures in some parts of translation process, but not all the way from model training (alignment) to decoding. Our system is a fully tree-based translation system where we use the Bayesian phrase alignment model on dependency trees and example-based translation. To improve the translation quality, we conduct some special processing for the IWSLT 2012 OLYMPICS task, including sub-sentence splitting, non-parallel sentence filtering, adoption of an optimized Chinese segmenter and rule-based decoding constraints.
This paper presents the LIG participation to the E-F MT task of IWSLT 2012. The primary system proposed made a large improvement (more than 3 point of BLEU on tst2010 set) compared to our last year participation. Part of this improvment was due to the use of an extraction from the Gigaword corpus. We also propose a preliminary adaptation of the driven decoding concept for machine translation. This method allows an efficient combination of machine translation systems, by rescoring the log-linear model at the N-best list level according to auxiliary systems: the basis technique is essentially guiding the search using one or previous system outputs. The results show that the approach allows a significant improvement in BLEU score using Google translate to guide our own SMT system. We also try to use a confidence measure as an additional log-linear feature but we could not get any improvment with this technique.
This paper describes the MIT-LL/AFRL statistical MT system and the improvements that were developed during the IWSLT 2012 evaluation campaign. As part of these efforts, we experimented with a number of extensions to the standard phrase-based model that improve performance on the Arabic to English and English to French TED-talk translation task. We also applied our existing ASR system to the TED-talk lecture ASR task, and combined our ASR and MT systems for the TED-talk SLT task. We discuss the architecture of the MIT-LL/AFRL MT system, improvements over our 2011 system, and experiments we ran during the IWSLT-2012 evaluation. Specifically, we focus on 1) cross-domain translation using MAP adaptation, 2) cross-entropy filtering of MT training data, and 3) improved Arabic morphology for MT preprocessing.
This paper describes our methods used in the NAIST-NICT submission to the International Workshop on Spoken Language Translation (IWSLT) 2012 evaluation campaign. In particular, we propose two extensions to minimum bayes-risk decoding which reduces a expected loss.
This paper presents efforts in preparation of the Polish-to-English SMT system for the TED lectures domain that is to be evaluated during the IWSLT 2012 Conference. Our attempts cover systems which use stems and morphological information on Polish words (using two different tools) and stems and POS.
We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses.
The paper presents the system developed by RACAI for the ISWLT 2012 competition, TED task, MT track, Romanian to English translation. We describe the starting baseline phrase-based SMT system, the experiments conducted to adapt the language and translation models and our post-translation cascading system designed to improve the translation without external resources. We further present our attempts at creating a better controlled decoder than the open-source Moses system offers.
WedescribetheTU ̈B ̇ITAKsubmissiontotheIWSLT2012 Evaluation Campaign. Our system development focused on utilizing Bayesian alignment methods such as variational Bayes and Gibbs sampling in addition to the standard GIZA++ alignments. The submitted tracks are the Arabic-English and Turkish-English TED Talks translation tasks.
Lecture at Workshop on Free/Open-Source Rule-Based Machine Translation, June 2012 Summary 
The GF Eclipse Plugin provides an integrated development environment (IDE) for developing grammars in the Grammatical Framework (GF). Built on top of the Eclipse Platform, it aids grammar writing by providing instant syntax checking, semantic warnings and cross-reference resolution. Inline documentation and a library browser facilitate the use of existing resource libraries, and compilation and testing of grammars is greatly improved through single-click launch configurations and an in-built test case manager for running treebank regression tests. This IDE promotes grammar-based systems by making the tasks of writing grammars and using resource libraries more efficient, and provides powerful tools to reduce the barrier to entry to GF and encourage new users of the framework.
Trond Trosterud and Kevin Brubeck Unhammer University of Tromsø, Kaldera spr˚akteknologi 2.1 Introduction 
Previous work on an interactive system aimed at helping non-expert users to enlarge the monolingual dictionaries of rule-based machine translation (MT) systems worked by discarding those inflection paradigms that cannot generate a set of inflected word forms validated by the user. This method, however, cannot deal with the common case where a set of different paradigms generate exactly the same set of inflected word forms, although with different inflection information attached. In this paper, we propose the use of an n-gram-based model of lexical categories and inflection information to select a single paradigm in cases where more than one paradigm generates the same set of word forms. Results obtained with a Spanish monolingual dictionary show that the correct paradigm is chosen for around 75{\%} of the unknown words, thus making the resulting system (available under an open-source license) of valuable help to enlarge the monolingual dictionaries used in MT involving non-expert users without technical linguistic knowledge.
In this paper, we present an open-source toolkit to enrich a phrase-based statistical machine translation system (Moses) with phrase pairs generated from the linguistic resources of a shallow-transfer rule-based machine translation system (Apertium). A system built with this toolkit was not outperformed by any other participant in the shared translation task of the Sixth Workshop on Statistical Machine Translation (WMT 11) for the Spanish{--}English language pair.
This paper describes the development of a one-way machine translation system from SerboCroatian to Macedonian on the Apertium platform. Details of resources and development methods are given, as well as an evaluation, and general directives for future work.
Cristina Espan˜a-Bonet, Gorka Labaka, Arantza D´iaz de Ilarraza, Llu´is M`arquez, Kepa Sarasola UPC Barcelona, University of the Basque Country The process of developing hybrid MT systems is usually guided by an evaluation method used to compare diﬀerent combinations of basic subsystems. This work presents a deep evaluation experiment of a hybrid architecture, which combines rule-based and statistical translation approaches. Diﬀerences between the results obtained from automatic and human evaluations corroborate the inappropriateness of pure lexical automatic evaluation metrics to compare the outputs of systems that use very diﬀerent translation approaches. An examination of sentences with controversial results suggested that linguistic well-formedness should be considered in the evaluation of output translations. Following this idea, we have experimented with a new simple automatic evaluation metric, which combines lexical and PoS information. This measure showed higher agreement with human assessments than BLEU in a previous study (Labaka et al., 2011). In this paper we have extended its usage throughout the system development cycle, focusing on its ability to improve parameter optimization. 
We describe the preparation of parallel corpora based on professional quality subtitles in seven European language pairs. The main focus is the effect of the processing steps on the size and quality of the ﬁnal corpora. 
In this paper we present two sets of English-Chinese and Chinese-English machine translation trials conducted by TAUS Labs on computer software content. The goal of this study is twofold: (1) to share our experience on training and optimizing of Moses-based engines driven by translation memories provided by industrial users and (2) to give to the users the idea of results, cost and effort associated with training of MT engines. 
This paper presents a machine translation prototype developed with the United Nations (UN) corpus for automatic translation of UN documents from English to Spanish. The tool is based on open source Moses technology and has been developed by the World Intellectual Property Organization (WIPO). The two organizations pooled resources to create a model trained on an extensive corpus of manually translated UN documents. The performance of the SMT system as a translation assistant was shown to be very satisfactory (using both automatic and human evaluation). The use of the system in production within UN is now under discussion 
Recent developments in search algorithms and software architecture have enabled multi-user web-based prototypes for Interactive Machine Translation (IMT), a technology that aims to assist, rather than replace, the human translator. Surprisingly, formal human evaluations of IMT systems are highly scarce in the literature. To this regard, we discuss experiences gained while testing IMT systems. We report the lessons learned from two user evaluations. Our results can provide researchers and practitioners with several guidelines towards the design of on-line IMT tools. 
We compare three methods of modeling morphological features in statistical machine translation (SMT) from English to Arabic, a morphologically rich language. Features can be modeled as part of the core translation process mapping source tokens to target tokens. Alternatively these features can be generated using target monolingual context as part of a separate generation (or post-translation inﬂection) step. Finally, the features can be predicted using both source and target information in a separate step from translation and generation. We focus on three morphological features that we demonstrate through a manual error analysis to be most problematic for English-Arabic SMT: gender, number and the determiner clitic. Our results show signiﬁcant improvements over a state-ofthe-art baseline (phrase-based SMT) of almost 1% absolute BLEU on a medium size training set. Our best conﬁguration models the determiner as part of core translation and predicts gender and number separately, and handles the rest of the features through generation. 
Unknown words and word segmentation granularity are two main problems in Chinese word segmentation for ChineseJapanese Machine Translation (MT). In this paper, we propose an approach of exploiting common Chinese characters shared between Chinese and Japanese in Chinese word segmentation optimization for MT aiming to solve these problems. We augment the system dictionary of a Chinese segmenter by extracting Chinese lexicons from a parallel training corpus. In addition, we adjust the granularity of the training data for the Chinese segmenter to that of Japanese. Experimental results of Chinese-Japanese MT on a phrase-based SMT system show that our approach improves MT performance signiﬁcantly. 
This paper presents a range of preprocessing solutions for Hebrew-English statistical machine translation. Our best system, using a morphological analyzer, increases 3.5 BLEU points over a no-tokenization baseline on a blind test set. The next best system uses Morfessor, an unsupervised morphological segmenter, and obtains almost 3.0 BLEU points over the baseline. 
 Anthony Hartley Toyohashi University of Technology a.hartley@imc.tut.ac.jp  Hitoshi Isahara Toyohashi University of Technology isahara@tut.jp  Kyo Kageura University of Tokyo kyo@p.u-tokyo.ac.jp  Toshio Okamoto Toyota Boshoku Corporation toshio_okamoto@toyotaboshoku.co.jp  Katsumasa Shimizu Toyota Boshoku Corporation katsumasa_shimizu@toyotaboshoku.co.jp  Abstract We report the early stages of an industrial-academic collaboration to build translation awareness within a global Japanese company where nonprofessional authors are called upon to write ‘global job manuals’ for internal dissemination. Following an analysis of current practice, we devised a document template and simple writing rules which we tested experimentally with two MT systems. Overall, native-speaker judges found that the quality of the Japanese was maintained or improved, while the impact on the raw English translations varied according to MT system. The case study has wider implications for the acceptance of structured authoring by nonprofessional and occasional writers. 
This paper presents a novel efﬁciencybased evaluation of sentence and word aligners. This assessment is critical in order to make a reliable use in industrial scenarios. The evaluation shows that the resources required by aligners differ rather broadly. Subsequently, we establish limitation mechanisms on a set of aligners deployed as web services. These results, paired with the quality expected from the aligners, allow providers to choose the most appropriate aligner according to the task at hand. 
We present a set of free tools for building rule-based machine translation systems for polysynthetic languages. As there are no large corpora for most of the “small” languages, it is often impossible to use statistical methods. There are some free MT tools but very little work has been done on polysynthetic languages. The aim of this project is to provide computational tools for morphological and syntactic processing for such languages. 
g.thurmair @linguatec.de  Andrea Agnoletto Fondazione Don Gnocchi Onlus Milano aagnoletto @dongnocchi.it  Valerio Gower Fondazione Don Gnocchi Onlus Milano vgower @dongnocchi.it  Roberts Rozis Tilde Riga roberts.rozis @tilde.lv  Abstract The document describes an application of language technology to improve the access to a database of Assistive Technology in the EASTIN-CL project. It focuses on engineering aspects of language technology integration. The paper describes the collection of a multilingual terminology database of the domain, and its use in multilingual and multimodal frontend components, especially the design, implementation and test of the query component. The system will be online for public web access under www.eastin.eu 1.  group relevant products under each heading of this classification. In 2005 the major European AT information providers joined in the European Assistive Technology Information Network (EASTIN). EASTIN provides a portal (www.eastin.eu) where people can access all databases of its national members simultaneously; the central server collects information on all products existing in one of the databases of the associated partners, so information seekers search on European level.  
This user study reports on an ongoing pilot that aims at using machine translation on a large scale, for the translation of technical documentation for a globally acting automotive supplier. The pilot is conducted by a language service provider and a research institution. First results go beyond expectations. 
Software localization requires translating short text strings appearing in user interfaces (UI) into several languages. These strings are usually unrelated to the other strings in the UI. Due to the lack of semantic context, many ambiguity problems cannot be solved during translation. However, UI are composed of several visual components to which text strings are associated. Although this association might be very valuable for word disambiguation, it has not been exploited. In this paper, we present the problem of lack of context awareness for UI localization, providing real examples and identifying the main research challenges. 
We present an approach for translating subtitles where standard time and space constraints are modeled as part of the generation of translations in a phrase-based statistical machine translation system (PBSMT). We propose and experiment with two promising strategies for jointly translating and compressing subtitles from English into Portuguese. The quality of the automatic translations is measured via the human post-editing of such translations so that they become adequate, ﬂuent and compliant with time and space constraints. Experiments show that carefully selecting the data to tune the model parameters in the PB-SMT system already improves over an unconstrained baseline and that adding speciﬁc model components to guide the translation process can further improve the ﬁnal translations under certain conditions. 
Automatic post-editors (APEs) enable the re-use of black box machine translation (MT) systems for a variety of tasks where different aspects of translation are important. In this paper, we describe APEs that target adequacy errors, a critical problem for tasks such as cross-lingual question-answering, and compare different approaches for post-editing: a rule-based system and a feedback approach that uses a computer in the loop to suggest improvements to the MT system. We test the APEs on two different MT systems and across two different genres. Human evaluation shows that the APEs signiﬁcantly improve adequacy, regardless of approach, MT system or genre: 30-56% of the post-edited sentences have improved adequacy compared to the original MT. 
In spite of much ongoing research on machine translation evaluation there is little quantitative work that directly measures users’ intuitive or emotional preferences regarding different types of machine translation errors. However, the elicitation and modeling of user preferences is an important prerequisite for future research on user adaptation and customization of machine translation engines. In this paper we explore the use of conjoint analysis as a formal quantitative framework to gain insight into users’ relative preferences for different translation error types. Using English-Spanish as the translation direction we conduct a crowd-sourced conjoint analysis study and obtain utility values for individual error types. Our results indicate that word order errors are clearly the most dispreferred error type, followed by word sense, morphological, and function word errors. 
Statistical-based methods are the prevalent approaches for implementing machine translation systems today. However the resulted translations are usually flawed to some degree. We assume that a statistical baseline system can be re-used to automatically learn how to (partially) correct translation errors, i.e. to turn a “broken” target translation into a better one. By training and testing on initial bilingual data, we constructed a system S1 which was used to translate the source language part of the training corpus. The new translated corpus and its reference translation are used to train and test another similar system S2. Without any additional data, the chain S1+S2 shows a sensible quality increase against S1 in terms of BLEU scores, for both translation directions (English to Romanian and Romanian to English). 
Stuhlsatzenhausweg 3, 66123 Saarbrücken, Germany sabine.hunsicker@dfki.de  Abstract This paper presents a fast and accurate parallel sentence mining algorithm for comparable corpora called LEXACC based on the Cross-Language Information Retrieval framework combined with a trainable translation similarity measure that detects pairs of parallel and quasi-parallel sentences. LEXACC obtains state-of-the-art results in comparison with established approaches. 
 In order to achieve optimal performance, an  We tackle the problem of domain adaptation of Statistical Machine Translation by exploiting domain-speciﬁc data acquired by domain-focused web-crawling. We design and evaluate a procedure for automatic acquisition of monolingual and parallel data and their exploitation for training, tuning, and testing in a phrase-based Statistical Machine Translation system. We present a strategy for using such resources depending on their availability and quantity supported by results of a large-scale evaluation on the domains of Natural Environment and Labour Legislation and two language pairs: English–French, English-Greek. The average observed increase of BLEU is substantial at 49.5% relative. 
 The usefulness of a translated text for gisting purposes strongly depends on the overall translation quality of the text, but especially on the translation quality of the most informative portions of the text. In this paper we address the problems of ranking translated sentences within a document and ranking translated documents within a set of documents on the same topic according to their informativeness and translation quality. An approach combining quality estimation and sentence ranking methods is used. Experiments with French-English translation using four sets of news commentary documents show promising results for both sentence and document ranking. We believe that this approach can be useful in several practical scenarios where translation is aimed at gisting, such as multilingual media monitoring and news analysis applications.  
This paper reports a set of domain adaptation techniques for improving Statistical Machine Translation (SMT) for usergenerated web forum content. We investigate both normalization and supplementary training data acquisition techniques, all guided by the aim of reducing the number of Out-Of-Vocabulary (OOV) items in the target language with respect to the training data. We classify OOVs into a set of types, and address each through dedicated normalization and/or supplementary training material selection-based approaches. We investigate the effect of these methods both in an additive as well as a contrastive scenario. Our ﬁndings show that (i) normalization and supplementary training material techniques can be complementary, (ii) for general forum data, fully automatic supplementary training data acquisition can perform as well or sometimes better than semi-automatic normalization (although tackling different types of OOVs) and (iii) for very noisy data, normalization really pays off. 
Long-distance reordering of syntactically divergent language pairs is a critical problem. SMT has had limited success in handling these reorderings during inference, and thus deterministic preprocessing based on reordering parse trees is used. We consider German-to-English translation using Hiero. We show how to effectively model long-distance reorderings during search. Our work is novel in that we look at reordering distances of up to 50 words, and conduct a detailed manual analysis based on a new gold standard. 
In Statistical Machine Translation, indomain and out-of-domain training data are not always clearly delineated. This paper investigates how we can still use mixture-modeling techniques for domain adaptation in such cases. We apply unsupervised clustering methods to split the original training set, and then use mixturemodeling techniques to build a model adapted to a given target domain. We show that this approach improves performance over an unadapted baseline, and several alternative domain adaptation methods. 
In this paper, we describe two approaches to extending syntactic constraints in the Hierarchical Phrase-Based (HPB) Statistical Machine Translation (SMT) model using Combinatory Categorial Grammar (CCG). These extensions target the limitations of previous syntax-augmented HPB SMT systems which limit the coverage of the syntactic constraints applied. We present experiments on Arabic–English and Chinese–English translation. Our experiments show that using extended CCG labels helps to increase nonterminal label coverage and achieve signiﬁcant improvements over the baseline for Arabic– English translation. In addition, combining extended CCG labels with CCGaugmented glue grammar helps to improve the performance of the Chinese–English translation over the baseline systems. 
In this paper we describe a module (rule formalism, rule compiler and rule processor) designed to provide ﬂexible support for lexical selection in rule-based machine translation. The motivation and implementation for the system is outlined and an efﬁcient algorithm to compute the best coverage of lexical-selection rules over an ambiguous input sentence is described. We provide a demonstration of the module by learning rules for it on a typical training corpus and evaluating against other possible lexicalselection strategies. The inclusion of the module, along with rules learnt from the parallel corpus provides a small, but consistent and statistically-signiﬁcant improvement over either using the highest-scoring translation according to a target-language model or using the most frequent aligned translation in the parallel corpus which is also found in the system’s bilingual dictionaries. 
This paper presents a statistical approach to adapt out-of-domain machine translation systems to the medical domain through an unsupervised post-editing step. A statistical post-editing model is built on statistical machine translation (SMT) outputs aligned with their translation references. Evaluations carried out to translate medical texts from French to English show that an out-of-domain machine translation system can be adapted a posteriori to a speciﬁc domain. Two SMT systems are studied: a state-of-the-art phrasebased implementation and an online publicly available system. Our experiments also indicate that selecting sentences for post-editing leads to signiﬁcant improvements of translation quality and that more gains are still possible with respect to an oracle measure. 
This paper investigates the feasibility of using crowd-sourcing services for the human assessment of machine translation quality of translations into non-English target languages. Non-expert graders are hired through the CrowdFlower interface to Amazon’s Mechanical Turk in order to carry out a ranking-based MT evaluation of utterances taken from the travel conversation domain for 10 Indo-European and Asian languages. The collected human assessments are analyzed for their worker characteristics, evaluation costs, and quality of the evaluations in terms of the agreement between non-expert graders and expert/oracle judgments. Moreover, data quality control mechanisms including “locale qualification” “qualificatio testing”, and “on-the-fl verification are investigated in order to increase the reliability of the crowd-based evaluation results. 
 Rei Miyata University of Tokyo  isahara@tut.jp  kyo@p.u-tokyo.ac.jp rei@p.u-tokyo.ac.jp  Abstract We report on an experiment to test the efficacy of ‘controlled language’ authoring of technical documents in Japanese, with respect both to the readability of the Japanese source and the quality of the English machine-translated output. Using four MT systems, we tested two sets of writing rules designed for two document types written by authors with contrasting professional profiles. We elicited judgments from native speakers to establish the positive or negative impact of each rule on readability and translation quality. 
This paper describes the ﬁrst steps towards a minimum-size phrase table implementation to be used for phrase-based statistical machine translation. The focus lies on the size reduction of target language data in a phrase table. Rank Encoding (REnc), a novel method for the compression of word-aligned target language in phrase tables is presented. Combined with Huffman coding a relative size reduction of 56 percent for target phrase words and alignment data is achieved when compared to bare Huffman coding without R-Enc. In the context of the complete phrase table the size reduction is 22 percent. 
Vera Aleksić Linguatec v.aleksic@linguatec.de  Abstract This document describes a tool which extracts term and lexicon entries from SMT phrase tables, without further reference to monolingual data. It applies filters to such tables, and builds lexicon entries from the ‘good’ candidates. Error rates of the tool can be as low as 7.3%, accumulated from source, target, and transfer errors.1 
We describe here a Web inventory named WIT3 that offers access to a collection of transcribed and translated talks. The core of WIT3 is the TED Talks corpus, that basically redistributes the original content published by the TED Conference website (http://www.ted.com). Since 2007, the TED Conference, based in California, has been posting all video recordings of its talks together with subtitles in English and their translations in more than 80 languages. Aside from its cultural and social relevance, this content, which is published under the Creative Commons BYNC-ND license, also represents a precious language resource for the machine translation research community, thanks to its size, variety of topics, and covered languages. This effort repurposes the original content in a way which is more convenient for machine translation researchers. 
This work presents a HMT system for patent translation. The system exploits the high coverage of SMT and the high precision of an RBMT system based on GF to deal with speciﬁc issues of the language. The translator is speciﬁcally developed to translate patents and it is evaluated in the English-French language pair. Although the number of issues tackled by the grammar are not extremely numerous yet, both manual and automatic evaluations consistently show their preference for the hybrid system in front of the two individual translators. 
We present a sub-sentential alignment algorithm that relies on association scores between words or phrases. This algorithm is inspired by previous work on alignment by recursive binary segmentation and on document clustering. We evaluate the resulting alignments on machine translation tasks and show that we can obtain state-ofthe-art results, with gains up to more than 4 BLEU points compared to previous work, with a method that is simple, independent of the size of the corpus to be aligned, and directly computes symmetric alignments. This work also provides new insights regarding the use of “heuristic” alignment scores in statistical machine translation. 
Enriching statistical models with linguistic knowledge has been a major concern in Machine Translation (MT). In monolingual data, adjuncts are optional constituents contributing secondarily to the meaning of a sentence. One can therefore hypothesize that this secondary status is preserved in translation, and thus that adjuncts may align consistently with their adjunct translations, suggesting they form optional phrase pairs in parallel corpora. In this paper we verify this hypothesis on French-English translation data, and explore the utility of compiling adjunct-poor data for augmenting the training data of a phrase-based machine translation model. 
We show in an empirical study that not only did all cross-lingual alternations of verb frames across Chinese–English translations fall within the reordering capacity of Inversion Transduction Grammars, but more surprisingly, about 97% of the alternations were expressible by the far more restrictive Linear Transduction Grammars. Also, about 71% of the cross-lingual verb frame alternations turn out to be monotonic even for diverse language pairs such as Chinese–English. We also observe that a source verb frame alternation pattern translates into a small subset of the possible target verb frame alternation patterns, based on the construction of the source sentence and the frame set deﬁnitions. As a part of our evaluation, we also present a novel linear time algorithm to determine whether a particular syntactic alignment falls within the expressiveness of Linear Transduction Grammars. To our knowledge, this is the ﬁrst study that attempts to analyze the cross-lingual alternation behavior of semantic frames and the extent of their coverage under syntax-based machine translation formalisms. 
The performance of Phrase-Based Statistical Machine Translation (PBSMT) systems mostly depends on training data. Many papers have investigated how to create new resources in order to increase the size of the training corpus in an attempt to improve PBSMT performance. In this work, we analyse and characterize the way in which the in-domain and outof-domain performance of PBSMT is impacted when the amount of training data increases. Two different PBSMT systems, Moses and Portage, two of the largest parallel corpora, Giga (French-English) and UN (Chinese-English) datasets and several in- and out-of-domain test sets were used to build high quality learning curves showing consistent logarithmic growth in performance. These results are stable across language pairs, PBSMT systems and domains. We also analyse the respective impact of additional training data for estimating the language and translation models. Our proposed model approximates learning curves very well and indicates the translation model contributes about 30% more to the performance gain than the language model. 
In this paper, we propose novel extensions of hierarchical phrase-based systems with a discriminative lexicalized reordering model. We compare different feature sets for the discriminative reordering model and investigate combinations with three types of non-lexicalized reordering rules which are added to the hierarchical grammar in order to allow for more reordering ﬂexibility during decoding. All extensions are evaluated in standard hierarchical setups as well as in setups where the hierarchical recursion depth is restricted. We achieve improvements of up to +1.2 %BLEU on a large-scale Chinese→English translation task. 
This paper presents a novel approach to pivot-based machine translation (MT): while the state-of-the-art uses two statistical systems, this proposal treats the second system as a black box. Our approach effecively provides pivot-based MT to target languages for which no suitable bilingual corpora are available to build statistical systems, as long as any other kind of MT system is available. We experiment with an algorithm that uses two features to ﬁnd the best translation: the translation score provided by the ﬁrst system and ﬂuency of the ﬁnal translation. Despite its simplicity, this approach yields signiﬁcant improvements over the baseline, which translates the source sentences using the two MT systems sequentially. We have experimented with two scenarios, technical documentation in Romance languages and newswire in Slavic languages, obtaining 11.88% and 13.32% relative improvements in terms of BLEU, respectively. 
This paper introduces a publicly available database of recorded translation sessions for Translation Process Research (TPR). User activity data (UAD) of translators behavior was collected over the past 5 years in several translation studies with Translog 1 , a data acquisition software which logs keystrokes and gaze data during text reception and production. The database compiles this data into a consistent format which can be processed by various visualization and analysis tools.
Post-editing machine translations has been attracting increasing attention both as a common practice within the translation industry and as a way to evaluate Machine Translation (MT) quality via edit distance metrics between the MT and its post-edited version. Commonly used metrics such as HTER are limited in that they cannot fully capture the effort required for post-editing. Particularly, the cognitive effort required may vary for different types of errors and may also depend on the context. We suggest post-editing time as a way to assess some of the cognitive effort involved in post-editing. This paper presents two experiments investigating the connection between post-editing time and cognitive effort. First, we examine whether sentences with long and short post-editing times involve edits of different levels of difficulty. Second, we study the variability in post-editing time and other statistics among editors.
Pauses are known to be good indicators of cognitive demand in monolingual language production and in translation. However, a previous effort by O{'}Brien (2006) to establish an analogous relationship in post-editing did not produce the expected result. In this case study, we introduce a metric for pause activity, the average pause ratio, which is sensitive to both the number and duration of pauses. We measured cognitive effort in a segment by counting the number of complete editing events. We found that the average pause ratio was higher for less cognitively demanding segments than for more cognitively demanding segments. Moreover, this effect became more pronounced as the minimum threshold for pause length was shortened.
Post-editing of machine translation has become more common in recent years. This has created the need for a formal method of assessing the performance of post-editors in terms of whether they are able to produce post-edited target texts that follow project specifications. This paper proposes the use of formalized structured translation specifications (FSTS) as a basis for post-editor assessment. To determine if potential evaluators are able to reliably assess the quality of post-edited translations, an experiment used texts representing the work of five fictional post-editors. Two software applications were developed to facilitate the assessment: the Ruqual Specifications Writer, which aids in establishing post-editing project specifications; and Ruqual Rubric Viewer, which provides a graphical user interface for constructing a rubric in a machine-readable format. Seventeen non-experts rated the translation quality of each simulated post-edited text. Intraclass correlation analysis showed evidence that the evaluators were highly reliable in evaluating the performance of the post-editors. Thus, we assert that using FSTS specifications applied through the Ruqual software tools provides a useful basis for evaluating the quality of post-edited texts.
Automatic post-editors (APEs) can improve adequacy of MT output by detecting and reinserting dropped content words, but the location where these words are inserted is critical. In this paper, we describe a probabilistic approach for learning reinsertion rules for specific languages and MT systems, as well as a method for synthesizing training data from reference translations. We test the insertion logic on MT systems for Chinese to English and Arabic to English. Our adaptive APE is able to insert within 3 words of the best location 73{\%} of the time (32{\%} in the exact location) in Arabic-English MT output, and 67{\%} of the time in Chinese-English output (30{\%} in the exact location), and delivers improved performance on automated adequacy metrics over a previous rule-based approach to insertion. We consider how particular aspects of the insertion problem make it particularly amenable to machine learning solutions.
It is a well-known fact that the amount of content which is available to be translated and localized far outnumbers the current amount of translation resources. Automation in general and Machine Translation (MT) in particular are one of the key technologies which can help improve this situation. However, a tool that integrates all of the components needed for the localization process is still missing, and MT is still out of reach for most localisation professionals. In this paper we present an online translation environment which empowers users with MT by enabling engines to be created from their data, without a need for technical knowledge or special hardware requirements and at low cost. Documents in a variety of formats can then be post-edited after being processed with their Translation Memories, MT engines and glossaries. We give an overview of the tool and present a case study of a project for a large games company, showing the applicability of our tool.
In the last few years the European Parliament has witnessed a significant increase in translation demand. Although Translation Memory (TM) tools, terminology databases and bilingual concordancers have provided significant leverage in terms of quality and productivity the European Parliament is in need for advanced language technology to keep facing successfully the challenge of multilingualism. This paper describes an ongoing large-scale machine translation post-editing evaluation campaign the purpose of which is to estimate the business benefits from the use of machine translation for the European Parliament. This paper focuses mainly on the design, the methodology and the tools used by the evaluators but it also presents some preliminary results for the following language pairs: Polish-English, Danish-English, Lithuanian-English, English-German and English-French.
This paper is a partial report of a research effort on evaluating the effect of crowd-sourced post-editing. We first discuss the emerging trend of crowd-sourced post-editing of machine translation output, along with its benefits and drawbacks. Second, we describe the pilot study we have conducted on a platform that facilitates crowd-sourced post-editing. Finally, we provide our plans for further studies to have more insight on how effective crowd-sourced post-editing is.
The increasing role of post-editing as a way of improving machine translation output and a faster alternative to translating from scratch has lately attracted researchers{'} attention and various attempts have been proposed to facilitate the task. We experiment with a method to provide support for the post-editing task through error detection. A deep linguistic error analysis was done of a sample of English sentences translated from Portuguese by two Rule-based Machine Translation systems. We designed a set of rules to deal with various systematic translation errors and implemented a subset of these rules covering the errors of tense and number. The evaluation of these rules showed a satisfactory performance. In addition, we performed an experiment with human translators which confirmed that highlighting translation errors during the post-editing can help the translators perform the post-editing task up to 12 seconds per error faster and improve their efficiency by minimizing the number of missed errors.
In this paper, we present the Moses-based infrastructure we developed and use as a productivity tool for the localisation of software documentation and user interface (UI) strings at Autodesk into twelve languages. We describe the adjustments we have made to the machine translation (MT) training workflow to suit our needs and environment, our server environment and the MT Info Service that handles all translation requests and allows the integration of MT in our various localisation systems. We also present the results of our latest post-editing productivity test, where we measured the productivity gain for translators post-editing MT output versus translating from scratch. Our analysis of the data indicates the presence of a strong correlation between the amount of editing applied to the raw MT output by the translators and their productivity gain. In addition, within the last calendar year our system has processed over thirteen million tokens of documentation content of which we have a record of the performed post-editing. This has allowed us to evaluate the performance of our MT engines for the different languages across our product portfolio, as well as spotlight potential issues with MT in the localisation process.
This tutorial is for people who are beginning to evaluate how well machine translation will fit their needs or who are curious to know more about how it is used. We assume no previous knowledge of machine translation. We focus on background knowledge that will help you both get more out of the rest of AMTA2010 and to make better decisions about how to invest in machine translation. Past participants have ranged from tech writers and freelance translators who want to keep up to date to VPs and CEOs who are evaluating technology strategies for their organizations. The main topics for discussion are common FAQs about MT (Can machines really translate? Can we fire our translators now?) and limitations (Why is the output so bad? What is MT good for?), workflow (Why buy MT if it{'}s free on the internet? What other kinds of translation automation are there? How do we use it?), return on investment (How much does MT cost? How can we convince our bosses to buy MT?), and steps to deployment (Which MT system should we buy? What do we do next?).
This session will cover how to increase localization efficiency with a SYSTRAN desktop product and a server solution. First we will demonstrate how to integrate MT in a localization workflow, interaction with TM matching tools, hands-on MT customization using various tools and dictionaries, and final post-edition using SYSTRAN Premium Translator, a desktop product. We will also walk through the complete cycle of automatic quality improvement using SYSTRAN Training Server, part of the Enterprise Server 7 suite. It covers managing bilingual and monolingual data using Corpus Manager, training hybrid or statistical translation models with Training Manager, and evaluating quality using automatic scoring and side-by-side translation comparison. It also includes other useful tools that automatically extract and validate dictionary entries, and create TMs from unaligned bilingual sentences automatically. Finally, localization efficiency with or without MT integration/customization is compared with the actual cost benefits.
Arabic poses many interesting challenges to machine translation: ambiguous orthography, rich morphology, complex morpho-syntactic behavior, and numerous dialects. In this tutorial, we introduce the most important themes of challenges and solutions for people working on translation from/to Arabic or any of its dialects. The tutorial is intended for researchers and developers working on MT. The discussion of linguistic issues and how they are addressed in MT will help linguists and professional translators understand the issues machine translation faces when dealing with Arabic and other morphologically rich languages. The tutorial does not expect the attendees to be able to speak/read/write Arabic.
This tutorial will present a survey of how machine translation is integrated into current CAT tools and illustrate how the technology can be used appropriately and profitably by the professional translator.
If you are interested in open-source machine translation but lack hands-on experience, this is the tutorial for you! We will start with background knowledge of statistical machine translation and then walk you through the process of installing and running an SMT system. We will show you how to prepare input data, and the most efficient way to train and use your translation systems. We shall also discuss solutions to some of the most common issues that face LSPs when using SMT, including how to tailor systems to specific clients, preserving document layout and formatting, and efficient ways of incorporating new translation memories. Previous years{'} participants have included software engineers and managers who need to have a detailed understanding of the SMT process. This is a fast-paced, hands-on tutorial that will cover the skills you need to get you up and running with open-source SMT. The teaching will be based on the Moses toolkit, the most popular open-source machine translation software currently available. No prior knowledge of MT is necessary, only an interest in it. A laptop is required for this tutorial, and you should have rudimentary knowledge of using the command line on Windows or Linux.
Several studies have recently reported significant productivity gains by human translators when besides translation memory (TM) matches they do also receive suggestions from a statistical machine translation (SMT) engine. In fact, an increasing number of language service providers and in-house translation services of large companies is nowadays integrating SMT in their workflow. The technology transfer of state-of-the-art SMT technology from research to industry has been relatively fast and simple also thanks to development of open source software, such as MOSES, GIZA++, and IRSTLM. While a translator is working on a specific translation project, she evaluates the utility of translating versus post-editing a segment based on the adequacy and fluency provided by the SMT engine, which in turn depends on the considered language pair, linguistic domain of the task, and the amount of available training data. Statistical models, like those employed in SMT, rely on a simple assumption: data used to train and tune the models represent the target translation task. Unfortunately, this assumption cannot be satisfied for most of the real application cases, simply because for most of the language pairs and domains there is no sufficient data to adequately train an SMT system. Hence, common practice is to train SMT systems by merging together parallel and monolingual data from the target domain with as much as possible data from any other available source. This workaround is simple and gives practical benefits but is often not the best way to exploit the available data. This tutorial copes with the optimal use of in-domain and out-of-domain data to achieve better SMT performance on a given application domain. Domain adaptation, in general, refers to statistical modeling and machine learning techniques that try to cope with the unavoidable mismatch between training and task data that typically occurs in real life applications. Our tutorial will survey several application cases in which domain adaptation can be applied, and presents adaptation techniques that best fit each case. In particular, we will cover adaptation methods for n-gram language models and translation models in phrase-based SMT. The tutorial will provide some high-level theoretical background in domain adaptation, it will discuss practical application cases, and finally show how the presented methods can be applied with two widely used software tools: Moses and IRSTLM. The tutorial is suited for any practitioner of statistical machine translation. No particular programming or mathematical background is required.
The paper presents a novel technique for speech translation using hierarchical phrased-based statistical machine translation (HPB-SMT). The system is based on translation of speech from phone sequences as opposed to conventional approach of speech translation from word sequences. The technique facilitates speech translation by allowing a machine translation (MT) system to access to phonetic information. This enables the MT system to act as both a word recognition and a translation component. This results in better performance than conventional speech translation approaches by recovering from recognition error with help of a source language model, translation model and target language model. For this purpose, the MT translation models are adopted to work on source language phones using a grapheme-to-phoneme component. The source-side phonetic confusions are handled using a confusion network. The result on IWLST'10 English- Chinese translation task shows a significant improvement in translation quality. In this paper, results for HPB-SMT are compared with previously published results of phrase-based statistical machine translation (PB-SMT) system (Baseline). The HPB-SMT system outperforms PB-SMT in this regard.
Aligning a sequence of words to one of its infrequent translations is a difficult task. We propose a simple and original solution to this problem that yields to significant gains over a state-of-the-art transpotting task. Our approach consists in aligning non parallel sentences from the training data in order to reinforce online the alignment models. We show that using only a few pairs of non parallel sentences allows to improve significantly the alignment of infrequent translations.
Discriminative training for MT usually involves numerous features and requires large-scale training set to reach reliable parameter estimation. Other than using the expensive human-labeled parallel corpora for training, semi-supervised methods have been proposed to generate huge amount of {``}hallucinated{''} data which relieves the data sparsity problem. However the large training set contains both good samples which are suitable for training and bad ones harmful to the training. How to select training samples from vast amount of data can greatly affect the training performance. In this paper we propose a method for selecting samples that are most suitable for discriminative training according to a criterion measuring the dataset quality. Our experimental results show that by adding samples to the training set selectively, we are able to exceed the performance of system trained with the same amount of samples selected randomly.
In this paper, we introduce a simple technique for incorporating domain information into a statistical machine translation system that significantly improves translation quality when test data comes from multiple domains. Our approach augments (conjoins) standard translation model and language model features with domain indicator features and requires only minimal modifications to the optimization and decoding procedures. We evaluate our method on two language pairs with varying numbers of domains, and observe significant improvements of up to 1.0 BLEU.
This paper defines a method for lexicon in the biomedical domain from comparable corpora. The method is based on compositional translation and exploits morpheme-level translation equivalences. It can generate translations for a large variety of morphologically constructed words and can also generate {'}fertile{'} translations. We show that fertile translations increase the overall quality of the extracted lexicon for English to French translation.
As machine translation quality continues to improve, the idea of using MT to assist human translators becomes increasingly attractive. In this work, we discuss and provide empirical evidence of the challenges faced when adapting traditional MT systems to provide automatic translations for human post-editors to correct. We discuss the differences between this task and traditional adequacy-based tasks and the challenges that arise when using automatic metrics to predict the amount of effort required to post-edit translations. A series of experiments simulating a real-world localization scenario shows that current metrics under-perform on this task, even when tuned to maximize correlation with expert translator judgments, illustrating the need to rethink traditional MT pipelines when addressing the challenges of this translation task.
When parallel or comparable corpora are harvested from the web, there is typically a tradeoff between the size and quality of the data. In order to improve quality, corpus collection efforts often attempt to fix or remove misaligned sentence pairs. But, at the same time, Statistical Machine Translation (SMT) systems are widely assumed to be relatively robust to sentence alignment errors. However, there is little empirical evidence to support and characterize this robustness. This contribution investigates the impact of sentence alignment errors on a typical phrase-based SMT system. We confirm that SMT systems are highly tolerant to noise, and that performance only degrades seriously at very high noise levels. Our findings suggest that when collecting larger, noisy parallel data for training phrase-based SMT, cleaning up by trying to detect and remove incorrect alignments can actually degrade performance. Although fixing errors, when applicable, is a preferable strategy to removal, its benefits only become apparent for fairly high misalignment rates. We provide several explanations to support these findings.
In this paper, we investigate large-scale lightly-supervised training with a pivot language: We augment a baseline statistical machine translation (SMT) system that has been trained on human-generated parallel training corpora with large amounts of additional unsupervised parallel data; but instead of creating this synthetic data from monolingual source language data with the baseline system itself, or from target language data with a reverse system, we employ a parallel corpus of target language data and data in a pivot language. The pivot language data is automatically translated into the source language, resulting in a trilingual corpus with unsupervised source language side. We augment our baseline system with the unsupervised source-target parallel data. Experiments are conducted for the German-French language pair using the standard WMT newstest sets for development and testing. We obtain the unsupervised data by translating the English side of the English-French 109 corpus to German. With careful system design, we are able to achieve improvements of up to +0.4 points BLEU / -0.7 points TER over the baseline.
We propose interpolated backoff methods to strike the balance between traditional surface form translation models and factored models that decompose translation into lemma and morphological feature mapping steps. We show that this approach improves translation quality by 0.5 BLEU (German{--}English) over phrase-based models, due to the better translation of rare nouns and adjectives.
In this paper, we discuss the development of statistical machine translation for English to/from White Hmong (Language code: mww). White Hmong is a Hmong-Mien language, originally spoken mostly in Southeast Asia, but now predominantly spoken by a large diaspora throughout the world, with populations in the United States, Australia, France, Thailand and elsewhere. Building statistical translation systems for Hmong proved to be incredibly challenging since there are no known parallel or monolingual corpora for the language; in fact, finding data for Hmong proved to be one of the biggest challenges to getting the project off the ground. It was only through a close collaboration with the Hmong community, and active and tireless participation of Hmong speakers, that it became possible to build up a critical mass of data to make the translation project a reality. We see this effort as potentially replicable for other severely resource poor languages of the world, which is likely the case for the majority of the languages still spoken on the planet. Further, the work here suggests that research and work on other severely under-resourced languages can have significant positive impacts for the affected communities, both for accessibility and language preservation.
In this paper, we propose a novel lattice-based MT combination methodology that we call Target-to-Target Decoding (TTD). The combination process is carried out as a {``}translation{''} from backbone to the combination result. This perspective suggests the use of existing phrase-based MT techniques in the combination framework. We show how phrase extraction rules and confidence estimations inspired from machine translation improve results. We also propose system-specific LMs for estimating N-gram consensus. Our results show that our approach yields a strong improvement over the best single MT system and competes with other state-of-the-art combination systems.
In an ideal cross-lingual information retrieval (CLIR) system, a user query would generate a search over documents in a different language and the relevant results would be presented in the user{'}s language. In practice, CLIR systems are typically evaluated by judging result relevance in the document language, to factor out the effects of translating the results using machine translation (MT). In this paper, we investigate the influence of four different approaches for integrating MT and CLIR on both retrieval accuracy and user judgment of relevancy. We create a corpus with relevance judgments for both human and machine translated results, and use it to quantify the effect that MT quality has on end-to-end relevance. We find that MT errors result in a 16-39{\%} decrease in mean average precision over the ground truth system that uses human translations. MT errors also caused relevant sentences to appear irrelevant {--} 5-19{\%} of sentences were relevant in human translation, but were judged irrelevant in MT. To counter this degradation, we present two hybrid retrieval models and two automatic MT post-editing techniques and show that these approaches substantially mitigate the errors and improve the end-to-end relevance.
We present a detailed analysis of a graph-based annotation strategy that we employed to annotate a corpus of 11,292 real-world English to Spanish automatic translations with relative (ranking) and absolute (adequate/non-adequate) quality assessments. The proposed approach, inspired by previous work in Interactive Evolutionary Computation and Interactive Genetic Algorithms, results in a simpler and faster annotation process. We empirically compare the method against a traditional, explicit ranking approach, and show that the graph-based strategy: 1) is considerably faster, and 2) produces consistently more reliable annotations.
Viewing machine translation as a structured classification problem has provided a gateway for a host of structured prediction techniques to enter the field. In particular, large-margin structured prediction methods for discriminative training of feature weights, such as the structured perceptron or MIRA, have started to match or exceed the performance of existing methods such as MERT. One issue with structured problems in general is the difficulty in obtaining fully structured labels, e.g., in machine translation, obtaining reference translations or parallel sentence corpora for arbitrary language pairs. Another issue, more specific to the translation domain, is the difficulty in online training of machine translation systems, since existing methods often require bilingual knowledge to correct translation output online. We propose a solution to these two problems, by demonstrating a way to incorporate binary-labeled feedback (i.e., feedback on whether a translation hypothesis is a {``}good{''} or understandable one or not), a form of supervision that can be easily integrated in an online manner, into a machine translation framework. Experimental results show marked improvement by incorporating binary feedback on unseen test data, with gains exceeding 5.5 BLEU points.
The IBM schemes use weighted cooccurrence counts to iteratively improve translation and alignment probability estimates. We argue that: 1) these cooccurrence counts should be combined differently to capture word correlation; 2) alignment probabilities adopt predictable distributions; and 3) consequently, no iteration is needed. This applies equally well to word-based and phrase-based approaches. The resulting scheme, dubbed HAL, outperforms the IBM scheme in experiments.
This paper introduces two novel approaches for extracting compact grammars for hierarchical phrase-based translation. The first is a combinatorial optimization approach and the second is a Bayesian model over Hiero grammars using Variational Bayes for inference. In contrast to the conventional Hiero (Chiang, 2007) rule extraction algorithm , our methods extract compact models reducing model size by 17.8{\%} to 57.6{\%} without impacting translation quality across several language pairs. The Bayesian model is particularly effective for resource-poor languages with evidence from Korean-English translation. To our knowledge, this is the first alternative to Hiero-style rule extraction that finds a more compact synchronous grammar without hurting translation performance.
In Machine Translation, it is customary to compute the model score of a predicted hypothesis as a linear combination of multiple features, where each feature assesses a particular facet of the hypothesis. The choice of a linear combination is usually justified by the possibility of efficient inference (decoding); yet, the appropriateness of this simple combination scheme to the task at hand is rarely questioned. In this paper, we propose an approach that replaces the linear scoring function with a non-linear scoring function. To investigate the applicability of this approach, we rescore n-best lists generated with a conventional machine translation engine (using a linear scoring function for generating its hypotheses) with a non-linear scoring function learned using the learning-to-rank framework. Moderate, though consistent, gains in BLEU are demonstrated on the WMT{'}10, WMT{'}11 and WMT{'}12 test sets.
We present a simple and effective infrastructure for domain adaptation for statistical machine translation (MT). To build MT systems for different domains, it trains, tunes and deploys a single translation system that is capable of producing adapted domain translations and preserving the original generic accuracy at the same time. The approach unifies automatic domain detection and domain model parameterization into one system. Experiment results on 20 language pairs demonstrate its viability.
This paper gives a detailed analysis of different approaches to adapt a statistical machine translation system towards a target domain using small amounts of parallel in-domain data. Therefore, we investigate the differences between the approaches addressing adaptation on the two main steps of building a translation model: The candidate selection and the phrase scoring. For the latter step we characterized the differences by four key aspects. We performed experiments on two different tasks of speech translation and analyzed the influence of the different aspects on the overall translation quality. On both tasks we could show significant improvements by using the presented adaptation techniques.
This paper shows how the disambiguation of discourse connectives can improve their automatic translation, while preserving the overall performance of statistical MT as measured by BLEU. State-of-the-art automatic classifiers for rhetorical relations are used prior to MT to label discourse connectives that signal those relations. These labels are used for MT in two ways: (1) by augmenting factored translation models; and (2) by using the probability distributions of labels in order to train and tune SMT. The improvement of translation quality is demonstrated using a new semi-automated metric for discourse connectives, on the English/French WMT10 data, while BLEU scores remain comparable to non-discourse-aware systems, due to the low frequency of discourse connectives.
The standard procedure to train the translation model of a phrase-based SMT system is to concatenate all available parallel data, to perform word alignment, to extract phrase pairs and to calculate translation probabilities by simple relative frequency. However, parallel data is quite inhomogeneous in many practical applications with respect to several factors like data source, alignment quality, appropriateness to the task, etc. We propose a general framework to take into account these factors during the calculation of the phrase-table, e.g. by better distributing the probability mass of the individual phrase pairs. No additional feature functions are needed. We report results on two well-known tasks: the IWSLT{'}11 and WMT{'}11 evaluations, in both conditions translating from English to French. We give detailed results for different functions to weight the bitexts. Our best systems improve a strong baseline by up to one BLEU point without any impact on the computational complexity during training or decoding.
This paper addresses the problem of reliably measuring productivity gains by professional translators working with a machine translation enhanced computer assisted translation tool. In particular, we report on a field test we carried out with a commercial CAT tool in which translation memory matches were supplemented with suggestions from a commercial machine translation engine. The field test was conducted with 12 professional translators working on real translation projects. Productivity of translators were measured with two indicators, post-editing speed and post-editing effort, on two translation directions, English{--}Italian and English{--}German, and two linguistic domains, legal and information technology. Besides a detailed statistical analysis of the experimental results, we also discuss issues encountered in running the test.
We present an approach for Hybrid Machine Translation, based on a Machine-Learning framework. Our method combines output from several source systems. We first define an extensible, total order on translations and use it to estimate a ranking on the sentence level for a given set of systems. We introduce and define the notion of joint, binarised feature vectors. We train an SVM-based classifier and show how its classification results can be used to create hybrid translations. We describe a series of oracle experiments on data sets from the WMT11 translation task in order to find an upper bound regarding the achievable level of translation quality. We also present results from first experiments with an implemented version of our system. Evaluation using NIST and BLEU metrics indicates that the proposed method can outperform its individual source systems. An interesting finding is that our approach allows to leverage good translations from otherwise bad systems as the translation quality estimation is based on sentence-level phenomena rather than corpus-level metrics. We conclude by summarising our findings and by giving an outlook to future work.
This paper investigates the usefulness of automatic machine translation metrics when analyzing the impact of source reformulations on the quality of machine-translated user generated content. We propose a novel framework to quickly identify rewriting rules which improve or degrade the quality of MT output, by trying to rely on automatic metrics rather than human judgments. We find that this approach allows us to quickly identify overlapping rules between two language pairs (English- French and English-German) and specific cases where the rules{'} precision could be improved.
Mismatches between training and test data are a ubiquitous problem for real SMT applications. In this paper, we examine a type of mismatch that commonly arises when translating from French and similar languages: available training data is mostly formal register, but test data may well be informal register. We consider methods for defining surface transformations that map common informal language constructions into their formal language counterparts, or vice versa; we then describe two ways to use these mappings, either to create artificial training data or to pre-process source text at run-time. An initial evaluation performed using crowd-sourced comparisons of alternate translations produced by a French-to-English SMT system suggests that both methods can improve performance, with run-time pre-processing being the more effective of the two.
We propose straightforward implementations of translation memory (TM) functionality for research purposes, using machine translation evaluation metrics as similarity functions. Experiments under various conditions demonstrate the effectiveness of the approach, but also highlight problems in evaluating the results using an MT evaluation methodology.
This paper describes a range of automatic and manual comparisons of phrase-based and syntax-based statistical machine translation methods applied to English-German and English-French translation of user-generated content. The syntax-based methods underperform the phrase-based models and the relaxation of syntactic constraints to broaden translation rule coverage means that these models do not necessarily generate output which is more grammatical than the output produced by the phrase-based models. Although the systems generate different output and can potentially be fruitfully combined, the lack of systematic difference between these models makes the combination task more challenging.
The technique of pruning phrase tables that are used for statistical machine translation (SMT) can achieve substantial reductions in bulk and improve translation quality, especially for very large corpora such at the Giga-FrEn. This can be further improved by conditioning each significance test on other phrase pair co-occurrence counts resulting in an additional reduction in size and increase in BLEU score. A series of experiments using Moses and the WMT11 corpora for French to English have been performed to quantify the improvement. By adhering strictly to the recommendations for the WMT11 baseline system, a strong reproducible research baseline was employed.
Most attempts at integrating word sense disambiguation with statistical machine translation have focused on supervised disambiguation approaches. These approaches are of limited use when the distribution of the test data differs strongly from that of the training data; however, word sense errors tend to be especially common under these conditions. In this paper we present different approaches to unsupervised word translation disambiguation and apply them to the problem of translating conversational speech under resource-poor training conditions. Both human and automatic evaluation metrics demonstrate significant improvements resulting from our technique.
This paper presents a detailed study of a method for morphology generalization and generation to address out-of-domain translations in English-to-Spanish phrase-based MT. The paper studies whether the morphological richness of the target language causes poor quality translation when translating out-of-domain. In detail, this approach first translates into Spanish simplified forms and then predicts the final inflected forms through a morphology generation step based on shallow and deep-projected linguistic information available from both the source and target-language sentences. Obtained results highlight the importance of generalization, and therefore generation, for dealing with out-of-domain data.
Content localisation via machine translation (MT) is a sine qua non, especially for international online business. While most applications utilise rule-based solutions due to the lack of suitable in-domain parallel corpora for statistical MT (SMT) training, in this paper we investigate the possibility of applying SMT where huge amounts of monolingual content only are available. We describe a case study where an analysis of a very large amount of monolingual online trading data from eBay is conducted by ALS with a view to reducing this corpus to the most representative sample in order to ensure the widest possible coverage of the total data set. Furthermore, minimal yet optimal sets of sentences/words/terms are selected for generation of initial translation units for future SMT system-building.
String comparison methods such as BLEU (Papineni et al., 2002) are the de facto standard in MT evaluation (MTE) and in MT system parameter tuning (Och, 2003). It is difficult for these metrics to recognize legitimate lexical and grammatical paraphrases, which is important for MT system tuning (Madnani, 2010). We present two methods to address this: a shallow lexical substitution technique and a grammar-driven paraphrasing technique. Grammatically precise paraphrasing is novel in the context of MTE, and demonstrating its usefulness is a key contribution of this paper. We use these techniques to paraphrase a single reference, which, when used for parameter tuning, leads to superior translation performance over baselines that use only human-authored references.
We have developed a two-stage machine translation (MT) system. The first stage consists of an automatically created pattern-based machine translation system (PBMT), and the second stage consists of a standard phrase-based statistical machine translation (SMT) system. We studied for the Japanese-English simple sentence task. First, we obtained English sentences from Japanese sentences using an automatically created Japanese-English pattern-based machine translation. We call the English sentences obtained in this way as {``}English{''}. Second, we applied a standard SMT (Moses) to the results. This means that we translated the {``}English{''} sentences into English by SMT. We also conducted ABX tests (Clark, 1982) to compare the outputs by the standard SMT (Moses) with those by the proposed system for 100 sentences. The experimental results indicated that 30 sentences output by the proposed system were evaluated as being better than those outputs by the standard SMT system, whereas 9 sentences output by the standard SMT system were thought to be better than those outputs by the proposed system. This means that our proposed system functioned effectively in the Japanese-English simple sentence task.
This paper presents a method to improve a word alignment model in a phrase-based Statistical Machine Translation system for a low-resourced language using a string similarity approach. Our method captures similar words that can be seen as semi-monolingual across languages, such as numbers, named entities, and adapted/loan words. We use several string similarity metrics to measure the monolinguality of the words, such as Longest Common Subsequence Ratio (LCSR), Minimum Edit Distance Ratio (MEDR), and we also use a modified BLEU Score (modBLEU). Our approach is to add intersecting alignment points for word pairs that are orthographically similar, before applying a word alignment heuristic, to generate a better word alignment. We demonstrate this approach on Indonesian-to-English translation task, where the languages share many similar words that are poorly aligned given a limited training data. This approach gives a statistically significant improvement by up to 0.66 in terms of BLEU score.
The performance of an SMT system heavily depends on the availability of large parallel corpora. Unavailability of these resources in the required amount for many language pair is a challenging issue. The required size of the resource involving morphologically rich and highly agglutinative language is essentially much more for the SMT systems. This paper investigates on some of the issues on enriching the resource for this kind of languages. Handling of inflectional and derivational morphemes of the morphologically rich target language plays important role in the enrichment process. Mapping from the source to the target side is carried out for the English-Manipuri SMT task using factored model. The SMT system developed shows improvement in the performance both in terms of the automatic scoring and subjective evaluation over the baseline system.
We aim to use statistical machine translation technology to correct grammar errors and style issues in monolingual text. Here, as a feasibility test, we focus on depassivization in German and we abstract from surface forms to parts of speech. Our results are not yet satisfactory but yield useful insights into directions for improvement.
As early as June 2003, the United States Army partnered with United States Joint Forces Command to review language requirements within the Army, and, to a lesser extent, the other United States Military Services. After review of missions that require language translation, in 2005 the Army completed an Analysis of Alternatives document, which served as an independent assessment of potential language translation alternatives: options and numerical assessments based on each option{'}s ability to address language translation requirements. Of the four identified alternatives (printed materials, government off the shelf, commercial off the shelf, and overarching program), incremental development of two-way speech and text translation software modules proved to be the most mission and cost effective. That same year, United States Department of Defense published the Defense Language Transformation Roadmap listing a requirement for a coherent, prioritized, and coordinated multi-language technology research, development and acquisition policy and program. Since 2005, the Army and the Joint Staff have validated requirements for machine foreign language translation capability. In the effort to develop a comprehensive machine foreign translation capability, the Army not only needs to enable software to handle one of the most complex systems that humans deal with, but we need to develop the architecture and processes to routinely produce and maintain this capability. The Army has made the initial effort, funding a machine foreign language translation program known as the Machine Foreign Language Translation System (MFLTS) Program. It is intended to be the overarching Army Program with Department of Defense interest to provide machine foreign language translation capabilities that meet language translation gaps. MFLTS will provide a basic communications and triage capability for speech and text translations and improve those capabilities as the technology advances. Capabilities are intended to be delivered through three configurations: over established networks (web based), in mobile (or desktop) configurations and on portable platforms (or man wearable microprocessors and/or handhelds). MFLTS software, as a mission enabler ported on other platforms and systems, will provide Joint, Allied/Coalition units and personnel with language translation capability within the full range of military operations. Most recently, the Army convened a Machine Foreign Language Translation System (MFLTS) General Office Steering Group (GOSG) in March 2012 and validated follow-on language, domain and technology required capabilities for the Army MFLTS Program beyond the initial capability scheduled for 2014.
Government agencies are investing in MT to boost production, but the future funding picture is uncertain. Decision makers (Congress, OMB, IC leadership) want evidence (quantitative/qualitative) of value for investments. Agencies can use positive ROIs to defend MT investment budgets, plans, and programs, but the information needs to be more than anecdotal.
The explosive growth of social media has led to a wide range of new challenges for machine translation and language processing. The language used in social media occupies a new space between structured and unstructured media, formal and informal language, and dialect and standard usage. Yet these new platforms have given a digital voice to millions of user on the Internet, giving them the opportunity to communicate on the first truly global stage {--} the Internet. Social media covers a broad category of communications formats, ranging from threaded conversations on Facebook, to microblog and short message content on platforms like Twitter and Weibo {--} but it also includes user-generated comments on YouTube, as well as the contents of the video itself, and even includes {`}traditional{'} blogs and forums. The common thread linking all of these is that the media is generated by, and is targeted at individuals. This talk will survey some of the most popular social media platforms, and identify key challenges in translating the content found in them {--} including dialect, code switching, mixed encodings, the use of {``}internet speak{''}, and platform-specific language phenomena, as well as volume and genre. In addition, we will talk about some of the challenges in analyzing social media from an operational point of view, and how language and translation issues influence higher-level analytic processes such as entity extraction, topic classification and clustering, geo-spatial analysis and other technologies that enable comprehension of social media. These latter capabilities are being adapted for social media analytics for US Government analysts under the support of the Technical Support Working Group at the US DoD, enabling translingual comprehension of this style of content in an operational environment.
In Developers producing language technology for under-resourced languages often find relatively little machine readable text for data required to train machine translation systems. Typically, the kinds of text that are most accessible for production of parallel data are news and news-related genres, yet the language that requires translation for analysts and decision-makers reflects a broad range of forms and contents. The proposed paper will describe an effort funded by the ODNI FLPO in which the Army Research Laboratory, assisted by MITRE language technology researchers, produced a Dari-English parallel corpus containing text in a variety of styles and genres that more closely resemble the kinds of documents needed by government users than do traditional news genres. The data production effort began with a survey of Dari documents catalogued in a government repository of material obtained from the field in Afghanistan. Because the documents in the repository are not available for creation of parallel corpora, the goal was to quantify the types of documents in the collection and identify their linguistic features in order to find documents that are similar. Document images were obtained from two sources: (1) the Preserving and Creating Access to Unique Afghan Records collection, an online resource produced by the University of Arizona Libraries and the Afghanistan Centre at Kabul University and (2) The University of Nebraska Arthur Paul Afghanistan Collection. For the latter, document images were obtained by camera capture of books and by selecting pdf images of microfiche records. A set of 1395 document page images was selected to provide 250,000 translated English words in 10 content domains. The images were transcribed and translated according to specifications designed to maximize the quality and usefulness of the data. The corpus will be used to create a Dari-English glossary, and an experiment will quantify improvements to Dari-English translation of multi-genre text when a generic Dari-English machine translation system is customized using the corpus. The proposed paper will present highlights from these efforts.
The government and the research community have strived for the past few decades to develop machine translation capabilities. Historically, DARPA took the lead in the grand challenge aiming at surpassing human translation quality. While we have made strides from rule based, to statistical and hybrid machine translation engines, we cannot rely solely on machine translation to overcome the language barrier and accomplish the mission. Machine Translation is often misunderstood or misplaced in the operational settings as expectations are unrealistic and optimization not achieved. With the increase in volume, variety and velocity of data, new paradigms are needed when choosing machine translation software and embedding it into a business process so as to achieve the operational goals. The talk will focus on the operational requirements and frame where, when and how to use machine translation. We will also outline some gaps and suggest new areas for research, development, and implementation.
Making the right connections hinges on linking data from disparate sources. Frequently the link may be a person or place, so something as simple as a mistranslated name will cause a search to miss relevant documents. To swiftly and accurately exploit a growing flood of foreign language information acquired for the defense of the nation, Intelligence Community (IC) linguists and analysts need assistance in both translation accuracy and productivity. The name translation and standardizing component of a Computer-Aided Translation (CAT) tool such as the Highlight language analysis suite ensures fast and reliable translation of names from Arabic, Dari, Farsi, and Pashto according to a number of government transliteration standards. Highlight improves efficiency and maximizes the utilization of scarce human resources.
This paper proposes some strategies and techniques for creating phrase-level user parallel corpora for Systran translation engine. Though not all strategies and techniques discussed here will apply to other translation engines, the concept will.
The purpose of this presentation is to discuss recent efforts within the government to address issues of evaluation and return on investment. Pressure to demonstrate value has increased with the growing amount of foreign language information available, with the variety of languages needing to be exploited, and with the increasing gaps between numbers of language-enabled people and the amount of work to be done. This pressure is only growing as budgets shrink, and as global development grows. Over the past year, the ODNI has led an effort to pull together different government stakeholders to determine some baseline standards for determining Return on Investment via task-based evaluation. Stakeholder consensus on major HLT tasks has involved examination of the different approaches to determining return on investment and how it relates use of HLT in the workflow. In addition to reporting on the goals and progress of this group, we will present future directions and invite community input.
It is common knowledge that translation is an ambiguous, 1-to-n mapping process, but to date, our community has produced no empirical estimates of this ambiguity. We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations for a sentence. Our findings show that naturally occurring sentences have billions of translations. Having access to such large sets of meaning-equivalent translations enables us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics using data from the most recent Open MT NIST evaluation and we discuss how HyTER representations can be used to inform a data-driven inquiry into natural language semantics.
Over the years, the government has translated reams of material, transcribed decades of audio, and processed years of text. Where is that material now? How valuable would it be to have that material available to push research and applications and to support foreign language training? Over 20 years ago, DARPA funded the Linguistic Data Consortium (LDC) at the University of Pennsylvania to collect, catalog, store and provide access to language resources. Since that time, the LDC has collected thousands of corpora in many different genres and languages. Although the government has access to the full range of LDC data through a community license, until recently corpora specific to government needs were usually deleted soon after they were created. In order to address the need for a government-only catalog and repository, the Government Catalog of Language Resources was funded through the ODNI, and an initial prototype has been built. The GCLR will be transferred to a government executive agent who will be responsible for making improvements, adding corpora, and maintaining and sustaining the effort. The purpose of this talk is to present the model behind GCLR, to demonstrate its purpose, and to invite attendees to contribute and use contents. Background leading up to the current version will be presented. Use cases of parallel corpora in teaching, technology development and language maintenance will also be covered. Learning from the LDC on how corpora are used, and linking with the LDC will be part of future directions to enable government applications to utilize these resources.
Translation memory (TM) software allows a user to leverage previously translated material in the form of parallel corpora to improve the quality, efficiency, and consistency of future translation work. Within the intelligence community (IC), one of the major bottlenecks in implementing TM systems is developing a relevant parallel corpus. In particular, the IC needs to explore methods of deploying open source corpora for use with TM systems in a classified setting. To address this issue we are devising automated metrics for comparing various corpora in order to predict their usefulness to serve as vaults for particular translation needs. The proposed methodology will guide the use of these corpora, as well as the selection and optimization of novel corpora. One of the critical factors in TM vault creation is optimizing the trade-off between vault size and domain-specificity. Although a larger corpus may be more likely to contain material that matches words or phrases in the material to be translated, there is a danger that some of the proposed matches may include translations that are inappropriate for a given context. If the material in the vault and the material to be translated cover similar domains, the matches provided by the vault may be more likely to occur in the appropriate context. To explore this trade-off we are developing and implementing computational similarity metrics (e.g., n-gram overlap, TF-IDF) for comparison of corpora covering 12 different domains. We are also examining summary statistics produced by TM systems to test the degree to which material from each domain serves as a useful vault for translating material from each of the other domains, as well as the degree to which vault size improves the number and quality of proposed matches. The results of this research will help translation managers and other users assess the utility of a given parallel corpus for their particular translation needs, and may ultimately lead to improved tagging within TM systems to help translators identify the most relevant matches. Use of open source materials allows tool developers and users to leverage existing corpora, thus holding the promise of driving down costs of vault creation and selection. Optimizing vaults also promises to improve the quality, efficiency, and consistency of translation processes and products.
Online communications are playing an unprecedented role in propelling the revolutionary changes that are sweeping throughout the Middle East. A significant portion of that communication is in Romanized Arabic chat (Arabizi), which uses a combination of numerals and Roman characters, as well as non-Arabic words, to write Arabic in place of conventional Arabic script. Language purists in the Arabic-speaking world are lamenting that the use of Arabizi is becoming so profound that it is {``}destroying the Arabic language.{''} Despite its widespread use, and significant effect on emerging societies, Government agencies and others have been unable to extract any useful data from Arabizi because of its unconventional characteristics. Therefore, they have had to rely on human, computer-savvy translators, who often are a burden on dwindling resources, and are easily overwhelmed by the sheer volume of incoming data. Our presentation will explore the challenges of triaging and analyzing the Romanized Arabic format and describe Basis Technology{'}s Arabic chat translation software. This system will convert, for instance, mo2amrat, mo2amaraat, or mou{'}amret to مؤامرات. The output of standard Arabic can then be exploited for relevant information with a full set of other tools that will index/search, carry out linguistic analyses, extract entities, translate/transliterate names, and machine translate from the Arabic into English or other languages. Because of the nature of Arabizi {--} writers are able to express themselves in their native Arabic dialects, something that is not so easily done with Modern Standard Arabic {--} there is a bonus feature in that now we are also able to identify the probable geographical origins of each writer, something that is of great intelligence value. Looking at real-world scenarios, we will discuss how the chat translator can be built into solutions for users to overcome technological, linguistic, and cultural obstacles to achieve operational success and complete tasks.
We present the Reverse Palladius (RevP) program developed by the Air Force Research Laboratory's Speech and Communication Research, Engineering, Analysis, and Modeling (SCREAM) Laboratory for the National Air and Space Intelligence Center (NASIC). The RevP program assists the linguist in correcting the transliteration of Mandarin Chinese names during the Russian to English translation process. Chinese names cause problems for transliteration, because Russian writers follow a specific Palladius mapping for Chinese sounds. Typical machine translation of Russian into English then applies standard transliteration of the Russian sounds in these names, producing errors that require hand-correction. For example, the Chinese name Zhai Zhigang is written in Cyrillic as Чжай Чжиган, and standard transliteration via Systran renders this into English as Chzhay Chzhigan. In contrast, the RevP program uses rules that reverse the Palladius mapping, yielding the correct form Zhai Zhigang. When using the RevP program, the linguist opens a Russian document and selects a Chinese name for transliteration. The rule-based algorithm proposes a reverse Palladius transliteration, as well as a stemmed option if the word terminates in a possible Russian inflection. The linguist confirms the appropriate version of the name, and the program both corrects the current instance and stores the information for future use. The resulting list of name mappings can be used to pre-translate names in new documents, either via stand-alone operation of the RevP program, or through compilation of the list as a Systran user dictionary. The RevP program saves time by removing the need for post-editing of Chinese names, and improves consistency in the translation of these names. The user dictionary becomes more useful over time, further reducing the time required for translation of new documents.
This paper describes the role of machine translation (MT) for multilingual information access, a service that is desired by digital libraries that wish to provide cross-cultural access to their collections. To understand the performance of MT, we have developed HeMT: an integrated multilingual evaluation platform (http://txcdk-v10.unt.edu/HeMT/) to facilitate human evaluation of machine translation. The results of human evaluation using HeMT on three online MT services are reported. Challenges and benefits of crowdsourcing and collaboration based on our experience are discussed. Additionally, we present the analysis of the translation errors and propose Multi-engine MT strategies to improve translation performance.
How to achieve the optimal balance of quality and cost when the need for translation is sky-rocketing? Can machine translation be the solution? What system to choose? Finding the right MT solution for your organization is not easy. In this paper, we would like to share our experience at Nikon Precision Inc. in quest of the right tool, focusing on rule-based Japanese MT software and the results of a small pilot project, together with our plans for the future and the challenges we are facing.
Despite the growth of statistical machine translation (SMT) research and development in recent years, it remains somewhat out of reach for the translation community where programming expertise and knowledge of statistics tend not to be commonplace. While the concept of SMT is relatively straightforward, its implementation in functioning systems remains difficult for most, regardless of expertise. More recently, however, developments such as SmartMATE have emerged which aim to assist users in creating their own customized SMT systems and thus reduce the learning curve associated with SMT. In addition to commercial uses, translator training stands to benefit from such increased levels of inclusion and access to state-of-the-art approaches to MT. In this paper we draw on experience in developing and evaluating a new syllabus in SMT for a cohort of post-graduate student translators: we identify several issues encountered in the introduction of student translators to SMT, and report on data derived from repeated measures questionnaires that aim to capture data on students{'} self-efficacy in the use of SMT. Overall, results show that participants report significant increases in their levels of confidence and knowledge of MT in general, and of SMT in particular. Additional benefits {--} such as increased technical competence and confidence {--} and future refinements are also discussed.
This paper reports on a project whose aims are to investigate the usability of raw machine translated technical support documentation for a commercial online file storage service. Following the ISO/TR 16982 definition of usability - goal completion, satisfaction, effectiveness, and efficiency - comparisons are drawn for all measures between the original user documentation written in English for a well-known online file storage service and raw machine translated output in four target languages: Spanish, French, German and Japanese. Using native speakers for each language, we found significant differences between the source and MT output for three out of the four measures: goal completion, efficiency and user satisfaction. This leads to a tentative conclusion that there is a difference in usability between well-formed content and raw machine translated content, and we suggest avenues for further work.
All types of Machine Translation technologies have pros and cons. At PayPal, we have been working with MT for 3 years (2 of them in a production environment). The aim of this paper is to share our experience and discuss strengths and weaknesses for Rule-based Machine Translation, Statistical Machine Translation and Hybrid Machine Translation. We will also share pointers for successful implementation of any of these technologies.
Statistical Machine Translation (SMT) systems specialized for one domain often perform poorly when applied to other domains. Domain adaptation techniques allow SMT models trained from a source domain with abundant data to accommodate different target domains with limited data. This paper evaluates the performance of two adaptive techniques based on log-linear and mixture models on data from the legal domain in real-world settings. Performance evaluation includes post-editing time and effort required by a professional post-editor to improve the quality of machine-generated translations to meet industry standards, as well as traditional automated scoring techniques (BLEU scores). Results indicates that the domain adaptation techniques can yield a significant increase in BLEU score (up to three points) and a significant reduction in post-editing time of about one second per word in an operational environment.
Machine translation resurfaced as a viable business solution about 5 years ago, with much hype. With the amount of content requiring translation, and a mellowing of user expectations about translation quality, it seemed there was real business value in developing machine translation solutions. Since then, however, the discounts offered to enterprise customers have remained stubbornly meager in the 10-20{\%} range, with high, up-front costs{---}far from the anticipated savings. This paper provides an overview of the challenges encountered in the value chain between customer and Language Service Provider (LSP) which keep translation costs high and limit machine translation adoption, discusses existing and potential solutions to these challenges, and offers suggestions on how to enlist the support of the LSP and freelance translator community to address these challenges.
This paper presents a case-study of work done by Applied Language Solutions (ALS) for a large social networking provider who claim to have built the world{'}s first multi-language social network, where Internet users from all over the world can communicate in languages that are available in the system. In an initial phase, the social networking provider contracted ALS to build Machine Translation (MT) engines for twelve language-pairs: Russian⇔English, Russian⇔Turkish, Russian⇔Arabic, Turkish⇔English, Turkish⇔Arabic and Arabic⇔English. All of the input data is user-generated content, so we faced a number of problems in building large-scale, robust, high-quality engines. Primarily, much of the source-language data is of {`}poor{'} or at least {`}non-standard{'} quality. This comes in many forms: (i) content produced by non-native speakers, (ii) content produced by native speakers containing non-deliberate typos, or (iii) content produced by native speakers which deliberately departs from spelling norms to bring about some linguistic effect. Accordingly, in addition to the {`}regular{'} pre-processing techniques used in the building of our statistical MT systems, we needed to develop routines to deal with all these scenarios. In this paper, we describe how we handle shortforms, acronyms, typos, punctuation errors, non-dictionary slang, wordplay, censor avoidance and emoticons. We demonstrate automatic evaluation scores on the social network data, together with insights from the the social networking provider regarding some of the typical errors made by the MT engines, and how we managed to correct these in the engines.
Managing large scale MT post-editing projects is a challenging endeavor. From securing linguists buy-in to ensuring consistency of the output, it is important to develop a set of specific processes and tools that facilitate this task. Drawing from years of experience in such projects, we will attempt here to describe the challenges associated to the management of such projects and to define best practices.
This document introduces the strategy implemented at CA Technologies to exploit Machine Translation (MT) at the corporate-wide level. We will introduce the different approaches followed to further improve the quality of the output of the machine translation engine once the engines have reached a maximum level of customization. Senior team support, clear communication between the parties involved and improvement measurement are the key components for the success of the initiative.
In this paper, we present SAIC{'}s hybrid machine translation (MT) system and show how it was adapted to the needs of our customer {--} a major global fashion company. The adaptation was performed in two ways: off-line selection of domain-relevant parallel and monolingual data from a background database, as well as on-line incremental adaptation with customer parallel and translation memory data. The translation memory was integrated into the statistical search using two novel features. We show that these features can be used to produce nearly perfect translations of data that fully or to a large extent partially matches the TM entries, without sacrificing on the translation quality of the data without TM matches. We also describe how the human post-editing effort was reduced due to significantly better MT quality after adaptation, but also due to improved formatting and readability of the MT output.
Machine Translation (MT) is said to be the next lingua franca. With the evolution of new technologies and the capacity to produce a humungous number of written digital documents, human translators will not be able to translate documentation fast enough. However, some applications require a level of quality that is still beyond that provided by MT. Thanks to the increased capacity of communication provided by new technologies, people can also interact and collaborate to work remotely. With this, crowd computing is becoming more common and it has been proposed as a feasible solution for translation. In this paper, we discuss about the relationship between crowdsourcing and MT, and the main challenges for the MT community to multiply the potential of the crowd.
Ford Motor Company is at the forefront of the global economy and with this comes the need for communicating with regional manufacturing staff and plant employees in their own languages. Asian employees, in particular, do not necessarily learn English as a second language as is often the case in European countries, so manufacturing systems are now mandated to support local languages. This support is required for plant floor system applications where static data (labels, menus, and messages) as well as dynamic data (user entered controlled and free text) is required to be translated from/to English and the local languages. This facilitates commonization of business methods where best practices can be shared globally between plant and staff members. In this paper and presentation, we will describe our experiences in bringing Machine Translation technology to a large multinational corporation such as Ford and discuss the lessons we learned as well as both the successes and failures we have experienced.
The Church of Jesus Christ of Latter-day Saints undertook an extensive effort at the beginning of this year to deploy machine translation (MT) in the translation workflow for the content on its principal website, www.lds.org. The objective of this effort is to reduce by at least 50{\%} the time required by human translators to translate English content into nine other languages and publish it on this site. This paper documents the experience to date, including selection of the MT system, preparation and use of data to customize the system, initial deployment of the system in the Church{'}s translation workflow, post-editing training for translators, the resulting productivity improvements, and plans for future deployments.
We describe a system to rapidly generate high-quality closed captions and subtitles for live broadcasted TV shows, using automated components, namely Automatic Speech Recognition and Machine Translation. The human stays in the loop for quality assurance and optional post-editing. We also describe how the system feeds the human edits and corrections back into the different components for improvement of these components and with that of the overall system. We finally describe the operation of this system in a real life environment within a broadcast network, where we implemented the system to transcribe, process broadcast transmissions and generate high-quality closed captions in Arabic and translate these into English subtitles in short time.
This paper reports on three business opportunities encountered by Spoken Translation, Inc., a developer of software systems for automatic spoken translation: (1) a healthcare organization needing improved communications between limited-English patients and their caregivers; (2) a networking and communications firm aiming to add UN-style simultaneous interpreting to their telepresence facilities; and (3) the retail arm of a device manufacturer hoping to enable more effective in-store consulting for customers with imperfect command of an outlet's native language. None of these openings has yet led to substantial business, but one remains in negotiation. We describe how the business introductions came to us; the proposed use cases; demonstrations, presentations, tests, etc.; and issues/challenges. We also comment on early consumer-oriented products for spoken language translation. The aim is to provide a snapshot of one company's business possibilities and challenges at the dawn of the era of automatic interpreting.
Intellectual Property professionals frequently need to carry out patent searches for a variety of reasons. During a typical search, they will retrieve approximately 30{\%} of their results in a foreign language. The machine translation (MT) options currently available to patent searchers for these foreign-language patents vary in their quality, consistency, and general level of service. In this article, we introduce IPTranslator; an MT web service designed to cater for the needs of patent searchers. At the core of IPTranslator is a set of MT systems developed specifically for translating patent text. We describe the challenges faced in adapting MT technology to such a complex domain, and how the systems were evaluated to ensure that the quality was fit for purpose. Finally, we present the framework through which the IPTranslator service is delivered to users, and the value-adding features which address many of the issues with existing solutions.
Discourse connectives can often signal multiple discourse relations, depending on their context. The automatic identification of the Arabic translations of seven English discourse connectives shows how these connectives are differently translated depending on their actual senses. Automatic labelling of English source connectives can help a machine translation system to translate them more correctly. The corpus-based analysis of Arabic translations also enables the definition of a connective-specific evaluation metric for machine translation, which is here validated by human judges on sample English/Arabic translation data.
A preliminary implementation of AraMWE, a hybrid project that includes a statistical component and a CCG symbolic component to extract and treat MWEs and idioms in Arabic and Eng- lish parallel texts is presented, together with a general sketch of the system, a thorough description of the statistical component and a proof of concept of the CCG component.
The translation quality of Statistical Machine Translation (SMT) depends on the amount of input data especially for morphologically rich languages. Farsi (Persian) language is such a language which has few NLP resources. It also suffers from the non-standard written characters which causes a large variety in the written form of each character. Moreover, the structural difference between Farsi and English results in long range reorderings which cannot be modeled by common SMT reordering models. Here, we try to improve the existing English-Farsi SMT system focusing on these challenges first by expanding our bilingual limited-domain corpus to an open-domain one. Then, to alleviate the character variations, a new text normalization algorithm is offered. Finally, some hand-crafted rules are applied to reduce the structural differences. Using the new corpus, the experimental results showed 8.82{\%} BLEU improvement by applying new normalization method and 9.1{\%} BLEU when rules are used.
In this paper, we study the problem of finding named entities in the Arabic text. For this task we present the development of our pipeline software for Arabic named entity recognition (ARNE), which includes tokenization, morphological analysis, Buckwalter transliteration, part of speech tagging and named entity recognition of person, location and organisation named entities. In our first attempt to recognize named entites, we have used a simple, fast and language independent gazetteer lookup approach. In our second attempt, we have used the morphological analysis provided by our pipeline to remove affixes and observed hence an improvement in our performance. The pipeline presented in this paper, can be used in future as a basis for a named entity recognition system that recognized named entites not only using gazetteers, but also making use of morphological information and part of speech tagging.
This paper discusses a hybrid approach to transliterating and matching Arabic names, as implemented in the DataFlux Quality Knowledge Base (QKB), a knowledge base used by data management software systems from SAS Institute, Inc. The approach to transliteration relies on a lexicon of names with their corresponding transliterations as its primary method, and falls back on PERL regular expression rules to transliterate any names that do not exist in the lexicon. Transliteration in the QKB is bi-directional; the technology transliterates Arabic names written in the Arabic script to the Latin script, and transliterates Arabic names written in the Latin script to Arabic. Arabic name matching takes a similar approach and relies on a lexicon of Arabic names and their corresponding transliterations, falling back on phonetic transliteration rules to transliterate names into the Latin script. All names are ultimately rendered in the Latin script before matching takes place. Thus, the technology is capable of matching names across the Arabic and Latin scripts, as well as within the Arabic script or within the Latin script. The goal of the authors of this paper was to build a software system capable of transliterating and matching Arabic names across scripts with an accuracy deemed to be acceptable according to internal software quality standards.
In this paper, we focus on the use of Arabic transliteration to improve the results of a linguistics-based word alignment approach from parallel text corpora. This approach uses, on the one hand, a bilingual lexicon, named entities, cognates and grammatical tags to align single words, and on the other hand, syntactic dependency relations to align compound words. We have evaluated the word aligner integrating Arabic transliteration using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the Moses statistical machine translation system. The obtained results show that Arabic transliteration improves the quality of both alignment and translation.
Research done on Arabic sentiment analysis is considered very limited almost in its early steps compared to other languages like English whether at document-level or sentence-level. In this paper, we test the effect of preprocessing (normalization, stemming, and stop words removal) on the performance of an Arabic sentiment analysis system using Arabic tweets from twitter. The sentiment (positive or negative) of the crawled tweets is analyzed to interpret the attitude of the public with regards to topic of interest. Using Twitter as the main source of data reflects the importance of the system for the Middle East region, which mostly speaks Arabic.
Improving speech recognition accuracy through linguistic knowledge is a major research area in automatic speech recognition systems. In this paper, we present a syntax-mining approach to rescore N-Best hypotheses for Arabic speech recognition systems. The method depends on a machine learning tool (WEKA-3-6-5) to extract the N-Best syntactic rules of the Baseline tagged transcription corpus which was tagged using Stanford Arabic tagger. The proposed method was tested using the Baseline system that contains a pronunciation dictionary of 17,236 vocabularies (28,682 words and variants) from 7.57 hours pronunciation corpus of modern standard Arabic (MSA) broadcast news. Using Carnegie Mellon University (CMU) PocketSphinx speech recognition engine, the Baseline system achieved a Word Error Rate (WER) of 16.04 {\%} on a test set of 400 utterances ( about 0.57 hours) containing 3585 diacritized words. Even though there were enhancements in some tested files, we found that this method does not lead to significant enhancement (for Arabic). Based on this research work, we conclude this paper by introducing a new design for language models to account for longer-distance constrains, instead of a few proceeding words.
We annotate a small corpus of religious Arabic with morphological segmentation boundaries and fine-grained segment-based part of speech tags. Experiments on both segmentation and POS tagging show that the religious corpus-trained segmenter and POS tagger outperform the Arabic Treebak-trained ones although the latter is 21 times as big, which shows the need for building religious Arabic linguistic resources. The small corpus we annotate improves segmentation accuracy by 5{\%} absolute (from 90.84{\%} to 95.70{\%}), and POS tagging by 9{\%} absolute (from 82.22{\%} to 91.26) when using gold standard segmentation, and by 9.6{\%} absolute (from 78.62{\%} to 88.22) when using automatic segmentation.
We present simple and effective methods for extracting comparable corpora and bilingual lexicons from Wikipedia. We shall exploit the large scale and the structure of Wikipedia articles to extract two resources that will be very useful for natural language applications. We build a comparable corpus from Wikipedia using categories as topic restrictions and we extract bilingual lexicons from inter-language links aligned with statistical method or a combined statistical and linguistic method.
