This paper introduces seven genres of the digital media projects that can help the 21st century EFL learner develop intercultural communication capability (ICC) in English with creativity afforded by emerging digital media. It is based on a project by the author and a group of pre-service teachers during the Fall semester of 2012-2013. Guided by a list of suggested topics and the concept of learner-centeredness, the team discovered many useful project ideas. Although communicative language teaching, taskbased language teaching, and multiple intelligences are all behind the applications, we found that when learner-centeredness is the guiding principle and ICC is the goal, possibilities are abundant. It is true that we are only limited by our imagination when it comes to teaching a language with digital media. The purpose of this workshop is not to exhaust all the possibilities, but to help the audience become aware of the availability of the many language learning projects which afford opportunities for the key principle and the goal. As new tools emerge, the particular digital media introduced might become obsolete, but the key concepts presented here will make sure that new tools will be used in a creative and meaningful way to support language learning. 
This study aimed to investigate the use of iPads as a learning tool for college-level EFL students and to explore these language learners’ perceptions of iPad reading. Drawn from an intermediate EFL reading class, three students with limited experiences of iPad reading participated in this study. Data from weekly journals and interviews showed that the iPads’ palm size, light weight, and accessibility to the Internet through wireless connections not only promoted mobile learning outside the classroom but also achieved learning goals inside the classroom. The various iPad applications enabled students to learn English through games and easy access to helpful resources and thereby increased their motivation to learn. Students also improved their communication skills by using iPads to create videos. iPads have provided useful opportunities for new literacy instruction. 
To speak fluently is a complex skill. In order to help the learner to acquire it we propose an electronic version of an age old method: pattern drills (PD). While being highly regarded in the ﬁfties, pattern drills have become unpopular since then. Despite certain shortcomings we do believe in the virtues of this approach, at least with regard to the memorization of basic structures and the acquisition of fluency, the skill to produce language at a 'normal' rate. Of course, the method has to be improved, and we will show here how this can be achieved. Unlike tapes or books, computers are open media, allowing for dynamic changes, taking users’ performances and preferences into account. Our drill-tutor, a small webapplication still in its prototype phase, allows for this. It is a free, electronic version of pattern drills, i.e. an exercise generator, open and adaptable to the users’ ever changing needs. 
Collocation learning is one of the important building blocks for the development of language competence. Remarkably, it is influenced by L1 and L2 congruency. The present study thus focused on the distinguishability of the computational similarity values between L2 collocates and L1 counterparts to establish the use of semantic similarity measure as a research instrument. The results showed that the inconsistency between human (subjective) and computational (objective) congruency classification of verb-noun collocations. 
Coxhead’s (2000) Academic Word List (AWL) has been frequently used in EAP classrooms and re-examined in light of various domain-specific corpora. Although well-received, the AWL has been criticized for ignoring the fact that words tend to show irregular distributions and be used in different ways across disciplines (Hyland and Tse, 2007). One such difference concerns collocations. Academic words (e.g. analyze) often co-occur with different words across domains and contain different meanings. What EAP students need is a “disciplinebased lexical repertoire” (p.235). Inspired by Hyland and Tse, we develop an online corpus-based tool, TechCollo, which is meant for EAP students to explore collocations in one domain or compare collocations across disciplines. It runs on textual data from six specialized corpora and utilizes frequency, traditional mutual information, and normalized MI (Wible et al., 2004) as measures to decide whether co-occurring word pairs constitute collocations. In this article we describe the current released version of TechCollo and how to use it in EAP studies. Additionally, we discuss a pilot study in which we used TechCollo to investigate whether the AWL words take different collocates in different domainspecific corpora. This pilot basically confirmed Hyland and Tse and demonstrates that many AWL words show uneven distributions and collocational differences across domains.  provide students with a list of academic vocabulary 1 irrespective of their specialized domain(s). There are two main reasons why academic vocabulary receives so much attention in EAP instruction. First, academic vocabulary accounts for a substantial proportion of words in academic texts (Nation, 2001). Sutarsyah et al. (1994), for example, found that about 8.4% of the tokens in the Learned and Scientific sections of the Lancaster-Oslo/Bergen (Johansson, 1978) and Wellington corpora (Bauer, 1993). Second, academic words very often are non-salient in written texts and less likely to be emphasized by content teachers in class (Flowerdew, 1993). Consequently, EAP researchers have been convinced that students need a complete list of academic vocabulary, and several lists were thus compiled. Among the attempts to collect academic lexical items, Coxhead’s (2000) Academic Word List (AWL) has been considered the most successful work to date. In the AWL, Coxhead offered 570 word families which were relatively frequent in a 3.5-milliontoken corpus of academic texts. The corpus was composed of writings from four disciplines: arts, commerce, law, and science. By considering certain selection principles such as frequency and range, Coxhead gathered a group of word families which were specialized in academic discourse and generalized across different fields of specialization. On average, the AWL accounted for 10% of Coxhead’s academic corpus and showed distributions of 9.1-12% of the four disciplines. Since its publishment, the AWL has been frequently used in EAP classes,  
We present an English miscollocation identification system based on dependency relations drawn from the Stanford parser. We test our system against a subset of error-tagged Chinese Learner English Corpus (CLEC)and obtain an overall precision of 0.75. We describe some applications and limitations of our system and suggest directions for future research. 1. Introduction Collocations play a very important role in second language learning (cf. Lewis, 1993). They reflect users’depth of vocabulary knowledge as well as their language proficiency levels (cf.Schimitt, 2000, 2010; Nation, 2001; Nation and Webb, 2011).Researchhas shown that collocations are one of the most significant feature which distinguishes native from non-native writings. Furthermore, non-native writers tend to makecollocation errors unconsciously, many of which arise from first language interference. All these suggest the necessity of developing a miscollocation identification system to help learners detect their collocation errors as well as raise their language awareness. .Such a system might also havegreat impact for second language acquisition (SLA) research, as collections and analyses of collocation errors are vital to our understanding of the difficulties and problems learners encounter (cf. Nesselhauf, 2005). Just like other errors in learner corpora, error-tagged miscollocations are not widely and readily accessible to researchers. Traditionally, miscollocations can only be identified viavery time-consuming process of manual error tagging. Thanks to recent advance in natural language processing (NLP), automatic identification of miscollocations has been made possible. This paper presents an English miscollocation identification system by drawing on NLP tools  and resources such as the Stanford parser, Google 1T ngrams, and WordNet. We will show that such a system not only has pedagogical valuebut also can facilitate the study of English miscollocations by non-native speakers. 2. Literature Review There are two approaches to the study of collocations, namely, the frequency-based approach (Sinclair, 1987) and the phraseological approach (Cowie, 1981; Benson, 1989). Drawing on natural language processing tools, researchers have proposed automated procedures to retrieve collocations from corpora by using statistical methods such as mutual information and t-score (Church and Hanks, 1990) as well as log likelihood ratio (Dunning , 1993).In addition to statistical measures, dependency relations derived from parsers play an important role in identifying collocations (cf. Church and Hanks, 1990; Smadja, 1993;Kilgarriff, 2004). (Jian, Chang, and Chang, 2003) present TANGO, a program which given a keyword and its part-of-speech can extract English example of four English collocation patterns (i.e. v-n, n-p, v-n-p, a-n) together with their Chinese translations from parallel corpora. (Shei and Pain, 2000) present a conceptual frameworkto detect and correct collocation errors by Chinese learners of English. They draw on a learner corpus, a reference corpus, a dictionary of synonyms derived from WordNet, and a paraphrase database compiled using learner data. Addressing the same problem of miscollocations caused by first language interference, (Chang et al., 2008) focus on the identification and correction of V-N miscollocations by Chinese learners of English. They extract V-N collocations from British National Corpus (BNC) and leaner corpora and use a bilingual English-Chinese dictionary to identify the meanings intended by the learners. They then use the collocations extracted from BNC to pinpoint the miscollocations in the learner corpora and suggest correct collocations  550 Copyright 2013 by Zhao-Ming Gao 27th Pacific Asia Conference on Language, Information, and Computation  pages 550555  PACLIC-27  which learners intended to use. (Futagi et al. 2008)notice that some collocation errors are in fact due to spelling errors. They use spelling checkers to identify and correct misspelled words. They then identify miscollocation candidates by part-of-speech tags and rank-ratio statistics calculated over 1 billion word corpus by native speakers. 3. Using Dependency Relations to Identify Collocations We follow the phraseological approach taken by(Cowie, 1981; Benson, 1989) and consider collocations a type of word combinations. As pointed out by Smadja (1993), many collocations involve pedicative relations such as subject-verb, verb-object, adjective-noun. These word combinations are easier to identify by using dependency parsers than statistical measures such as mutual information and t-score, which are useful to finding significant collocations and idioms. Our proposed miscollocation identification system is based on authentic English corpora of 14.5 million words. The system follows the lines of (Church, 1990; Smadja, 1993, Lin, 1998; Kilgarriff, 2004) in using parsers to retrieve collocations. Our approach consists of three major steps. The first step is to identify and correct spelling errors. The second step is.to identify and store the predicative relations (also known as dependency relations) occurring in the reference corpus in a dependency relation database The third step is to identify the dependency relations in a learner sentence and check them against the database of dependency relations derived from reference corpus. The technology underlying the system is similar to (Lin, 1998; Kilgarriff, 2004). To identify dependency relations in an English sentence, the Stanford parser is used (c.f.de Marneffe, 2006). Stanford parser can identify numerous dependency relations, including modifier-noun, subject-verb, verb-noun, etc. (1) is the output of the Stanford parser, which outputs the part-of-speech tags of each word in the sentence, its syntactic structures, and dependency relations. For example, the relationnn (prices-2, Stock-1) in (1)indicates that the first word ‘Stock’ modifies the second word ‘prices’ and form a N-N dependency relation. Similarly, the second word ‘prices’ and the third word ‘plunged’ form a subject-verb relation. (1) Stock prices plunged on many global markets Monday.  Stock/NNP prices/NNS plunged/VBD on/IN many/JJ global/JJ markets/NNS Monday/NNP  (ROOT (S (NP (NNP Stock) (NNS prices)) (VP (VBD plunged) (PP (IN on) (NP (JJ many) (JJ global) (NNS markets))) (NP (NNP Monday)))))  nn(prices-2, Stock-1) nsubj(plunged-3, prices-2) prep(plunged-3, on-4) amod(markets-7, many-5) amod(markets-7, global-6) pobj(on-4, markets-7) dobj(plunged-3, Monday-8)  The performance of the Stanford parser varies  with the complexity of the input sentence. If the  sentence is short and the structure is not  ambiguous or complicated, it can achieve  relatively high accuracy.  There are six major types of dependency  relations stored in our database, namely,  subject-verb, verb-object, verb-adverb,  noun-noun,  adjective-noun,  and  adverb-adjective.  We use two corpora. The first is a reference  corpus totaling 14.5 million words extracted  from authentic English texts (i.e. the reference  corpus). The second is an error-tagged learner  corpus used to evaluate the accuracy of our  system. The learner corpus is the subcorpus st2  in the Chinese Learner English Corpus (CLEC)  and totals 251558 tokens. Each sentence in the  reference corpus has been parsed by the Stanford  parser to extract the dependency relations.  Important dependency relations such as  subject-verb, verb-object, adjective-noun,  verb-adverb, and noun-noun are identified and  stored in the dependency relation database for  the reference corpus. The tables of.dependency  relation database include the information of  ahead word (the primary key in the database), its  part-of-speech, the dependency relation between  the headword and its collocation, the collocate  of the headword, as well as the part-of-speech of  the collocate. The part-of-speech information of  the keyword includes noun, verb, adjective,  adverb, and preposition. Nouns in the subject  and object positions are distinguished to  facilitate the retrieval of subject-verb and  verb-object relation. Preposition is included for  collocational patterns involving a verb and a  551  PACLIC-27  preposition (e.g. ‘rely on’) or a noun and a preposition (e.g. ‘under attack’). 4. Identifying Miscollocations A program is written which converts the dependency relation database into a collocation database. When a query is made, the program will search the collocation database, find all the collocations of the word in accordance with the conditions input by the user. Figure 1 is the interface of the collocation retrieval system. If the user inputs the keyword “responsibility”, “noun in the object position’ as its part-of-speech, and “verb” as the part-of-speech of the collocate, the system will return a list of potential verb collocates of the noun ‘responsibility’ such as: ‘take’, ‘shoulder’, ‘fulfill’, ‘bear’, ‘assume’, ‘accept’, ‘have’, ‘evade’, ‘shirk’. ‘avoid’. It should be noted that the frequency information and the dependency relations we use in our program are based on lemmas (i.e. the basic form of a word). For instance, take, took, taken, taking, takes all have the same lemma ‘take’. We use WordNet 3.1 for converting a word into its lemma. Following (Futagi, 2008), we identify and correct spelling errors in learner sentences in order to identify more miscollocations. We incorporate the open source spelling checker A spell and the information of language model based on the Google 1T ngram data. The correct spelling is chosen if the candidate word is the closest to the wrongly spelled word in terms of minimal edit distance and ngram probabilities. Figure 1. The Inteface of our collocation retrieval system  Each of the dependency relations extracted from learners’ sentences not involving a personal pronoun or a proper name is checked against our English collocation retrieval program. Personal pronouns and proper names are identified by using the part-of-speech tag information output by the Stanford parser. Dependency relations with these tags are directly ignored by our collocation checker. If a dependency relation in a learner sentence cannot be found in our English collocation database, it is considered a candidate of miscollocation.  5. Evaluations  We test our proposed system usingst2, a 251558 token subcorpus of the Chinese Learner English Corpus (CLEC)(cf. Gui and Yang, 2003), whose error tags facilitate automatic evaluation of our system. There are six types of collocation errors in the CLEC including CC1 (noun-noun), CC2 (noun-verb), CC3 (verb-noun), CC4 (adjective-noun), CC5 (verb-adverb), and CC6 (adverb-adjective). The precision rates of the six types of collocation errors are 0.77, 0.87, 0.72, 0.75, 0.83, and 0.63, respectively. Our system performs the best with CC2 (noun-verb), which has0.87 accuracy. The lowest precision is 0.63 found in CC6 (Adverb Adjective). The overall precision rate is about 0.75.  Table 1. Precision of our proposed method  CC CC CC CC CC CC  
With the growing popularity of Japanese learning, a large number of learning support tools or systems have been developed to help Japanese learners in various situations. We have particularly noticed the increasing necessity of systems developed as web applications, most of which are free and easily accessed, and hence regarded to be the most significant resources for Japanese learners. However, noun of the existing studies has considered the difference in language ability among Japanese learners. Learning contents and instructional method in these systems usually remain unchanged at all times without taking account of individual variations while in some cases they are supposed to vary with the real language ability of each Japanese learner. In this paper, we have developed a web application to provide appropriate suggestions and different learning materials for each Japanese learner based on their individual Japanese abilities. Specifically, we divide the language ability into several elements, propose different methods to quantify each element, and generate feedbacks or training questions for the Japanese learners. Experimental results have partially shown the effectiveness of our methods. 
Unlike competent human readers capable of inferring, tracing, and filling out gaps or hurdles left behind by authors' use of transformations in their writing such as permutation, addition, deletion, and substitution (PADS), these operations are challenging to computer readers and new foreign language learners. This paper reports a parser's use of a suite of NLP technologies - clause boundary detection, resolution of different anaphors, interevent relation finding, and case frame building - to fill out PADS gaps and output a much more explicit kernel-like meaning representation that includes case relation tuples of "Who Did What to Whom" and the inter-event relations based on conjoining, embedding, branching, insertion and apposition. According to Halliday and Hasan (1976), those gaps serve as cohesive devices to achieve better texture of the text organization. The transformations are ruled-based and they are important part of native speakers' competence. Though the rule-based parser is still short of perfection, the necessary design is in place and it has quite a few encouraging results. This report will also show the usefulness of PADS restoration technology in CALL and information extraction. Key words: English parser, PADS gaps, PADS restoration, case frame building, clause boundary detection, zero anaphor resolution, anaphor resolution, event relation finding, pronoun co-reference resolution, PP attachment, garden-path, information extraction, computer reading, meaning representation, CALL  
It has been disputed whether scalar implicatures (= SIs) arise globally or locally. Basically SIs should be global because they arise by comparing strengths of whole alternative statements. On the other hand, there are a lot of examples in which local SIs are preferable. Linguists like Chierchia (2002) and Fox (2006) even claim that SIs arise by applying an operator to syntactic constituents to get their stronger meanings. In this paper, I claim that SIs are global and seemingly local implicatures are effects of contexts on global implicatures. Moreover, I will show that no syntactic analyses work. 
The use of an obligatory numeral classifier (C) on N in general does not co-occur with mandatory plural marking (PM) (Greenberg 1990[1972], Sanches and Slobin 1973). Borer (2005) and Her (2012a) take this generalization further and see Cs and PMs as the same category. This unification implies that C/PM are mutually exclusive on N. In this paper, we first provide a mathematical foundation for this unification, i.e., C/PM both function as a multiplicand with the precise value of 1 (Her 2012a), and then explore empirically to what extent C/PM’s complimentary distribution is borne out. We obtain from the WALS database a total of 22 languages with both Cs and PMs, including Mandarin, Japanese, and Vietnamese. Our survey finds C/PM co-occurring on N in 11 languages. We then set out to formally account for the unification of C/PM and explain its exceptions, taking Mandarin as an example, with a double-headed classifier construction. This study thus adds merit to the unification of C/PM and concludes with its implication on a universal lexical count/mass distinction.  explain this generalization, Greenberg (1972) links the emergence of Cs in a language to its loss of plural markers (PMs), and as Peyraube (1998) observes, this is true for the rise of Cs in Chinese. However, this generalization is noncommittal on the complimentary distribution of Cs and PMs, as it says nothing about the cases where either C or PM is optional. Borer (2005:94) and Her (2012a:1682) take this generalization further and claim that Cs and PMs are the same category. The –s suffix in English, for example, applicable to all count nouns, is seen as a general classifier, similar to the Chinese ge in (1a) (Her 2012a:1682); the two thus share the same constituent structure, as in (2).  (1) a. 三 個 杯子 san ge beizi 3 C cup b. three cups  (2)  NumP  Num CLP  3 CL NP  
Head-Internal Relatives (HIRs) in Japanese are regarded as rich context-setters within Dynamic Syntax (DS): the propositional tree of the HIR clause is mapped onto a ‘partial’ tree, which establishes a rich context for the embedding clause to be parsed. This partial tree contains a situation node decorated with the Relevancy restriction and a node for an internal head. This account handles some new data and makes a novel prediction. Further, it is shown that the past DS analysis of HIRs in fact models change relatives (but not HIRs). 
Abstracts are quite useful when one is trying to understand the content of a paper, or conducting a survey with a large number of scientific documents. The situation is even clearer for the domain of social science, as most papers are very long and some of them don’t even have any abstracts at all. In this work, we narrow our attention down to the social scientific papers and try to generate their abstracts automatically. Specifically, we put weight on three points: important keywords, readability as an abstract, and features of social scientific papers. Experimental results show the effectiveness of our method, whereas some problems remain and will need to be solved in the future. 
The demand is increasing recently for nontask-oriented conversation system in various scenes. Previous studies provide various strategies to enrich the methods for generating utterances, thus making the conversation systems or agents appear more interesting. However, most previous works tend to rely on templates and therefore are not able to perform flexible conversationutterance generation. We propose here in this paper a thorough modification to a previous work to address this problem. Specifically, we introduce an automatic utterance generation in consideration of the embedded structure of sentences based on the principle of nominative maintenance. Moreover, emotion presumption has been implemented to add entertaining elements into the conversation with a user. The experimental results show that our approach proposed in this study has helped improve the performance of a conversation system. 
A key goal for participants in language communication is to bring about a mutually shared experience of ideas, event narratives, and emotional responses. This goal is achieved not only through the exchange of lexical meaning, but also through interactive signaling to coordinate information status. Our results show that prosodic synchrony (convergence) and dissynchrony (divergence) both occur in conversation, and that synchrony is achieved gradually as participants cooperate to build up a shared information and involvement state. Our analysis further indicates that feedback is a critical component of cooperative adaptation to new information, bringing about convergent speaker states. 
Studies of spontaneous conversational speech grounded on large and richly annotated corpora are still rare due to the scarcity of such resources. Comparative studies based on such resources are even more rarely found because of the extra-need of comparability in terms of content, genre and speaking style. The present paper presents our efforts for establishing such a dataset for two typologically diverse languages: French and Taiwan Mandarin. To the primary data, we added morphosyntactic, chunking, prosodic and discourse annotation in order to be able to carry out quantitative comparative studies of the syntaxdiscourse-prosody interfaces. We introduced our work on the data creation itself as well as some preliminary results of the boundary alignment between prosodic and discourse units and how POS and chunks are distributed on these boundaries. 
There has long been a growing interest in journal articles (JA) abstract writing, and this pervading interest has boosted the exigency for further instructive research. This current study aims to investigate both the variant application of the verb tense as well as the rhetorical structure within JA abstracts. A 9.9 million word corpus of 1000 JAs was collected based on four prestigious journals, i.e., Journal of Pragmatics, Journal of Research in Reading, Journal of Second Language Writing, and Reading and Writing, respectively. The quantitative analysis indicates the tendency of tense shown in the commonly applied reporting verbs. On the other hand, the qualitative analysis shows the prevailing adoption of three-, four-, and five-move theories in terms of the CARS model, the IMRD structure, and the IPMPrC structure. The results not only reveal the explicit tendency of the variance within reporting verbs but also suggest a distinct pervasiveness of the IMRD structure over the other models. These findings not only present a more systematic pattern within JA abstracts, but also show potentials for enlightening further pedagogy-oriented composition instruction for JA abstract. 1. Introduction Previous studies have highlighted the indispensable importance of JA abstract in the contemporary flow. The pivotal role of JA abstract has received considerable attention in academic written genre among the international community. Swales (1990) appeals to the academia, claiming that the research in JA abstracts ought not to be ignored inasmuch of its influential significance upon the genre  investigation and disciplinary discourse communities. As the knowledge of proficient preferences for language choice as well as rhetorical structure has a great influence on academic written genre, many investigators have recently turned to the relevant research in relation to genre analysis, thematic organization, formulaic language, rhetorical structure, etc. (Cortes, 2004; Hyland, 2008a; Lorés, 2004; Martín, 2002; Swales, 1990; Wang & Chan, 2011; Wang & Kao, 2012). Furthermore, research in terms of corpora decoding for rhetorical structures such as moves and steps is also regarded as one of the recommendations for further research expansion by Flowerdew (2010). Taking the contribution of the previous studies, this current reserach sets out to explore the variation of tense within the reporting verbs among the transitions of moves via the structural analysis in JA abstracts. 2. Literature review In the respect of genre analysis, move analysis has been always considered to be one of the most influential elements. A move is a rhetorical element which serves the function of correlating and cohering within the written or spoken context (Lorés , 2004; Swales, 2004). However, it is not a definite unit which is constraint to perform in a fixed pattern because it is able to vary along with the context. In other words, move functions as a communicative role between each transition of the rhetorical structure. A brief elaboration of the most pervasive move theory in terms of three-, four-, and fivemoves is described in sequence. 2.1. Three-move theory Create a Research Space (CARS) model, proposed by Swales (1990), has been widely used by scholars to outshine their publication in this competitive academia (Cheng, 2006). CARS model is divided into three moves,  102 Copyright 2013 by Pin-ning Tu and Shih-Ping Wang 27th Pacific Asia Conference on Language, Information, and Computation  pages 102107  PACLIC-27  including establishing a territory, establishing a niche, and occupying the niche (Swales, 1990). In other words, Move 1: establishing a territory can be commensurate with “goal” and “current capacity,” synthesizing the research aim with the previous research (Swales, 1990, p. 142). As a consequence, Move 2: establishing a niche functions as offering a space for research gap and possible research questions (Swales, 1990). Under this circumstance, Move 3: occupying the niche will provide a “solution of criteria of evaluation” that taps into the intricacies which came up with in Move 2 section (Swales, 1990, p. 142). 2.2. Four-move theory The most well-known and considerably applied structure in academic writing is the IMRD structure (i.e., Introduction, Methods, Results, and Discussion) (Golebiowski, 2009). It was first proposed by Ventola (1994). To illustrate the content in depth, the introduction segment would cover the further elaborations of the purpose and objective of the current research. Lorés (2004) additionally comments that any other questions that could possibly bring out further open discussion might also be included in this Introduction segment. When it comes to the second stage – Method, a clarification of the scheme adopted in the research will be described (Lorés, 2004). As the lines progress, the Result segment is expected to offer critical information in relation to the findings from the implement of the research (Lorés, 2004). The final Discussion section is required to contain a further discussion of the findings, an exploration of possible research space and practical application (Lorés, 2004). 2.3. Five-move theory Differing from the above discussed structures, the five-move theory – IPMPrC structure, proposed by Hyland (2004), is especially designed to access the RA abstracts. Nevertheless, it is clarified in the first place that the aim of setting this five move structure lies in providing an assertion in relation to JA abstract conducting as well as an inter-textual projection in terms of the significance of each research instead of addressing definite move steps (Hyland, 2004). In an attempt to provide a clearer framework of the main characteristics of the IPMPrC structure, Table 1 elaborates the primary functions of each move in the five-move theory (Hyland, 2004, p. 67).  Move Introduction Purpose Method Product Conclusion  Function Establishes context of the paper and motivates the study or discussion. Indicates purpose, thesis or hypothesis, outlines the intention behind the paper. Provides information on design, procedures, assumptions, approach, data, etc. States main findings or results, the argument, or what was accomplished. Interprets or extends results beyond scope of paper, draws inferences, points to applications or wider implications.  Table 1: IPMPrC structure  3. Methodology In an attempt to shed lights on the various dimensions that are possibly exposed from the academic written genre, a total of 1000 journal articles, which comprises 9,983,482 tokens out of 117,855 types of distinct words, were extracted evenly from four prestigious academic journals: Journal of Pragmatics (JOP), Journal of Research in Reading (JRR), Journal of Second Language Writing (JSLW), and Reading and Writing (R&W). In accordance with the principle aim of this current research, that is, to specify the variation of verb tense and rhetorical structure in JA abstracts, 1000 abstracts were additionally extracted from the retrieved research materials, and constructed as the primary research corpora. Table 2 compares the tokens and types of the five primary corpora.  Type of Corpora JOP abstracts corpus JRR abstracts corpus JSWL abstracts corpus R&W abstracts corpus 1,000 abstracts corpus  No. 250 250 250 250 1,000  Tokens 47,074 39,699 45,520 45,700 177,945  Types 6,068 4,001 4,265 3,679 9,711  Table 2: Comparison among research corpora  The data analysis is twofold. On the one hand, the quantitative analysis focuses on the investigation of verb tense, especially set out for reporting verbs, by manipulating the analytical instruments such as MonoConc Pro and WordSmith version 5.0. On the other hand, the qualitative analysis of consists in the assessment of the rhetorical structure in accordance with CARS model, the  103  PACLIC-27  IMRD structure, and the IPMPrC structure. Table 3 below illustrates the comparison of different transitions among the applied theories from three to five moves.  Three moves CARS model Context Gap Present study  Four moves IMRD structure Introduction Methods Results Discussion  Five moves IPMPrC structure Introduction Purpose Methods Product Conclusion  Table 3: Comparison of move theories  4. Results  The current study reports on two dimensions in JA abstracts: verb tense and rhetorical structure. Firstly, the analysis of verb has uncovered a prevailing application of be-verbs, such as is, are, was, were, as well as reporting verbs such as show, examine, suggest, investigate, and find, in applied frequency sequence. It is reasonably assumed that the different application pattern of be-verb contains two possibilities: plain statement as well as passive voice. Prior to tackling the various findings on the different tense of verbs, a clearer comparison of how be-verb is applied in each of the research corpora is shown in Table 4.  Form is are was were be total  JOP 526 329 75 83 192 1,205  JRR 200 206 300 374 110 1,190  JSLW 275 213 134 243 140 1,005  R&W 173 153 355 442 94 1,217  ALL 1204 901 864 1142 536 4,647  Table 4: Frequency of be-verbs  As can be seen from Table 4, it is apparent that JOP has its tendency to use present tense whereas JRR and R&W has similar tendency to apply past tense. This finding also reflects on the results obtained from the reporting verbs, as shown in Table 5.  Form show -s -ing  JOP JRR JSLW R&W ALL  63 21 32 19 135  22 8  11  2  43  3  6  
With the popularity of network applications, new words become more common and bring the poor performance of natural language processing related applications including web search. Identifying new words automatically from texts is still a very challenging problem, especially for Chinese. In this paper, we propose a novel schemaoriented approach for Chinese new word identiﬁcation (named “ChNWI”). This approach has three main steps: (1) we suggest three composition schemas that cover nearly all two-character up to four-character Chinese word surfaces; (2) we employ support vector machine (SVM) to classify Chinese new words of three schemas using their unique linguistic characteristics; and (3) we design various rules to ﬁlter identiﬁed Chinese new words of three schemas. Our extensive evaluations with two corpora (Chinese news titles and CIPS-SIGHAN 2012 CSMB) show ChNWI’s efﬁciency on Chinese new word identiﬁcation. 
We investigate whether suffix related features can significantly improve the performance of character-based approaches for Chinese word segmentation (CWS). Since suffixes are quite productive in forming new words, and OOV is the main error source for CWS, many researchers expect that suffix information can further improve the performance. With this belief, we tried several suffix related features in both generative and discriminative approaches. However, our experiment results have shown that significant improvement can hardly be achieved by incorporating suffix related features into those widely adopted surface features, which is against the commonly believed supposition. Error analysis reveals that the main problem behind this surprising finding is the conflict between the degree of reliability and the coverage rate of suffix related features. 
This study aims to review, through experiment proof of a salient effect of articulatory gestures on L2 perception, the time-honored but still put-to-sideways motor theory of speech perception. On one hand, previous studies in support to motor theory were largely done by tests of mismatch in duplex perception of acoustic/speech data; or by L1 development observations. On the other hand, L2 learning studies had seldom followed the motor theory framework. The current study employed two experiments on experienced L2 English speakers from a Cantonese L1 background to finish discrimination tasks on both 1) same allophone [tr] and [tʃ] but with different gestural overlapping in real words 2) the crucial acoustic cue of distinguishing the gestural differences of the same contrast by native speakers in isolation -- namely, the CV transitions. Results showed that non-native speakers could perform nativelike in experiment 2 but not in experiment 1. Though both experiments contain the same acoustic information, only experiment 1 contains the entire gestural information. It is concluded that, at least, errors in second language acquisition has a gestural basis, which might partly support the motor theory from a new perspective. 
Tonal errors pose a serious problem to Mandarin Chinese learners, making them stumble in their communication. The purpose of this paper is to investigate beginner level Japanese students’ difficulties in the perception and pronunciation of disyllabic words, particularly to find out which combinations of tones these errors mostly occur in. As a result, the errors made by the 10 subjects were mostly found in tonal patterns 1-3, 2-1, 2-3, 3-2 and 4-3 in both perception and pronunciation. Furthermore, by comparing the ratio of tonal errors of initial to final syllables, we can tell that the initial syllables appear more difficult than the final syllables in perception, but in pronunciation this tendency is not found. Moreover, there seems to be some connection between learners’ perception and pronunciation in their acquisition process. 
Our internal repository of words, often known as the mental lexicon, has primarily been modelled by psychologists as some kind of network. One way to probe its organisation and access mechanisms is by means of word association techniques, which have rarely been applied on Chinese. This paper reports on the design and implementation of a pilot word association test on native Hong Kong Cantonese speakers. The test contains 500 stimulus words, carefully selected and controlled on important factors including word frequency, part-of-speech, syllabicity, concreteness and vocabulary type. The resulting association norms based on 58 participants reveal interesting properties of the Chinese mental lexicon, such as the dominance of disyllabic and nominal concepts, and collocational associations. Despite its current small scale, the word association norms obtained from this study do not only offer first-hand psycholinguistic evidence for investigating the Chinese mental lexicon but also provide a useful resource to inform future studies in Chinese lexical access, lexical semantics and lexicography. 
Learner corpora are receiving special attention as an invaluable source of educational feedback and are expected to improve teaching materials and methodology. However, they include various types of incorrect sentences. Error type classiﬁcation is an important task in learner corpora which enables clarifying for learners why a certain sentence is classiﬁed as incorrect in order to help learners not to repeat errors. To address this issue, we deﬁned a set of error type criteria and conducted automatic classiﬁcation of errors into error types in the sentences from the NAIST Goyo Corpus and achieved an accuracy of 77.6%. We also tried inter-corpus evaluation of our system on the Lang-8 corpus of learner Japanese and achieved an accuracy of 42.3%. To know the accuracy, we also investigated the classiﬁcation method by human judgement and compared the difference in classiﬁcation between the machine and the human. 
This study explored Mandarin-speaking children’s ability in maintaining narrative coherence. Thirty Mandarin-speaking fiveyear-olds, 30 nine-year-olds and 30 adults participated. The narrative data were elicited using Frog, where are you? Narrative coherence was assessed in terms of causal networks. The results displayed children’s development in achieving narrative coherence by establishing causal relations between narrative events. Results were considered in relation to capacities for working memory and theory of mind. Narrators’ differences in communicative competence and cognitive preferences were also discussed. 
Given that Mandarin is a verb-serializing language, Russian a satellite-framed language, and Spanish a verb-framed language, the current study examines Mandarin college students’ acquisition of Russian and Spanish as L2, to understand the strength of L1 preferences for expression of PATH on Russian and Spanish majors’ second language acquisition in Taiwan. Based on oral narrative data, the study focuses on lexicalization and concatenation preferences in L1 and L2 languages. First, Russian majors’ morphosyntactic preferences show that L1 Mandarin affects students’ acquisition of Russian at the elementary level. However, in the acquisition of Spanish, learners’ native language does not hold strength; Spanish majors’ morphosyntactic patterning conforms more to that in L2 Spanish for both elementary and intermediate levels. Moreover, the Spanish majors appear to be developing their L2 concatenation patterning in a way that is divergent from the target L2 Spanish. The findings provide a deeper understanding of the different degrees of L1 influence on learners’ acquisition of L2 Russian and of L2 Spanish at various levels of proficiency. 
This study investigates age related differences in standardized tests scores of language usage and reading from elementary to high school for students who are either monolinguals whose L1 is English or bilinguals whose L1 is not English. An interactioneffect between grade level andreading and language usage standardized test scores was hypothesized because as bilinguals become proficient in Cognitive Academic Language Proficiency (CALP) in English,they are able tonarrow the ‘achievement gap’in comparison to their monolingual classmates and even experience cognitive advantages (Cummins, 1999).Participants were 1081 students from an international school. Language usage and readingwere measured using MAP standardized achievement tests.The2x2 ANOVA showed an interaction between grade level and languages spoken on language usage (p<0.05).There was a main effect for languages spoken and grade level on language usage (p<0.05). No interaction was found forgrade level and languages spoken on reading (p>0.05). A main effect was found for languages spoken andgrade level on reading (p<0.05).Significant differences exist between bilingual and monolingualsand these differences change over time. As bilingual students are immersed in English education, their performance on standardized tests catches up with their monolingual counterparts by grade 5 for language usage but not for reading, but no cognitive advantages are shown. 1Introduction We live in an increasingly globalized world where bilingualism is very common as global societies become more interconnected. In this context, it is of vital importance to understand how bilingualism relatesstudents’  language abilities, cognitive abilities and  academic  performance.Understanding 
This paper reports the compilation of a corpus of Taiwanese students’ spoken English, which is one of the twenty subcorpora of the Louvain International Database of Spoken English Interlanguage (LINDSEI) (Gilquin et al., 2010). LINDSEI is one of the largest corpora of learner speech. The compilation process follows the design criteria of LINDSEI so as to ensure comparability across sub-corpora. The participants, procedures for data collection and process of transcription are all recorded. Sixty third- or fourth-year English majors in Taiwan are interviewed and recorded in English. Each interview is accompanied by a profile which contains information about such learner variables as age, gender, mother tongue, country, English learning context, knowledge of other foreign languages, amount of time spent in English-speaking countries and such interviewer variables as gender, mother tongue, knowledge of foreign languages and degree of familiarity with the interviewees. Another variable, the learners’ English proficiency level based on the results of international standardised tests is collected; this is not available in other sub-corpora of LINDSEI. The participants’ proficiency is similarly distributed across B1 to C1 levels in the Common European Framework of Reference. This paper concludes with a discussion of the contributions and research potential of the corpus. 
Temporal information extraction can be split into the following three tasks: temporal expression extraction, time normalisation, and temporal ordering relation resolution. This paper describes a time expression and temporal ordering annotation schema for Japanese, employing the Balanced Corpus of Contemporary Written Japanese, or BCCWJ. The annotation is aimed at allowing the development of better Japanese temporal ordering relation resolution tools. The annotation schema is based on an ISO annotation standard – TimeML. We extract verbal and adjective event expressions as ⟨EVENT⟩ in a subset of BCCWJ. Then, we annotate temporal ordering relation ⟨TLINK⟩ on the above pairs of event and time expressions by previous work. We identify several issues in the annotation. 
 analysis with SFL is done manually, a time- and effort- consuming process.  In this paper, we present our recent experience in constructing a first-of-its-kind functional corpus based on the theoretical framework of Systemic Functional Linguistics. Annotated on selected texts from the Penn Treebank, the corpus was built by a collaborative team on web-based annotation platform with several advanced features. After a discussion on the background and motivation of the project, we present our solutions to some of the challenges encountered in the collaborative annotation process. With fine-grained annotations of an initial corpus now available, the corpus can serve as a valuable linguistic resource that complements existing semantically annotated corpora and aid in the development of a largerscale resource crucial for automated systems for analysis of linguistic function.  We are motivated in our study to extend the power of the framework to computational analysis. The difficulty in automating analysis of linguistic functions lies in both the fuzziness in the functional domain and a lack of relevant computational resources. The most significant lack of resource is a high-quality reference corpus crucial to statistical analysis and modeling. In the following sections, we discuss our initial efforts in constructing such a resource on a collaborative annotation platform and present the initial results from the corpus. The corpus is our first step in bridging the gap between the linguistic theory and application of such theory including automated analysis of language functions. 2 Related Works  
The paper aims at the investigation of free word order. It concentrates on the relationship between (formal) dependencies and word order. The investigation is performed by means of a semi-automatic application of a method of analysis by reduction to Czech syntactically annotated data. The paper also presents the analysis of introspectively created Czech sentences demonstrating complex phenomena which are not sufﬁciently represented in the corpus. The focus is on non-projective structures, esp. those connected with the position of clitics in Czech. The freedom of word order is expressed by means of a number of necessary shifts in the process of analysis by reduction. The paper shows that this measure provides a new view of the problem, it is orthogonal to measures reﬂecting the word order freedom based on a number of non-projective constructions or clitics in a sentence. It also helps to identify language phenomena that generally pose a problem for dependencybased formalisms. 
Formal properties of functions denoted by higher order anaphors like each other and syntactically complex expressions containing each other are studied. A partial comparison between these functions and functions denoted by (simple and complex) reﬂexives is draw. In particular it is shown that both types of function are predicate invariant (in a generalised sense). These results allows us to understand the anaphoric character of both reﬂexive and reciprocal expressions. 
In this paper, we present our effort in the development of a HPSG grammar for Chinese. We present the basic notions of the HPSG framework, review existing theoretical analyses and implementations of Chinese grammar fragments in HPSG and present a range of deep linguistic analyses that are part of our own implementation. 
 Using Chinese characters as an intermediate equivalent unit, we decompose machine translation into two stages, semantic translation and grammar translation. This strategy is tentatively applied to machine translation between Vietnamese and Chinese. During the semantic translation, Vietnamese syllables are one-by-one converted into the corresponding Chinese characters. During the grammar translation, the sequences of Chinese characters in Vietnamese grammar order are modified and rearranged to form grammatical Chinese sentence. Compared to the existing single alignment model, the division of two-stage processing is more targeted for research and evaluation of machine translation. The proposed method is evaluated using the standard BLEU score and a new manual evaluation metric, understanding rate. Only based on a small number of dictionaries, the proposed method gives competitive and even better results compared to existing systems. 
Extracting plausible transliterations from historical literature is a key issues in historical linguistics and other resaech ﬁelds. In Chinese historical literature, the characters used to transliterate the same loanword may vary because of different translation eras or different Chinese language preferences among translators. To assist historical linguiatics and digial humanity researchers, this paper propose a transliteration extraction method based on the conditional random ﬁeld method with the features based on the characteristics of the Chinese characters used in transliterations which are suitable to identify transliteration characters. To evaluate our method, we compiled an evaluation set from the two Buddhist texts, the Samyuktagama and the Lotus Sutra. We also construct a baseline approach with sufﬁx array based extraction method and phonetic similarity measurement. Our method outperforms the baseline approach a lot and the recall of our method achieves 0.9561 and the precision is 0.9444. The results show our method is very effective to extract transliterations in classical Chinese texts. 
Linguistically motivated reordering methods have been developed to improve word alignment especially for Statistical Machine Translation (SMT) on long distance language pairs. However, since they highly rely on the parsing accuracy, it is useful to explore the relationship between parsing and reordering. For Chinese-toJapanese SMT, we carry out a three-stage incremental comparative analysis to observe the effects of different parsing errors on reordering performance by combining empirical and descriptive approaches. For the empirical approach, we quantify the distribution of general parsing errors along with reordering qualities whereas for the descriptive approach, we extract seven inﬂuential error patterns and examine their correlation with reordering errors. 
This paper investigates the formal semantics of reduplication in Cantonese, i.e. how the meaning of reduplicated forms are encoded and computed with the given meaning from the base forms. In particular, this paper argues that reduplication denotes a summation function that adds up arguments (be they object-, event- or degreearguments) and return a collection of the elements. The surface difference across categories is accounted for in terms of cumulativity and quantization (Krifka, 1998; Krifka, 2001; Rothstein, 2004). The present approach makes use of scalar structure and summation as formal tools to model the cross-categorial behaviour of reduplication. It provides the advantage of a uniﬁed theory for lexical composition across categories nouns, verbs and adjectives. Keywords: reduplication, formal semantics, cumulativity, cross-categorial behaviour 
This paper presents yet another argument for the view that both the causative and inchoative verbs in the Japanese causative alternation are syntactically equally complex, derived independently from common bases. While morphological considerations alone suggest that such an approach must be taken to account for cases involving equipollent alternations, which have overt morphemes for both the causative and inchoative affixes, the question remains unsettled as to whether there are cases where such processes as causativization of inchoative verbs or anticausativization of causative verbs are involved. In an attempt to answer this question, this paper examines the three approaches to the causative alternation by utilizing the possibility of having idiomatic interpretations as a probe into the syntactic structure. Evidence from idioms reveals that the common base approach still fares better. As a further consequence, postulation of a phonologically null morpheme is forced for some causative and inchoative affixes. 
Functional Data Analysis (FDA) is used to investigate Tone 3 sandhi in Taiwan Mandarin. Tone 3 sandhi is a tone change phenomenon that arises when two low tones occur in succession resulting in the first tone being realised as a rising tone. Tone dyads T2T3 and T3T3 were compared in terms of their F0 contours and velocity profiles. No difference was found between the F0 contours of the two tone dyads. In contrast, velocity profiles showed an increased difference in the later part of the seemingly similar rising movements of T2 and sandhi T3, with a steeper rising-falling movement in the former than the latter. This research demonstrates that FDA can elucidate more detail in the time dimension than that of conventional techniques commonly employed in the current phonetic literature. 
This study examines the sound profiles of sites of initiation in French and Mandarin recycling repair (also disfluent repetitions). 150 examples of disfluent repetitions were extracted from comparative speech corpora of naturally occurred, face-to-face Mandarin and French interaction. By the approach of interactional prosody plus impressionistic judgments, each instance of recycling repair was annotated manually for its prosodic realization, including relative pitch height and duration between the R1/R2 of the repair, as well as silence and other sound cues for initiating the repair. Through comparing the results of acoustic measurements, it is suggested that interlocutors of the two languages may orient to different methods of initiating the repair in spontaneous interaction in that, French speakers tend to incorporate lengthening at the end of R1 plus optional filled pauses to initiate the repair, while Mandarin speakers employ quick cut-offs for repair initiation, followed by immediate repair. 
This study investigates of-constructions in the predicates of two verbs, demonstrate and show, in academic discourse. A construction perspective is taken to examine how the two predicate constructions (‘demonstrate N1 of N2’ and ‘show N1 of N2’) would differ when the information-weighting of N1 and N2 are considered. The noun phrases were compared following Sinclair’s (1991) conception of semantic headedness. He notes the peculiarity of of through the expression of double-headed constructions (i.e., considering both N1 and N2 as the semantic heads). This study adopts this framework and applies it to analyze the of-constructions of the two synonymous verbs. The results show that headedness of the of-constructions can be used to identify the subtle differences between the two synonyms. Demonstrate displays greater information weight predominated by doubleheaded constructions and tends to be associated with abstract conception. Show follows closely after demonstrate, but further analysis reveals that show tends to provide more ‘relational’ evidence described in terms of partitive uses through nouns like variety, degree, incidence, level, rate and range. 
 This  study  investigates  the  conceptualization of our bodily orientation  in a quantitative corpus-based approach of  collostructional analysis. Based on the  symbolic nature of constructions, we  examine the correlation patterns of the  covarying collexeme NPs and 13 major  spatial particles in English Preposition  Construction through exploratory statistical  methods. The distributional patterns of the  spatial particles have far-reaching  implications for the embodiment of  conceptual metaphors. It is concluded that  the (a)symmetry of metaphorical patterns  along each spatial dimension may be  attributed to the recurring (a)symmetrical  daily interaction and bodily experiences  with the surrounding physical environment.  While cultural specificity is of great concern  for future study, a hypothesis for the  implicational scale of conceptual symmetry  in bodily orientation is proposed.  
Wu(1994) listed ten words as CFs’ markers in Chinese, they are 早 (early) ， 了 (perfect/perfective marker), 要不是/要不然 (had it not been the case), 没(didn’t), 就好了 (would have been great if only)，还以为(had thought), 原来应该(should have been), ...的 话 (in the case), 真 的 (really). However, according to our definitions, none of them are dedicated CFs markers but only CFE 1 markers except 要不是. Several observations can be summarized from these markers: (1)although they can be applied to deliver a counterfactual reading, they can never ensure a counterfactual reading; (2) the counterfactuality delivered by them can be easily cancelled by inserting another sentence following behind; (3)counterfactuality can be expressed in absense of these CFE markers.  
It is important for a natural language dialogue system to interpret relations among event concepts appearing in a dialogue. The more complex a dialog becomes, the more essential it becomes for a natural language dialogue system to perform this kind of interpretation. Traditionally, many studies have focused on this problem. Some dialogue systems supported such semantic analysis by using rules and/or models designed for particular scenes involving specific type of dialogue and/or specific problem solving. However, these frameworks require system developers to reconstruct those rules/models even if a slight change is added to the targeted scene. In many cases, their rules/models heavily depend on specific type of dialogue/problem solving, and they do not have high reusability and modularity. Since those rules/models have scene-depending design, they cannot be used to incrementally construct a bigger rule or model. In this research, we focus on a set of event concepts which are usually expected to occur sequentially. In a dialogue, a spoken event concept enables the listeners to guess a sequence of events. The sequence may sometimes be logically inferred, and it may be understood based on general common sense. We believe that a concept model of sequential events can be designed for each bigger event concept that consists of a series of smaller events. Using the sequentiality of the events in the model, a dialogue system can analyze time and loca-  tion of each event in a dialogue. In this paper, we design a structure of the event sequence model and propose a framework for analyzing time and location of event concepts appearing in a dialogue. We implemented this framework in a dialogue system, and designed some event sequence models. We confirmed that this system could analyze time and location of sequential events without scene-depending rules. 
Current study is with the aim to identify similarities and distinctions between irony and sarcasm by adopting quantitative sentiment analysis as well as qualitative content analysis. The result of quantitative sentiment analysis shows that sarcastic tweets are used with more positive tweets than ironic tweets. The result of content analysis corresponds to the result of quantitative sentiment analysis in identifying the aggressiveness of sarcasm. On the other hand, from content analysis it shows that irony owns two senses. The first sense of irony is equal to aggressive sarcasm with speaker awareness. Thus, tweets of first sense of irony may attack a specific target, and the speaker may tag his/her tweet irony because the tweet itself is ironic. These tweets though tagged as irony are in fact sarcastic tweets. Different from this, the tweets of second sense of irony is tagged to classify an event to be ironic. However, from the distribution in sentiment analysis and examples in content analysis, irony seems to be more broadly used in its second sense. 
We propose a method of collective sentiment classiﬁcation that assumes dependencies among labels of an input set of reviews. The key observation behind our method is that the distribution of polarity labels over reviews written by each user or written on each product is often skewed in the real world; intolerant users tend to report complaints while popular products are likely to receive praise. We encode these characteristics of users and products (referred to as user leniency and product popularity) by introducing global features in supervised learning. To resolve dependencies among labels of a given set of reviews, we explore two approximated decoding algorithms, “easiest-ﬁrst decoding” and “twostage decoding”. Experimental results on two real-world datasets with product and user/product information conﬁrmed that our method contributed greatly to the classiﬁcation accuracy. 
This paper aims to introduce the Korean Sentiment Analysis Corpus named KOSAC. KOSAC is a corpus consisting of 332 news articles taken from the Sejong Syntactic Parsed Corpus. These sentences have been manually-tagged for sentimental features. The corpus includes 7,713 sentence subjectivity tags and 17,615 opinionated expression tags based on the annotation scheme called KSML which reflects the characteristics of the Korean language. The results of sentence subjectivity and polarity classification experiements using the corpus show the wide possibilities of application the KSML scheme and the tagged information of the KOSAC comprehensively to other corpus. What is innovative about our work is that it pulls together both the concept of private states and nested-sources into one linguistic annotation scheme. We believe that this corpus could be used by researchers as a gold standard for various NLP tasks related to sentiment analysis. 
Wikipedia is an online multilingual encyclopedia that contains a very large number of articles covering most written languages. However, one critical issue for Wikipedia is that the pages in different languages are rarely linked except for the cross-lingual link between pages about the same subject. This could pose serious difficulties to humans and machines who try to seek information from different lingual sources. In order to address above issue, we propose a hybrid approach that exploits anchor strength, topic relevance and entity knowledge graph to automatically discovery cross-lingual links. In addition, we develop CELD, a system for automatically linking key terms in Chinese documents with English Concepts. As demonstrated in the experiment evaluation, the proposed model outperforms several baselines on the NTCIR data set, which has been designed especially for the cross-lingual link discovery evaluation.  of monolingual and cross-lingual alignment in Chinese and English Wikipedia. As it can be seen that there are 2.6 millions internal links within English Wikipedia and 0.32 millions internal links within Chinese Wikipedia, but only 0.18 millions links between Chinese Wikipedia pages to English ones. For example, in Chinese Wikipedia page “武术(Martial arts)”, anchors are only linked to related Chinese articles about different kinds of martial arts such as “ 拳 击 (Boxing)”, “柔道(Judo)” and “击剑(Fencing)”. But, there is no anchors linked to other related English articles such as “Boxing”, “Judo and “Fencing”. This makes information flow and knowledge propagation could be easily blocked between articles of different languages.  
 However, the locational verb aru ‘be’ shows the following alternation between ni and de.  This paper proposes two syntax-semantics  correspondence rules which consistently account for the distribution of Japanese locative postpositions ni and de. We demonstrate how to adapt the machinery of the oc-  (2) a. Kono hoteru-{ni/*de} hooru-ga aru.  This hotel-in  hall-NOM is  ‘There is a hall in this hotel.’  currence of the postpositions based on the assumption of Conceptual Semantics (Jack-  b. Kono hoteru-{*ni/de} konsaato-ga aru.  endoff, 1983; 1990; 1991) to ﬁt the organization of Japanese grammar. The correspondence rules correlate with semantic distinc-  This hotel-in  concert-NOM is  ‘There is a concert in this hotel.’  tion of verb classes: the semantic ﬁeld distinction between Spatial and Temporal with  Since the postposition de can be used with the sta-  respect to the BE-function encoded in the  tive verb aru ‘be’ as shown in (2b), we cannot  lexical conceptual structure of several verbs. As a result, this paper elucidates the mechanism of locative alternation of the verb aru ‘be’, which has not been fully explicated.  simply refer to the stative/non-stative distinction of the predicate involved in order to predict the distribution of ni and de. Although many descriptive and theoretical stud-  ies have discussed the syntactic and semantic  
Hindi is the lingua-franca of India. Although all non-native speakers can communicate well in Hindi, there are only a few who can read and write in it. In this work, we aim to bridge this gap by building transliteration systems that could transliterate Hindi into at-least 7 other Indian languages. The transliteration systems are developed as a reading aid for non-Hindi readers. The systems are trained on the transliteration pairs extracted automatically from a parallel corpora. All the transliteration systems perform satisfactorily for a non-Hindi reader to understand a Hindi text. 
This paper presents a hybrid model for handling out-of-vocabulary words in Japaneseto-English statistical machine translation output by exploiting parallel corpus. As the Japanese writing system makes use of four different script sets (kanji, hiragana, katakana, and romaji), we treat these scripts differently. A machine transliteration model is built to transliterate out-ofvocabulary Japanese katakana words into English words. A Japanese dependency structure analyzer is employed to tackle outof-vocabulary kanji and hiragana words. The evaluation results demonstrate that it is an effective approach for addressing out-ofvocabulary word problems and decreasing the OOVs rate in the Japanese-to-English machine translation tasks. 
Question Classification plays a significant part in Question Answering system. In order to obtain a classifier, we present in this paper1 a pragmatic approach that utilizes simple sentence structures observed and learned from the question sentence patterns, trains a set of Finite State Machines (FSM) based on keywords appearing in the sentences and uses the trained FSMs to classify various questions to their relevant classes. Although, questions can be placed using various syntactic structures and keywords, we have carefully observed that this variation is within a small finite limit and can be traced down using a limited number of FSMs and a simple semantic understanding instead of using complex semantic analysis. WordNet semantic meaning of various keywords to extend the FSMs capability to accept a wide variety of wording used in the questions. Various kinds of questions written in English language and belonging to diverse classes from the Conference and Labs of the Evaluation Forum’s Question Answering track are used for the training purpose and a separate set of questions from the same track is used for analyzing the FSMs competence to map the questions to one of the recognizable classes. With the use of learning strategies and application of simple voting functions along with training the weights for the keywords appearing in the questions, we have managed to achieve a classification accuracy as high as 94%. The system was trained by placing questions in various orders to see if the system built up from those orders have any subtle impact on the accuracy rate. The usability of this approach lies in its simplicity and yet it performs well to cope up with various sentence patterns. 
Topic models can be used in an unsupervised domain adaptation for Word Sense Disambiguation (WSD). In the domain adaptation task, three types of topic models are available: (1) a topic model constructed from the source domain corpus: (2) a topic model constructed from the target domain corpus, and (3) a topic model constructed from both domains. Basically, three topic features made from each topic model are added to the normal feature used for WSD. By using the extended features, SVM learns and solves WSD. However, the topic features constructed from source domain have weights describing the similarity between the source corpus and the entire corpus because the topic features made from the source domain can reduce the accuracy of WSD. In six transitions of domain adaptation using three domains, we conducted experiments by varying the combination of topic features, and show the effectiveness of the proposed method. 
Vietnamese accentless texts exist on parallel with official vietnamese documents and play an important role in instant message, mobile SMS and online searching. Understanding correctly these texts is not simple because of the lexical ambiguity caused by the diversity in adding diacritics to a given accentless sequence. There have been some methods for solving the vietnamese accentless texts problem known as accent prediction and they have obtained promising results. Those methods are usually based on distance matching, n-gram, dictionary of words and phrases and heuristic techniques. In this paper, we propose a new method solving the accent prediction. Our method combine the strength of previous methods (combining n-gram method and phrase dictionary in general). This method considers the accent predicting as statistical machine translation (SMT) problem with source language as accentless texts and target language as accent texts, respectively. We also improve quality of accent predicting by applying some techniques such as adding dictionary, changing order of language model and tuning. The achieved result and the ability to enhance proposed system are obviously promising. 
Frequent patterns are useful in many data mining problems including query suggestion. Frequent patterns can be mined through frequent pattern tree (FPtree) data structure which is used to store the compact (or compressed) representation of a transaction database (Han, et al, 2000). In this paper, we propose an algorithm to compress frequent pattern set into a smaller one, and store the set in a modified version of FP-tree (called compact FP-tree) as an inverted indexing of patterns for later quick retrieval (for query suggestion). With the compact FP-tree, we can also restore the original frequent pattern set. Our experiment results show that our compact FP-tree has a very good compression ratio, especially on sparse dataset which is the nature of query log. 
In this paper we present a new method for machine learning-based optimization of linguist-written Constraint Grammars. The effect of rule ordering/sorting, grammarsectioning and systematic rule changes is discussed and quantitatively evaluated. The F-score improvement was 0.41 percentage points for a mature (Danish) tagging grammar, and 1.36 percentage points for a half-size grammar, translating into a 7-15% error reduction relative to the performance of the untuned grammars. 
Newspapers remain an important media from which people obtain a wide variety of information. In Japan, there are five major newspapers, having their own opinions and ideologies. Although these are readily recognized, they are infrequently investigated from the viewpoint of their textual characteristics. This study analyzes these differences among the five newspaper editorials. We apply morphological analysis and count the frequency of morphemes within the text data. We then apply principal component analysis and random forests classification experiments to examine their similarities and differences. Throughout these statistical analyses, we use function words and content words as features, which enables us to determine which of the two characteristics -styles or content- more powerfully affects the classification types. This study contributes to text classification studies by deliberately comparing the classification performances provided by different feature sets, function words and content words. In addition, this study will provide an empirical basis for understanding the similarities and differences among the five newspapers. 1. Introduction Newspapers are an important media from which people obtain a wide variety of information, ranging from contemporary political and economic issues to ordinary incidents. Particularly in Japan, where newspapers delivery remains popular, many people read them in their own spaces and have continued to use them as popular information resources, even after the advent and spread of the Web. According to  Nihon Shinbun Kyokai (2012), 87.3 percent people in Japan read newspapers, which is second only to television (98.7 percent) among the five surveyed media including newspaper, television, radio, magazines, and Internet. There are five major newspapers in Japan: Asahi, Mainichi, Nikkei, Sankei, and Yomiuri, which have publication offices in Tokyo, Osaka, and other areas, and are distributed to almost all regions of Japan. Though all of these newspapers regard correctness, neutrality and unbiased reporting as important, they have their own opinions and ideologies. According to the Hosyu (conservative)-Kakushin (liberal) image survey by Shinbun Tsushin Chosakai (2009), the five major newspapers scored as follows: Yomiuri 5.6, Sankei 5.3, Nikkei 5.2, Mainichi 5.0, and Asahi 4.4, where larger numbers indicate a newspaper perceived as more conservative and smaller numbers indicate a newspaper perceived as more liberal. Excluding Nikkei, which is a specialized newspaper that focuses on economic issues, the survey results show that people see Yomiuri and Sankei as more conservative and Mainichi and Asahi as more liberal. These differences might affect the textual characteristics of newspapers; however, they have not been investigated in a comprehensive and systematic manner. With the development of natural language processing techniques and the creation of many online text corpora, quantitative text analysis has been expanding in scope. Such methods have begun to be recognized as important tools for solving many theoretical and practical social science research questions. Particular to newspapers, some studies have applied these quantitative text analysis methods. For example, Newman and Block (2006) determined topics using a probabilistic mixture decomposition method with the Pennsylvania Gazette, a major colonial U.S. newspaper that  450 Copyright 2013 by Takafumi Suzuki, Erina Kanou, and Yui Arakawa 27th Pacific Asia Conference on Language, Information, and Computation pages 450458  PACLIC-27  was in publication from 1728 to 1800. Higuchi (2011) investigated whether there is significant association between the content of newspaper articles and social consciousness trends by using three Japanese newspapers, Asahi, Yomiuri, and Mainichi. However, these previous newspaper text analyses focused only on the content. Examination of textual characteristics, such as styles of texts, which can reveal attitudes, personalities, psychologies, emotions, text genres, and authors (Argamon et al., 2007; Suzuki, 2009), have rarely been focused despite being intriguing aspects for analysis. Therefore, this study analyzes the differences among editorials in the five major Japanese newspapers. Among the many types of articles, editorials are one of the most intriguing and colorful, wherein respective viewpoints are expressed (Goto, 1999), and thus are good materials for investigation. We first apply principal component analysis (PCA) to observe the overall distribution of these texts in scatter plots and investigate the factors affecting the textual characteristics. Next, we apply random forests classification experiments using newspapers, editorial dates, and ideology types as classes in order to examine the classification performance and important features of these experiments. Throughout these analyses, we use function words as well as content words as features, which is useful for investigating the similarities and differences of these classes. In addition, these features enable us to clarify which of the two characteristics-styles or content-more powerfully affects these classification types, which is also an interesting text analysis topic. This study contributes to text classification studies by deliberately comparing the classification performance provided by different feature sets, function words and content words. In addition, this study provides empirical findings useful for understanding the characteristics of the five newspapers. 2. Data and methods 2.1. Data This study focused on the five major Japanese newspapers: Asahi, Mainichi, Nikkei, Sankei, and Yomiuri. We constructed the editorial texts using the following databases. Yomiuri: Yomidasu Rekishikan (1874-now) Asahi: Kikuzo II Visual for Libraries (19451984, pocket edition 1985-now)  Mainichi: Mainichi News Pack (1987-now) Nikkei: Nikkei Terekon 21 (1975-now) Sankei: The Sankei Archives (1992.9-now) We selected two editorial dates for each newspaper, Jan. 1 and Aug. 15 from 2000 to 2010. As Jan. 1 is New Year’s Day, each newspaper runs an editorial reflecting their primary opinions and interests. The Aug. 15 is the anniversary of the end of the Pacific War in Japan, and each newspaper runs an editorial reflecting their view on the war. New Year’s Day editorials reflect the general vision of each newspaper and the end-of-war editorials reflect specific visions of the newspapers. In this study, we used 31 editorials from Nikkei and 22 editorials from each of the other newspapers.1 We removed symbols, lines, and parentheses, i.e., analysis noises, and applied morphological analysis using MeCab. 2 We divided the morphemes into content words and function words using the tags assigned by MeCab.3 The relative frequencies of morphemes were counted; three types of text-feature matrices (bag-ofwords models) were constructed using all morphemes, content words, and function words as features. 2.2. Methods 2.2.1. Principal component analysis We applied PCA using the variances-covariance matrices constructed from three types of textfeature matrices in order to observe the distribution of the newspaper texts and to examine the factors affecting their textual characteristics. 2.2.2. Random forests Next, we applied random forests (Breiman, 2001) for classification experiments. Random forests is an improved means of bagging (Breiman, 1996), which is an ensemble-learning method. The basic objective of ensemble learning is to improve the classification performance of previous statistical methods, i.e. decision trees in this case, by repeatedly performing the experiments and calculating the mean or majority votes of the results. However, 
It has been generally assumed that a violation of island constraints indicates that the relevant syntactic phenomena involves movement. That is, if what look like displacements violate island constraints but remain acceptable, this means that they should not be derived by movement. A careful examination of postverbal constructions in Japanese reveals that no movement is involved in the derivation of the construction despite the fact that in some cases island effects are observed. The effects, which have up to now been dealt with purely in syntax, can receive a better account in terms of language processing. This suggests that the human parser should undertake explanations of part of the output of the competence system.  elements in sentence-final position as postverbal elements (PVE).3 Some researchers (e.g., Endo, 1989; Kaiser, 1999; Whitman, 2000; Tanaka, 2001; and Abe, 2004) claim that the PVE is derived by movement because of the obedience of the PVE to island constraints such as the so-called Complex NP Constraint (CNPC), as shown in (2). In (2), e is used to mark the position associated with the moved element, namely the PVE, and the identical subscript indicates that the PVE corresponds to e.  (2) *?[NP [CP [ei Sonkeisiteiru] sensei]-ga  respect  teacher-NOM  fueteimasu yo, gakuseitati-gai.  increase FP students-NOM  ‘The number of the teachers who theyi  respect is increasing, studentsi.’  
In the development of corpus linguistics, the creation of corpora has had a critical role in corpus-based studies. The majority of created corpora have been associated with English and native languages, while other languages and types of corpora have received relatively less attention. Because an increasing number of corpora have been constructed, and each corpus is constructed for a definite purpose, this study identifies the functions of corpora and combines the values of various types of corpora for auto-learning based on the existing corpora. Specifically, the following three corpora are adopted: (a) the Corpus of Spanish; (b) the Corpus of Taiwanese Learners of Spanish; and (c) the Parallel Corpus of Spanish, English, and Chinese. These corpora represent a type of native, learner, and parallel language, respectively. We apply these corpora as auxiliary resources to identify the advantages of applying various types of corpora in language learning from a learner’s perspective. In the environment of auto-learning, 28 participants completed frequency questions related to semantic and lexical aspects. After analyzing the questionnaire data, we obtained the following findings: (a) the native corpus requires a more advanced level of Spanish proficiency to manage ampler and deeper context; (b) the learners’ corpus facilitates the distinction between error and correction during the learning process; (c) the parallel corpus assists learners in connecting form and meaning; (d) learning is more efficient if the learner can capitalizes on specific functions provided by various corpora in the application order of parallel, learner and native corpora.  
This paper presents a novel method using graph-based semi-supervised learning (SSL) to improve the syntax parsing of unknown words. Different from conventional approaches that uses hand-crafted rules, rich morphological features, or a character-based model to handle unknown words, this method is based on a graph-based label propagation technique. It gives greater improvement on grammars trained on a smaller amount of labeled data and a large amount of unlabeled one. A transductiv1 graph-based SSL method is employed to propagate POS and derive the emission distributions from labeled data to unlabeled one. The derived distributions are incorporated into the parsing process. The proposed method effectively augments the original supervised parsing model by contributing 2.28% and 1.72% absolute improvement on the accuracy of POS tagging and syntax parsing for Penn Chinese Treebank respectively. 
This paper investigates the tonal patterns in the 15th century from a corpus-based approach, focusing on two historical sources, 日 本 館 譯 語 Rìbĕn kuăn yìyŭ ‘A Wordlist of Chinese-Japanese Phrases’ and 朝 鮮 館 譯 語 Cháoxiān kuăn yìyŭ ‘A Wordlist of ChineseKorean Phrases’. The results suggest that Japanese and Korean are signiifcantly different in the phonetic transcription of low tone in monosyllabic words and in the first syllable of a disyllabic word. The results also suggest that Mandarin Chinese in the 15th century tends to be a falling tone in the second syllable of a disyllablic word. 
This paper proposes a guideline to determine Thai elementary discourse units (EDUs) based on rhetorical structure theory. Carson and Marcu’s (2001) guideline for segmenting English EDUs is modified to propose a suitable guideline for segmenting EDUs in Thai. The proposed principles are used in tagging EDUs for constructing a corpus of discourse tree structures. It can also be used as the basis for implementing automatic Thai EDU segmentation. The problems of determining Thai EDUs both manually and automatically are also explored and discussed in this paper. 
In this paper, we propose a method for automatic clause boundary annotation in the Hindi Dependency Treebank. We show that the clausal information implicitly encoded in a dependency structure can be made explicit with no or less human intervention. We exercised the proposed approach on 16,000 sentences of Hindi Dependency Treebank. Our approach gives an accuracy of 94.44% for clause boundary identiﬁcation evaluated over 238 clauses. The resultant corpus has varied usages and can be utilized for developing a statistical clause boundary identiﬁer. 
This paper examines some popular misanalyses in Korean morphology. For example, contrary to popular myth, the verbal ha- and the element -(nu)n- cannot be analyzed as a derivational affix and as a present tense marker, respectively. We will see that ha- is an independent word and that -(nu)n- is part of a portmanteau morph. In providing reasonable analyses of them, we will consider some computational implications of the misanalyses. It is really mysterious that such wrong analyses can become so popular in a scientific field of linguistics. 
We present a discontinuous variant of treesubstitution grammar (tsg) based on Linear Context-Free Rewriting Systems. We use this formalism to instantiate a Data-Oriented Parsing model applied to discontinuous treebank parsing, and obtain a significant improvement over earlier results for this task. The model induces a tsg from the treebank by extracting fragments that occur at least twice. We give a direct comparison of a tree-substitution grammar implementation that implicitly represents all fragments from the treebank, versus one that explicitly operates with a significant subset. On the task of discontinuous parsing of German, the latter approach yields a 16 % relative error reduction, requiring only a third of the parsing time and grammar size. Finally, we evaluate the model on several treebanks across three Germanic languages. 
Many applications (not necessarily only from computational linguistics), involving record- or graph-like structures, would beneﬁt from a framework which would allow to efﬁciently test a single structure φ under various operations against a compact representation of a set of similar structures: φ . Besides a Boolean answer, we would also like to see those structures stored in which are entailed by operation . In our case, we are especially interested in s that implement feature structure subsumption and uniﬁability. The urgent need for such a kind of framework is related to our work on the approximation of (P)CFGs from uniﬁcationbased grammars. We not only deﬁne the mathematical apparatus for this in terms of ﬁnite-state automata, but also come up with an efﬁcient implementation mostly along the theoretical basis, together with measurements in which we compare our implementation of against a discrimination tree index. 
We present the first known experiments incorporating unsupervised bilingual nonterminal category learning within end-to-end fully unsupervised transduction grammar induction using matched training and testing models. Despite steady recent progress, such induction experiments until now have not allowed for learning differentiated nonterminal categories. We divide the learning into two stages: (1) a bootstrap stage that generates a large set of categorized short transduction rule hypotheses, and (2) a minimum conditional description length stage that simultaneously prunes away less useful short rule hypotheses, while also iteratively segmenting full sentence pairs into useful longer categorized transduction rules. We show that the second stage works better when the rule hypotheses have categories than when they do not, and that the proposed conditional description length approach combines the rules hypothesized by the two stages better than a mixture model does. We also show that the compact model learned during the second stage can be further improved by combining the result of different iterations in a mixture model. In total, we see a jump in BLEU score, from 17.53 for a standalone minimum description length baseline with no category learning, to 20.93 when incorporating category induction on a Chinese– English translation task. 
Discourse relation parsing is an important task with the goal of understanding text beyond the sentence boundaries. One of the subtasks of discourse parsing is the extraction of argument spans of discourse relations. A relation can be either intra-sentential – to have both arguments in the same sentence – or inter-sentential – to have arguments span over different sentences. There are two approaches to the task. In the ﬁrst approach the parser decision is not conditioned on whether the relation is intra- or intersentential. In the second approach relations are parsed separately for each class. The paper evaluates the two approaches to argument span extraction on Penn Discourse Treebank explicit relations; and the problem is cast as token-level sequence labeling. We show that processing intra- and inter-sentential relations separately, reduces the task complexity and signiﬁcantly outperforms the single model approach. 
Parsing and named entity recognition are two standalone techniques in natural language processing community. We expect that these two types of annotations should provide useful information to each other, and that modeling them jointly should improve performance and produce consistent outputs. Employing more ﬁne-grained named entity annotations helps to parse complex named entity structures correctly. Thus, we integrate parsing and named entity recognition in a uniﬁed framework: 1. Through a joint representation of syntactic and named entity structures, we annotate named entity information to Penn Chinese Treebank5.0 (CTB5.0); 2. We annotate the nested structures for all nested named entities; 3. A latent annotation probabilistic context-free grammar (PCFGLA) model is trained on the data with joint representation. Experiment results demonstrate the mutual beneﬁts for both Chinese parsing and named entities recognition tasks.  bracketing phrases with complex structure, and many complex phrases are named entities. In Chinese there are a large number of named entities. Named entities (NEs) can be generally divided into three types: entity names, temporal expressions, and number expressions. They are /unique identiﬁers0of entities (organizations, persons, locations), time (date, times), and quantities (monetary values, percentages). According to Chinese Treebank ﬁfth edition (CTB5.0) (Xue et al., 2002), every sentence contains over 1.5 entity names. Table 1 shows the distribution of these named entities in CTB5.0.  
Recently, several statistical parsers have been trained and evaluated on the dependency version of the French TreeBank (FTB). However, older symbolic parsers still exist, including FRMG, a wide coverage TAG parser. It is interesting to compare these different parsers, based on very different approaches, and explore the possibilities of hybridization. In particular, we explore the use of partially supervised learning techniques to improve the performances of FRMG to the levels reached by the statistical parsers. 
We compare three different approaches to parsing into syntactic, bi-lexical dependencies for English: a ‘direct’ data-driven dependency parser, a statistical phrase structure parser, and a hybrid, ‘deep’ grammar-driven parser. The analyses from the latter two are post-converted to bilexical dependencies. Through this ‘reduction’ of all three approaches to syntactic dependency parsers, we determine empirically what performance can be obtained for a common set of dependency types for English, across a broad variety of domains. In doing so, we observe what trade-offs apply along three dimensions, accuracy, efﬁciency, and resilience to domain variation. Our results suggest that the hand-built grammar in one of our parsers helps in both accuracy and cross-domain performance. 
In this paper, we investigate the inﬂuence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding (NLMWE) method (Bengio et al., 2003) to generate distributed word feature vectors and then perform K-means based word clustering to generate word classes. We designed feature templates by making use of words, part-of-speech (POS) tags, coarse-grained POS (CPOS) tags, NLMWE-based word classes and their combinations. NLMWE-based word classes is shown to be an important supplement of POS-tags, especially when POS-tags are automatically generated. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWE-based word classes yield the best unlabelled attachment scores (UASs). Our ﬁnal UAS−p (excluding punctuations) of 86.79% on the CTB5 test set is comparable to state-of-theart results. Our ﬁnal UAS−p of 86.80% and 87.05% on the CTB7 Stanford dependency test set and original test set is signiﬁcantly better than three well known open-source dependency parsers. 
We adapt the dynamic-oracle training method of Goldberg and Nivre (2012; 2013) to train classiﬁers that produce probabilistic output. Evaluation of an Arc-Eager parser on 6 languages shows that the AdaGrad-RDA based training procedure results in models that provide the same high level of accuracy as the averagedperceptron trained models, while being sparser and providing well-calibrated probabilistic output. 
Previous researches on incremental dependency parsing have not discussed how a parser should output the information about dependency relation where the modiﬁed word has not been inputted yet. This paper proposes a dependency structure which a Japanese incremental dependency parser should produce in the middle of an input sentence. The dependency structure also expresses the fact that a word depends on a word which the parser would receive later. In addition, we present a method for incremental dependency parsing to produce our proposed dependency structure. As the result of an experiment on incremental parsing, we conﬁrmed the feasibility of our incremental dependency parsing. Furthermore, this paper shows its application in the simultaneous linefeed insertion for real-time generation of more readable captions, and then describes the effectiveness of our proposed dependency structure. 
Data-driven dependency parsers need a large annotated corpus to learn how to generate dependency graph of a given sentence. But annotations on structured corpora are expensive to collect and requires a labor intensive task. Active learning is a machine learning approach that allows only informative examples to be selected for annotation and is usually used when the number of annotated data is abundant and acquisition of more labeled data is expensive. We will provide a novel framework in which a committee of dependency parsers collaborate to improve their efﬁciency using active learning techniques. Queries are made up only from uncertain tokens, and the annotations of the remaining tokens of selected sentences are voted among committee members. 
Amharic is one of the most morphologically complex and under-resourced languages which effectively hinder the development of efficient natural language processing applications. Amharic words, especially verbs, are marked for a combination of several grammatical functions which makes grammar checking complex. This paper describes the design and development of statistical grammar checker for Amharic by treating its morphological features. In a given Amharic sentence, the morphologies of individual words making up the sentence are analyzed and then n-gram based probabilistic methods are used to check grammatical errors in the sentence. The system is tested with a test corpus and experimental results are reported. 
In data-driven parsing with Linear Context-Free Rewriting System (LCFRS), markovized grammars are obtained through the annotation of binarization non-terminals during grammar binarization, as in the corresponding work on PCFG parsing. Since there is indication that directional parsing with a non-binary LCFRS can be faster than parsing with a binary LCFRS, we present a debinarization procedure with which we can obtain a non-binary LCFRS from a previously binarized one. The resulting grammar retains the markovization information. The algorithm has been implemented and successfully applied to the German NeGra treebank. 
We propose a Korean dependency parsing system that can learn the relationships between Korean words from the Treebank corpus and a large raw corpus. We ﬁrst reﬁne the training dataset to better represent the relationship using a different POS tagging granularity type. We also introduce lexical information and propose an almost fully lexicalized probabilistic model with case frames automatically extracted from a very large raw corpus. We evaluate and compare systems with and without POS granularity reﬁnement and case frames. The proposed lexicalized method outperforms not only the baseline systems but also a state-of-the-art supervised dependency parser. 
In this work we take a view of syntactic analysis as processing ‘raw’, running text instead of idealised, pre-segmented inputs—a task we dub document parsing. We observe the state of the art in sentence boundary detection and tokenisation, and their effects on syntactic parsing (for English), observing that common evaluation metrics are ill-suited for the comparison of an ‘end-to-end’ syntactic analysis pipeline. To provide a more informative assessment of performance levels and error propagation throughout the full pipeline, we propose a uniﬁed evaluation framework and gauge document parsing accuracies for common processors and data sets. 
We present a method of semantic parsing within lexicalized grammars that generates neoDavidsonian logical forms. We augment an existing LFG grammar with predicate logic formulae in the lexicon and logical annotations in the context-free rules to obtain logical forms of a sequence of sentences that can be used to construct a model, i.e., formulae with resolved anaphoric and indexical expressions, by skolemization and discourse resolution based on salience.  
In this paper, we present our efforts towards identifying probable incorrect edges and then suggesting k-best alternates for the same in a typed-dependency framework. Such a setup is beneﬁcial in human aided NLP systems where the decisions are largely automated with minimal human intervention. Minimizing the human intervention calls for automatic identiﬁcation of ambiguous cases. We have employed an entropy based confusion measure to capture uncertainty exerted by the parser oracle and later ﬂag the highly uncertain predictions. To further assist human decisions, k-best alternatives are supplied in the order of their likelihood. Our experiments, conducted for Hindi, establish the effectiveness of the proposed approach towards increasing the label accuracy with economically viable manual intervention. This work leads to new directions for parser development and also in the human-aided NLP systems. 
Work on both the graphical user interface (GUI) and the conversational user interface (CUI) started at about the same time, about 40 years ago. The GUI was a lot easier to implement, and it made computing and information resources available to ordinary people—but over the years it has lost much of its simplicity and charm. The CUI has taken many more years to develop, requiring major scientiﬁc and engineering advances in speech, natural language processing, user-modeling, and reasoning, not to mention increases in cost-effective computation. But the infrastructure is now in place for the widespread distribution of conversational interfaces, and we have begun to imagine and create sophisticated ways of exploiting this new mode of interaction. This may well be the “killer app” for deep natural language processing and complex reasoning. 
Figurative language poses a serious challenge to NLP systems. The use of idiomatic and metaphoric expressions is not only extremely widespread in natural language; many ﬁgurative expressions, in particular idioms, also behave idiosyncratically. These idiosyncrasies are not restricted to a non-compositional meaning but often also extend to syntactic properties, selectional preferences etc. To deal appropriately with such expressions, NLP tools need to detect ﬁgurative language and assign the correct analyses to non-literal expressions. While there has been quite a bit of work on determining the general ‘idiomaticity’ of an expression (type-based approaches), this only solves part of the problem as many expressions, such as break the ice or play with ﬁre, can also have a literal, perfectly compositional meaning (e.g. break the ice on the duck pond). Such expressions have to be disambiguated in context (token-based approaches). Token-based approaches have received increased attention recently. In this talk, I will present an unsupervised method for token-based idiom detection. The method exploits the fact that well-formed texts exhibit lexical cohesion, i.e. words are semantically related to other words in the context. I will show how cohesion can be modelled and how the cohesive structure can be used to distinguish literal and idiomatic usages and even detect newly coined ﬁgurative expressions. 
In computational linguistics we develop tools and on-line services for everything from literature to social media data, but our tools are often optimized to minimize expected error on a single annotated dataset, typically newspaper articles—and evaluated on held-out data sampled from the same dataset. Signiﬁcance testing across data points randomly sampled from a standard dataset only tells us how likely we are to see better performance on more data points sampled this way, but says nothing about performance on other datasets. This talk discusses how to modify learning algorithms to minimize expected error on future, unseen datasets, with applications to PoS tagging and dependency parsing, including cross-language learning problems. It also discusses the related issue of how to best evaluate NLP tools (intrinsically) taking their possible out-of-domain applications into account. 
The Nordic e-Infrastructure Collaboration (NeIC) is a distributed organization of IT-experts working at various national HPC centers throughout the Nordic region. The mission of NeIC is to facilitate the development of high-quality e-Infrastructure solutions in areas of joint Nordic interest. It is owned by the research councils and national e-Infrastructure provider organisations from Denmark, Finland, Iceland, Norway, and Sweden. NeIC is hosted by NordForsk in Oslo. Current collaboration areas include high-energy physics (CERN-related) and life sciences. A call for Letters of Interest in 2012 elicited several opportunities within the humanities, including ﬁelds such as computational analysis of language and semantic annotation. This presentation will give an overview of the NeIC and its modus operandi, and aims to provide a basis for discussions at NoDaLiDa on how NeIC may be put into use as a vehicle for facilitating development of common e-Infrastructure services for linguistics and related ﬁelds. 
Until about six years ago, our research group used non-trivial amounts of project funds and researcher time on maintaining a dedicated server farm in the basement of our department. Rack space and cooling (just as much as funds and time) were in short supply, and we never quite got around to implementing automated load balancing across compute nodes, tuning the Linux kernel and ﬁlesystem for optimum performance, or connecting to the uninterruptible power supply. When pointed to the Norwegian National High-Performance Computing Initiative, we were intially doubtful that Natural Language Processing should be among their target user groups. Also, we were a tad hesitant to give up control of our own equipment and of course worried we would miss what we thought were our fancy toys. Today, any member of the group can access thousands of cpus simultaneously, we have about ﬁve terabytes of project data on-line, and our research has scaled to dataset sizes and turn-around times that would be just inconceivable on group-local hardware—at no charge to our project funds and no administrator responsibilities. For example, ‘deep’ semantic parsing of the about 900 million words of the English Wikipedia we can typically complete in less than one day (while expending what would be about eight sequential years of computation). Or, when searching for the best-performing features and hyper-parameters in a machine learning problem, we can explore a large ‘grid’ of possible conﬁgurations in parallel, without much need for a staged, partly manual, ‘coarseto-ﬁne’ search strategy. Access to the very large-scale Norwegian National eInfrastructure and its high-quality technical support have enabled a comparatively computation-heavy research proﬁle of our group and has thus contributed to its international competitiveness. In this presentation, I will review some of our experiences in establishing a dialogue with the HPC crowd and propose HPC for the Masses as a candidate vision in the on-going development trend towards more and more large-scale computational sciences. 
In this presentation I will discuss the design and implementation of Let’s MT!, a collaborative platform for building statistical machine translation systems. The goal of this platform is to make MT technology, that has been developed in academia, accessible for professional translators, freelancers and every-day users without requiring technical skills and deep background knowledge of the approaches used in the backend of the translation engine. The main challenge in this project was the development of a robust environment that can serve a growing community and large numbers of user requests. The key for success is a distributed environment that allows a maximum of scalability and robustness. With this in mind, we developed a modular platform that can be scaled by adding new nodes to the different components of the system. We opted for a cloud-based solution based on Amazon EC2 to create a cost-efﬁcient environment that can dynamically be adjusted to user needs and system load. In the presentation I will explain our design of the distributed resource repository, the SMT training facilities and the actual translation service. I will mention issues of data security and optimization of the training procedures in order to ﬁt our setup and the expected usage of the system. 
Studies have shown that modern methods of readability assessment, using automated linguistic analysis and machine learning (ML), is a viable road forward for readability classiﬁcation and ranking. In this paper we present a study of different levels of analysis and a large number of features and how they affect an ML-system’s accuracy when it comes to readability assessment. We test a large number of features proposed for different languages (mainly English) and evaluate their usefulness for readability assessment for Swedish as well as comparing their performance to that of established metrics. We ﬁnd that the best performing features are language models based on part-of-speech and dependency type. KEYWORDS: Readability assessment, Machine learning, Dependency parsing, Weka. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 27 of 474]  
In this work, we present the ﬁrst results of a project aiming at a Finnish Proposition Bank, an annotated corpus of semantic roles. The annotation is based on an existing treebank of Finnish, the Turku Dependency Treebank, annotated using the well-known Stanford Dependency scheme. We describe the use of the dependency treebank for PropBanking purposes and show that both annotation layers present in the treebank are highly useful for the annotation of semantic roles. We also discuss the speciﬁc features of Finnish inﬂuencing the development of a PropBank as well as the methods employed in the annotation, and ﬁnally, we present preliminary evaluation of the annotation quality. KEYWORDS: PropBank, Finnish, dependency. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 41 of 474]  
This article presents a novel way of combining ﬁnite-state transducers (FSTs) with electronic dictionaries, thereby creating efﬁcient reading comprehension dictionaries. We compare a North Saami - Norwegian and a South Saami - Norwegian dictionary, both enriched with an FST, with existing, available dictionaries containing pre-generated paradigms, and show the advantages of our approach. Being more ﬂexible, the FSTs may also adjust the dictionary to different contexts. The ﬁnite state transducer analyses the word to be looked up, and the dictionary itself conducts the actual lookup. The FST part is crucial for morphology-rich languages, where as little as 10 % of the wordforms in running text actually consists of lemma forms. If a compound or derived word, or a word with an enclitic particle is not found in the dictionary, the FST will give the stems and derivation afﬁxes of the wordform, and each of the stems will be given a separate translation. In this way, the coverage of the FST-dictionary will be far larger than an ordinary dictionary of the same size. KEYWORDS: Lexicography, Computational Morphology, Orthographic Variation, Finite-state Transducers, Electronic Dictionaries. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 59 of 474]  
Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 73 of 474]  
There is an increasing interest in the NLP community in developing tools for annotating historical data, for example, to facilitate research in the ﬁeld of corpus linguistics. In this work, we experiment with several PoS taggers using a sub-corpus of the Icelandic Saga Corpus. This is carried out in three main steps. First, we evaluate taggers, which were trained on Modern Icelandic, when tagging Old Icelandic. Second, we semi-automatically correct errors in the training corpus using a bootstrapping method. Finally, we evaluate the taggers on the corrected training corpus. The best performing single tagger is Stagger, a tagger based on the averaged perceptron algorithm, obtaining an accuracy of 91.76%. By combining the output of three taggers, using a simple voting scheme, the accuracy increases to 92.32%. KEYWORDS: Historical Data, Icelandic Saga Corpus, Part-of-Speech Tagging. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 89 of 474]  
In this paper, we experiment with using Stagger, an open-source implementation of an Averaged Perceptron tagger, to tag Icelandic, a morphologically complex language. By adding languagespeciﬁc linguistic features and using IceMorphy, an unknown word guesser, we obtain stateof-the-art tagging accuracy of 92.82%. Furthermore, by adding data from a morphological database, and word embeddings induced from an unannotated corpus, the accuracy increases to 93.84%. This is equivalent to an error reduction of 5.5%, compared to the previously best tagger for Icelandic, consisting of linguistic rules and a Hidden Markov Model. KEYWORDS: Averaged Perceptron, Part-of-Speech Tagging, Morphological Database, Linguistic Features, Word Embeddings. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 105 of 474]  
This paper deals with multimodal turn management in an annotated Danish corpus of video recorded dyadic conversations between young people who meet for the ﬁrst time. Conversation participants indicate whether they wish to give, take or keep the turn through speech as well as body behaviours. In this study we present an analysis of turn management body behaviours as well as classiﬁcation experiments run on the annotated data in order to investigate how far it is possible to distinguish between the different types of turn management expressed by body behaviours using their shape and the co-occurring speech expressions. Our study comprises body behaviours which have not been previously investigated with respect to turn management, so that it not only conﬁrms preceding studies on turn management in English but also provides new insight on how speech and body behaviours are used together in communication. The classiﬁcation experiments indicate that the shape annotations of all kinds of body behaviour together with information about the gesturer’s co-occurring speech are useful to classify turn management types, and that the various behaviours contribute to the expression of turn features in different ways. Thus, knowledge of the different cues used by speakers in face-to-face communication to signal different types of turn shift provides the basis for modelling turn management, which is in turn key to implement natural conversation ﬂow in multimodal dialogue systems. KEYWORDS: Multimodal Communication, Turn Management, Multimodal Corpora, Ma- chine Learning. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 133 of 474]  
seaton@hum.ku.dk, kadri.vider@ut.ee ABSTRACT During the last few years, extensive wordnets have been built locally for the Nordic and Baltic languages applying very different compilation strategies. The aim of the present investigation is to consolidate and examine these wordnets through an alignment via Princeton Core WordNet and thereby compare them along the measures of taxonomical structure, synonym structure, and assigned relations to approximate to a best practice. A common web interface and visualizer “WordTies” is developed to facilitate this purpose. Four bilingual wordnets are automatically processed and evaluated exposing interesting differences between the wordnets. Even if the alignments are judged to be of a good quality, the precision of the translations vary due to considerable differences in hyponymy depth and interpretation of the synset. All seven monolingual and four bilingual wordnets as well as WordTies have been made available via META-SHARE through the META-NORD project. KEYWORDS: wordnets, multilingual links, wordnet web interface, Nordic and Baltic languages, META-NORD. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 147 of 474]  
Natural language processing for historical text imposes a variety of challenges, such as to deal with a high degree of spelling variation. Furthermore, there is often not enough linguistically annotated data available for training part-of-speech taggers and other tools aimed at handling this speciﬁc kind of text. In this paper we present a Levenshtein-based approach to normalisation of historical text to a modern spelling. This enables us to apply standard NLP tools trained on contemporary corpora on the normalised version of the historical input text. In its basic version, no annotated historical data is needed, since the only data used for the Levenshtein comparisons are a contemporary dictionary or corpus. In addition, a (small) corpus of manually normalised historical text can optionally be included to learn normalisation for frequent words and weights for edit operations in a supervised fashion, which improves precision. We show that this method is successful both in terms of normalisation accuracy, and by the performance of a standard modern tagger applied to the historical text. We also compare our method to a previously implemented approach using a set of hand-written normalisation rules, and we see that the Levenshtein-based approach clearly outperforms the hand-crafted rules. Furthermore, the experiments were carried out on Swedish data with promising results and we believe that our method could be successfully applicable to analyse historical text for other languages, including those with less resources. KEYWORDS: Digital Humanities, Natural Language Processing, Historical Text, Normalisa- tion, Levenshtein Edit Distance, Compound Splitting, Part-of-Speech Tagging, Underresourced Languages, Less-Resource Languages. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 163 of 474]  
Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 181 of 474]  
metanord@tilde.lv ABSTRACT This paper describes scientific, technical, and legal work done on the creation of the linguistic infrastructure for the Nordic and Baltic countries. The paper describes the research on assessment of language technology support for the languages of the Baltic and Nordic countries, work on establishing a language resource sharing infrastructure, and collection and description of linguistic resources. We present improvements necessary to ensure usability and interoperability of language resources, discuss issues related to intellectual property rights for complex resources, and describe extension of infrastructure through integration of language-resource specific repositories. Work on treebanks, wordnets, terminology resources, and finite-state technology is described in more detail. Finally, our approach on ensuring the sustainability of infrastructure is discussed. KEYWORDS: language resources and tools, linguistic infrastructure, under-resourced languages, multilinguality, treebanks, wordnets, terminology banks. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 195 of 474]  
Compared to well-resourced languages such as English and Dutch, NLP tools for linguistic analysis in Afrikaans are still not abundant. In order to facilitate corpus-based linguistic research for Afrikaans, we are creating a treebank based on the Taalkommissie corpus. We adapted a tokenizer and a shallow parser, while using a TnT tagger to do part-of-speech annotation. A ﬁrst linguistic phenomenon we are investigating is the occurrence of inﬁnitivus pro participio (IPP) in Afrikaans. IPP refers to constructions with a perfect auxiliary, in which an inﬁnitive appears instead of the expected past participle. The phenomenon has been studied extensively in Dutch and German, but studies on Afrikaans IPP triggers are sparse. In contrast to the former two languages, it is often mentioned in the literature that in Afrikaans, IPP occurs optionally. We want to check this statement doing a corpus analysis. KEYWORDS: Afrikaans, tokenizer, parser, chunker, corpus search tool, IPP. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 213 of 474]  
The information obtained from the Web is increasingly important for decision making and for our everyday tasks. Due to the growth of uncertiﬁed sources, blogosphere, comments in the social media and automatically generated texts, the need to measure the quality of text information found on the Internet is becoming of crucial importance. It has been suggested that factual density can be used to measure the informativeness of text documents. However, this was only shown on very speciﬁc texts such as Wikipedia articles. In this work we move to the sphere of the arbitrary Internet texts and show that factual density is applicable to measure the informativeness of textual contents of arbitrary Web documents. For this, we compiled a human-annotated reference corpus to be used as ground truth data to measure the adequacy of automatic prediction of informativeness of documents. Our corpus consists of 50 documents randomly selected from the Web, which were ranked by 13 human annotators using the MaxDiff technique. Then we ranked the same documents automatically using ExtrHech, an open information extraction system. The two rankings correlate, with Spearman’s coefﬁcient ρ = 0.41 at signiﬁcance level of 99.64%. KEYWORDS: quality of texts, Web, fact extraction, open information extraction, informa- tiveness, natural language processing. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 227 of 474]  
We study content-based recommendation of Finnish news in a system with a very small group of users. We compare three standard methods, Naïve Bayes (NB), K-Nearest Neighbor (kNN) Regression and Regulairized Linear Regression in a novel online simulation setting and in a coldstart simulation. We also apply Latent Dirichlet Allocation (LDA) on the large corpus of news and compare the learned features to those found by Singular Value Decomposition (SVD). Our results indicate that Naïve Bayes is the worst of the three models. K-Nearest Neighbor performs consistently well across input features. Regularized Linear Regression performs generally worse than kNN, but reaches similar performance as kNN with some features. Regularized Linear Regression gains statistically signiﬁcant improvements over the word-features with LDA both on the full data set and in the cold-start simulation. In the cold-start simulation we ﬁnd that LDA gives statistically signiﬁcant improvements for all the methods. KEYWORDS: Recommender Systems, Content-Based Recommendation, Topic Models, Latent Dirichlet Allocation, Cold-start. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 239 of 474]  
People tend to have various opinions about topics. In discussions, they can either agree or disagree with another person. The recognition of agreement and disagreement is a useful prerequisite for many applications. It could be used by political scientists to measure how controversial political issues are, or help a company to analyze how well people like their new products. In this work, we develop an approach for recognizing agreement and disagreement. However, this is a challenging task. While keyword-based approaches are only able to cover a limited set of phrases, machine learning approaches require a large amount of training data. We therefore combine advantages of both methods by using a bootstrapping approach. With our completely unsupervised technique, we achieve an accuracy of 72.85%. Besides, we investigate the limitations of a keyword based approach and a machine learning approach in addition to comparing various sets of features. KEYWORDS: Text Classiﬁcation, Agreement, Disagreement, Opinion Mining. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 253 of 474]  
We describe the methods and resources used to build FinnTreeBank-3, a 76.4 million token corpus of Finnish with automatically produced morphological and dependency syntax analyses. Starting from a deﬁnition of the target dependency scheme, we show how existing resources are transformed to conform to this deﬁnition and subsequently used to develop a parsing pipeline capable of processing a large-scale corpus. An independent formal evaluation demonstrates high accuracy of both morphological and syntactic annotation layers. The parsed corpus is freely available within the FIN-CLARIN infrastructure project. KEYWORDS: dependency parsing, Finnish, CLARIN, parsebank, treebank. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 291 of 474]  
This report describes a series of exploratory experiments to establish whether terms of different semantic type can be distinguished in useful ways in a semantic space constructed from distributional data. The hypotheses explored in this paper are that some words are more variant in their distribution than others; that the varying semantic character of words will be reﬂected in their distribution; and this distributional difference is encoded in current distributional models, but that the information is not accessible through the methods typically used in application of them. This paper proposes some new measures to explore variation encoded in distributional models but not usually put to use in understanding the character of words represented in them. These exploratory ﬁndings show that some proposed measures show a wide range of variation across words of various types. KEYWORDS: Term typology, distributional semantics. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 311 of 474]  
Automatic speech recognition (ASR) relies on three resources: audio, orthographic transcriptions and a pronunciation dictionary. The dictionary or lexicon maps orthographic words to sequences of phones or phonemes that represent the pronunciation of the corresponding word. The quality of a speech recognition system depends heavily on the dictionary and the transcriptions therein. This paper presents an analysis of phonetic/phonemic features that are salient for current Danish ASR systems. This preliminary study consists of a series of experiments using an ASR system trained on the DK-PAROLE corpus. The analysis indicates that transcribing e.g. stress or vowel duration has a negative impact on performance. The best performance is obtained with coarse phonetic annotation and improves performance 1% word error rate and 3.8% sentence error rate. KEYWORDS: Automatic speech recognition, phonetics, phonology, speech, phonetic transcrip- tion. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 321 of 474]  
Since the emergence of translation memory software, translation companies and freelance translators have been accumulating translated text for various languages and domains. This data has the potential of being used for training domain-speciﬁc machine translation systems for corporate or even personal use. But while the resulting systems usually perform well in translating domain-speciﬁc language, their out-of-domain vocabulary coverage is often insufﬁcient due to the limited size of the translation memories. In this paper, we demonstrate that small in-domain translation memories can be successfully complemented with freely available general-domain parallel corpora such that (a) the number of out-of-vocabulary words (OOV) is reduced while (b) the in-domain terminology is preserved. In our experiments, a German– French and a German–Italian statistical machine translation system geared to marketing texts of the automobile industry has been signiﬁcantly improved using Europarl and OpenSubtitles data, both in terms of automatic evaluation metrics and human judgement. KEYWORDS: Machine Translation, Translation Memory, Domain Adaptation, Perplexity Mini- mization. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 331 of 474]  
The article presents the Giellatekno & Divvun language technology resources, more speciﬁcally the effort to utilise open-source tools to improve the build infrastructure, and the solutions to help adapt to best practices for software development. The article especially discusses how the infrastructure has been remade to cope with an increasing number of languages without incurring extra overhead for the maintainers, and at the same time let the linguists concentrate on the linguistic work. Finally, the article discusses how a uniform infrastructure like the one presented can be used to easily compare languages in terms of morphological or computational complexity, coverage or for cross-lingual applications. KEYWORDS: NoDaLiDa 2013, Infrastructure, Computational linguistics, Finite-state transduc- ers, Language resources, Multilinguality. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 343 of 474]  
Segmenting documents into discrete, sentence-like units is usually a ﬁrst step in any natural language processing pipeline. However, current segmentation tools perform poorly on text that contains markup. While stripping markup is a simple solution, we argue for the utility of the extra-linguistic information encoded by markup and present a scheme for normalising markup across disparate formats. We further argue for the need to maintain accountability when preprocessing text, such that a record of modiﬁcations to source documents is maintained. Such records are necessary in order to augment documents with information derived from subsequent processing. To facilitate adoption of these principles we present a novel tool for segmenting text that contains inline markup. By converting to plain text and tracking alignment, the tool is capable of state-of-the-art sentence boundary detection using any external segmenter, while producing segments containing normalised markup, with an account of how to recreate the original form. Keywords: Accountability, Markup, Normalisation, Sentence Boundary Detection, Traceability. ∗ Work carried out at the University of Oslo. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 365 of 474]  
This paper presents experiments with document-level machine translation with readability constraints. We describe the task of producing simpliﬁed translations from a given source with the aim to optimize machine translation for speciﬁc target users such as language learners. In our approach, we introduce global features that are known to affect readability into a documentlevel SMT decoding framework. We show that the decoder is capable of incorporating those features and that we can inﬂuence the readability of the output as measured by common metrics. This study presents the ﬁrst attempt of jointly performing machine translation and text simpliﬁcation, which is demonstrated through the case of translating parliamentary texts from English to Swedish. KEYWORDS: Machine Translation, Text Simpliﬁcation, Readability. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 375 of 474]  
Negation detection is a key component in clinical information extraction systems, as health record text contains reasonings in which the physician excludes different diagnoses by negating them. Many systems for negation detection rely on negation cues (e.g. not), but only few studies have investigated if the syntactic structure of the sentences can be used for determining the scope of these cues. We have in this paper compared three different systems for negation detection in Swedish clinical text (NegEx, PyConTextNLP and SynNeg), which have different approaches for determining the scope of negation cues. NegEx uses the distance between the cue and the disease, PyConTextNLP relies on a list of conjunctions limiting the scope of a cue, and in SynNeg the boundaries of the sentence units, provided by a syntactic parser, limit the scope of the cues. The three systems produced similar results, detecting negation with an F-score of around 80%, but using a parser had advantages when handling longer, complex sentences or short sentences with contradictory statements. KEYWORDS: clinical text, negation detection, syntactic analysis. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 387 of 474]  
The RWAAI (Repository and Workspace for Austroasiatic Intangible heritage) project aims at building a digital archive out of existing legacy data from the Austroasiatic language family. One aspect of the project is the preservation of analogue legacy data. In this context, we have at our hands a large number of mostly-phonemic transcriptions of narrative monologues, often with accompanying sound recordings, in the unwritten Kammu language of northern Laos. Some of the transcriptions, however, lack tone marks, which for a tonal language such as Kammu makes them substantially less useful. The problem of restoring tones can be recast as one of word sense disambiguation, or, more generally, lexical ambiguity resolution. We attack it by decision lists, along the lines of Yarowsky (1994), using the tone-marked part of the corpus (120kW) as training data. The performance ceiling of this corpus is uncertain: the stories were all annotated, primarily for human rather than machine consumption, by a single person during almost 40 years, with slowly emerging idiosyncratic conventions. Thus, both inter-annotator and intra-annotator agreement figures are unknown. Nevertheless, with the data from this one annotator as a gold standard, we improve from an already-high baseline accuracy of 95.7% to 97.2% (by 10-fold cross-validation). Keywords: word sense disambiguation, Kammu, decision lists, lexical ambiguity resolu- tion, tone restoration, legacy data. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 399 of 474]  
The identiﬁcation of discourse units is an essential step in discourse parsing, the automatic construction of a discourse structure from a text. We present a rule-based algorithm to identify elementary discourse units (EDUs) in Dutch written text. Contrary to approaches that focus on the determination of segment boundaries, we identify complete discourse units, which is especially helpful for the recognition of interrupted EDUs that contain embedded discourse units. We use syntactic and lexical information to decompose sentences into EDUs. Experimental results show that our algorithm for EDU identiﬁcation performs well on texts of various genres. KEYWORDS: discourse analysis, elementary discourse units, segmentation. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 411 of 474]  
Although several syntactically annotated corpora (or treebanks) exist for Dutch, they are seldomly used for descriptive linguistic research because there are no easy-to-use exploitation tools available. This demonstration paper describes GrETEL, a linguistic search engine (http:// nederbooms.ccl.kuleuven.be/eng/gretel) that enables non-technical users to consult treebanks in a user-friendly way. Instead of a formal search expression, a natural language example is used as input to the system, allowing users to search for similar constructions as the example they provide. In the ﬁrst version of GrETEL, only written Dutch (LASSY) was included. Based on user requests we have now included the Spoken Dutch Corpus (CGN) as well. KEYWORDS: Dutch, treebank, querying, example-based. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 423 of 474]  
A central activity in Språkbanken, an R&D unit at the University of Gothenburg, is the systematic construction of a research infrastructure based on interoperability and widely accepted standards for metadata and data. The two main components of this infrastructure deal with text corpora and with lexical resources. For modularity and ﬂexibility, both components have a backend, or server-side part, accessed through an API made up of a set of well-deﬁned web services. This means that there can be any number of different user interfaces to these components, corresponding, e.g., to different research needs. Here, we will demonstrate the standard corpus and lexicon search interfaces, designed primarily for linguistic searches: Korp and Karp. Keywords: Swedish, corpora, lexical resources, research infrastructure. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 429 of 474]  
Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 435 of 474]  
In this paper we present a simple and useful Javascript application programming interface for performing basic online operations with weighted and unweighted ﬁnite-state machines, such as word lookup, transductions, and least-cost-path ﬁnding. The library, jsfst, provides access to frequently used online functionality in ﬁnite-state machine-based language technology. The library is technology-agnostic in that it uses a neutral representation of ﬁnite-state machines into which most formats can be converted. We demonstrate the usefulness of the library through addressing a task that is useful in web and mobile environments—a multilingual spell checker application that also detects real-word errors. KEYWORDS: Finite-state technology, Javascript, spell checking, perceptrons. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 441 of 474]  
This demonstration presents a ﬁrst operable pilot of the Language Analysis Portal (LAP), an ongoing project within the Norwegian CLARINO initiative that aims at providing easy access to Language Technology (LT) tools running on a powerful High-Performance Computing (HPC) cluster. The system is built on top of the Galaxy framework, giving users an on-line platform where they can design experiments using an array of processors. These processors can be combined into complex workﬂows using a visual editor. The current implementation functions as a testbed for further development, hosting a limited collection of tools addressing common use-cases in the LT-realm; the long-term goal for LAP is to reach beyond the ﬁeld and be an enabling platform for LT-powered research in the humanities and social sciences. KEYWORDS: research infrastructure, High-Performance Computing, web portal, CLARINO. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 447 of 474]  
This paper brieﬂy describes the current state of the evolving INESS infrastructure in Norway which is developing treebanks as well as making treebanks more accessible to the R&D community. Recent work includes the hosting of more treebanks, including parallel treebanks, and increasing the number of parsed and disambiguated sentences in the Norwegian LFG treebank. Other recent improvements include the presentation of metadata and license handling for restricted treebanks. The infrastructure is fully operational and accessible, but will be further improved during the lifetime of the INESS project. KEYWORDS: treebanks, research infrastructure, parsed corpora, metadata, IPR, INESS, META- NORD, CLARIN, CLARINO. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 453 of 474]  
Språkbanken at the National Library of Norway is currently building up gold-standard Dependency Grammar treebanks for Norwegian Bokmål and Nynorsk. The treebanks are manually annotated for morphological features, syntactic functions and dependency relations. This paper explains the choice of texts and format of the treebanks, some key aspects of the morphological and syntactic annotation, and it is illustrated how the treebanks can be used. KEYWORDS: Treebanking, Dependency Grammar, Morphology, Syntax, Norwegian. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 459 of 474]  
 Carlos A. Iglesias, J. Fernando Sánchez-Rada Dept. Ing. Sist. Telemáticos, Univ. Politécnica de Madrid, Spain {cif, jfernando}@gsi.dit.upm.es  Carlo Strapparava Human Language Technology FBK, Italy strappa@fbk.eu  
This research describes SemLink, a comprehensive resource for Natural Language Processing that maps and uniﬁes several highquality lexical resources: PropBank, VerbNet, FrameNet, and the recently added OntoNotes sense groupings. Each of these resources was created for slightly different purposes, and therefore each carries unique strengths and limitations. SemLink allows users to leverage the strengths of each resource and provides the groundwork for incorporating these lexical resources effectively into linked data resources. SemLink and the resources included therein are discussed with a focus on the value of using lexical resources in a complementary fashion. Recent improvements to SemLink, including the addition of a new resource, the OntoNotes sense groupings, are described. Work to address future goals, including further expansion of SemLink, is also discussed. 
The OntoLex W3C Community Group has been working for more than a year on realizing a proposal for a standard ontology lexicon model. As the corespecification of the model is almost complete, the group started development of additional modules for specific tasks and use cases. We think that in many usage scenarios (e.g. linguistic enrichment, localization and alignment of ontologies) the discovery and exploitation of linguistically grounded datasets may benefit from summarizing information about their linguistic expressivity. While the VoID vocabulary covers the need for general metadata about linked datasets, this more specific information demands a dedicated extension. In this paper, we fill this gap by introducing LIME (Linguistic Metadata), a new vocabulary aiming at completing the OntoLex standard with specifications for linguistic metadata. 
In this short paper, we describe how we converted dictionary and wordlist data made available by the QuantHistLing project into the Lexicon Model for Ontologies. By doing so, we leverage Linked Data to combine disparate lexical resources – more than ﬁfty lexicons and dictionaries – by converting the lexical data into an RDF model that is speciﬁed by Lemon. The resulting new Linked Data resource, what we call the QHL dataset, provides researchers with a translation graph, which allows users to query across the underlying lexicons and dictionaries to extract semantically-aligned wordlists. 
Developing language resources requires much time, funding and effort. This is why they need to be reused in new projects and developments, so that they may both serve a wider scientific community and sustain their cost. The main problems that prevent this from happening are that (1) language resources are rarely free and/or easy to locate; and (2) they are hardly ever interoperable. Therefore, the language resource community is now working to transform their most valuable assets into open and interoperable resources, which can then be shared and linked with other open and interoperable resources. This will allow data to be reanalyzed and repurposed. In this paper, we present the first steps taken to transform a set of such resources, namely the Data Transcription and Analysis Tool’s (DTA) metadata and data, into an open and interoperable language resource. These first steps include the development of two ontologies that formalize the conceptual model underlying the DTA metadata and the labels used in the DTA to annotate both utterances and their transcriptions at several annotation levels. 
In this paper we describe an implemented framework for releasing multimodal corpora as Linked Data. In particular, we describe our experiences in releasing a multimodal corpus based on an online chat game as Linked Data. Building on an internal multimodal data model we call FiESTA, we have implemented a library that enhances existing libraries and classes by functionality allowing to convert the data to RDF. Our framework is implemented on the Rails web application framework. We argue that this work can be highly useful for further contributions to the Linked Data community, especially from the ﬁelds of spoken dialogue and multimodal communication. 
In this short report on language data and RDF tools, we describe the transformation process that we undertook to convert spreadsheet data about a group of endangered languages and where they are spoken in West Africa into an RDF triple store. We use RDF tools to organize and visualize these data on a world map, accessible through a web browser. The functionality we develop allows researchers to see where these languages are spoken and to query the language data. This type of development not only showcases the power of RDF, but it provides a powerful tool for linguists trying the solve the mysteries of the genealogical relatedness of the Dogon languages. 
This paper undertakes the modelling experiment of translating excerpts of the natural language play - “Faust” by Johann Wolfgang von Goethe - into a RDF structure, so that it is accessible by machines on a word or concept level. Thereby, it is crucial that statements made in the logic of the play can be distinguished from the usual, general purpose Linked Open Data. The goal is to ﬁnd a standard compliant solution, stressing RDF’s central role in the Web of Data as a format for arbitrary data. 
This paper describes the practice and the reality of OWL conversion of Japanese WordNet and Japanese dictionary IPAdic. The outcomes of OWL conversion are linked to DBpedia Japanese dataset using lexical word matching. The difﬁculty originating from the specialty of Japanese, which is shareable by non-English languages, is focused. The potential of LOD in linguistics is also discussed. The goal of our study on Linguistics by LOD is to provide an open and rich environment in linguistics that propels multi-lingual studies for linguistics researchers and bottom-up style ontology buildings for ontologists. 
Semantic feature norms, originally utilized in the ﬁeld of psycholinguistics as a tool for studying human semantic representation and computation, have recently attracted the attention of some NLP/IR researchers who wish to improve their task performances. However, currently available semantic feature norms are, by nature, not well-structured, making them difﬁcult to integrate into existing resources of various kinds. In this paper, by examining an actual set of semantic feature norms, we investigate which types of semantic features should be migrated into Linked Data in Linguistics (LDL) and how the migration could be done. 
This paper describes the conversion of ItalwordNet and of a domain WordNet into RDF and their linking to the (L)LOD cloud and to other existing resources. A brief presentation of the resources is given, and the conversion and resulting datasets are described. 
In this brief note, I explore the cognitive mechanisms involved in interpreting the meanings of events, as conveyed through language. Speciﬁcally, I examine the notion of event simulation in the construction of linguistic meaning. Simulations are a special class of minimal models, generated from linguistic input, under a number of agentoriented cognitive constraints. An integral part of this model is a dynamic representation of processes and events, such as the Dynamic Event Structure presented here. I show how simulations are composed of entity and event habitats, which are contextualization functions, acting to embed a proposition into a minimal model. 
This study takes a corpus-based approach to examine twenty Chinese verbs that have been found to coerce their NP complements into an event type (cf. Lin et al. 2009), with an aim of creating a coercion profile for each verb. A cluster analysis is further conducted on the coercion profiles. The resulting clusters in our analysis show a bi-directional distribution: the verbs in Cluster 1 are found to coerce their complements more frequently, while the verbs in Cluster 2 are found to coerce more noun types. Moreover, many lexical pairs (e.g., antonyms and near-synonyms) are identified in the two clusters. Our quantitative analysis suggests that semantically related verbs can have similar coercion profiles. The empirical findings of the present study complement intuition-based studies on the complement coercion operation in Chinese (e.g., Lin and Liu 2004, Liu 2003) and shed new light on the theoretical framework of the Generative Lexicon. 
The work presented here depicts experiments toward the automatic classification of complex-type nominals using distributional information. We conducted two experiments: classifying complex-type nominals as members of multiple individual lexical classes, and building a dedicated classifier for complex-type nominals, distinguishing them from simple types. We discuss the promising results obtained, with a focus on asymmetries observed and on lines to be explored in the future. 
This paper aims to find out the qualia roles involved in metaphorical noun-noun compounds in Mandarin Chinese. Metaphor concerns the resemblance between things. From the perspective of qualia structure proposed by Generative Lexicon, the resemblance in metaphorical compounds can be interpreted in the way that a compound and its metaphorical component share the same quale role. A preliminary investigation shows that no matter which constituent (the modifying noun or the head noun) takes on a metaphorical meaning, only three qualia roles are found in metaphorical compounds, which are FORMAL, CONSTITUTIVE and TELIC, ordered from the most to the least frequent. AGENTIVE role is excluded. Among the values of FORMAL role, shape is the most frequent. CONSTITUTIVE role mainly relates to body part terms. TELIC role is mainly concerned with artifactual type nouns. Also, this study reveals some fine-grained distinctions between nouns of different types. 
This paper describes a case study on methods for automatically extracting qualia relations from dictionary glosses in Italian, namely the Senso Comune De Mauro Dictionary (SCDM). The qualia extraction has been addressed by means of a pattern-based approach and lexical match with an Italian generative lexicon based language resource, PAROLE-SIMPLECLIPS (PSC). The evaluation of the extraction approaches has been performed with respect to a manually built Gold Standard containig 174 different qualia. The results obtained are encouraging (P = 0.84, R = 0.08 for the pattern extraction approach and P=0.73 and R=0.16 for the merging of pattern extraction and lexical match) and suggest that the information contained in the SCDM glosses is complementary with that in PSC. 
This research describes efforts to expand the lexical resource VerbNet with additional class members and completely new verb classes. Several approaches to this in the past have involved automatic methods for expansion, but this research focuses on the addition of frequent, yet particularly challenging verbs that require manual additions after a survey of each verb’s syntactic behaviors and semantic features. Sketch Engine has been an invaluable tool in this process, allowing for a comprehensive, yet detailed view of the behavior of a given verb, along with efﬁcient comparisons to the behaviors of other verbs that might be included in VerbNet already. The incorporation of light verbs into VerbNet has presented particular challenges to this process, these are described along with a proposed resource to supplement VerbNet with information on light verbs. 
The event structure (aktionsart) is a widely discussed issue for the representation of verbal semantics in languages. However, there is still problems for the classiﬁcation of verbs into state, activity, accomplishment, achievement and semelfactive. It is also not clear where are the differences of them embedded in terms of lexical, semantic or syntactic levels. In this paper, we will give a discussion on the primitives of events from an ontological point of view. We suggest that event types should be discussed in the usage level of language. Based on the Generative Lexicon theory, we provide a semantic representation of verbs which can give a better explanation how the semantics of verbs and the composition with their complements can determine the event type they denote. 
In this paper we look at how Generative Lexicon theory can assist in providing a more thorough deﬁnition of word senses as links between items in a RDF-based lexicon and concepts in an ontology. We focus on the deﬁnition of lexical sense in lemon and show its limitations before deﬁning a new model based on lemon and which we term lemonGL. This new model is an initial attempt at providing a way of structuring lexico-ontological resources as linked data in such a way as to allow a rich representation of word meaning (following the GL theory) while at the same time (attempting to) re-main faithful to the separation between the lexicon and the ontology as recommended by the lemon model. 
Knowledge about semantic associations between words is effective to disambiguate word senses. The aim of this paper is to investigate the role and the relevance of telic information from SIMPLE in the disambiguation of basic action types of Italian HOLD verbs (prendere, ‘to take’, raccogliere, ‘to pick up’, pigliare ‘to grab’ etc.). We propose an experiment to compare the results obtained with telic information from SIMPLE with basic co-occurrence information extracted from corpora (most salient verbs modifying nouns) classified in terms of general semantic classes to avoid data sparseness. 
This paper describes an effort to capture the sense alternation of dot-type nominals using Word Sense Induction (WSI). We propose dot-type nominals generate more semantically consistent groupings when clustered into more than two clusters, accounting for literal, metonymic and underspeciﬁed senses. Using a class-based approach, we replace individual lemmas with a placeholder representing the entire dot type, which also compensates for data sparsity. Although the distributional evidence does not motivate an individual cluster for each sense, we discuss how our results empirically support theoretical proposals regarding dot types. 
In this paper we examine the role that compositional mechanisms and lexical semantics play in the determination of informativeness at the phrasal and clausal level. While the computation of the “relevance” of an utterance is largely determined by pragmatic factors (such as quantity), we argue that phrasal informativeness can, in many cases, be computed compositionally and independently of pragmatics. To illustrate this, we focus on the well-documented contrast between predicative and derived participial modiﬁcational constructions in English (build a house results in well-formed sentences, while *a built house does not). In our analysis, informativeness within an NP is computed in terms of minimal model generation (Blackburn and Bos, 2008), using the semantics associated with the qualia of the head noun; that is, modiﬁcation is informative whenever a qualia value is not satisﬁed in all models. 
This paper proposes an elaboration of the Generative Lexicon (GL) in Pustejovsky (1995) based on a survey of BCCWJ (2009). I manually classiﬁed the Japanese NP1-no NP2 “NP1’s NP2” construction in accordance with semantic relations between the two nominals. The result indicates the need for the expansion of GL for computing the meaning of the NP1no NP2 construction by incorporating referential module, as I call, that predicates temporary location, time, and manner of the referent. For example, in ima-no nihon “the present Japan,” ima-no modiﬁes the time of the event argument in the referential module. 
In the Generative Lexicon Theory (GLT), co-composition is one of the generative devices proposed to explain the cases of verbal polysemous behavior where more than one function application is allowed. The English baking verbs were used as one of the examples to illustrate how their complements co-specify the verb with qualia uniﬁcation. In this paper, we begin by exploring the polysemy of Chinese baking verb, where the ﬁrst two senses in Chinese Wordnet (CWN) are assumed. Features including linguistic cues and common sense knowledge are involved in the experiment with Weibo corpus and computed with SVM for closer investigation. From the analysis, it is found that though there are various cases found in senses of change of state and creation, a coarse but systematic approach combined with certain features in disambiguating CWN senses could be arranged. In addition, we further observe that the usage of various instruments cases and classiﬁers would be harnessed by underlying background knowledge to help select an appropriate sense based on the context. Keywords: The generative lexicon, cocomposition, baking verbs 
 zái noun phrase ~ (locative particle) ‘at’  In this paper, we present a novel approach using LDA (Latent Dirichlet Analysis, Blei, David, Andrew, Michael, and Jordan, 2003) to analyze synonym groups appearing in fixed frames containing Chinese locative phrases, such as [zái noun phrase (yǐ/zhī) shàng/xià/etc. biān/miàn/etc.], and to understand noun meanings related to the syntactic forms of locative phrases. We mapped the different noun phrases to their collocating synonym groups before we generated similarity comparison among different combinations. We collected locative phrases using 11 monosyllabic locative words and 5 locative compoundformation patterns from Sketch Engine, and we aligned these compounds with Chinese Synonym Forest (Mei, Zhu, & Gao 1983) before clustering. A Hive Plot (Krzywinski, Birol, Jones, and Marra, 2012) visualizer was constructed in order to help understand the relationship of locative nouns and their synonym groups. The results showed not only the semantic meaning within a locative phrase, but also the corresponding semantic meanings among locative phrases. 
“Translation Difficulties and Information Processing Problems with Eastern European Languages” Cristina Vertan and Walther v.Hahn Published din the volume “Multilingual Processing in Eastern and Southern EU Languages”, Cambridge Scholar Press, 2012 It is still popular today to blame machine translation (MT) for poor translations of literary texts. However, even inelegant translations are an industrial factor in producing MT software, in selling multilingual retrieval for relevance scanning or in opening markets by issuing simple foreign-language descriptions. Information retrieval (IR) technologies are effective even if their degree of linguistic correctness is low. The success story of Machine Translation is partly owed to some simplifications, which made its start-up easier (leaving aside the political presetting of English-Russian translations). Simplifying reality was a promising approach,  because the reduction of parameters from syntax, morphology, and domain coverage formed the basis for the demonstration of MT’s feasibility. Moreover, the statistical approach in MT nourished the hope that reasonable results for English can be seen as evidence for the fact that MT can be done with similar quality for any other language. In subsequent decades experiments were performed with numerous other language pairs around the world including languages even, for which detailed linguistic knowledge was unavailable . The goal of these scientific and industrial research efforts was mainly to estimate the quality and costs of acceptable MT products for the commercially meaningful language pairs. The same holds true for multilingual information retrieval and multilingual information processing on other fields. With the ever growing number of language pairs for which customers require cost-efficient processing, four aspects became clear: 1. There are domains and language pairs for which not even human translation/IR is available, e.g., financial law texts from Finish to, say, Hausa. The question remains how to obtain these at all. 2. There is no representative bilingual data collection (a "corpus") for these language pairs at all. Statistical approaches hence will not be feasible within the next 5-10 years. How to obtain inexpensive translations in the mid-term for these "low resourced" languages? 3. Many languages (e.g., Hausa) have more than one writing system or changing orthography (compare the post-reform rules for German orthogography that are in force since 2006). This poses the challenge of how to obtain homogeneous corpora. 4. Multinational or global companies need language processing for promotion, local instruction, or contracts, that affords legally binding results.  2 Proceedings of the Adaptation of Language Resources and Tools for Closely Related Languages and Language Variants, pages 2–6, Hissar, Bulgaria, 13 September 2013.  The optimism of the pioneering years has yielded to scepticism regarding general recipes for multilingual processing such as translation, even for the traditional Western languages. In Europe, the expansion of the EU additionally demonstrated that democratic co-operation requires a huge work load of translation and bilingual information processing among today’s 23 official languages. The sheer number of languages, their diverse linguistic structure and their different public use are reasons enough to give up some of the starting assumptions and simplifications of the first decades. We discusses the rather different situations in Europe with regards to cross-lingual processing tasks in an English and American context along the following dimensions: 
In this paper we describe the construction of a parallel corpus between the standard and a non-standard language variety, speciﬁcally standard Austrian German and Viennese dialect. The resulting parallel corpus is used for statistical machine translation (SMT) from the standard to the non-standard variety. The main challenges to our task are data scarcity and the lack of an authoritative orthography. We started with the generation of a base corpus of manually transcribed and translated data from spoken text encoded in a speciﬁcally developed orthography. This data is used to train a ﬁrst phrasebased SMT. To deal with out-of-vocabulary items we exploit the strong proximity between source and target variety with a backoff strategy that uses character-level models. To arrive at the necessary size for a corpus to be used for SMT, we employ a boot-strapping approach. Integrating additional available sources (comparable corpora, such as Wikipedia) necessitates to identify parallel sentences out of substantially differing parallel documents. As an additional task, the spelling of the texts has to be transformed into the above mentioned orthography of the target variety. 
Pronominal and verbal voseo is a wellestablished variant in spoken language, and also very common in some written contexts web sites, literary works, screenplays or subtitles - in Rio de la Plata Spanish. An implementation of Río de la Plata Spanish (including voseo) was made in the open source collaborative system Apertium, whose design is suited for the development of new translation pairs. This work includes: development of a translation pair for Río de la Plata SpanishEnglish (back and forth), based on the Spanish-English pairs previously included in Apertium; creation of a bilingual corpus based on subtitles of movies; evaluation on this corpus of the developed Apertium variant by comparing it to the original Apertium version and to a statistical translator in the state of the art. 
In this paper, we examine the benefit of applying text segmentation methods to perform language identification in forums. The focus here is on forums containing a mixture of information written in Greek, English as well as Greeklish. Greeklish can be defined as the use of Latin alphabet for rendering Greek words with Latin characters. For the evaluation, a corpus was manually created by collecting web pages from Greek university forums and most specifically, pages containing information that combines Greek with English technical terminology and Greeklish. The evaluation using two well known text segmentation algorithms leads to the conclusion that despite the difficulty of the problem examined, text segmentation seems to be a promising solution. 
We introduce a generic approach for transferring part-of-speech annotations from a resourced language to a non-resourced but etymologically close language. We ﬁrst infer a bilingual lexicon between the two languages with methods based on character similarity, frequency similarity and context similarity. We then assign partof-speech tags to these bilingual lexicon entries and annotate the remaining words on the basis of sufﬁx analogy. We evaluate our approach on ﬁve language pairs of the Iberic peninsula, reaching up to 95% of precision on the lexicon induction task and up to 85% of tagging accuracy. 
Ekavian and Ijekavian are two different variants of the contemporary standard Serbian language. The difference between them is related to the reﬂex of the old Slavic vowel jat and it inﬂuences both the speaking and writing language norms. The sensibility of existing language identiﬁcation tools for both variants is of great importance for building representative corpora and development of relevant linguistics resources and tools underlying an automatic text processing. In this paper we present the results obtained after testing the three popular tools for language identiﬁcation on corpora containing documents from each of the two variants. As it will be reported, the identiﬁcation of Ijekavian variant is a much more difﬁcult task since the observed tools are not adopted to it at all. 
lected examples for main types of linguistic resources in the LLOD cloud, and objectives leading to the adaptation of Linked Data principles for any of these. Further, the talk elaborates on history and goals behind this effort, its relation to established standardization initiatives in the ﬁeld, and on-going community activities conducted under the umbrella of the Open Linguistics Working Group (OWLG) of the Open Knowledge Foundation (Chiarcos et al., 2012a), an initiative of experts from various ﬁelds concerned with linguistic data, which works towards 1. promoting the idea of open linguistic resources, 2. developing means for their representation, and 3. encouraging the exchange of ideas and resources across different disciplines. As the Linked Data paradigm can be used to facilitate any of these aspects, the OWLG identiﬁed potential application scenarios for linked and/or open resources in linguistics since its formation in 2010. The Working Group also has intensiﬁed its community-building efforts by means of a series of workshops, accompanying publications and data set releases. As a result of this process, numerous resources have been provided in a LinkedData-compliant way, and linked with each other, as sketched here for selected examples. References Roberto Busa. Index Thomisticus. FrommannHolzboog, Stuttgart, 1974. Christian Chiarcos, Sebastian Hellmann, Sebastian Nordhoff, Steven Moran, Richard Littauer, Judith Eckle-Kohler, Iryna Gurevych, Silvana Hartmann, Michael Matuschek, and Christian  
SPARQL queries have become the standard for querying linked open data knowledge bases, but SPARQL query construction can be challenging and timeconsuming even for experts. SPARQL query generation from natural language questions is an attractive modality for interfacing with LOD. However, how to evaluate SPARQL query generation from natural language questions is a mostly open research question. This paper presents some issues that arise in SPARQL query generation from natural language, a test suite for evaluating performance with respect to these issues, and a case study in evaluating a system for SPARQL query generation from natural language questions. 
In this paper we consider the prospect of extracting translations for words from the web of linked data. By searching for entities that have labels in both English and German we extract 665,000 translations. We then also consider a linguistic linked data resource, lemonUby, from which we extract a further 115,000 translations. We combine these translations with the Moses statistical machine translation, and we show that the translations extracted from the linked data can be used to improve the translation of unknown words. 
We describe work on porting linguistic and semantic annotation applied to the Austrian Baroque Corpus (ABaC:us) to a format supporting its publication in the Linked Open Data Framework. This work includes several aspects, like a derived lexicon of old forms used in the texts and their mapping to modern German lemmas, the description of morphosyntactic features and the building of domainspecific controlled vocabularies for covering the semantic aspects of this historical corpus. As a central and recurrent topic in the texts is death and dying, a first step in our work was geared towards the establishment of a deathrelated taxonomy. In order to provide for linguistic information to their textual content, labels of the taxonomy are pointing to linked data in the field of language resources. 
Formalization and representation of the language resources life cycle in a formal language to support the creation, update and application of the language resource instances is made possible via the developments in the area of ontologies and Linked Open Data. In the paper we present some of the basic functionalities of a system to support dynamic language resources. 
Digitized biodiversity literature provides a wealth of content for using biodiversity knowledge by machines. However, identifying taxonomic names and the associated semantic metadata is a difficult and labour intensive process. We present a system to support human assisted creation of semantic metadata. Information extraction techniques automatically identify taxonomic names from scanned documents. They are then presented to users for manual correction or verification. The tools that support the curation process include taxonomic name identification and mapping, and communitydriven taxonomic name verification. Our research shows the potential for these information extraction techniques to support research and curation in disciplines dependent upon scanned documents. 
In this paper we present a novel methodology for automatic information extraction from natural language texts, based on the integration of linguistic rules, multiple ontologies and inference resources, integrated with an abstraction layer for linguistic annotation and data representation. The SAURON system was developed to implement and integrate the methodology phases. The knowledge domain of legal realm has been used for the case study scenario through a corpus collected from the State Superior Court website in Brazil. The main contribution presented is related to the exploration of the flexibility of linguistic rules and domain knowledge representation, through their manipulation and integration by a reasoning system. Therefore, it is possible to the system to continuously interact with linguistic and domain experts in order to improve the set of linguistic rules or the ontology components. The results from the case study indicate that the proposed approach is effective for the legal domain. 
While it is widely recognized that streams of social media messages contain valuable information, such as important trends in the users’ interest in consumer products and markets, uncovering such trends is problematic, due to the extreme volumes of messages in such media. In the case Twitter messages, following the interest in relation to all known products all the time is technically infeasible. IE narrows topics to search. In this paper, we present experiments on using deeper NLP-based processing of product-related events mentioned in news streams to restrict the volume of tweets that need to be considered, to make the problem more tractable. Our goal is to analyze whether such a combined approach can help reveal correlations and how they may be captured. 
Active learning is a popular research area in machine learning and general domain natural language processing (NLP) communities. However, its applications to the clinical domain have been studied very little and no work has been done on using active learning for phenotyping tasks. In this paper we experiment with a speciﬁc kind of active learning known as uncertainty sampling in the context of four phenotyping tasks. We demonstrate that it can lead to drastic reductions in the amount of manual labeling when compared to its passive counterpart. 
This paper reports the automatic extraction of eleven negative symptoms of schizophrenia from patient medical records. The task offers a range of difﬁculties depending on the consistency and complexity with which mental health professionals describe each. In order to reduce the cost of system development, rapid prototypes are built with minimal adaptation and conﬁguration of existing software, and additional training data is obtained by annotating automatically extracted symptoms for which the system has low conﬁdence. The system was further improved by the addition of a manually engineered rule based approach. Rule-based and machine learning approaches are combined in various ways to achieve the optimal result for each symptom. Precisions in the range of 0.8 to 0.99 have been obtained. 
In order to analyse the information present in medical records while maintaining patient privacy, there is a basic need for techniques to automatically de-identify the free text information in these records. This paper presents a machine learning deidentiﬁcation system for clinical free text in Dutch, relying on best practices from the state of the art in de-identiﬁcation of English-language texts. We combine string and pattern matching features with machine learning algorithms and compare performance of three different experimental setups using Support Vector Machines and Random Forests on a limited data set of one hundred manually obfuscated texts provided by Antwerp University Hospital (UZA). The setup with the best balance in precision and recall during development was tested on an unseen set of raw clinical texts and evaluated manually at the hospital site. 
Rare diseases are not that rare: worldwide, one in 12-17 people will be affected by a rare disease. Newborn screening for rare diseases has been adopted by many European and North American jurisdictions. The results of genetic testing are given to millions of families and children’s guardians who often turn to the Internet to find more information about the disease. We found 42 medical forums and blogs where parents and other related adults form virtual communities to discuss the disease diagnosis, share related knowledge and seek moral support. Many people (up to 75% in some population groups) look for professional medical publications to find reliable information. How can it be made easier for these nonmedical professionals to understand such texts? We suggest that recommender systems, installed on web sites of research and teaching health care organizations, can be a tool that helps parents to navigate a massive amount of available medical information. In this paper, we discuss NLP architecture of such a system. We concentrate on processing epistemic modal expressions and helping the general public to evaluate the certainty of an event. 
Traditional information retrieval models assume keyword-based queries and use unstructured document representations. There is an abundance of event-centered texts (e.g., breaking news) and event-oriented information needs that often involve structure that cannot be expressed using keywords. We present a novel retrieval model that uses a structured event-based representation. We structure queries and documents as graphs of event mentions and employ graph kernels to measure the query-document similarity. Experimental results on two event-oriented test collections show signiﬁcant improvements over state-ofthe-art keyword-based models. 
We introduce an interactive visualization component for the JoBimText project. JoBimText is an open source platform for large-scale distributional semantics based on graph representations. First we describe the underlying technology for computing a distributional thesaurus on words using bipartite graphs of words and context features, and contextualizing the list of semantically similar words towards a given sentential context using graphbased ranking. Then we demonstrate the capabilities of this contextualized text expansion technology in an interactive visualization. The visualization can be used as a semantic parser providing contextualized expansions of words in text as well as disambiguation to word senses induced by graph clustering, and is provided as an open source tool. 
WordNet, a widely used sense inventory for Word Sense Disambiguation(WSD), is often too ﬁne-grained for many Natural Language applications because of its narrow sense distinctions. We present a semi-supervised approach to learn similarity between WordNet synsets using a graph based recursive similarity deﬁnition. We seed our framework with sense similarities of all the word-sense pairs, learnt using supervision on humanlabelled sense clusterings. Finally we discuss our method to derive coarse sense inventories at arbitrary granularities and show that the coarse-grained sense inventory obtained signiﬁcantly boosts the disambiguation of nouns on standard test sets.  To generate a coarse sense inventory, many researchers have focused on generating coarse senses for each word by merging the ﬁne-grained senses (Chugur et al., 2002) (Navigli, 2006). This approach has two problems. First, it requires a stopping criterion for each word — for example the number of ﬁnal classes. The right number of classes for each word cannot usually be predetermined even if the application is known. So such systems cannot be used to derive coarse senses for all the words. Second, inconsistent sense clusters are obtained because coarse senses are independently generated for each word. This leads to transitive closure errors and suggests that for deriving consistent coarse senses, instead of clustering senses for each word separately we should cluster synsets.  
Distance metric learning from high (thousands or more) dimensional data with hundreds or thousands of classes is intractable but in NLP and IR, high dimensionality is usually required to represent data points, such as in modeling semantic similarity. This paper presents algorithms to scale up learning of a Mahalanobis distance metric from a large data graph in a high dimensional space. Our novel contributions include random projection that reduces dimensionality and a new objective function that regularizes intra-class and inter-class distances to handle a large number of classes. We show that the new objective function is convex and can be efﬁciently optimized by a stochastic-batch subgradient descent method. We applied our algorithm to two different domains; semantic similarity of documents collected from the Web, and phenotype descriptions in genomic data. Experiments show that our algorithm can handle the high-dimensional big data and outperform competing approximations in both domains. 
In this work, we propose a graph-based approach to computing similarities between words in an unsupervised manner, and take advantage of heterogeneous feature types in the process. The approach is based on the creation of two separate graphs, one for words and one for features of different types (alignmentbased, orthographic, etc.). The graphs are connected through edges that link nodes in the feature graph to nodes in the word graph, the edge weights representing the importance of a particular feature for a particular word. High quality graphs are learned during training, and the proposed method outperforms experimental baselines. 
After recasting the computation of a distributional thesaurus in a graph-based framework for term similarity, we introduce a new contextualization method that generates, for each term occurrence in a text, a ranked list of terms that are semantically similar and compatible with the given context. The framework is instantiated by the deﬁnition of term and context, which we derive from dependency parses in this work. Evaluating our approach on a standard data set for lexical substitution, we show substantial improvements over a strong non-contextualized baseline across all parts of speech. In contrast to comparable approaches, our framework deﬁnes an unsupervised generative method for similarity in context and does not rely on the existence of lexical resources as a source for candidate expansions. 
Bootstrapping has recently become the focus of much attention in natural language processing to reduce labeling cost. In bootstrapping, unlabeled instances can be harvested from the initial labeled “seed” set. The selected seed set affects accuracy, but how to select a good seed set is not yet clear. Thus, an “iterative seeding” framework is proposed for bootstrapping to reduce its labeling cost. Our framework iteratively selects the unlabeled instance that has the best “goodness of seed” and labels the unlabeled instance in the seed set. Our framework deepens understanding of this seeding process in bootstrapping by deriving the dual problem. We propose a method called expected model rotation (EMR) that works well on not well-separated data which frequently occur as realistic data. Experimental results show that EMR can select seed sets that provide signiﬁcantly higher mean reciprocal rank on realistic data than existing naive selection methods or random seed sets.  reduce the cost of labeling instances, which is especially needed for tasks with high labeling costs. The performance of bootstrapping algorithms, however, depends on the selection of seeds. Although various bootstrapping algorithms have been proposed, randomly chosen seeds are usually used instead. Kozareva and Hovy (2010) recently reports that the performance of bootstrapping algorithms depends on the selection of seeds, which sheds light on the importance of selecting a good seed set. Especially a method to select a seed set considering the characteristics of the dataset remains largely unaddressed. To this end, we propose an “iterative seeding” framework, where the algorithm iteratively ranks the goodness of seeds in response to current human labeling and the characteristics of the dataset. For iterative seeding, we added the following two properties to the bootstrapping; • criteria that support iterative updates of goodness of seeds for seed candidate unlabeled instances.  
Review quality is determined by identifying the relevance of a review to a submission (the article or paper the review was written for). We identify relevance in terms of the semantic and syntactic similarities between two texts. We use a word order graph, whose vertices, edges and double edges help determine structure-based match across texts. We use WordNet to determine semantic relatedness. Ours is a lexico-semantic approach, which predicts relevance with an accuracy of 66% and f -measure of 0.67. 
Many organizations possess large collections of textual reports that document how a problem is solved or analysed, e.g. medical patient records, industrial accident reports, lawsuit records and investigation reports. Effective use of expert knowledge contained in these reports may greatly increase productivity of the organization. In this article, we propose a method for automatic extraction of reasoning chains that contain information used by the author of a report to analyse the problem at hand. For this purpose, we developed a graph-based text representation that makes the relations between textual units explicit. This representation is acquired automatically from a report using natural language processing tools including syntactic and discourse parsers. When applied to aviation investigation reports, our method generates reasoning chains that reveal the connection between initial information about the aircraft incident and its causes. 
Entity Resolution is the task of identifying which records in a database refer to the same entity. A standard machine learning pipeline for the entity resolution problem consists of three major components: blocking, pairwise linkage, and clustering. The blocking step groups records by shared properties to determine which pairs of records should be examined by the pairwise linker as potential duplicates. Next, the linkage step assigns a probability score to pairs of records inside each block. If a pair scores above a user-deﬁned threshold, the records are presumed to represent the same entity. Finally, the clustering step turns the input records into clusters of records (or proﬁles), where each cluster is uniquely associated with a single real-world entity. This paper describes the blocking and clustering strategies used to deploy a massive database of organization entities to power a major commercial People Search Engine. We demonstrate the viability of these algorithms for large data sets on a 50-node hadoop cluster. 
This paper presents a system that performs skill extraction from text documents. It outputs a list of professional skills that are relevant to a given input text. We argue that the system can be practical for hiring and management of personnel in an organization. We make use of the texts and the hyperlink graph of Wikipedia, as well as a list of professional skills obtained from the LinkedIn social network. The system is based on ﬁrst computing similarities between an input document and the texts of Wikipedia pages and then using a biased, hub-avoiding version of the Spreading Activation algorithm on the Wikipedia graph in order to associate the input document with skills. 
We present a number of semi-supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger, yet still relatively small, set of unlabelled sentences. We take two popular dependency parsers – one graph-based and one transition-based – and compare results for both. Results show that using semisupervised learning in the form of self-training and co-training yields only very modest improvements in parsing accuracy. We also try to use morphological information in a targeted way and fail to see any improvements. 
We present the ﬁrst statistical dependency parsing results for Lithuanian, a morphologically rich language in the Baltic branch of the Indo-European family. Using a greedy transition-based parser, we obtain a labeled attachment score of 74.7 with gold morphology and 68.1 with predicted morphology (77.8 and 72.8 unlabeled). We investigate the usefulness of different features and ﬁnd that rich morphological features improve parsing accuracy signiﬁcantly, by 7.5 percentage points with gold features and 5.6 points with predicted features. As expected, CASE is the single most important morphological feature, but virtually all available features bring some improvement, especially under the gold condition. 
We investigate statistical dependency parsing of two closely related languages, Croatian and Serbian. As these two morphologically complex languages of relaxed word order are generally under-resourced – with the topic of dependency parsing still largely unaddressed, especially for Serbian – we make use of the two available dependency treebanks of Croatian to produce state-of-the-art parsing models for both languages. We observe parsing accuracy on four test sets from two domains. We give insight into overall parser performance for Croatian and Serbian, impact of preprocessing for lemmas and morphosyntactic tags and inﬂuence of selected morphosyntactic features on parsing accuracy. 
This paper describes cross-task ﬂexible transition models (CTF-TMs) and demonstrates their effectiveness for Arabic natural language processing (NLP). NLP pipelines often suffer from error propagation, as errors committed in lower-level tasks cascade through the remainder of the processing pipeline. By allowing a ﬂexible order of operations across and within multiple NLP tasks, a CTF-TM can mitigate both cross-task and within-task error propagation. Our Arabic CTF-TM models tokenization, afﬁx detection, afﬁx labeling, partof-speech tagging, and dependency parsing, achieving state-of-the-art results. We present the details of our general framework, our Arabic CTF-TM, and the setup and results of our experiments. 
In this paper we use statistical dependency parsing techniques to detect NULL or Empty categories in the Hindi sentences. We have currently worked on Hindi dependency treebank which is released as part of COLINGMTPIL 2012 Workshop. Earlier Rule based approaches are employed to detect Empty heads for Hindi language but statistical learning for automatic prediction is not explored. In this approach we used a technique of introducing complex labels into the data to predict Empty categories in sentences. We have also discussed about shortcomings and difﬁculties in this approach and evaluated the performance of this approach on different Empty categories. 
This paper investigates the impact of different morphological and lexical information on data-driven dependency parsing of Persian, a morphologically rich language. We explore two state-of-the-art parsers, namely MSTParser and MaltParser, on the recently released Persian dependency treebank and establish some baselines for dependency parsing performance. Three sets of issues are addressed in our experiments: effects of using gold and automatically derived features, ﬁnding the best features for the parser, and a suitable way to alleviate the data sparsity problem. The ﬁnal accuracy is 87.91% and 88.37% labeled attachment scores for MaltParser and MSTParser, respectively. 
We present an empirical study on constructing a Japanese constituent parser, which can output function labels to deal with more detailed syntactic information. Japanese syntactic parse trees are usually represented as unlabeled dependency structure between bunsetsu chunks, however, such expression is insufﬁcient to uncover the syntactic information about distinction between complements and adjuncts and coordination structure, which is required for practical applications such as syntactic reordering of machine translation. We describe a preliminary effort on constructing a Japanese constituent parser by a Penn Treebank style treebank semi-automatically made from a dependency-based corpus. The evaluations show the parser trained on the treebank has comparable bracketing accuracy as conventional bunsetsu-based parsers, and can output such function labels as the grammatical role of the argument and the type of adnominal phrases. 
This paper revisits the work of (Malladi and Mannem, 2013) which focused on building a Statistical Morphological Analyzer (SMA) for Hindi and compares the performance of SMA with other existing statistical analyzer, Morfette. We shall evaluate SMA in various experiment scenarios and look at how it performs for unseen words. The later part of the paper presents the effect of the predicted morph features on dependency parsing and extends the work to other morphologically rich languages: Hindi and Telugu, without any language-speciﬁc engineering. 
This paper presents our preliminary conclusions as part of an ongoing effort to construct a new dependency representation framework for Turkish. We aim for this new framework to accommodate the highly agglutinative morphology of Turkish as well as to allow the annotation of unedited web data, and shape our decisions around these considerations. In this paper, we ﬁrstly describe a novel syntactic representation for morphosyntactic sub-word units (namely inﬂectional groups (IGs) in Turkish) which allows inter-IG relations to be discerned with perfect accuracy without having to hide lexical information. Secondly, we investigate alternative annotation schemes for coordination structures and present a better scheme (nearly 11% increase in recall scores) than the one in Turkish Treebank (Oﬂazer et al., 2003) for both parsing accuracies and compatibility for colloquial language. 
CoGrOO is an open source project. It is capable of identifying Portuguese mistakes like pronoun placement, noun agreement, subject-verb agreement, usage of the accent stress marker (‘), and other common errors found in Brazilian Portuguese writing. The CoGrOO grammar checker takes the user’s text as input, and outputs a list of possible errors. To accomplish this, it performs a shallow parsing followed by rulebased checking. Initially it analyzes the text using Natural Language Processing (NLP) techniques. The text goes through a pipeline of annotators to identify sentence and token boundaries, to assign a part-of-speech (POS) tag for each word and to ﬁnd phrase chunks 
Resumo. Segmentação topical visa segmentar um texto em passagens que representam subtópicos diferentes, os quais desenvolvem um tópico principal de um texto. A identificação de subtópicos é útil para diversas aplicações de Processamento de Linguagem Natural. Este artigo descreve a anotação de subtópicos em um córpus de textos jornalísticos em Português do Brasil. Em particular, foca-se em responder as questões cientificas a respeito da anotação do córpus, visando discutir e lidar com questões importantes de anotação e disponibilização de um córpus de referência para pesquisas sobre estruturação e segmentação topical. 1. Introduction Subtopic segmentation aims at finding the boundaries among text passages that represent different subtopics, which usually develop the main topic in a text. For example, a text about a football match would have “football” as the main topic and passages that might represent the subtopics “preparation and training for the match”, “moves of the match and final score”, and “schedule of next matches”. This task is useful for many important applications in Natural Language Processing (NLP), such as automatic summarization, question answering, and information retrieval and extraction. For instance, Prince and Labadie (2007) explain 49 Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology, pages 49–58, Fortaleza, CE, Brazil, October 21–23, 2013. c 2013 Sociedade Brasileira de Computa¸ca˜o  that information retrieval with the identification of subtopics in the retrieved texts may provide the user with text fragments that are semantically and topically related to a given query. This makes it easier for the user to quickly find the information of interest. Oh et al. (2007) suggest that a question answering system, which aims to answer a question/query submitted by the user, may link this query to the subtopics in a text in order to increase the accuracy of the identification of the answer. Wan (2008) says that, given some subtopic segmentation, automatic summarization may produce summaries that select different aspects from the collection of texts, producing better summaries. Given its usefulness, it is common to prepare a reference segmentation that supports not only the study and understanding of the phenomenon, but also the development and evaluation of systems for automatic subtopic segmentation. As the construction of corpora is a time consuming and very expensive task, it is necessary to follow procedures to systematize it and to ensure a reliable annotation, in order to produce a scientifically sound resource. Hovy and Lavid (2010) claim that it is necessary to be concerned with the reliability, validity, and consistency of the corpus annotation process. Because of this, many researchers, with such procedure in mind, suggest some methodological research questions, which may be summarized in the following 7 steps: (1) choosing the phenomenon to annotate and the underlying theory, (2) selecting the appropriate corpus, (3) selecting and training the annotators, (4) specifying the annotation procedure, (5) designing the annotation interface, (6) choosing and applying the evaluation measures, and (7) delivering and maintaining the product. In this paper, we report the subtopic annotation of a corpus of news texts written in Brazilian Portuguese. The corpus, called CSTNews1 (Cardoso et al., 2011), was originally designed for multi-document processing and contains 50 clusters, with each cluster having 2 or 3 texts on the same topic. In particular, we focus on answering the 7 research questions cited above, aiming at both discussing and dealing with important annotation decisions and making available a reference corpus for research on automatic subtopic structuring and segmentation. The remainder of this paper is organized into two main sections. Section 2 shows a brief discussion about relevant issues related to corpus annotation and some work on subtopic segmentation. Section 3 describes our annotation under the light of the 7 annotation questions, including the description of our dataset and the quality evaluation of the subtopic segmentation. Section 4 presents final remarks. 2. Related work There are several initiatives to create corpora that are linguistically annotated with varied phenomena from diverse perspectives, both for written and for spoken/transcribed data. We briefly overview some of these works in what follows. Hearst (1997) was one of the pioneer works in the area of subtopic segmentation with the proposal of the TextTiling algorithm. The author used a corpus of 12 magazine (expository) articles that had their subtopics segmented by technical researchers. The size of the texts varied from 1,800 to 2,500 words. In order to produce a reference 
The training data, in the form of an annotated corpus, should be large enough to allow the tagger to generalize what it learns to unseen sentences. The two most widespread corpora with annotated POS tags in Portuguese are Mac-Morpho [Alu´ısio et al. 2003], with around one million words, and Bosque [Afonso et al. 2002], with around 185 thousand. Both corpora cannot be combined to provide a larger resource, since each one deﬁnes a different tagset. For example, while Bosque has different tags for verbs in the inﬁnitive, gerund, participle and inﬂected forms, Mac-Morpho only distinguishes participles from the other three. On the other hand, Mac-Morpho has different tags for auxiliary and main verbs, which Bosque does not. The quality of the data is also important: too much noise (such as wrongly assigned tags) may affect the learning process. As the annotation is done by humans, mistakes are often introduced, and thus a rigourous checking procedure must be carried out. As for learning algorithms, there are many that have been proposed and succesfully applied in this task, usually capable of being employed in different languages. In Portuguese, experiments reported in the literature include Transformation Based Learning [dos Santos et al. 2008], Hidden Markov Models (HMM) and Variable Length Markov Chain (VLMC) [Kepler and Finger 2006], HMM with a character language model [Maia and Xexe´o 2011], among others. 98 Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology, pages 98–107, Fortaleza, CE, Brazil, October 21–23, 2013. c 2013 Sociedade Brasileira de Computa¸c˜ao  The work reported in this paper aimed at both these points. First, we performed a thorough error veriﬁcation and cleaning process on the Mac-Morpho corpus, which we used as training data for our models. We report the problems found and make our revised version publicly available. Also, we joined contracted forms (such as do for de + o), which appear splitted in the corpus, in order to reﬂect a real world scenario. Most works reported in the literature don’t mention this step. Second, we trained a POS tagger similar to the work found in [Collobert et al. 2011], based on multilayer perceptron neural networks and vector space models, and that achieves state-of-the-art performance in English. Our resulting tagger is also available online. Besides our model, we performed experiments with the OpenNLP POS tagger1 for comparison. 2. Corpus Treatment We chose to use the Mac-Morpho corpus because it is the biggest one available with POS tags in Portuguese. Mac-Morpho is composed of 109 ﬁles with texts from the Brazilian newspaper Folha de Sa˜o Paulo, and is divided in 10 sections, each having a given topic (such as agriculture, politics, sports, etc.). We identiﬁed some common errors in the corpus, such as missing words and repeated sentences. One example of sentence with a missing word is A degradac¸a˜o das terras pelo mau uso dos solos avanc¸a no. (“Land degradation due to inappropriate soil usage advances in the.”). These are prejudicial for machine learning, as a classiﬁer will get examples of impossible sequences (caused by missing words) or may be biased towards repeated examples. We developed simple heuristic rules to identify sentences with missing words, listed below. If any of the rules triggered for a given sentence, it was removed from the corpus, as there was no way to determine the missing word. 1. The sentence ends with a conjunction, preposition or article (including contractions). 2. An article appears before a verb, except for the case of ao (contraction between a and o), which is commonly used before inﬁnitives (e.g., ao ver, “upon seeing”). 3. An article appears before a preposition, conjunction or punctuation sign. 4. A comma appears before a period, colon, semicolon, exclamation or question mark. 5. The same punctuation sign appears twice in a row, except for exclamation and question marks. We are aware that these rules might trigger for false positives, but they are very rare in comparison with the actual mistakes. Using them, we removed 1,232 sentences. We also checked repeated sentences, discarding 2,947. Adding up, we removed 4,179 sentences from a total of 54,169 in the corpus (7.7%). Another problem was that half of the ﬁles in the corpus don’t indicate sentence boundaries; instead, these ﬁles have one token per line. Examining sentences separately is important since most taggers require that they be provided one at a time, so we used the Punkt sentence tokenizer from NLTK2 [Bird et al. 2009] to split the text into sentences. 1Available at http://opennlp.apache.org/ 2Available at http://www.nltk.org 99  This tool works by classifying periods as sentence delimiters or not (as in the case of abbreviations), and also tries to correctly handle quotation marks and parentheses in the end of a sentence. The other half of the ﬁles contained XML-like tags to indicate paragraph and sentence boundaries. Due to tag mismatches we couldn’t feasibly correct, we had to put off six of these ﬁles. With the remaining, we could easily split the sentences. After searching for characters not used in Portuguese, we found a few typographical errors such as the use of a diaresis instead of the proper accent, e.g., contra¨rio instead of contra´rio. We also found mistyped tags (identiﬁable as tags not in the deﬁned tagset) and a spurious $ symbol before some punctuation signs. All of these could be manually corrected. As for tokenization, important decisions in Portuguese concern how to treat preposition contractions and clitic pronouns. Mac-Morpho presents all components of these structures separately, but indicates that they were originally joined. In our experiments, we chose to redo all contractions appearing in the text, aiming at simulating a real world scenario. The tags for the contractions were obtained as a concatenation of the component tags and a plus sign, e.g. PREP+ART for the contraction of a preposition (tagged as PREP) and an article (ART). However, we chose to keep clitic pronouns separated from verbs. This decision was motivated by two factors: ﬁrst, it is trivial to identify clitics by simple pattern matching. Second, if we consider other NLP tasks such as semantic role labeling, separating pronouns from verbs is much more important than splitting preposition contractions. We set aside every tenth sentence in the corpus for testing, leaving the rest for training. This resulted in 4,999 and 44,991 sentences, respectively. Table 1 shows the number of occurrences for each of the 30 tags3. We can see that some tags, especially those involving contractions, are very rare.  Tag ADJ ADV-KS ART IN KS NPROP PCP PREP PREP+ART PREP+PRO-KS PREP+PROPESS PROADJ PRO-KS-REL PROSUB V  Train 39,009 289 61,905 267 10,816 82,541 17,623 82,103 52,579 28 479 13,767 8,298 5,710 75,686  Test 4,264 31 6,804 17 1,275 9,237 1,927 9,296 5,680 4 54 1,647 863 672 8,415  Total 43,273 320 68,709 284 12,091 91,778 19,550 91,399 58,259 32 533 15,414 9,161 6,382 84,101  Tag ADV ADV-KS-REL CUR KC N NUM PDEN PREP+ADV PREP+PROADJ PREP+PRO-KS-REL PREP+PROSUB PRO-KS PROPESS PU VAUX  Train 22,306 648 2,235 21,034 180,835 14,506 5,120 72 1,549 168 638 1,594 10,308 124,881 13,969  Test 2,509 69 239 2,333 20,181 1,692 546 13 166 19 72 165 1,228 14,025 1,552  Total 24,815 717 2,474 23,367 201,016 16,198 5,666 85 1,715 187 710 1,759 11,536 138,906 15,521  Table 1. Distribution of tags in the corpus  3The description of the tags used in Mac-Morpho can be found at http://www.nilc.icmc.usp. br/lacioweb/english/manuais.htm 100  3. The Tagger We implemented the model presented in [Collobert et al. 2011] for training a POS tagger. It receives a window of tokens as input and maps them to feature vectors, which are then concatenated and fed to a multilayer perceptron neural network. Figure 1 shows a window of three tokens being converted into vectors. There must be one neuron in the network input layer for each of these values. Figure 1. Example of a window of size 3 being converted into feature vectors 3.1. Word Representations In this architecture, each word has a corresponding real valued vector4. The representations used by [Collobert et al. 2011] were obtained in semi-supervised fashion through a neural language model, which was trained to distinguish positive examples of word sequences (extracted from a corpus) from negative ones (random perturbations of the positive example). In their training process, the authors sampled word sequences from a huge corpus and corrupted them by randomly replacing the middle word. Then, the neural model was fed both sequences and had to output a score for the original one higher by a given margin than for the corrupted one. The corrections in the network parameters were backpropagated to the word representations. As a result, words with similar meaning and usage had vectors with a small euclidian distance. Using such representations brings a couple of advantages: the automatic classiﬁer can easily detect words that should be treated similarly, and words not seen in the training data for a tagging task are not completely unknown, as long as they have a feature vector. Thus, out-of-vocabulary (OOV) impact is expected to be lesser. The unsupervised training for generating word representations, however, is extremely slow: the authors report weeks of training time. Motivated by the observations from [Turian et al. 2010] that representations obtained in different ways may be used by a classiﬁer to obtain good results in NLP tasks, we turned our attention to methods based on distributional semantics [Turney and Pantel 2010], which are much faster. In [Huang and Yates 2009], word representations generated with such methods are also employed for POS tagging in English; however, the system architecture in that work is very different from the one explored here. We used the software package Semantic Vectors5 [Widdows and Ferraro 2008] to induce representations from a collection of texts composed of the Portuguese Wikipedia6 and the PLN-BR corpus [Bruckschen et al. 2008]. We used the method known as Hyperspace Analogue to Language (HAL) [Lund and Burgess 1996], which consists in creating a table counting the occurrences of each word in the vocabulary next to each other word. 4Actually, not only words, but rather all types, including punctuation, numbers, etc. can have a feature vector. We use the term word here because it is commonly found in the literature. 5Available at https://code.google.com/p/semanticvectors/ 6Available at http://dumps.wikimedia.org/ptwiki 101  We induced vectors for all 89,075 word types that occurred at least 40 times in the corpus. Other words are mapped to a special vector generated randomly. Two other vectors were also generated randomly for the padding before after the limits of a sentence. We experimented with vectors having from 50 to 300 dimensions, and after examining the results, we concluded that the overall quality was about the same for all numbers of dimensions. We chose to keep the 50-dimension vectors for computational efﬁciency. Besides encoding word types, feature vectors can also represent discrete attributes such as presence of capitalization. To this end, each possible value of the attribute must have a corresponding vector; in the case of capitalization, values could be: all lowercase letters, initial uppercase letter, other combinations and a N/A value for punctuation and numbers. Thus, when the network is given a token, its type vector is concatenated with all other feature vectors. Figure 2 exempliﬁes this process.  Type na˜o sei . ...  Vector 0,97 -0,34 0,16 -0,81 0,09 -0,21 0,49 0,82 0,63 ...  Value All lowercase Uppercase initial Other case combinations N/A  Vector 0,04 0,72 -0,59 0,18 -0,12 -0,65 0,94 0,51  Token  Resulting Vector  Na˜o 0,97 -0,34 0,16 -0,59 0,18  sei  -0,81 0,09 -0,21 0,04 0,72  .  0,49 0,82 0,63 0,94 0,51  Figure 2. Representations including a discrete attribute  3.2. Simple Word Window Approach In the most basic setup, the simple word window approach, the network has one hidden layer and performs usual operations (weighted sum followed by a sigmoid function). It outputs a score fj for the token in the middle of the input window having each tag j; so, in order to tag all tokens in a given sentence, the network must examine each window at a time. In the case of tokens near the beginning or the end of a sentence, the input window is complemented with pseudo-tokens serving as padding. These pseudo-tokens also have their own corresponding feature vectors. Figure 3 shows an example of all possible windows obtained from a sentence.  Figure 3. Windows of size 3 produced from a sentence. The neural network is trained via backpropation, doing a gradient ascent aimed at maximizing the log likelihood over training data. Due to the paucity of space, we refer the reader to [Collobert et al. 2011] for a complete demonstration of the differentiation of the system output. Gradients are backpropagated until the input layer, so word representations 102  can be adjusted in the same way as the network connections. The corrections take place after running the network for each window. 3.3. Sentence Approach The network score may also be combined with tag transition scores, encoding knowledge such as “an article is very likely to be followed by a noun”, and thus working similarly to a Hidden Markov Model. These scores are encoded in a transition matrix A, which contains in each cell Ai,j the score for a token tagged with i being followed by another one tagged with j. Thus, denoting the neural network parameters with θ and the network score for tag j at the t-th token with ft,j, the score for a given sentence x of size T having the tag sequence y is:  T  
Speech is the most simple and natural form of human communication. Automatic speech processing technology is relatively recent and still enjoying its youth. Studies in speech processing show much promise and the trend of man-machine interaction seems to be leaning evermore towards speech commands, rather than mice and keyboards. This study aimed to bring contributions to the field of automatic speech processing of Brazilian Portuguese, whether for the linguistic studies that underpin it or for the software industry focused on NLP (Natural Language Processing). The objective was to conduct a case study, based on Brazilian Portuguese (hereafter BP) regarding the automatic disambiguation of homographic heterophone pairs (hereafter HH): pairs of words with the same spelling but unique pronunciations. In the present study, we analyzed HH pairs with distinct grammatical classes which differ in pronunciation, consisting of mid vowels [e, E] and [o, O], for example: inter[e]sse (noun) ~ inter[E]sse (verb); s[o]bre (preposition) ~ s[O]bre (verb); and j[o]go (noun) ~ j[O]go (verb). The principal difficulty lies in determining the transcription of <e> and <o> typed mid vowels when they contain a stressed syllable within a word that is not marked by an accent, due to the fact that it can be pronounced at times either as an open [E] or [O], while other times as the closed [e] or [o] varieties. 126 Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology, pages 126–137, Fortaleza, CE, Brazil, October 21–23, 2013. c 2013 Sociedade Brasileira de Computa¸c˜ao  Portuguese orthography is phonographic and alphabetic in nature. The graphic symbols that are used in writing BP seek to represent sounds that occur in speech [Robinson 2006]. Although alphabetic systems are those in which greater similarity between speech and orthography can be observed, there is no pure natural system. In other words, no orthographic system with natural origins in a speech community has a perfectly bi-univocal relationship between phone(me) and grapheme. This is due to the tensions between the dynamism of phonetic evolution within all living languages and the conservative nature, typical of all orthographies. This type of non-isomorphism between graphemes and phone(me)s in BP causes difficulties in situations where it is necessary to convert grapheme to phone(me) and vice versa. An example of this can be noted in writing acquisition (by both native and non-native speakers): the lack of parity between phone(me)s and graphemes is problematic and often, spelling errors committed by students arise due to transcription generalizations while writing a word based on how it is pronounced [Miranda 2006]. Another example of these difficulties can be seen with the development of automatic speech synthesis and recognition systems. In view of the non-mutual unambiguity between graphemes and phone(me)s, the tasks of synthesizing or recognizing speech can be regarded as quite complex, since the direct mapping between the sound signal and the written representation of a particular word or phrase does not necessarily match, adjustments must be made where the relationship between grapheme and phone(me) is not explicit. HH disambiguation is of great importance, even if the amount of HHs existing in an excerpt represent a very small number of instances in relation to the whole. In the context of speech synthesis, when an inappropriate sound (not agreeable/acceptable or even unintelligible to the listener/user of a system) is played, it ends up attracting an unnecessarily great deal of user attention to the error. Although the error rate could almost be considered negligible compared to the system’s success, the user will tend to question and often evaluate the efficiency of the synthesis system inadequately. Given its importance, HH research has been widely explored by the scientific community for many languages including English, Japanese, Chinese and Thai. In the case of European Portuguese (EP), the problems of converting graphemes and phone(me)s emerge mainly in the transcription of stressed mid vowels [e, E] and [o, O] [Veiga et al. 2011]. In that study, it was found that over 80% of conversion errors were caused by these exact same mid vowel shifts in stressed syllables. Mid vowels involve a range of linguistic phenomena in BP as they: (i) have different status [Miranda 2006], (ii) are realized in the context of rising pre-stressed syllables [Bortoni et al. 1992], [Viegas 2006], (iii) are metaphonic in nouns and verbs [Tomaz 2006] and (iv) shift vowel spaces [Roces 2010]. In this paper, we discuss a specific case of problems involving mid vowels: the automatic disambiguation of HH word pairs, that belong to different grammatical classes and are distinguished by openness of a mid vowel in stressed syllables (e.g., inter[e]sse (noun) ~ inter[E]sse (verb); and s[o]bre (preposition) ~ s[O]bre (verb)). With respect to stressed mid vowels [e, E] and [o, O] HH pairs, disambiguation is alike whether in BP or EP. Undoubtedly, language usage is different between BP and EP, but the linguistic 127  structural context that determines mid vowel stressed HH pair disambiguation is the same. Previous studies have discussed this problem for EP [Braga and Marques 2007; Braga 2008], using a rule-based algorithm for a minor group of HH pairs with mid vowels. For BP, Silva et al. (2009) employed a rule-based method which uses morphosyntactic information from word libraries built specifically for disambiguation. Manual classification methods were also put to use for BP [Cristófaro-Silva 2005]. In this paper, we propose a disambiguation method which addresses an extensive number of HH pairs with mid vowels, by using two contextual rules and morphosyntactic information from the automatic tagger, MXPOST. It should be noted that smaller sets of such words exist which must be treated by other methods. It should be reinforced that this paper only explores the large number of HH pairs with mid vowels which can be disambiguated by PoS tags alone. 2. Methodology 2.1 Selection and classification of HH word pairs Initially, we collected HH pairs present in two BP dictionaries published on the topic, namely: O Dicionário de Palavras Homógrafas, by Walmírio de Macedo (1961); and O Novo Dicionário de Acentuação das Palavras Homógrafas Heterófonas, by Pandiá Pandu (1972), a total of 1,812 pairs were collected. Of these, 772 repeated pairs (present in both dictionaries) as well as 141 containing the same grammatical category were excluded; therefore, 899 HH pairs were initially considered. Next, we verified the occurrence of the pairs considered in the MAC-Morpho [Aluísio et al. 2003]1 corpus. The choice of MACMorpho was due to several reasons: first, it is a corpus of BP, which contains 1,167,183 words from newspaper articles taken from the Folha de São Paulo in 1994 and, more specifically, from the following 10 sections: Agronomia (ag), Brasil (br), Cotidiano (co), Dinheiro (di), Esporte(es), Ciência (fc), Informática (if), Ilustrada (il), Mais! (ma) e Mundo (mu). Secondly, the MAC-Morpho has morphosyntactic annotation: initially, the corpus was annotated automatically by the parser “Palavras” [Bick 2000]; then, was reviewed manually by linguists. Thirdly, the corpus is freely available to the public on the web. After a thorough automatic analysis, 226 of the 899 HH pairs also appeared in MAC-Morpho. This small number of pairs could be due to the discrepancies which exist between the outdated dictionaries, containing all but dead; archaic words, and the modern lexicon (specifically in a journalistic setting). The 673 remaining pairs, registered in the dictionaries but which did not occur in the MAC-Morpho, were disregarded in the analysis due to the lack of reliable annotations. The next step consisted of a typological pair analysis, according to its oppositional grammatical nature and phonetic alternation (openness) present in varieties of the stressed 
We propose and evaluate a modular entity-centric Sentiment Analysis (ESA) method over Twitter data for the Portuguese language. The current paper is structured as follows: we present the most inﬂuential work on entity-based sentiment analysis and opinion mining on Twitter microtexts in Section 2. On Section 3, we present our proposal, which combines multiple techniques already developed in the literature to perform entity-centric sentiment analysis. We, then, evaluate our methods (Section 4). 2. Related Work While multiple solutions have been proposed for identiﬁcation of opinionated expressions in text, work on entity-centric sentiment analysis, i.e. to associate opinions with its referent, fall over three major approaches: those which use the context of an entity - as a ﬁxed window of words around the entity or its syntactic context - to identify an opinion about the entity [Grefenstette et al. 2004, Hu and Liu 2004]; those which use pre-deﬁned rules and linguistics resources - such as FrameNet - to identify the opinion reference as [Ding et al. 2008, Kim and Hovy 2006, Wu et al. 2009]; and those which relies on machine learning techniques as [Popescu and Etzioni 2005, Kobayashi et al. 2007, Ding and Liu 2010]. More related to our work, however, are the work of Jansen et al. [Jansen et al. 2009] and Silva et al. [Silva and TEAM 2011]. Jansen et al. use out-of-the-box commercial tool - no longer available - to perform Entity-centric subsentential sentiment analysis on Twitter. They apply their strategy on brand names for word-of-mouth detection. 173 Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology, pages 173–177, Fortaleza, CE, Brazil, October 21–23, 2013. c 2013 Sociedade Brasileira de Computa¸c˜ao  Silva et al. [Silva and TEAM 2011] describe the construction of the Twitto´metro - a tool for subsentential sentiment analysis on Twitter for the political domain. They explore a dictionary-based approach combined with lexico-syntactic rules to identify and compose opinions and to attribute reference to them. We believe that, while these work address a problem similar to ours, their strategies are not adequate for our case since they do not perform the analysis on a sufﬁciently grained fashion, as in [Jansen et al. 2009], or they rely too strongly on the structure of the domain, as in [Silva and TEAM 2011]. 3. Entity-centric sentiment analysis on Twitter data Given the difﬁculty of working with Twitter data - an extremely noisy channel - and the inexistence of Twitter speciﬁc linguistic processors for the Portuguese language, we opted to use only shallow linguistic information, such as lexical and morphological information. We perform the necessary steps to perform entity-centric sentiment analysis, such as entity identiﬁcation and opinion expression identiﬁcation separately and integrate the partial results with a opinion reference resolver. 3.1. Subsentential sentiment analysis In the opinion identiﬁcation and polarity classiﬁcation, we rely on a dictionary-based approach, similar to [Souza and Vieira 2012]. The method may be summarized by searching the opinion expression of the lexicon in the tweet. Since many words on the lexicon are in the canonical form, we apply a Stemmer and search for the words in the tweet if there is a polarized word with the same stem in the lexicon. The polarity is determined by the lexicon and the presence of a negation particle in the vicinity of the opinion expression. As opinion lexicon, we employ the OpLexicon [Souza et al. 2011] which already contains polarized emoticon and hastags - Twitter user-generated metadata. 3.2. Named entity recognition Following the work of Liu et al. [Liu et al. 2011] for NER on Twitter data and the Ratinov and Roth [Ratinov and Roth 2009], we developed a NER system based on a Conditional Random Fields tagger. As features for the NER system, we provide Ratinov and Roth’s [Ratinov and Roth 2009] lexical and morphological features and external information features - based on Repentino name gazetteer [Sarmento et al. 2006]. 3.3. Opinion reference resolution To identify which opinion-bearing expressions reference which named entity, we apply a opinion reference resolution method. In this phase, those opinion expressions that do not refer to a mentioned entity will be discarded. The results of this phase is then the annotated text. We implement a linear Support Vector Machine (SVM) classiﬁer with the features: • Positional features: location of the opinon expression (OE) and entity in the sentence, distance between the OE and the entity, centrality of the OE and entity in the sentence; • Number of identiﬁed entities in the sentence; • Length of the sentence; 174  • Number concordance of expression and entity. 
1. Introduction Academic researchers anywhere in the world must publish in English. However, their ability to produce well-written documents that get published is hindered by their linguistic capabilities. Researchers at Núcleo Interinstitucional de Linguística Computacional (NILC) have studied scientific writing over the last 20 years [Fontana et al. 1993; Aluísio and Oliveira 1995; Aluísio et al. 2001; 2005; Schuster et al. 2005; Dayrell et al. 2012]. They have developed techniques and software tools to assist novice researchers with their writing. Still, many students fail to publish their research findings because the grammatical and lexical errors in their writing interfere with their ability to convey their message clearly. Writing well is a pervasive problem among non-native speakers of English and several researchers [Han et al. 2006; Genoves et al. 2007; Lee 2009; Umezawa et al. 2013] have focused on this problem. This paper describes specific problem areas that can help students improve their writing. Section 2 describes our corpus-based approach. Section 3 discusses what we have learned from this analysis; we can turn these into teaching strategies to help writers. 2. Methods and materials: A Corpus-based Approach We collected 115 abstracts from students enrolled in five graduate scientific writing courses at several universities in Brazil, beginning in 20041. Students came from various disciplines, including pharmacy, chemistry, biology/genetics, physics, and computer 
This paper deals with the fast bootstrapping of Grapheme-to-Phoneme (G2P) conversion system, which is a key module for both automatic speech recognition (ASR), and text-to-speech synthesis (TTS). The idea is to exploit language contact between a local dominant language (Malay) and a very under-resourced language (Iban - spoken in Sarawak and in several parts of the Borneo Island) for which no resource nor knowledge is really available. More precisely, a pre-existing Malay G2P is used to produce phoneme sequences of Iban words. The phonemes are then manually post-edited (corrected) by an Iban native. This resource, which has been produced in a semi-supervised fashion, is later used to train the ﬁrst G2P system for Iban language. As a by-product of this methodology, the analysis of the “pronunciation distance” between Malay and Iban enlighten the phonological and orthographic relations between these two languages. The experiments conducted show that a rather efﬁcient Iban G2P system can be obtained after only two hours of post-edition (correction) of the output of Malay G2P applied to Iban words. 
The increasing popularity of social media has a large impact on the evolution of language usage. The evolution includes the transformation of some existing terms to enhance the expression of the writer’s emotion and feeling. Text processing tasks on social media texts have become much more challenging. In this paper, we propose LexToPlus, a Thai lexeme tokenizer with term normalization process. LexToPlus is designed to handle the intentional errors caused by the repeated characters at the end of words. LexToPlus is a dictionary-based parser which detects existing terms in a dictionary. Unknown tokens with repeated characters are merged and removed. We performed statistical analysis and evaluated the performance of the proposed approach by using a Twitter corpus. The experimental results show that the proposed algorithm yields an accuracy of 96.3% on a test data set. The errors are mostly caused by the out-ofvocabulary problem which can be solved by adding newly found terms into the dictionary. 
This paper proposes a post-editing model in which our three-level rule-based automatic post-editing engine called Grafix is presented to refine the output of machine translation systems. The type of corrections on sentences varies from lexical transformation to complex syntactical rearrangement. The experimental results both in manual and automatic evaluations show that the proposed system is able to improve the quality of our state-of-the-art English-Persian SMT system. 
Stemming is a process that groups morphologically related words into the same class and is widely used in information retrieval for improving recall rate. Here we study a set of statistical stemmers for Kannada, a resource-poor language with highly inﬂectional and agglutinative morphology. We compare stemming using simple truncation, clustering and an unsupervised morpheme segmentation algorithm on a sample from a text collection. We observe that a distance measure that rewards longest preﬁx matches is the best performing clustering-based stemmer. However, using a reasonably performing unsupervised morpheme segmentation seems to outperform the other stemming schemes considered. 
While stochastic route has been explored in solving the stemming problem, Conditional Random Field (CRF), a conditional probability based statistical model, has not been applied yet. We applied CRF to train a set of stemmers for Bengali natural language text. Care had been taken to design it language neutral so that same approach can be applied for other languages. The experiments yielded more than 86% accuracy. 
Transliteration is a process of transcribing a word of the source language into the target language such that when the native speaker of the target language pronounces it, it sounds as the native pronunciation of the source word. Statistical techniques have brought significant advances and have made real progress in various fields of Natural Language Processing (NLP). In this paper, we have analysed the application of Statistical Machine Translation (SMT) for solving the problem of Urdu Hindi transliteration using a parallel lexicon. We have designed total 24 Statistical Transliteration (ST) systems by combining different types of alignments, translation models and target language models. We have performed total 576 experiments and have reported significant results. From Hindi–to–Urdu transliteration, we have achieved the maximum word-level accuracy of 71.5%. From Urdu–to–Hindi transliteration, the maximum word-level accuracy is 77.8% when the input Urdu text contains all necessary diacritical marks and 77% when the input Urdu text does not contain all necessary diacritical marks. At character-level, transliteration accuracy is more than 90% in both directions.  
Spelling of words of a language are standardized by language authorities or consortiums and available in dictionaries or lexicons. For instance, “produkt” does not belong to English dictionary. Similarly “‫ “درميان‬is a correctly spelled word, while “‫ “ درميانر‬is a non-word in Urdu. Electronic representation of text is commonly used in today’s computing environment. The rich resourced languages like English have many applications with added tools. On the other hand, application development is in its infancy for less resourced language like Urdu. Spelling plays a vital role while humans write text electronically in computers. It is oblivious that terabytes of text is added in form of corpus or otherwise that is required to be spell checked, which is practically impossible to be done manually. In this work, various techniques for spellchecking have been studied and analyzed. All of them separately or a combination thereof can be used for the process of spell checking. Edit Distance technique has been widely used in spell checkers of various language, and a variation of this technique i.e. Reverse Edit Distance technique selected for suggesting correct words for nonwords. For Urdu, candidates are found by making 86n+41 comparisons for an ’n character’ length Urdu word. 
There are billions of believers of various religions in the world and Islam is the second largest religion having 1.6 billion followers. The primary written sources of religious beliefs and practices of Muslims are the Quran and the Hadith (saying and practices of their prophet Muhammad Peace Be Upon Him). Written text of the Quran and the Hadith books is of manageable size and hence state of the art text mining techniques can easily be applied on it. In this paper first a comprehensive review of existing applications offering various type of the Quran and the Hadith information retrieval is presented then a framework based on text mining techniques is proposed. Finally an application is developed to demonstrate the Quran and the Hadith information retrieval framework. This application is evaluated with the help of an end user assessment questionnaire. It is recorded that end users have observed salient advantages of the designed application. 
 Pashto clitics normally occurs in the second position (2P) of  a clause or sentence [2], however they may occur in various  other positions in sentences as well, but never occurs at the  beginning of a sentence as the following examples show.  ‫دې‬  ‫وقاص دې ورور‬  de  wroor dee waqas  aux brother CLT(yours)Waqas  Waqas is your brother.  ‫لو ستو‬  ‫دې‬  ‫کتاب‬  lwasto  dee  kitaab  were reading CLT  book  (You) were reading a book.  The following table gives a complete list of clitics used in Pashto language [3]. However, endoclitics generation in Pashto language occurs only with pronominal clitics mee, dee and yee, am.  Pashto Clitics ‫مي‬ ‫دې‬ ‫ې‬ ‫ام‬ ‫مو‬ ‫بو‬ ‫دي‬ ‫خو‬ ‫نو‬ ‫را‬ ‫د ر‬ ‫ور‬  Table I Pashto clitics  Gloss mee dee yee am mo ba de kho no Ra der wer  Type Pronominal Pronominal Pronominal Pronominal Pronominal Modal Modal Adverbial Adverbial Oblique Pronominal Oblique Pronominal Oblique Pronominal  Pashto clitics display properties commonly attributed to postlexical clitics as they are prosodically dependent on an adjacent prosodic element and co-occur with hosts from a limited set of syntactic categories. Tagey [4] derives the generalization that 2P Clitics appear after the “First stress bearing” phrasal constituent in the Pashto clause. The phrasal host must be stress-bearing and must contain at least one primary accent. 2P Clitics normally are not hosted by unaccented constituents. In general, it has been demonstrated in work done so far by other authors, that clitic placement in a phrase or a sentence is driven by syntactic, morphological and prosodic rules. The following example shows clitic occurring after a phrasal constituent. The unstressed material infront of the verb makes the clitic appear at the very right edge of the phrase. [‫]اغو د شلو کالو دنګو او خائستو پيغلو‬ NP [Peeghla khaaysta aw danga kaloo shaloo da aagha] [Girl pretty and tall years twenty postp that]NP  77 The 4th Workshop on South and Southeast Asian NLP (WSSANLP), International Joint Conference on Natural Language Processing, pages 77–82, Nagoya, Japan, 14-18 October 2013.  ‫دي نن بيا وليده‬ Wa‟lida Bya nen dee Saw again today CLT-you  Similarly, in the perfective form of the verb, the verb [akhistal] is prefixed with [wa] perfective marker resulting the following sentential form.  You saw that twenty years old tall and pretty girl again today. The rest of the paper is organized as follows. In section II, we describe the related works about Pashto endoclitics generation with examples. Section III reviews Syntactic and Phonological Features of Clitics. In section IV, we presented clitics placement rules. Conclusions are presented in section V. II. PASHTO ENDOCLITICS  ‫ما وا اخستل‬ Akhist-el wa maa buy3sg PERF 1SG I bought them. In the above sentence, deleting the strong pronoun [maa] introduces the clitic [mee]. This is shown by the sentence below.  Pashto allows clitics to be inserted into morphological words. The clitics with this property are called endoclitics. By definition endoclitics are inserted inside a word (verb in Pashto is split by endoclitic) by splitting the word into separate nonadjacent and semantically vacuous pieces. Endoclitics may not be regarded as morphological inflections as their semantics are unrelated to the host word in most of the cases. Morphologically endoclitics violate principle of Lexical Integrity (which states that syntactic operations may not interfere with morphology of words) [5]. The following example from [4] shows the occurrence of an endoclitic in a Pashto sentence with imperfective verb form.  ‫اخستل‬  ‫ما‬  Akhist-el  maa  buy3sg  1sg  I was buying them.(Tagey 1977:89)  Pashto is strictly a verb final language (word order in Pashto is SOV). The verb [akhist-el] appears non-finally and clitic [mee] occurs after it, because the clitic needs a host element if the strong pronoun maa is deleted. Sentences can thereby consist of simply a verb and a clitic.  ‫مې‬  ‫اخستل‬  mee  akhist-el  1SG  buy3sg  I was buying them.  Tagey observes that a-initial verbs can be split apart by clitics. Specifically, in the presence of a clitic the initial [a] of these verbs can split off from the rest of the verb root rendering the above sentence as show below. It is important to note that the part of verb appearing before the verb cannot be classified as either affix or an independent word.  ‫مې خستل‬  ‫ا‬  Khist-el mee a  buy3sg CLT ?? I bought them.  ‫وا‬ wa PERF  For explanatory purpose another example in which a clitic introduces as endoclitic is demonstrated by the following sentences.  ‫ا نو خستل‬ Khist-el na a buy3sg not ?? I did not buy it.  ‫وا‬ wa PERF  ‫مې‬  ‫ىغو‬  mee agha  CLT(1sg) 3SG  ‫ه‬  ‫واه‬  ah  waha  AUX3SG beat He beats him.  ‫و‬ wa PERF  ‫ې‬  ‫ىغو‬  yee agha  CLT(3sg) 3SG  Clitics always maintain second position. For example, if the strong pronoun [agha] is deleted from the second sentence above, the endoclitic would still be in second position after the perfective marker [wa], resulting in a sentence in which perfective marker [wa] (suffix) is no longer attached to the verb.  ‫ه‬  ‫واه‬  ‫ې‬  ‫و‬  a  Waha yee  wa  Aux3SG best  CLT(3sg) PERF  He beats him. (the pronoun agha deleted)  If the perfective marker [wa] is removed, the endoclitic is again placed in the second position, and moves to the last position in the sentence.  ‫مې خستل‬  ‫ا‬  Khist-el mee a  buy3sg CLT(I) ??  I was buying them.  ‫ې‬  ‫ه‬  ‫واه‬  yee  a  waha  CLT Aux3SG beat He was beating him.  78  There is another example which illustrates the insertion of tickle  CLT PERF  clitic between perfective marker and verb.  I tickled (her). (Tagey 1977:92)  ‫ولولو‬  ‫ې‬  ‫تو‬  walwala yee  ta  read  it(CLT) you  You read it.  When the strong pronoun [ta] is deleted, a new sentence is generated with endoclitic as shown below.  ‫لولو‬  ‫ې‬  lwaala yee  read  it(CLT)  You read it.  ‫و‬ wa PERF  Pashto verb has been identified to play important role in clitic placement. Kopris describes following five different classes of verb that have different behaviors in the presence of endoclitics [5].  1. Imperfective and Perfective verb 2. a-initial verb 3. Simple verb 4. Derivative verb 5. Doubly irregular verb In Bogel‟s analysis, endoclitics are subject to prosodic as well as syntactic constraints [6]. Prosodically, a clitic is placed after the first item bearing lexical stress in a sentence. Pashto is classified as an argument-dropping language, which is made possible by the syntactic agreement system on verbs and nouns. The endoclitics appear after aspect-caused stressed constituents. With regard to stress, Pashto verbs fall roughly into three classes, depending on their word-internal structure [6]. Bogel defines three classes of verbs with respect to clitics and endoclitics.  Class 1 Verbs: Monomorphemic imperfective verbs bear stress on the last syllable; the clitic is placed after the verb. The perfective monomorphemic verbs take on a perfective prefix [wa] that bears the main stress and the clitic occurs after the prefix. . The following shows an example.  ‫مې‬  ‫تښنولو‬  me texnawala  CLT  tickle  I was tickling (her). (Tagey 1977: 86)  In the perfective aspect the [wa] marker attaches to the verb  as a prefix and clitic occurs after it. In this case [wa] prefix is  stressed.  ‫تښنولو‬  ‫و مې‬  texnawala me wa  Class 2 Verbs: (compound prefix + root): These verbs form the perfective by means of a stress on the first syllable of the verb. A class-2 verb is bi-morphemic and is formed by a derivational prefix and a root. Syntactically these verbs are viewed as one unit.  Class 3 Verbs: (compound lexical item + auxiliary verb): They are similar to class-2 verbs, but are complex predicates (light verb + adjective/adverb/noun). These verbs are also split by clitics as shown by the next two example sentences.  ‫مې‬  ‫پوري وستو‬  mee pore wasta  1SG carry across(3sg,FEM,PAST)  I carried her across.  ‫وستو‬  ‫مې‬  Wasta  mee  PERF  1SG  I carried her across.  ‫پوري‬ pore Carry across  It has been suggested by Tagey [4], that there is a separate group of a-initial verbs, which has nine verbs that start with vowel [a]. These verbs show a very distinct behavior with regard to optional stress in the imperfective aspect. These verbs are: [akhistal] „to buy‟, [aleyal] „to singe‟, [acawal] „to throw‟, [agustal] „to put on‟, [alwtal] „to fly‟, [astawal] „to send‟, [arawal] „to turn over‟, [azmeyal] „to test‟, and [awral] „to hear‟. Some researchers have concluded that [a] was originally a prefix clitic [7], though [a] is no longer a recognizable prefix in Pashto. The class-2 and class-3 verbs can be thought of allowing clitic to be inserted post-lexically (at phonological level) into verb, without violating the principle of Lexical Integrity. In the perfective tense, a-initial verbs take the perfective prefix [we] like all other class-1 verbs. Perfective a-initial verbs display vowel coalescence, a process that is assumed to take place in the lexicon. The a-initial verbs in class-1 undergo vowel coalescence when they are preceded by a particle ending in a vowel i.e. [we] [na] and [ma].The Pashto rule of vowel coalescence (VC) and its interaction with clitic placement was studied by Tegey [4]. The following example illustrates the vowel coalescence.  ‫واخلو‬  ‫ې‬  waxla yee  buyPERF it You buy it.  ‫ې مو اخلو‬  maxla yee  ‫تو‬ ta you ‫تو‬ ta  *ta yee waaxla *ta yee maaxla  79  not-buy it you Don‟t buy it.  ‫نو اخلي‬  ‫تو ې‬  naxla yee  ta  no-buy it  you  Don‟t buy it.  *ta yee naaxla  The interaction of clitic and vowel coalescence is shown by the sentence below, as the clitic is inserted between vowel coalesced parts [wa] and [staw-el ].  ‫ستول‬  ‫وا‬  ‫مې‬  ‫نن‬  staw-el wa  mee  none  sent  PERF CLT today  I sent them today.  ‫ستول‬  ‫وا مې‬  staw -el mee wa  sent  CLT PERF  I sent them.  Tegey supposed that a syntactic rule for clitic placement applied after phonological rule (vowel coalescence). According to Kassie the phonological motivation of VC is the elimination of haitus (phonological gapping) [8]. She suggests the following process for VC. [ə]particle + [a, ɑ]verb→[ɑ]  Kassie concludes that VC is a type of lexically restricted phonological process and only a- for a-initial verbs undergo VC [8]. Therefore a- is considered as a morphological prefix, thereby claiming that no verb stem begins with a vowel. The a-initial verbs are described as midway between class-1 and class-2, as they take the perfective particle, but contain a stressable prefix. Clitics never move in the syntax, but may only move in the phonology to find a host to their left by the process of prosodic inversion. Bogel concludes that clitics are inserted into the morphological word post lexically, and are subjected to prosodic constraints and stress [6]. Moreover she assumes that prosody inserts clitics post lexically after an accent-bearing element, thereby asserting that attachment to a host is a strong prosodic constraint.  III. SYNTACTIC AND PHONOLOGICAL FEATURES OF CLITICS The first detailed study of Pashto clitics was carried out by Tagey [4] in his Phd dissertation. Tagey proposed that the clitic placement was syntactic, without elaborating on the exact syntactic mechanisms that determine clitic placement. Kassiere affirmed that the Pashto clitics can be dealt with only syntax and morphology. In Tagey‟s analysis “clitics are placed after the first major surface constituent that bears at least one main stress”. Apparently the suggestion posits that  phonology interacts with syntax inorder to place clitics in  correct position in sentences. In a later publication  Muhammad and Babrakzai proposed that clitic placement  can be treated as syntactic agreement [2]. According to Dost  clitics placement within sentences and clauses is governed by  constraints on syntax, prosody, lexical and sublexical levels,  thereby blurring the distinction and interaction between these  different levels [9].  In the analysis of Roberts, clitics are divided into two groups:  one appearing in the second position of the clause, and  another that appearing nearer to the verb [10]. In Robert‟s  analysis Pashto 2P clitics identify oblique-case NPs (in  ergative, accusative and genitive cases) and license null  oblique-case arguments. Clitics do not intervene among  conjuncts, and among the parts of any clause-initial  constituent.  ‫خو‬  ‫مې و اخستل‬  [‫او کاپۍ‬  ‫] کتاب‬  Kho wakhist-el mee ConjP[ copy aw kitaab]  Adv.CLT bought CLT-I notebook and book  I bought a notebook and a book but …..‟  But the native speakers cannot speak it as below:  ‫واخستل‬ wakhist-el Bought ‫واخستل‬ wakhist-el bought  ‫او کاپۍ‬  ‫کتاب مې‬  copy aw mee kitab  notebook and CLT-1sg book  Or  ‫کاپۍ‬  ‫مې‬  ‫کتاب او‬  copy  mee aw kitaab  notebook CLT-1sg and book  The ordering of pronominal clitics within a cluster (a series of adjacent clitics) is determined by person feature syntactically instead of a morphological template. Clitics bear person and number features which are not unique. Possessive clitics are dislocated from overt nominal with which they are semantically associated. There is a strong relationship between strong pronouns and pronominal clitics as stated by Roberts [10]. Strong pronouns occur at the same positions as the full NPs, but discourse neutral (topic) pronouns tend to appear in the form of second position Clitics. Pashto clitics have been studied from pure phonological aspect as well [10]. Roberts attempted to incorporate Pashto clitics into Chomsky‟s Minimalist Program. He states that 2P pronominal clitics are agreement morphemes based on the observation (also made in [2]) that pronominal 2P Clitics are in complementary distribution with verbal agreement morphology. This leads to the prediction that only ergative and accusative arguments may be criticized, whereas nominative or absolutive arguments cannot be criticized. Each clitic heads an agreement projection, whose specifier licenses a null pronominal argument. As an example the constituent tree for the  80  sentence [waqas me wanalide] „I didn‟t not see Waqas (a person).‟ is shown below from [9]. TP  NP  CliticP  waqasi prok  Clitic'  Clitic  AspP  mee 1SG NegP  Asp  IV. CLITIC PLACEMENT Pashto 2P clitics are unaccented, monosyllabic words that need single stressed phonological structure to their immediate left. According to existing clitic theories, Pashto clitics occur after first syntactic/prosodic structure in a sentence, which may be a word, phrase or prosodic unit. The first structure has to bear primary stress. Therefore, when placing clitics, syntax as well as prosodic constraints on phrases, words and units smaller than words have to be considered. Below Fig.1shows the sentence with strong pronoun on the top down parsing tree on the left side and with clitic on the right.  Neg  vP  wa-PERF  ne  NP  VP  tk ti lide 'see(MASC3SG)'  The NP leaves a trace and overtly moves to the subject position in the sentence thereby positioning itself with respect to clitic. Dost [9] objects to the proposal brought forward by Roberts and identifies three problems with the syntax only analysis. 1. 2P clitics should be double overt pronominal arguments based on the agreement hypothesis (“clitics as agreement hypothesis”), but that do not. 2. The second problem is with endoclitics (that clitic in-fixation). Tagey in his analysis, identifies three classes of main verbs in Pashto, that sometimes require 2P Clitics to appear after the first accented syllable of the verb, rather than after the verb itself.When stress occurs on the first syllable of the verb (in perfective aspect), 2P Clitics obligatorily occur after the stressed syllable. According to Dost, Robert‟s analysis does not consider clitics to be another natural word class, but rather considers them agreement morphemes and wrongly predicts the behavior of endoclitics. 3. The third problem identified by Dost is that 2P Clitics are prosodically required to have hosts. This property is lost by the syntactic treatment and EPP (Extended Projection Principle: the requirement that clauses have subjects), which is a syntactic requirement and not a prosodic one. According to Dost the hypothesis, that 2p clitics are agreement morphemes is wrong. The suggestion made by earlier studies that clitics should co-occur with full nominal arguments, is not supported by the actual linguistic data [6]. As an alternate Dost describes a Domain Based Approach to clitic placement in Head-Driven Phrase Structure Grammar (HPSG).  Figure 1. Cliticization  Clitics occur with wide range of syntactic hosts, and show low degree of selection with respect to hosts. Table II specifies the rules for replacing strong pronouns with 2P (second position) clitics based on syntactic constraints only. This process is called cliticization. The abbreviations used in Table II are defined in Table III.  Table II Rules for 2P Clitics  Rule 
Clause boundary identification has a significant role in improving the performance of different practical NLP systems. In this paper we have dealt with automatically identifying various types of clausal structures in Malayalam, a Dravidian language. The clausal sentences were collected from tourism and health domain available in the Web. We discuss about the annotation schema and the inter-annotators agreement for various clauses and also the automatic identification of clause boundaries using CRFs a Machine learning approach. To smooth the errors obtained from the CRFs tagging, we have used linguistic rules. For Inter-annotators agreement we have used kappa coefficient as the agreement statistic. The evaluation gave encouraging result. 
This paper describes a method to extract medical information from texts. The method targets to extract complaints and diagnoses from electronic health record texts. Complaints and diagnoses are fundamental information and can be used for more complex medical tasks. The method utilizes several medical knowledge resources to enhance the performance of extraction. With an evaluation using NTCIR10 MedNLP data, our method marked 86.53 in F1 score with a cross validation. The score is comparable to top scoring teams in NTCIR-10 MedNLP task. The approach taken to incorporate knowledge resources has a high generality. It is not restricted to the resources presented in this paper and can be applied to various other resources. 
Clinical decision support systems necessitate a disease knowledge base, which comprises a set of clinical ﬁndings for each disease. To efﬁciently represent the ﬁndings, this paper explores the relationship between clinical vocabulary and ﬁndings in medical literature through quantitative and qualitative analysis of representative disease databases. Although the data volume and the analyzed features are limited, the observations suggested the following. First, there are sets of clinical ﬁndings that are essential for physicians, but the majority of ﬁndings in medical literature are not the essential ones. Second, deviation of term frequency for clinical ﬁndings vocabulary is minimal, and clinical ﬁndings require appropriate grammar for efﬁcient representation of ﬁndings. Third, appropriate mapping of clinical ﬁndings with clinical vocabulary would allow the efﬁcient expression of clinical ﬁndings. 
With the increase of the number of medical records written in an electronic format, natural language processing techniques in the medical domain have become more and more important. For the purpose of the development and evaluation of machine learning-based systems to extract medical information, we recently participated in the NTCIR-10 MedNLP task. The task focused on Japanese medical records and aimed at evaluating different information extraction techniques on the common data set provided by the organizers. We implemented our baseline system based on structured perceptron and have developed its extensions. In this paper, we describe our systems and report on the evaluation of and the analysis on their performance. 
This year's MedNLP (Morita and Kano, et al., 2013) has two tasks: de-identification and complaint and diagnosis. We tested both machine learning based methods and an ad-hoc rule-based method for the two tasks. For the de-identification task, the rule-based method achieved slightly higher results, while for the complaint and diagnosis task, the machine learning based method had much higher recalls and overall scores. These results suggest that these methods should be applied selectively depending on the nature of the information to be extracted, that is to say, whether it can be easily patternized or not. 
The use of Electronic Health Records (EHRs) is spreading rapidly in several countries. The systems currently used, however, are not designed to permit secondary use of collected data. We present a new design for an EHR system that is capable of connecting information from multiple sites for use in clinical studies by means of an accounting information system and a hospital information system (HIS). This EHR system was designed for healthcare facilities in the Kyoto region. This paper describes how the conventional system can be extended into an EHR system that serves as a clinical information hub. 
This paper presents a notiﬁcation system to identify earthquakes from ﬁrsthand reports published on Twitter. Tweets from target regions in Australia and New Zealand are checked for earthquake keyword frequency bursts and then processed to identify evidence of an earthquake. The beneﬁt of our earthquake detector is that it relies on evidence of ﬁrsthand ‘felt’ reports from Twitter, provides an indication of the earthquake intensity and will be the trigger for further classiﬁcation of Tweets for impact analysis. We describe how the detector has been incrementally improved, most notably by the introduction of a text classiﬁer. During its initial ﬁve months of operation the system has generated 49 notiﬁcations of which 29 related to real earthquake events. 
In order to achieve high-level resilience against disasters, effective utilization of previous emergency management information is necessary. The goal of this project is to establish effective utilization of emergency management information and emergency response logs that are accumulated as a fundamental dataset to learn lessons for emergencies in the future. More precisely, we develop a framework that simpliﬁes structuring emergency management information and creating databases through various media or formats by exploiting technologies such as natural language processing to ﬁx the bottlenecks for inputting information in emergency response sites, to share disaster state, and to contribute towards achieving more effective use of human resources. The academic aim of this project is to establish the task of creating a database of emergency management information as a subﬁeld of natural language processing applications. 
At the early phase of the Great East Japan Earthquake a vast number of tweets were made on Twitter. Even though many of them were calling for emergency rescue, they were not found timely due to the vast number of tweets including wellintentioned tweets to support those emergency rescues. In order to deal with the situation, the authors developed and launched a website on March 16, 2011, which automatically extracts rescue requests, categorizes similar statements into several statements and then lists them. This paper covers in detail not only the technology of the system but also how it has already collaborated and been applied to #99japan, a project to support delivering emergency rescue requests. Note that #99japan is an activity to monitor the process of the rescue based on Twitter and coming from the thread started by temporary volunteers who organized on a Japanese textboard 2 ͪΌΜͶΔ “2channel.” 
This research proposes a framework for efﬁcient information extraction and ﬁltering in situations where 1) extreme reliability is important, 2) the amount of information to be combed through is massive, and 3) we can expect a relatively large number of human workers to be available. In particular, we are motivated by needs in times of crisis, and assume that in order to ensure the high level of reliability required, it will be necessary to have at least one human worker conﬁrm all extracted information. Given this setting, we propose a method to improve the efﬁciency of manual veriﬁcation by deciding which information to present to workers using machine learning techniques. Even given this efﬁcient search framework, the amount of information on the internet is still too much for one user to handle, so we additionally create a web-based framework that allows for collaborative work, and an algorithm that allows for this framework to work on large data in real-time. We perform an evaluation using data from Twitter after the Great East Japan Earthquake, and compare efﬁciency using both traditional keyword search and the proposed learningbased method. 
During the 2011 East Japan Earthquake and Tsunami Disaster, we had found a number of false information spread on Twitter, e.g., “The Cosmo Oil explosion causes toxic rain.” This paper extracts pieces of false information exhaustively from all the tweets within one week after the earthquake. Designing a set of linguistic patterns that correct false information, this paper proposes a method for detecting false information. More speciﬁcally, the method extracts text passages that match to the correction patterns, clusters the passages into topics of false information, and selects, for each topic, a passage explaining the false information the most suitably. In the experiment, we report the performance of the proposed method on the data set extracted manually from Web sites that are specialized in collecting false information. 
This paper clariﬁes the occurrence factors of commuters unable to return home and the returning-home decision-making at the time of the Great East Japan Earthquake by using Twitter data. First, to extract the behavior data from the tweet data, we identify each user’s returning-home behavior using support vector machines. Second, we create non-verbal explanatory factors using geotag data and verbal explanatory factors using tweet data. Then, we model users’ returning-home decisionmaking by using a discrete choice model and clarify the factors quantitatively. Finally, by sensitivity analysis, we show the effects of the existence of emergency evacuation facilities and line of communication. 
The Philippines is considered to be one of the world’s most disaster prone countries. Since the use of mobile devices continues to grow, many generated applications for mobile devices that will aid during disaster. While most of the people residing in the urban areas are mostly smartphone users, people who are living in the rural areas are still using low-cost phones. In order to equally provide information that will be needed on or during disaster, we created BahaBa a SMS-based route generation system targeted for mobile devices. The system accepts a SMS and generates a template-based response to the sender, containing instructions on the shortest path to the nearest safe place in a community. 
In this talk, we are going to give a systematic view of lexical semantics of Chinese language. From macro perspective point of view, lexical conceptual meanings are classified into hierarchical semantic types and each type plays some particular semantic functions of Host, Attribute, and Value to form a semantic compositional system. Lexical senses and their compositional functions will be exemplified by the semantic expressions of E-HowNet. Entities and relations are two major semantic types of the compositional system. Lexical senses and phrasal senses are compositions of these two types. From micro perspective point of view, each lexical word has individual idiosyncratic semantic contents, focuses and features. Hence words of same semantic type may have various different syntactic properties which make automatic language processing very difficult. On the other hand lexical syntactic properties are strongly influenced by lexical semantic structures. Morpho-semantic structures may systematically lead the way to derive lexical senses and syntactic behaviors of lexemes. It was observed that allowable alternations of sentence-patterns for verbs are mainly determined by their lexical semantic structures. It follows that senses and syntactic properties of out-ofvocabulary words become predictable and lexical compositional properties do shed light on automatic Chinese language understanding. Supporting evidences and logical interpretations of semantic and syntactic interactions will be presented in this talk. Vita Keh-Jiann Chen obtained a B.S. in mathematics from National Cheng Kung University in 1972. He received a Ph.D. in computer science from the State University of New York at Buffalo in 1981. Since then he joined the Institute of Information Science as an associate research fellow and became a research fellow in 1989. He was the deputy director of the institute from August 1991 to July 1994. His research interests include Chinese language processing, lexical semantics, lexical knowledge representation, and corpus linguistics. He had been and continued in developing the research environments for Chinese natural language processing including Chinese lexical databases, corpora, Treebank, lexical analyzer and parsers. Dr. Chen is one of the founding members of the Association of Computational Linguistic and Chinese Language Processing Society (also known as ROCLING). He had served as 2nd term president of the society from 1991 to 1993. Currently he is the board member of the Chinese Language Computer Society, the advisory board member of the International Journal of Computational Linguistics and Chinese Language Processing, the editor of journal of Computer Processing of Oriental Language. 
It is often assumed that Minimum Description Length (MDL) is a good criterion for unsupervised word segmentation. In this paper, we introduce a new approach to unsupervised word segmentation of Mandarin Chinese, that leads to segmentations whose Description Length is lower than what can be obtained using other algorithms previously proposed in the literature. Suprisingly, we show that this lower Description Length does not necessarily corresponds to better segmentation results. Finally, we show that we can use very basic linguistic knowledge to coerce the MDL towards a linguistically plausible hypothesis and obtain better results than any previously proposed method for unsupervised Chinese word segmentation with minimal human eﬀort. 
The accuracy of Chinese parsers trained on Penn Chinese Treebank is evidently lower than that of the English parsers trained on Penn Treebank. It is plausible that the essential reason is the lack of surface syntactic constraints in Chinese. In this paper, we present evidences to show that strict deep syntactic constraints exist in Chinese sentences and such constraints cannot be effectively described with context-free phrase structure rules as in the Penn Chinese Treebank annotation; we show that such constraints may be described precisely by the idea of Sentence Structure Grammar; we introduce how to develop a broad-coverage rule-based grammar for Chinese based on this idea; we evaluated the grammar and the evaluation results show that the coverage of the current grammar is 94.2%. 
Conversational spoken dialogue systems can assist individuals to communicate with machine to obtain relevant information to their problems efficiently and effectively. By referring to relevant response, individuals can understand how to interact with an intelligent system according to recommendations of dialogue systems. This work presents a response generation based on hierarchical semantic structure with POMDP Re-ranking for conversational dialogue systems to achieve this aim. The hierarchical semantic structure incorporates the historical information according to dialogue discourse to keep more than one possible values for each slot. According to the status of concept graph, the candidate sentences are generated. The near optimal response selected by POMDP Re-ranking strategy to achieve human-like communication. The MOS and recall/precision rates are considered as the criterion for evaluations. Finally, the proposed method is adopted for dialogue system in travel domain, and indicates its superiority in information retrieval over traditional approaches. 
This paper introduces an overview of Chinese Spelling Check task at SIGHAN Bake-off 2013. We describe all aspects of the task for Chinese spelling check, consisting of task description, data preparation, performance metrics, and evaluation results. This bake-off contains two subtasks, i.e., error detection and error correction. We evaluate the systems that can automatically point out the spelling errors and provide the corresponding corrections in students’ essays, summarize the performance of all participants’ submitted results, and discuss some advanced issues. The hope is that through such evaluation campaigns, more advanced Chinese spelling check techniques will be emerged. 
Spelling correction can assist individuals to input text data with machine using written language to obtain relevant information efficiently and effectively in. By referring to relevant applications such as web search, writing systems, recommend systems, document mining, typos checking before printing is very close to spelling correction. Individuals can input text, keyword, sentence how to interact with an intelligent system according to recommendations of spelling correction. This work presents a novel spelling error detection and correction method based on N-gram ranked inverted index is proposed to achieve this aim, spelling correction. According to the pronunciation and the shape similarity pattern, a dictionary is developed to help detect the possible spelling error detection. The inverted index is used to map the potential spelling error character to the possible corresponding characters either in character or word level. According to the N-gram score, the ranking in the list corresponding to possible character is illustrated. Herein, E-How net is used to be the knowledge representation of tradition Chinese words. The data sets provided by SigHan 7 bakeoff are used to evaluate the proposed method. Experimental results show the proposed methods can achieve accepted performance in subtask one, and outperform other approaches in subtask two. 
Chinese spelling check is an important component for many NLP applications, including word processor and search engines. However, compared to checkers for alphabetical languages (e.g., English or French), Chinese spelling checkers are more difficult to develop, because there are no word boundaries in Chinese writing system, and errors may be caused by various Chinese input methods. In this paper, we proposed a novel method to Chinese spelling checking. Our approach involves error detection and correction based on the phrasal statistical machine translation framework. The results show that the proposed system achieves significantly better accuracy in error detecting and more satisfactory performance in error correcting. 
In order to accomplish the tasks of identifying incorrect characters and error correction, we developed two error detection systems with different dictionaries. First system, called CKIP-WS, adopted the CKIP word segmentation system which based on CKIP dictionary as its core detection procedure; another system, called G1-WS, used Google 1T uni-gram data to extract pairs of potential error word and correction candidates as dictionary. Both detection systems use the confusion character set provided by the bakeoff organizer to reduce the suggested correction candidates. A simple maximizing tri-gram frequency model based on Google 1T tri-gram was designed to validate and select the correct answers. The CKIP group of Academia Sinica participated in both Sub-Task1 (Error Detection) and Sub-Task2 (Error Correction) in 2013 SIGHAN bakeoff. The evaluation results show that the performances of our systems are pretty good on both tasks. 
A ready set of commonly confused words plays an important role in spelling error detection and correction in texts. In this paper, we present a system named ACE (Automatic Confusion words Extraction), which takes a Chinese word as input (e.g., “不脛而走”) and automatically outputs its easily confused words (e.g., “不徑而走”, “不逕而走”). The purpose of ACE is similar to web-based set expansion – the problem of finding all instances (e.g. “Halloween”, “Thanksgiving Day”, “Independence Day”, etc.) of a set given a small number of class names (e.g. “holidays”). Unlike set expansion, our system is used to produce commonly confused words of a given Chinese word. In brief, we use some handcoded patterns to find a set of sentence fragments from search engine, and then assign an array of tags to each character in each sentence fragment. Finally, these tagged fragments are served as inputs to a pre-learned conditional random fields (CRFs) model. We present experiment results on 3,211 test cases, showing that our system can achieve 95.2% precision rate while maintaining 91.2% recall rate. 
This paper describes our Chinese spelling check system submitted to SIGHAN Bake-off 2013 evaluation. The main idea is to exchange potential error character with its confusable ones and rescore the modified sentence using a conditional random field (CRF)-based word segmentation/part of speech (POS) tagger and a tri-gram language model (LM) to detect and correct possible spelling errors. Experimental results on the Bakeoff 2013 tasks showed the proposed method achieved 0.50 location detection and 0.24 error location F-scores in subtask1 and 0.49 location and 0.40 correction accuracies and 0.40 correction precision in subtask2. 
Spelling check identifies incorrect writing words in documents. For the reason of input methods, Chinese spelling check is much different from English and it is still a challenging work. For the past decade years, most of the methods in detecting errors in documents are lexicon-based or probability-based, and much progress are made. In this paper, we propose a new method in Chinese spelling check by using maximum entropy (ME). Experiment shows that by importing a large raw corpus, maximum entropy can build a well-trained model to detect spelling errors in Chinese documents. 
Chinese spelling check (CSC) is still an open problem today. To the best of our knowledge, language modeling is widely used in CSC because of its simplicity and fair predictive power, but most systems only use the conventional n-gram models. Our work in this paper continues this general line of research by further exploring different ways to glean extra semantic clues and Web resources to enhance the CSC performance in an unsupervised fashion. Empirical results demonstrate the utility of our CSC system. 
In this paper, we describe in brief our system for Chinese Spelling Check Backoff sponsored by ACL-SIGHAN. It consists of three main components, namely potential incorrect character detection with a multiple-level analysis, correction candidate generation with similar character sets and correction scoring with n-grams. We participated in all the two sub-tasks at the Bakeoff. We also make a summary of this work and give some analysis on the results. 
This paper describes our system in the Bake-Off 2013 task of SIGHAN 7. We illustrate that Chinese spell checking and correction can be efficiently tackled with by utilizing word segmenter. A graph model is used to represent the sentence and a single source shortest path (SSSP) algorithm is performed on the graph to correct spell errors. Our system achieves 4 first ranks out of 10 metrics on the standard test set. 
We developed a Chinese spelling check system for error detection and error correction subtasks in the 2013 SIGHAN-7 Chinese Spelling Check Bake-off. By using the resources of Chinese phonology and orthographic components, our system contains four parts: high confidence pattern matcher, the detection module, the correction module, and the merger. We submitted 2 official runs for both subtasks. The evaluation result show that our system achieved 0.6016 in error detection F-score of subtask 1, and 0.448 in correction accuracy of subtask 2.1 
How to detect and correct misspelled words in documents is a very important issue for Mandarin and Japanese. This paper uses phonological similarity and orthographic similarity co-occurrence to train linear regression model. Using ACL-SIGHAN 2013 Bake-off Dataset, experimental results indicate that the detection F-score, error location F-score of our proposed method for Subtask 1 is 0.70 and 0.43 respectively, and the correction accuracy of the proposed method for Subtask 1 is 0.39. 
This paper describes details of NTOU Chinese spelling check system participating in SIGHAN-7 Bakeoff. The modules in our system include word segmentation, N-gram model probability estimation, similar character replacement, and filtering rules. Three dry runs and three formal runs were submitted, and the best one was created by bigram probability comparison without applying preference and filtering rules. 
Chinese character correction involves two major steps: 1) Providing candidate corrections for all or partially identified characters in a sentence, and 2) Scoring all altered sentences and identifying which is the best corrected sentence. In this paper a web-based measure is used to score candidate sentences, in which there exists one continuous error character in a sentence in almost all sentences in the Bakeoff corpora. The approach of using a web-based measure can be applied directly to sentences with multiple error characters, either consecutive or not, and is not optimized for one-character error correction of Chinese sentences. The results show that the approach achieved a fair precision score whereas the recall is low compared to results reported in this Bakeoff. 
Bilingual corpora play an important role as resources not only for machine translation research and development but also for studying tasks in comparative linguistics. Manual annotation of word alignments is of signiﬁcance to provide a gold-standard for developing and evaluating machine translation models and comparative linguistics tasks. This paper presents research on building an English-Vietnamese parallel corpus, which is constructed for building a Vietnamese-English machine translation system. We describe the speciﬁcation of collecting data for the corpus, linguistic tagging, bilingual annotation, and the tools specially developed for the manual annotation. An English-Vietnamese bilingual corpus of over 800,000 sentence pairs and 10,000,000 English words as well as Vietnamese words has been collected and aligned at the sentence level, and over 45,000 sentence pairs of this corpus have been aligned at the word level. Moreover, the 45,000 sentence pairs have been tagged using other linguistics tags, including word segmentation for Vietnamese text, chunker and named entity tags. 
Princeton WordNet (PWN) is one of the most influential resources for semantic descriptions, and is extensively used in natural language processing. Based on PWN, three Chinese wordnets have been developed: Sinica Bilingual Ontological Wordnet (BOW), Southeast University WordNet (SEW), and Taiwan University WordNet (CWN). We used SEW to sense-tag a corpus, but found some issues with coverage and precision. We decided to make a new Chinese wordnet based on SEW to increase the coverage and accuracy. In addition, a small scale Chinese wordnet was constructed from open multilingual wordnet (OMW) using data from Wiktionary (WIKT). We then merged SEW and WIKT. Starting from core synsets, we formulated guidelines for the new Chinese Open Wordnet (COW). We compared the five Chinese wordnets, which shows that COW is currently the best, but it still has room for further improvement, especially with polysemous words. It is clear that building an accurate semantic resource for a language is not an easy task, but through consistent efforts, we will be able to achieve it. COW is released under the same license as the PWN, an open license that freely allows use, adaptation and redistribution. 
This paper discusses the detection of missing annotation disagreements (MADs), in which an annotator misses annotating an annotation instance while her counterpart correctly annotates it. We employ annotator eye gaze as a clue for detecting this type of disagreement together with linguistic information. More precisely, we extract highly frequent gaze patterns from the pre-extracted gaze sequences related to the annotation target, and then use the gaze patterns as features for detecting the MADs. Through the empirical evaluation using the data set collected in our previous study, we investigated the effectiveness of each type of information. The results showed that both eye gaze and linguistic information contributed to improving performance of our MAD detection model compared with the baseline model. Furthermore, our additional investigation revealed that some speciﬁc gaze patterns could be a good indicator for detecting the MADs. 
The paper discusses HPSG as a framework for the computational analysis of Mandarin Chinese. We point out the main characteristics of the framework and show how they can be exploited to target languagespeciﬁc issues, describe existing grammar engineering work for Chinese and present our own effort in the implementation of a grammar for Chinese. The grammar is illustrated with two ﬁelds of phenomena, namely semantic and syntactic marking and valence alternations. We aim at the integration of work in theoretical linguistics into computational applications in order to complement statistical methods and thus increase their accuracy and scalability. 
This paper proposes the impacts of event and event actor alignment in English and Bengali phrase based Statistical Machine Translation (PB-SMT) System. Initially, events and event actors are identified from English and Bengali parallel corpus. For events and event actor identification in English we proposed a hybrid technique and it was carried out within the TimeML framework. Events in Bengali are identified based on the concept of complex predicate structures. There can be one-to-one and one-to-many mappings between English and Bengali events and event actors. We preprocess the parallel corpus by single tokenizing the multiword events and event-actors which reflects some significant gain on the PB-SMT system. We represent a hybrid alignment approach of events and event-actors in both English-Bengali training corpus by defining a rule based aligner and a statistical hybrid aligner. The rule base aligner assumes a heuristic that the sequence of events and event actors on the source (English) side are also maintained in the target (Bengali) side. The performance of PB-SMT system could vary depending on the number of events and event-actors that are identified in the parallel training data. The proposed system achieves significant improvements (5.79 BLEU points absolute, 53.02% relative improvement) over the baseline system on an English-Bengali translation task. 
Engineering Malaviya National Institute of Technology, Jaipur jkgarvit@gmail.com  Nitin Bania Department of Computer Engineering Malaviya National Institute of Technology, Jaipur nittinuts@gmail.com  Prateek Pareek Department of Computer Engineering Malaviya National Institute of Technology, Jaipur prtkpareek@gmail.com  Abstract: With recent developments in web technologies, percentage of web content in Hindi language is growing up at a lightning speed. Opinion classification research has gained tremendous momentum in recent times mostly for English language. However, there has been little work in this area for Indian languages. There is a need to analyse the Hindi language content and get insight of opinions expressed by people and various communities. In this paper, a method is proposed to increase the coverage of the Hindi SentiWordNet for better classification results. In addition to this, impact of the negation and discourse rules are investigated for Hindi sentiment analysis. Proposed algorithm produces 82.89% for positive reviews and 76.59 % for negative reviews, and an overall accuracy of 80.21%. Keywords: Sentiment Analysis, HSWN, Discourse and negation for Hindi Reviews. 1. Introduction Sentiment Analysis is a natural language processing task that deals with the extraction of opinion from a piece of text with respect to a topic (Pang et al., 2008). A large number of advertising industries and recommendation systems work on understanding liking and disliking of the people from their reviews. Hindi is the fourth highest speaking language in the world. The increasing user-generated content on the Internet is the motivation  behind the sentiment analysis research. Majority of the existing work in this field is for English language. Very little attention has been paid in direction of sentiment analysis for Hindi Language. Information content in Hindi is important to be analysed for the use of industries and government(s). Sentiment analysis is very difficult for Hindi language due to numerous reasons as follows. (1) Unavailability of well annotated standard corpora, therefore supervised machine learning algorithms cannot be applied. (2) Hindi is a resource scarce language; there are no efficient parser and tagger for this language. (3) Limited resources available for this language like HindiSentiWordNet (HSWN). It consists of limited numbers of adjectives and adverbs. All the words are available in inflected forms. Even all the inflected forms of the word are not present. HSWN is created using the Hindi WordNet and English SentiWordNet (SWN). During the creation of this resource for Hindi language, it is assumed that all synonyms have the same polarity while all antonyms have the reverse polarity of a word. This assumption neglected word sense intensity in terms of polarity, however polarity intensity of their word is important in opinion mining. (4) Even, Translation dictionaries may not account for all the words because of the  45 International Joint Conference on Natural Language Processing, pages 45–50, Nagoya, Japan, 14-18 October 2013.  language variations. Same words may be used in multiple contexts and context dependent word mapping is a difficult task, error prone and requires manual efforts. Using Translation method for generating subjective lexicon, there is a high possibility of losing the contextual information and sometimes may have translation errors. In this paper, an efficient approach is proposed for identifying sentiments and opinions from user generated content in Hindi. Main contributions of this paper are as follows. (1) Developed an annotated corpus for Hindi Movie Reviews. (2) Improve the existing HindiSentiWordNet (HSWN) by incorporating more opinion words into it. (3) Proposed new rules for negation handling and discourse relation for Hindi language reviews. This paper is organised as follows. Section 2 presents related work. Proposed approach is described in detail in Section 3. Section 4 discusses the experimental setup and results. Finally, Section 5 concludes and presents the future work. 2. Related Work Identifying the sentiment polarity is a complex task. To address the problem of sentiment classification various methods have been proposed (Agarwal et al. 2012, Agarwal et al. 2013, Pang et al. 2008). Joshi et al. (2010) proposed a fallback strategy in their paper. This strategy follows three approaches: In-language Sentiment Analysis, Machine Translation and Resource Based Sentiment Analysis. The final accuracy achieved by them is 78.14 %. They developed a lexical resource, HindiSentiWordNet (HSWN) based on its English format. Bakliwal et al. (2012) created lexicon using a graph based method. They explored how the synonym and antonym relations can be exploited using simple graph traversal to generate the subjectivity lexicon. Their proposed algorithm achieved approximately 79% accuracy on classification of reviews and 70.4% agreement with human  annotated. Mukherjee et al. (2012) showed that the incorporation of discourse markers in a bag-of-words model improves the sentiment classification accuracy by 2 - 4%. Bakliwal et al. (2011) proposed a method to classify Hindi reviews as positive or negative. They devised a new scoring function and test on two different approaches. They also used a combination of simple N-gram and POSTagged N-gram approaches. Ambati et al. (2011) proposed a novel approach to detect errors in the treebanks. This approach can significantly reduce the validation time. They tested it on Hindi dependency treebank data and were able to detect 76.63% of errors at dependency level. 3. Proposed Approach Proposed approach for Sentiment Analysis of Hindi review documents works as follows. Initially, annotated dataset is created for testing of the proposed algorithm. Then, rules are devised for handling negation and discourse relation which highly influence the sentiments expressed in the review. Further, HindiSentiWordNet (HSWN) is used for polarity values of words. Method for improving the HSWN is also proposed. Finally, overall semantic orientation of the review document is determined by aggregating the polarity values of all the words in the document 3.1. Preparation of Annotated Dataset Initially, 900 reviews are crawled from Hindi review websites, out of these 900 reviews, 130 reviews were rejected due to their objective nature manually. Next, for remaining 770 reviews, agreement was established on 662 reviews using Cohen‟s kappa. Out of these 662 total reviews, 380 were agreed as positive and 282 as negative. After that, Fleiss kappa was used for the agreement and achieved 0.8092 as kappa coefficient. This falls under the substantial agreement according to Fleiss kappa. Average size of the reviews in our dataset is 104 words.  46  3.2 Negation Handling  The negation operator (Example: , ,  etc.) inverts the sentiment of the word  following it. The usual way of handling negation in sentiment analysis is to consider a window of size n (typically 3 to 5) and reverse the polarity of all the words in the window. We reverse all the words in the window by adding (!) to every word, till either the sentence is completed or a violating expectation (or a contrast) conjunction or a delimiter is encountered. Negation on the basis of sentence structure may be applied either in forward or in backward direction. Some rules are proposed to handle negation, are discussed in following cases. CASE 1: If a sentence has only one single  negate word (“ ”, “ ”) i.e. negation is  present in a simple sentence. For example.  (1) य  । (2)  प  ई  प  यय | In the above sentence, due to negation, all the  words before the negation word “ ” would  be negated and the reverse polarity of the negated words would be considered further. The above examples will be negated as  (1) !य ! !  ।  (2) !  !प !  ! ई!  ! !  ! !  ! प  य य  But this negation rule may be invalid for  sarcastic and special form of sentences.  e.g. ई  य  ।  CASE 2: If a sentence has a negate word and conjunction, and index of conjunction is more than the index of negated word, forward negation is applied. For example:  (1) प  पई आ  आ  प य  |  (2)  । In the above sentences, negate word and the conjunction words are present and the index of conjunction is greater than the index of negate word; therefore, forward negation is applied. In above example, all the words after the conjunction will be negated .The above  examples will be negated as a) प  प ई !आ  !आ ! !प ! य ! ! ! |  b)  ! !  !! ! ।  CASE 3: If a sentence has “ ” multiple  times in sub-sentences separated by commas.  For example: (1)  , प  औ  |  “ ” usually occurs multiple times in this example sentence, with sub sentences separated by commas. Here for each “ ” the negation is applied in forward direction until a delimiter is encountered. The above example  will be negated as follows. !  ! !  ! , !प  औ !!  |  3.3 Discourse Relations  An essential phenomenon in natural language  processing is the use of discourse relations to  establish a coherent relation, linking phrases  and clauses in a text. The presence of  linguistic constructs like connectives, modals,  and conditional can alter sentiment at the  sentence level as well as the clausal or phrasal  level (Wolf et al., 2005). A coherent relation  reflects how different discourse segments  interact. Discourse segments are non-  overlapping spans of text. In this paper,  Violated Expectations like  ,  ,  etc. are handled. Violating expectation conjunctions oppose or refute the neighboring discourse segment. These conjunctions are categorized into the following two sub-categories: Conj_After and Conj_Infer. 3.3.1 Conj_After: It is the set of conjunctions that give more importance to the discourse segment that follows them. It means that actual segment is mostly reflected by the statement following the conjunction. So, in all the below examples, the discourse segments after the Conj_After (in bold) are given preferences and the previous sentences are dropped.  47  For example:  , ,  ,  :  ,  य  |  मगर:  ई  प |  :  |  र :  आ  प ,  य  य  ओ  |  3.3.2 Conclusive or Inferential Conjunctions These are the set of conjunctions, Conj_infer, that tend to draw a conclusion or inference. Hence, the discourse segment following them should be given more weight.  For example:  ,  म र:  „  ‟  प  |  3.4 Improvement of HSWN  Existing version of HindiSentiWordNet  consists of limited numbers of adjectives and adverbs. All those words are available in inflected forms. Even all the inflected forms  of the word are not present. HSWN is created  using the Hindi WordNet and English  SentiWordNet (SWN). During the creation of this resource for Hindi language, it is assumed that all synonyms have the same polarity  while all antonyms have the reverse polarity  of a word. HSWN is improved in the same way as it was developed initially. The main focus during the improvement was on missing  and inflected adjectives and adverbs.  Therefore, all the inflected words of the  existing root words are also included in the improved HSWN. Proposed approach is describes in Algorithm 1. In Step 4, Google translator is used in our experiment. In Step 6,  in case of sense disambiguation, the suitable  sense of the word refers to the sense which is  suitable according to the domain. Algorithm 1. Improvement of HSWN Step 1: Find out the adjectives and adverbs in the corpus that are not in HSWN. Step 2: Extract adjectives and adverb from document corpus. Step3: Now for each of the extracted word in Step 2. Step 4: Translate the given word into its English meaning using a bilingual resource. Step 5: Find the polarity of the translated word using English SentiWordNet. If single entry is found then go to step7. Step 6: Select the entry with the suitable and most common sense of the word. Step 7: Translate the word back to Hindi Step 8: Add it to the HSWN Step 9: return In our case the domain is the movie review dataset. If multiple senses are possible in the same domain, then select the most common sense among these words, which implies that multiple resources may need to be created for different domains. 3.5 Proposed Algorithm for Sentiment Analysis of Hindi Reviews The first step of the proposed algorithm is the pre-processing. Algorithm 2. Proposed Algorithm Step 1: For each document in the corpus Step 2: Apply Pre-Processing (a) Remove the Stop Words. (b) Apply Rules (Negation and Discourse). End of For Loop of Step 1; Step 3: For each token in the document. Step 4: Retrieve polarity (POL) from modified HSWN. Step 5: If (word is present in HSWN) Then go to Step 6 Else Add it to Missing Word List Step 6: If (word is negated) Then word.POL=-POL; Else Word.POL=POL; End of For Loop of Step 3; Step 7: Compute the aggregate polarity of the document (doc.POL) by adding the polarities values of all the token. Step 8: If (doc.POL > zero) Then label the document as positive Else If (doc.POL<zero) Then label the document as negative Else Classify the document as neutral. Step 9: Return the set of Labelled Documents.  48  Review documents are pre-processed by  applying stemming, negation and discourse  relations as discussed in previous sections.  After, the pre-processing, polarity value is  retrieved  from  the  improved  HindiSentiWordNet (HSWN). Finally, by  aggregating the polarity values of all the words  semantic orientation of the review document is  determined. Proposed approach is describes in  Algorithm 2.  4. Results and Discussions Proposed algorithm is tested on 662 movie review dataset created as described in previous sections. For various experimental settings, results are reported in Table 1. Initially, semantic orientation of a document is determined by aggregating the total polarity value of all the words in the document using existing HSWN. Experimental results show an accuracy of 50.45%, which is very less. The main reason for this observation was that most of the words in our dataset were not present in the HSWN and some words are inflected forms of the available words in HSWN. Further, proposed algorithm without any negation and discourse handling is applied using improved HSWN, and experimental results show that accuracy increased up to 69.79%. The proposed algorithm performs well for positive reviews, for the negatives performance needs to be improved.  Table 1. Accuracy of various experiments ACCURACY (In %)  S. Experimental No. Setup  Positi Negat Over  ve  ive all  
This paper addresses the resolution of inter-annotator disagreement in corpus construction. Given the consistency requirement which is regarded as a critical criterion of annotation quality, interannotator disagreement is usually considered harmful to the accuracy and reliability of annotation, and thus has to be resolved through various means. We claim that strictly adhering to consistency would also neglect the legitimate disagreement originating from ambiguity in natural languages. We highlight the values of preserving legitimate disagreement in annotation, and show that the possible problems resulting from inconsistency are avoidable. A preliminary annotation scheme is suggested for supporting multiple versions of annotation, without giving up the virtue of consistency. 
Named-Entity Recognition (NER) plays a significant role in classifying or locating atomic elements in text into predefined categories such as the name of persons, organizations, locations, expression of times, quantities, monetary values, temporal expressions and percentages. Several Statistical methods with supervised and unsupervised learning have applied English and some other Indian languages successfully. Malayalam has a distinct feature in nouns having no subject-verb agreement, which is of free order, makes the NER identification a complex process. In this paper, a hybrid approach combining rule based machine learning with statistical approach is proposed and implemented, which shows 73.42% accuracy. 
We have introduced here a new type of corpus annotation which we call Etymological Annotation (EA). We propose this new type because although, over the years, scientists have proposed corpus annotation of various types (Atkins, Clear and Ostler 1992, Biber 1993, Leech 2005), nobody has ever suggested that words included within corpora need to be annotated at their etymological level so that one can retrieve necessary linguistic information relating to antiquity of words and terms used in corpora. The applicational relevance of etymologically annotated corpora may be visualized in language description, language planning, language education, lexicology, language technology as well as in compilation of general, historical, learner and special dictionaries. In case of those languages, where one comes across large number of words borrowed from neighbouring and foreign languages, the proper identification of source of origin of words carries tremendous referential relevance in cross-lingual lexical database generation, morphological processing, part-of-speech tagging, e-learning, digital lexical profile generation, information retrieval, machine learning, and language documentation. Thus, etymologically annotated corpora become an essential resource of applied linguistics and language technology. We propose here to define this new event with necessary direction and guidance to develop etymologically tagged language corpora for all natural languages. 
UNL-ization is the process of converting Natural Language resource to Universal Natural Language (i.e., UNL). UNL is based on Interlingua approach, specifically designed by UNDL foundation for storing, summarizing, representing and describing information in a format which is independent to a natural language. This paper illustrates UNL-ization of Punjabi language with the help of IAN (i.e., Interactive ANalysis) tool. UNL-ization of major part-of-speeches of a Natural language viz Preposition, Conjunction, Determiner, Verb, Noun, Adjective, Time, Numbers and Ordinals has been done. In this paper UNL-ization process is explained with the help of three example sentences. Total 257 TRules and 623 Dictionary entries have been created, and the system has been tested successfully for Corpus500 (provided by UNDL Foundation) for Five hundred Punjabi sentences, comprising of all the major part-of-speeches and its FMeasure comes out to be 0.936 (on a scale of 0 to 1). 
In Taiwan, there are different types of TV programs, and each program usually has its broadcast length and frequency. We accumulate the broadcasted TV programs’ word-ofmouth on Facebook and apply the Backpropagation Network to predict the latest program audience rating. TV audience rating is an important indicator regarding the popularity of programs and it is also a factor to influence the revenue of broadcast stations via advertisements. Currently, the present media environments are drastically changing our media consumption patterns. We can watch TV programs on YouTube regardless location and timing. In this paper, we develop a model for predicting TV audience rating. We also present the audience rating trend analysis on demo system which is used to describe the relation between predictive audience rating and Nielsen TV rating. 
Due to the explosive growth of social media usage in Thailand, many businesses and organizations including market research agencies are seeking for tools which could perform real-time sentiment analysis on the large contents. In this paper, we propose S-Sense, a framework for analyzing sentiment on Thai social media. The proposed framework consists of analysis modules and language resources. Two main analysis modules, intention and sentiment, are based on classiﬁcation algorithm to automatically assign appropriate intention and sentiment class labels for a given text. To train classiﬁcation models, language resources, i.e., corpus and lexicon, are needed. Corpus consists of a collection of texts manually labeled with appropriate intention and sentiment classes. Lexicon consists of both general terms from dictionary and clue terms which help identifying the intention and sentiment. To evaluate performance and robustness of the analysis modules, we prepare a data set from Twitter posts and Pantip web board in mobile service domain. The experiments are set up to compare the performance between two different lexicon sets, i.e., general and clue terms. The results show that incorporating clue terms into feature vectors for constructing the classiﬁcation models yield signiﬁcant improvement in terms of accuracy. 
With massive social media data, e.g., comments, blog articles, or tweets, become available, there is a rising interest towards automatic metaphor detection from open social text. One of the most well-known approaches is detecting the violation of selectional preference. The idea of selectional preference is that verbs tend to have semantic preferences of their arguments. If we find that in some text, any arguments of these predicates are not of their preferred semantic classes, and it’s very likely to be a metaphor. However, previously only few papers have focuses on leveraging topical analysis techniques in metaphor detection. Intuitively, both predicates and arguments exhibit strong tendencies towards a few specific topics, and this topical information provides additional evidence to facilitate identification of selectional preference among text. In this paper, we study how the metaphor detection technique can be influenced by topical analysis techniques based on our proposed threestep approach. We formally define the problem, and propose our approach for metaphor detection, and then we conduct experiments on a real-world data set. Though our experimental result shows that topics do not have strong impacts on the metaphor detection techniques, we analyze the result and present some insights based on our study. 
Several Governments across the world are trying to move closer to their citizens to achieve transparency and engagement. The explosion of social media is opening new opportunities to achieve it. In this work we proposed an approach to monitor and analyze the citizen sentiment in social media by Governments. We also applied this approach to a real-world problem and presented how Government agencies can get benefited out of it. 
The paper aims to address an opinion mining problem: to find the helpful reviews from online consumer reviews before mining the detail. This task can benefit both the consumers and the companies. Consumers can read only the helpful opinions from helpful reviews before they purchase a product, while the companies can acquire the true reason why one product is liked or hated. A system is built to assess the difficulty of the problem. The experiment results show that helpful reviews can be identified with high precision from unhelpful ones. 
Wikipedia is the well-nigh successful and most popular free encyclopedia developed by many editors in collaborative manner. It provides multitude of opportunities for online large scale knowledge sharing between virtual communities by letting the viewer to create and edit articles directly in the web browser. Information on Wikipedia is expanding largely, but the increase in quantity is not proportional to quality of the content. The cursory observer of Wikipedia may not be able to differentiate between the good and the bad quality of the content. Despite the success of Wikipedia, trust on Wikipedia content is still questioned because of its open editing model. In this paper primarily the challenges for trust evaluation mechanisms, caused by the significant characteristics of Wikipedia’s knowledge base are discussed. Existing Wikipedia trust evaluation models are comprehensively surveyed and key issues related to these are highlighted. Finally based on this study new dimensions for effective trust evaluation mechanisms are proposed, which are aimed to setup clear goals for future research in this area. 
 Lexicon-based classifier is in the long term one of the main and most effective methods of polarity classification used in sentiment analysis, i.e. computational study of opinions, sentiments and emotions expressed in text (see Liu, 2010). Although it achieves relatively good results also for Czech, the classifier still shows some error rate. This paper provides a detailed analysis of such errors caused both by the system and by human reviewers. The identified errors are representatives of the challenges faced by the entire area of opinion mining. Therefore, the analysis is essential for further research in the field and serves as a basis for meaningful improvements of the system. 
This paper proposes a method to extract sentiment topics from a text collection. The method utilizes sentiment clues and a relaxed labeling schema to extract sentiment topics. Experiments with a quantitative and a qualitative evaluations was done to conﬁrm the performance of the method. The quantitative evaluation with a polarity classiﬁcation marked the accuracy of 0.701 in tweets and 0.691 in newswire texts. These performances are comparable to support vector machine baselines. The qualitative evaluation of polarity topic extraction showed an overall accuracy of 0.729, and a higher accuracy of 0.889 for positive topic extraction. The result indicates the efﬁcacy of our method in extracting sentiment topics. 
This paper presents the results of GRID project which aimed at studying the semantics of 24 emotion terms in 23 languages belonging to 8 language families (Indo-European, IndoIranian, Afro-Asiatic, Altaic, Uralic, Japonic, Sino-Tibetan, Niger-Congo, and Unclassified). We limit ourselves in this article only to two Slavic languages – Slovak and Czech and to two emotion terms – love and hatred – and try show how greatly information technologies helped the psychologists first of all to obtain, and then to process large volume of information from a bit less than 5000 people, active project participants, who live in 30 countries. 
The popularity of internet, downloading and purchasing music from online music shops are growing dramatically. As an intimate relationship presents between music and human emotions, we often choose to listen a song that suits our mood at that instant. Thus, the automatic methods are needed to classify music by moods even from the uploaded music files in social networks. However, several studies on Music Information Retrieval (MIR) have been carried out in recent decades. In the present task, we have built a system for classifying moods of Hindi songs using different audio related features like rhythm, timber and intensity. Our dataset is composed of 230 Hindi music clips of 30 seconds that consist of five mood clusters. We have achieved an average accuracy of 51.56% for music mood classification on the above data. 
 Several discourse annotated corpora now exist for NLP. But they use diﬀerent, not easily comparable annotation schemes: are the structures these schemes describe incompatible, incomparable, or do they share interpretations? In this paper, we relate three types of discourse annotation used in corpora or discourse parsing: (i) RST, (ii) SDRT, and (iii) dependency tree structures. We oﬀer a common language in which their structures can be deﬁned and furnished a range of interpretations. We deﬁne translations between RST and DT preserving these interpretations, and introduce a similarity measure for discourse representations in these frameworks. This will enable researchers to exploit diﬀerent types of discourse annotated data for automated tasks. 
This work proposes a generative model to infer latent semantic structures on top of manual speech transcriptions in a spoken dialog reservation task. The proposed model is akin to a standard semantic role labeling system, except that it is unsupervised, it does not rely on any syntactic information and it exploits concepts derived from a domain-speciﬁc ontology. The semantic structure is obtained with unsupervised Bayesian inference, using the Metropolis-Hastings sampling algorithm. It is evaluated both in terms of attachment accuracy and purity-collocation for clustering, and compared with strong baselines on the French MEDIA spoken-dialog corpus. 
The identiﬁcation of causal relations between verbal events is important for achieving natural language understanding. However, the problem has proven notoriously difﬁcult since it is not clear which types of knowledge are necessary to solve this challenging problem close to human level performance. Instead of employing a large set of features proved useful in other NLP tasks, we split the problem in smaller sub problems. Since verbs play a very important role in causal relations, in this paper we harness, explore, and evaluate the predictive power of causal associations of verb-verb pairs. More speciﬁcally, we propose a set of knowledge-rich metrics to learn the likelihood of causal relations between verbs. Employing these metrics, we automatically generate a knowledge base (KBc) which identiﬁes three categories of verb pairs: Strongly Causal, Ambiguous, and Strongly Non-causal. The knowledge base is evaluated empirically. The results show that our metrics perform significantly better than the state-of-the-art on the task of detecting causal verbal events. 
An appealing methodology for natural language generation in dialogue systems is to train the system to match a target corpus. We show how users can provide such a corpus as a natural side effect of interacting with a prototype system, when the system uses mixed-initiative interaction and a reversible architecture to cover a domain familiar to users. We experiment with integrated problems of sentence planning and realization in a referential communication task. Our model learns general and context-sensitive patterns to choose descriptive content, vocabulary, syntax and function words, and improves string match with user utterances to 85.8% from a handcrafted baseline of 54.4%. 
Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research community’s ability to develop better theories, as well as good off-the-shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the ﬁrst time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic-independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%. 
In this paper we focus on modeling friendships between humans as a way of working towards technology that can initiate and sustain a lifelong relationship with users. We do this by predicting friendship status in a dyad using a set of automatically harvested verbal and nonverbal features from videos of the interaction of students in a peer tutoring study. We propose a new computational model used to model friendship status in our data, based on a group sparse model (GSM) with L2,1 norm which is designed to accommodate the sparse and noisy properties of the multi-channel features. Our GSM model achieved the best overall performance compared to a non-sparse linear model (NLM) and a regular sparse linear model (SLM), as well as outperforming human raters. Dyadic features, such as number and length of conversational turns and mutual gaze, in addition to low level features such as F0 and gaze at task, were found to be good predictors of friendship status. 
Online debate forums provide a rich collection of differing opinions on various topics. In dual-sided debates, users present their opinions or judge other’s opinions to support their stance. In this paper, we examine the use of users’ intentions and debate structure for stance classiﬁcation of the debate posts. We propose a domain independent approach to capture users’ intent at sentence level using its dependency parse and sentiWordNet and to build the intention structure of the post to identify its stance. To aid the task of classiﬁcation, we deﬁne the health of the debate structure and show that maximizing its value leads to better stance classiﬁcation accuracies. 
Our aim is to acquire the attributes of concepts denoted by unknown words from users during dialogues. A word unknown to spoken dialogue systems can appear in user utterances, and systems should be capable of acquiring information on it from the conversation partner as a kind of selflearning process. As a ﬁrst step, we propose a method for generating more speciﬁc questions than simple wh-questions to acquire the attributes, as such questions can narrow down the variation of the following user response and accordingly avoid possible speech recognition errors. Speciﬁcally, we obtain an appropriately distributed conﬁdence measure (CM) on the attributes to generate more speciﬁc questions. Two basic CMs are deﬁned using (1) character and word distributions in the target database and (2) frequency of occurrence of restaurant attributes on Web pages. These are integrated to complement each other and used as the ﬁnal CM. We evaluated distributions of the CMs by average errors from the reference. Results showed that the integrated CM outperformed the two basic CMs. 
In situated dialogue, because humans and agents have mismatched capabilities of perceiving the shared physical world, referential grounding becomes difﬁcult. Humans and agents will need to make extra efforts by collaborating with each other to mediate a shared perceptual basis and to come to a mutual understanding of intended referents in the environment. In this paper, we have extended our previous graph-matching based approach to explicitly incorporate collaborative referring behaviors into the referential grounding algorithm. In addition, hypergraph-based representations have been used to account for group descriptions that are likely to occur in spatial communications. Our empirical results have shown that incorporating the most prevalent pattern of collaboration with our hypergraph-based approach signiﬁcantly improves reference resolution in situated dialogue by an absolute gain of over 18%. 
This paper presents a quantitative description of the lexical items used for linguistic feedback in the Corpus of Interactional Data (CID). The paper includes the raw ﬁgures for feedback lexical item as well as more detailed ﬁgures concerning interindividual variability. This effort is a ﬁrst step before a broader analysis including more discourse situations and featuring communicative function annotation. Index Terms: Feedback, Backchannel, Corpus, French Language 
In this paper, we describe novel methods for topic segmentation based on patterns of discourse organization. Using a corpus of news texts, our results show that it is possible to use discourse features (based on Rhetorical Structure Theory) for topic segmentation and that we outperform some well-known methods. 
This paper presents a practical methodology for the integration of reinforcement learning during the design of a Spoken Dialogue System (SDS). It proposes a method that enables SDS designers to know, in advance, the number of dialogues that their system will need in order to learn the value of each state-action couple. We ask the designer to provide a user model in a simple way. Then, we run simulations with this model and we compute conﬁdence intervals for the mean of the expected return of the state-action couples. 
Intelligent Tutoring Systems (ITSs) are now recognised as an interesting alternative for providing learning opportunities in various domains. The Reinforcement Learning (RL) approach has been shown reliable for ﬁnding efﬁcient teaching strategies. However, similarly to other human-machine interaction systems such as spoken dialogue systems, ITSs suffer from a partial knowledge of the interlocutor’s intentions. In the dialogue case, engineering work can infer a precise state of the user by taking into account the uncertainty provided by the spoken understanding language module. A model-free approach based on RL and Echo State Newtorks (ESNs), which retrieves similar information, is proposed here for tutoring. 
 2 Annotating Importance  Some things people say are more important, and some less so. Importance varies from moment to moment in spoken dialog, and contextual prosodic features and patterns signal this. A simple linear regression model over such features gave estimates that correlated well, 0.83, with human importance judgments. 
We use hand-crafted simulated negotiators (SNs) to train and evaluate dialogue policies for two-issue negotiation between two agents. These SNs differ in their goals and in the use of strong and weak arguments to persuade their counterparts. They may also make irrational moves, i.e., moves not consistent with their goals, to generate a variety of negotiation patterns. Different versions of these SNs interact with each other to generate corpora for Reinforcement Learning (RL) of argumentation dialogue policies for each of the two agents. We evaluate the learned policies against hand-crafted SNs similar to the ones used for training but with the modiﬁcation that these SNs no longer make irrational moves and thus are harder to beat. The learned policies generally do as well as, or better than the hand-crafted SNs showing that RL can be successfully used for learning argumentation dialogue policies in twoissue negotiation scenarios. 
In this work, we study the effectiveness of state-of-the-art, sophisticated supervised learning algorithms for dialogue act modeling across a comprehensive set of different spoken and written conversations including: emails, forums, meetings, and phone conversations. To this aim, we compare the results of SVM-multiclass and two structured predictors namely SVMhmm and CRF algorithms. Extensive empirical results, across different conversational modalities, demonstrate the effectiveness of our SVM-hmm model for dialogue act recognition in conversations. 
Determining the quality of an ongoing interaction in the ﬁeld of Spoken Dialogue Systems is a hard task. While existing methods employing automatic estimation already achieve reasonable results, still there is a lot of room for improvement. Hence, we aim at tackling the task by estimating the error of the applied statistical classiﬁcation algorithms in a two-stage approach. Correcting the hypotheses using the estimated model error increases performance by up to 4.1 % relative improvement in Unweighted Average Recall. 
SCXML was proposed as one description language for dialog control in the W3C Multimodal Architecture but lacks the facilities required for grounding and reasoning. This prohibits the application of many dialog modeling techniques for multimodal applications following this W3C standard. By extending SCXML with a Prolog datamodel and scripting language, we enable those techniques to be employed again. Thereby bridging the gap between respective dialog modeling research and a standardized architecture to access and coordinate modalities. 
We address the problem of localized error detection in Automatic Speech Recognition (ASR) output to support the generation of targeted clariﬁcations in spoken dialogue systems. Localized error detection ﬁnds speciﬁc mis-recognized words in a user utterance. Targeted clariﬁcations, in contrast with generic ‘please repeat/rephrase’ clariﬁcations, target a speciﬁc mis-recognized word in an utterance (Stoyanchev et al., 2012a) and require accurate detection of such words. We extend and modify work presented in (Stoyanchev et al., 2012b) by experimenting with a new set of features for predicting the likelihood of a local error in an ASR hypothesis on an unsifted version of the original dataset. We improve over baseline results, where only ASRgenerated features are used, by constructing optimal feature sets for utterance and word mis-recognition prediction. The f-measure for identifying incorrect utterances improves by 2.2% and by 3.9% for identiﬁying incorrect words. 
We model human responses to speech recognition errors from a corpus of human clariﬁcation strategies. We employ learning techniques to study 1) the decision to either stop and ask a clariﬁcation question or to continue the dialogue without clariﬁcation, and 2) the decision to ask a targeted clariﬁcation question or a more generic question. Targeted clariﬁcation questions focus speciﬁcally on the part of an utterance that is misrecognized, in contrast with generic requests to ‘please repeat’ or ‘please rephrase’. Our goal is to generate targeted clariﬁcation strategies for handling errors in spoken dialogue systems, when appropriate. Our experiments show that linguistic features, in particular the inferred part-ofspeech of a misrecognized word are predictive of human clariﬁcation decisions. A combination of linguistic features predicts a user’s decision to continue or stop a dialogue with accuracy of 72.8% over a majority baseline accuracy of 59.1%. The same set of features predict the decision to ask a targeted question with accuracy of 74.6% compared with the majority baseline of 71.8%.1 
Abstract1 In this demonstration, we will showcase BBN’s Speech-to-Speech (S2S) translation system that employs novel interaction strategies to resolve errors through user-friendly dialog with the speaker. The system performs a series of analysis on input utterances to detect out-ofvocabulary (OOV) named-entities and terms, sense ambiguities, homophones, idioms and ill-formed inputs. This analysis is used to identify potential errors and select an appropriate resolution strategy. Our evaluation shows a 34% (absolute) improvement in cross-lingual transfer of erroneous concepts in our English to Iraqi-Arabic S2S system. 
This demo paper describes our Artificial Intelligent Dialogue Agent (AIDA), a dialogue management and orchestration platform under development at the Institute for Infocomm Research. Among other features, it integrates different human-computer interaction engines across multiple domains and communication styles such as command, question answering, task-oriented dialogue and chat-oriented dialogue. The platform accepts both speech and text as input modalities by either direct microphone/keyboard connections or by means of mobile device wireless connection. The output interface, which is supported by a talking avatar, integrates speech and text along with other visual aids. 
We summarize the status of an ongoing project to develop and evaluate a companion for isolated older adults. Four key scientiﬁc issues in the project are: embodiment, interaction paradigm, engagement and relationship. The system architecture is extensible and handles realtime behaviors. The system supports multiple activities, including discussing the weather, playing cards, telling stories, exercise coaching and video conferencing. A live, working demo system will be presented at the meeting. 
We demonstrate a conversational interface that assists pedestrian users in navigating within urban environments and acquiring tourist information by combining spoken dialogue system, question-answering (QA), and geographic information system (GIS) technologies. In contrast to existing mobile applications which treat these problems independently, our Android agent addresses the problem of navigation and touristic question-answering in an integrated fashion using a shared dialogue context with multiple interleaved dialogue threads. In this paper, we present the architecture and features of our latest system, extended from an earlier version which was built and evaluated with real users (Janarthanam et al., 2013). The new features include navigation based on visible landmarks, navigation adapted to the user’s previous route knowledge, and tourist information pushing based on visible and proximal points-of-interest. The system also uses social media to infer “popularity” of geographical entities. 
The Parlance system for interactive search processes dialogue at a microturn level, displaying dialogue phenomena that play a vital role in human spoken conversation. These dialogue phenomena include more natural turn-taking through rapid system responses, generation of backchannels, and user barge-ins. The Parlance demonstration system diﬀerentiates from other incremental systems in that it is data-driven with an infrastructure that scales well. 
While natural language as an interaction modality is increasingly being accepted by users, remaining technological challenges still hinder its widespread employment. Tools that better support the design, development and improvement of these types of applications are required. This demo presents a prototyping framework for Spoken Dialog System (SDS) design which combines existing language technology components for Automatic Speech Recognition (ASR), Dialog Management (DM), and Text-to-Speech Synthesis (TTS) with a multi-step component for Natural Language Understanding (NLU). 
The Wizard of Oz (WOZ) method has been used for a variety of purposes in early-stage development of dialogue systems and language technology applications, from data collection, to experimentation, prototyping and evaluation. However, software to support WOZ experimentation is often developed ad hoc for speciﬁc application scenarios. In this demo we present WebWOZ, a web-based WOZ prototyping platform that aims at supporting a variety of experimental settings and combinations of different language technology components. We argue that a generic and distributed platform such as WebWOZ can increase the usefulness of the WOZ method. 
In this paper, we present a user study where a robot instructs a human on how to draw a route on a map, similar to a Map Task. This setup has allowed us to study user reactions to the robot’s conversational behaviour in order to get a better understanding of how to generate utterances in incremental dialogue systems. We have analysed the participants' subjective rating, task completion, verbal responses, gaze behaviour, drawing activity, and cognitive load. The results show that users utilise the robot’s gaze in order to disambiguate referring expressions and manage the flow of the interaction. Furthermore, we show that the user’s behaviour is affected by how pauses are realised in the robot’s speech. 
In situated dialogue, speakers share time and space. We present a statistical model for understanding natural language that works incrementally (i.e., in real, shared time) and is grounded (i.e., links to entities in the shared space). We describe our model with an example, then establish that our model works well on nonsituated, telephony application-type utterances, show that it is effective in grounding language in a situated environment, and further show that it can make good use of embodied cues such as gaze and pointing in a fully multi-modal setting. 
We describe the annotation of a multimodal corpus that includes pointing gestures and haptic actions (force exchanges). Haptic actions are rarely analyzed as fullﬂedged components of dialogue, but our data shows haptic actions are used to advance the state of the interaction. We report our experiments on recognizing Dialogue Acts in both ofﬂine and online modes. Our results show that multimodal features and the dialogue game aid in DA classiﬁcation. 
We explore the presence of indicators of psychological distress in the linguistic behavior of subjects in a corpus of semistructured virtual human interviews. At the level of aggregate dialogue-level features, we identify several signiﬁcant differences between subjects with depression and PTSD when compared to nondistressed subjects. At a more ﬁne-grained level, we show that signiﬁcant differences can also be found among features that represent subject behavior during speciﬁc moments in the dialogues. Finally, we present statistical classiﬁcation results that suggest the potential for automatic assessment of psychological distress in individual interactions with a virtual human dialogue system. 
Proceedings of the SIGDIAL 2013 Conference, page 203, Metz, France, 22-24 August 2013. c 2013 Association for Computational Linguistics 
Learning dialogue management models poses significant challenges. In a complex taskoriented domain in which information is exchanged via parallel, interleaved dialogue and task streams, effective dialogue management models should be able to make dialogue moves based on both the dialogue and the task context. This paper presents a data-driven approach to learning dialogue management models that determine when to make dialogue moves to assist users’ task completion activities, as well as the type of dialogue move that should be selected for a given user interaction context. Combining features automatically extracted from the dialogue and the task, we compare two alternate modeling approaches. The results of an evaluation indicate the learned models are effective in predicting both the timing and the type of system dialogue moves. 
Existing spoken dialogue systems are typically designed to operate in a static and well-deﬁned domain, and are not well suited to tasks in which the concepts and values change dynamically. To handle dynamically changing domains, techniques will be needed to transfer and reuse existing dialogue policies and rapidly adapt them using a small number of dialogues in the new domain. As a ﬁrst step in this direction, this paper addresses the problem of automatically extending a dialogue system to include a new previously unseen concept (or slot) which can be then used as a search constraint in an information query. The paper shows that in the context of Gaussian process POMDP optimisation, a domain can be extended through a simple expansion of the kernel and then rapidly adapted. As well as being much quicker, adaptation rather than retraining from scratch is shown to avoid subjecting users to unacceptably poor performance during the learning stage. 
This paper describes a new approach to automatic learning of strategies for social multi-user human-robot interaction. Using the example of a robot bartender that tracks multiple customers, takes their orders, and serves drinks, we propose a model consisting of a Social State Recogniser (SSR) which processes audio-visual input and maintains a model of the social state, together with a Social Skills Executor (SSE) which takes social state updates from the SSR as input and generates robot responses as output. The SSE is modelled as two connected Markov Decision Processes (MDPs) with action selection policies that are jointly optimised in interaction with a Multi-User Simulation Environment (MUSE). The SSR and SSE have been integrated in the robot bartender system and evaluated with human users in hand-coded and trained SSE policy variants. The results indicate that the trained policy outperformed the hand-coded policy in terms of both subjective (+18%) and objective (+10.5%) task success. 
Due to the mobile Internet revolution, people tend to browse the Web while driving their car which puts the driver’s safety at risk. Therefore, an intuitive and nondistractive in-car speech interface to the Web needs to be developed. Before developing a new speech dialog system in a new domain developers have to examine what the user’s preferred interaction style is in order to use such a system. This paper reports from a very recent driving simulation study and its preliminary results which are conducted in order to compare different speech dialog strategies. The use of command-based and conversational SDS prototypes while driving is evaluated on usability and driving performance. Different GUIs are designed in order to support the respective dialog strategy the most and to evaluate the effect of the GUI on usability and driver distraction. The preliminary results show that the conversational speech dialog performs more efﬁcient than the command-based dialog. However, the conversational dialog distracts more from driving than the command-based. Furthermore, the results indicate that an SDS supported by a GUI is more efﬁcient and better accepted by the user than without GUI. 
Goal-oriented dialog agents are expected to recognize user-intentions from an utterance and execute appropriate tasks. Typically, such systems use a semantic parser to solve this problem. However, semantic parsers could fail if user utterances contain out-of-grammar words/phrases or if the semantics of uttered phrases did not match the parser’s expectations. In this work, we have explored a more robust method of task prediction. We deﬁne task prediction as a classiﬁcation problem, rather than “parsing” and use semantic contexts to improve classiﬁcation accuracy. Our classiﬁer uses semantic smoothing kernels that can encode information from knowledge bases such as Wordnet, NELL and Freebase.com. Our experiments on two spoken language corpora show that augmenting semantic information from these knowledge bases gives about 30% absolute improvement in task prediction over a parserbased method. Our approach thus helps make a dialog agent more robust to user input and helps reduce number of turns required to detected intended tasks. 
We present virtual human dialogue models which primarily operate on the surface text level and can be extended to incorporate additional information state annotations such as topics or results from simpler models. We compare these models with previously proposed models as well as two human-level upper baselines. The models are evaluated by collecting appropriateness judgments from human judges for responses generated for a set of ﬁxed dialogue contexts. Our results show that the best performing models achieve close to human-level performance and require only surface text dialogue transcripts to train. 
Based on German production data from the ‘Kiel Corpus of Spontaneous Speech’, we conducted two perception experiments, using an innovative interactive task in which participants gave real oral responses to resynthesized question stimuli. Differences in the time interval between stimulus question and response show that segmental reduction, intensity level, and the shape of the phrase-final rise all function as cues to turn-taking in conversation. Thus, the phonetics of turntaking goes beyond the traditional triad of duration, voice quality, and F0 level. 
A fundamental problem in manual based gesture semantics reconstruction is the speciﬁcation of preferred semantic concepts for gesture trajectories. This issue is complicated by problems human raters have annotating fast-paced three dimensional trajectories. Based on a detailed example of a gesticulated circular trajectory, we present a data-driven approach that covers parts of the semantic reconstruction by making use of motion capturing (mocap) technology. In our FA3ME framework we use a complex event processing approach to analyse and annotate multi-modal events. This framework provides grounds for a detailed description of how to get at the semantic concept of circularity observed in the data. 
In many environments (e. g. sports commentary), situations incrementally unfold over time and often the future appearance of a relevant event can be predicted, but not in all its details or precise timing. We have built a simulation framework that uses our incremental speech synthesis component to assemble in a timely manner complex commentary utterances. In our evaluation, the resulting output is preferred over that from a baseline system that uses a simpler commenting strategy. Even in cases where the incremental system overcommits temporally and requires a ﬁlled pause to wait for the upcoming event, the system is preferred over the baseline. 
In this paper, we propose a framework for conversational robots that facilitates fourparticipant groups. In three-participant conversations, the minimum unit for multiparty conversations, social imbalance, in which a participant is left behind in the current conversation, sometimes occurs. In such scenarios, a conversational robot has the potential to facilitate situations as the fourth participant. Consequently, we present model procedures for obtaining conversational initiatives in incremental steps to engage such four-participant conversations. During the procedures, a facilitator must be aware of both the presence of dominant participants leading the current conversation and the status of any participant that is left behind. We model and optimize these situations and procedures as a partially observable Markov decision process. The results of experiments conducted to evaluate the proposed procedures show evidence of their acceptability and feeling of groupness. 
In this paper, we propose a novel approach to infer dialogue acts using the notion of tacit contracts. We describe the interpersonal linguistic features that our analysis grammar can identify in uttered texts and present an inference procedure that strictly separates the semantic and pragmatic steps of utterance understanding, thereby meeting a higher degree of modularity, a prerequisite for extending robot functionality. Keywords: Dialogue System; Dialogue Act; Attitude; Stance 
This study explores laughter distribution around topic changes in multiparty conversations. The distribution of shared and solo laughter around topic changes was examined in corpora containing two types of spoken interaction; meetings and informal conversation. Shared laughter was signiﬁcantly more frequent in the 15 seconds leading up to topic change in the informal conversations. A sample of informal conversations was then analysed by hand to gain further insight into links between laughter and topic change. 
We explore hedging in web forum conversations, which is interestingly different to hedging in academic articles, the main focus of recent automatic approaches to hedge detection. One of our main results is that forum posts using hedges are more likely to get high ratings of their usefulness. We also make a case for focusing annotation efforts on hedges that take the form of ﬁrst-person epistemic phrases. 
A challenge in dialogue act recognition is the mapping from noisy user inputs to dialogue acts. In this paper we describe an approach for re-ranking dialogue act hypotheses based on Bayesian classiﬁers that incorporate dialogue history and Automatic Speech Recognition (ASR) N-best information. We report results based on the Let’s Go dialogue corpora that show (1) that including ASR N-best information results in improved dialogue act recognition performance (+7% accuracy), and (2) that competitive results can be obtained from as early as the ﬁrst system dialogue act, reducing the need to wait for subsequent system dialogue acts. 
Can speaker gaze and speaker arm movements be used as a practical information source for naturalistic conversational human–computer interfaces? To investigate this question, we recorded (with eye tracking and motion capture) a corpus of interactions with a (wizarded) system. In this paper, we describe the recording, analysis infrastructure that we built for such studies, and analysis we performed on these data. We ﬁnd that with some initial calibration, a “minimally invasive”, stationary camera-based setting provides data of sufﬁcient quality to support interaction. 
Unsupervised dialogue act modeling holds great promise for decreasing the development time to build dialogue systems. Work to date has utilized manual annotation or a synthetic task to evaluate unsupervised dialogue act models, but each of these evaluation approaches has substantial limitations. This paper presents an incontext evaluation framework for an unsupervised dialogue act model within tutorial dialogue. The clusters generated by the model are mapped to tutor responses by a handcrafted policy, which is applied to unseen test data and evaluated by human judges. The results suggest that incontext evaluation may better reflect the performance of a model than comparing against manual dialogue act labels. 
We explore the plausibility of using automated spoken dialog systems (SDS) for administering survey interviews. Because the goals of a survey dialog system differ from more traditional information-seeking and transactional applications, different measures of task accuracy and success may be warranted. We report a large-scale experimental evaluation of an SDS that administered survey interviews with questions drawn from government and social scientific surveys. We compare two dialog confirmation strategies: (1) a traditional strategy of explicit confirmation on low-confidence recognition; and (2) no confirmation. With explicit confirmation, the small percentage of residual errors had little to no impact on survey data measurement. Even without confirmation, while there are significantly more errors, impact on the substantive conclusions of the survey is still very limited. 
Even though open-domain conversational dialogue systems are required in many ﬁelds, their development is complicated because of the ﬂexibility and variety of user utterances. To address this ﬂexibility, previous research on conversational dialogue systems has selected system utterances from web articles based on surface cohesion and shallow semantic coherence; however, the generated utterances sometimes contain irrelevant sentences with respect to the input user utterance. We propose a template-based approach that ﬁlls templates with the most salient words in a user utterance and with related words that are extracted using web-scale dependency structures gathered from Twitter. Our open-domain conversational dialogue system outperforms retrieval-based conventional systems in chat experiments. 
Learning and improving natural turn-taking behaviors for dialogue systems is a topic of growing importance. In task-oriented dialogue where the user can engage in task actions in parallel with dialogue, unrestricted turn taking may be particularly important for dialogue success. This paper presents a novel Markov Decision Process (MDP) representation of dialogue with unrestricted turn taking and a parallel task stream in order to automatically learn effective turn-taking policies for a tutorial dialogue system from a corpus. It also presents and evaluates an approach to automatically selecting features for an MDP state representation of this dialogue. The results suggest that the MDP formulation and the feature selection framework hold promise for learning effective turn-taking policies in taskoriented dialogue systems. 
Natural Language call routing remains a complex and challenging research area in machine intelligence and language understanding. This paper is in the area of classifying user utterances into different categories. The focus is on design of algorithm that combines supervised and unsupervised learning models in order to improve classification quality. We have shown that the proposed approach is able to outperform existing methods on a large dataset and do not require morphological and stop-word filtering. In this paper we present a new formula for term relevance estimation, which is a modification of fuzzy rules relevance estimation for fuzzy classifier. Using this formula and only 300 frequent words for each class, we achieve an accuracy rate of 85.55% on the database excluding the “garbage” class (it includes utterances that cannot be assigned to any useful class or that can be assigned to more than one class). Dividing the “garbage” class into the set of subclasses by agglomerative hierarchical clustering we achieve about 9% improvement of accuracy rate on the whole database. 
In this paper, we introduce our counseling dialog system. Our system interacts with users by recognizing what the users say, predicting the context, and following the users‟ feelings. For this interaction, our system follows three basic counseling techniques: paraphrasing, asking open questions, and reflecting feelings. To follow counseling techniques, we extracted 5W1H information and user emotions from user utterances, and we generated system utterances while using the counseling techniques. We used the conditional random field algorithm to extract 5W1H information, and constructed our counseling algorithm using a dialog strategy that was based on counseling techniques. A total of 16 adults tested our system and rated it with a higher score as an interactive communicator compared with the baseline system. 
The goal of the SIMSI (Safe In-vehicle Multimodal Speech Interaction) project is threefold. Firstly, to integrate a dialogue system for menu-based dialogue with a GUI-driven in-vehicle infotainment system. Secondly, to further improve the integrated system with respect to driver distraction, thus making the system safer to use while driving. Thirdly, to verify that the resulting system decreases visual distraction and cognitive load during interaction. This demo paper describes the integration of the two existing systems, and the test environment designed to enable evaluation of the system. 
We present two dialogue systems for language learning which both restrict the dialog to a speciﬁc domain thereby promoting robustness and the learning of a given vocabulary. The systems vary in how much they constrain the learner’s answer : one system places no other constrain on the learner than that provided by the restricted domain and the dialog context ; the other provides the learner with an exercise whose solution is the expected answer. The ﬁrst system uses supervised learning for simulating a human tutor whilst the second one uses natural language generation techniques to produce grammar exercises which guide the learner toward the expected answer. 
The demo shows Wikipedia-based opendomain information access dialogues with a talking humanoid robot. The robot uses face-tracking, nodding and gesturing to support interaction management and the presentation of information to the partner. 
We present a Wizard of Oz (WoZ) environment that was designed to build an artiﬁcial embodied intelligent tutoring system (ITS) that is capable of empathic conversations with school pupils aged between 10-13. We describe the components and the data that we plan to collect using the environment. 
 The demonstrator presents a test-bed for collecting data on human–computer dialogue: a fully automated dialogue system that can perform Map Task with a user. In a first step, we have used the test-bed to collect human–computer Map Task dialogue data, and have trained various data-driven models on it for detecting feedback response locations in the user’s speech. One of the trained models has been tested in user interactions and was perceived better in comparison to a system using a random model. The demonstrator will exhibit three versions of the Map Task dialogue system—each using a different trained data-driven model of Response Location Detection. 
We demonstrate a robotic agent in a 3D virtual environment that understands human navigational instructions. Such an agent needs to select actions based on not only instructions but also situations. It is also expected to immediately react to the instructions. Our agent incrementally understands spoken instructions and immediately controls a mobile robot based on the incremental understanding results and situation information such as the locations of obstacles and moving history. It can be used as an experimental system for collecting human-robot interactions in dynamically changing situations. 
We present an online system that provides a complete web-based sandbox for creating, testing and publishing embodied conversational agents. The tool, called Roundtable, empowers many different types of authors and varying team sizes to create flexible interactions by automating many editing workflows while limiting complexity and hiding architectural concerns. Finished characters can be published directly to web servers, enabling highly interactive applications. 
We present a data-driven model for detecting suitable response locations in the user’s speech. The model has been trained on human–machine dialogue data and implemented and tested in a spoken dialogue system that can perform the Map Task with users. To our knowledge, this is the first example of a dialogue system that uses automatically extracted syntactic, prosodic and contextual features for online detection of response locations. A subjective evaluation of the dialogue system suggests that interactions with a system using our trained model were perceived significantly better than those with a system using a model that made decisions at random. 
Barge-in enables the user to provide input during system speech, facilitating a more natural and efﬁcient interaction. Standard methods generally focus on singlestage barge-in detection, applying the dialogue policy irrespective of the barge-in context. Unfortunately, this approach performs poorly when used in challenging environments. We propose and evaluate a barge-in processing method that uses a prediction strategy to continuously decide whether to pause, continue, or resume the prompt. This model has greater task success and efﬁciency than the standard approach when evaluated in a public spoken dialogue system. Index Terms: spoken dialogue systems, barge-in 
We present an analysis of several publicly available automatic speech recognizers (ASRs) in terms of their suitability for use in different types of dialogue systems. We focus in particular on cloud based ASRs that recently have become available to the community. We include features of ASR systems and desiderata and requirements for different dialogue systems, taking into account the dialogue genre, type of user, and other features. We then present speech recognition results for six different dialogue systems. The most interesting result is that different ASR systems perform best on the data sets. We also show that there is an improvement over a previous generation of recognizers on some of these data sets. We also investigate language understanding (NLU) on the ASR output, and explore the relationship between ASR and NLU performance. 
In a spoken dialog system, dialog state tracking deduces information about the user’s goal as the dialog progresses, synthesizing evidence such as dialog acts over multiple turns with external data sources. Recent approaches have been shown to overcome ASR and SLU errors in some applications. However, there are currently no common testbeds or evaluation measures for this task, hampering progress. The dialog state tracking challenge seeks to address this by providing a heterogeneous corpus of 15K human-computer dialogs in a standard format, along with a suite of 11 evaluation metrics. The challenge received a total of 27 entries from 9 research groups. The results show that the suite of performance metrics cluster into 4 natural groups. Moreover, the dialog systems that beneﬁt most from dialog state tracking are those with less discriminative speech recognition conﬁdence scores. Finally, generalization is a key problem: in 2 of the 4 test sets, fewer than half of the entries out-performed simple baselines. 
For robust spoken conversational interaction, many dialog state tracking algorithms have been developed. Few studies, however, have reported the strengths and weaknesses of each method. The Dialog State Tracking Challenge (DSTC) is designed to address this issue by comparing various methods on the same domain. In this paper, we present a set of techniques that build a robust dialog state tracker with high performance: wide-coverage and well-calibrated data selection, feature-rich discriminative model design, generalization improvement techniques and unsupervised prior adaptation. The DSTC results show that the proposed method is superior to other systems on average on both the development and test datasets. 
This paper presents a generic dialogue state tracker that maintains beliefs over user goals based on a few simple domainindependent rules, using basic probability operations. The rules apply to observed system actions and partially observable user acts, without using any knowledge obtained from external resources (i.e. without requiring training data). The core insight is to maximise the amount of information directly gainable from an errorprone dialogue itself, so as to better lowerbound one’s expectations on the performance of more advanced statistical techniques for the task. The proposed method is evaluated in the Dialog State Tracking Challenge, where it achieves comparable performance in hypothesis accuracy to machine learning based systems. Consequently, with respect to different scenarios for the belief tracking problem, the potential superiority and weakness of machine learning approaches in general are investigated. 
Statistical approaches to dialog state tracking synthesize information across multiple turns in the dialog, overcoming some speech recognition errors. When training a dialog state tracker, there is typically only a small corpus of well-matched dialog data available. However, often there is a large corpus of mis-matched but related data – perhaps pertaining to different semantic concepts, or from a different dialog system. It would be desirable to use this related dialog data to supplement the small corpus of well-matched dialog data. This paper addresses this task as multi-domain learning, presenting 3 methods which synthesize data from different slots and different dialog systems. Since deploying a new dialog state tracker often changes the resulting dialogs in ways that are difﬁcult to predict, we study how well each method generalizes to unseen distributions of dialog data. Our main result is the ﬁnding that a simple method for multi-domain learning substantially improves performance in highly mis-matched conditions. 
Many dialog state tracking algorithms have been limited to generative modeling due to the influence of the Partially Observable Markov Decision Process framework. Recent analyses, however, raised fundamental questions on the effectiveness of the generative formulation. In this paper, we present a structured discriminative model for dialog state tracking as an alternative. Unlike generative models, the proposed method affords the incorporation of features without having to consider dependencies between observations. It also provides a flexible mechanism for imposing relational constraints. To verify the effectiveness of the proposed method, we applied it to the Let’s Go domain (Raux et al., 2005). The results show that the proposed model is superior to the baseline and generative model-based systems in accuracy, discrimination, and robustness to mismatches between training and test datasets. 
In this paper, we describe two dialogue state tracking models competing in the 2012 Dialogue State Tracking Challenge (DSTC). First, we detail a novel discriminative dialogue state tracker which directly estimates slot-level beliefs using deterministic state transition probability distribution. Second, we present a generative model employing a simple dependency structure to achieve fast inference. The models are evaluated on the DSTC data, and both signiﬁcantly outperform the baseline DSTC tracker. 
This paper presents our approach to dialog state tracking for the Dialog State Tracking Challenge task. In our approach we use discriminative general structured conditional random ﬁelds, instead of traditional generative directed graphic models, to incorporate arbitrary overlapping features. Our approach outperforms the simple 1-best tracking approach. 
We describe our experience with engineering the dialog state tracker for the ﬁrst Dialog State Tracking Challenge (DSTC). Dialog trackers are one of the essential components of dialog systems which are used to infer the true user goal from the speech processing results. We explain the main parts of our tracker: the observation model, the belief reﬁnement model, and the belief transformation model. We also report experimental results on a number of approaches to the models, and compare the overall performance of our tracker to other submitted trackers. An extended version of this paper is available as a technical report (Kim et al., 2013). 
The talk will distil experience and results from several projects, over more than a decade, which have researched and developed the application of speech recognition as an input modality for assistive technology (AT). Current interfaces to AT for people with severe physical disabilities, such as switch-scanning, can be prohibitively slow and tiring to use. Many people with severe physical disabilities also have some speech, though many also have poor control of speech articulators, leading to dysarthria. Nonetheless, recognition of dysarthric speech can give people more control options than using body movement alone. Speech can therefore be an attractive option for AT input. Techniques that have been developed for optimising the recognition of dysarthric speech will be described, resulting in recognition rates of greater than 80% for people with even the most severe dysarthria. Speech recognition has been applied as a means of controlling the home (via an environmental control system) and, probably for the ﬁrst time, as a means of controlling a communication aid. The development of the Voice Input Voice Output Communication Aid (VICOCA) will be described and some early results of its evaluation presented. The talk will discuss some of the lessons learnt from these projects, such as: • The need to work in interdisciplinary teams including speech technologists, speech and language therapists, health researchers and assistive technologists. • The value of user-centred design, involving users in deﬁning their wants and needs and then working with them, in an iterative manner, to reﬁne the AT such that it becomes usable and acceptable. • The gap that exists between the results that can be achieved in the lab and those achievable in peoples homes under real usage conditions – something that is not often covered in research papers. • The practical approaches that can be applied to optimising recognition for individuals. It is often possible to make signiﬁcant improvements in recognition rates by altering the conﬁguration of the AT set-up. The talk will conclude by describing some of the future potential applications of speech technology that are being developed, or considered, for people with disabilities as well as for frail older people and people with long-term conditions. 
We present in this paper a voice conversion (VC) method for a person with an articulation disorder resulting from athetoid cerebral palsy. The movements of such speakers are limited by their athetoid symptoms, and their consonants are often unstable or unclear, which makes it difﬁcult for them to communicate. In this paper, exemplar-based spectral conversion using Nonnegative Matrix Factorization (NMF) is applied to a voice with an articulation disorder. In order to preserve the speaker’s individuality, we use a combined dictionary that was constructed from the source speaker’s vowels and target speaker’s consonants. Also, in order to avoid an unclear converted voice, which is constructed using the combined dictionary, we used localityconstrained NMF. The effectiveness of this method was conﬁrmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method. Index Terms: Voice Conversion, NMF, Articulation Disorders, Assistive Technologies 1. Introduction In this study, we propose assistive technology for people with speech impediments. There are 34,000 people with speech impediments associated with an articulation disorder in Japan alone. Articulation disorders are classiﬁed into three types. Functional articulation disorders exist in the absence of any apparent cause and are related to deﬁciencies in the relatively peripheral motor processes. Organic articulation disorders are articulation problems that are associated with structural abnormalities and known impairments, such as cleft lip and palate, tongue tie, hearing impairment, etc. Motor speech disorders involve problems with strength and control of the speech musculature. We propose a voice conversion system, which converts an articulation-disordered voice into a non-disordered voice, for people with motor speech disorders. Cerebral palsy is one of the typical causes of motor speech disorders. About two babies in 1,000 are born with cerebral palsy [1]. Cerebral palsy results from damage to the central nervous system, and the damage causes movement disorders. Three general times are given for the onset of the disorder: before birth, at the time of delivery, and after birth. Cerebral palsy is classiﬁed into the following types: 1) spastic, 2) athetoid, 3) ataxic, 4) atonic, 5) rigid, and a mixture of these types [2]. In this study, we focused on a person with an articulation disorder resulting from the athetoid type of cerebral palsy. Athetoid symptoms develop in about 10-20% of cerebral palsy sufferers [1]. In the case of a person with this type of articulation disorder, his/her movements are sometimes more unstable than usual. Because of this symptom, their utterances (espe-  cially their consonants) are often unstable or unclear. Most people suffering from athetoid cerebral palsy cannot communicate by sign language, writing or voice synthesizer [3, 4, 5] because athetoid symptoms also restrict the movement of the sufferer’s arms and legs. For this reason, there is a great need for a voice conversion (VC) system for such people. Automatic speech recognition system for people with articulation disorders resulting from athetoid cerebral palsy has been studied. Matsumasa et al. [6] proposed robust feature extraction based on PCA (Principal Component Analysis) with more stable utterance data instead of DCT. Miyamoto et al. [7] used multiple acoustic frames (MAF) as an acoustic dynamic feature to improve the recognition rate of a person with an articulation disorder, especially in speech recognition using dynamic features only. In spite of these efforts, the recognition rate for articulation disorders is still lower than that of physically unimpaired persons. The recognition rate for people with articulation disorders using a speaker-independent model trained by nondisordered speech is 3.5%. This result implies that the speech of a person with an articulation disorder is difﬁcult to understand for people who have not communicated with them before. A GMM-based approach is widely used for VC because of its ﬂexibility and good performance [8]. This approach has been applied to various tasks, such as speaker conversion [9], emotion conversion [10, 11], and so on. In the ﬁeld of assistive technology, Nakamura et al. [12] proposed a GMM-based speaking aid system for electrolaryngeal speech. In this approach, the conversion function is interpreted as the expectation value of the target spectral envelope. The conversion parameters are evaluated using Minimum Mean-Square Error (MMSE) using a parallel training set. If the person with an articulation disorder is set as a source speaker and a physically unimpaired person is set as a target speaker, an articulation-disordered voice may be converted into a non-disordered voice. However, because the GMM-based approach has been developed mainly for speaker conversion [9], the source speaker’s voice individuality is also converted into the target speaker’s individuality. In this paper, we propose a VC method for articulation disorders. There are two main beneﬁts to our VC method. 1) We convert the speaker’s voice into a non-disordered voice, thus preserving their voice individuality. People with articulation disorders wish to communicate by their own voice if they can therefore, this is important for VC as assistive technology. 2) Our method outputs a natural-sounding voice. Because our VC is exemplar-based and there is no statistical model, we can create a natural sounding voice. In the research discussed in this paper, we conducted VC for articulation disorders using Non-negative Matrix Factorization (NMF) [13]. NMF is a well-known approach for source  3 SLPAT 2013, 4th Workshop on Speech and Language Processing for Assistive Technologies, pages 3–8, Grenoble, France, 21–22 August, 2013. c 2013 Association for Computational Linguistics  separation and speech enhancement. In these approaches, the observed signal is represented by a linear combination of a small number of elementary vectors, referred to as the basis, and its weights. In some approaches for source separation, the bases are grouped for each source, and the mixed signals are expressed with a sparse representation of these bases. Gemmeke et al. proposes an exemplar-based method for noise robust speech recognition [14]. In our study, we adopt the supervised NMF approach [15], with a focus on VC from poorly articulated speech resulting from articulation disorders into non-disordered articulation. The parallel exemplars (called the ‘dictionary’ in this paper), which consist of articulation-disordered exemplars and a nondisordered exemplars, are extracted from the parallel data. An input spectrum with an articulation disorder is represented by a linear combination of articulation-disordered exemplars using NMF. By replacing an articulation-disordered basis with a nondisordered basis, the original speech spectrum is replaced with a non-disordered spectrum. In the voice of a person with an articulation disorder, their consonants are often unstable and that makes their voices unclear. Their vowels are relatively-stable compared to their consonants. Hence, by replacing the articulation-disordered basis of consonants only, a voice with an articulation disorder is converted into a non-disordered voice that preserves the individuality of the speaker’s voice. In order to avoid a mixture of the source and target spectra in a converted phoneme which is constructed using the combined dictionary, we adopted localityconstraint to the supervised NMF. The rest of this paper is organized as follows: In Section 2, NMF-based VC is described, the experimental data is evaluated in Section 3, and the ﬁnal section is devoted to our conclusions.  2. Voice Conversion Based on NMF  2.1. Basic Approach of Exemplar-Based Voice Conversion  In the exemplar-based approach, the observed signal is represented by a linear combination of a small number of bases.  ∑ J  xl ≈ aj hj,l = Ahl  (1)  j=1  xl is the l-th frame of the observation. aj and hj,l are the j-th basis and the weight, respectively. A = [a1 . . . aJ ] and hl = [h1,l . . . hJ,l]T are the collection of the bases and the stack of weights. When the weight vector hl is sparse, the observed signal can be represented by a linear combination of a small number of bases that have non-zero weights. In this paper, each basis denotes the exemplar of the spectrum, and the collection of exemplar A and the weight vector hl are called ‘dictionary’ and ‘activity’, respectively. Fig. 1 shows the basic approach of our exemplar-based VC using NMF. D, d, L, and J represent the number of dimensions of source features, dimensions of target features, frames of the dictionary, and basis of the dictionary, respectively. Our VC method needs two dictionaries that are phonemically parallel. One dictionary is a source dictionary, which is constructed from source features. Source features are constructed from an articulation-disordered spectrum and its segment features. The other dictionary is a target dictionary, which is constructed from target features. Target features are mainly constructed from a well-ordered spectrum. These two dictionaries consist of the same words and are aligned with dynamic time warping (DTW). Hence, these dictionaries have the same number of bases.  Input source features Xs, which consist of an articulationdisordered spectrum and its segment features, are decomposed into a linear combination of bases from the source dictionary As by NMF. The weights of the bases are estimated as an activity Hs. Therefore, the activity includes the weight information of input features for each basis. Then, the activity is multiplied by a target dictionary in order to obtain converted spectral features Xˆ t which are represented by a linear combination of bases from the target dictionary. Because the source and target dictionary are parallel phonemically, the bases used in the converted features is phonemically the same as that of the source features. Fig. 2 shows an example of the activity matrices estimated from a word “ikioi” (“vigor” in English). One is uttered by a person with an articulation disorder, and the other is uttered by a physically unimpaired person. To show an intelligible example, each dictionary was structured from just the one word “ikioi” and aligned with DTW. As shown in Fig. 2, these activities have high energies at similar elements. For this reason, when there are parallel dictionaries, the activity of the source features estimated with the source dictionary may be able to be substituted with that of the target features. Therefore, the target speech can be constructed using the target dictionary and the activity of the source signal as shown in Fig. 1. Spectral envelopes extracted by STRAIGHT analysis [16] are used in the source and target features. The other features extracted by STRAIGHT analysis, such as F0 and the aperiodic components, are used to synthesize the converted signal without any conversion.  Figure 1: Basic approach of NMF-based voice conversion  Basis ID in source dictionary Basis ID in target dictionary  50  100  150  200  250  20  40  60  80 100 120 140  Frame of source speech  50  100  150  200  250  20  40  60  80  100  120  Frame of target speech  Figure 2: Activity matrices for the articulation-disordered utterance (left) and well-ordered utterance (right)  4  Constructing Dictionary  Source speaker’s training speech  Articulationdisordered  Spectral envelope  STRAIGHT  Mel + Log + DCT  Aligned spectrum All frames vvccvv Segment features  Source features vvccvv Segment features  Add to  Alignment Information Mel + Log + DCT Target speaker’s training speech Spectral envelope STRAIGHT Non-disordered  vvccvv Aligned spectrum  Vowel frames  Target features  v v c c v v Add to  vvccvv  Consonant frames  Conversion  As Source Dictionary (D x J)  Activity of Source Features (J x L)  vvccvvccvvcvv Hs  Articulation disordered spectrum Non-disordered spectrum v Vowel frame  Parallel data  c Copy  Consonant frame  At Target Dictionary (d x J) vvccvvccvvcvv Consonants from Non-disordered spectrum  Hs  Xˆ t vvcccvv Converted Target Features (d x L)  Figure 3: Individuality-preserving voice conversion  2.2. Constructing Dictionary to Preserve Individuality In order to make a parallel dictionary, some pairs of parallel utterances are needed, where each pair consists of the same text. One is spoken by a person with an articulation disorder (source speaker), and the other is spoken by a physically unimpaired person (target speaker). The left side of Fig. 3 shows the process for constructing a parallel dictionary. STRAIGHT spectrum is extracted from parallel utterances. The extracted STRAIGHT spectra are phonemically aligned with DTW. The Mel-cepstral coefﬁcient, which is converted from the STRAIGHT spectrum, is used to align. In order to estimate the activities of the source features precisely, segment features of source features, which consist of some consecutive frames, are constructed. Target features are constructed from consonant frames of the target’s aligned spectrum and vowel frames of the source’s aligned spectrum. Source and target dictionaries are constructed by lining up each of the features extracted from parallel utterances. The right side of Fig. 3 shows how to preserve a source speaker’s voice individuality in our VC method. Fig. 4 shows examples of the spectrogram for the word “ikioi” (“vigor” in English) of a person with an articulation disorder and a physically unimpaired person. The vowels of a person’s voice strongly imply a speaker’s individuality. On the other hand, the consonants of people with articulation disorders are often unstable. In Fig. 4, the area labeled “k” in the articulation-disordered spectrum is not clear, compared to that of the same region spoken by a physically unimpaired person. Therefore, by combining the source speaker’s vowels and target speaker’s consonants in the target dictionary, the individuality of the source speaker’s voice can be preserved.  2.3. Estimation of Activity with Locality Constraint  In the NMF-based approach, the spectrum source signal at frame l is approximately expressed by a non-negative linear combination of the source dictionary and the activities.  xl = xsl  ∑ J  ≈  asj hsj,l  (2)  j=1  Frequency [Hz]  i  k  i  oi  6000  5000  4000  3000  2000  1000  0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  Time [Sec]  (a) Spoken by a person with an articulation disorder  6000  i  k  i  oi  5000  4000  3000  2000  1000  0  0.2  0.4  0.6  0.8  
Ambient Assisted Living aims at providing assistance by allowing people with special needs to perform tasks which they have increasing difﬁculty with and to provide reassurance through surveillance in order to detect distress and accidental falls. Aged people are among the ones who might beneﬁt from advances in ICT to live as long as possible in their own home. Voice-base smart home is a promising way to provide AAL, but even mature technologies must be evaluated from the perspective of its potential beneﬁciaries. In this paper, we investigate which characteristics of the ageing voice that challenge a state of the art ASR system. Though in the literature, chronological age is retained as the sole factor predicting decrease in performance, we show that degree of loss of autonomy is even more correlated to ASR performance. Index Terms: Ambient Assisted Living (AAL), Dependency, Elderly speech, Voice command 1. INTRODUCTION With advances in medicine, life expectancy has increased. However, this phenomenon coupled with a low birthrate has led to an ageing population in industrialised countries. To help elderly people to live as long as possible in their home, solutions have been developed based on robotics, automation, cognitive science, and computer networks. These solutions are being developed to compensate their possible physical or mental decline to keep them with a good degree of autonomy. The aim is to provide assistance by allowing them to perform tasks which they have increasing difﬁculty with and to provide reassurance through surveillance in order to detect distress and accidental falls. Such a system must allow independence of elderly while facilitating social contact, with a major impact on well-being and health. In addition, it helps caregivers and reassure relatives. However, technological solutions must be able to adapt to the needs and the speciﬁc capacities of this population. Indeed, elderly are often confused by complex interfaces of devices. Therefore, the usual interfaces (remote controls, mice or keyboards) must be complemented by more accessible and natural interfaces such as a system of Automatic Speech Recognition (ASR) [1]. In this context, the CIRDO project1 wherein the authors take part aims to promote autonomy and support of elderly people by caregivers through a social inclusion product. The objective of the project is to integrate an ASR system into this product to perform detection of distress situations, distress calls and voice commands. Such kind of voice based interaction is an emerging feature of many AAL related research projects [2, 3, 4, 5, 6] but this remains a very challenging area due to the 1http://liris.cnrs.fr/cirdo/  atypical nature of the application (distant speech, aged people, noise, uncontrolled area, multi-speaker, etc.) [7]. One of the main challenges in this domain is to make sure that the ASR performance will be good enough to deliver a high quality voice order recognition system. This is a fear of the elderly population who are inclined to switch the system off if it has difﬁculties in understanding them. Most of the deployed ASR systems have reached a very good recognition rate in close, noise free talking, but their performances were rarely assessed with aged or children voice. A few studies compared ageing voice vs. non-ageing voice on ASR performance [8, 9, 10, 11], but their ﬁelds was quite far from our topic of home automation commands recognition. Moreover, an issue for our work was the non-existence of a speech corpus in French containing distress signals and automation commands. The purpose of this study was to determine the impact of ageing voice on the ASR system performance and to ﬁnd out which people characteristics might serve to predict ASR performance. The method we used is detailed in Section 3 after having discussed the related work in Section 2. Then the results of the evaluation are presented in Section 4 and an outlook on further work is given Section 5. 2. RELATED WORK The perception of voice alteration with age has been the subject of many studies [12, 13, 14, 15, 16]. Elderly speakers are characterized by tremors, hesitations, imprecise production of consonants, broken voice, and slower articulation [13]. Regarding women, the changes seem partly due to an increase of the vocal cords mass due to some changing levels of certain hormones [17]. Regarding men, perception of gasp come from an incomplete closure of the vocal cords that would be compensated by an increasing tension in larynx [18]. From the anatomical point of view, some studies have shown age-related degeneration with atrophy of vocal cords, calciﬁcation of laryngeal cartilages, and changes in muscles of larynx [19, 20]. Some studies have shown a signiﬁcant increase in the standard deviation measures of the fundamental frequency of elderly, both men and women [21, 16, 15]. Stability of the fundamental frequency (F0) is reduced in elderly voice [12] and is associated with variability in the peak-to-peak amplitude of speech signal. Hesitations and gasping in pathological voices have been associated with increased noise in the speech signal driven by an aperiodic vibration of the vocal cords [22, 23]. Some measures of the ratio between noise energy and harmonics have quantiﬁed this phenomenon by comparing older and younger speakers [21, 24]. Incomplete closure of vocal cords was observed during vocalisation [14]. The study cited above [12] conﬁrms fundamental frequency instabilities and the increasing noise on both sexes for healthy people with an average  9 SLPAT 2013, 4th Workshop on Speech and Language Processing for Assistive Technologies, pages 9–15, Grenoble, France, 21–22 August, 2013. c 2013 Association for Computational Linguistics  age of 70. These studies show that aged voice presents a much greater variability than typical voice. Ability of state-of-theart ASR systems to handle this kind of population can thus be questioned. A more general study of Gorham-Rowan and Laures-Gore [12] highlights the effects of ageing on the speech utterance and the consequences on the speech recognition. The experiments carried out in automatic speech recognition have shown performance degradation for “atypical” population such as children or elderly people [25, 10, 26] and have shown the interests of an adaptation to the target populations [27, 26]. Speech recognition adapted to the voice of elderly people is still an underexplored area. The relevant languages are mainly English [10] and Asian languages such as Japanese [8]. A very interesting study [10] used some recordings of speeches delivered in the Supreme Court of the United States over a decade. These recordings are particularly interesting because they were used to study the evolution of recognition performance on the same person depending on his age over 7-8 years. These studies show that the performance of recognition systems decreases steadily with age, and that a special adaptation to each speaker can get closer to the scores obtained from the youngest speakers without adaptation. The implicit consequence is that the recognition system is adapted to a single speaker. To make the system adapted to the person, Renouard et al. [28] proposed to use the recognized words to adapt online the recognition models. Proposed in the context of home assistance, this research does not appear to have been pursued. From an applicative point of view in the smart home context, speech recognition has been mainly implemented marginally in the ﬁeld of voice commands in English. Indeed, most current studies use conventional sensors (presence sensor, door contact, etc.) and tactile interfaces (remote controls, handset, touch-screens) more reliable but less natural, offering fewer opportunities for interaction and comfort (for example: need to walk to reach the remote device). Among the advances in the ﬁeld of voice controlled devices, a study conducted by Anderson [27] showed that a voice interface which is adapted (models acquired on 300 elderly speakers) allows to make voice requests on computer with the same performance than a query typed at the keyboard. This study has also revealed that only 2 of the 37 participants preferred the keyboard compared to the voice interface. In the same ﬁeld, Kumiko [29] proposed a computer voice command interface that takes into account the possible sources of error (duration, intensity, vocabulary) to improve performance and feedback. While Interactive Voice Response is a pervasive component of today’s telephone communication, some of which take into account the different voice population [30], voice control in smart home is clearly in its infancy. A large number of issues, such as noisy environment, number of sound sources (for example: several people), vocabulary coverage, coverage of speakers, etc. still need to be addressed [7]. Recently, Moir and Filho [31] proposed a low-coverage system using adaptive ﬁlters for a good recognition of keywords. But this research remains still exploratory. To the best of our knowledge, no application of voice control in smart home has explicitly considered the problem of voice recognition of French elderly speakers, even though major advances in terms of ergonomics, safety and data acquisition with high semantic value can be made by this modality. From this short literature review, it can be emphasized that no study had considered French aged voice in smart home condition. Moreover, most studies considered the chronological age as global explanatory factor while many other effects can also  be responsible for ASR performance degradation as raised by [11]. There is thus no certainty that age can predict the reliability of a voice-based control system. That is why our study includes an evaluation from the dependence perspective. 3. METHOD To assess the impact of the ageing voice on ASR performance, we started by acquiring a corpus targeted to the elderly population. From this corpus and a non-aged one, the ﬁrst task was to identify the most problematic phonemes and to check whether standard adaptation can be employed to reduce the discrepancy between aged and non-aged speakers at phoneme level. Once adapted, the second task was to assess whether measures other than strictly chronological age can explain ASR performance degradation. 3.1. Corpus collection The corpus collection was performed sporadically from 2009 to 2012 in collaboration with a rehabilitation centre, volunteers and a nursing home. Targeted speakers were persons aged of more than 60 years old, able to read and with no mental disorder or pathologies altering the voice. The recording was done with a single microphone positioned about 30 cm from the speaker’s mouth. Most speakers were sat, but some were in a wheelchair or laying in a bed. The recording was done using a computer and a home made software to prompt sentences to be read by the speaker and to record the utterances using voice activity detection. Given the targeted application (in-home voice commands and distress calls) the participants were requested to read a list of short distress/home automation and casual sentences such as Aidez-moi (Help me) or Il fait beau (It’s sunny). Based on [32], who interviewed elderly people in nursing homes to identify and describe what situations of distress they could have experienced, we created a list of home automation orders the person could utter during a distress situation to request for assistance. Ten samples of each kind are given in Table 1. The non-aged corpus was previously recorded in our laboratory in 2004 and was complemented in 2013 with sentences based on [32]. The procedure was similar to the aged corpus acquisition. This aged and non-aged corpus is called the AD corpus (Anodin-Détresse: anodin means colloquial and détresse means distress). Finally, another aged corpus, the ERES38 corpus (Entretiens RESidences 38: Entretiens means interviews) was acquired for model adaptation purpose. This corpus was recorded in 2011 in the living place of the person. During the interviews, we requested each speaker to read a text but they were also asked to talk freely about their life. The text was an article about gardening created by the experimenters in order to target phoneme issues reported in [9, 33]. All the corpora were annotated at the sentence level using the Transcriber software. 3.2. ASR system The ASR toolkit chosen in our study was Sphinx3 [34]. This decoder used a context-dependent acoustic model with 3-state left-to-right HMM. The acoustic vectors are composed of 13 MFCC coefﬁcients, the delta and the delta delta of each coefﬁcient. This HMM-based context-dependent acoustic model was trained on the BREF120 corpus [35] which is composed of about 100 hours of annotated speech from 120 non-elderly  10  Sample 1 2 4 5 3 6 7 8 9 10  Distress Sentence Aidez-moi ! Au secours ! Je me sens mal ! Je suis tombé ! Du secours s’il vous plaît ! Je ne peux plus bouger ! Je ne suis pas bien ! Je suis blessé ! Je ne peux pas me relever ! Ma jambe ne me porte plus !  Home Automation Order e-lio appelle le samu ! e-lio appelle les pompiers ! e-lio appelle les secours ! e-lio appelle un docteur ! e-lio appelle une ambulance ! e-lio appelle une inﬁrmière ! e-lio appelle ma ﬁlle ! e-lio appelle mon ﬁls ! e-lio tu peux téléphoner au samu ? e-lio il faut appeler les secours !  Casual Sentence Bonjour madame ! Ça va très bien. Ce livre est intéressant. Il fait soleil. J’ai ouvert la porte. Je dois prendre mon médicament ! J’allume la lumière ! Je me suis endormi tout de suite ! Le café est brûlant ! Où sont mes lunettes ?  Table 1: Examples of sentences of the AD corpus  French speakers. We called it the generic acoustic model.  3.3. Language model A general language model (LM) was estimated from the French Gigaword corpus which is a archive of newswire text data that has been acquired over several years by the Linguistic Data Consortium (LDC) at the University of Pennsylvania2. It was 1-gram with 11018 words. Moreover, to reduce the linguistic variability, a 3-gram domain language model was learned from the sentences used during the corpus collection described in Section 3.1, with 88 1-gram, 193 2-gram and 223 3-gram models. Finally, the language model was a 3-gram-type which results from the combination of the general language model (with a 10% weight) and the domain one (with 90% weight). This combination has been shown as leading to the best WER for domain speciﬁc application [36]. The interest of such combination is to bias the recognition towards the domain LM but when the speaker deviates from the domain, the general LM makes it possible to correctly recognise the utterances.  3.4. Word error rate and phoneme matching  The simplest and most common way to evaluate ASR perfor-  mances is to compute the Word Error Rate (WER). The WER  is computed by ﬁrst aligning the output (the decoded speech)  with the reference (i.e., the ground truth) and then applying  tWionEsR, de=letiIo+nDNa+nSd  where I, D and S is the substitution of words and  number of inserN is the number  of words in the reference.  Though this measure was used in many related studies  [8, 10, 11], it does not indicate which speciﬁc phonemes play a  role in the ASR performance degradation. To do so, the anno-  tation should be performed at the phoneme level. However, this  is a very laborious and time-consuming task which furthermore  requires a good level of expert agreement. That is why we anal-  ysed the results of the forced alignments. The forced alignment  algorithm that was used is the one of Sphinx3.  Forced alignment consists in ﬁnding the boundaries of  phonemes in an utterance knowing the uttered sentence. This  sentence is mapped in phoneme (using a dictionary) which is  used to constrain an optimal alignment between the acoustic  model and the speech utterance. The forced alignment scores  are for each signal segment within a boundary, the likelihood of  belonging to a phoneme model. This score can be interpreted  as a proximity to the “standard” pronunciation, modelled by the  2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId= LDC2006T17  Unvoiced Plosive Voiced Plosive Nasal Consonant Liquid Consonant Unvoiced Fricative Voiced Fricative Front Vowel Central Vowel Back Vowel Open Vowel Nasal Vowel Semi-Vowel  p, t, k b, d, g m, n, N, ñ l f, s, S v, z, Z, R i, e, E y, ø, œ, @ u, o, O a, A e˜, ã, õ, œ˜ 4, j, w  Table 2: Phoneme categories (IPA symbols)  generic acoustic model. The differences in scores of phoneme categories between the aged group and the non-aged group allow to identify which phonemes are the most difﬁcult for the ASR system. We are not aware of any study having used such method to assess ASR performances. Phonemes were grouped according to their highest level catgories as shown in table 2. 3.5. Adaptation with MLLR Once the phonemes are identiﬁed, the most common method to overcome the ASR limitation is to apply speaker adaptation. Speaker adaptation consists in generating a new acoustic model from a generic one and some new annotated speech in limited quantity. One of the most popular technique is to apply the Maximum Likelihood Linear Regression (MLLR) which is particularly adapted when a limited amount of data per class is available. MLLR is an adaptation technique that uses small amounts of data to train a linear transform which warps the Gaussian means so as to maximize the likelihood of the data. The principle is that acoustically close classes are grouped and transformed together. 3.6. Assessing the level of autonomy Despite the acoustic adaptation, there might be a disparity between the WERs of the elderly group even in aged people of the same age category. Therefore, we investigated other criteria and focused on elderly dependence. As reference, we used a French national test which is daily used in assessing the degree of loss of autonomy: the AGGIR (Autonomie Gérontolo-  11  gie Groupes Iso-Ressources) grid3. The degree of autonomy loss is evaluated in terms of physical and cognitive decline. According to the result of this test, the person can receive ﬁnancial support: the Personalized Allocation of Autonomy (APA). The evaluation is done using 17 variables. Ten variables refer to the loss of physical and cognitive autonomy: coherence, orientation, washing, feeding, disposal, transfers (to rise, to lie down, to sit down), internal displacement, external displacement and remote communication. Seven variables refer to the loss of domestic and social autonomy: personal management of budget and possessions, cooking, cleaning, transporting, purchasing, treatment monitoring and past time activities. Each variable is coded with A (independent), B (partially dependent) and C (totally dependent). The GIR (Iso-Ressources Group) score is computed from the variables to classify the person in one of the six groups: GIR 1 (total dependence) to GIR 6 (total autonomy). 4. RESULTS 4.1. Collected Corpus 4.1.1. The AD80 French test corpus The AD corpus (cf. 3.1) was acquired from 95 speakers (36 men and 59 women) which are divided into two groups: the elderly group composed of 43 speakers (11 men and 32 women), 62 to 94 years old, with 2796 distress and home automation sentences for a duration of 1 hour 5 minutes, and 3006 casual sentences for a duration of 1 hour 6 minutes, and the non-elderly group composed of 52 speakers, 18 to 64 years old, with 3903 distress and home automation sentences for a duration of 1 hour 18 minutes, and 3897 casual sentences for a duration of 1 hour 12 minutes. We ﬁxed the limit of the non-aged group at 65 years old, but we recorded 2 people aged 62 and 63 years old with autonomy loss, looking very aged physically and living in nursing home. Thus we included this two persons, as exceptions, in the aged group. For the 43 speakers of the aged AD corpus, a GIR score was obtained after clinicians ﬁlled the AGGIR grid. Finally, the AD corpus is made up of 13,602 annotated sentences, with 4 hours and 42 minutes of recording. 4.1.2. Collection of the training ERES38 corpus The ERES38 (cf. 3.1) corpus was acquired from 22 elderly people (14 women and 8 men) between 68 and 98 years old. The corpus included 48 minutes of read speeches (around 2 minutes per speaker) and 17 hours of interviews. The speakers lived in specialized institutes, such as nursing homes and were cognitively intact without severe disabilities. 4.2. Phoneme distance between aged and non-aged voice When performing ASR using the generic acoustic model on the distress/home automation sentences of the AD corpus, we obtained an average WER of 9.07% for the non-elderly group, and an average WER of 43.47% for the elderly group. Thus, we observed a signiﬁcant performance degradation of ASR for elderly speech, with an absolute difference of 34.40%. Figure 1 represents the WER according to the chronological age for both groups. It shows that the WER is globally higher for elderly group as previous studies showed [8, 10, 11]. However, it can also be seen that the variability between speakers also increases 3http://vosdroits.service-public.fr/F1229.xhtml  Alignment Score Unvoiced Plosive Voiced Plosive Nasal Consonants Liquid Consonants Unvoiced Fricative Voiced Fricative Front Vowel Central Vowel Back Vowel Open Vowel Nasal Vowel Semi-Vowel  WER (%)  with the age. For instance, some 83 years old speakers have their WER ranging from 13.6% to 80.2%. Standard deviation is 6% for the non-elderly group and 17.27% for the elderly group. In other words, the WER is far less predictable in the elderly group than in the non-elderly group. Consequently, we have to deal with the fact that a speech recognition with such a system can work very well with some of the elderly speakers, and very badly with others.  100  90 80  WER aged  70  WER non-aged  60  50  40  30  20  10  0  0  10  20  30  40  50  60  70  80  90 100  Age (year)  Figure 1: WER as a function of age for aged and non-aged groups  The forced alignment scores on both AD groups nonelderly and elderly with the generic acoustic model are presented in Figure 2 based on phonemic categories.  -30000.00 -25000.00 -20000.00 -15000.00 -10000.00 -5000.00 0.00  Elderly Non-elderly  Phonemes Category Figure 2: Forced alignment scores by phonemes categories before adaptation The relative differences of forced alignment scores observed between both groups led to sort the phoneme categories in descending order of differences: nasal vowels (-100,34%), voiced plosives (-56,55%), unvoiced fricatives (-50,48%), unvoiced plosives (-44,05%), nasal consonants (-41,03%), open vowels (-37,12%), central vowels (-34,80%), voiced fricatives (-31,30%), back vowels (-29,26%), semi-vowels (-29,18%), liquids (-19,99%), and front vowels (-11,89%). The repartition of French phonemes inside the different groups are presented in Table 2. For the elderly group, the alignment scores are lower than those obtained for the non-elderly group especially for plosives and nasal vowels. Based on the relative differences, the phoneme categories most affected for elderly group are nasal vowels, plosive consonants, unvoiced fricatives and nasal consonants.  12  WER aged group after adaptation (%) WER (%)  4.3. Impact of the acoustic adaptation on ASR performance with aged voice  100 90 80 70 60 50 40 30 20 10 0 0 10 20 30 40 50 60 70 80 90 100 WER aged group before adaptation (%) Figure 3: WER on aged group before and after adaptation  Figure 3 shows that using the MLLR adapted acoustic model was able to reduce the WER signiﬁcantly for all speakers of the AD corpus. With the global MLLR adaptation using ERES38, the average WER was 14.52%. Compared to the 43.47% WER without adaptation (see Section 4.2), the absolute difference was -28.95%. Furthermore, the speaker with the worst performance had his error rate reduced from 80.2% to 44.9%, and the speaker with the best performance had his error rate reduced from 9% to 1.8%. Also, the standard deviation was reduced from 17.27% to 10.34%, showing a reduction of the variability between the speakers. A comparison between the forced alignment scores obtained for non-elderly without adaptation and for elderly after adaptation using the ERES38 corpus is shown in Figure 4. On the whole, the scores for the elderly after adaptation are better than those of non-elderly with the generic acoustical model.  -30000.00 -25000.00 -20000.00 -15000.00 -10000.00 -5000.00 0.00  Non-elderly nonadapted Elderly adapted  Phonemes Category Figure 4: Forced alignment scores by phonemes categories for non-elderly with the generic acoustical model and for elderly after adaptation  Indeed, the use of an acoustical model adapted to elderly people reduces the mismatching of phonemes. The alignment scores of the adapted model presented in Figure 4 show that the average distance has reduced below the non-aged one for all phonemes except for unvoiced fricatives and nasal vowels. From an applicative point of view, this test shows that we can use a database of elderly speech in MLLR adaptation with speakers which are different from the test database. Even though the size of the corpus is small, we have a signiﬁcant improvement of WER. Furthermore, this demonstrates that the voices of ageing people have common characteristics. 4.4. Inﬂuence of elderly dependence on ASR system Despite the acoustic adaptation, there is a great variability between the WERs of the elderly group. Therefore, we investigated to establish if the level of elderly dependence can be an indicator of the ASR performance for the elderly group. Figure 5 shows a box-and-whisker diagram of the WER from MLLR adaptation as a function of the elderly dependence. Four speakers were in GIR 2, two speakers were in GIR 3, 21 speakers were in GIR 4, one speaker was in GIR 5 and 15 speakers were in GIR 6. No speaker was represented in GIR 1. Due to the small number of speakers in GIR 2, GIR 3 and GIR 5, we merged GIR 2 with GIR 3 and GIR 4 with GIR 5.  50,0 40,0 30,0 20,0 10,0 0,0  GIR 2-3  GIR 4-5  GIR6  Figure 5: WER as a function of levels of dependence  From Figure 5 it can be seen that WERs are different according to the GIR category. Indeed, the WER averages for GIR 2-3, GIR 4-5 and GIR 6 are respectively 25.2%, 13.2% and 12.2%, and the WER standard deviations are respectively 16.8%, 8.4% and 7.6%. Then, we performed an ANOVA test on the groups GIR 2-3, GIR 4-5 and GIR 6. From this test, the GIR score have a signiﬁcant effect on WER (F (2, 40) = 4.3; p < 0.05%). We conducted a Bonferroni post-hoc analysis to characterize which groups were signiﬁcantly different from other groups. The post-hoc test highlighted that there was a signiﬁcant difference between the GIR 2-3 group and both groups GIR 4-5 and GIR 6, while there is no signiﬁcant difference between GIR 4-5 and GIR-6. 5. CONCLUSION The paper presents our study on the behavior of an ASR system with elderly voices. Given the absence of a corpus containing the voice of elderly in French language usable for testing ASR system, we recorded the AD corpus. From this corpus, we observed an increase of the average WER of the ASR system for elderly people, with an absolute difference between non-elderly  Alignment Score Unvoiced Plosive Voiced Plosive Nasal Consonants Liquid Consonants Unvoiced Fricative Voiced Fricative Front Vowel Central Vowel Back Vowel Open Vowel Nasal Vowel Semi-Vowel  13  and elderly voice of 34.4%. With forced alignment, we analyzed which phonemes for elderly speech were posing the most problems to ASR systems. These results allowed us to proceed to the recording of the ERES38 corpus, allowing us to adapt the generic acoustic model to the voice of elderly people through the MLLR adaptation method. The global MLLR adaptation was interesting because with less than one hour of recordings from speakers different from the test speakers, we obtained a WER close to the case of recognition with the generic acoustic model on non-elderly group, with a WER of 14.53%, against 43.47% before adaptation. Moreover, we showed that inside the elderly group, the WER was not correlated with the age but could be correlated with the level of dependence due to a general physical degradation. The continuation of our work would be to show how the different parameters of the AGGIR grid are correlated to the WER. Therefore, predicting the ASR behavior would allow in facilitating the use of these new technologies in the daily life of the dependent elderly people. 6. ACKNOWLEDGMENT This study was funded by the National Agency for Research under the project CIRDO - Industrial Research (ANR-2010TECS-012). The authors would like to thank to Mrs Vézignol and Bonnefond-Jimenez, Mr Debrus, Mrs Aman, Bron, Lalande and Martins of the medical institutions SSR ”Les Cadières” and EHPAD ”Château de Labahou” for their help in corpus recording. Special thanks to R. Dugheanu, J. Le Grand and Y. Sasa for their active contribution, and to various elderly and caregivers who agreed to participate in the recordings. 7. References [1] F. Portet, M. Vacher, C. Golanski, C. Roux, and B. Meillon, “Design and evaluation of a smart home voice interface for the elderly — Acceptability and objection aspects,” Personal and Ubiquitous Computing, vol. 17, no. 1, pp. 127–144, 2013. 
An attractive approach to enable the use of vocal interfaces by impaired users with dysarthric speech is the use of a system which learns from the end-user. To enable such technology, it is imperative that the learning is fast to reduce the time spent training the interface. In this paper we investigate to what extend various machine learning techniques are able to learn from only a single or a few spoken training samples. Additionally, we explore whether these techniques can be combined through boosting to improve the performance. Our evaluations on a small, but highly realistic home automation database reveal that nonnegative matrix factorization seems best suited for fast learning and that some of the boosting approaches can indeed improve performance, especially for small amounts of training data. Index Terms: vocal user interface, self-taught learning, machine learning, boosting 1. Introduction Vocal user interfaces (VUIs) allow us to control a wide range of appliances and devices such as computers, smart phones, car navigation and other domestic devices and environments. While for most the use of a VUI is just a luxury, for individuals with a physical disability using a VUI can greatly improve their independence and quality of living, because for them operating and controlling devices would often require exhausting physical effort [1]. Conventional speech recognition systems employed in VUIs are trained by the developer using vast amounts of speech material. While offering impressive performances for users whose word choice, grammar and speech conforms to the training material used, performance suffers in the presence of accented, dialectical and disordered speech. A possible solution, adaptation of existing acoustic models, may not sufﬁce for severe speech pathologies [2, 3, 4, 5, 6]. The goal of this research is to explore methods which allow training speech commands by the end-user himself. This way, the acoustic models of the VUI are maximally adapted to the end-user’s speech while at the same time bringing development costs down. The challenge is to employ a learning strategy that can learn from only one or a few examples, in order to minimize the time the end-user spends on training the system. In this work, we will offer a comparison between multiple popular machine learning strategies to evaluate their effectiveness in developing a fast learning self-taught VUI. In previous work, we obtained encouraging results on fast vocabulary acquisition [7] using non-negative matrix factorization (NMF). Although in that work the learning speed of acquiring acoustic models was investigated, it focused on larger amounts of training data than targeted in this work. Moreover, it considered a multi-label task in which spoken utterances were  associated with multiple labels at once, which penalized other machine learning methods less suited for multi-label learning. In contrast, in this work we will focus on speech classiﬁcation, labelling a spoken utterance with a single label. Our contribution is twofold. First, we compare the performance of ﬁve machine learning techniques: Dynamic Time Warping (DTW), Gaussian Mixture Models (GMMs), Hidden Markov Models (HMMs), Support Vector Machines (SVMs) and Non-negative Matrix Factorization (NMF). Each of these techniques have their strengths and weaknesses; for example, while a HMM improves upon a GMM by being able to model temporal structure, it does require more parameters to be trained. When training with only one or a few training samples, this may lead to overﬁtting. Second, we investigate to what extent combining the aforementioned classiﬁcation techniques, ‘boosting’, can improve results. We do this by comparing a number of combination rules operating at the class label posterior level [8]. The remainder of the paper is organised as follows. In Section 2 we give an overview of the speech classiﬁcation methods that are investigated. In Section 3 we describe the various boosting approaches that will be considered. In Sections 4 and 5 we describe the experimental setup for evaluation on a small, but highly realistic home automation database collected in the ALADIN project [9]. The results of these experiments are presented in Section 6 and discussed in Section 7, and we conclude with our summary and thoughts for future work in Section 8. 2. Classiﬁcation methods In a speech classiﬁcation problem an unlabelled speech signal X is assigned to one of the m possible commands {ω1, . . . , ωm}. The classiﬁcation methods Dynamic Time Warping (DTW), Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM) use a spectrographic feature vector representation X = [x1, ..., xT] of a speech signal X, with T the number of frames. The number of rows of X is the dimension of the feature vector xt for one frame. The classiﬁcation methods Non-negative Matrix Factorization (NMF) and Support Vector Machine (SVM) use a utterance based feature vector x of a speech signal X. The utterance based feature vector x is a column vector whose dimension N depends on the kind of feature vector. More information about the feature vectors used in this work for each of the techniques is given in Section 4.2. A classiﬁcation method evaluates how well the unlabelled speech signal X resembles speech signals associated with a command class ωk. This similarity is expressed by the positive number Π(ωk, X), which is method dependent. The larger Π(ωk, X), the larger the similarity between X and the speech signals belonging to command ωk. The speech signal X is as-  21 SLPAT 2013, 4th Workshop on Speech and Language Processing for Assistive Technologies, pages 21–28, Grenoble, France, 21–22 August, 2013. c 2013 Association for Computational Linguistics  signed to the command ωj with the highest similarity:  assign X → ωj = argmax Π(ωk, X).  (1)  ωk  2.1. Dynamic Time Warping Dynamic Time Warping (DTW) [10, 11, 12], is a method in which an unlabelled speech signal is compared with a large collection of labelled speech signals (exemplars) extracted from the training data. Since such a comparison needs to take different signal lengths and speech rate variations into account, DTW ﬁrst ﬁnds an optimal alignment between each pair of utterances through non-linear time warping. The unlabelled speech signal is labelled with the command which is associated with the most similar exemplar. As a learning method, DTW has the advantage that it makes optimal use of all the available labelled data, because all training samples can be used as an exemplar. A drawback of DTW is that the computational complexity of classiﬁcation increases linearly with the number of training samples. Formally, the similarity between the frames of the unlabelled speech signal X and the frames of each exemplar Xi is represented by a T × Ti matrix DX,Xi containing the cosine distance between the spectrographic based feature vectors representations X and Xi:  DX,Xi =  X Xi X Xi  .  (2)  The similarity between two speech signals is expressed as  ttmhheeatrrsiicxgohDrtelXoD˜,wXXei,r.XcTiohraenloeonrpg(tFimtihgaeulropepa1ttih(ma, )af)rl,opmmaitnthhimethliresoefutsguthhpeptehcreucmdoiursnltaaetnrivctoee  acoustic differences and the total number of steps. The optimal  path is determined using a dynamical programming approach  with the Needleman-Wunsch algorithm [13].  The unlabelled speech signal X is assigned to the command  of the exemplar Xi with the highest similarity. The similarity between X and ωk is expressed by the positive number  Π(ωk ,  X)  =  max Xi of ωk  D˜ X,Xi  .  (3)  2.2. Gaussian Mixture Model In a Gaussian Mixture Model (GMM), the probability density function is used to determine the acoustic likelihood of a command given the feature vectors of the unlabelled speech signal [14]. Using a GMM is attractive, because it is a parametric model in which the classiﬁcation of an unlabelled speech signal takes the same time independent of the amount of training data. Another advantage is that the use of a parametric model typically allows better generalisation to unseen data, provided enough training data is available to accurately estimate the parameters. Each command ωk is represented by a weighted sum of multivariate Gaussian distributions  M  fk(xt, µ, Σ) = wiN (xt, µi, Σi),  (4)  i=1  with µi the mean spectrographic based feature vector, Σi the covariance matrix, wi the weights for each multivariate Gaussian distribution in the GMM and M the number of multivariate Gaussian distributions in the GMM. The weighted sum of multivariate Gaussian distributions for each command ωk is  trained on the collection of speech signals {X(1), . . . , X(N)} in the training data belonging to command ωk. By applying the Expectation Maximization algorithm [15], the mean spectrographic based feature vector µi, the covariance matrix Σi and the weights wi are obtained for each multivariate Gaussian distribution in the GMM. The similarity between X and ωk is expressed by the positive number  Π(ωk, X) = exp  
We report on the development of a system which will bring personalised state-of-the-art automatic speech recognition into the homes of people who require voice-controlled assistive technology. The ASR will be sited remotely (‘in-the-cloud’) and run over a broadband link. This will enable us to adapt the system to the user’s requirements and improv the accuracy and range of the system while it is in use. We outline a methodology for this: the ‘Virtuous Circle’. A case study indicates that we can obtain acceptable performance by adapting speakerindependent recognisers with 10 examples of each word in a 30word command-and-control vocabulary. We explain the idea of a PAL - a Personal Adaptive Listener - which we intend to develop out of this study. Index Terms: dysarthric speech recognition, ‘in-the-ﬁeld’ speech recognition, cloud-based speech recognition 1. Introduction With an ageing population and the increasing acceptance of community-based care, there is a growing demand for electronic assistive technology (EAT). One of the major uses of EAT is to support independent living, particularly among the elderly and the physically impaired. Devices such as environmental control systems (ECSs) allow people to control many aspects of their home environment through a single control interface. Typically these systems will be operated using a switch-scanning interface which accommodates the limited motor control abilities of users who have physical disabilities. A major drawback of switch-scanning interfaces is that they can be time-consuming and effortful to use. It is therefore appropriate to consider alternative input-methods for EAT that can accommodate users with limited physical abilities. The use of speech is an attractive alternative to switch-scanning interfaces. Indeed the prospect of using automatic speech recognition (ASR) as an alternative input-method for EAT has been discussed in the literature for more than thirty years [1, 2]. A signiﬁcant proportion of people requiring EAT have dysarthria, a motor speech disorder associated with their physical disability [3]. As a result of the effect of dysarthria on speech production, inexperienced listeners ﬁnd speech from people with dysarthria difﬁcult to recognise [4]. Machine recognition of dysarthric speech is also considered a difﬁcult problem. Large vocabulary speaker adaptive recognition systems have been successfully used for people with mild and moderate dysarthria as a means of inputting text. These systems, however, have been shown to be less successful for people with se-  vere dysarthria (e.g. [5, 6]). Speciﬁc modiﬁcations to speaker adaptive speech recognition algorithms with the aim of improving the recognition of dysarthric speech patterns have been described but they have not yet appeared in a widely available form [7, 8]. Speaker dependent speech recognition has often been thought to be more appropriate for users with severe dysarthria. This is because models can be trained directly with the speaker’s utterances rather than assuming their speech is similar to the typical speech the models were originally trained with [9]. Speaker dependent recognisers have been shown to perform well for severely dysarthric users in several studies [10, 11]. In these examples however, the input vocabularies were quite small, which can limit the potential usefulness of the EAT system. In recent years, new corpora of dysarthric speech have become available [12, 13]. These data sets have enabled researchers to conduct more systematic studies than before [14, 15], and open the possibility of comparing techniques using reference test sets. These corpora are however small compared to those used in modern, mainstream ASR. One reason for their relatively small size is the fact that prolonged speaking for people with severe dysarthria can be tiring. Therefore passive data collection from this population is likely to remain limited, unlike data collection for the typical speaking population. The only way to acquire substantial amounts of data is from a system which is being actively used. Most voice-enabled EATs described in the literature have been systems that have been developed for relatively small scale studies and with the main focus being on the observed ASR performance. There are some real challenges to be solved when porting such systems and setups to more ‘realistic’ scenarios, especially because of the larger number of users involved, and the need for a large degree of automation whilst still accommodating the needs of the individual users for personalisation. This paper describes recent work on designing a real ‘in-the-ﬁeld’ ASR-based EAT system where scalability and ease of initialisation has been at the forefront of the design from the onset. We have focused on two issues: how to most effectively setup an initial system for a given speaker (ﬁnding their optimal ‘operating point’) and how to use cloud-based ASR servers to allow the researcher free access to maintain and update ASR models. We present the homeService system in which we are developing state-of-the-art ASR. homeService is part of the UK EPSRC Project in Natural Speech Technology project, a collaboration between the Universities of Edinburgh, Cambridge and Shefﬁeld. homeService users are being provided with speechdriven ECS and eventually spoken access to other digital appli-  29 SLPAT 2013, 4th Workshop on Speech and Language Processing for Assistive Technologies, pages 29–34, Grenoble, France, 21–22 August, 2013. c 2013 Association for Computational Linguistics  cations. We are in the process of recruiting around 10 users to a longitudinal study: each user will be engaged with homeService for at least 6 months. From our experience in previous projects [10, 16], which included user requirement studies, we will continue to work with users in a collaborative way: the users effectively become part of the research team. As part of this process, users will inform the design and speciﬁcation of the functionality of their personal system. In addition we will work with users to close what we have referred to as the ‘virtuous circle’. By working with each user we will establish an initial ‘operating point’: a task which is sufﬁciently simple that we can expect good performance from the ASR and yet sufﬁciently useful that the user’s interest is maintained. We deploy this system and provide software which enables the user to practice with it. Practice improves the user’s pronunciation consistency and, crucially, provides more data for ASR training. The exercises provide the user with feedback, not based on the match to a standard pronunciation but on how well a new utterance ﬁts the user’s current model. When the performance of the system has improved sufﬁciently, we widen the vocabulary and range of target devices homeService controls. This process is iterated: the ’virtuous circle’. This is an example of Participatory Design [17]. As part of the ethical approval obtained for the study, the informed consent of users will enable us to collect examples of speech data from their interactions with the homeService system. These interactions will be stored and used to create a database which will become available to the research team but will not be made publicly available due to privacy issues. To further reduce any concern users might have about the system’s ability to ‘listen’ to them, the interface will clearly indicate when the microphone is open - typically only a couple of seconds for each voice command. At any time participants will also be able to ”opt-out” of the recording process, or even request recordings be deleted and not used in the database. The ASR will run remotely ‘in-the-cloud’, and be connected to the homeService users’ home by a dedicated broadband link. This is a novel approach for providing speech-driven EAT which will enable us to collect speech data, train new statistical models, experiment with adaptation algorithms, change vocabularies and so on without having to modify the equipment in the user’s home. This will reduce the amount of researcher time spent travelling to visit users, but more importantly will enable us to modify the system rapidly. This means new models can be deployed when they are ready, and new data can be analysed as soon as it is collected. We explain the homeService setup in more detail in section 2. The development of the ‘in-the-cloud’ recognition system is described in section 3. In section 5 our participatory design methodology is further developed. Some preliminary results of the speech recognition system are presented in section 4. 2. homeService setup A schematic diagram of the homeService system is shown in ﬁgure 1. The system consists of two distinct parts: the atHome system and the atLab system. The atHome system will be deployed in a user’s home and comprises a PC and a series of input and output devices to enable the system to receive spoken commands and interact with devices in the home environment, for example through the transmission of infrared signals. The atLab system resides at the university and comprises the main server which operates the ASR system and maintains the system state for each atHome system.  @Lab Audio  Main servers - ASR - speech synthesis - system state Recognition result  Tablet @Home  PC hub  Infrared sender  Microphone  Figure 1: Diagram of the homeService system with its two distinct parts: the atHome component in a user’s home and the atLab ‘in-the-cloud’ part. For simplicity, only one user is drawn here but the cloud-based ASR server enables us to scale to many simultaneous users. The system hardware consists of ‘off-the-shelf’ items such as a microphone array, an Android tablet for display and an infrared transmitters, which reduces the overall cost of each installation, and means the system will not need to rely on specialist hardware. In the following sections the components of the system are described in detail. 2.1. Components 2.1.1. The PC The atHome software is designed to run on a Linux-based PC. This PC will act as the main hub for the atHome system. It maintains the communication between the atLab part of the system and the peripherals in the atHome part of the system. The software controls the recording of audio from the system microphone, sends the audio back to the lab via a broadband link, provides feedback to the user, and controls the sending of infrared signals to various devices in the home. The software also sends updates to the screen of the tablet, and when appropriate, will play synthesised speech output. Although, from an operational point of the view, the PC is at the heart of the atHome system, the design philosophy of the atHome system ensures that the PC is as unobtrusive as possible. Consequently, from the users’ perspective the system microphone and the tablet PC are the key parts of the system. The requirements for the PC are that it should to be relatively small, quiet and discrete, with a low energy consumption.  30  For this a Shuttle XH61v with a core i3 3220 was chosen (30.5 x 6.4 x 21.6 cm). 2.1.2. Microphone For speech data capture, we use a high-quality USB microphone array (Dev-audio Microcone). It has a hexagonal design with 6 microphones placed in each of the six sectors, each covering approximately 60◦ of the surroundings. The Software Development Kit gives access to each of the 6 individual microphone channels as well as a stereo output of the beam-formed and noise-reduced signal, which will help us to reduce cross-talk from other speakers, the TV and so on. The Microcone also has a pleasing design, which is important as it will have to have a relatively prominent and very visible position in the users’ homes throughout the full study. 2.1.3. Infrared transmitter Remote control of the devices (such as TV, radio, lights etc.) is performed by an USB infrared (IR) emitter (IRtransWiFi IRDB). To make it personalised for each home, there is a conﬁguration step where the emitter is trained with the IR commands from the original remote controls of the home devices. The researcher has to perform this step manually, using the software provided with the IR emitter. After this step is completed, the system is able to associate system actions (e.g. “turn on TV”) to the speciﬁc IR commands for the devices it is controlling. 2.1.4. Android tablet The Android tablet acts as a personalised, visual interface for the user. This has several advantages; during system operation it will • display a representation of the system state, • display the options available for the user (this directly corresponds to the current ASR vocabulary), • act as a touch input if necessary. In addition, the tablet will have an app which will enable the system to acquire additional training data from the user. Software for user practice exercises will run on the tablet. The conﬁguration of the display is loaded from a XML ﬁle, where the description of each device is written by the system developer. This permits the personalisation of the display. 2.1.5. atLab Server The audio signal which is to be recognised is transferred across to the atLab part of the homeService system over the broadband link and subsequently passed on to the ASR server, also running at the university. When the recognition result is known it is ‘acted’ upon by the atLab software: for the environmental control system this means determining the next state of the system including possible infrared-codes which need transmitting and whether the tablet screen activity needs updating. All of the information concerning the state is then communicated back to the home of the user and acted upon. The two main communication links in the system (to the home and to the lab) are governed by individual APIs. The atLab software runs on a dedicated server at the university. Apart from being the main interface to the individual users, it also handles the communication to and from a bank of ASR servers (one for each user) which will provide online speech recognition based on models and setups that are personalised to each user.  3. ASR One of the main design aims was to base the system on ‘inthe-cloud’ ASR. This provides the research team with full control over the speciﬁcs of the ASR for each user; it is relatively straight-forward to change for example acoustic models, vocabularies and lexicons without disturbing the user unnecessarily. It also gives the researchers more scope for monitoring the state of the atHome systems, and crucially, for much more immediate trouble-shooting. Software components can easily be taken down and re-started. In the future, we also envisage having short remote chat-sessions with the users/carers to discuss any issues about the system. It is important to bear in mind that this easy access design does impose constraints on the research team. For instance, given that data will be collected from the microphone for speech events while the system is in use, all users must be carefully briefed about how these recordings will be made and stored before they can provide informed consent to take part in the study. In the future it is envisaged that the system will be used in ‘open mic’ sessions when all the audio from the microphone will be gathered at agreed times of the day. Again, careful brieﬁng of the users will be required as are procedures for users to retrospectively opt-out of these data collection sessions. Each user has a dedicated ASR server which will be preloaded with personal acoustic and language models as well as grammars. To maximise performance we intend to use grammars which restrict the vocabulary according to the given state the system is in. For example, if the system is operating in the environmental control mode and the user has just turned on the guide on the TV, a state-dependent grammar would contain words needed for navigating the guide, e.g. ’up’, ’down’, ’left’, ’right’, ’ok’ and ’exit’ as well as certain power or meta words which would allow the user the change state, for example by saying ’home’ or ’back’. The ASR server’s recognition technology is built around an in-house decoder based on weighted ﬁnite state transducers (WFSTs). This decoder was the winner in the NIST meeting recognition evaluations in 2007 and 2008. For details see [18, 19]. Every recognition cycle (consisting of audio being recorded, transferred across to the servers and subsequently recognised) will trigger the possibility of a change of state dependent on the current state and the newly recognised word. To further support this, the ASR server can dynamically load the next WFST from a set of pre-computed WFSTs matching all of the possible states of the system. We plan to expand this to enable online compilation of WFSTs. 4. Experimental setup Recruitment of users is underway for the homeService study. In preparation for setting up dedicated ASR systems for each user, we have carried out a pilot-study using data from a potential user, which we recorded during previous studies. This user (F01) is a female, in her mid ﬁfties at the time the recordings were made, who has cerebral palsy. Her speech is classiﬁed as spastic dysarthric of a severe nature. She has always been a very keen participant in our studies, and as such is a valued member of our extended research team. We have chosen her as one of the ﬁrst users in the homeService study as she has previously demonstrated that she is a highly motivated adopter of new technology; she is also a keen PC user. She currently uses a switch mounted on the headrest of her  31  wheelchair to access her scanning-based environmental control system and as well as to control her PC via dedicated software. 4.1. Data F01 has provided speech recordings for two research projects in the last decade, which is of interest here. These are all isolated words initially recorded with the aim of providing training material for whole word ASR models used in an an ECS system similar to the primary homeService task. The word lists consisted of isolated words such as ”TV”, ”on”, ”off”, ”channel”, etc. In total we have 1286 individual word recordings covering a vocabulary of 33 words (approximately 38 examples of each word). In this study we wish to train tri-phone derived word models, and the ideal training data would be sets of phonetically rich words or sentences. However, given the nature of this data set of isolated words, it is possible to quickly create a realistic test set using examples drawn from the data set. After a process of initial alignment to remove extraneous silences, around 40 minutes of data recorded from two different projects remained; project A provided 23 minutes of 8 kHz data (for the work here, this data has been up-sampled to 16 kHz) recorded using a headset microphone (SkyTronic Tie-Clip Microphone) onto a dedicated Arm-based embedded device (Balloon 3 board with a GEWA PROG III infrared micro chip). The remaining data from project B was recorded at 16 kHz on a laptop using a microphone array (the Acoustic Magic Voice Tracker array) [10]. 4.2. Acoustic modelling All hidden Markov models (HMMs) were trained using the maximum likelihood (ML) criterion. State-clustered, triphones having Gaussian mixture models with 16 components per state were used. 4.3. F01 case study Although the amount of data we have available from speaker F01 is relatively small compared to what one would normally need to train a high-performance, personalised ASR system, it far exceeds what we could expect to be able to record from a new homeService user in a typical enrolment session. What it does do is enable us to explore the effect of having access to different amounts of data for e.g., adaptation purposes. The experiments presented here aim to investigate the relationship between the quantity of training and recognition performance. When recruiting new users for homeService this will be a useful indicator of how much enrolment data will need to be recorded to provide a good, initial operating point. 4.4. Results First though, it is useful to assess F01’s data in terms of baseline performance. Table 1 shows some baseline results for her, where we have tested all of her speech on highperformance models trained on typical speech meeting data and on good, speaker-independent models trained on the dysarthric UASpeech corpus [12]. The achieved accuracies of 8.9% and 13.5% are very low and indicate the severity of F01’s speech impairment. The UASpeech result is in a range comparable to what has been reported for some of those speakers as well [20, 15]. Table 1 also shows the results from using some of F01’s data to perform a maximum a posteriori (MAP) adaptation from  System  Accuracy  Meeting (SI) Meeting+MAP (SD) UASpeech (SI) UASpeech+MAP (SD)  8.9 % 74.7 % 13.5 % 75.5 %  Table 1: Word accuracy rates for baseline systems. Please see text for further explanation.  
Automatic sign language recognition (ASLR) is a special case of automatic speech recognition (ASR) and computer vision (CV) and is currently evolving from using artiﬁcial labgenerated data to using ’real-life’ data. Although ASLR still struggles with feature extraction, it can beneﬁt from techniques developed for ASR. We present a large-vocabulary ASLR system that is able to recognize sentences in continuous sign language and uses features extracted from standard single-view video cameras without using additional equipment. ASR techniques such as the multi-layer-perceptron (MLP) tandem approach, speaker adaptation, pronunciation modelling, and parallel hidden Markov models are investigated. We evaluate the inﬂuence of each system component on the recognition performance. On two publicly available large vocabulary databases representing lab-data (25 signer, 455 sign vocabulary, 19k sentence) and unconstrained ’real-life’ sign language (1 signer, 266 sign vocabulary, 351 sentences) we can achieve 22.1% respectively 38.6% WER. Index Terms: Continuous Sign Language Recognition, Large Vocabulary, ASR, Computer Vision, Recognition System 1. Introduction Sign languages are natural languages that develop in communities of deaf people around the world and vary from region to region. A sign consists of manual and non-manual components that partly occur in parallel but are not perfectly synchronous [1]. Manual components comprise hand conﬁguration, place of articulation, hand movement and hand orientation while non-manual components include body pose and facial expression. ASLR is a subﬁeld of CV and ASR allowing methods of both worlds to be deployed but it also inherits their respective challenges. Large inter-/intra-personal signing variability, strong coarticulation effects, context dependent classiﬁer gestures, no agreed written form or phoneme-like deﬁnition in conjunction with partly parallel information streams, high signing speed inducing motion blur, missing features and the need for automatic hand and face tracking make video-based ASLR a notoriously challenging research ﬁeld. Although ASLR is starting to tackle ’real-life’ data, the majority of work in the community still focusses on the recognition of isolated signs, particularly in the context of gesture recognition. Deng and Tsui [2] and Wang et al. [3] use parallel HMMs to recognize isolated signs in American Sign Language or Chinese Sign Language, respectively, achieving recognition accu-  racies over 90%. Ong et al. [4] use boosted sequential pattern trees to recognize isolated signs in British sign language (BSL) allowing to combine partly parallel, not perfectly synchronous, automatically mined phoneme-like units in the recognition process. Pitsikalis et al. [5] extract subunit deﬁnitions from linguistic annotation in HamNoSys [6], whereas Koller et al. [7] employ an open SignWriting [8] dictionary to produce and align linguistically meaningful subunits to signs in German sign language (GSL). However, in real tasks ASLR is more likely to face continuous signing, that is what this work focusses on. In this context, Cooper et al. [9] compare boosted sequential pattern trees to HMMs using linguistically inspired subunits and 3D tracking information ﬁnding that the trees outperform HMMs for BSL. Forster et al. [10] investigate techniques to combine not perfectly synchronous information streams within an HMMbased ASLR system ﬁnding that synchronization just at word boundaries improves the recognition performance. Recognizing a sign language sentence by spotting individual signs has been investigated by several authors [11, 12, 13, 14] reporting promising results. Finally Yang et al. [15] use a nested dynamic programming approach to handle coarticulation movements between signs. Given the cited work and the works described in the survey on sign language recognition by Ong and Ranganath[16], two approaches to ASLR are observable. On the one hand, ASLR is viewed as a pure CV problem neglecting the natural language processing nature of the task and focussing on developing tailor-made solutions for gestures. However, we believe to be soon able to tackle real-world problems, ASLR should much more be seen as application of ASR, exploiting previous knowledge gained in that area. Following that track, we provide systematically gathered knowledge on how to create a large vocabulary ASLR system for continuous SL evaluating which techniques from ASR are applicable. Speciﬁcally, we investigate the impact of CV and ASR techniques on the recognition performance. Among others, the impact of the performance of automatic hand tracking on the recognition performance is investigated. Tackling the question of suitable features for nonrigid objects such as the hands, HoG3D [17] features proposed in the area of action recognition, appearance based features and learned MLP features used in ASR are investigated. Addressing inter signer variability, the technique of automatic signer adaptation is adopted from ASR (speaker adaptation) and tested within our proposed large-vocabulary, HMM-based sign language recognition system. Additionally, techniques to combine  41 SLPAT 2013, 4th Workshop on Speech and Language Processing for Assistive Technologies, pages 41–46, Grenoble, France, 21–22 August, 2013. c 2013 Association for Computational Linguistics  Video Input X1T Feature Analysis  xT1  argmaxG￿lPobra(wl S1Ne)ar·cPh:r(xT1  |w1N  ￿ )  w1N  Pr(xT1 |w1N ) Pr(w1N )  Word Model Inventory Language Model  wˆ1N Recognized Word Sequence  Figure 1: Bayes’ decision rule used in ASLR.  partly parallel information streams/modalities are presented and evaluated. The system and its components are tested in the context of continuous ASLR for two publicly available, largevocabulary databases. One database represents lab-data created for pattern recognition purposes and one database represents ’real-life’ data recorded from German public TV. Comparing ﬁndings on lab-data and ’real-life’ data we investigate which ﬁndings on lab-data generalize to ’real-life’ scenarios. 2. System overview and features The ASLR system described here follows the system design proposed in [18] and is based on Bayes’s decision rule but differs in several aspects. Speciﬁcally, features adapted from action recognition, learned features, a number of techniques to combine different modalities within the system, class-based language models, gap/noise models and signer adaptation techniques for multi-signer data are employed. The recognition result of the system is the sequence of words that best ﬁts the trained word models and the language model (see Figure 1). One has to note that linguistically this represents a major simpliﬁcation but the use of gloss annotations (see Section 2.1 for a short deﬁnition) is a common practice within the recognition community to deal with the non-availability of a common writing system for sign languages. While linguistically motivated writing notations such as HamNoSys[6] or SignWriting [8] cover information about different modalities used within sign languages, they are still a weak labeling scheme for signs because they do not give an annotation of the movement, facial expression, etc. per time frame. Furthermore, using glosses as target classes and annotation scheme allows for faster annotation of large amounts sign language data which is needed for a automatic statistical recognition approach. Finally, the proposed recognition system has been tested on the two publicly available databases SIGNUM [19] and RWTH-PHOENIX-Weather (PHOENIX) [20] for GSL which are among the biggest datasets available for continuous ASLR. 2.1. Visual modeling Albeit the cited work on automatic subunit extraction from sign language videos, it is still unclear how signs can be split into subunits. Furthermore, the majority of sign language corpora including those used in this work (see Section 3) is annotated using glosses effectively labeling the meaning of a sign rather than its appearance. Therefore, the proposed system is based on  whole-word models. The visual model (VM) of a sign consists of a left-to-right HMM in Bakis topology [21] where each segment of the model (each pair of consecutive states) is modelled by a separate Gaussian mixture model (GMM) with globally pooled covariance matrix. The number of segments per model is estimated from manually annotated sign boundaries on the training data. Due to strong visual pronunciation variances (3 different signs for Sunday exist in GSL), the effect of explicit visual pronunciation modelling is investigated in Section 3. 2.2. Language models Language models (LMs) play a crucial role in state-of-the-art ASR and ASLR systems. Dreuw et al. [18] showed that the impact of the well-known LM scale on the recognition performance of an ASLR system is in the same order of magnitude as in an ASR system. Therefore, the LM scale is optimized for all experiments presented in this work. In contrast to ASR where it is possible to obtain languagespeciﬁc almost arbitrarily large text collections for every language and domain, here the LM can only be trained on the transcribed training data of any given database for ASLR inheriting the problem of singletons and infrequent signs which often make up more than 40% of the available vocabulary of typically 200 to 500 signs. Inspired by the idea of class and topic LMs in ASR [22, 23, 24] and statistical sign language translation [25], we propose to use classes of visually and contextual similar signs within the LM. Class selection is based on the analysis of errors of a baseline system without LM classes. In this work, all LMs are trained using the SRILM toolkit [26] with modiﬁed Kneser-Ney discounting with interpolation [27]. 2.3. Manual and non-manual features GSL conveys information through manual and non-manual parameters. Manual parameters comprise both hands’ shape, their orientation and position. There are two-handed, as well as single-handed signs. Single-handed signs are usually signed using the dominant hand which in the databases used in this work corresponds to the right hand for all subjects in PHOENIX and all but two in the SIGNUM database. Manual features: For full coverage of a sign, manual features of both hands are used as well as non-manual features of the face and upper-body. To extract hand features, tracking is performed for both hands separately using a robust tracking algorithm with decision back-tracing originally proposed in [28]. Four different kinds of manual features are extracted. The ﬁrst one are colored image patches cut out around the tracked positions of the dominant hand with a size of 32 × 32 Pixel for SIGNUM and 53×65 Pixel for PHOENIX. As second feature, histograms of oriented image gradients in 3D space (HoG3D) [17] are extracted using a non-dense spatio-temporal grid from video volumes of ±4 cropped patches. Third, the movement trajectory of the right hand is extracted, represented by the position relative to the nose and the eigenvectors and eigenvalues of the movement within a time window of 2δ + 1 frames. Fourth, MLP features have been successfully used in ASR [29] and optical character recognition [30]. Here a feed-forward network with one hidden layer of 2000 nodes is trained using frame alignments from a previously trained HMM system as labels and PCA reduced hand patches in case of SIGNUM and HoG3D and trajectory features in case of PHOENIX. The training of the MLP has been performed on the training set of the HMM system. Cross validation is used to adjust the learning rate and to avoid over-ﬁtting.  42  Non-manual features: Face patches are extracted using the same tracking approach as described above. Furthermore, a position and orientation invariant active appearance model (POIAAM) [31] is ﬁtted to each frame obtaining a 109 dimensional shape descriptor, including shape model parameters, head rotation in space, mouth and eye openings and degrees of eyebrow raise. Finally, every frame of a video sequence is scaled down to 32 × 32 and 53 × 65 respectively to get a simple upper body feature as originally proposed in [18]. For all features, temporal context is included by stacking ±4 video frames for SIGNUM and ±2 frames for PHOENIX. Since the resulting feature dimension is too high to robustly estimate HMM parameters, PCA is applied. All features but the movement trajectory are reduced to 200 dimensions. In case of the colored hand and face patches PCA is applied to each color channel (red, green, blue) separately, yielding a ﬁnal feature dimension of 210. The movement trajectory feature itself has only limited discriminative power and is therefore combined with the HoG3D features of the right hand. 2.4. Signer adaptation and modality combination Sign languages use partly parallel, but not perfectly synchronous information streams/modalities to convey meaning. These modalities must be handled in the recognition process but it is an open question how to incorporate different modalities within such a system. A similar situation exists in audiovisual speech recognition (AVSR) where acoustic features and visual features of the mouth are combined. Following the work in AVSR, we investigated feature combination (concatenation), system combination using (i)ROVER [32] as well as combination between HMMs on state level (synchronous combination) and at word boundaries (asynchronous combination). Experimental results show that the ﬁrst two types of combination are not effective for current ASLR because either the resulting feature space dimension is too high or the systems make too similar recognition errors [10]. Here, only results for synchronous and asynchronous combination are presented. Signer adaptation: ASR systems trained on different speakers have to address the speakers’ voice and speech patterns to achieve good recognition performance. A common approach is to use speaker adaptive training (SAT) and learn speaker dependent feature transformation matrices using constraint maximum likelihood linear regression (CMLLR). Analogous to ASR, ASLR has to tackle signing styles. Therefore, SAT/CMLLR is evaluated in the context of ASLR for 25 signers. 3. Experimental results The SIGNUM database [19] contains lab recordings of 25 signers wearing black long-sleeve clothes in front of a dark blue background signing predeﬁned sentences. Videos are recorded at 780 × 580 Pixel and 30 frames per second (fps). Each signer signs the 603 unique training and 177 testing sentences once, whereas they are signed thrice in the single signer setup. 3.6% of the glosses are out of vocabulary (OOV). Table 1 shows statistics of the single signer setup only. The multi signer setup has the same vocabulary and OOV rate but 15k sentences (92k running glosses) for training and 4.4k sentences (23k running glosses) for testing. If not stated explicitly otherwise, all presented SIGNUM results refer to the single signer setup. The PHOENIX [20] database contains ’real-life’ sign language footage recorded from weather-forecasts aired by the  Table 1: Statistics for SIGNUM single signer and PHOENIX  SIGNUM  PHOENIX  Train Test Train Test  # sentences  1809 531 304 47  # running glosses  11,109 2805 3309 487  vocabulary size  455  - 266  -  # singletons  0  -  90  -  # OOV [%]  - 3.6  - 1.6  perplexity (3-gram) 17.8 72.2 15.9 34.9  public German TV-station PHOENIX. ’Real-life’ is meant from a computer vision point of view, where the signers were not artiﬁcially restricted in any sense in their signing (sentence structure, choice of vocabulary, size and intensity of signs, . . . ) and where the recording conditions have a much larger variance than on other signing corpora (lighting, camera-signer position, . . . ). The video footage has not been created for pattern recognition purposes or linguistic research. From a linguistic point of view the employed language has to be classiﬁed as non-native, as the signer is a hearing interpreter, whose parents are deaf. The videos (210 × 260 Pixel, 25 fps interlaced) show the interpreter wearing dark clothes in front of an artiﬁcial gray gradient background and pose a strong challenge to CV and ASLR due to high signing speed (majority of signs spans less than 10 frames), strong coarticulation effects and more than 30% of the vocabulary being singletons. Statistics of both databases are shown in Table 1. The system is trained using maximum likelihood and the EM-algorithm. The number of Gaussian densities and the LM-scale are optimized. For PHOENIX, the system uses 1433 emission distributions with a total of 4k Gaussians and a globally pooled covariance matrix. The same applies to SIGNUM, but the numbers are 1366 emission distributions with 24k Gaussians for single signer and 198k for multi-signer. Recognition uses word-conditioned tree search and Viterbi approximation. Basic Features: In order to build a well performing ASLR system, the feature selection plays a crucial role. The full video images can be seen as a global descriptor of manual and non-manual parameters and are, thus, a good starting point. As the hands are known to carry the most information in signing, tracked and cut out hand patches have often been preferred [18] over full frames. Comparing both features, hand patches outperform full images on both databases (see Table 2, Row 1). Model length estimation: In ASR, the HMM model of a word is formed by the linked models of the word’s subunit HMMs. Thereby, the typical temporal length of a word is modelled. This approach is not yet possible in ASLR because the deﬁnition and extraction of subunits is still an open research question. PHOENIX includes word boundary annotations from which the number of segments for each gloss HMM can been estimated by choosing the median of the lengths minus 20% and adjusting the length in case the adapted median is shorter than the shortest utterance of the gloss. The hand patch baseline presented above uses this approach. Using uniform length for all glosses, the recognition result is 60.8% instead of 55.0% WER. ’Bootstrapping’ the initial system alignment using the word boundary ground truth, we achieve 57.5% WER. No word boundary ground truth is available for SIGNUM. Model length estimation is performed using statistics on the  43  Table 2: WERs for competing features (Rows 1.-6.), WERs  without and with speciﬁc techniques (Rows 7.-11.). ’+’ denotes  a synchronous, asynchronous or feature combination. Please  see corresponding text parts for explanations. HoG3D uses  tracked hand locations. For PHOENIX, in rows 3.-5., manual  ground truth annotation has been used instead.  Competing Features  PHOENIX SIGNUM  1. Full image  Hand patch 80.1 55.0 31.6 16.0  2. Hand patch HoG3D  55.0 49.7 16.0 12.5  3. HoG3D  +Traj  45.2 42.1 12.5 14.2  4. HoG3D+Traj +Face  42.1 41.9 14.2 14.2  5. HoG3D  +Full  45.2 45.2 12.5 10.7  Impact of Techniques  WER [%]  WER [%]  6. Model Length Estimation  60.8 55.0 16.0 17.5  7. Temporal Context  51.3 49.7 12.7 12.4  8. MLP  39.8 43.3 16.0 13.0  9. Manual Tracking Annotation 55.0 48.3  –  –  10. Gap Models  42.1 39.8  –  –  11. Class LM  39.8 38.6  –  –  frame alignment of an HMM system with uniform length. No improvement over uniform length is observed due to the estimation on the frame alignment having limited accuracy and the signs in the video already sharing a similar length. Visual pronunciation variants: Sign languages exhibit strong pronunciation variation which manifest in visual sign variants. Visual variants are not explicitly labeled in PHOENIX or SIGNUM. While in SIGNUM no variants exist because of the artiﬁcial nature of the database, PHOENIX shows high variability within signs annotated by the same gloss. This arises mainly from the interpreter mixing different dialects. We have manually annotated the variants with regard to the visual appearance and the motion of the hand yielding on average 2.7 variants per gloss and a total of 711 different variants. Using these annotations, each variant is modelled by a distinct HMM with model length estimation achieving 56.5% WER in contrast to the baseline of 55.0%. Further, both systems outperform the 62.2% WER of a ’nearest-neighbor’ style system where each gloss occurrence is modelled independently. Apparently, increasing the number of dedicated HMMs per gloss worsens recognition. Coherent manual deﬁnition of variants is likely to be a problematic factor, as well as the HMMs not generalizing well over unseen data because of the reduction in training data per HMM and strong coarticulation effects. Tracking Inﬂuence: The presented hand patch baselines rely on tracking to localize the hands of the signer. Tracking is not perfect and errors propagate through the recognition system. Figure 2 shows the impact of tracking quality measured in tracking error rate (TrackEr) [28] counting a tracked position as wrong if it differs by more than 20 Pixel from ground truth on ASLR for PHOENIX. The TrackEr of 0 at 48.3% WER refers to using ground truth tracking annotation (see Table 2, Row 9). HoG3D: HoG3D features encode the shape and its change over time of a tracked hand. The latter aspect is not covered by hand patch features. Further, HoG3D features are more compact than hand patches, and robust against local illumination changes. Comparing to the hand patch baselines, recognition results are improved from 55.0% to 49.7% WER for PHOENIX and from 16.0% to 12.5% WER for SIGNUM. The result on PHOENIX  WER in % (PHOENIX) WER in % (SIGNUM)  Legend PHOENIX: TrackEr vs. WER PHOENIX: temporal context vs. WER SIGNUM: temporal context vs. WER  TrackEr in %  0 5 10 15 20 25 30 35 40  80  14  75  13.5  70  13  65 12.5 60  55  12  50  11.5  45  11  0  
Narrative speech can provide a valuable source of information about an individual’s linguistic abilities across lexical, syntactic, and pragmatic levels. However, analysis of narrative speech is typically done by hand, and is therefore extremely time-consuming. Use of automatic speech recognition (ASR) software could make this type of analysis more efﬁcient and widely available. In this paper, we present the results of an initial attempt to use ASR technology to generate transcripts of spoken narratives from participants with semantic dementia (SD), progressive nonﬂuent aphasia (PNFA), and healthy controls. We extract text features from the transcripts and use these features, alone and in combination with acoustic features from the speech signals, to classify transcripts as patient versus control, and SD versus PNFA. Additionally, we generate artiﬁcially noisy transcripts by applying insertions, substitutions, and deletions to manually-transcribed data, allowing experiments to be conducted across a wider range of noise levels than are produced by a tuned ASR system. We ﬁnd that reasonably good classiﬁcation accuracies can be achieved by selecting appropriate features from the noisy transcripts. We also ﬁnd that the choice of using ASR data or manually transcribed data as the training set can have a strong effect on the accuracy of the classiﬁers. Index Terms: automatic speech recognition, classiﬁcation, progressive aphasia 1. Introduction Primary progressive aphasia (PPA) is a neurodegenerative disorder in which language is the most affected aspect of cognitive functioning. There are two main variants of PPA: progressive nonﬂuent aphasia (PNFA), in which speech is hesitant and effortful, and semantic dementia (SD), in which speech is ﬂuent but with severe word ﬁndings difﬁculties [1]. A third subtype, logopenic progressive aphasia, has been identiﬁed in recent years but is not considered here. The features of narrative speech in each variant of PPA have been characterized to some extent, but they are not yet fully understood. Evaluation of spoken output is an important part of diagnosis of PPA and in identiﬁcation of the variant. From a clinical perspective, analysis of narrative speech has the advantage that it can provide a lot of information from a relatively brief assessment. A narrative speech sample can contain rich information about the speaker’s ability to choose appropriate content and function words, construct sentences, and convey meaning. Systematic analysis of narrative speech is typically done manually, which is time-consuming and may be prohibitively expensive. The automated approach evaluated here has several advan-  tages. For example, this method enables simultaneous consideration of multiple aspects of speech. Also, it should ultimately provide greater sensitivity to changes occurring in the earliest stages of disease, thereby facilitating early diagnosis. Similarly, it should provide objective measures of changes over time in language production, thereby enabling more accurate assessment of disease progression; this is important for patients and their families, as well as for evaluation of efﬁcacy in drug trials (as potentially disease modifying drugs become available). Fully automated analysis of narrative speech will require automatic speech recognition (ASR) in order to extract lexical and syntactic features from acoustic signals. Despite major improvements in ASR technology over the past few decades, accuracy for unrestricted (i.e., ‘dictation-style’) speech remains decidedly imperfect, as described in the next section. In order to estimate how effective a classiﬁer of PPA and its subtypes might be when given textual transcripts derived from ASR, a wide range of potential system performances must be considered, to account for real-world variation. This research approximates various levels of ASR performance by randomly corrupting human transcripts according to pre-deﬁned levels of error and compares these results against actual output from a leading commercial dictation system. Error levels are quantiﬁed by word-error rate (WER), which is the total number of erroneous insertions, deletions, and substitutions of words in an ASR transcript, divided by the total number of words in a reference transcript1. Simulated ASR errors have been used in various contexts, such as training dialogue systems [2] and for testing the safety of dictation systems for use in automobiles [3]. 2. Related Work In general, the accuracy of ASR systems on elderly voices tends to decrease with the age of the speaker [4]. Elderly voices typically have increased breathiness, jitter, shimmer, and a decreased rate of speech [4]. Older speakers may also exhibit articulation difﬁculties, changes in fundamental frequency, and decreased voice intensity [5]. These factors can result in speech that is less intelligible to both human listeners and ASR systems. For example, Hakkani-Tur et al. [6] found that in automatic scoring of a speech-based cognitive test, their ASR system had a higher WER for healthy speakers over the age of 70 than for those under the age of 70, with WERs between 26.3% and 34.1% for the elderly speakers, depending on the task and the gender of the speaker, while the error rates ranged between 21.1% and 28.2% for the younger speakers. 1If the number of insertions is large, it can overwhelm the total number of words in the reference transcript, therefore allowing for WERs above 100%.  47 SLPAT 2013, 4th Workshop on Speech and Language Processing for Assistive Technologies, pages 47–54, Grenoble, France, 21–22 August, 2013. c 2013 Association for Computational Linguistics  Effective speech recognition can be further challenged by the presence of linguistic impairments such as those occurring in PPA. To our knowledge, there has only been one previous study on automatic speech recognition of PPA speakers. Peintner et al. [7] analyzed speech from patients with PNFA and SD as well as patients with a dementia affecting behavior and deportment, but not language. They achieved a WER of 37% for SD and 61% for PNFA. They also tested a control group, who had an average WER of 20%. In this study, we use speech recognition as the input to a system that can analyze a spoken narrative and predict whether the speaker is cognitively normal or has a subtype of PPA. Peintner et al. [7] also attempted this task, although they did not report how the high error rates affected the lexical features studied or their classiﬁcation accuracy. Other studies in this area have used manually transcribed transcripts [8]. One strategy which combines ASR technology with manual transcripts is to use forced-alignment with manual transcripts to measure acoustic features such as rate of speech and length of pauses [9, 10]. However, for a speech analysis system to be available online or as part of an in-home continuous monitoring system, there must be no reliance on manual transcriptions at the word-level, which forced-alignment requires. 3. Data 3.1. Narrative samples Our data set comprises speech samples from 24 patients with PPA and 16 age- and education-matched controls. Of the 24 PPA patients, 14 were diagnosed with PNFA and 10 with SD. The speech samples were collected as part of a longitudinal study on language impairment in PPA in the Department of Speech-Language Pathology at the University of Toronto. See Table 1 for demographic information about the participants. Narrative speech samples were elicited following the procedure described by Saffran et al. [11]. Participants were given a wordless picture book of the well-known fairy tale “Cinderella”, and were asked to look through the book. The book was then removed, and participants were asked to tell the story in their own words. The narrative samples were recorded on a digital audio recorder, and transcribed by trained research assistants. The manual transcriptions include ﬁlled pauses, repetitions, and false starts. Sentence boundaries were marked according to semantic, syntactic, and prosodic cues. The SD patients produced an average of 380 words and 20 sentences, the PNFA patients produced an average of 302 words and 16 sentences, and the control group produced an average of 403 words and 16 sentences.  Age Years of education Sex  SD (n = 10) 65.6 (7.4) 17.5 (6.1) 3F  PNFA (n = 14) 64.9 (10.1) 14.3 (3.6) 6F  Controls (n = 16) 67.8 (8.2) 16.8 (4.3) 7F  Table 1: Demographic information for each participant group. Averages (and standard deviations) are given for age and years of education.  3.2. Features Two types of features are extracted for each participant individually, namely textual transcripts and acoustic samples. From these, we derive 31 lexical/syntactic features from the text transcripts and 23 features from the acoustics, giving a total of 54 available features, described below. 3.2.1. Text features A number of features can be extracted from the text transcripts. Some of our features are based on the part-of-speech (POS) tags assigned by the Stanford tagger [12]. SD patients have been observed to produce proportionally fewer nouns and more verbs and pronouns, while PNFA patients tend to produce more nouns and fewer verbs [13, 14, 15]. PNFA patients also tend to omit function words, such as determiners or auxiliaries [13, 16]. We look up the frequency of each word in the SUBTL norms, which are derived from a large corpus of subtitles from ﬁlm and television [17]. We calculate the average frequency over all words as well as specically for nouns and verbs. Similarly, we calculate the average familiarity, imageability, and age of acquisition of the words in each transcript using the combined Bristol norms and Gilhooly-Logie norms [18, 19]. Each word in these psycholinguistic databases has been ranked according to human perception of how familiar the word is, how easily the word evokes an image in the mind, and the approximate age at which a word is learned. Frequency, familiarity, imageability, and age of acquisition have all been found to inﬂuence speech production in aphasia [14, 20, 21, 22, 23]. The coverage of these norms on our data is variable. The frequency norms have excellent coverage – between 0.92 and 0.95 across the three groups on the manually transcribed data. The coverage for the familiarity, imageability, and age of acquisition norms is not as good, possibly due to the fact that the authors of the norms speciﬁcally excluded high frequency words [18]. The coverage for those norms ranges from 0.25 to 0.31 for all content words across the three groups for the manual transcripts. From the transcripts we also measure such quantities as the average length of the words and the type-token ratio, as well as measures of ﬂuency such as the number of ﬁlled pauses produced. We measure the combined occurrence of all ﬁlled pauses, as well as the individual counts for “um” and “uh”, since it has been suggested that they may indicate different types of hesitation [24]. In previous work using manual transcripts, researchers have also examined measures which can be derived from parse trees, such as Yngve depth, or the number and length of different syntactic constructions [8, 9]. However, such parse trees will depend on the location of the sentence boundaries in the transcript, the placement of which can be a difﬁcult task for ASR systems [25]. Indeed, the Nuance system used here does not place punctuation except by explicit command. For the purposes of this preliminary study, we avoid using features which depend on accurate sentence boundaries. 3.2.2. Acoustic features We follow the work of Pakhomov et al. [10] and measure pauseto-word ratio (i.e., the ratio of non-silent segments to silent segments longer than 150 ms), mean fundamental frequency (F0) and variance, total duration of speech, long pause count (> 0.4 ms), and short pause count (> 0.15 ms and < 0.4 ms). To this we add mean pause duration and phonation rate (the amount of the recording spent in voiced speech) [9], as well as the mean  48  and variance for the ﬁrst 3 formants (F 1, F 2, F 3), mean instantaneous power, mean and maximum ﬁrst autocorrelation function, skewness, kurtosis, zero-crossing rate, mean recurrence period density entropy (a method for measuring the periodicity of a signal, which has been applied to pathological speech generally [26]), jitter [27], and shimmer. Slow, effortful speech is one of the core symptoms of PNFA, and apraxia of speech can be an early feature [1]. PNFA patients may make speech sound errors and exhibit disordered prosody [1, 28]. Similarly, typical F0 range and variance have been shown to be indicative of articulatory neuropathologies within the context of speech recognition [29, 30]. In contrast, speech production is generally spared in SD, although SD patients may produce long pauses as they search for words [13]. 4. Methods 4.1. ASR and simulated errors We use two methods to produce errorful textual transcripts. The ﬁrst method represents the current leader in commercial dictation software, Nuance Dragon NaturallySpeaking Premium; here, audio ﬁles are transcribed by Nuance’s desktop dictation software. The second method corrupts human-produced transcripts according to pre-deﬁned levels of WER; this method allows for an indirect approximation of the performance given a wide range of potential alternative ASR systems. The Nuance Dragon NaturallySpeaking 12.5 Premium for 64-bit Windows dictation system (hereafter, ‘Nuance’) is based on traditional hidden Markov modeling of acoustics and, historically, on trigram language modeling [31]. This system is initialized with the default ‘older voice’ model suitable for individuals 65 years of age and older. The default vocabulary consists of 150,478 words, plus additional control phrases for use during normal desktop dictation (e.g., “new paragraph”, “end of sentence”); this feature cannot be deactivated. The core vocabulary, however, can be changed. In order to get a more restricted vocabulary, all words used in our manually transcribed Cinderella data set plus all words used in a selection of 9 stories about Cinderella from the Gutenberg project (totalling 22,168 word tokens) were combined to form a reduced vocabulary of 2633 word types. Restricted vocabularies, by their nature, have higher random baselines and less phonemically confusable word pairs, usually resulting in proportionally higher accuracies in ASR. The Nuance system scales the language model to the reduced vocabulary. For the simulated ASR transcripts, each word in the manual transcript is modiﬁed with a probability equal to the desired WER. In this set of experiments, we use a language model obtained from the Gigaword corpus [32], since the Nuance language model is proprietary and not accessible to the user. A word w can be modiﬁed in one of three ways: • Substitution – w is replaced with a new word wS. • Insertion – w is followed be a new word wI . • Deletion – w is removed. In the case of insertion, the word to be inserted is chosen randomly according to the bigram distribution of the language model. That is, words that frequently occur after w are more likely to be chosen as wI . If w is not found in the Gigaword vocabulary, then wI is chosen randomly according to the unigram distribution of the language model. In the case of substitution, the new word is randomly chosen from a ranked list of words  with minimal phonemic edit distance from the given word, as computed by the Levenshtein algorithm. Once it has been determined that a word will be modiﬁed, it is assigned one of the above modiﬁcations according to a predeﬁned distribution. Different ASR systems may tend towards different distributions of insertion errors (IE), substitution errors (SE), and deletion errors (DE). We create data noise according to three distributions, each of which favours one type of error over the others: [60% IE, 20% SE, 20% DE], [20% IE, 60% SE, 20% DE], and [20% IE, 20% SE, 60% DE]. We then also adjust these proportions according to proportions observed in Nuance output, as described in Section 5. 4.2. Classiﬁcation We use stratiﬁed leave-one-out cross-validation to test our diagnostic classiﬁers. For each fold, one transcript is removed as test data. We then apply a simple feature selection algorithm to the remaining transcripts: we calculate a Welch’s t-test for each feature individually and determine the signiﬁcance of the difference between the groups on that feature. We then rank each feature by increasing p-value, and include as input to the classiﬁer only the top ten most signiﬁcant features in the list. For each fold, different training data is used and therefore different features may be prioritized in this manner. Similar methods for feature selection have been used in previous studies on the classiﬁcation of dementia subtypes [7, 9, 33]. Once the features have been selected, we train three types of classiﬁer: na¨ıve Bayes (NB), support vector machine with sequential minimal optimization (SVM), and random forests (RF). The classiﬁers are then tested with the same subset of features derived from the held-out transcript. This procedure is repeated for every transcript in the data set, and the average accuracy is computed. We consider two classiﬁcation tasks, PPA-vs.-control and SD-vs.-PNFA, since these binary tasks allow for less confusion than a trinary classiﬁcation task and can be cascaded. For each task, there are two possible feature sets: text features only, or a combination of text and acoustic features. There are also two possible training sets for each task: i) the classiﬁers can be trained on the human-transcribed data and tested on the ASR data2, and ii) the classiﬁers are both trained and tested on the noisy ASR (or simulated ASR) data. We test our classiﬁers on each combination of these variables. 5. Results 5.1. Features and feature selection First, we examine whether the feature selection method selects different types of features depending on the WER. It might be expected that as the WER increases, the text features will become less signiﬁcant. Figure 1 shows the p-values, averaged across folds, for the text and acoustic features selected at each WER for each noise distribution. Note that the values of the acoustic features do not change with the noise levels, but the average p-value will change as different features are selected in each case, depending on the values of the text features. For the case of PPA versus controls, a mix of text and acoustic features are chosen, and the features tend to be signiﬁcant at p < 0.05, even when the error rate is high. A combination of text and acoustic features are also selected for SD versus PNFA at all 2This represents the scenario in which researchers have access to a corpus of manual transcriptions for training purposes  49  noise levels; however in this case the mean p-values are often not signiﬁcant, suggesting that the features are not as discriminative between these groups. This effect is reﬂected in the lower classiﬁcation accuracies for the SD versus PNFA task reported below. So, Figure 1 does not support the hypothesis that text features become irrelevant at the highest noise levels, but rather suggests that the transcripts still contain some information which is at least as valuable as the acoustic information in the speech signal.  p-value  Nuance default vocabulary  verb imageability  0.0006  noun frequency  0.002  noun familiarity  0.04  Nuance reduced vocabulary  average word length 0.003  noun frequency  0.006  noun imageability 0.01  noun familiarity  0.02  frequency  0.04  PPA mean 401 3.51 575 5.44 3.13 487 558 3.60  Control mean 354 3.26 558 6.21 2.77 554 531 3.20  Table 2: Signiﬁcant text features (p < 0.05) for PPA vs. Controls using the Nuance system with default and reduced vocabularies.  p-value  Nuance default vocabulary  noun familiarity 0.002  familiarity  0.002  Nuance reduced vocabulary  None  N/A  SD mean 596 594 N/A  PNFA mean 560 568 N/A  Table 3: Signiﬁcant text features (p < 0.05) for SD vs. PNFA using the Nuance system with default and reduced vocabularies.  Some text features are still signiﬁcant in the Nuance data as well, despite the high WER. Table 2 shows the text features that were signiﬁcant (p < 0.05) when comparing PPA and controls using the two Nuance models. As before, since the feature set changes with each fold in the cross-validation, the p-value is an average across folds. The means for the two groups are also shown to indicate the direction of the difference. Using the default vocabulary, there are three signiﬁcant text features: verb imageability, noun frequency, and noun familiarity. These three features are all signiﬁcant in the manually-transcribed data as well, and with the same direction. For the system trained on the reduced vocabulary, there are ﬁve signiﬁcant text features, as indicated, only one of which (noun imageability) is not signiﬁcant in the manual transcripts. All ﬁve features show differences in the same direction. Table 3 shows that only noun familiarity and overall familiarity are signiﬁcant in the SD vs. PNFA case using the default vocabulary system, as they are in the manually transcribed data, with the difference in the same direction. There are no signiﬁcant text features using the reduced vocabulary system. The signiﬁcant acoustic features for each classiﬁcation task are shown in Tables 4 and 5. These features remain the same regardless of the transcription method. For a complete discussion of the acoustic features of this data set, see [33].  phonation rate mean duration of pauses mean recurrence period density entropy long pause count skewness mean instantaneous power short pause count kurtosis shimmer  p-value 0.0000006 0.00002  PPA mean 0.733 37 800  0.00002 0.549  0.0006 0.0006 0.0003 0.002 0.005 0.05  34.7 -0.0733 -26.1 49.9 20.4 0.00560  Control mean 0.920 14 500 0.477 10.6 -0.532 -22.1 22.1 14.1 0.00748  Table 4: Signiﬁcant acoustic features (p < 0.05) for PPA vs. Controls.  p-value mean ﬁrst autocor- 0.02 relation function  SD mean 0.848  PNFA mean 0.730  Table 5: Signiﬁcant acoustic features (p < 0.05) for SD vs. PNFA.  5.2. Recognizing PPA speech Table 6 shows the WER of the Nuance system across populations and vocabularies. Somewhat surprisingly, using the reduced vocabulary reduces accuracy considerably, despite all words in the test set being present in the vocabulary. A possible explanation may be found in the distribution of error types across the uses of both vocabularies, which is shown in table 7. In particular, when using the reduced vocabulary, Nuance makes signiﬁcantly more deletion errors, which may be attributed to a lower conﬁdence assigned to its word sequence hypotheses which in turn may be attributed to a language model that is not adapted to non-default vocabularies. A general language model may assign a high lexical probability to a series of words that are phonemically similar to an utterance but if those words are not in the reduced vocabulary, a more domainspeciﬁc sequence of words may be assigned a low lexical probability and therefore a low conﬁdence. When conﬁdence in a hypothesis is below some threshold, that hypothesis may not be returned, resulting in an increase in deletion errors. Not having access to these internals of the Nuance engine prohibits modiﬁcation at this level. Another point to highlight is that, given Nuance’s default vocabulary, there is no signiﬁcant difference between the WER obtained with the control and PNFA groups (t(26.78) = −0.62, p = 0.54, CI = [−0.16, 0.08]), nor with the con-  SD PNFA Control All  Default Vocabulary 73.1 67.7 64.0 67.5  Reduced Vocabulary 98.1 97.3 97.1 97.5  Table 6: Mean word error rates for the Nuance systems on each of the participant groups.  50  (a) PPA vs. control, 0.2 IE, 0.2 SE, 0.6 DE (b) PPA vs. control, 0.2 IE, 0.6 SE, 0.2 DE (c) PPA vs. control, 0.6 IE, 0.2 SE, 0.2 DE  (d) SD vs. PNFA, 0.2 IE, 0.2 SE, 0.6 DE (e) SD vs. PNFA, 0.2 IE, 0.6 SE, 0.2 DE (f) SD vs. PNFA, 0.6 IE, 0.2 SE, 0.2 DE Figure 1: Acoustic features (ﬁlled bars) and text features (empty bars) selected for the feature sets at each WER for each distribution of insertion errors (IE), substitution errors (SE), and deletion errors (DE). Each bar represents one standard deviation from the mean, and the lines indicate the minimum and maximum values.  Insertion errors Substitution errors Deletion errors  Default Vocabulary 0.00602 0.39999 0.59398  Reduced Vocabulary 0.00008 0.11186 0.88804  Table 7: Distribution of error types for the Nuance systems.  trol and SD groups (t(23.77) = −1.47, p = 0.16, CI = [−0.22, 0.04]), although the differences in Table 7 might seem large. 5.3. Diagnosing PPA and its subtypes We evaluate the accuracy of diagnosing PPA and its subtypes based on the selected features across the three classiﬁcation methods using the simulated ASR method. In practice, classiﬁcation models might be trained on data that have been manually transcribed by humans (clinicians or otherwise). However, as the amount of data increases, this becomes less practical and it may become necessary to train these models from transcripts that were automatically generated from ASR. We replicate our experiments once on data that have been manually transcribed and once on the same data, but with transcripts corrupted by synthetic word errors (in which case the training data and test data have the same WER). Classiﬁers trained on human-produced transcripts have an average accuracy of 65.71% (σ = 12.42) and those trained on ‘noisy’ transcripts have an average accuracy of 70.72% (σ = 13.89), which is signiﬁcant at heteroscedastic t(543) = −4.47, p < 0.00001, CI = [−0.072, −0.028]. These differences can be observed in Figure 2. Interestingly, the classiﬁers trained with  ‘noisy’ transcripts outperform those trained with ‘clean’ transcripts fairly consistently in the PPA vs. control task, but this is far less pronounced (and to some extent reversed) in the SD vs. PNFA task. This may be partially explained by a signiﬁcant three-way interaction between WER, the task (i.e., the participant groups), and the training set (i.e., ‘noisy’ vs. ‘clean’) on a followup ANOVA (F (6) = 2.43, p < 0.05). This trend is also apparent when the classiﬁers are tested using the Nuance transcripts. Figure 3 shows the classiﬁcation accuracies for each classiﬁer on each diagnostic task using the data generated using the default and reduced vocabularies. When classiﬁying PPA versus controls, training on the ‘noisy’ Nuance data always leads to equal or greater accuracies than training on the ‘clean’ (human-transcribed) data. For SD versus PNFA, the results are mixed, although the results from the reduced vocabulary suggest the opposite trend. We compare the diagnostic accuracies across all classiﬁers given transcripts from Nuance using the reduced vocabulary with the accuracies of the synthetic WER method using the nearest WER (100%) and the associated error type distribution (i.e., 10% substitutions, 90% deletions, over all errors). We ﬁnd no difference between results obtained with Nuance data and those obtained with the synthetic method (t(44.25) = 1.1072, p = 0.27, CI = [−0.04, 0.13]). We repeat this analysis with the default Nuance vocabulary and its equivalent synthetic WER (70%) and distribution (i.e., 40% substitution, 60% deletion) and again ﬁnd no signiﬁcant difference (t(44.61) = 1.46, p = 0.15, CI = [−0.02, 0.11]). Here, distributions of WER are approximately Gaussian over the various parameterizations of the systems. The lack of apparent difference in diagnosis when using the Nuance ASR and the synthetic method supports the use of the latter in these experi-  51  (a) PPA vs. control (text)  (b) PPA vs. control (text & acoustic)  (c) SD vs. PNFA (text)  (d) SD vs. PNFA (text & acoustic)  Figure 2: Accuracy in diagnosing the indicated classes given features derived from potentially error-full textual transcriptions alone and in combination with features derived directly from the acoustics. Lines marked with x’s, circles, and pluses indicate the use of the na¨ıve Bayes, random forest, and support vector machine classiﬁers. Solid lines indicate those trained with human-transcribed (clean) data and dashed lines indicate those trained with corrupted data.  ments. Among the simulated ASR data, an n-ary ANOVA reveals signiﬁcant main effects for each of the classiﬁcation problems (PPA-vs.-control or PNFA-vs.-SD; F (1) = 124.19, p = 0), WER (F (5) = 31.69, p = 0), error distribution (proportions of IE, SE, and DE; F (4) = 6.32, p < 0.0005), and training set (‘noisy’ or ‘clean’; F (1) = 35.41, p = 0) on the accuracy of classiﬁcation; there is no effect of the classiﬁer, however (F (2) = 2.27, p = 0.1039). There were signiﬁcant interaction effects between WER and the classiﬁcation problem (F (5) = 5.18, p < 0.0005), error distribution (F (12) = 2.2, p < 0.05), and the training set (F (5) = 4.95, p < 0.0005), but not with the data subset (text or text with acoustics; F (5) = 1.42, p = 0.2146), or the classiﬁer (F (10) = 0.49, p = 0.8993). 6. Discussion Our goal is to provide assistive technologies, including diagnostic software, to various populations with pathological speech and language, including those with PPA. This study represents an initial step towards ASR for this population. One main result of this research is that fairly accurate diagnosis of PPA and of its subtypes can remain relatively accurate, even at very high levels of WER, by selecting appropriate features from the data at training time. Acoustic features are valuable, as they remain constant as the WER increases. However, our data suggest that some features from the text can still be informative, even when the transcripts are very noisy. One important direction for future work is to improve ASR for clinical populations. Clearly, modern speech recognition has  greater difﬁculty in recognizing PPA speech relative to speech the general elderly population, especially for individuals with SD. While more appropriate acoustic models built for olderadult voices will be important (based on available data), a focus on improving language modeling and the pruning of the lattices produced by hidden Markov models may be more fruitful if the cause of the pathology is semantic or lexical. Another limitation of our approach is that the t-test method for feature selection does not consider interactions between features. In the future we would like to examine these interactions, particularly between text and acoustic features. In this study we did not take into account any syntactic features, although agrammatism and/or syntactic simpliﬁcation are characteristic of PNFA. Presumably, including information of this type could increase the classiﬁcation accuracy. One approach would be to apply a sentence boundary detection algorithm to the ASR transcripts and extract traditional syntactic complexity measures (e.g. Yngve depth). Another approach would be to explore localized complexity metrics which do not depend on full sentence parses. 7. Acknowledgements This work was supported by the Canadian Institutes of Health Research (CIHR), Grant #MOP-82744, and the Natural Sciences and Engineering Research Council of Canada (NSERC). 8. References [1] M. L. Gorno-Tempini, A. E. Hillis, S. Weintraub, A. Kertesz, M. Mendez, S. F. Cappa, J. M. Ogar, J. D. Rohrer, S. Black, B. F.  52  (a) Text features, default vocabulary  (b) Text and acoustic features, default vocabulary  (c) Text features, reduced vocabulary  (d) Text and acoustic features, reduced vocabulary  Figure 3: Classiﬁcation accuracies using transcripts from the Nuance system with the default and reduced vocabularies, for random forest (RF), support vector machine (SVM), and na¨ıve Bayes (NB) classiﬁers. Empty bars indicate the accuracy achieved when training on the clean, human-transcribed data, while ﬁlled bars indicate the accuracy when training on the noisy ASR data.  Boeve, F. Manes, N. F. Dronkers, R. Vandenberghe, K. Rascovsky, K. Patterson, B. L. Miller, D. S. Knopman, J. R. Hodges, M. M. Mesulam, and M. Grossman, “Classiﬁcation of primary progressive aphasia and its variants,” Neurology, vol. 76, pp. 1006–1014, 2011. [2] J. Schatzmann, B. Thomson, and S. Young, “Error simulation for training statistical dialogue systems,” in Automatic Speech Recognition & Understanding, 2007. ASRU. IEEE Workshop on. IEEE, 2007, pp. 526–531. [3] M. Labsky´, J. Cuˇr´ın, T. Macek, J. Kleindienst, L. Kunc, H. Young, A. Thyme-Gobbel, and H. Quast, “Impact of word error rate on driving performance while dictating short texts,” in Proceedings of the 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, ser. AutomotiveUI ’12. ACM, 2012, pp. 179–182. [4] R. Vipperla, S. Renals, and J. Frankel, “Longitudinal study of ASR performance on ageing voices,” in Proceedings of INTERSPEECH, 2008, pp. 2550–2553. [5] V. Young and A. Mihailidis, “Difﬁculties in automatic speech recognition of dysarthric speakers and implications for speechbased applications used by the elderly: A literature review,” Assistive Technology, vol. 22, no. 2, pp. 99–112, 2010. [6] D. Hakkani-Tur, D. Vergyri, and G. Tur, “Speech-based automated cognitive status assessment,” in Proceedings of INTERSPEECH, 2010, pp. 258–261. [7] B. Peintner, W. Jarrold, D. Vergyri, C. Richey, M. L. G. Tempini, and J. Ogar, “Learning diagnostic models using speech and language measures,” in Engineering in Medicine and Biology Society, 2008. EMBS 2008. 30th Annual International Conference of the IEEE, 2008, pp. 4648–4651. [8] K. C. Fraser, J. A. Meltzer, N. L. Graham, C. Leonard, G. Hirst, S. E. Black, and E. Rochon, “Automated classiﬁcation of primary 
In this work we describe research aimed at developing an assistive vocal interface for users with a speech impairment. In contrast to existing approaches, the vocal interface is self-learning, which means it is maximally adapted to the end-user and can be used with any language, dialect, vocabulary and grammar. The paper describes the overall learning framework and the vocabulary acquisition technique, and proposes a novel grammar induction technique based on weakly supervised hidden Markov model learning. We evaluate early implementations of these vocabulary and grammar learning components on two datasets: recorded sessions of a vocally guided card game by non-impaired speakers and speech-impaired users engaging in a home automation task. Index Terms: vocal user interface, self-taught learning, dysarthric speech, non negative matrix factorization, hidden Markov models 1. Introduction These days, vocal user interfaces (VUIs) allow us to control computers, smart phones, car navigation systems and domestic devices by voice. While still generally perceived as a luxury, assistive technology employing a VUI can make a prominent difference in the lives of individuals with a physical disability for whom operating and controlling devices would require exhaustive physical effort [1]. Unfortunately, even state-of-the-art speech recognition systems offer little, if any, robustness to dialectic or dysarthric speech (often encountered with disabled users), and are often restricted in their vocabulary and grammar. In practice, it is not feasible to design speech interfaces featuring custom acoustic and language models that cater to the dialectic and/or pathological speech of individual users, and adaptation of existing acoustic models is limited to only very mild speech pathologies [2, 3, 4, 5, 6]. Moreover, the user’s voice may change over time due to progressive speech impairments. Our aim is to build a VUI that is trained by the end-user himself, which means that it is maximally adapted to the — possibly dysarthric — speech of the user, and can be used with any vocabulary and grammar. The challenge is to learn both acoustics and grammar from a small number of examples, with as only supervisory information coarse annotation in the form of associated actions. For example, the annotation of the command “Turn on the television please”, accompanied by a button press, would only be annotated at the utterance level with a device label (television) and an action label (turn on). Our learning approach consists of two components that interact. Vocabulary acquisition ﬁrst builds recurrent acoustic pat-  terns representing words or parts of spoken commands, while grammar induction attempts to model the relationships between these patterns. For vocabulary acquisition, we build on existing work on child language learning modeling with non-negative matrix factorisation (NMF) [7]. For grammar induction, we propose the use of a weakly supervised Hidden Markov Model (HMM). In short, we ﬁrst use NMF to ﬁnd recurrent acoustic patterns by mining utterance-level acoustic representations, supervised with relevant information about the action that was performed, such as a ‘television’ device and a ’turn on’ action. Building on these, we then use the temporal occurrence of these patterns in the training data as observation features to train a multi-label version of a discrete HMM [8, 9]. In the HMM, the hidden states represent the collection of possible values in the data structures (devices and actions in the example). By mining the temporal occurrence of the NMF-based observations and the commonalities and differences across commands, the HMM is able to discover temporal structure in the commands, related to the data structures representing the actions. The goals of our work are similar to those of [10, 11] in that we aim to discover acoustic patterns that recur in utterances and ground these by linking them to other modalities. However, to accommodate pathological voices, our work does not rely on pre-trained models, but they are learned from the speakerspeciﬁc acoustic data. In that sense, it shows similarities to the work in [12], but we learn form continuous speech and do not model low-level acoustics with an HMM. In terms of grammar learning, our task approaches unsupervised grammar induction [13, 14], but on a restricted domain with a small vocabulary. We evaluate our learning framework on two databases: PATCOR, recorded sessions of a vocally guided card game by nonimpaired speakers, and DOMOTICA-2, speech-impaired users engaging in a home automation task. The users were free to choose their own words and grammatical constructs to address the systems during the recording sessions. The remainder of the paper is organised as follows. In section 2, we present an overview of the learning framework, describe the acoustic representations and introduce the NMF and HMM learning approaches. In section 3, the experimental setup is explained and in sections 4 and 5 the experimental results are presented and discussed. We conclude with our conclusions and thoughts for future work in section 6. 2. Architecture 2.1. Semantic frame representation of an utterance A semantic frame is a data structure that contains all the relevant information (semantic concepts) associated with the ac-  73 SLPAT 2013, 4th Workshop on Speech and Language Processing for Assistive Technologies, pages 73–81, Grenoble, France, 21–22 August, 2013. c 2013 Association for Computational Linguistics  Figure 1: Overview of the vocal interface framework. The white boxes indicate events or systems outside the learning framework. The top panel shows the training phase and the bottom panel indicates the usage phase. tion that is expressed in the spoken command. Semantic frames have been used in many spoken language processing applications [15]. A frame contains at least one slot representing a speciﬁc aspect of the action. Each slot in a frame can only be ﬁlled with a single value. A frame description of an action on the other hand, identiﬁes a single frame out of the possible frames where the action is speciﬁed by the actual slot values. 2.2. The learning framework The framework (Figure 1) is designed so it can learn from user interaction examples, i.e. a spoken command accompanied by an action on the device’s user interface. For instance, users might say “ Turn on the light” while pressing the button to switch on the light themselves or through the help of a care taker. The action performed on the device is translated into a frame description, which constitutes an abstraction layer making the learning algorithms application independent. During the training phase, the word ﬁnding module looks for word-sized recurring acoustic patterns in the audio input that correlate well with the frame description. The frame description acts as a weak form of supervision in ﬁnding the recurring acoustic patterns. Here the term weak supervision is used because the supervision does not provide explicit information about the sequence of words within the spoken utterance. The grammar induction module learns the relation between the different parts of a command. Given the frame description and the output of the word ﬁnding module, the grammar induction module learns the structure within commands, as well as the relation with the frame description during the training phase. During the usage phase, when only audio input is available, the grammar constrains the decoding process [16] and allows to propose a frame description of the spoken command. This frame description is then mapped onto an actual action on the device. 2.3. Audio representation The word ﬁnding module in the training phase as well as the decoder in the usage phase need a suitable representation for the input speech. Both learning and recognition are based on  NMF (section 2.4.1), which requires that the audio representation of an utterance be the sum of the representations of individual words. Therefore, and unlike main-stream ASR systems, an utterance is mapped to a vector of ﬁxed size in three steps which are described below. 2.3.1. Spectral Representation The ﬁrst step of the audio processing chain extracts a 12dimensional Mel Frequency Cepstral Coefﬁcient (MFCC) representation of the short-term spectrum from speech segments of 25 ms with 10 ms overlap. The 12-dimensional MFCC is augmented with the log energy and the ∆ and ∆∆ features are appended, forming a 39 dimensional spectral feature stream. 2.3.2. Intermediate representations The obtained MFCC spectral representations are further processed to form posteriorgrams from which the ﬁnal representations described in section 2.3.3, are obtained. Two different forms of posteriorgrams are considered here: a spectral feature vector is either transformed into a vector of posterior probabilities of Gaussians forming a code book (soft VQ), or it is transformed to the posterior probability of phone classes. In Soft Vector Quantisation, each spectral feature vector is softly assigned to all clusters in a code book. Each cluster is characterized by a Gaussian with full covariance. The degree of assignment is measured by the posterior probability of a Gaussian given the spectral feature vector. The code book training starts off from a single cluster describing all training data. It is then split along the dominant eigenvector of its covariance matrix into two subclusters. The centres are reﬁned with k-means iterations after which each subcluster is characterised by a full covariance Gaussian. This process is repeated, each time splitting the cluster with the largest volume as measured by the determinant of the covariance matrix. This process is either stopped when the desired number of clusters are obtained [17], which we will refer to by Soft VQ, or when the number of spectral feature vectors assigned to a cluster falls below a threshold, minimum-number of frames, which is referred to as Adaptive Soft VQ, because the number of clusters will depend on the amount of training data. Phone posteriorgrams are constructed from 50 monophone HMMs (including a model for silence), each modeled by three states with GMM emission densities, connected in a strict leftto-right topology. The utterance is ﬁrst transcribed into a phone lattice without using a phone-level language model. The acoustic likelihoods associated with the arcs are subsequently renormalised to posterior probabilities, which allows us to compute a posterior probability for each phone at any time. A major difference with Soft VQ is that phone posteriorgrams exploit prior knowledge about the phone inventory that the user can produce. 2.3.3. Utterance-level HAC representation The posteriorgrams of spectral feature clusters or of phone classes are not suitable to model directly with an NMF. To be able to discover recurring patterns in utterances, they need to be mapped to a representation of ﬁxed dimension in which linearity holds, i.e. that the utterance-level speech representation is approximately equal to the sum of the speech representations  74  of the acoustic patterns it contains [18, 19]. A mapping that exhibits this property is the so-called histogram of acoustic cooccurrences (HAC) [19]. The HAC of a speech segment is the posterior joint probability of two acoustic events happening at a predeﬁned time lag τ , accumulated over the entire segment. An acoustic event is the observation of a spectral feature vector from a particular cluster in the case of soft VQ, or the observation of a phone in the case of phone posteriorgrams. Since the HAC representation considers event pairs, its dimensionality is the square of the number of acoustic event classes. In this paper, we stack HAC vectors computed for multiple values of the time lag τ = 20, 50, 90 and 200 ms into a single augmented HAC vector to characterise an utterance. When multiple (training) utterances are available, their augmented HAC representations are arranged as columns of a matrix Va. 2.4. Non-negative matrix factorisation NMF uses non-negativity constraints for decomposing a matrix into its components [20, 21, 22, 23], i.e given a non-negative matrix V of size [M xN ], NMF approximately decomposes it into its non-negative components W of size [M xR] and H of size [RxN ]. Under the right conditions, NMF is able to ﬁnd parts in data. In ASR, NMF is used to discover recurring acoustic patterns (word units) through some grounding information [24, 25, 26]. In this paper, we use the Kullback-Leibler divergence to quantify the approximation quality of the NMF as expressed in Eq 1.  (H, W) = arg min DKL( V || [WH])  (1)  (H,W)  Finding the W and H that minimize this approximation metric for a given data matrix V is achieved using multiplicative update rules[20].  2.4.1. Supervised NMF word learning To employ NMF for word learning, we use a weak form of supervision represented by Vg, which is used together with the augmented HAC acoustic representation of all the training utterances stacked into a matrix Va. The supervision information links the discovered acoustic patterns to slot values and also helps NMF to avoid local optima of the Kullback-Leibler divergence. The supervision Vg is a label matrix where each column represents an utterance and each row represents a slot value. The presence of a slot value in an utterance is represented in the label matrix with a ‘1’ and its absence with a ‘0’.  Through the factorization of the composite matrix constructed by vertical concatenation of Vg and Va, NMF discovers latent slot value representations in each column of Wa. The columns of Wg link the learned acoustic patterns in columns of Wa to the slot values represented by the rows of Vg. Furthermore, some extra columns of Wa and Wg are used to represent ﬁller words (words which are present in the utterance but are not related to any slot value). The columns of H matrix indicate which columns of Wa and Wg are combined to reconstruct Va and Vg respectively. The learned acoustic patterns in Wa and labeling information in Wg as given in Eq. 2 will be used in the testing phase to detect the learned acoustic units within unseen test utterances.  »  Vg Va  –» ≈  Wg Wa  – H  (2)  2.4.2. NMF in the usage phase The learned NMF model is applied in two different approaches to decoding. Both decoders apply the learned NMF model to word-sized segments of speech in a sliding window analysis. A sliding window of a width of 300 ms and a shift of 100 ms is used to produce an augmented HAC vector at 100 ms intervals across an utterance. As a result, an utterance is represented by a matrix Vs, containing one column per window position. By employing the NMF factorization Eq. 3, which is called the local NMF, the corresponding slot value activations are calculated.  Hs = arg min DKL(Vs||WaHs)  (3)  Hs  This is followed by the calculation of the activation matrix As. Each column of the activation matrix contains labeling information of all slot values for a particular window position.  As = WgHs  (4)  In the simplest form of decoding, called NMF decoding, the slot values are inferred directly from the local (sliding window) NMF. The activations for all slot values are accumulated over all window positions, i.e. over the complete utterance. Since each slot can have at most one value assigned, only the value hypothesis with the largest accumulated activation is kept per slot. The slot value is considered to be detected, only if the accumulated activation exceeds a threshold. The order in which the acoustic patterns related to the slot values occur in the utterance is therefore ignored. Since this procedure may result in multiple possible frames, we select the frame with the highest average probability mass.  In a reﬁnement, called HMM decoding, the local NMF model generates a data stream which is modeled by an HMM. The HMM captures the relation between word usage – including word order – and frame descriptions of actions. Since the HMM models the sequential aspects of the utterance (such as word order), we consider the learning of this HMM a form of grammar induction. The details of this approach are explained in the next section. 2.5. Grammar induction Identical or similar words (e.g. numbers) may refer to different slots, so slot-value pairs can only be assigned correctly from spoken input if grammar is taken into account. HMM decoding ﬁxes the major shortcoming of NMF decoding, i.e. that the order in which slot values occur, is ignored. The local NMF stream is then modeled by an HMM, which is learned from the user interaction examples.  2.5.1. HMM learning The activation sequence is modeled by a multi-labeling HMM [9]. Like in discrete-density HMMs, each state q is characterized by probabilities bj(q) over observations j. In this framework, the observation is characterized by a probability distribution derived from NMF atom activations, obtained as Hs, normalized to sum to unity. The state probability is then  75  the inner product of this distribution with the state distribution. Applied to this problem, each semantic frame is modeled by an HMM in which each slot value is assigned an HMM state referred to as slot value state. States are fully connected, with two exceptions. First, within slot transitions are prohibited, since each slot needs to be assigned only one value. Second, states can only transition to slot-value states within the same semantic frame, since each spoken command can only correspond to a single semantic frame. To limit the number of transition probabilities to be estimated, all transitions from states associated with a particular slot, to all states associated with another slot, share the same transition probability. The HMM will hence learn the sequence of slots in the user’s utterances, but not the sequence of individual words. All the states can be initial or ﬁnal states. HMM training is done using the Baum-Welch algorithm [27]. Supervision information provided by the labeling matrix Vg, is used to only assign non-zero state posteriors to slot values that are present in the frame description of an utterance. All non-zero entries of the state-transition matrix are initialised to (properly normalised) random values. The emission matrix is initialised by Wg. 2.5.2. HMM decoding During decoding, the maximum likelihood state sequence is obtained using the Viterbi algorithm for the given observation sequence Hs. Visiting a state in an HMM corresponding to a semantic frame implies the corresponding slot value is detected. Since states representing slot values can only transition to states within the same semantic frame, the Viterbi search implicitly selects the most likely frame. 3. Experimental Setup In this section, we give a description of the databases used for evaluation, the evaluation procedure and metrics. 3.1. Databases 3.1.1. PATCOR The database PATCOR contains recordings of subjects playing a card game called “Patience” using spoken commands. The database contains 8 speakers with in total more than two thousand commands. The data was collected from unimpaired subjects with non-pathological speech, speaking Belgian Dutch. The users were free to choose their vocabulary and grammar, although in practice the vocabulary was limited indirectly by the number of cards, card positions and functionality. A typical utterance in PATCOR is “Put the four of clubs on the ﬁve of hearts”. In this type of utterance, the order of the  Table 1: parameters of the speech databases  Database number of speakers number of frames number of slots number of slot values number of blocks  PATCOR 8 2 9 58 8  DOMOTICA 20 4 7 27 6  Intelligiblity score  100 90 80 70 60 50 40 30 20 10 0 41 32 33 42 30 37 35 28 47 29 31 46 34 45 40 48 17 44 43 11 Speakers id Figure 2: Speech intelligibility measurements of the speakers in DOMOTICA-2. The speakers are order by intelligibility score. Generally speaking, a score higher than 85% is nonpathological (see the dashed line). words plays a key role in discovering the utterance’s meaning. The gold-standard frame descriptions of the utterances were created manually. In Table 1 an overview of the total number of frames, slots and slot values used is given. Since not all possible slot values occur for all speakers, Table 7 gives the actual number of slot values for each speaker. For a more detailed description of the frame descriptions that were used, as well as the slot values used for each speaker, we refer the reader to the technical report [28]. 3.1.2. DOMOTICA-2 The DOMOTICA-2 database contains recordings of impaired, dysarthric speakers controlling a home automation system. A typical DOMOTICA-2 utterance would look like “Turn on the kitchen light”. Since collecting a large number of realistic, spontaneous spoken commands is difﬁcult due to the targeted users getting tired quickly, a two-phase data collection method was used. In the ﬁrst phase, 9 users were asked to control 31 different appliances in a 3D environment [28], guided by a visualised scenario in order to ensure an unbiased choice of words and grammar. In the second phase, these command lists were read back repeatedly by 21 test users. Of these 21, 8 speakers were selected based on their increased risk for degenerate voice rather than currently having a pathological voice. For all speakers, speech intelligibility scores were obtained by analysing their recorded speech using an automated tool [29]. These scores are shown in Fig. 2. Table 1 gives an overview of the total number of frames, slots and slot values. For some speakers some slot values were not used, since some commands were not spoken enough times to allow a meaningful evaluation; Table 7 gives the actual number of slot values for each speaker. For a more detailed description of the slot values used for each speaker we refer the reader to the technical report [28]. 3.2. Methodology The goal of the experiments is to evaluate the performance as a function of the amount of training data used. However, since  76  this means the amount of training data can be very small, a form of cross validation is needed to obtain statistically meaningful scores. First, we divide the spoken commands (utterances) of each speaker into equal or nearly equal parts called blocks. The k blocks are created by minimising the Jensen-Shannon divergence (JSD) between the slot value distributions of all blocks. This optimisation is performed in an iterative process starting by dividing all utterances randomly into k blocks and then swapping at each iteration those two utterances that minimise the JSD the most from one block to the other one. The process stops when the JSD is minimised, i.e. when there are no swaps left that can lower the JSD. The slot values are then approximately evenly distributed throughout the blocks. Under the constraint that each slot value should occur at least once in each block, some slot values are excluded from the frame structure, meaning that the spoken words corresponding to these slot values, become ﬁller words: they are not supervised and they are not scored anymore. Such adaptation to the supervision is speaker dependent and the number of slot values used for each speaker can be found in [28]. Utterances without any slot values were removed from the training and test sets. To evaluate the learning speed of our framework, we created a k × k latin square in which each block occurs exactly once in each row and in each column. We selected ﬁve rows of the latin square to create a ﬁve-fold cross-validation experiment in which the train and test sets respectively increase and decrease in size. In each fold, we start with an experiment where only one block is used for training while the remaining k − 1 blocks are used for testing. We incrementally increase the number of blocks n used for training in the subsequent experiments and the last experiment will be performed with n = k − 1 training blocks and one test block. Throughout the folds, the train and test sets are always composed of different blocks allowing for a more reliable scoring. 3.3. Parameters The number of frames needed to have a reliable estimation of the cluster centres, depends on the dimensionality of the feature vectors. The minimum number of frames used for adaptive codebook training is chosen to be 78, two times the dimensionality of the MFCC feature vectors. For PATCOR, the resulting VQ codebook sizes typically ranged from 40 for the smallest training set to 145 for the largest training set. For DOMOTICA2, the resulting codebook sizes typically ranged from 36 for the smallest training set to 118 for the largest training set. For both databases, phone posteriorgrams were obtained using a free phone recognizer using a unigram language model. The phone recognizer was trained on a dataset containing recordings of selected radio and television news broadcasts in the same language as the collected databases. Phones are modeled with 3-state HMMs and in total 48845 tied Gaussians are used in the acoustic model. The phonetic alphabet includes one noise unit and one silence unit in addition to 48 phones. For the utterance-based HAC representations, from both VQ and phone posteriorgrams, only the top-three largest indices at each time frame were retained. 3.4. Evaluation For each utterance in the databases, we have a manually constructed gold standard frame description, which is used as a reference for system evaluation. In this reference frame description, the slot values that are expressed in the utterance, are  ﬁlled in. The system was evaluated by comparing the automatically induced frame descriptions to the gold standard reference frames. The used metric is the slot Fβ=1-score, which is the harmonic mean of the slot precision and the slot recall. These metrics are commonly used for the evaluation of frame-based systems for spoken language understanding [15]. The following formulas were used for calculation:  slot precision slot recall slot Fβ=1-score  =  # correctly ﬁlled slots # total ﬁlled slots in induced frame  (5)  =  # correctly ﬁlled slots # total ﬁlled slots in reference frame  (6)  =  2  ·  slot precision · slot recall slot precision + slot recall  (7)  This means that only slots that are ﬁlled with a correct value are rewarded, and both slots that are falsely ﬁlled and slots that are falsely left empty are penalised. When an induced frame is of another type than the corresponding reference frame, the ﬁlled slots in the induced frame and in the reference frame are consequently different, which automatically results in a relatively large drop in the slot F-score. It should be noted that the reported F-scores aggregate slot counts over all ﬁve folds.  4. Results In Fig. 3, F-scores for eight speakers per database are depicted as a function of the average number of utterances in the training set. The F-scores against increasing train set sizes provides some insight into the self-learning aspect of the framework. For each database, there are two graphs, one graph depicting NMF learning of slot value representations and one graph depicting HMM-based grammar induction. For visibility, Fig. 3 does not contain all speakers from DOMOTICA-2. For this dataset, all F-scores for the NMF-based word ﬁnding module are presented in Table 2 and all scores for the HMM-based grammar induction module are presented in Table 3. There is one column for each speaker and the rows indicate the number of blocks in the training sets.  4.1. PATCOR When we compare the respective F-scores for each speaker and for each training set size, we ﬁnd a signiﬁcant difference between the scores of the word ﬁnding module and the grammar induction module using a paired student’s t-test, t(55) = 5, 11, p < 0.001. On average, the grammar induction module improves the F-score with 5%, but the improvement varies between speakers. For some speakers, the induced grammar provides a considerable improvement, for instance for speaker 3, The improvement is 16% on average, t(6) = 33, 16, p < 0.001. However, for instance, for speaker 5, we don’t ﬁnd a substantial improvement using the grammar induction module. In any case, using grammar induction does not seem to degrade the performance for any user in PATCOR.  77  F−score  0.8 0.7 0.6 0.5 0.4 0.3 0.2 0  5  5  58  2  2  8  5  82  
Distributional Compositional Semantics (DCS) methods combine lexical vectors according to algebraic operators or functions to model the meaning of complex linguistic phrases. On the other hand, several textual inference tasks rely on supervised kernel-based learning, whereas Tree Kernels (TK) have been shown suitable to the modeling of syntactic and semantic similarity between linguistic instances. While the modeling of DCS for complex phrases is still an open research issue, TKs do not account for compositionality. In this paper, a novel kernel called Compositionally Smoothed Partial Tree Kernel is proposed integrating DCS operators into the TK estimation. Empirical results over Semantic Text Similarity and Question Classiﬁcation tasks show the contribution of semantic compositions with respect to traditional TKs. 
In the knowledge representation and reasoning research area, argumentation theory aims at representing and reasoning over information items called arguments. In everyday life, arguments are reasons to believe and reasons to act, and they are usually expressed in natural language. Even if ad-hoc natural language examples are often provided in argumentation theory works, no automated processing of such natural language arguments is carried out, making it impossible to exploit the results of this research area in real world scenarios. In this paper, we propose to adopt textual entailment to address this issue. In particular, we discuss and evaluate, on a sample of natural language arguments extracted from Debatepedia, the support and attack relations among arguments in bipolar abstract argumentation with respect to the more speciﬁc notions of textual entailment and contradiction. 
This work describes the evaluations of three different approaches, Lexical Match, Sense Similarity based on Personalized Page Rank, and Semantic Match based on Shallow Frame Structures, for word sense alignment of verbs between two Italian lexical-semantic resources, MultiWordNet and the Senso Comune Lexicon. The results obtained are quite satisfying with a ﬁnal F1 score of 0.47 when merging together Lexical Match and Sense Similarity. 
Abduction allows us to model interpretation of discourse as the explanation of observables, given additional knowledge about the world. In an abductive framework, many explanations can be constructed for the same observation, requiring an approach to estimate the likelihood of these alternative explanations. We show that, for discourse interpretation, weighted abduction has advantages over alternative approaches to estimating the likelihood of hypotheses. However, weighted abduction has no probabilistic interpretation, which makes the estimation and learning of weights difﬁcult. To address this, we propose a formal probabilistic abductive framework that captures the advantages weighted abduction when applied to discourse interpretation. 
This paper provides a ﬁrst investigation over existing textual inference paradigms in order to propose a generic framework able to capture major semantic aspects in Human Robot Interaction (HRI). We investigate the use of general semantic paradigms used in Natural Language Understanding (NLU) tasks, such as Semantic Role Labeling, over typical robot commands. The semantic information obtained is then represented under the Abstract Meaning Representation. AMR is a general representation language useful to express different level of semantic information without a strong dependence to the syntactic structure of an underlying sentence. The ﬁnal aim of this work is to ﬁnd an effective synergy between HRI and NLU. 
University of Wolverhampton 2"3-45%%$6(7+,-."%/"(01  A bst r act This article proposes a new approach to verb classification based on Semantic Types selected in corpus-based verb patterns. This work !"#$%& '(& )#(*%+%& ,-.'"/& '0& 1'"2%& #(!& 3xploitations (Hanks 2013) and applies Corpus Pattern Analysis to a subset of verbs from Lev4(+%&56'4%'(+&78#%%9&4(78:!4(;&<."=%&%:7-&#%& hang and stab. These patterns are taken from the Pattern Dictionary of English Verbs, which aims at recording prototypical phraseological patterns for the most frequent verbs of English using the British National Corpus. 
This paper deﬁnes a representation of universal quantiﬁcation within distributional semantic space. We propose a discourse-internal approach to the meaning of limited instances of every, highlighting the possibilities and limitations of doing textual logic in a purely distributional framework. 
This paper presents an alternative method to measuring word-word semantic relatedness in distributional semantics framework. The main idea is to represent target words as rankings of all co-occurring words in a text corpus, ordered by their tf – idf weight and use a metric between rankings (such as Jaro distance or Rank distance) to compute semantic relatedness. This method has several advantages over the standard approach that uses cosine measure in a vector space, mainly in that it is computationally less expensive (i.e. does not require working in a high dimensional space, employing only rankings and a distance which is linear in the rank’s length) and presumably more robust. We tested this method on the standard WS353 Test, obtaining the co-occurrence frequency from the Wacky corpus. The results are comparable to the methods which use vector space models; and, most importantly, the method can be extended to the very challenging task of measuring phrase semantic relatedness. 
In this paper, we want to start the analysis of the models for compositional distributional semantics (CDS) with respect to the distributional similarity. We believe that this simple analysis of the properties of the similarity can help to better investigate new CDS models. We show that, looking at CDS models from this point of view, these models are strictly related with convolution kernels (Haussler, 1999), e.g.: tree kernels (Collins and Duffy, 2002). We will then examine how the distributed tree kernels (Zanzotto and Dell’Arciprete, 2012) are an interesting result to draw a stronger link between CDS models and convolution kernels. 
The paper presents a supervised approach to semantic parsing, based on a new semantic resource, the Pattern Dictionary of English Verbs (PDEV). PDEV lists the most frequent patterns of English verbs identiﬁed in corpus. Each argument in a pattern is semantically categorized with semantic types from the PDEV ontology. Each pattern is linked to a set of sentences from the British National Corpus. The article describes PDEV in details and presents the task of pattern classiﬁcation. The system described is based on a distributional approach, and achieves 66% in Micro-average F1 across a sample of 25 of the most frequent verbs. 
 Natural Language understanding over a set of sentences or a document is a challenging problem. We approach this problem using semantic extraction and building an ontology for answering questions based on the data. There is more information in a sentence than found by extracting out the visible terms and their obvious relations between one another. Keeping track of inferences, quantities, inheritance, properties, and set related information is what gives this approach the advantage over alternatives. Our methodology was tested against the FraCas Test Suite with near perfect results for the sections: Generalized Quantifiers, Plurals, Adjectives, Comparatives, Verbs, and Attitudes. The results indicate that extracting the visible semantics as well as the unseen semantics and their interrelations using an ontology to logically provide reliable and provable answers to questions validating our methodology.  
Comparisons are phrases that express the likeness of two entities. They are usually marked by linguistic patterns, among which the most discussed are X is like Y and as X as Y. We propose a simple slot-based dependency-driven description of such patterns that reﬁnes the phrase structure approach of Niculae and Yaneva (2013). We introduce a simple similaritybased approach that proves useful for measuring the degree of ﬁgurativeness of a comparison and therefore in simile (ﬁgurative comparison) identiﬁcation. We propose an evaluation method for this task on the VUAMC metaphor corpus. 
The Textual Entailment task has become inﬂuential in NLP and many researchers have become interested in applying it to other tasks. However, the two major issues emerging from this body of work are the fact that NLP applications need systems that (1) attain results which are not corpus dependent and (2) assume that the text for entailment cannot be incorrect or even contradictory. In this paper we propose a system which decomposes the text into chunks via a shallow text analysis, and determines the entailment relationship by matching the information contained in the is − a pattern. The results show that the method is able to cope with the two requirements above. 
It has been observed that the inclusion of morphosyntactic information in dependency treebanks is crucial to obtain high results in dependency parsing for some languages. In this paper we explore in depth to what extent it is useful to include morphological features, and the impact of diverse morphosyntactic annotations on statistical dependency parsing of Spanish. For this, we give a detailed analysis of the results of over 80 experiments performed with MaltParser through the application of MaltOptimizer. Our goal is to isolate conﬁgurations of morphosyntactic features which would allow for optimizing the parsing of Spanish texts, and to evaluate the impact that each feature has, independently and in combination with others. 
Turkish is an agglutinative language with rich morphology-syntax interactions. As an extension of this property, the Turkish Treebank is designed to represent sublexical dependencies, which brings extra challenges to parsing raw text. In this work, we use a joint POS tagging and parsing approach to parse Turkish raw text, and we show it outperforms a pipeline approach. Then we experiment with incorporating morphological feature prediction into the joint system. Our results show statistically signiﬁcant improvements with the joint systems and achieve the state-ofthe-art accuracy for Turkish dependency parsing. 
We present, here, our analysis of systematic divergences in parallel EnglishHindi dependency treebanks based on the Computational Paninian Grammar (CPG) framework. Study of structural divergences in parallel treebanks not only helps in developing larger treebanks automatically, but can also be useful for many NLP applications such as data-driven machine translation (MT) systems. Given that the two treebanks are based on the same grammatical model, a study of divergences in them could be of advantage to such tasks, along with making it more interesting to study how and where they diverge. We consider two parallel trees divergent based on differences in constructions, relations marked, frequency of annotation labels and tree depth. Some interesting instances of structural divergences in the treebanks have been discussed in the course of this paper. We also present our task of alignment of the two treebanks, wherein we talk about our extraction of divergent structures in the trees, and discuss the results of this exercise. 
 compared to (Penn Tree Bank, Marcus et al. 1993). But the creation of a treebank following  This article presents a new approach of using dependency treebanks in theoretical syntactic research: the view of dependency treebanks as combined networks. This allows the usage of advanced tools for network analysis that quite easily provide novel insight into the syntactic structure of language. As an example of this approach, we will show how the network approach can provide clear structural distinctions among the Chinese function words, which are very difficult to obtain directly from the original treebank. We hope to illustrate the enormous potential of the language network approach through a simple example.  a specific syntactic theory cannot in itself be considered as a confirmation of this theory (other than being a sociological proof of the existence of sufficient support for the theory to be able to create a treebank). What is crucially missing in this picture is the usage of treebanks for linguistic discovery and theory confirmation or refutation that goes beyond searching for examples in the annotated data. Simple concordancers exist, some of them with sophisticated query languages (Zeldes et al. 2009) but it is up to the syntactician to go through the results and make conclusions. No generally accepted approach on how to  
This article proposes a simple modeling of Korean word order within the framework of the topological dependency grammar – the first topological modeling for this language – a system of formal rules accounting for the correspondence between the dependency tree of a sentence and an ordered constituent structure. We show that a fairly small number of linearization rules can account for the word order facts of Korean, considered to be a language with a relatively free order. These rules will be described, especially the non-projectivity phenomenon based of the notion of “verb cluster”, a cohesive topological constituent, which appears in a syntaxtopology interface. 
We report on a rule-based procedure of extracting and labeling English verb collocates from a dependency-parsed corpus. Instead of relying on the syntactic labels provided by the parser, we use a simple topological sequence that we fill with the extracted collocates in a prescribed order. A more accurate syntactic labeling will be obtained from the topological fields by comparison of corresponding collocate positions across the most common syntactic alternations. So far, we have extracted and labeled verb forms and predicate complements according to their morphosyntactic structure. In the next future, we will provide the syntactic labeling of the complements. 
Systemic Functional Linguistics provides a semiotic perspective on language. The text analysis described in Systemic Functional Linguistics (SFL) can be of critical value in real-world applications. But parsing with SFL grammars is computationally intensive task and parsers for this level of description to date have not been able to operate on unrestricted input. This paper describes a graph-based method to automatically generate simplified SFL mood and transitivity parses of English sentences from Stanford Dependency parses and a database providing transitivity categories for each verb. 
Closely related words tend to be close together in monolingual language use. This paper suggests that this is different in bilingual language use. The Distance Hypothesis (DH) proposes that long dependency distances between syntactically related units facilitate bilingual code-switching. We test the DH on a 9,023 word German/English and a 19,766 word Chinese/English corpus. Both corpora support the DH in that they present longer mixed dependencies than monolingual ones. Selected major dependency types (subject, object, adjunct) also have longer dependency distances when the head word and its dependent are from different languages. We discuss how processing motivations behind the DH make it a potentially viable motivator for bilingual language use. 
This paper presents the Arborator, an online tool for collaborative dependency annotation together with a case study of crowdsourcing in a pedagogical university context. In greater detail, we explore what generally distinguishes dependency annotation tools from phrase structure annotation tools and we introduce existing tools for dependency annotation as well as the distinctive features and design choices of our tool. Finally we show how to setup a crowdsourced dependency annotation experiment as an exercise for university students. We explore constraints, results, and conclusions to draw. 
The paper describes overt marking of information structure in the indigenous Andean language Aymara. In this paper we show that although the word order is free, Aymara is not a discourse-conﬁgurational language (Kiss, 1994); instead, information structure is expressed only morphologically by pragmatic sufﬁxes. The marking of ‘topic’ is more ﬂexible that the marking of ‘focus’ (be it at the clause level or NP-internal). Since overt marking of information structure is partial, this paper also devotes considerable attention to the resolution of underspeciﬁcation in Aymara. 
The overall goal of our work is to build a dependency grammar-based human sentence processor for Hindi. As a ﬁrst step towards this end, in this paper we present a dependency grammar that is motivated by psycholinguistic concerns. We describe the components of the grammar that have been automatically induced using a Hindi dependency treebank. We relate some aspects of the grammar to relevant ideas in the psycholinguistics literature. In the process, we also extract statistics and patterns for phenomena that are interesting from a processing perspective. We ﬁnally present an outline of a dependency grammar-based human sentence processor for Hindi.  Mary, ‘John’ and ‘Mary’ are connected via arcs to ‘kissed’; the former arc bears the label ‘subject’ and the latter arc the label ‘object’. Taken together, these nodes with their connections form a tree. Figure 1 shows the dependency and the phrase structure trees for the above sentence.  
 This paper addresses a hot topic of Hungarian syntactic research, viz. the treatment of “discontinuous” constructions involving auxiliaries. The case is made for a projective dependency grammar (DG) account built on the notions of rising and catenae (Groß and Osborne, 2009). Additionally, the semantic basis of the dependency created by rising is described with a view to analogy and constructional meaning.  
This paper is focused on description of hy­ potactic constructions (constructions with sub­ ordinating conjunctions) with “elaborative” meanings. In dependency­based linguistic lit­ erature, they are referred to as hypotactic co­ ordinations or also (a subset of) false depen­ dent clauses. The analysis makes use of syn­ tax­ and discourse­annotated corpora of Czech and English and thus offers an empirically grounded contrastive study of the phenomena. 
This paper shows how to introduce predicative adjunction in a dependency grammar inspired from TAG, MTT and RG. The addition of predicative adjunction allows us to obtain a modular surface syntactic grammar, the derivation structures of which can be interpreted as semantic representations. 
Light verb constructions (LVCs) pose a serious challenge for both theoretical and applied linguistics as their syntactic structures are not solely determined by verbs alone but also by predicative nouns. In this contribution, we introduce an initial step to a new formal lexicographic representation of LVCs for the valency lexicon of Czech verbs, VALLEX. The main idea underlying our representation is to decompose the information on an LVC between (i) the verbal valency frame and (ii) the nominal valency frame. Both deep and surface syntactic structures of LVCs can be easily derived from the information given in the verbal and nominal frames by application of formal rules as they are introduced in this contribution. 
We describe a Deterministic Dependency Parser for Sanskrit. The parse is developed following a Depth First traversal of a graph whose nodes represent morphological analyses of the words in a sentence. During the traversal, relations at each node are checked for local compatibility, and ﬁnally for each full path, the relations on the path are checked for global compatibility. Stacking of intermediate results guarantees dynamic programming. We also describe an interface that displays multiple parses compactly and facilitates users to select the desired parse among various possible solutions with a maximum of n − 1 choices for a sentence with n words. 
We are interested in a graph-based Knowledge Representation (KR) formalism that would allow for the representation, manipulation, query, and reasoning over dependency structures, and linguistic knowledge of the lexicon in the Meaning-Text Theory framework. Neither the semantic web formalisms nor the conceptual graphs appear to be suitable for this task, and this led to the introduction of the new Unit Graphs (UG) framework. In this paper we will overview the foundational concepts of this framework: the UGs are deﬁned over a UG-support that contains: i) a hierarchy of unit types which is strongly driven by the actantial structure of unit types, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identiﬁers. Based on these foundational concepts and on the deﬁnition of UGs, this paper justiﬁes the use of a deep semantic representation level to represent meanings of lexical units. Rules over UGs are then introduced, and lexicographic deﬁnitions of lexical units are added to the hierarchy of unit types. Finally this paper provides UGs with semantics (in the logical sense), and pose the entailment problem, so as to enable the reasoning in the UGs framework. 
In this paper, we provide a quantitative analysis of non-projective constructions attested in the Ancient Greek Dependency Treebank (AGDT). We consider the different types of formal constraints and metrics that have become standardized in the literature on non-projectivity (planarity, wellnestedness, gap-degree, edge-degree). We also discuss some of the linguistic factors that cause non-projective edges in Ancient Greek. Our results conﬁrm the remarkable extension of non-projectivity in the AGDT, both in terms of quantitative incidence of non-projective nodes and for their complexity, which is not paralleled by the corpora of modern languages considered in the literature. At the same time, the usefulness of other constraint (especially well-nestedness) is conﬁrmed by our researches. 
The Stanford dependency scheme aims to provide a simple and intuitive but linguistically sound way of annotating the dependencies between words in a sentence. In this paper, we address two limitations the scheme has suffered from: First, despite providing good coverage of core grammatical relations, the scheme has not offered explicit analyses of more difﬁcult syntactic constructions; second, because the scheme was initially developed primarily on newswire data, it did not focus on constructions that are rare in newswire but very frequent in more informal texts, such as casual speech and current web texts. Here, we propose dependency analyses for several linguistically interesting constructions and extend the scheme to provide better coverage of modern web data. 
This paper provides an analysis of the representation of various grammatical phenomena in both constituency structure and dependency structure (hereafter c-structure and d-structure), including agreement, case marking, and word order in transitive sentences, as well as three theoretical constructs, and the interface between the form of a sentence and its meaning. There is a crucial structural relationship for all of these phenomena in both constituency grammar and dependency grammar, but only the version used in dependency grammar is fully reliable. This insight evokes the following question: Why do linguists working with constituency grammars think that so many nodes are necessary? Upon examination, the extra nodes succeed only in confusing our understanding of syntactic phenomena. 
The focus of this paper is the description of the surface syntax relations in the simple clause in Old French and the way they can be described in a dependency grammar. The declension system of Old French is not reliable enough to cope with the identiﬁcation of the dependents of the main verb, but it remains true that related grammatical markers are still observable and obey rules that forbid them to appear in speciﬁc syntactic positions. This study relies on three previous accounts; Igor Mel’cˇuk’s “criteria B”, the criteria that are used to determine which is the syntactic governor in a syntactic dependency relation, Thomas Groß’s intraword analysis, which grants morphs node status in the tree, and the concept of speciﬁcation as used by Alain Lemaréchal, who understands grammatical markers as a set of formal constraints that stack over a relation. I demonstrate that the structure of the nominal dependents of the verb is highly unstable, ranging from explicit marking of the relation to constructions that do not make use of any segmental marker: some structures use bound morphemes, some others use free morphemes and some use only semantic features to express the relations. Moreover, markers are mainly optional and can stack up in a hierarchical way, which results in variable structural organization of the nominal phrase. 
There is an increasing need for the annotation of multiple types of linguistic information that are rather different in their nature, e.g., word order, morphological features, syntactic and semantic relations, etc. Quite frequently, their annotation is combined in a single structure, which not only results in inadequate annotations of treebanks and consequent low-quality applications trained on them, but also is deﬁcient from a theoretical (linguistic) perspective. We present a new corpus of Spanish annotated on four independent levels, morphology, surface-syntax, deep-syntax and semantics, as well as the methodology that allows for obtaining it with fewer cost while maintaining a high inter-annotator agreement. 
 meaning of the input and producing a target lan-  The paper presents our work on the annotation of intra-chunk dependencies on an English treebank that was previously annotated with Inter-chunk dependencies, and for which there exists a fully expanded parallel Hindi dependency treebank. This provides fully parsed dependency trees for the English treebank. We also report an analysis of the inter-annotator agreement for this chunk expansion task. Further, these fully expanded parallel Hindi and English treebanks were word aligned and an analysis for the task has been given. Issues related to intra-chunk expansion and alignment for the language pair HindiEnglish are discussed and guidelines for these tasks have been prepared and released.  guage tree helps to ensure the grammaticality of the output (Cowan, 2008). Thus there is a need for aligned parallel treebanks with alignment information on top of their parsing information. And, with the availability of a number of treebanks of various languages now, parallel treebanks are being put to use for analysis and further experiments. A parallel treebank comprises syntactically annotated aligned sentences in two or more languages. In addition to this, the trees are aligned on a sub-sentential level. (Tinsley et al., 2009). Further, such resources could be useful for many applications, e.g. as training or evaluation corpora for word/phrase alignment, as also for data driven MT systems and for the automatic induction of transfer rules. (Hearne et al., 2007) Our work using two parallel dependency treebanks is another such effort in this direction. It includes:  
 In this paper, we present the results of the parallel Czech coreference and bridging annotation in the Prague Dependency Treebank 2.0. The annotation is carried out on dependency trees (on the tectogrammatical layer). We describe the inter-annotator agreement measurement, classify and analyse the most common types of annotators’ disagreement. On two selected long texts, we asked the annotators to mark the degree of certainty they have in some most problematic points; we compare the results to the inter-annotator agreement measurement.  
In this paper, we explore the benefits of dependency trees and tectogrammatical structure used in the Prague Dependency Treebank for annotating language phenomena that cross the sentence boundary, namely coreference and bridging relations. We present the benefits of dependency trees such as the detailed processing of ellipses, syntactic decisions for coordination and apposition structures that make possible coding coreference relations in cases that are not so easy when annotating on the raw texts. We introduce the coreference decision for nonreferring constructions and present some tectogrammatical features that are useful for annotation of coreference. 
In this work, we present a data-driven method to enhance syntax trees with additional dependencies as deﬁned in the wellknown Stanford Dependencies scheme, so as to give more information about the structure of the sentence. This hybrid method utilizes both machine learning and a rule-based approach, and achieves a performance of 93.1% in F1-score, as evaluated using an existing treebank of Finnish. The resulting tool will be integrated into an existing Finnish parser and made publicly available at the address http://bionlp.utu.fi/. 
A recent project to produce a much belated English translation of Lucien Tesnière’s Éléments de syntaxe structurale has provided the opportunity for an in depth look at Tesnière’s theory of syntax. This contribution examines a few aspects of Tesnière’s work through the lens of modern syntactic theory. Tesnière’s understandings of constituents and phrases, auxiliary verbs, prepositions, gapping, right node raising, propositional infinitives, and exocentric structures are all briefly considered. Concerning some of these areas, we see that Tesnière was visionary with his analysis, whereas in other areas, modern syntactic theory now rejects his account. Of particular interest is the fact that Tesnière’s theory was not entirely dependency-based. His account of transfer (Fr. translation) acknowledged exocentric structures, which means his system was also employing constituency. In this regard, one can, surprisingly, classify Tesnière’s theory as a hybrid dependency-constituency grammar. 
This contribution provides a dependency grammar analysis of the distribution of floating quantifiers in English and German. Floating quantifiers are deemed to be “base generated”, meaning that they are not moved into their surface position by a transformation. Their distribution is similar to that of modal adverbs. The nominal (noun or pronoun) over which they quantify is an argument of the predicate to which they attach. Variation in their placement across English and German is due to independent word order principles associated with each language. 
Exploiting data from a parallel treebank recently developed for Italian, English and French, the paper discusses issues related to the development of a dependency-based alignment system. We focus on the alignment of linguistic expressions and constructions which are structurally different in the languages that have to be aligned, and on how to deal with them using dependency rather than constituency. In order to analyze in particular the shifts related to syntactic structure, we present a selection of cases where a dependencybased and a constituency-based representation has been applied and compared. 
This paper describes the work process for a Multilingual Treebank Annotation Project executed for Google and coordinated by a small core team supervising the linguistic work conducted by linguists working online in various locations across the globe. The task is to review an output of a dependency-syntactic parser, including the POS types, dependency types and relations between the tokens, fix errors in output and prepare the data to a shape that can be used for further training of the parser engine. In this paper we focus on the implemented Quality Assurance processes and methodology that are used to monitor the output of the four language teams engaged in the project. On the quantitative side we monitor the throughput to spot any issues in particular language that would require intervention or improving the process. This is combined with a qualitative analysis that is performed primarily by comparing the incoming parsed data, the reviewed data after the first round and after the final crossreview using snapshots to compile and compare statistics. In addition, the possible inconsistencies in the annotations are checked and corrected automatically, where possible, in appropriate stages of the process to minimize the manual work. 
We establish quantitative methods for comparing and estimating the quality of dependency annotations or conversion schemes. We use generalized tree-edit distance to measure divergence between annotations and propose theoretical learnability, derivational perplexity and downstream performance for evaluation. We present systematic experiments with treeto-dependency conversions of the PennIII treebank, as well as observations from experiments using treebanks from multiple languages. Our most important observations are: (a) parser bias makes most parsers insensitive to non-local differences between annotations, but (b) choice of annotation nevertheless has signiﬁcant impact on most downstream applications, and (c) while learnability does not correlate with downstream performance, learnable annotations will lead to more robust performance across domains. 
 2 Resources and Method  We describe a system for detecting and correcting instances of a small class of frequently occurring grammatical error types in a corpus of essays which have been manually annotated for these errors. Our system employs a precise broad-coverage grammar of English which has been augmented with a set of mal-rules and malentries to explicitly license certain types of erroneous expressions. The derivation tree produced by a parser using this grammar identiﬁes the location and type of an error in an ill-formed sentence, enabling a postprocessing script to use the tree and the inventory of error types to delete and/or insert tokens in order to produce a corrected version of the original sentence. 
Active learning and domain adaptation are both important tools for reducing labeling effort to learn a good supervised model in a target domain. In this paper, we investigate the problem of online active learning within a new active domain adaptation setting: there are insufﬁcient labeled data in both source and target domains, but it is cheaper to query labels in the source domain than in the target domain. Given a total budget, we develop two costsensitive online active learning methods, a multi-view uncertainty-based method and a multi-view disagreement-based method, to query the most informative instances from the two domains, aiming to learn a good prediction model in the target domain. Empirical studies on the tasks of cross-domain sentiment classiﬁcation of Amazon product reviews demonstrate the efﬁcacy of the proposed methods on reducing labeling cost. 
 Within the natural language processing  (NLP) community, active learning has  been widely investigated and applied in or-  der to alleviate the annotation bottleneck  faced by developers of new NLP systems  and technologies. This paper presents the  ﬁrst theoretical analysis of stopping active  learning based on stabilizing predictions  (SP). The analysis has revealed three ele-  ments that are central to the success of the  SP method: (1) bounds on Cohen’s Kappa  agreement between successively trained  models impose bounds on differences in  F-measure performance of the models; (2)  since the stop set does not have to be la-  beled, it can be made large in practice,  helping to guarantee that the results trans-  fer to previously unseen streams of ex-  amples at test/application time; and (3)  good (low variance) sample estimates of  Kappa between successive models can be  obtained. Proofs of relationships between  the level of Kappa agreement and the dif-  ference in performance between consecu-  tive models are presented. Speciﬁcally, if  the Kappa agreement between two mod-  els exceeds a threshold T (where T > 0),  then the difference in F-measure perfor-  mance between those models is bounded  above of the  bpyos4it(i1vT−eTc)oinnjuanllctciaosneso.f  If precision the models  is assumed to be p, then the bound can be  tightened  to  4(1−T ) (p+1)T  .  
We design a new co-occurrence based word association measure by incorporating the concept of signiﬁcant cooccurrence in the popular word association measure Pointwise Mutual Information (PMI). By extensive experiments with a large number of publicly available datasets we show that the newly introduced measure performs better than other co-occurrence based measures and despite being resource-light, compares well with the best known resource-heavy distributional similarity and knowledge based word association measures. We investigate the source of this performance improvement and ﬁnd that of the two types of signiﬁcant co-occurrence - corpus-level and document-level, the concept of corpus level signiﬁcance combined with the use of document counts in place of word counts is responsible for all the performance gains observed. The concept of document level signiﬁcance is not helpful for PMI adaptation. 
We discuss data-driven morphological segmentation, in which word forms are segmented into morphs, the surface forms of morphemes. Our focus is on a lowresource learning setting, in which only a small amount of annotated word forms are available for model training, while unannotated word forms are available in abundance. The current state-of-art methods 1) exploit both the annotated and unannotated data in a semi-supervised manner, and 2) learn morph lexicons and subsequently uncover segmentations by generating the most likely morph sequences. In contrast, we discuss 1) employing only the annotated data in a supervised manner, while entirely ignoring the unannotated data, and 2) directly learning to predict morph boundaries given their local sub-string contexts instead of learning the morph lexicons. Speciﬁcally, we employ conditional random ﬁelds, a popular discriminative log-linear model for segmentation. We present experiments on two data sets comprising ﬁve diverse languages. We show that the fully supervised boundary prediction approach outperforms the state-of-art semi-supervised morph lexicon approaches on all languages when using the same annotated data sets. 
We present a ﬂexible formulation of semisupervised learning for structured models, which seamlessly incorporates graphbased and more general supervision by extending the posterior regularization (PR) framework. Our extension allows for any regularizer that is a convex, differentiable function of the appropriate marginals. We show that surprisingly, non-linearity of such regularization does not increase the complexity of learning, provided we use multiplicative updates of the structured exponentiated gradient algorithm. We illustrate the extended framework by learning conditional random ﬁelds (CRFs) with quadratic penalties arising from a graph Laplacian. On sequential prediction tasks of handwriting recognition and part-ofspeech (POS) tagging, our method makes signiﬁcant gains over strong baselines. 
This paper proposes a boosting algorithm that uses a semi-Markov perceptron. The training algorithm repeats the training of a semi-Markov model and the update of the weights of training samples. In the boosting, training samples that are incorrectly segmented or labeled have large weights. Such training samples are aggressively learned in the training of the semi-Markov perceptron because the weights are used as the learning ratios. We evaluate our training method with Noun Phrase Chunking, Text Chunking and Extended Named Entity Recognition. The experimental results show that our method achieves better accuracy than a semi-Markov perceptron and a semi-Markov Conditional Random Fields. 
We derive a spectral algorithm for learning the parameters of a reﬁnement HMM. This method is simple, efﬁcient, and can be applied to a wide range of supervised sequence labeling tasks. Like other spectral methods, it avoids the problem of local optima and provides a consistent estimate of the parameters. Our experiments on a phoneme recognition task show that when equipped with informative feature functions, it performs signiﬁcantly better than a supervised HMM and competitively with EM. 
Sentence compression techniques often assemble output sentences using fragments of lexical sequences such as ngrams or units of syntactic structure such as edges from a dependency tree representation. We present a novel approach for discriminative sentence compression that uniﬁes these notions and jointly produces sequential and syntactic representations for output text, leveraging a compact integer linear programming formulation to maintain structural integrity. Our supervised models permit rich features over heterogeneous linguistic structures and generalize over previous state-of-theart approaches. Experiments on corpora featuring human-generated compressions demonstrate a 13-15% relative gain in 4gram accuracy over a well-studied language model-based compression system. 
This paper proposes passage reranking models that (i) do not require manual feature engineering and (ii) greatly preserve accuracy, when changing application domain. Their main characteristic is the use of relational semantic structures representing questions and their answer passages. The relations are established using information from automatic classiﬁers, i.e., question category (QC) and focus classiﬁers (FC) and Named Entity Recognizers (NER). This way (i) effective structural relational patterns can be automatically learned with kernel machines; and (ii) structures are more invariant w.r.t. different domains, thus fostering adaptability. 
In most previous research on distributional semantics, Vector Space Models (VSMs) of words are built either from topical information (e.g., documents in which a word is present), or from syntactic/semantic types of words (e.g., dependency parse links of a word in sentences), but not both. In this paper, we explore the utility of combining these two representations to build VSM for the task of semantic composition of adjective-noun phrases. Through extensive experiments on benchmark datasets, we ﬁnd that even though a type-based VSM is effective for semantic composition, it is often outperformed by a VSM built using a combination of topic- and type-based statistics. We also introduce a new evaluation task wherein we predict the composed vector representation of a phrase from the brain activity of a human subject reading that phrase. We exploit a large syntactically parsed corpus of 16 billion tokens to build our VSMs, with vectors for both phrases and words, and make them publicly available. 
In this paper, we propose a new method for semantic class induction. First, we introduce a generative model of sentences, based on dependency trees and which takes into account homonymy. Our model can thus be seen as a generalization of Brown clustering. Second, we describe an efﬁcient algorithm to perform inference and learning in this model. Third, we apply our proposed method on two large datasets (108 tokens, 105 words types), and demonstrate that classes induced by our algorithm improve performance over Brown clustering on the task of semisupervised supersense tagging and named entity recognition. 
Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way. 
Most compositional-distributional models of meaning are based on ambiguous vector representations, where all the senses of a word are fused into the same vector. This paper provides evidence that the addition of a vector disambiguation step prior to the actual composition would be beneﬁcial to the whole process, producing better composite representations. Furthermore, we relate this issue with the current evaluation practice, showing that disambiguation-based tasks cannot reliably assess the quality of composition. Using a word sense disambiguation scheme based on the generic procedure of Schütze (1998), we ﬁrst provide a proof of concept for the necessity of separating disambiguation from composition. Then we demonstrate the beneﬁts of an “unambiguous” system on a composition-only task. 
Determining the stance expressed by an author from a post written for a two-sided debate in an online debate forum is a relatively new problem in opinion mining. We extend a state-of-the-art learningbased approach to debate stance classiﬁcation by (1) inducing lexico-syntactic patterns based on syntactic dependencies and semantic frames that aim to capture the meaning of a sentence and provide a generalized representation of it; and (2) improving the classiﬁcation of a test post via a novel way of exploiting the information in other test posts with the same stance. Empirical results on four datasets demonstrate the effectiveness of our extensions. 
Large databases of facts are prevalent in many applications. Such databases are accurate, but as they broaden their scope they become increasingly incomplete. In contrast to extending such a database, we present a system to query whether it contains an arbitrary fact. This work can be thought of as re-casting open domain information extraction: rather than growing a database of known facts, we smooth this data into a database in which any possible fact has membership with some conﬁdence. We evaluate our system predicting held out facts, achieving 74.2% accuracy and outperforming multiple baselines. We also evaluate the system as a commonsense ﬁlter for the ReVerb Open IE system, and as a method for answer validation in a Question Answering task. 
Coreference resolution systems can beneﬁt greatly from inclusion of global context, and a number of recent approaches have demonstrated improvements when precomputing an alignment to external knowledge sources. However, since alignment itself is a challenging task and is often noisy, existing systems either align conservatively, resulting in very few links, or combine the attributes of multiple candidates, leading to a conﬂation of entities. Our approach instead performs joint inference between within-document coreference and entity linking, maintaining ranked lists of candidate entities that are dynamically merged and reranked during inference. Further, we incorporate a large set of surface string variations for each entity by using anchor texts from the web that link to the entity. These forms of global context enables our system to improve classiﬁer-based coreference by 1.09 B3 F1 points, and improve over the previous state-of-art by 0.41 points, thus introducing a new state-of-art result on the ACE 2004 data. 
Previous incremental parsers have used monotonic state transitions. However, transitions can be made to revise previous decisions quite naturally, based on further information. We show that a simple adjustment to the Arc-Eager transition system to relax its monotonicity constraints can improve accuracy, so long as the training data includes examples of mistakes for the nonmonotonic transitions to repair. We evaluate the change in the context of a stateof-the-art system, and obtain a statistically signiﬁcant improvement (p < 0.001) on the English evaluation and 5/10 of the CoNLL languages. 
This paper presents a collapsed variational Bayesian inference algorithm for PCFGs that has the advantages of two dominant Bayesian training algorithms for PCFGs, namely variational Bayesian inference and Markov chain Monte Carlo. In three kinds of experiments, we illustrate that our algorithm achieves close performance to the Hastings sampling algorithm while using an order of magnitude less training time; and outperforms the standard variational Bayesian inference and the EM algorithms with similar training time. 
Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We ﬁnd their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications. 
In this work, we present an approach for multilingual portability of Spoken Language Understanding systems. The goal of this approach is to avoid the effort of acquiring and labeling new corpora to learn models when changing the language. The work presented in this paper is focused on the learning of a speciﬁc translator for the task and the mechanism of transmitting the information among the modules by means of graphs. These graphs represent a set of hypotheses (a language) that is the input to the statistical semantic decoder that provides the meaning of the sentence. Some experiments in a Spanish task evaluated with input French utterances and text are presented. They show the good behavior of the system, mainly when speech input is considered. 
The use of pivot languages and wordalignment techniques over bilingual corpora has proved an effective approach for extracting paraphrases of words and short phrases. However, inherent ambiguities in the pivot language(s) can lead to inadequate paraphrases. We propose a novel approach that is able to extract paraphrases by pivoting through multiple languages while discriminating word senses in the input language, i.e., the language to be paraphrased. Text in the input language is annotated with “senses” in the form of foreign phrases obtained from bilingual parallel data and automatic word-alignment. This approach shows 62% relative improvement over previous work in generating paraphrases that are judged both more accurate and more ﬂuent. 
We propose a ﬂexible and effective framework for extracting a bilingual dictionary from comparable corpora. Our approach is based on a novel combination of topic modeling and word alignment techniques. Intuitively, our approach works by converting a comparable document-aligned corpus into a parallel topic-aligned corpus, then learning word alignments using co-occurrence statistics. This topicaligned corpus is similar in structure to the sentence-aligned corpus frequently used in statistical machine translation, enabling us to exploit advances in word alignment research. Unlike many previous work, our framework does not require any languagespeciﬁc knowledge for initialization. Furthermore, our framework attempts to handle polysemy by allowing multiple translation probability models for each word. On a large-scale Wikipedia corpus, we demonstrate that our framework reliably extracts high-precision translation pairs on a wide variety of comparable data conditions. 
In this paper, we address the problem of identifying relevant product aspects in a collection of online customer reviews. Being able to detect such aspects represents an important subtask of aspect-based review mining systems, which aim at automatically generating structured summaries of customer opinions. We cast the task as a terminology extraction problem and examine the utility of varying term acquisition heuristics, ﬁltering techniques, variant aggregation methods, and relevance measures. We evaluate the different approaches on two distinct datasets (hotel and camera reviews). For the best conﬁguration, we ﬁnd signiﬁcant improvements over a state-of-the-art baseline method. 
The acquisition of Belief verbs lags behind the acquisition of Desire verbs in children. Some psycholinguistic theories attribute this lag to conceptual differences between the two classes, while others suggest that syntactic differences are responsible. Through computational experiments, we show that a probabilistic verb learning model exhibits the pattern of acquisition, even though there is no difference in the model in the difﬁculty of the semantic or syntactic properties of Belief vs. Desire verbs. Our results point to the distributional properties of various verb classes as a potentially important, and heretofore unexplored, factor in the observed developmental lag of Belief verbs. 
This paper describes the process of composing problems that are suitable for competitions in linguistics. The type of problems described is “Rosetta Stone”—a bilingual problem where typically one of the languages is unknown, and the other is the native language of the person solving the problem. The process includes selecting phenomena, composing and arranging the data and assignments in order to illustrate the phenomena, and verifying the solvability and complexity of the problem. 
The paper is focused on self-contained linguistic problems based on text corpora. We argue that corpus-based problems differ from traditional linguistic problems because they make it possible to represent language variation. Furthermore, they often require basic statistical thinking from the students. The practical value of using data obtained from text corpora for teaching linguistics through linguistic problems is shown. 
Linguistics olympiads, now offered in more than 20 countries, provide secondary-school students a compelling introduction to an unfamiliar ﬁeld. The North American Computational Linguistics Olympiad (NACLO) includes computational puzzles in addition to purely linguistic ones. This paper explores the computational subject matter we seek to convey via NACLO, as well as some of the challenges that arise when adapting problems in computational linguistics to an audience that may have no background in computer science, linguistics, or advanced mathematics. We present a small library of reusable design patterns that have proven useful when composing puzzles appropriate for secondary-school students. 
Multilinguality has been an essential feature of the International Linguistic Olympiad since its conception. Although deemed most desirable, the production of a problem set in several parallel versions and the verification of their equivalence is a time-consuming and errorprone task. This paper tells about the efforts to develop tools and methods which increase its efficiency and reliability. 
The Australian Computational and Linguistics Olympiad (OzCLO) started in 2008 in only two locations and has since grown to a nationwide competition with almost 1500 high school students participating in 2013. An Australian team has participated in the International Linguistics Olympiad (ILO) every year since 2009. This paper describes how the competition is run (with a regional First Round and a final National Round) and the organisation of the competition (a National Steering Committee and Local Organising Committees for each region) and discusses the particular challenges faced by Australia (timing of the competition and distance between the major population centres). One major factor in the growth and success of OzCLO has been the introduction of the online competition, allowing participation of students from rural and remote country areas. The organisation relies on the good-will and volunteer work of university and school staff but the strong interest among students and teachers shows that OzCLO is responding to a demand for linguistic challenges. 
What is it that we want to achieve with our Linguistic Olympiads and how do the contests vary in diﬀerent countries? The Swedish Olympiad has been running for 7 years now and is primarily focused on public outreach - spreading linguistics to secondary school students. The contest involves not only a test but also lectures, school visits and teaching material. The eﬀort is put on promoting the interest of linguistics to students through fun material and good contact with teachers of languages. This presentation contains an overview of the Swedish version of Olympiads in Linguistics as well as some concrete examples of workshop material on linguistic problems for secondary school students. 
We present the concept of a correspondence seminar as a way to complement and support one-time contests, especially olympiads. We evaluate speciﬁc payoffs of this way of teaching linguistics, and compare its nature to that of a linguistics olympiad. We believe that the correspondence seminar is a great way to introduce talented high school students to linguistics. 
In this paper we present a choreography that explains the process of supervised machine learning. We present how a perceptron (in its dual form) uses convolution kernels to learn to differentiate between two categories of objects. Convolution kernels such as string kernels and tree kernels are widely used in Natural Language Processing (NLP) applications. However, the baggage associated with learning the theory behind convolution kernels, which extends beyond graduate linear algebra, makes the adoption of this technology intrinsically difﬁcult. The main challenge in creating this choreography was that we were required to represent these mathematical equations at their meaning level before we could translate them into the language of movement. By orchestrating such a choreography, we believe, we have obviated the need for people to posses advanced math background in order to appreciate the core ideas of using convolution kernels in a supervised learning setting. 
Data-driven research in linguistics typically involves the processes of data annotation, data visualization and identification of relevant patterns. We describe our experience in incorporating these processes at an undergraduate course on language information technology. Students collectively annotated the syntactic structures of a set of Classical Chinese poems; the resulting treebank was put on a platform for corpus search and visualization; finally, using this platform, students investigated research questions about the text of the treebank. 
We present in the paper our experience of involving the students of the department of theoretical and computational linguistics of the Moscow State University into full-cycle activities of preparing and evaluating the results of the NLP Evaluation forums, held in 2010 and 2012 in Russia. The forum of 2010 started as a new initiative and was the first independent evaluation of morphology parsers for Russian in Russia. At the same time the forum campaign has been a source of a successful academic course which resulted in a closeknit student team, strong enough to implement the two-year research for the second forum on syntax, held in 2012. The new forum of anaphora (to be held in 2014) is now prepared mostly by students. 
 We present an open-source virtual manipulative for conditional log-linear models. This web-based interactive visualization lets the user tune the probabilities of various shapes—which grow and shrink accordingly—by dragging sliders that correspond to feature weights. The visualization displays a regularized training objective; it supports gradient ascent by optionally displaying gradients on the sliders and providing “Step” and “Solve” buttons. The user can sample parameters and datasets of different sizes and compare their own parameters to the truth. Our website, http://cs.jhu.edu/˜jason/ tutorials/loglin/, guides the user through a series of interactive lessons and provides auxiliary readings, explanations, practice problems and resources. 
In this paper we discuss our experience of teaching basic Natural Language Processing (NLP) and Machine Learning (ML) in an introductory course to Information Science. We discuss the challenges we faced while incorporating NLP and ML to the curriculum followed by a presentation of how we met these challenges. The overall response (of students) to the inclusion of this new topic to the curriculum has been positive. Students this semester are pursuing NLP/ML projects, formulating their own tasks (some of which are novel and presented towards the end of the paper), collecting and annotating data and building models for their task. 
This paper describes a seminar course designed by IBM and Columbia University on the topic of Semantic Technologies, in particular as used in IBM WatsonTM — a large scale Question Answering system which famously won at Jeopardy! R against two human grand champions. It was ﬁrst offered at Columbia University during the 2013 spring semester, and will be offered at other institutions starting in the fall semester. We describe the course’s ﬁrst successful run and its unique features: a class centered around a speciﬁc industrial technology; a large-scale class project which student teams can choose to participate in and which serves as the basis for an open source project that will continue to grow each time the course is offered; publishable papers, demos and start-up ideas; evidence that the course can be self-evaluating, which makes it potentially appropriate for an online setting; and a unique model where a large company trains instructors and contributes to creating educational material at no charge to qualifying institutions. 
We present a new approach to dialogue processing in terms of “meaning units”. In our annotation task, we asked speakers of English and Chinese to mark boundaries where they could construct the maximal concept using minimal words. We compared English data across genres (news, literature, and policy). We analyzed the agreement for annotators using a state-ofthe-art segmentation similarity algorithm and compared annotations with a random baseline. We found that annotators are able to identify meaning units systematically, even though they may disagree on the quantity and position of units. Our analysis includes an examination of phrase structure for annotated units using constituency parses. 
A number of approaches have been taken to improve lexical consistency in Statistical Machine Translation. However, little has been written on the subject of where and when to encourage consistency. I present an analysis of human authored translations, focussing on words belonging to different parts-of-speech across a number of different genres. 
Explicit discourse connectives in a source language text are not always translated to comparable words or phrases in the target language. The paper provides a corpus analysis and a method for semi-automatic detection of such cases. Results show that discourse connectives are not translated into comparable forms (or even any form at all), in up to 18% of human reference translations from English to French or German. In machine translation, this happens much less frequently (up to 8% only). Work in progress aims to capture this natural implicitation of discourse connectives in current statistical machine translation models. 
We present a suggestive ﬁnding regarding the loss of associative texture in the process of machine translation, using comparisons between (a) original and backtranslated texts, (b) reference and system translations, and (c) better and worse MT systems. We represent the amount of association in a text using word association proﬁle – a distribution of pointwise mutual information between all pairs of content word types in a text. We use the average of the distribution, which we term lexical tightness, as a single measure of the amount of association in a text. We show that the lexical tightness of humancomposed texts is higher than that of the machine translated materials; human references are tighter than machine translations, and better MT systems produce lexically tighter translations. While the phenomenon of the loss of associative texture has been theoretically predicted by translation scholars, we present a measure capable of quantifying the extent of this phenomenon. 
The correct translation of verb tenses ensures that the temporal ordering of events in the source text is maintained in the target text. This paper assesses the utility of automatically labeling English Simple Past verbs with a binary discursive feature, narrative vs. non-narrative, for statistical machine translation (SMT) into French. The narrativity feature, which helps deciding which of the French past tenses is a correct translation of the English Simple Past, can be assigned with about 70% accuracy (F1). The narrativity feature improves SMT by about 0.2 BLEU points when a factored SMT system is trained and tested on automatically labeled English-French data. More importantly, manual evaluation shows that verb tense translation and verb choice are improved by respectively 9.7% and 3.4% (absolute), leading to an overall improvement of verb translation of 17% (relative). 
The paper presents machine translation experiments from English to Czech with a large amount of manually annotated discourse connectives. The gold-standard discourse relation annotation leads to better translation performance in ranges of 4–60% for some ambiguous English connectives and helps to ﬁnd correct syntactical constructs in Czech for less ambiguous connectives. Automatic scoring conﬁrms the stability of the newly built discourseaware translation systems. Error analysis and human translation evaluation point to the cases where the annotation was most and where less helpful. 
We present a novel approach to the translation of the English personal pronoun it to Czech. We conduct a linguistic analysis on how the distinct categories of it are usually mapped to their Czech counterparts. Armed with these observations, we design a discriminative translation model of it, which is then integrated into the TectoMT deep syntax MT framework. Features in the model take advantage of rich syntactic annotation TectoMT is based on, external tools for anaphoricity resolution, lexical co-occurrence frequencies measured on a large parallel corpus and gold coreference annotation. Even though the new model for it exhibits no improvement in terms of BLEU, manual evaluation shows that it outperforms the original solution in 8.5% sentences containing it. 
We present an approach to feature weight optimization for document-level decoding. This is an essential task for enabling future development of discourse-level statistical machine translation, as it allows easy integration of discourse features in the decoding process. We extend the framework of sentence-level feature weight optimization to the document-level. We show experimentally that we can get competitive and relatively stable results when using a standard set of features, and that this framework also allows us to optimize documentlevel features, which can be used to model discourse phenomena. 
We present vector space semantic parsing (VSSP), a framework for learning compositional models of vector space semantics. Our framework uses Combinatory Categorial Grammar (CCG) to deﬁne a correspondence between syntactic categories and semantic representations, which are vectors and functions on vectors. The complete correspondence is a direct consequence of minimal assumptions about the semantic representations of basic syntactic categories (e.g., nouns are vectors), and CCG’s tight coupling of syntax and semantics. Furthermore, this correspondence permits nonuniform semantic representations and more expressive composition operations than previous work. VSSP builds a CCG semantic parser respecting this correspondence; this semantic parser parses text into lambda calculus formulas that evaluate to vector space representations. In these formulas, the meanings of words are represented by parameters that can be trained in a task-speciﬁc fashion. We present experiments using noun-verbnoun and adverb-adjective-noun phrases which demonstrate that VSSP can learn composition operations that RNN (Socher et al., 2011) and MV-RNN (Socher et al., 2012) cannot. 
In this paper, we address the problem of how to use semantics to improve syntactic parsing, by using a hybrid reranking method: a k-best list generated by a symbolic parser is reranked based on parsecorrectness scores given by a compositional, connectionist classiﬁer. This classiﬁer uses a recursive neural network to construct vector representations for phrases in a candidate parse tree in order to classify it as syntactically correct or not. Tested on the WSJ23, our method achieved a statistically signiﬁcant improvement of 0.20% on F-score (2% error reduction) and 0.95% on exact match, compared with the state-ofthe-art Berkeley parser. This result shows that vector-based compositional semantics can be usefully applied in syntactic parsing, and demonstrates the beneﬁts of combining the symbolic and connectionist approaches. 
In this paper we present a novel approach (SDSM) that incorporates structure in distributional semantics. SDSM represents meaning as relation speciﬁc distributions over syntactic neighborhoods. We empirically show that the model can effectively represent the semantics of single words and provides signiﬁcant advantages when dealing with phrasal units that involve word composition. In particular, we demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches for composing distributional semantic representations on an artiﬁcial task of verb sense disambiguation and a real-world application of judging event coreference. 
We present a letter-based encoding for words in continuous space language models. We represent the words completely by letter n-grams instead of using the word index. This way, similar words will automatically have a similar representation. With this we hope to better generalize to unknown or rare words and to also capture morphological information. We show their inﬂuence in the task of machine translation using continuous space language models based on restricted Boltzmann machines. We evaluate the translation quality as well as the training time on a German-to-English translation task of TED and university lectures as well as on the news translation task translating from English to German. Using our new approach a gain in BLEU score by up to 0.4 points can be achieved. 
Classiﬁcation and learning algorithms use syntactic structures as proxies between source sentences and feature vectors. In this paper, we explore an alternative path to use syntax in feature spaces: the Distributed Representation “Parsers” (DRP). The core of the idea is straightforward: DRPs directly obtain syntactic feature vectors from sentences without explicitly producing symbolic syntactic interpretations. Results show that DRPs produce feature spaces signiﬁcantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information. 
In recent years, there has been widespread interest in compositional distributional semantic models (cDSMs), that derive meaning representations for phrases from their parts. We present an evaluation of alternative cDSMs under truly comparable conditions. In particular, we extend the idea of Baroni and Zamparelli (2010) and Guevara (2010) to use corpus-extracted examples of the target phrases for parameter estimation to the other models proposed in the literature, so that all models can be tested under the same training conditions. The linguistically motivated functional model of Baroni and Zamparelli (2010) and Coecke et al. (2010) emerges as the winner in all our tests. 
 2 Diagnostic properties of additive decomposition  We introduce a new 50-dimensional embedding obtained by spectral clustering of a graph describing the conceptual structure of the lexicon. We use the embedding directly to investigate sets of antonymic pairs, and indirectly to argue that function application in CVSMs requires not just vectors but two transformations (corresponding to subject and object) as well.  The standard model of lexical decomposition (Katz and Fodor, 1963) divides lexical meaning in a systematic component, given by a tree of (generally binary) features, and an accidental component they call the distinguisher. Figure 1 gives an example. bachelor noun  
This paper presents a comparative study of 5 different types of Word Space Models (WSMs) combined with 4 different compositionality measures applied to the task of automatically determining semantic compositionality of word expressions. Many combinations of WSMs and measures have never been applied to the task before. The study follows Biemann and Giesbrecht (2011) who attempted to ﬁnd a list of expressions for which the compositionality assumption – the meaning of an expression is determined by the meaning of its constituents and their combination – does not hold. Our results are very promising and can be appreciated by those interested in WSMs, compositionality, and/or relevant evaluation methods. 
With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing. In this paper, we address shortcomings in the ability of current models to capture logical operations such as negation. As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models. 
Most distributional models of word similarity represent a word type by a single vector of contextual features, even though, words commonly have more than one sense. The multiple senses can be captured by employing several vectors per word in a multi-prototype distributional model, prototypes that can be obtained by ﬁrst constructing all the context vectors for the word and then clustering similar vectors to create sense vectors. Storing and clustering context vectors can be expensive though. As an alternative, we introduce Multi-Sense Random Indexing, which performs on-the-ﬂy (incremental) clustering. To evaluate the method, a number of measures for word similarity are proposed, both contextual and non-contextual, including new measures based on optimal alignment of word senses. Experimental results on the task of predicting semantic textual similarity do, however, not show a systematic difference between singleprototype and multi-prototype models. 
We present a novel compositional, generative model for vector space representations of meaning. This model reformulates earlier tensor-based approaches to vector space semantics as a top-down process, and provides efﬁcient algorithms for transformation from natural language to vectors and from vectors to natural language. We describe procedures for estimating the parameters of the model from positive examples of similar phrases, and from distributional representations, then use these procedures to obtain similarity judgments for a set of adjective-noun pairs. The model’s estimation of the similarity of these pairs correlates well with human annotations, demonstrating a substantial improvement over several existing compositional approaches in both settings. 
While words in documents are generally treated as discrete entities, they can be embedded in a Euclidean space which reﬂects an a priori notion of similarity between them. In such a case, a text document can be viewed as a bag-ofembedded-words (BoEW): a set of realvalued vectors. We propose a novel document representation based on such continuous word embeddings. It consists in non-linearly mapping the wordembeddings in a higher-dimensional space and in aggregating them into a documentlevel representation. We report retrieval and clustering experiments in the case where the word-embeddings are computed from standard topic models showing signiﬁcant improvements with respect to the original topic models. 
We develop a recursive neural network (RNN) to extract answers to arbitrary natural language questions from supporting sentences, by training on a crowdsourced data set (to be released upon presentation). The RNN deﬁnes feature representations at every node of the parse trees of questions and supporting sentences, when applied recursively, starting with token vectors from a neural probabilistic language model. In contrast to prior work, we ﬁx neither the types of the questions nor the forms of the answers; the system classiﬁes tokens to match a substring chosen by the question’s author. Our classiﬁer decides to follow each parse tree node of a support sentence or not, by classifying its RNN embedding together with those of its siblings and the root node of the question, until reaching the tokens it selects as the answer. A novel co-training task for the RNN, on subtree recognition, boosts performance, along with a scheme to consistently handle words that are not well-represented in the language model. On our data set, we surpass an open source system epitomizing a classic “pattern bootstrapping” approach to question answering. 
The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. The discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers. Without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classiﬁcation experiment. 
This document overviews the strategy, effort and aftermath of the MultiLing 2013 multilingual summarization data collection. We describe how the Data Contributors of MultiLing collected and generated a multilingual multi-document summarization corpus on 10 different languages: Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian and Spanish. We discuss the rationale behind the main decisions of the collection, the methodology used to generate the multilingual corpus, as well as challenges and problems faced per language. This paper overviews the work on Arabic, Chinese, English, Greek, and Romanian languages. A second part, covering the remaining languages, is available as a distinct paper in the MultiLing 2013 proceedings. 
This document overviews the strategy, effort and aftermath of the MultiLing 2013 multilingual summarization data collection. We describe how the Data Contributors of MultiLing collected and generated a multilingual multi-document summarization corpus on 10 different languages: Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian and Spanish. We discuss the rationale behind the main decisions of the collection, the methodology used to generate the multilingual corpus, as well as challenges and problems faced per language. This paper overviews the work on Czech, Hebrew and Spanish languages. 
The MultiLing 2013 Workshop of ACL 2013 posed a multi-lingual, multidocument summarization task to the summarization community, aiming to quantify and measure the performance of multi-lingual, multi-document summarization systems across languages. The task was to create a 240–250 word summary from 10 news articles, describing a given topic. The texts of each topic were provided in 10 languages (Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian, Spanish) and each participant generated summaries for at least 2 languages. The evaluation of the summaries was performed using automatic and manual processes. The participating systems submitted over 15 runs, some providing summaries across all languages. An automatic evaluation task was also added to this year’s set of tasks. The evaluation task meant to determine whether automatic measures of evaluation can function well in the multi-lingual domain. This paper provides a brief description related to the data of both tasks, the evaluation methodology, as well as an overview of participation and corresponding results. 
The 2013 Association for Computational Linguistics MultiLing Pilot posed a task to measure the performance of multilingual, single-document, summarization systems using a dataset derived from many Wikipedias. The objective of the pilot was to assess automatic summarization of multilingual text documents outside the news domain and the potential of using Wikipedia articles for such research. This report describes the pilot task, the dataset, the methods used to evaluate the submitted summaries, and the overall performance of each participant’s system. 
This report provides a description of the methods applied in CIST system participating ACL MultiLing 2013. Summarization is based on sentence extraction. hLDA topic model is adopted for multilingual multi-document modeling. Various features are combined to evaluate and extract candidate summary sentences. 
In this paper we present a linear model for the problem of text summarization, where a summary preserves the information coverage as much as possible in comparison to the original document set. We reduce the problem of ﬁnding the best summary to the problem of ﬁnding the point on a convex polytope closest to the given hyperplane, and solve it efﬁciently with the help of fractional linear programming. We supply here an overview of our system, titled POLY2, that participated in the MultiLing contest at ACL 2013. 
The paper describes our participation in the Multi-document summarization task of Multiling-2013. The community initiative was born as a pilot task for the Text Analysis Conference in 2011. This year the corpus was extended by new three languages and another ﬁve topics, covering in total 15 topics in 10 languages. Our summariser is based on latent semantic analysis and it is in principle language independent. Its results on the Multiling-2011 corpus were promising. The generated summaries were ranked ﬁrst in several languages based on various metrics. The summariser with minor changes was run on the updated 2013 corpus. Although we do not have the manual evaluation results yet the ROUGE-2 score indicates good results again. The summariser produced best summaries in 6 from 10 considered languages according to the ROUGE-2 metric. 
In this paper we present three term weighting approaches for multi-lingual document summarization and give results on the DUC 2002 data as well as on the 2013 Multilingual Wikipedia feature articles data set. We introduce a new intervalbounded nonnegative matrix factorization. We use this new method, latent semantic analysis (LSA), and latent Dirichlet allocation (LDA) to give three term-weighting methods for multi-document multi-lingual summarization. Results on DUC and TAC data, as well as on the MultiLing 2013 data, demonstrate that these methods are very promising, since they achieve oracle coverage scores in the range of humans for 6 of the 10 test languages. Finally, we present three term weighting approaches for the MultiLing13 single document summarization task on the Wikipedia featured articles. Our submissions signiﬁcantly outperformed the baseline in 19 out of 41 languages. 
In this paper we show the results of our participation in the MultiLing 2013 summarisation tasks. We participated with single-document and multi-document corpus-based summarisers for both Arabic and English languages. The summarisers used word frequency lists and log likelihood calculations to generate single and multi document summaries. The single and multi summaries generated by our systems were evaluated by Arabic and English native speaker participants and by different automatic evaluation metrics, ROUGE, AutoSummENG, MeMoG and NPowER. We compare our results to other systems that participated in the same tracks on both Arabic and English languages. Our single-document summarisers performed particularly well in the automatic evaluation with our English singledocument summariser performing better on average than the results of the other participants. Our Arabic multi-document summariser performed well in the human evaluation ranking second. 
This paper describes the architecture of UAIC1’s Summarization system participating at MultiLing – 2013. The architecture includes language independent text processing modules, but also modules that are adapted for one language or another. In our experiments, the languages under consideration are Bulgarian, German, Greek, English, and Romanian. Our method exploits the cohesion and coherence properties of texts to build discourse structures. The output of the parsing process is used to extract general summaries.  obtained as a sequence of discourse clauses extracted from the original text, after obtaining the discourse structure of the text and exploiting the cohesion and coherence properties.  
MUltilingual Sentence Extractor (MUSE) is aimed at multilingual single-document summarization. MUSE implements a supervised language-independent summarization approach based on optimization of multiple sentence ranking methods using a Genetic Algorithm. The main advantage of MUSE is its language-independency – it is using statistical sentence features, which can be calculated for sentences in any language. In our previous work, the performance of MUSE was found to be signiﬁcantly better than the best known state-of-the-art extractive summarization approaches and tools in three different languages: English, Hebrew, and Arabic. Moreover, our experimental results in the cross-lingual domain suggest that MUSE does not need to be retrained on a summarization corpus in each new language, and the same weighting model can be used across several languages (Last and Litvak, 2012). MUSE participated in the MultiLing 2013 single document summarization task on three languages: English, Hebrew and Arabic. Due to a very limited time that was given to the participants to run their systems on the MultiLing 2013 data, the results submitted to evaluation were obtained by summarizing the documents using models pre-trained on different corpora. As such, no training has been performed on the MultiLing 2013 corpus.  
We present three ways of inducing probability distributions on derivation trees produced by Minimalist Grammars, and give their maximum likelihood estimators. We argue that a parameterization based on locally normalized log-linear models balances competing requirements for modeling expressiveness and computational tractability. 
Adjuncts are characteristically optional, but many, such as adverbs and adjectives, are strictly ordered. In Minimalist Grammars (MGs), it is straightforward to account for optionality or ordering, but not both. I present an extension of MGs, MGs with Adjunction, which accounts for optionality and ordering simply by keeping track of two pieces of information at once: the original category of the adjoined-to phrase, and the category of the adjunct most recently adjoined. By imposing a partial order on the categories, the Adjoin operation can require that higher adjuncts precede lower adjuncts, but not vice versa, deriving order. 
We study the complexity of uniform membership for Linear Context-Free Rewriting Systems, i.e., the problem where we are given a string w and a grammar G and are asked whether w ∈ L(G). In particular, we use parameterized complexity theory to investigate how the complexity depends on various parameters. While we focus primarily on rank and fan-out, derivation length is also considered. 
Timelines interpreting interval temporal logic formulas are segmented into strings which serve as semantic representations for tense and aspect. The strings have bounded but reﬁnable granularity, suitable for analyzing (im)perfectivity, durativity, telicity, and various relations including branching. 
This paper develops a compositional vector-based semantics of relative pronouns within a categorical framework. Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modiﬁed noun phrase, as well as copying, combining, and discarding parts of the relative clause. We develop two instantiations of the abstract semantics, one based on a truth-theoretic approach and one based on corpus statistics. 
Attested and ‘pathological’ vowel harmony patterns are studied in the context of subclasses of regular functions. The analysis suggests that the computational complexity of phonology can be reduced from regular to weakly deterministic. 
This paper shows how factored ﬁnitestate representations of subregular language classes are identiﬁable in the limit from positive data by learners which are polytime iterative and optimal. These representations are motivated in two ways. First, the size of this representation for a given regular language can be exponentially smaller than the size of the minimal deterministic acceptor recognizing the language. Second, these representations (including the exponentially smaller ones) describe actual formal languages which successfully model natural language phenomenon, notably in the subﬁeld of phonology. 
We present Minimum Description Length techniques for learning the structure of weighted languages. MDL is already widely used both for segmentation and classiﬁcation tasks, and here we show it can be used to formalize further important tools in the descriptive linguists’ toolbox, including the distinction between accidental and systematic gaps in the data, the detection of ambiguity, the selective discarding of data, and the merging of categories. Introduction The Minimum Description Length (MDL, see Rissanen 1978) framework is primarily about data compression: if we are given some data D, our goal is to ﬁnd a model M, and a correction term E, such that the model output and the correction term together describe the data, and transmitting M and E takes fewer bits than transmitting any competing M and E . From the very beginning, starting with Pa¯n. ini, linguists have put a premium on brevity. The hope is that the shortest theory is the best theory (see Vitanyi and Li 2000), at least if we are willing to posit a theory of Universal Grammar (UG) that will let us specify M brieﬂy, since we can assume UG to be amortized over many languages. In this paper we study the problem of compressing weighted languages by presenting them via weighted ﬁnite state automata (WFSA). The theoretical approach we discuss here has a long history: the founding paper of Kolmogorov complexity, Solomonoff (1964), already studied the problem of inferring a grammar from data, and Gru¨nwald (1996) uses MDL to infer CFGs from corpora, there conceived of as long strings over a ﬁnite alphabet. It is fair to say that this theory has not had much impact on computational practice,  where grammatical inference is dominated by the standard n-gram based language modeling methods, see Jelinek (1997) for an excellent summary of the basic ideas and techniques, most of which are still in wide use. While the two approaches may coincide in certain cases (see Gru¨nwald 1996), and in theory ngram models are just a special case of the general WFSA, in practice they are divided by a fundamental difference in modeling unseen data. From an engineering standpoint, Church et al (2007) are entirely right in saying: No matter how much data we have, we never have enough. Nothing has zero probability. Linguists, starting perhaps with Chomsky (1965), draw a bright line between accidentally and systematically missing data, and would prefer to restrict backoff techniques to the accidental gaps. The distinction is often lost in applied work, because the models need to be built in a noisy environment, where frequent typos like *teh and similar performance errors can easily overwhelm genuine items like boisterous or mopeds by an order of magnitude or even more. In the eyes of many linguists, this observation alone is sufﬁcient to rob probabilistic models of grammatical content, since this makes it impossible to deﬁne a single threshold g such that all and only strings with weight greater than g are grammatical. Aside from this subtle but important distinction between accidental and systematic gaps, both kinds of language modeling can be cast in the same formal terms: we ﬁt a model M that minimizes some function E (typically, the squared sum) of the error E. Obviously, the more parameters M has, the better ﬁt we can obtain. Much of contemporary computational linguistics follows the route of training simple models such  as Hidden Markov Models (HMMs) and probabilistic context-free grammars (PCFGs) with very many parameters, and stops adding more only when compressing the memory footprint is of paramount importance. As (Church et al., 2007) notes, applications like the contextual speller of Microsoft Ofﬁce simply could not ship without keeping the language model within reasonable size limits. In such cases, we are quite willing to trade in E for gains in the size M of M, and considerations of optimizing the sum of the two are simply irrelevant. In contrast, our strategy is to search for model which measures both M and E in bits, and optimizes the sum M + E, not because we put such a premium on data compression, but rather because we follow in Pa¯n. ini’s footsteps. Our goal is ﬁnding structural models capable of distinguishing structurally excluded (ungrammatical) strings like furiously sleep ideas green colorless from low probability but grammatical strings like colorless green ideas sleep furiously (Pereira, 2000). For this more ambitious goal comparing models with different number of parameters is a key issue, and this is precisely where MDL is helpful. The rest of this Introduction provides the basic deﬁnitions, notation, and terminology, all fairly standard except for the use of Moore rather than Mealy machines – the signiﬁcance of this choice will be discussed in Section 2. In Section 1 we bring a fundamental idea of signal processing, quantization error, to bear on the problem of model selection, illustrating the issue on a real example, the proquant system of Hungarian. In Section 2 we show how one of the most powerful tools at disposal of the linguist, ambiguity, can be detected by MDL, bringing another standard idea, signal to noise ratio to bear. In Section 3 we discuss another real example, Hungarian morphotactics, and show that two methods widely (but shamefacedly) used in practice, discarding data and merging descriptive categories, can be used on a principled basis within MDL. Our goal is to show that by consistent application of MDL principles we can automatically set up the kind of models that linguists would set up. Ultimately, both man and machine work toward the same goal, optimization of grammar elegance or, what is the same, brevity. Deﬁnition 1. Given some ﬁnite alphabet Σ, a weighted language p over this alphabet is deﬁned  as a mapping p : Σ∗ → R taking non-negative values such that α∈Σ∗ p(α) = 1. This is less general than the standard notion of noncommutative power series with weights taken in arbitrary semirings (Eilenberg 1974, Salomaa 1978) but will sufﬁce here. The stringset {α|p(α) > 0} is called the support of p and will be denoted by S(p). Deﬁnition 2. Given two weighted languages p and q, we say the Kullback-Leibler (KL) approximation error Q of q relative to p is α∈S(q) p(α) log(p(α)/q(α)). The entropy of p is deﬁned as − α∈S(p) p(α) log(p(α)). Deﬁnition 3. A WFSA M is deﬁned by a square transition matrix M whose element mij give the probability of transition from state i to state j, an emission list h that gives a string hi ∈ Σ∗ for each i = 0, and an acceptance vector a whose i-th component is 1 if i is an accepting state and 0 otherwise. There is a unique initial state which starts the state numbering at 0, and we permit states with empty outputs. Rows of M must sum to 1. Thus we have deﬁned WFSA as normalized probabilityweighted nondeterministic Moore machines. Deﬁnition 4. The weight a WFSA assigns to a generation path is the product of the weights on the edges traversed, and the weight it assigns to a string α is the sum of the weights assigned to all paths that generate α. 
In this paper we investigate the theoretical causes of the disparity between the theoretical and practical running times for the A∗ algorithm proposed in Corlett and Penn (2010) for deciphering letter-substitution ciphers. We argue that the difference seen is due to the relatively low entropies of the probability distributions of character transitions seen in natural language, and we develop a principled way of incorporating entropy into our complexity analysis. Speciﬁcally, we ﬁnd that the low entropy of natural languages can allow us, with high probability, to bound the depth of the heuristic values expanded in the search. This leads to a novel probabilistic bound on search depth in these tasks. 
The consistency method has been established as the standard strategy for extracting high quality translation rules in statistical machine translation (SMT). However, no attention has been drawn to why this method is successful, other than empirical evidence. Using concepts from graph theory, we identify the relation between consistency and components of graphs that represent word-aligned sentence pairs. It can be shown that phrase pairs of interest to SMT form a sigma-algebra generated by components of such graphs. This construction is generalized by allowing segmented sentence pairs, which in turn gives rise to a phrase-based generative model. A by-product of this model is a derivation of probability mass functions for random partitions. These are realized as cases of constrained, biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence. 
In this paper, we introduce a syntax-based sentence simpliﬁer that models simpliﬁcation using a probabilistic synchronous tree substitution grammar (STSG). To improve the STSG model speciﬁcity we utilize a multi-level backoff model with additional syntactic annotations that allow for better discrimination over previous STSG formulations. We compare our approach to T3 (Cohn and Lapata, 2009), a recent STSG implementation, as well as two state-of-the-art phrase-based sentence simpliﬁers on a corpus of aligned sentences from English and Simple English Wikipedia. Our new approach performs signiﬁcantly better than T3, similarly to human simpliﬁcations for both simplicity and ﬂuency, and better than the phrasebased simpliﬁers for most of the evaluation metrics. 
In this paper we report our experiments in creating a parallel corpus using German/Simple German documents from the web. We require parallel data to build a statistical machine translation (SMT) system that translates from German into Simple German. Parallel data for SMT systems needs to be aligned at the sentence level. We applied an existing monolingual sentence alignment algorithm. We show the limits of the algorithm with respect to the language and domain of our data and suggest ways of circumventing them. 
This article addresses the lack of common approaches for text simpliﬁcation evaluation, by presenting the ﬁrst attempt for a common evaluation metrics. The article proposes reading comprehension evaluation as a method for evaluating the results of Text Simpliﬁcation (TS). An experiment, as an example application of the evaluation method, as well as three formulae to quantify reading comprehension, are presented. The formulae produce an unique score, the C-score, which gives an estimation of user’s reading comprehension of a certain text. The score can be used to evaluate the performance of a text simpliﬁcation engine on pairs of complex and simpliﬁed texts, or to compare the performances of different TS methods using the same texts. The approach can be particularly useful for the modern crowdsourcing approaches, such as those employing the Amazon’s Mechanical Turk1 or CrowdFlower2. The aim of this paper is thus to propose an evaluation approach and to motivate the TS community to start a relevant discussion, in order to come up with a common evaluation metrics for this task. 
In this paper, we introduce a new baseline for language-independent text difﬁculty assessment applied to the Interagency Language Roundtable (ILR) proﬁciency scale. We demonstrate that reading level assessment is a discriminative problem that is best-suited for regression. Our baseline uses z-normalized shallow length features and TF-LOG weighted vectors on bag-of-words for Arabic, Dari, English, and Pashto. We compare Support Vector Machines and the Margin-Infused Relaxed Algorithm measured by mean squared error. We provide an analysis of which features are most predictive of a given level. 
The paper discusses the main issues regarding the reading skills and comprehension proﬁciency in written Bulgarian of people with communication difﬁculties, and deaf people, in particular. We consider several key components of text comprehension which pose a challenge for deaf readers and propose a rule-based system for automatic modiﬁcation of Bulgarian texts intended to facilitate comprehension by deaf people, to assist education, etc. In order to demonstrate the beneﬁts of such a system and to evaluate its performance, we have carried out a study among a group of deaf people who use Bulgarian Sign Language (BulSL) as their primary language (primary BulSL users), which compares the comprehensibility of original texts and their modiﬁed versions. The results shows a considerable improvement in readability when using modiﬁed texts, but at the same time demonstrates that the level of comprehension is still low, and that a complex set of modiﬁcations will have to be implemented to attain satisfactory results. 
Comma placements in Chinese text are relatively arbitrary although there are some syntactic guidelines for them. In this research, we attempt to improve the readability of text by optimizing comma placements through integration of linguistic features of text and gaze features of readers. We design a comma predictor for general Chinese text based on conditional random ﬁeld models with linguistic features. After that, we build a rule-based ﬁlter for categorizing commas in text according to their contribution to readability based on the analysis of gazes of people reading text with and without commas. The experimental results show that our predictor reproduces the comma distribution in the Penn Chinese Treebank with 78.41 in F1-score and commas chosen by our ﬁlter smoothen certain gaze behaviors. 
An increasing range of features is being used for automatic readability classiﬁcation. The impact of the features typically is evaluated using reference corpora containing graded reading material. But how do the readability models and the features they are based on perform on real-world web texts? In this paper, we want to take a step towards understanding this aspect on the basis of a broad range of lexical and syntactic features and several web datasets we collected. Applying our models to web search results, we ﬁnd that the average reading level of the retrieved web documents is relatively high. At the same time, documents at a wide range of reading levels are identiﬁed and even among the Top-10 search results one ﬁnds documents at the lower levels, supporting the potential usefulness of readability ranking for the web. Finally, we report on generalization experiments showing that the features we used generalize well across different web sources.  time, the question which features generalize to different types of documents and whether the readability models are appropriate for real-life applications has only received little attention. Against this backdrop, we want to see how well a state-of-the-art readability assessment approach using a broad range of features performs when applied to web data. Based on the approach introduced in Vajjala and Meurers (2012), we thus set out to explore the following two questions in this paper: • Which reading levels can be identiﬁed in a systematic sample of web texts? • How well do the features used generalize to different web sources? The paper is organized as follows: Section 2 surveys related work. Section 3 introduces the corpus and the features we used. Section 4 describes our readability models. Section 5 discusses our experiments investigating the applicability of these models to web texts. Section 6 reports on a second set of experiments conducted to test the generalizability of the features used. Section 7 concludes the paper with a discussion of our results.  
The task of identifying complex words (CWs) is important for lexical simpliﬁcation, however it is often carried out with no evaluation of success. There is no basis for comparison of current techniques and, prior to this work, there has been no standard corpus or evaluation technique for the CW identiﬁcation task. This paper addresses these shortcomings with a new corpus for evaluating a system’s performance in identifying CWs. Simple Wikipedia edit histories were mined for instances of single word lexical simpliﬁcations. The corpus contains 731 sentences, each with one annotated CW. This paper describes the method used to produce the CW corpus and presents the results of evaluation, showing its validity. 
In this paper we report the results of a pilot study of basing readability prediction on training data annotated with reading time. Although reading time is known to be a good metric for predicting readability, previous work has mainly focused on annotating the training data with subjective readability scores usually on a 1 to 5 scale. Instead of the subjective assessments of complexity, we use the more objective measure of reading time. We create and evaluate a predictor using the binary classiﬁcation problem; the predictor identiﬁes the better of two documents correctly with 68.55% accuracy. We also report a comparison of predictors based on reading time and on readability scores. 
A current increasing trend in machine translation is to combine data-driven and rule-based techniques. Such combinations typically involve the hybridization of different paradigms such as, for instance, the introduction of linguistic knowledge into statistical paradigms, the incorporation of data-driven components into rulebased paradigms, or the pre- and postprocessing of either sort of translation system outputs. Aiming at bringing together researchers and practitioners from the different multidisciplinary areas working in these directions, as well as at creating a brainstorming and discussion venue for Hybrid Translation approaches, the HyTra initiative was born. This paper gives an overview of the Second Workshop on Hybrid Approaches to Translation (HyTra 2013) concerning its motivation, contents and outcomes. 
We explore the selection of training data for language models using perplexity. We introduce three novel models that make use of linguistic information and evaluate them on three different corpora and two languages. In four out of the six scenarios a linguistically motivated method outperforms the purely statistical state-of-theart approach. Finally, a method which combines surface forms and the linguistically motivated methods outperforms the baseline in all the scenarios, selecting data whose perplexity is between 3.49% and 8.17% (depending on the corpus and language) lower than that of the baseline. 
We have implemented a rule-based prototype of a Spanish-to-Cuzco Quechua MT system enhanced through the addition of statistical components. The greatest difﬁculty during the translation process is to generate the correct Quechua verb form in subordinated clauses. The prototype has several rules that decide which verb form should be used in a given context. However, matching the context in order to apply the correct rule depends crucially on the parsing quality of the Spanish input. As the form of the subordinated verb depends heavily on the conjunction in the subordinated Spanish clause and the semantics of the main verb, we extracted this information from two treebanks and trained different classiﬁers on this data. We tested the best classiﬁer on a set of 4 texts, increasing the correct subordinated verb forms from 80% to 89%. 
Dependency parsers are almost ubiquitously evaluated on their accuracy scores, these scores say nothing of the complexity and usefulness of the resulting structures. The structures may have more complexity due to their coordination structure or attachment rules. As dependency parses are basic structures in which other systems are built upon, it would seem more reasonable to judge these parsers down the NLP pipeline. We show results from 7 individual parsers, including dependency and constituent parsers, and 3 ensemble parsing techniques with their overall effect on a Machine Translation system, Treex, for English to Czech translation. We show that parsers’ UAS scores are more correlated to the NIST evaluation metric than to the BLEU Metric, however we see increases in both metrics. 
Chinese and Japanese have a different sentence structure. Reordering methods are effective, but need reliable parsers to extract the syntactic structure of the source sentences. However, Chinese has a loose word order, and Chinese parsers that extract the phrase structure do not perform well. We propose a framework where only POS tags and unlabeled dependency parse trees are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. We show signiﬁcant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods. 
Reordering is pre-processing stage for Statistical Machine Translation (SMT) system where the words of the source sentence are reordered as per the syntax of the target language. We are proposing a rich set of rules for better reordering. The idea is to facilitate the training process by better alignments and parallel phrase extraction for a phrase based SMT system. Reordering also helps the decoding process and hence improving the machine translation quality. We have observed significant improvements in the translation quality by using our approach over the baseline SMT. We have used BLEU, NIST, multi-reference word error rate, multi-reference position independent error rate for judging the improvements. We have exploited open source SMT toolkit MOSES to develop the system. 
Phrase-based statistical machine translation systems can generate translations of reasonable quality in the case of language pairs with similar structure and word order. However, if the languages are more distant from a grammatical point of view, the quality of translations is much behind the expectations, since the baseline translation system cannot cope with long distance reordering of words and the mapping of word internal grammatical structures. In our paper, we present a method that tries to overcome these problems in the case of English-Hungarian translation by applying reordering rules prior to the translation process and by creating morpheme-based and factored models. Although automatic evaluation scores do not reliably reﬂect the improvement in all cases, human evaluation of our systems shows that readability and accuracy of the translations were improved both by reordering and applying richer models. 
We explore the intersection of rule-based and statistical approaches in machine translation, with a particular focus on past and current work here at Microsoft Research. Until about ten years ago, the only machine translation systems worth using were rule-based and linguistically-informed. Along came statistical approaches, which use large corpora to directly guide translations toward expressions people would actually say. Rather than making local decisions when writing and conditioning rules, goodness of translation was modeled numerically and free parameters were selected to optimize that goodness. This led to huge improvements in translation quality as more and more data was consumed. By necessity, the pendulum is swinging towards the inclusion of linguistic features in MT systems. We describe some of our statistical and non-statistical attempts to incorporate linguistic insights into machine translation systems, showing what is currently working well, and what isn’t. We also look at trade-offs in using linguistic knowledge (“rules”) in pre- or post-processing by language pair, with a particular eye on the return on investment as training data increases in size. 
We present a minimalist, unsupervised learning model that induces relatively clean phrasal inversion transduction grammars by employing the minimum description length principle to drive search over a space defined by two opposing extreme types of ITGs. In comparison to most current SMT approaches, the model learns a very parsimonious phrase translation lexicons that provide an obvious basis for generalization to abstract translation schemas. To do this, the model maintains internal consistency by avoiding use of mismatched or unrelated models, such as word alignments or probabilities from IBM models. The model introduces a novel strategy for avoiding the pitfalls of premature pruning in chunking approaches, by incrementally splitting an ITG while using a second ITG to guide this search. 
This paper presents a hybrid approach to the enhancement of English to Arabic statistical machine translation quality. Machine Translation has been defined as the process that utilizes computer software to translate text from one natural language to another. Arabic, as a morphologically rich language, is a highly flexional language, in that the same root can lead to various forms according to its context. Statistical machine translation (SMT) engines often show poor syntax processing especially when the language used is morphologically rich such as Arabic. In this paper, to overcome these shortcomings, we describe our hybrid approach which integrates knowledge of the Arabic language into statistical machine translation. In this framework, we propose the use of a featured language model SFLM (Smaïli et al., 2004) to be able to integrate syntactic and grammatical knowledge about each word. In this paper, we first discuss some challenges in translating from English to Arabic and we explore various techniques to improve performance on this task. We apply a morphological segmentation step for Arabic words and we present our hybrid approach by identifying morpho-syntactic class of each segmented word to build up our statistical feature language model. We propose the scheme for recombining the segmented Arabic word, and describe their effect on translation. 
This paper presents the methods which are based on the part-of-speech (POS) and auto alignment information to improve the quality of machine translation result and the word alignment. We utilize different types of POS tag to restructure source sentences and use an alignment-based reordering method to improve the alignment. After applying the reordering method, we use two phrase tables in the decoding part to keep the translation performance. Our experiments on Korean-Chinese show that our methods can improve the alignment and translation results. Since the proposed approach reduces the size of the phrase table, multi-tables are considered. The combination of all these methods together would get the best translation result. 
Since the Tunisian revolution, Tunisian Dialect (TD) used in daily life, has became progressively used and represented in interviews, news and debate programs instead of Modern Standard Arabic (MSA). This situation has important negative consequences for natural language processing (NLP): since the spoken dialects are not officially written and do not have standard orthography, it is very costly to obtain adequate corpora to use for training NLP tools. Furthermore, there are almost no parallel corpora involving TD and MSA. In this paper, we describe the creation of Tunisian dialect text corpus as well as a method for building a bilingual dictionary, in order to create language model for speech recognition system for the Tunisian Broadcast News. So, we use explicit knowledge about the relation between TD and MSA. 
This paper proposes a hybrid word alignment model for Phrase-Based Statistical Machine translation (PB-SMT). The proposed hybrid alignment model provides most informative alignment links which are offered by both unsupervised and semi-supervised word alignment models. Two unsupervised word alignment models (GIZA++ and Berkeley aligner) and a rule based aligner are combined together. The rule based aligner only aligns named entities (NEs) and chunks. The NEs are aligned through transliteration using a joint source-channel model. Chunks are aligned employing a bootstrapping approach by translating the source chunks into the target language using a baseline PB-SMT system and subsequently validating the target chunks using a fuzzy matching technique against the target corpus. All the experiments are carried out after single-tokenizing the multi-word NEs. Our best system provided significant improvements over the baseline as measured by BLEU. 
We present initial work on an inexpensive approach for building largevocabulary lexical selection modules for hybrid RBMT systems by framing lexical selection as a sequence labeling problem. We submit that Maximum Entropy Markov Models (MEMMs) are a sensible formalism for this problem, due to their ability to take into account many features of the source text, and show how we can build a combination MEMM/HMM system that allows MT system implementors ﬂexibility regarding which words have their lexical choices modeled with classiﬁers. We present initial results showing successful use of this system both in translating English to Spanish and Spanish to Guarani. 
In the context of a hybrid French-toEnglish SMT system for translating online forum posts, we present two methods for addressing the common problem of homophone confusions in colloquial written language. The ﬁrst is based on hand-coded rules; the second on weighted graphs derived from a large-scale pronunciation resource, with weights trained from a small bicorpus of domain language. With automatic evaluation, the weighted graph method yields an improvement of about +0.63 BLEU points, while the rulebased method scores about the same as the baseline. On contrastive manual evaluation, both methods give highly signiﬁcant improvements (p < 0.0001) and score about equally when compared against each other. 
Resource limitation is challenging for crossdomain adaption. This paper employs patterns identified from a monolingual in-domain corpus and patterns learned from the post-edited translation results, and translation model as well as language model learned from pseudo bilingual corpora produced by a baseline MT system. The adaptation from a government document domain to a medical record domain shows the rules mined from the monolingual in-domain corpus are useful, and the effect of using the selected pseudo bilingual corpus is significant. 
The present article provides a comprehensive review of the work carried out on developing PRESEMT, a hybrid language-independent machine translation (MT) methodology. This methodology has been designed to facilitate rapid creation of MT systems for unconstrained language pairs, setting the lowest possible requirements on specialised resources and tools. Given the limited availability of resources for many languages, only a very small bilingual corpus is required, while language modelling is performed by sampling a large target language (TL) monolingual corpus. The article summarises implementation decisions, using the Greek-English language pair as a test case. Evaluation results are reported, for both objective and subjective metrics. Finally, main error sources are identified and directions are described to improve this hybrid MT methodology. 
Cultural heritage collections usually organise sets of items into exhibitions or guided tours. These items are often accompanied by text that describes the theme and topic of the exhibition and provides background context and details of connections with other items. The PATHS project brings the idea of guided tours to digital library collections where a tool to create virtual paths are used to assist with navigation and provide guides on particular subjects and topics. In this paper we characterise and analyse paths of items created by users of our online system. The analysis highlights that most users spend time selecting items relevant to their chosen topic, but few users took time to add background information to the paths. In order to address this, we conducted preliminary investigations to test whether Wikipedia can be used to automatically add background text for sequences of items. In the future we would like to explore the automatic creation of full paths. 
Language transformation can be deﬁned as translating between diachronically distinct language variants. We investigate the transformation of Middle Dutch into Modern Dutch by means of machine translation. We demonstrate that by using character overlap the performance of the machine translation process can be improved for this task. 
Differences between large-scale historical population archives and small decentralized databases can be used to improve data quality and record connectedness in both types of databases. A parser is developed to account for differences in syntax and data representation models. A matching procedure is described to discover records from different databases referring to the same historical event. The problem of veriﬁcation without reliable benchmark data is addressed by matching on a subset of record attributes and measuring support for the match using a different subset of attributes. An application of the matching procedure for comparison of family trees is discussed. A visualization tool is described to present an interactive overview of comparison results. 
Cross-period (diachronic) thesaurus construction aims to enable potential users to search for modern terms and obtain semantically related terms from earlier periods in history. This is a complex task not previously addressed computationally. In this paper we introduce a semi-automatic iterative Query Expansion (QE) scheme for supporting cross-period thesaurus construction. We demonstrate the empirical beneﬁt of our scheme for a Jewish crossperiod thesaurus and evaluate its impact on recall and on the effectiveness of lexicographer manual effort. 
We present an extension of the DUALIST tool that enables social scientists to engage directly with large Twitter datasets. Our approach supports collaborative construction of classiﬁers and associated gold standard data sets. The tool can be used to build classiﬁer cascades that decomposes tweet streams, and provide analysis of targeted conversations. A central concern is to provide an environment in which social science researchers can rapidly develop an informed sense of what the datasets look like. The intent is that they develop, not only an informed view as to how the data could be fruitfully analysed, but also how feasible it is to analyse it in that way. 
In our paper, we present a computational morphology for Old and Middle Hungarian used in two research projects that aim at creating morphologically annotated corpora of Old and Middle Hungarian. In addition, we present the web-based disambiguation tool used in the semi-automatic disambiguation of the annotations and the structured corpus query tool that has a unique but very useful feature of making corrections to the annotation in the query results possible. 
In this paper we describe an application of language technology to policy formulation, where it can support policy makers assess the acceptance of a yet-unpublished policy before the policy enters public consultation. One of the key concepts is that instead of relying on thematic similarity, we extract arguments expressed in support or opposition of positions that are general statements that are, themselves, consistent with the policy or not. The focus of this paper in this overall pipeline, is identifying arguments in text: we present and empirically evaluate the hypothesis that verbal tense and mood are good indicators of arguments that have not been explored in the relevant literature. 
We develop a pipeline consisting of various text processing tools which is designed to assist political scientists in ﬁnding speciﬁc, complex concepts within large amounts of text. Our main focus is the interaction between the political scientists and the natural language processing groups to ensure a beneﬁcial assistance for the political scientists and new application challenges for NLP. It is of particular importance to ﬁnd a “common language” between the different disciplines. Therefore, we use an interactive web-interface which is easily usable by non-experts. It interfaces an active learning algorithm which is complemented by the NLP pipeline to provide a rich feature selection. Political scientists are thus enabled to use their own intuitions to ﬁnd custom concepts. 
Manually assigned keywords provide a valuable means for accessing large document collections. They can serve as a shallow document summary and enable more efﬁcient retrieval and aggregation of information. In this paper we investigate keywords in the context of the Dutch Folktale Database, a large collection of stories including fairy tales, jokes and urban legends. We carry out a quantitative and qualitative analysis of the keywords in the collection. Up to 80% of the assigned keywords (or a minor variation) appear in the text itself. Human annotators show moderate to substantial agreement in their judgment of keywords. Finally, we evaluate a learning to rank approach to extract and rank keyword candidates. We conclude that this is a promising approach to automate this time intensive task. 
We propose to bring together two kinds of linguistic resources—interlinear glossed text (IGT) and a language-independent precision grammar resource—to automatically create precision grammars in the context of language documentation. This paper takes the ﬁrst steps in that direction by extracting major-constituent word order and case system properties from IGT for a diverse sample of languages. 
In this paper, we argue that comparable collections of historical written resources can help overcoming typical challenges posed by heritage texts enhancing spelling normalization, POS-tagging and subsequent diachronic linguistic analyses. Thus, we present a comparable corpus of historical German recipes and show how such a comparable text collection together with the application of innovative MT inspired strategies allow us (i) to address the word form normalization problem and (ii) to automatically generate a diachronic dictionary of spelling variants. Such a diachronic dictionary can be used both for spelling normalization and for extracting new ”translation” (word formation/change) rules for diachronic spelling variants. Moreover, our approach can be applied virtually to any diachronic collection of texts regardless of the time span they represent. A ﬁrst evaluation shows that our approach compares well with state-of-art approaches. 
We present current work dealing with the integration of a multilingual thesaurus for social sciences in a NLP framework for supporting Knowledge-Driven Information Extraction in the field of social sciences. We describe the various steps that lead to a running IE system: lexicalization of the labels of the thesaurus and semi-automatic generation of domain specific IE grammars, with their subsequent implementation in a finite state engine. Finally, we outline the actual field of application of the IE system: analysis of social media for recognition of relevant topics in the context of elections. 
Applying machine translation (MT) to literary texts involves the same domain shift challenges that arise for any sublanguage (e.g. medical or scientiﬁc). However, it also introduces additional challenges. One focus in the discussion of translation theory in the humanities has been on the human translator’s role in staying faithful to an original text versus adapting it to make it more familiar to readers. In contrast to other domains, one objective in literary translation is to preserve the experience of reading a text when moving to the target language. We use existing MT systems to translate samples of French literature into English. We then use qualitative analysis grounded in translation theory and real example outputs in order to address what makes literary translation particularly hard and the potential role of the machine in it. 
In this paper we look at a task at border of natural language processing, historical linguistics and the study of language development, namely that of identifying the time when a text was written. We use machine learning classiﬁcation using lexical, word ending and dictionary-based features, with linear support vector machines and random forests. We ﬁnd that lexical features are the most helpful. 
As the amount of cultural data available on the Semantic Web is expanding, the demand of accessing this data in multiple languages is increasing. Previous work on multilingual access to cultural heritage information has shown that at least two different problems must be dealt with when mapping from ontologies to natural language: (1) mapping multilingual metadata to interoperable knowledge sources; (2) assigning multilingual knowledge to cultural data. This paper presents our effort to deal with these problems. We describe our experiences with processing museum data extracted from two distinct sources, harmonizing this data and making its content accessible in natural language. We extend prior work in two ways. First, we present a grammar-based system that is designed to generate coherent texts from Semantic Web ontologies in 15 languages. Second, we describe how this multilingual system is exploited to form queries using the standard query language SPARQL. The generation and retrieval system builds on W3C standards and is available for further research. 
Cross-linguistic studies on unsupervised word segmentation have consistently shown that English is easier to segment than other languages. In this paper, we propose an explanation of this ﬁnding based on the notion of segmentation ambiguity. We show that English has a very low segmentation ambiguity compared to Japanese and that this difference correlates with the segmentation performance in a unigram model. We suggest that segmentation ambiguity is linked to a trade-off between syllable structure complexity and word length distribution. 
Computational work in the past decade has produced several models accounting for phonetic category learning from distributional and lexical cues. However, there have been no computational proposals for how people might use another powerful learning mechanism: generalization from learned to analogous distinctions (e.g., from /b/–/p/ to /g/–/k/). Here, we present a new simple model of generalization in phonetic category learning, formalized in a hierarchical Bayesian framework. The model captures our proposal that linguistic knowledge includes the possibility that category types in a language (such as voiced and voiceless) can be shared across sound classes (such as labial and velar), thus naturally leading to generalization. We present two sets of simulations that reproduce key features of human performance in behavioral experiments, and we discuss the model’s implications and directions for future research. 
Recent work in computational psycholinguistics shows that morpheme lexica can be acquired in an unsupervised manner from a corpus of words by selecting the lexicon that best balances productivity and reuse (e.g. Goldwater et al. (2009) and others). In this paper, we extend such work to the problem of acquiring non-concatenative morphology, proposing a simple model of morphology that can handle both concatenative and non-concatenative morphology and applying Bayesian inference on two datasets of Arabic and English verbs to acquire lexica. We show that our approach successfully extracts the non-contiguous triliteral root from Arabic verb stems. 
We use a set of enriched n-gram models to track grammaticality judgements for different sorts of passive sentences in English. We construct these models by specifying scoring functions to map the log probabilities (logprobs) of an n-gram model for a test set of sentences onto scores which depend on properties of the string related to the parameters of the model. We test our models on classiﬁcation tasks for different kinds of passive sentences. Our experiments indicate that our n-gram models achieve high accuracy in identifying ill-formed passives in which ill-formedness depends on local relations within the n-gram frame, but they are far less successful in detecting non-local relations that produce unacceptability in other types of passive construction. We take these results to indicate some of the strengths and the limitations of word and lexical class n-gram models as candidate representations of speakers’ grammatical knowledge. 
Reading experiments using naturalistic stimuli have shown unanticipated facilitations for completing center embeddings when frequency eﬀects are factored out. To eliminate possible confounds due to surface structure, this paper introduces a processing model based on deep syntactic dependencies. Results on eye-tracking data indicate that completing deep syntactic embeddings yields signiﬁcantly more facilitation than completing surface embeddings. 
There are few computational models of second language acquisition (SLA). At the same time, many questions in the ﬁeld of SLA remain unanswered. In particular, SLA patterns are difﬁcult to study due to the large amount of variation between human learners. We present a computational model of second language construction learning that allows manipulating speciﬁc parameters such as age of onset and amount of exposure. We use the model to study general developmental patterns of SLA and two speciﬁc effects sometimes found in empirical studies: construction priming and a facilitatory effect of skewed frequencies in the input. Our simulations replicate the expected SLA patterns as well as the two effects. Our model can be used in further studies of various SLA phenomena. 
We augment an existing TAG-based incremental syntactic formalism, PLTAG, with a semantic component designed to support the simultaneous modeling effects of thematic ﬁt as well as syntactic and semantic predictions. PLTAG is a psycholinguistically-motivated formalism which extends the standard TAG operations with a prediction and veriﬁcation mechanism and has experimental support as a model of syntactic processing difﬁculty. We focus on the problem of formally modelling semantic role prediction in the context of an incremental parse and describe a ﬂexible neo-Davidsonian formalism and composition procedure to accompany a PLTAG parse. To this end, we also provide a means of augmenting the PLTAG lexicon with semantic annotation. To illustrate this, we run through an experimentally-relevant model case, wherein the resolution of semantic role ambiguities inﬂuences the resolution of syntactic ambiguities and vice versa. 
This paper summarizes the results of a large-scale evaluation study of bag-ofwords distributional models on behavioral data from three semantic priming experiments. The tasks at issue are (i) identiﬁcation of consistent primes based on their semantic relatedness to the target and (ii) correlation of semantic relatedness with latency times. We also provide an evaluation of the impact of speciﬁc model parameters on the prediction of priming. To the best of our knowledge, this is the ﬁrst systematic evaluation of a wide range of DSM parameters in all possible combinations. An important result of the study is that neighbor rank performs better than distance measures in predicting semantic priming. 
An increasing body of empirical evidence suggests that concreteness is a fundamental dimension of semantic representation. By implementing both a vector space model and a Latent Dirichlet Allocation (LDA) Model, we explore the extent to which concreteness is reflected in the distributional patterns in corpora. In one experiment, we show that that vector space models can be tailored to better model semantic domains of particular degrees of concreteness. In a second experiment, we show that the quality of the representations of abstract words in LDA models can be improved by supplementing the training data with information on the physical properties of concrete concepts. We conclude by discussing the implications for computational systems and also for how concrete and abstract concepts are represented in the mind 
Discourse connectives play an important role in making a text coherent and helping humans to infer relations between spans of text. Using the Penn Discourse Treebank, we investigate what information relevant to inferring discourse relations is conveyed by discourse connectives, and whether the speciﬁcity of discourse relations reﬂects general cognitive biases for establishing coherence. We also propose an approach to measure the eﬀect of a discourse marker on sense identiﬁcation according to the diﬀerent levels of a relation sense hierarchy. This will open a way to the computational modeling of discourse processing. 
We describe a method for learning an incremental semantic grammar from data in which utterances are paired with logical forms representing their meaning. Working in an inherently incremental framework, Dynamic Syntax, we show how words can be associated with probabilistic procedures for the incremental projection of meaning, providing a grammar which can be used directly in incremental probabilistic parsing and generation. We test this on child-directed utterances from the CHILDES corpus, and show that it results in good coverage and semantic accuracy, without requiring annotation at the word level or any independent notion of syntax. 
We propose a data-driven approach to enhance translation extraction from comparable corpora. Instead of resorting to an external dictionary, we translate source vector features by using a cross-lingual Word Sense Disambiguation method. The candidate senses for a feature correspond to sense clusters of its translations in a parallel corpus and the context used for disambiguation consists of the vector that contains the feature. The translations found in the disambiguation output convey the sense of the features in the source vector, while the use of translation clusters permits to expand their translation with several variants. As a consequence, the translated vectors are less noisy and richer, and allow for the extraction of higher quality lexicons compared to simpler methods. 
This paper presents a simple and effective method for automatic bilingual lexicon extraction from less-known language pairs. To do this, we bring in a bridge language named the pivot language and adopt information retrieval techniques combined with natural language processing techniques. Moreover, we use a freely available word aligner: Anymalign (Lardilleux et al., 2011) for constructing context vectors. Unlike the previous works, we obtain context vectors via a pivot language. Therefore, we do not require to translate context vectors by using a seed dictionary and improve the accuracy of low frequency word alignments that is weakness of statistical model by using Anymalign. In this paper, experiments have been conducted on two different language pairs that are bi-directional KoreanSpanish and Korean-French, respectively. The experimental results have demonstrated that our method for high-frequency words shows at least 76.3 and up to 87.2% and for the lowfrequency words at least 43.3% and up to 48.9% within the top 20 ranking candidates, respectively. 
This paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora. We study of the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors. For this purpose, we augment the standard approach by a Word Sense Disambiguation process relying on a WordNet-based semantic similarity measure. The aim of this process is to identify the translations that are more likely to give the best representation of words in the target language. On two specialized French-English comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach. 
Smoothing is a central issue in language modeling and a prior step in different natural language processing (NLP) tasks. However, less attention has been given to it for bilingual lexicon extraction from comparable corpora. If a ﬁrst work to improve the extraction of low frequency words showed signiﬁcant improvement while using distance-based averaging (Pekar et al., 2006), no investigation of the many smoothing techniques has been carried out so far. In this paper, we present a study of some widelyused smoothing algorithms for language n-gram modeling (Laplace, Good-Turing, Kneser-Ney...). Our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach (Fung, 1998) traditionally used for bilingual lexicon extraction. We show that using smoothing as a preprocessing step of the standard approach increases its performance signiﬁcantly. 
Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinese–Japanese. Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. We extract Chinese–Japanese parallel sentences from quasi–comparable corpora, which are available in far larger quantities. The task is signiﬁcantly more difﬁcult than the extraction from noisy parallel or comparable corpora. We extend a previous study that treats parallel sentence identiﬁcation as a binary classiﬁcation problem. Previous method of classiﬁer training by the Cartesian product is not practical, because it differs from the real process of parallel sentence extraction. We propose a novel classiﬁer training method that simulates the real sentence extraction process. Furthermore, we use linguistic knowledge of Chinese character features. Experimental results on quasi– comparable corpora indicate that our proposed approach performs signiﬁcantly better than the previous study. 
This paper discusses a modular and opensource focused crawler (ILSP-FC) for the automatic acquisition of domain-specific monolingual and bilingual corpora from the Web. Besides describing the main modules integrated in the crawler (dealing with page fetching, normalization, cleaning, text classification, de-duplication and document pair detection), we evaluate several of the system functionalities in an experiment for the acquisition of pairs of parallel documents in German and Italian for the "Health & Safety at work" domain. 
The paper explores the options for building bilingual dictionaries by automated methods. We deﬁne the notion ‘basic vocabulary’ and investigate how well the conceptual units that make up this language-independent vocabulary are covered by language-speciﬁc bindings in 40 languages. Introduction Globalization increasingly brings languages in contact. At the time of the pioneering IBM work on the Hansard corpus (Brown et al., 1990), only two decades ago, there was no need for a BasqueChinese dictionary, but today there is (Saralegi et al., 2012). While the methods for building dictionaries from parallel corpora are now mature (Melamed, 2000), there is a dearth of bilingual or even monolingual material (Zse´der et al., 2012), hence the increased interest in comparable corpora. Once we ﬁnd bilingual speakers capable of carrying out a manual evaluation of representative samples, it is relatively easy to measure the precision of a dictionary built by automatic methods. But measuring recall remains a challenge, for if there existed a high quality machine-readable dictionary (MRD) to measure against, building a new one would largely be pointless, except perhaps as a means of engineering around copyright restrictions. We could measure recall against Wiktionary, but of course this is a moving target, and more importantly, the coverage across language pairs is extremely uneven. What we need is a standardized vocabulary resource that is equally applicable to all language pairs. In this paper we describe our work toward creating such a resource by extending the 4lang conceptual dictionary (Kornai and Makrai, 2013)  to the top 40 languages (by Wikipedia size) using a variety of methods. Since some of the resources studied here are not available for the initial list of 40 languages, we extended the original list to 50 languages so as to guarantee at least 40 languages for every method. Throughout the paper, results are provided for all 50 languages, indicating missing data as needed. Section 1 outlines the approach taken toward deﬁning the basic vocabulary and translational equivalence. Section 2 describes how Wiktionary itself measures up against the 4lang resource directly and after triangulation across language pairs. Section 2.3 and Section 2.4 deals with extraction from multiply parallel and near-parallel corpora, and Section 3 offers some conclusions. 
We present a study on linguistic contrast and commonality in English scientiﬁc discourse on the basis of a monolingually comparable corpus. The focus is on selected scientiﬁc disciplines at the boundaries to computer science (computational linguistics, bioinformatics, digital construction, microelectronics). The data basis is the English Scientiﬁc Text Corpus (SCITEX) which covers a time range of roughly thirty years (1970/80s to early 2000s). In particular, we investigate the disciplinary diversiﬁcation/relatedness of scientiﬁc research articles in terms of register. Our results are relevant for research on multilingually comparable corpora as used in machine translation and related research, since they shed new light on the notion of ‘comparablity’. 
In this article, we present an automated approach of extracting English-Bengali parallel fragments of text from comparable corpora created using Wikipedia documents. Our approach exploits the multilingualism of Wikipedia. The most important fact is that this approach does not need any domain specific corpus. We have been able to improve the BLEU score of an existing domain specific EnglishBengali machine translation system by 11.14%. 
This paper presents a comparable translation corpus created to investigate translation variation phenomena in terms of contrasts between languages, text types and translation methods (machine vs. computer-aided vs. human). These phenomena are reﬂected in linguistic features of translated texts belonging to different registers and produced with different translation methods. For their analysis, we combine methods derived from translation studies, language variation and machine translation, concentrating especially on textual and lexico-grammatical variation. To our knowledge, none of the existing corpora can provide comparable resources for a comprehensive analysis of variation across text types and translation methods. Therefore, the corpus resources created, as well as our analysis results will ﬁnd application in different research areas, such as translation studies, machine translation, and others. 
Tools and techniques that automate the interpretation of multilingual corpora are useful on many fronts; scholars, as an example, could use such tools to more readily pinpoint relevant articles from journals in a wide variety of languages. This work describes techniques to build and characterize ontologies using collaborative knowledge bases, e.g., Wikipedia. These ontologies can then be used to search and classify texts. Originally developed for monolingual corpora, we extend the approach to multilingual texts and test the methods with Mandarin scientific abstracts. The presented techniques provide a novel and efficient mechanism to obtain contextually rich ontologies and measure document relevancy within multilingual corpora. 
We present a novel method to recognise semantic equivalents of biomedical terms in language pairs. We hypothesise that biomedical term are formed by semantically similar textual units across languages. Based on this hypothesis, we employ a Random Forest (RF) classiﬁer that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples. We apply our method on two language pairs: one that uses the same character set and another with a different script, English-French and EnglishChinese, respectively. We show that English-French pairs of terms are highly transliterated in contrast to the EnglishChinese pairs. Nonetheless, our method performs robustly on both cases. We evaluate RF against a state-of-the-art alignment method, GIZA++, and we report a statistically signiﬁcant improvement. Finally, we compare RF against Support Vector Machines and analyse our results. 
Multilingual sentiment analysis attracts increased attention as the massive growth of multilingual web contents. This conducts to study opinions across different languages by comparing the underlying messages written by different people having different opinions. In this paper, we propose Sentiment based Comparability Measures (SCM) to compare opinions in multilingual comparable articles without translating source/target into the same language. This will allow media trackers (journalists) to automatically detect public opinion split across huge multilingual web contents. To develop SCM, we need either to get or to build parallel sentiment corpora. Because this kind of corpora are not available, we decided to build them. For that, we propose a new method to automatically label parallel corpora with sentiment classes. Then we use the extracted parallel sentiment corpora to develop multilingual sentiment analysis system. Experimental results show that, the proposed measure can capture differences in terms of opinions. The results also show that comparable articles variate in their objectivity and positivity. 
Previous attempts in extracting parallel data from Wikipedia were restricted by the monotonicity constraint of the alignment algorithm used for matching possible candidates. This paper proposes a method for exploiting Wikipedia articles without worrying about the position of the sentences in the text. The algorithm ranks the candidate sentence pairs by means of a customized metric, which combines different similarity criteria. Moreover, we limit the search space to a speciﬁc topical domain, since our ﬁnal goal is to use the extracted data in a domain-speciﬁc Statistical Machine Translation (SMT) setting. The precision estimates show that the extracted sentence pairs are clearly semantically equivalent. The SMT experiments, however, show that the extracted data is not reﬁned enough to improve a strong in-domain SMT system. Nevertheless, it is good enough to boost the performance of an out-of-domain system trained on sizable amounts of data. 
 We present a new and unique paraphrase resource, which contains meaningpreserving transformations between informal user-generated text. Sentential paraphrases are extracted from a comparable corpus of temporally and topically related messages on Twitter which often express semantically identical information through distinct surface forms. We demonstrate the utility of this new resource on the task of paraphrasing and normalizing noisy text, showing improvement over several state-of-the-art paraphrase and normalization systems 1.  
Focusing on a systematic Latent Semantic Analysis (LSA) and Machine Learning (ML) approach, this research contributes to the development of a methodology for the automatic compilation of comparable collections of documents. Its originality lies within the delineation of relevant comparability characteristics of similar documents in line with an established definition of comparable corpora. These innovative characteristics are used to build a LSA vector-based representation of the texts. In accordance with this new reduced in dimensionality document space, an unsupervised machine learning algorithm gathers similar texts into comparable clusters. On a monolingual collection of less than 100 documents, the proposed approach assigns comparable documents to different comparable corpora with high confidence.  rable documents that are closely related to each other on the basis of a strict definition of comparable corpora. The proposed approach incorporates originally a Latent Semantic Analysis technique in order to match similar concepts instead of words thus contributing to better automatic learning of comparability between documents. 2 Comparable Corpora Definition Maia (2003) discusses the characteristics of comparable corpora. Nevertheless, the adopted definition of comparable corpora in this study is given by McEnery (2003): “Comparable corpora are corpora where series of monolingual corpora are collected for a range of languages, preferably using the same sampling and frame and with similar balance and representativeness, to enable the study of those languages in contrast.”  
This paper presents an efﬁcient approach to ﬁnding more bilingual webpage pairs with high credibility via link analysis, using little prior knowledge or heuristics. It extends from a previous algorithm that takes the number of bilingual URL pairs that a key (i.e., a URL pairing pattern) can match as the objective function to search for the best set of keys yielding the greatest number of webpage pairs within targeted bilingual websites. Enhanced algorithms are proposed to match more bilingual webpages following the credibility based on statistical analysis of the link relationship of the seed websites available. With about 12,800 seed websites as test set, the enhanced algorithms improve precision over baseline by more than 5%, from 94.06% to 99.40%, and hence ﬁnd above 20% more true bilingual URL pairs, illustrating that signiﬁcantly more bilingual webpages with high credibility can be mined with the help of the link analysis. 
Despite many methods that effectively solve sentiment classiﬁcation task for such widely used languages as English, there is no clear answer which methods are the most suitable for the languages that are substantially different. In this paper we attempt to solve Internet comments sentiment classiﬁcation task for Lithuanian, using two classiﬁcation approaches – knowledge-based and supervised machine learning. We explore an inﬂuence of sentiment word dictionaries based on the different parts-of-speech (adjectives, adverbs, nouns, and verbs) for knowledge-based method; different feature types (bag-ofwords, lemmas, word n-grams, character n-grams) for machine learning methods; and pre-processing techniques (emoticons replacement with sentiment words, diacritics replacement, etc.) for both approaches. Despite that supervised machine learning methods (Support Vector Machine and Na¨ıve Bayes Multinomial) signiﬁcantly outperform proposed knowledge-based method all obtained results are above baseline. The best accuracy 0.679 was achieved with Na¨ıve Bayes Multinomial and token unigrams plus bigrams, when pre-processing involved diacritics replacement. 
In this paper we describe our experience in conducting the ﬁrst open sentiment analysis evaluations in Russian in 2011-2012. These initiatives took part within Russian Information Retrieval Seminar (ROMIP), which is an annual TREC-like competition in Russian. Several test and train collections were created for such tasks as sentiment classiﬁcation in blogs and newswire, opinion retrieval. The paper describes the state of the art in sentiment analysis in Russian, collection characteristics, track tasks and evaluation metrics. 
Aspect-oriented opinion mining aims to identify product aspects (features of products) about which opinion has been expressed in the text. We present an approach for aspect-oriented opinion mining from user reviews in Croatian. We propose methods for acquiring a domain-speciﬁc opinion lexicon, linking opinion clues to product aspects, and predicting polarity and rating of reviews. We show that a supervised approach to linking opinion clues to aspects is feasible, and that the extracted clues and aspects improve polarity and rating predictions. 
Frequently asked questions (FAQ) are an efﬁcient way of communicating domainspeciﬁc information to the users. Unlike general purpose retrieval engines, FAQ retrieval engines have to address the lexical gap between the query and the usually short answer. In this paper we describe the design and evaluation of a FAQ retrieval engine for Croatian. We frame the task as a binary classiﬁcation problem, and train a model to classify each FAQ as either relevant or not relevant for a given query. We use a variety of semantic textual similarity features, including term overlap and vector space features. We train and evaluate on a FAQ test collection built speciﬁcally for this purpose. Our best-performing model reaches 0.47 of mean reciprocal rank, i.e., on average ranks the relevant answer among the top two returned answers. 
 S  We present an approach for natural language parsing in which dependency and constituency parses are acquired simultaneously. This leads to accurate parses represented in a speciﬁc way, richer than constituency or dependency tree. It also allows reducing parsing time complexity. Within the proposed approach, we show how to treat some signiﬁcant phenomena of the Russian language and also perform a brief evaluation of the parser implementation, known as DictaScope Syntax. 
We describe GPKEX, a keyphrase extraction method based on genetic programming. We represent keyphrase scoring measures as syntax trees and evolve them to produce rankings for keyphrase candidates extracted from text. We apply and evaluate GPKEX on Croatian newspaper articles. We show that GPKEX can evolve simple and interpretable keyphrase scoring measures that perform comparably to more complex machine learning methods previously developed for Croatian. 
We investigate state-of-the-art statistical models for lemmatization and morphosyntactic tagging of Croatian and Serbian. The models stem from a new manually annotated SETIMES.HR corpus of Croatian, based on the SETimes parallel corpus. We train models on Croatian text and evaluate them on samples of Croatian and Serbian from the SETimes corpus and the two Wikipedias. Lemmatization accuracy for the two languages reaches 97.87% and 96.30%, while full morphosyntactic tagging accuracy using a 600-tag tagset peaks at 87.72% and 85.56%, respectively. Part of speech tagging accuracies reach 97.13% and 96.46%. Results indicate that more complex methods of Croatian-toSerbian annotation projection are not required on such dataset sizes for these particular tasks. The SETIMES.HR corpus, its resulting models and test sets are all made freely available. 
We propose a language-independent word normalization method exempliﬁed on modernizing historical Slovene words. Our method relies on character-based statistical machine translation and uses only shallow knowledge. We present the relevant lexicons and two experiments. In one, we use a lexicon of historical word– contemporary word pairs and a list of contemporary words; in the other, we only use a list of historical words and one of contemporary ones. We show that both methods produce signiﬁcantly better results than the baseline. 
The present paper introduces approach to improve English-Russian sentence alignment, based on POS-tagging of automatically aligned (by HunAlign) source and target texts. The initial hypothesis is tested on a corpus of bitexts. Sequences of POS tags for each sentence (exactly, nouns, adjectives, verbs and pronouns) are processed as “words” and DamerauLevenshtein distance between them is computed. This distance is then normalized by the length of the target sentence and is used as a threshold between supposedly mis-aligned and “good” sentence pairs. The experimental results show precision 0.81 and recall 0.8, which allows the method to be used as additional data source in parallel corpora alignment. At the same time, this leaves space for further improvement. 
In this paper we present a corpus-based approach to automatic identiﬁcation of false friends for Slovene and Croatian, a pair of closely related languages. By taking advantage of the lexical overlap between the two languages, we focus on measuring the difference in meaning between identically spelled words by using frequency and distributional information. We analyze the impact of corpora of different origin and size together with different association and similarity measures and compare them to a simple frequency-based baseline. With the best performing setting we obtain very good average precision of 0.973 and 0.883 on different gold standards. The presented approach works on non-parallel datasets, is knowledge-lean and language-independent, which makes it attractive for natural language processing tasks that often lack the lexical resources and cannot afford to build them by hand. 
The task of Named Entity Recognition (NER) is to identify in text predeﬁned units of information such as person names, organizations and locations. In this work, we address the problem of NER in Estonian using supervised learning approach. We explore common issues related to building a NER system such as the usage of language-agnostic and languagespeciﬁc features, the representation of named entity tags, the required corpus size and the need for linguistic tools. For system training and evaluation purposes, we create a gold standard NER corpus. On this corpus, our CRF-based system achieves an overall F1-score of 87%. 
This paper reports on some experiments aiming at tuning a rule-based NER system designed for detecting names in Polish online news to the processing of targeted Twitter streams. In particular, one explores whether the performance of the baseline NER system can be improved through the incremental application of knowledge-poor methods for name matching and guessing. We study various settings and combinations of the methods and present evaluation results on ﬁve corpora gathered from Twitter, centred around major events and known individuals. 
In the paper we discuss the problem of low recall for the named entity (NE) recognition task for Polish. We discuss to what extent the recall of NE recognition can be improved by reducing the space of NE categories. We also present several extensions to the binary model which give an improvement of the recall. The extensions include: new features, application of external knowledge and post-processing. For the partial evaluation the ﬁnal model obtained 90.02% recall with 91.30% precision on the corpus of economic news. 
This paper describes a plug-in component to extend the PULS information extraction framework to analyze Russian-language text. PULS is a comprehensive framework for information extraction (IE) that is used for analysis of news in several scenarios from English-language text and is primarily monolingual. Although monolinguality is recognized as a serious limitation, building an IE system for a new language from the bottom up is very labor-intensive. Thus, the objective of the present work is to explore whether the base framework can be extended to cover additional languages with limited effort, and to leverage the preexisting PULS modules as far as possible, in order to accelerate the development process. The component for Russian analysis is described and its performance is evaluated on two news-analysis scenarios: epidemic surveillance and cross-border security. The approach described in the paper can be generalized to a range of heavilyinﬂected languages. 
In this paper we present a semi-automatic approach for acqusition of lexico-syntactic knowledge for event extraction in two Slavic languages, namely Bulgarian and Czech. The method uses several weaklysupervised and unsupervised algorithms, based on distributional semantics. Moreover, an intervention from a language expert is envisaged on different steps in the learning procedure, which increases its accuracy, with respect to unsupervised methods for lexical and grammar learning. 
We propose a method for cross-language identiﬁcation of semantic relations based on word similarity measurement and morphosemantic relations in WordNet. We transfer these relations to pairs of derivationally unrelated words and train a model for automatic classiﬁcation of new instances of (morpho)semantic relations in context based on the existing ones and the general semantic classes of collocated verb and noun senses. Our experiments are based on Bulgarian-English parallel and comparable texts but the method is to a great extent language-independent and particularly suited to less-resourced languages, since it does not need parsed or semantically annotated data. The application of the method leads to an increase in the number of discovered semantic relations by 58.35% and performs relatively consistently, with a small decrease in precision between the baseline (based on morphosemantic relations identiﬁed in wordnet) – 0.774, and the extended method (based on the data obtained through machine learning) – 0.721. 
For languages with complex morphologies, limited resources and tools, and/or lack of standard grammars, developing annotated resources can be a challenging task. Annotated resources developed under time/money constraints for such languages tend to tradeoff depth of representation with degree of noise. We present two methods for automatic correction and extension of morphological annotations, and demonstrate their success on three divergent Egyptian Arabic corpora. 
This paper presents a method for part-ofspeech tagging of historical data and evaluates it on texts from different corpora of historical German (15th–18th century). Spelling normalization is used to preprocess the texts before applying a POS tagger trained on modern German corpora. Using only 250 manually normalized tokens as training data, the tagging accuracy of a manuscript from the 15th century can be raised from 28.65% to 74.89%. 
The recent success of statistical parsing methods has made treebanks become important resources for building good parsers. However, constructing highquality annotated treebanks is a challenging task. We utilized two publicly available parsers, Berkeley and MST parsers, for feedback on improving the quality of part-of-speech tagging for the Vietnamese Treebank. Analysis of the treebank and parsing errors revealed how problems with the Vietnamese Treebank inﬂuenced the parsing results and real difﬁculties of Vietnamese parsing that required further improvements to existing parsing technologies. 
When creating a new resource, preprocessing the source texts before annotation is both ubiquitous and obvious. How the preprocessing affects the annotation effort for various tasks is for the most part an open question, however. In this paper, we study the effects of preprocessing on the annotation of dependency corpora and how annotation speed varies as a function of the quality of three different parsers and compare with the speed obtained when starting from a least-processed baseline. We also present preliminary results concerning the effects on agreement based on a small subset of sentences that have been doubly-annotated.1 
We explore the use of continuous rating scales for human evaluation in the context of machine translation evaluation, comparing two assessor-intrinsic qualitycontrol techniques that do not rely on agreement with expert judgments. Experiments employing Amazon’s Mechanical Turk service show that quality-control techniques made possible by the use of the continuous scale show dramatic improvements to intra-annotator agreement of up to +0.101 in the kappa coefﬁcient, with inter-annotator agreement increasing by up to +0.144 when additional standardization of scores is applied. 
Hierarchical or nested annotation of linguistic data often co-exists with simpler non-hierarchical or ﬂat counterparts, a classic example being that of annotations used for parsing and chunking. In this work, we propose a general strategy for comparing across these two schemes of annotation using the concept of entailment that formalizes a correspondence between them. We use crowdsourcing to obtain query and sentence chunking and show that entailment can not only be used as an effective evaluation metric to assess the quality of annotations, but it can also be employed to ﬁlter out noisy annotations. 
We introduce a framework for lightweight dependency syntax annotation. Our formalism builds upon the typical representation for unlabeled dependencies, permitting a simple notation and annotation workﬂow. Moreover, the formalism encourages annotators to underspecify parts of the syntax if doing so would streamline the annotation process. We demonstrate the eﬃcacy of this annotation on three languages and develop algorithms to evaluate and compare underspeciﬁed annotations. 
The paper addresses the challenge of converting MIDT, an existing dependency– based Italian treebank resulting from the harmonization and merging of smaller resources, into the Stanford Dependencies annotation formalism, with the ﬁnal aim of constructing a standard–compliant resource for the Italian language. Achieved results include a methodology for converting treebank annotations belonging to the same dependency–based family, the Italian Stanford Dependency Treebank (ISDT), and an Italian localization of the Stanford Dependency scheme. 
Discourse relation may entail sentiment information. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Finally, we highlight some language phenomena and give some remarks. 
Unstructured Information Management Architecture (UIMA) has been gaining popularity in annotating text corpora. The architecture deﬁnes common data structures and interfaces to support interoperability of individual processing components working together in a UIMA application. The components exchange data by sharing common type systems—schemata of data type structures—which extend a generic, top-level type system built into UIMA. This ﬂexibility in extending type systems has resulted in the development of repositories of components that share one or several type systems; however, components coming from different repositories, and thus not sharing type systems, remain incompatible. Commonly, this problem has been solved programmatically by implementing UIMA components that perform the alignment of two type systems, an arduous task that is impractical with a growing number of type systems. We alleviate this problem by introducing a conversion mechanism based on SPARQL, a query language for the data retrieval and manipulation of RDF graphs. We provide a UIMA component that serialises data coming from a source component into RDF, executes a user-deﬁned, typeconversion query, and deserialises the updated graph into a target component. The proposed solution encourages ad hoc conversions, enables the usage of heterogeneous components, and facilitates highly customised UIMA applications. 
This paper describes the importation of Manually Annotated Sub-Corpus (MASC) data and annotations into the linguistic database ANNIS, which allows users to visualize and query linguistically-annotated corpora. We outline the process of mapping MASC’s GrAF representation to ANNIS’s internal format relANNIS and demonstrate how the system provides access to multiple annotation layers in the corpus. This access provides information about inter-layer relations and dependencies that have been previously difﬁcult to explore, and which are highly valuable for continued development of language processing applications. 
This paper discusses the problem of annotating coreference relations with generic expressions in a large scale corpus. We present and analyze some existing theories of genericity, compare them to the approaches to generics that are used in the state-of-the-art coreference annotation guidelines and discuss how coreference of generic expressions is processed in the manual annotation of the Prague Dependency Treebank. After analyzing some typical problematic issues we propose some partial solutions that can be used to enhance the quality and consistency of the annotation. 
Anaphoric shell nouns such as this issue and this fact conceptually encapsulate complex pieces of information (Schmid, 2000). We examine the feasibility of annotating such anaphoric nouns using crowdsourcing. In particular, we present our methodology for reliably annotating antecedents of such anaphoric nouns and the challenges we faced in doing so. We also evaluated the quality of crowd annotation using experts. The results suggest that most of the crowd annotations were good enough to use as training data for resolving such anaphoric nouns. 
Various discourse theories have argued for data structures ranging from the simplest trees to the most complex chain graphs. This paper investigates the structure represented by the explicit connectives annotated in the multiplegenre Turkish Discourse Bank (TDB). The dependencies that violate tree-constraints are analyzed. The effects of information structure in the surface form, which result in seemingly complex configurations with underlying simple dependencies, are introduced; and the structural implications are discussed. The results indicate that our current approach to local discourse structure needs to accommodate properly contained arguments and relations, and partially overlapping as well as shared arguments; deviating further from simple trees, but not as drastically as a chain graph structure would imply, since no genuine cases of structural crossing dependencies are attested in TDB. 
In this paper, we present an annotation tool developed speciﬁcally for manual sentiment analysis of social media posts. The tool provides facilities for general and target based opinion marking on different type of posts (i.e. comparative, ironic, conditional) with a web based UI which supports synchronous annotation. It is also designed as a SaaS (Software as a Service). The tool’s outstanding features are easy and fast annotation interface, detailed sentiment levels, multi-client support, easy to manage administrative modules and linguistic annotation capabilities. 
This paper presents the DATOOL, a graphical tool for annotating conversations consisting of short messages (i.e., tweets), and the results we obtain in using it to annotate tweets for Darija, an historically unwritten Arabic dialect spoken by millions but not taught in schools and lacking standardization and linguistic resources. With the DATOOL, a native-Darija speaker annotated hundreds of mixedlanguage and mixed-script conversations at approximately 250 tweets per hour. The resulting corpus was used in developing and evaluating Arabic dialect classiﬁers described brieﬂy herein. The DATOOL supports downstream discourse analysis of tweeted “conversations” by mapping extracted relations such as, who tweets to whom in which language, into graph markup formats for analysis in network visualization tools. 
We describe a new annotation scheme for formalizing relation structures in research papers. The scheme has been developed through the investigation of computer science papers. Using the scheme, we are building a Japanese corpus to help develop information extraction systems for digital libraries. We report on the outline of the annotation scheme and on annotation experiments conducted on research abstracts from the IPSJ Journal. 
Semantically annotated corpora play an important role in natural language processing. This paper presents the results of a pilot study on building a sense-tagged parallel corpus, part of ongoing construction of aligned corpora for four languages (English, Chinese, Japanese, and Indonesian) in four domains (story, essay, news, and tourism) from the NTU-Multilingual Corpus. Each subcorpus is ﬁrst sensetagged using a wordnet and then these synsets are linked. Upon the completion of this project, all annotated corpora will be made freely available. The multilingual corpora are designed to not only provide data for NLP tasks like machine translation, but also to contribute to the study of translation shift and bilingual lexicography as well as the improvement of monolingual wordnets. 
In this paper, we discuss our efforts to annotate nominals in the Hindi Treebank with the semantic property of animacy. Although the treebank already encodes lexical information at a number of levels such as morph and part of speech, the addition of animacy information seems promising given its relevance to varied linguistic phenomena. The suggestion is based on the theoretical and computational analysis of the property of animacy in the context of anaphora resolution, syntactic parsing, verb classiﬁcation and argument differentiation. 
Automatic pre-annotation is often used to improve human annotation speed and accuracy. We address here out-of-domain named entity annotation, and examine whether automatic pre-annotation is still beneﬁcial in this setting. Our study design includes two different corpora, three pre-annotation schemes linked to two annotation levels, both expert and novice annotators, a questionnaire-based subjective assessment and a corpus-based quantitative assessment. We observe that preannotation helps in all cases, both for speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual beneﬁts measured in the annotation outcome. 
We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it. 
This paper presents a case study of a difﬁcult and important categorical annotation task (word sense) to demonstrate a probabilistic annotation model applied to crowdsourced data. It is argued that standard (chance-adjusted) agreement levels are neither necessary nor sufﬁcient to ensure high quality gold standard labels. Compared to conventional agreement measures, application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost. 
We investigate methods for evaluating agreement among a relatively large group of annotators who have not received extensive training and differ in terms of ability and motivation. We show that it is possible to isolate a reliable subgroup of annotators, so that aspects of the difﬁculty of the underlying task can be studied. Our task is to annotate the argumentative structure of short texts. 
Crowdsourcing, while ideally reducing both costs and the need for domain experts, is no all-purpose tool. We review how paraphrase recognition has beneﬁted from crowdsourcing in the past and identify two problems in paraphrase acquisition and semantic similarity evaluation that can be solved by employing a smart crowdsourcing strategy. First, we employ the CrowdFlower platform to conduct an experiment on sub-sentential paraphrase acquisition with early exclusion of lowaccuracy crowdworkers. Second, we compare two human intelligence task designs for evaluating phrase pairs on a semantic similarity scale. While the ﬁrst experiment conﬁrms our strategy successful at tackling the problem of missing gold in paraphrase generation, the results of the second experiment suggest that, for both semantic similarity evaluation on a continuous and a binary scale, querying crowdworkers for a semantic similarity value on a multi-grade scale yields better results than directly asking for a binary classiﬁcation. 
This paper presents an analysis of an annotator’s behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotator’s behaviour during annotation can be seen as reﬂecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal useful information for NLP tasks, in particular semantic and discourse processing that require deeper language understanding. We conducted an experiment for collecting annotator actions and eye gaze during the annotation of predicate-argument relations in Japanese texts. Our analysis of the collected data suggests that obtained insight into human annotation behaviour is useful for exploring effective linguistic features in machine learning-based approaches. 
In this paper we present the development of a corpus of French newswire texts annotated with enunciative and modal commitment information. The annotation scheme we propose is based on the detection of predicative cues referring to an enunciative and/or modal variation - and their scope at a sentence level. We describe how we have improved our annotation guideline by using the evaluation (in terms of precision, recall and F-Measure) of a first round of annotation produced by two expert annotators and by our automatic annotation system. 
We discuss in this paper a proposal to integrate the annotation of contexts with focussensitive expressions (namely the Portuguese exclusive adverb só ‘only’) in a modality scheme. We describe some properties of contexts involving both exclusive particles and modal triggers and discuss how to integrate this with an existing annotation scheme implemented for European Portuguese. We present the results of the application of this annotation scheme to a sample of 100 sentences. 
This paper describes LIMSI’s submissions to the shared WMT’13 translation task. We report results for French-English, German-English and Spanish-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams, and continuous space models in a post-processing step. The main novelties of this year’s participation are the following: our ﬁrst participation to the Spanish-English task; experiments with source pre-ordering; a tighter integration of continuous space language models using artiﬁcial text generation (for German); and the use of different tuning sets according to the original language of the text to be translated. 
We use feature decay algorithms (FDA) for fast deployment of accurate statistical machine translation systems taking only about half a day for each translation direction. We develop parallel FDA for solving computational scalability problems caused by the abundance of training data for SMT models and LM models and still achieve SMT performance that is on par with using all of the training data or better. Parallel FDA runs separate FDA models on randomized subsets of the training data and combines the instance selections later. Parallel FDA can also be used for selecting the LM corpus based on the training set selected by parallel FDA. The high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing SMT systems. The relevancy of the selected LM corpus can reach up to 86% reduction in the number of OOV tokens and up to 74% reduction in the perplexity. We perform SMT experiments in all language pairs in the WMT13 translation task and obtain SMT performance close to the top systems using signiﬁcantly less resources for training and development. 
This paper describes our WMT submissions CU-BOJAR and CU-DEPFIX, the latter dubbed “CHIMERA” because it combines on three diverse approaches: TectoMT, a system with transfer at the deep syntactic level of representation, factored phrase-based translation using Moses, and ﬁnally automatic rule-based correction of frequent grammatical and meaning errors. We do not use any off-the-shelf systemcombination method. 
This paper describes the English-Russian and Russian-English statistical machine translation (SMT) systems developed at Yandex School of Data Analysis for the shared translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation. We adopted phrase-based SMT approach and evaluated a number of different techniques, including data ﬁltering, spelling correction, alignment of lemmatized word forms and transliteration. Altogether they yielded +2.0 and +1.5 BLEU improvement for ru-en and enru language pairs. We also report on the experiments that did not have any positive effect and provide an analysis of the problems we encountered during the development of our systems. 
This paper describes the phrase-based SMT systems developed for our participation in the WMT13 Shared Translation Task. Translations for English↔German and English↔French were generated using a phrase-based translation system which is extended by additional models such as bilingual, ﬁne-grained part-ofspeech (POS) and automatic cluster language models and discriminative word lexica (DWL). In addition, we combined reordering models on different sentence abstraction levels. 
This paper describes TU¨ B˙ITAK-B˙ILGEM statistical machine translation (SMT) systems submitted to the Eighth Workshop on Statistical Machine Translation (WMT) shared translation task for German-English language pair in both directions. We implement phrase-based SMT systems with standard parameters. We present the results of using a big tuning data and the effect of averaging tuning weights of different seeds. Additionally, we performed a linguistically motivated compound splitting in the Germanto-English SMT system. 
We validated various novel and recently proposed methods for statistical machine translation on 10 language pairs, using large data resources. We saw gains from optimizing parameters, training with sparse features, the operation sequence model, and domain adaptation techniques. We also report on utilizing a huge language model trained on 126 billion tokens. The annual machine translation evaluation campaign for European languages organized around the ACL Workshop on Statistical Machine Translation offers the opportunity to test recent advancements in machine translation in large data condition across several diverse language pairs. Building on our own developments and external contributions to the Moses open source toolkit, we carried out extensive experiments that, by early indications, led to a strong showing in the evaluation campaign. We would like to stress especially two contributions: the use of the new operation sequence model (Section 3) within Moses, and — in a separate unconstraint track submission — the use of a huge language model trained on 126 billion tokens with a new training tool (Section 4). 
This paper describes Munich-EdinburghStuttgart’s submissions to the Eighth Workshop on Statistical Machine Translation. We report results of the translation tasks from German, Spanish, Czech and Russian into English and from English to German, Spanish, Czech, French and Russian. The systems described in this paper use OSM (Operation Sequence Model). We explain different pre-/post-processing steps that we carried out for different language pairs. For German-English we used constituent parsing for reordering and compound splitting as preprocessing steps. For Russian-English we transliterated the unknown words. The transliteration system is learned with the help of an unsupervised transliteration mining algorithm. 
We present the system we developed to provide efﬁcient large-scale feature-rich discriminative training for machine translation. We describe how we integrate with MapReduce using Hadoop streaming to allow arbitrarily scaling the tuning set and utilizing a sparse feature set. We report our ﬁndings on German-English and RussianEnglish translation, and discuss beneﬁts, as well as obstacles, to tuning on larger development sets drawn from the parallel training data. 
This paper describes the TALP participation in the WMT13 evaluation campaign. Our participation is based on the combination of several statistical machine translation systems: based on standard phrasebased Moses systems. Variations include techniques such as morphology generation, training sentence ﬁltering, and domain adaptation through unit derivation. The results show a coherent improvement on TER, METEOR, NIST, and BLEU scores when compared to our baseline system. 
We describe the LIA machine translation systems for the Russian-English and English-Russian translation tasks. Various factored translation systems were built using MOSES to take into account the morphological complexity of Russian and we experimented with the romanization of untranslated Russian words. 
This paper describes OmniﬂuentTM Translate – a state-of-the-art hybrid MT system capable of high-quality, high-speed translations of text and speech. The system participated in the English-to-French and Russian-to-English WMT evaluation tasks with competitive results. The features which contributed the most to high translation quality were training data sub-sampling methods, document-speciﬁc models, as well as rule-based morphological normalization for Russian. The latter improved the baseline Russian-to-English BLEU score from 30.1 to 31.3% on a heldout test set. 
We propose a pre-reordering scheme to improve the quality of machine translation by permuting the words of a source sentence to a target-like order. This is accomplished as a transition-based system that walks on the dependency parse tree of the sentence and emits words in target-like order, driven by a classiﬁer trained on a parallel corpus. Our system is capable of generating arbitrary permutations up to ﬂexible constraints determined by the choice of the classiﬁer algorithm and input features. 
We present the syntax-based string-totree statistical machine translation systems built for the WMT 2013 shared translation task. Systems were developed for four language pairs. We report on adapting parameters, targeted reduction of the tuning set, and post-evaluation experiments on rule binarization and preventing dropping of verbs. 
 This paper describes shallow  semantically-informed  Hierarchical  Phrase-based SMT (HPBSMT) and  Phrase-Based SMT (PBSMT) systems  developed at Dublin City University  for participation in the translation task  between EN-ES and ES-EN at the Work-  shop on Statistical Machine Translation  (WMT 13). The system uses PBSMT  and HPBSMT decoders with multiple  LMs, but will run only one decoding  path decided before starting translation.  Therefore the paper does not present a  multi-engine system combination. We  investigate three types of shallow seman-  tics: (i) Quality Estimation (QE) score,  (ii) genre ID, and (iii) context ID derived  from context-dependent language models.  Our results show that the improvement is  0.8 points absolute (BLEU) for EN-ES  and 0.7 points for ES-EN compared to  the standard PBSMT system (single best  system). It is important to note that we  developed this method when the standard  (confusion network-based) system com-  bination is ineffective such as in the case  when the input is only two.  
This paper describes the joint submission of the QUAERO project for the German→English translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013). The submission was a system combination of the output of four different translation systems provided by RWTH Aachen University, Karlsruhe Institute of Technology (KIT), LIMSI-CNRS and SYSTRAN Software, Inc. The translations were joined using the RWTH’s system combination approach. Experimental results show improvements of up to 1.2 points in BLEU and 1.2 points in TER compared to the best single translation. 
This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013). We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions. Both hierarchical and phrase-based SMT systems are applied. A number of different techniques are evaluated, including hierarchical phrase reordering, translation model interpolation, domain adaptation techniques, weighted phrase extraction, word class language model, continuous space language model and system combination. By application of these methods we achieve considerable improvements over the respective baseline systems. 
This paper describes the University of Cambridge submission to the Eighth Workshop on Statistical Machine Translation. We report results for the RussianEnglish translation task. We use multiple segmentations for the Russian input language. We employ the Hadoop framework to extract rules. The decoder is HiFST, a hierarchical phrase-based decoder implemented using weighted ﬁnitestate transducers. Lattices are rescored with a higher order language model and minimum Bayes-risk objective. 
We describe improvements made over the past year to Joshua, an open-source translation system for parsing-based machine translation. The main contributions this past year are signiﬁcant improvements in both speed and usability of the grammar extraction and decoding steps. We have also rewritten the decoder to use a sparse feature representation, enabling training of large numbers of features with discriminative training methods. 
This paper presents the experiments conducted by the Machine Translation group at DCU and Prompsit Language Engineering for the WMT13 translation task. Three language pairs are considered: SpanishEnglish and French-English in both directions and German-English in that direction. For the Spanish-English pair, the use of linguistic information to select parallel data is investigated. For the FrenchEnglish pair, the usefulness of the small indomain parallel corpus is evaluated, compared to an out-of-domain parallel data sub-sampling method. Finally, for the German-English system, we describe our work in addressing the long distance reordering problem and a system combination strategy. 
This paper describes QCRI-MES’s submission on the English-Russian dataset to the Eighth Workshop on Statistical Machine Translation. We generate improved word alignment of the training data by incorporating an unsupervised transliteration mining module to GIZA++ and build a phrase-based machine translation system. For tuning, we use a variation of PRO which provides better weights by optimizing BLEU+1 at corpus-level. We transliterate out-of-vocabulary words in a postprocessing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised transliteration mining system. For the Russian to English translation direction, we apply linguistically motivated pre-processing on the Russian side of the data. 
 2 The Docent Decoder  We describe the Uppsala University system for WMT13, for English-to-German translation. We use the Docent decoder, a local search decoder that translates at the document level. We add tunable distortion limits, that is, soft constraints on the maximum distortion allowed, to Docent. We also investigate cleaning of the noisy Common Crawl corpus. We show that we can use alignment-based ﬁltering for cleaning with good results. Finally we investigate effects of corpus selection for recasing. 
Supervised approaches to NLP tasks rely on high-quality data annotations, which typically result from expensive manual labelling procedures. For some tasks, however, the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications. In Machine Translation (MT) Quality Estimation (QE), for instance, using humanannotated data to train a binary classiﬁer that discriminates between good (useful for a post-editor) and bad translations is not trivial. Focusing on this binary task, we show that subjective human judgements can be effectively replaced with an automatic annotation procedure. To this aim, we compare binary classiﬁers trained on different data: the human-annotated dataset from the 7th Workshop on Statistical Machine Translation (WMT-12), and an automatically labelled version of the same corpus. Our results show that human labels are less suitable for the task. 
Many tasks in NLP and IR require efﬁcient document similarity computations. Beyond their common application to exploratory data analysis, latent variable topic models have been used to represent text in a low-dimensional space, independent of vocabulary, where documents may be compared. This paper focuses on the task of searching a large multilingual collection for pairs of documents that are translations of each other. We present (1) efﬁcient, online inference for representing documents in several languages in a common topic space and (2) fast approximations for ﬁnding near neighbors in the probability simplex. Empirical evaluations show that these methods are as accurate as—and signiﬁcantly faster than— Gibbs sampling and brute-force all-pairs search. 
Statistical machine translation (SMT) performance suffers when models are trained on only small amounts of parallel data. The learned models typically have both low accuracy (incorrect translations and feature scores) and low coverage (high out-of-vocabulary rates). In this work, we use an additional data resource, comparable corpora, to improve both. Beginning with a small bitext and corresponding phrase-based SMT model, we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora. Then, we supplement the model’s feature space with translation scores estimated over comparable corpora in order to improve accuracy. We observe improvements between 0.5 and 1.7 BLEU translating Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu into English. 
We propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation options—phrasal translations that are generated by auxiliary translation and postediting processes—to augment the default phrase inventory learned from parallel data. We apply our technique to the problem of producing English determiners when translating from Russian and Czech, languages that lack deﬁniteness morphemes. Our approach augments the English side of the phrase table using a classiﬁer to predict where English articles might plausibly be added or removed, and then we decode as usual. Doing so, we obtain signiﬁcant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classiﬁer. 
Our ﬁeld has seen signiﬁcant improvements in the quality of machine translation systems over the past several years. The single biggest factor in this improvement has been the accumulation of ever larger stores of data. However, we now ﬁnd ourselves the victims of our own success, in that it has become increasingly difﬁcult to train on such large sets of data, due to limitations in memory, processing power, and ultimately, speed (i.e., data to models takes an inordinate amount of time). Some teams have dealt with this by focusing on data cleaning to arrive at smaller data sets (Denkowski et al., 2012a; Rarrick et al., 2011), “domain adaptation” to arrive at data more suited to the task at hand (Moore and Lewis, 2010; Axelrod et al., 2011), or by speciﬁcally focusing on data reduction by keeping only as much data as is needed for building models e.g., (Eck et al., 2005). This paper focuses on techniques related to the latter efforts. We have developed a very simple n-gram counting method that reduces the size of data sets dramatically, as much as 90%, and is applicable independent of speciﬁc dev and test data. At the same time it reduces model sizes, improves training times, and, because it attempts to preserve contexts for all n-grams in a corpus, the cost in quality is minimal (as measured by BLEU ). Further, unlike other methods created specifically for data reduction that have similar effects on the data, our method scales to very large data, up to tens to hundreds of millions of parallel sentences.  
Multi-task learning has been shown to be effective in various applications, including discriminative SMT. We present an experimental evaluation of the question whether multi-task learning depends on a “natural” division of data into tasks that balance shared and individual knowledge, or whether its inherent regularization makes multi-task learning a broadly applicable remedy against overﬁtting. To investigate this question, we compare “natural” tasks deﬁned as sections of the International Patent Classiﬁcation versus “random” tasks deﬁned as random shards in the context of patent SMT. We ﬁnd that both versions of multi-task learning improve equally well over independent and pooled baselines, and gain nearly 2 BLEU points over standard MERT tuning. 
We present a novel online learning approach for statistical machine translation tailored to the computer assisted translation scenario. With the introduction of a simple online feature, we are able to adapt the translation model on the ﬂy to the corrections made by the translators. Additionally, we do online adaption of the feature weights with a large margin algorithm. Our results show that our online adaptation technique outperforms the static phrase based statistical machine translation system by 6 BLEU points absolute, and a standard incremental adaptation approach by 2 BLEU points absolute. 
We present an iterative technique to generate phrase tables for SMT, which is based on force-aligning the training data with a modiﬁed translation decoder. Different from previous work, we completely avoid the use of a word alignment or phrase extraction heuristics, moving towards a more principled phrase generation and probability estimation. During training, we allow the decoder to generate new phrases on-the-ﬂy and increment the maximum phrase length in each iteration. Experiments are carried out on the IWSLT 2011 Arabic-English task, where we are able to reach moderate improvements on a state-of-the-art baseline with our training method. The resulting phrase table shows only a small overlap with the heuristically extracted one, which demonstrates the restrictiveness of limiting phrase selection by a word alignment or heuristics. By interpolating the heuristic and the trained phrase table, we can improve over the baseline by 0.5% BLEU and 0.5% TER. 
We present Positive Diversity Tuning, a new method for tuning machine translation models specifically for improved performance during system combination. System combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other. We propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations, while also punishing them for translations that are too similar to those produced by other systems. The formulation of the Positive Diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines. We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets, while still obtaining good BLEU scores. When these individual systems are used together for system combination, our approach allows for significant gains of 0.8 BLEU even when the combination is performed using a small number of otherwise identical individual systems. 
This paper describes a set of experiments on two sub-tasks of Quality Estimation of Machine Translation (MT) output. Sentence-level ranking of alternative MT outputs is done with pairwise classiﬁers using Logistic Regression with blackbox features originating from PCFG Parsing, language models and various counts. Post-editing time prediction uses regression models, additionally fed with new elaborate features from the Statistical MT decoding process. These seem to be better indicators of post-editing time than blackbox features. Prior to training the models, feature scoring with ReliefF and Information Gain is used to choose feature sets of decent size and avoid computational complexity. 
We introduce referential translation machines (RTM) for quality estimation of translation outputs. RTMs are a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for estimating the quality of translation outputs, judging the semantic similarity between text, and evaluating the quality of student answers. RTMs achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation (SMT) quality. RTMs remove the need to access any SMT system speciﬁc information or prior knowledge of the training data or models used when generating the translations. We develop novel techniques for solving all subtasks in the WMT13 quality estimation (QE) task (QET 2013) based on individual RTM models. Our results achieve improvements over last year’s QE task results (QET 2012), as well as our previous results, provide new features and techniques for QE, and rank 1st or 2nd in all of the subtasks. 
In this paper we present the approach and system setup of the joint participation of Fondazione Bruno Kessler and University of Edinburgh in the WMT 2013 Quality Estimation shared-task. Our submissions were focused on tasks whose aim was predicting sentence-level Human-mediated Translation Edit Rate and sentence-level post-editing time (Task 1.1 and 1.3, respectively). We designed features that are built on resources such as automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Our models consistently overcome the baselines for both tasks and performed particularly well for Task 1.3, ranking ﬁrst among seven participants. 
This paper presents the LIG’s systems submitted for Task 2 of WMT13 Quality Estimation campaign. This is a word conﬁdence estimation (WCE) task where each participant was asked to label each word in a translated text as a binary ( Keep/Change) or multi-class (Keep/Substitute/Delete) category. We integrate a number of features of various types (system-based, lexical, syntactic and semantic) into the conventional feature set, for our baseline classiﬁer training. After the experiments with all features, we deploy a “Feature Selection” strategy to keep only the best performing ones. Then, a method that combines multiple “weak” classiﬁers to build a strong “composite” classiﬁer by taking advantage of their complementarity is presented and experimented. We then select the best systems for submission and present the ofﬁcial results obtained. 
This paper gives a detailed description of the ACT (Accuracy of Connective Translation) metric, a reference-based metric that assesses only connective translations. ACT relies on automatic word-level alignment (using GIZA++) between a source sentence and respectively the reference and candidate translations, along with other heuristics for comparing translations of discourse connectives. Using a dictionary of equivalents, the translations are scored automatically or, for more accuracy, semi-automatically. The accuracy of the ACT metric was assessed by human judges on sample data for English/French, English/Arabic, English/Italian and English/German translations; the ACT scores are within 2-5% of human scores. The actual version of ACT is available only for a limited language pairs. Consequently, we are participating only for the English/French and English/German language pairs. Our hypothesis is that ACT metric scores increase with better translation quality in terms of human evaluation. 
This paper is to describe our machine translation evaluation systems used for participation in the WMT13 shared Metrics Task. In the Metrics task, we submitted two automatic MT evaluation systems nLEPOR_baseline and LEPOR_v3.1. nLEPOR_baseline is an n-gram based language independent MT evaluation metric employing the factors of modified sentence length penalty, position difference penalty, n-gram precision and n-gram recall. nLEPOR_baseline measures the similarity of the system output translations and the reference translations only on word sequences. LEPOR_v3.1 is a new version of LEPOR metric using the mathematical harmonic mean to group the factors and employing some linguistic features, such as the part-of-speech information. The evaluation results of WMT13 show LEPOR_v3.1 yields the highest averagescore 0.86 with human judgments at systemlevel using Pearson correlation criterion on English-to-other (FR, DE, ES, CS, RU) language pairs. 
The linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximation to the human HMEANT MT evaluation metric which measures semantic frame similarity between MT output and reference translations. In this paper, we describe HKUST’s submission to the WMT 2013 metrics evaluation task, MEANT and UMEANT. MEANT is optimized by tuning a small number of weights—one for each semantic role label—so as to maximize correlation with human adequacy judgment on a development set. UMEANT is an unsupervised version where weights for each semantic role label are estimated via an inexpensive unsupervised approach, as opposed to MEANT’s supervised method relying on more expensive grid search. In this paper, we present a battery of experiments for optimizing MEANT on different development sets to determine the set of weights that maximize MEANT’s accuracy and stability. Evaluated on test sets from the WMT 2012/2011 metrics evaluation, both MEANT and UMEANT achieve competitive correlations with human judgments using nothing more than a monolingual corpus and an automatic shallow semantic parser. 
In this paper, we propose a novel syntactic based MT evaluation metric which only employs the dependency information in the source side. Experimental results show that our method achieves higher correlation with human judgments than BLEU, TER, HWCM and METEOR at both sentence and system level for all of the four language pairs in WMT 2010. 
Despite being closely related languages, German and English are characterized by important word order differences. Longrange reordering of verbs, in particular, represents a real challenge for state-of-theart SMT systems and is one of the main reasons why translation quality is often so poor in this language pair. In this work, we review several solutions to improve the accuracy of German-English word reordering while preserving the efﬁciency of phrase-based decoding. Among these, we consider a novel technique to dynamically shape the reordering search space and effectively capture long-range reordering phenomena. Through an extensive evaluation including diverse translation quality metrics, we show that these solutions can signiﬁcantly narrow the gap between phrase-based and hierarchical SMT. 
We introduce a lexicalized reordering model for hierarchical phrase-based machine translation. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by Tillmann (2004). While this type of lexicalized reordering model is a valuable and widely-used component of standard phrase-based statistical machine translation systems (Koehn et al., 2007), it is however commonly not employed in hierarchical decoders. We describe how phrase orientation probabilities can be extracted from wordaligned training data for use with hierarchical phrase inventories, and show how orientations can be scored in hierarchical decoding. The model is empirically evaluated on the NIST Chinese→English translation task. We achieve a signiﬁcant improvement of +1.2 %BLEU over a typical hierarchical baseline setup and an improvement of +0.7 %BLEU over a syntax-augmented hierarchical setup. On a French→German translation task, we obtain a gain of up to +0.4 %BLEU. 
This paper presents a dependencyconstrained hierarchical machine translation model that uses Moses open-source toolkit for rule extraction and decoding. Experiments are carried out for the German-English language pair in both directions for projective and non-projective dependencies. We examine effects on SCFG size and automatic evaluation results when constraints are applied with respect to projective or non-projective dependency structures and on the source or target language side. 
We present a method for inference in hierarchical phrase-based translation, where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling. We also present a ﬁrst implementation of that method along with experimental results shedding light on some fundamental issues. In hierarchical translation, inference needs to be performed over a high-complexity distribution deﬁned by the intersection of a translation hypergraph and a target language model. We replace this intractable distribution by a sequence of tractable upper-bounds for which exact optimisers and samplers are easy to obtain. Our experiments show that exact inference is then feasible using only a fraction of the time and space that would be required by the full intersection, without recourse to pruning techniques that only provide approximate solutions. While the current implementation is limited in the size of inputs it can handle in reasonable time, our experiments provide insights towards obtaining future speedups, while staying in the same general framework. 
Sentence alignment is an important step in the preparation of parallel data. Most aligners do not perform very well when the input is a noisy, rather than a highlyparallel, document pair. Evaluating aligners under noisy conditions would seem to require creating an evaluation dataset by manually annotating a noisy document for gold-standard alignments. Such a costly process hinders our ability to evaluate an aligner under various types and levels of noise. In this paper, we propose a new evaluation framework for sentence aligners, which is particularly suitable for noisy-data evaluation. Our approach is unique as it requires no manual labeling, instead relying on small parallel datasets (already at the disposal of MT researchers) to generate many evaluation datasets that mimic a variety of noisy conditions. We use our framework to perform a comprehensive comparison of three aligners under noisy conditions. Furthermore, our framework facilitates the ﬁne-tuning of a state-of-the-art sentence aligner, allowing us to substantially increase its recall rates by anywhere from 5% to 14% (absolute) across several language pairs. 
We apply multi-rate HMMs, a tree structured HMM model, to the word-alignment problem. Multi-rate HMMs allow us to model reordering at both the morpheme level and the word level in a hierarchical fashion. This approach leads to better machine translation results than a morphemeaware model that does not explicitly model morpheme reordering. 
We propose a novel unsupervised word alignment model based on the Hidden Markov Tree (HMT) model. Our model assumes that the alignment variables have a tree structure which is isomorphic to the target dependency tree and models the distortion probability based on the source dependency tree, thereby incorporating the syntactic structure from both sides of the parallel sentences. In English-Japanese word alignment experiments, our model outperformed an IBM Model 4 baseline by over 3 points alignment error rate. While our model was sensitive to posterior thresholds, it also showed a performance comparable to that of HMM alignment models. 
The Discriminative Word Lexicon (DWL) is a maximum-entropy model that predicts the target word probability given the source sentence words. We present two ways to extend a DWL to improve its ability to model the word translation probability in a phrase-based machine translation (PBMT) system. While DWLs are able to model the global source information, they ignore the structure of the source and target sentence. We propose to include this structure by modeling the source sentence as a bag-of-n-grams and features depending on the surrounding target words. Furthermore, as the standard DWL does not get any feedback from the MT system, we change the DWL training process to explicitly focus on addressing MT errors. By using these methods we are able to improve the translation performance by up to 0.8 BLEU points compared to a system that uses a standard DWL. 
Statistical natural language generation from abstract meaning representations presupposes large corpora consisting of text–meaning pairs. Even though such corpora exist nowadays, or could be constructed using robust semantic parsing, the simple alignment between text and meaning representation is too coarse for developing robust (statistical) NLG systems. By reformatting semantic representations as graphs, ﬁne-grained alignment can be obtained. Given a precise alignment at the word level, the complete surface form of a meaning representations can be deduced using a simple declarative rule. 
The increasing amount of machinereadable data available in the context of the Semantic Web creates a need for methods that transform such data into human-comprehensible text. In this paper we develop and evaluate a Natural Language Generation (NLG) system that converts RDF data into natural language text based on an ontology and an associated ontology lexicon. While it follows a classical NLG pipeline, it diverges from most current NLG systems in that it exploits an ontology lexicon in order to capture context-speciﬁc lexicalisations of ontology concepts, and combines the use of such a lexicon with the choice of lexical items and syntactic structures based on statistical information extracted from a domain-speciﬁc corpus. We apply the developed approach to the cooking domain, providing both an ontology and an ontology lexicon in lemon format. Finally, we evaluate ﬂuency and adequacy of the generated recipes with respect to two target audiences: cooking novices and advanced cooks. 
In this paper we describe a natural language generation system which produces complex sentences from a biology knowledge base. The NLG system allows domain experts to discover errors in the knowledge base and generates certain parts of answers in response to users’ questions in an e-textbook application. The system allows domain experts to customise its lexical resources and to set parameters which inﬂuence syntactic constructions in generated sentences. The system is capable of dealing with certain types of incomplete inputs arising from a knowledge base which is constantly edited and includes a referring expression generation module which keeps track of discourse history. Our referring expression module is available for download as the open source Antfarm tool1. 
We show that Nakatsu & White’s (2010) proposed enhancements to the SPaRKy Restaurant Corpus (SRC; Walker et al., 2007) for better expressing contrast do indeed make it possible to generate better texts, including ones that make effective and varied use of contrastive connectives and discourse adverbials. After ﬁrst presenting a validation experiment for naturalness ratings of SRC texts gathered using Amazon’s Mechanical Turk, we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them. We conclude with a discussion of possible ways of improving the ranker in future work. 
In this paper, we focus on the task of generating elliptic sentences. We extract from the data provided by the Surface Realisation (SR) Task (Belz et al., 2011) 2398 input whose corresponding output sentence contain an ellipsis. We show that 9% of the data contains an ellipsis and that both coverage and BLEU score markedly decrease for elliptic input (from 82.3% coverage for non-elliptic sentences to 65.3% for elliptic sentences and from 0.60 BLEU score to 0.47). We argue that elided material should be represented using phonetically empty nodes and we introduce a set of rewrite rules which permits adding these empty categories to the SR data. Finally, we evaluate an existing surface realiser on the resulting dataset. We show that, after rewriting, the generator achieves a coverage of 76% and a BLEU score of 0.74 on the elliptical data. 
We present an Integer Linear Programming model of content selection, lexicalization, and aggregation that we developed for a system that generates texts from OWL ontologies. Unlike pipeline architectures, our model jointly considers the available choices in these three text generation stages, to avoid greedy decisions and produce more compact texts. Experiments with two ontologies conﬁrm that it leads to more compact texts, compared to a pipeline with the same components, with no deterioration in the perceived quality of the generated texts. We also present an approximation of our model, which allows longer texts to be generated efﬁciently. 
Planning-based approaches to reference provide a uniform treatment of linguistic decisions, from content selection to lexical choice. In this paper, we show how the issues of lexical ambiguity, vagueness, unspeciﬁc descriptions, ellipsis, and the interaction of subsective modiﬁers can be expressed using a belief-state planner modiﬁed to support context-dependent actions. Because the number of distinct denotations it searches grows doublyexponentially with the size of the referential domain, we present representational and search strategies that make generation and interpretation tractable. 
When they introduced the Graph-Based Algorithm (GBA) for referring expression generation, Krahmer et al. (2003) ﬂaunted the natural way in which it deals with relations between objects; but this feature has never been tested empirically. We ﬁll this gap in this paper, exploring referring expression generation from the perspective of the GBA and focusing in particular on generating human-like expressions in visual scenes with spatial relations. We compare the original GBA against a variant that we introduce to better reﬂect human reference, and ﬁnd that although the original GBA performs reasonably well, our new algorithm offers an even better match to human data (77.91% Dice). Further, it can be extended to capture speaker variation, reaching an 82.83% Dice overlap with human-produced expressions. 
Pointing gestures are pervasive in human referring actions, and are often combined with spoken descriptions. Combining gesture and speech naturally to refer to objects is an essential task in multimodal NLG systems. However, the way gesture and speech should be combined in a referring act remains an open question. In particular, it is not clear whether, in planning a pointing gesture in conjunction with a description, an NLG system should seek to minimise the redundancy between them, e.g. by letting the pointing gesture indicate locative information, with other, nonlocative properties of a referent included in the description. This question has a bearing on whether the gestural and spoken parts of referring acts are planned separately or arise from a common underlying computational mechanism. This paper investigates this question empirically, using machine-learning techniques on a new corpus of dialogues involving multimodal references to objects. Our results indicate that human pointing strategies interact with descriptive strategies. In particular, pointing gestures are strongly associated with the use of locative features in referring expressions. 
In this overview paper we present the outcome of the ﬁrst content selection challenge from open semantic web data, focusing mainly on the preparatory stages for deﬁning the task and annotating the data. The task to perform was described in the challenge’s call as follows: given a set of RDF triples containing facts about a celebrity, select those triples that are reﬂected in the target text (i.e., a short biography about that celebrity). From the initial nine expressions of interest, ﬁnally two participants submitted their systems for evaluation. 
Ehud Reiter, Albert Gatt, Franc¸ois Portet, and Marian van der Meulen. 2008. The importance of narrative and other lessons from an evaluation of an nlg system that summarises clinical data. In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08, pages 147–156, Stroudsburg, PA, USA. Association for Computational Linguistics. 
When instructors prepare learning materials for students, they frequently develop accompanying questions to guide learning. Natural language processing technology can be used to automatically generate such questions but techniques used have not fully leveraged semantic information contained in the learning materials or the full context in which the question generation task occurs. We introduce a sophisticated template-based approach that incorporates semantic role labels into a system that automatically generates natural language questions to support online learning. While we have not yet incorporated the full learning context into our approach, our preliminary evaluation and evaluation methodology indicate our approach is a promising one for supporting learning. 
We describe a statistical Natural Language Generation (NLG) method for summarisation of time-series data in the context of feedback generation for students. In this paper, we initially present a method for collecting time-series data from students (e.g. marks, lectures attended) and use example feedback from lecturers in a datadriven approach to content selection. We show a novel way of constructing a reward function for our Reinforcement Learning agent that is informed by the lecturers’ method of providing feedback. We evaluate our system with undergraduate students by comparing it to three baseline systems: a rule-based system, lecturerconstructed summaries and a Brute Force system. Our evaluation shows that the feedback generated by our learning agent is viewed by students to be as good as the feedback from the lecturers. Our ﬁndings suggest that the learning agent needs to take into account both the student and lecturers’ preferences. 
[kokil, chriskhoo]@pmail.ntu.edu.sg, tjcna@ntu.edu.sg  Abstr act This study is conducted in the area of multidocument summarization, and develops a literature review framework based on a deconstruction of human-written literature review sections in information science research papers. The first part of the study presents the results of a multi-level discourse analysis to investigate their discourse and content characteristics. These findings were incorporated into a framework for literature reviews, focusing on their macro-level document structure and the sentence-level templates, as well as the information summarization strategies. The second part of this study discusses insights from this analysis, and how the framework can be adapted to automatic summaries resembling human written literature reviews. Summaries generated from a partial implementation are evaluated against human written summaries and assessors’ comments are discussed to formulate recommendations for future work. 
We propose a novel end-to-end framework for abstractive meeting summarization. We cluster sentences in the input into communities and build an entailment graph over the sentence communities to identify and select the most relevant sentences. We then aggregate those selected sentences by means of a word graph model. We exploit a ranking strategy to select the best path in the word graph as an abstract sentence. Despite not relying on the syntactic structure, our approach signiﬁcantly outperforms previous models for meeting summarization in terms of informativeness. Moreover, the longer sentences generated by our method are competitive with shorter sentences generated by the previous word graph model in terms of grammaticality. 
This paper focuses on a subtask of natural language generation (NLG), voice selection, which decides whether a clause is realised in the active or passive voice according to its contextual information. Automatic voice selection is essential for realising more sophisticated MT and summarisation systems, because it impacts the readability of generated texts. However, to the best of our knowledge, the NLG community has been less concerned with explicit voice selection. In this paper, we propose an automatic voice selection model based on various linguistic information, ranging from lexical to discourse information. Our empirical evaluation using a manually annotated corpus in Japanese demonstrates that the proposed model achieved 0.758 in F-score, outperforming the two baseline models. 
The cross-disciplinary MIME project aims to develop a mobile medical monitoring system that improves handover transactions in rural pre-hospital scenarios between the ﬁrst person on scene and ambulance clinicians. NLG is used to produce a textual handover report at any time, summarising data from novel medical sensors, as well as observations and actions recorded by the carer. We describe the MIME project with a focus on the NLG algorithm and an initial evaluation of the generated reports. 
We present the results from an elicitation experiment in which human speakers were asked to produced quantiﬁed referring expressions (QREs), as in ‘The crate with 10 apples’, ‘The crate with many apples’, etc. These results suggest that some subtle contextual factors govern the choice between different types of QREs, and that numerals are highly preferred for subitizable quantities despite the availability of coarser-grained expressions. 
In this paper we present the preliminary work of a Basque poetry generation system. Basically, we have extracted the POS-tag sequences from some verse corpora and calculated the probability of each sequence. For the generation process we have defined 3 different experiments: Based on a strophe from the corpora, we (a) replace each word with other according to its POS-tag and suffixes, (b) replace each noun and adjective with another equally inflected word and (c) replace only nouns with semantically related ones (inflected). Finally we evaluate those strategies using a Turing Test-like evaluation. 
We present ﬁrst results of our project on the generation of contextually adequate greeting exchanges in video role playing games. To make greeting exchanges computable, an analysis of the factors inﬂuencing greeting behavior as well as the factors inﬂuencing greeting exchanges is given. Based on the politeness model proposed by Brown & Levinson (1987) we develop a simple algorithm for the generation of greeting exchanges. An evaluation, comparing dialog from the video role playing game Skyrim to dialog determined by our algorithm, shows that our algorithm is able to generate greeting exchanges that are contextually more adequate than those featured by Skyrim. 
We introduce GenNext, an NLG system designed speciﬁcally to adapt quickly and easily to different domains. Given a domain corpus of historical texts, GenNext allows the user to generate a template bank organized by semantic concept via derived discourse representation structures in conjunction with general and domain-speciﬁc entity tags. Based on various features collected from the training corpus, the system statistically learns template representations and document structure and produces well–formed texts (as evaluated by crowdsourced and expert evaluations). In addition to domain adaptation, GenNext’s hybrid approach signiﬁcantly reduces complexity as compared to traditional NLG systems by relying on templates (consolidating micro-planning and surface realization) and minimizing the need for domain experts. In this description, we provide details of GenNext’s theoretical perspective, architecture and evaluations of output. 
This paper describes SimpleNLG-EnFr, an adaption of the English realisation engine SimpleNLG (Gatt and Reiter, 2009) for bilingual English-French realisation. Grammatical similarities between English and French that could be exploited and specifics of French that needed adaptation are discussed. 
Paraphrasing is expressing the same semantic content using different linguistic means. Although previous work has addressed linguistic variations at different levels of language, paraphrasing in Turkish has not been yet thoroughly studied. This paper presents the ﬁrst study towards Turkish paraphrase alignment. We perform an analysis of different types of paraphrases on a modest Turkish paraphrase corpus and present preliminary results on that analysis from different standpoints. We also explore the impact of human interpretation of paraphrasing on the alignment of paraphrase sentence pairs. 
This position paper presents an on-going work on a natural language generation framework that is particularly tailored for summary text generation from body area networks. We present an overview of the main challenges when considering this type of sensor devices used for at home monitoring of health parameters. This paper describes the ﬁrst steps towards the implementation of a system which collects information from heart rate and respiration rate using a wearable sensor. The paper further outlines the direction for future work and in particular the challenges for NLG in this application domain. 
We present the ﬁrst prototype of a handover report generator developed for the MIME (Managing Information in Medical Emergencies) project. NLG applications in the medical domain have been varied but most are deployed in clinical situations. We develop a mobile device for prehospital care which receives streamed sensor data and user input, and converts these into a handover report for paramedics. 
This demo showcases Thoughtland, an end-to-end system that takes training data and a selected machine learning model, produces a cloud of points via crossvalidation to approximate its error function, then uses model-based clustering to identify interesting components of the error function and natural language generation to produce an English text summarizing the error function. 
Institut fu¨r Maschinelle Sprachverarbeitung  University of Stuttgart, Germany  sina.zarriess,kyle@ims.uni-stuttgart.de  
This document describes the University of Delaware’s entry into KBGen 2013 Challenge which provided teams with input data representation from the AURA knowledge base (KB), developed in the context of the HALO Project at SRI International, along with a lexicon mapping for concepts present on those input files. Training sentences were also provided. The task was to accurately generate an English sentence depicting the information from a set of triples from the knowledge base.  lar function in the KB triples, e.g. has-function, subevent, plays. These functions provided the main verb and sentence structures, and other KB relations were fit into this structure (in subject/object position or as adjuncts) in a rulebased way. For instance, Figure 1 shows a triples file from the testing data that was identified under the cluster has-function, along with the sentence generated by our system and the rule used to realize the cluster for this relation type.  
This paper described UIC-CSC, the entry we submitted for the Content Selection Challenge 2013. Our model consists of heuristic rules based on co-occurrences of predicates in the training data. 
We describe a biological event detection method implemented for the Genia Event Extraction task of BioNLP 2013. The method relies on syntactic dependency relations provided by a general NLP pipeline, supported by statistics derived from Maximum Entropy models for candidate trigger words, for potential arguments, and for argument frames. 
In the perspective of annotating a text with respect to an ontology, we have participated in the subtask 1 of the BB BioNLPST whose aim is to detect, in the text, Bacteria Habitats and associate to them one or several categories from the OntoBiotope ontology provided for the task. We have used a rule-based machine learning algorithm (WHISK) combined with a rule-based automatic ontology projection method and a rote learning technique. The combination of these three sources of rules leads to good results with a SER measure close to the winner and a best F-measure. 
The goal of the Genic Regulation Network task (GRN) is to extract a regulation network that links and integrates a variety of molecular interactions between genes and proteins of the well-studied model bacterium Bacillus subtilis. It is an extension of the BI task of BioNLP-ST’11. The corpus is composed of sentences selected from publicly available PubMed scientific abstracts. The paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results. 
This paper describes the information extraction techniques developed in the framework of the participation of IRISATexMex to the following BioNLP-ST13 tasks: Bacterial Biotope subtasks 1 and 2, and Graph Regulation Network. The approaches developed are general-purpose ones and do not rely on specialized preprocessing, nor specialized external data, and they are expected to work independently of the domain of the texts processed. They are classically based on machine learning techniques, but we put the emphasis on the use of similarity measures inherited from the information retrieval domain (Okapi-BM25 (Robertson et al., 1998), language modeling (Hiemstra, 1998)). Through the good results obtained for these tasks, we show that these simple settings are competitive provided that the representation and similarity chosen are well suited for the task. 
This research analyzed the clinical notes of epilepsy patients using techniques from corpus linguistics and machine learning and predicted which patients are candidates for neurosurgery, i.e. have intractable epilepsy, and which are not. Information-theoretic and machine learning techniques are used to determine whether and how sets of clinic notes from patients with intractable and nonintractable epilepsy are different. The results show that it is possible to predict from an early stage of treatment which patients will fall into one of these two categories based only on text data. These results have broad implications for developing clinical decision support systems. 
Identification of complex clinical phenotypes among critically ill patients is a major challenge in clinical research. The overall research goal of our work is to develop automated approaches that accurately identify critical illness phenotypes to prevent the resource intensive manual abstraction approach. In this paper, we describe a text processing method that uses Natural Language Processing (NLP) and supervised text classification methods to identify patients who are positive for Acute Lung Injury (ALI) based on the information available in free-text chest x-ray reports. To increase the classification performance we enhanced the baseline unigram representation with bigram and trigram features, enriched the n-gram features with assertion analysis, and applied statistical feature selection. We used 10-fold cross validation for evaluation and our best performing classifier achieved 81.70% precision (positive predictive value), 75.59% recall (sensitivity), 78.53% f-score, 74.61% negative predictive value, 76.80% specificity in identifying patients with ALI. 
The clinical narrative contains a great deal of valuable information that is only understandable in a temporal context. Events, time expressions, and temporal relations convey information about the time course of a patient’s clinical record that must be understood for many applications of interest. In this paper, we focus on extracting information about how time expressions and events are related by narrative containers. We use support vector machines with composite kernels, which allows for integrating standard feature kernels with tree kernels for representing structured features such as constituency trees. Our experiments show that using tree kernels in addition to standard feature kernels improves F1 classiﬁcation for this task. 
In order to integrate heterogeneous clinical information sources, semantically correlating information entities have to be linked. Our discussions with radiologists revealed that anatomical entities with pathological ﬁndings are of particular interest when linking radiology text and images. Previous research to identify pathological ﬁndings focused on simplistic approaches that recognize diseases or negated ﬁndings, but failed to establish a holistic approach. In this paper, we introduce our syntacto-semantic parsing approach to classify sentences in radiology reports as either pathological or non-pathological based on the ﬁndings they describe. Although we operate with an incomplete, RadLex-based linguistic resource, the obtained results show the effectiveness of our approach by identifying a recall value of 74.3% for the classiﬁcation task. 
The various ways in which one can refer to the same clinical concept needs to be accounted for in a semantic resource such as SNOMED CT. Developing terminological resources manually is, however, prohibitively expensive and likely to result in low coverage, especially given the high variability of language use in clinical text. To support this process, distributional methods can be employed in conjunction with a large corpus of electronic health records to extract synonym candidates for clinical terms. In this paper, we exemplify the potential of our proposed method using the Swedish version of SNOMED CT, which currently lacks synonyms. A medical expert inspects two thousand term pairs generated by two semantic spaces – one of which models multiword terms in addition to single words – for one hundred preferred terms of the semantic types disorder and ﬁnding. 
In this paper, a new self–training method for domain adaptation is illustrated, where the selection of reliable parses is carried out by an unsupervised linguistically– driven algorithm, ULISSE. The method has been tested on biomedical texts with results showing a signiﬁcant improvement with respect to considered baselines, which demonstrates its ability to capture both reliability of parses and domain– speciﬁcity of linguistic constructions. 
While interest in biomedical question answering has been growing, research in consumer health question answering remains relatively sparse. In this paper, we focus on the task of consumer health question understanding. We present a rule-based methodology that relies on lexical and syntactic information as well as anaphora/ellipsis resolution to construct structured representations of questions (frames). Our results indicate the viability of our approach and demonstrate the important role played by anaphora and ellipsis in interpreting consumer health questions. 
Text mining methods for the biomedical domain have matured substantially and are currently being applied on a large scale to support a variety of applications in systems biology, pathway curation, data integration and gene summarization. Community-wide challenges in the BioNLP research ﬁeld provide goldstandard datasets and rigorous evaluation criteria, allowing for a meaningful comparison between techniques as well as measuring progress within the ﬁeld. However, such evaluations are typically conducted on relatively small training and test datasets. On a larger scale, systematic erratic behaviour may occur that severely inﬂuences hundreds of thousands of predictions. In this work, we perform a critical assessment of a large-scale text mining resource, identifying systematic errors and determining their underlying causes through semi-automated analyses and manual evaluations1. 
It has long been realized that sublanguages are relevant to natural language processing and text mining. However, practical methods for recognizing or characterizing them have been lacking. This paper describes a publicly available set of tools for sublanguage recognition. Closure properties are used to assess the goodness of ﬁt of two biomedical corpora to the sublanguage model. Scientiﬁc journal articles are compared to general English text, and it is shown that the journal articles ﬁt the sublanguage model, while the general English text does not. A number of examples of implications of the sublanguage characteristics for natural language processing are pointed out. The software is made publicly available at [edited for anonymization]. 
We present a set of new measures designed to reveal latent information of language use in children at the lexico-syntactic level. We used these metrics to analyze linguistic patterns in spontaneous narratives from children developing typically and children identiﬁed as having a language impairment. We observed significant differences in the z-scores of both populations for most of the metrics. These ﬁndings suggest we can use these metrics to aid in the task of language assessment in children. 
Sentence types typical to Swedish clinical text were extracted by comparing sentence part-of-speech tag sequences in clinical and in standard Swedish text. Parsings by a syntactic dependency parser, trained on standard Swedish, were manually analysed for the 33 sentence types most typical to clinical text. This analysis resulted in the identiﬁcation of eight error types, and for two of these error types, preprocessing rules were constructed to improve the performance of the parser. For all but one of the ten sentence types affected by these two rules, the parsing was improved by pre-processing. 
MEDLINE/PubMed contains structured abstracts that can provide argumentative labels. Selection of abstract sentences based on the argumentative label has shown to improve the performance of information retrieval tasks. These abstracts make up less than one quarter of all the abstracts in MEDLINE/PubMed, so it is worthwhile to learn how to automatically label the non-structured ones. We have compared several machine learning algorithms trained on structured abstracts to identify argumentative labels. We have performed an intrinsic evaluation on predicting argumentative labels for non-structured abstracts and an extrinsic evaluation to predict argumentative labels on abstracts relevant to Gene Reference Into Function (GeneRIF) indexing. Intrinsic evaluation shows that argumentative labels can be assigned effectively to structured abstracts. Algorithms that model the argumentative structure seem to perform better than other algorithms. Extrinsic results show that assigning argumentative labels to non-structured abstracts improves the performance on GeneRIF indexing. On the other hand, the algorithms that model the argumentative structure of the abstracts obtain lower performance in the extrinsic evaluation. 
Child language narratives are used for language analysis, measurement of language development, and the detection of language impairment. In this paper, we explore the use of Latent Dirichlet Allocation (LDA) for detecting topics from narratives, and use the topics derived from LDA in two classiﬁcation tasks: automatic prediction of coherence and language impairment. Our experiments show LDA is useful for detecting the topics that correspond to the narrative structure. We also observed improved performance for the automatic prediction of coherence and language impairment when we use features derived from the topic words provided by LDA. 
In this paper we take a fresh look at parallels between linguistics and biology. We expect that this new line of thinking will propel cross fertilization of two disciplines and open up new research avenues. 
The problem of ﬁnding the consensus / most probable string for a distribution generated by a probabilistic ﬁnite automaton or a hidden Markov model arises in a number of natural language processing tasks: it has to be solved in several transducer related tasks like optimal decoding in speech, or ﬁnding the most probable translation of an input sentence. We provide an algorithm which solves these problems in time polynomial in the inverse of the probability of the most probable string, which in practise makes the computation tractable in many cases. We also show that this exact computation compares favourably with the traditional Viterbi computation. 
Partially observable Markov decision Processes provide an excellent statistical framework to deal with spoken dialog systems that admits global optimization and deal with uncertainty of user goals. However its put in practice entails intractable problems that need efﬁcient and suboptimal approaches. Alternatively some pattern recognition techniques have also been proposed. In this framework the joint probability distribution over some semantic language provided by the speech understanding system and the language of actions provided by the dialog manager need to be estimated. In this work we propose to model this joint probability distribution by stochastic regular bi-languages that have also been successfully proposed for machine translation purposes. To this end a Probabilistic Finite State Bi-Automaton is deﬁned in the paper. As an extension to this model we also propose an attributed model that allows to deal with the task attribute values. Valued attributed are attached to the states in such a way that usual learning and smoothing techniques can be applied as shown in the paper. As far as we know it is the ﬁrst approach based on stochastic bi-languages formally deﬁned to deal with dialog tasks. 
 1.1 Scansion  We present a ﬁnite state technology based system capable of performing metrical scansion of verse written in English. Scansion is the traditional task of analyzing the lines of a poem, marking the stressed and non-stressed elements, and dividing the line into metrical feet. The system’s workﬂow is composed of several subtasks designed around ﬁnite state machines that analyze verse by performing tokenization, part of speech tagging, stress placement, and unknown word stress pattern guessing. The scanner also classiﬁes its input according to the predominant type of metrical foot found. We also present a brief evaluation of the system using a gold standard corpus of human-scanned verse, on which a per-syllable accuracy of 86.78% is reached. The program uses open-source components and is released under the GNU GPL license.  Conventionally, scanning a line of poetry should yield a representation where every syllable is marked with a level of stress—typically two or more levels are used—and groups of syllables are divided into units of feet. Consider, for example, the following line from John Keats’ poem To autumn.  To swell the gourd, and plump the hazel shells  Here, a natural analysis is as follows:  -’  -’  -’  - ’- ’  To swell |the gourd |and plump |the haz|el shells  We use the symbol ’ to denote marked (ictic) syllables, and - to denote unmarked ones (non-ictic). That is, we have analyzed the line in question to follow a stress pattern  
 this paper is to propose a novel approach for the  We propose a novel approach for the maxstring problem in acyclic nondeterministic weighted FSA’s, which is based on a convexity-related notion of domination among intermediary results, and which can be seen as a generalization of the usual dynamic programming technique for ﬁnding the max-path (a.k.a. Viterbi approximation) in such automata.  max-string problem over the “sum-times” semiring Ks ≡ (R∞ + , +, ·, 0, 1), involving a generalization of the Viterbi procedure. A naive approach to the max-string problem would consist in enumerating all the paths, summing the weights of paths corresponding to the same string, and outputting the maximum string. Another, more appealing, approach consists in noting that in the case of a deterministic weighted automaton A , the max-string and max-  
 We propose a new kind of ﬁnite-state automata, suitable for structured input char-  (POS ! = N ∨ WORD = ".*sch") ∧ ∃i∈READ (i.GEND = m ∧ i.NUMB = sg) ∗  acters corresponding to unranked trees of small depth. As a motivating application, we regard executing morphosyntactic queries on a richly annotated text corpus.  describing sequences of segments having a masculine-singular reading, in which all nouns end with sch. In this paper, we propose an adjustment of exist-  
We list the major properties of some important classes of subrational relations, mostly to make them easily accessible to computational linguists. We then argue that there are good linguistic reasons for using no class smaller than the class of synchronous regular relations for morphological analysis, and good mathematical reasons for using no class which is larger. 
 parsing of German adjectives. To do so, we created  We present a method for probabilistic parsing of German words. Our approach uses a morphological analyzer based on weighted ﬁnitestate transducers to segment words into lexical units and a probabilistic context free grammar  a corpus of manually annotated word trees for 5,000 structurally ambiguous adjectives. We describe types of ambiguity and their distribution in the training set and report results of the parsing process in dependence of various grammar transformations.  trained on a manually created set of word trees for the parsing step.  1.1 Word Formation and Structures Word formation is the combination of morphemes to  
We consider ﬁnite-state optimization of morphosyntactic analysis of richly and ambiguously annotated corpora. We propose a general algorithm which, despite being surprisingly simple, proved to be effective in several applications for rulesets which do not match frequently. 
The existing Latvian morphological analyzer was developed more than ten years ago. Its main weaknesses are: low processing speed when processing a large text corpus, complexity of adding new entries to the lexical data base, and limitations for usage on different operational platforms. This paper describes the creation of a new Latvian morphology tool. The tool has the capability to return lemma and morphological analysis for a given word form; it can generate the required word form if lemma and form description is given; it can also generate all possible word forms for a given lemma. As Finite state transducer (FST) technology is used for the morphology tool, it is easy to extend the lexicon, the tool can be reused on different platforms and it has good performance indicators. Introduction An efficient way to generate forms and obtain morphological information about a word in a text is to apply morphological analysis tools. Such tools and their efficient implementation are especially important for languages that have a rich morphology. In this paper, we describe a new morphological processing tool for the Latvian language. The Latvian language is an inflectional language. As described in (Skadiņa et al., 2012), words change form according to grammatical function. Most word forms are built by adding an affix to the stem of the word. The endings are ambiguous. The same lexical ending can  symbolize several grammatical word forms. There can also be changes in a stem – regular consonant changes at the end of a stem, or a stem can be completely different for a word form. For example, for the verbs of the first conjugation, the full set of inflectional word forms is generated using three different stems - infinitive, present tense, and past tense stems. To describe the morphological lexicon, the relationships between stems and different affixes must be defined. The existing Latvian morphological analyzer was developed more than ten years ago. It is based on a relational lexical data base of contemporary Latvian language. Prefixes, stems, and endings are stored in separate tables and are marked by different predefined declension groups. The relationship tables define eligible combinations of affixes. To be used by the morphological analyzer, this data base is compiled into a proprietary format. The same data base is used for building a spelling checker data file for Latvian. The main weaknesses of the existing Latvian morphological analyzer are: low processing speed when processing a large text corpus, complexity of adding new entries to the lexical data base, and limitations on platforms (it works only on the Windows platform). These factors promoted the search for a new solution. In next chapters, we describe in detail the proposed solution. 
 on graphs has been hampered, due, in part,  Work on probabilistic models of natural language tends to focus on strings and trees, but there is increasing interest in more general graph-shaped structures since they seem to be better suited for representing natural language semantics, ontologies, or other varieties of knowledge structures. However, while there are relatively simple approaches to deﬁning generative models over strings and trees, it has proven more challenging for more general graphs. This paper describes a natural generalization of the n-gram to graphs, making use of Hyperedge Replacement Grammars to deﬁne generative models of graph languages.  to the absence of a general agreed upon formalism for processing and modeling such data structures. Where string and tree modeling beneﬁts from the wildly popular Probabilistic Context Free Grammar (PCFG) and related formalisms such as Tree Substitution Grammar, Regular Tree Grammar, Hidden Markov Models, and n-grams, there is nothing of similar popularity for graphs. We need a slightly diﬀerent formalism, and Hyperedge Replacement Grammar (HRG) (Drewes et al., 1997), a variety of context-free grammar for graphs, suggests itself as a reasonable choice given its close analogy with CFG. Of course, in order to make use of the formalism we need actual grammars, and this paper ﬁlls that gap by introducing a procedure for automatically extracting grammars from a corpus of graphs.  
ρB(α1 · · · αn) := (α1 ∩ B) · · · (αn ∩ B).  guages provided by Monadic SecondOrder Logic (MSO), under the Bu¨chiElgot-Trakhtenbrot theorem. String  For example, assuming no state qi belongs to the alphabet Σ of L(A),  symbols are structured as sets to succinctly express MSO-sentences, with  ρΣ( a1, q1 · · · an, qn ) = a1 · · · an  auxiliary symbols conceived as variables bound by quantiﬁers.  where we draw boxes instead of curly braces for sets used as string symbols.  
 the processing of a large amount of grammar rules  In this paper we propose a system to parse and annotate motion constructions expressed in Italian language. We used NooJ as a software tool to implement ﬁnite-state transducers in order to recognize linguistic elements constituting motion events. In this paper we describe the model we adopted for semantic description of events (grounded on Talmy’s Cognitive Semantics theories) and then we illustrate how the system works with a domainspeciﬁc corpus, the structure of annotation that our system will perform, some annotation structures of example sentences expressing motion and then an attempt to evaluate the system’s performance. 
We investigate the composition of ﬁnitestate automata in a multiprocessor environment, presenting a parallel variant of a widely-used composition algorithm. We provide an approximate upper bound for composition speedup of the parallel variant with respect to serial execution, and empirically evaluate the performance of our implementation with respect to this bound.  to improve composition performance in special cases. In Holub and Sˇ tekr (2009), a parallel implementation for the case of string lookup in a deterministic ﬁnite-state acceptor (FSA) is presented. A generalization to n operands which prevents the construction of large intermediate results is given in Allauzen and Mohri (2009). A good deal of work has focussed on dynamic, on-the-ﬂy, or lazy implementations (Hori et al., 2004; Cheng et al., 2007; Mohri et al., 2007), in which the composition of FSTs is only partly computed, new states and transitions being added to the result only when  
 transducer model has inspired further research  Building ﬁnite-state transducers from written autosegmental grammars of tonal languages involves compiling the rules into a notation provided by the ﬁnitestate tools. This work tests a simple, human readable approach to compile and debug autosegmental rules using a simple string encoding for autosegmental representations. The proposal is based on brackets that mark the edges of the tone autosegments. The bracket encoding of the autosegments is compact and directly human readable. The paper also presents a usual ﬁnite-state transducer for transforming a concatenated string of lexemes where each lexeme (such as ”babaa|HH”) consists of a segmental substring and a tonal substring into a chronological master string (”b[a]b[aa]”) where the tone autosegments are associated with their segmental spans.  on multi-tape automata (Wiebe, 1992) and linear codes (Kornai, 1995) that encode events when an autosegmental representation is scanned from left to right. Kornai (1995) has qualiﬁed the proposed codes with a set of desiderata. All these desiderata cannot be, however, fully satisﬁed by any of the linear codes (Wiebe, 1992). An alternative to these multi-tape approaches is proposed by Bird and Ellison (1994) who posit that all tiers are partical descriptions of the common synchronized structure and they can, therefore, be combined via intersection. This constraint-based approach is very natural and it has nice formal properties such as declarativeness and connection to logic. However, the resulting one-level phonology (Bird and Ellison, 1994) is also somewhat incompatible with the autosegmental theory. For example, it does not posit ﬂoating tones that are a crucial formal device in many existing accounts of Bantu tone. The key idea in this paper is to represent the tone units, i.e., autosegments, as time spans that have a start and an end marked with brackets in  
 We have deﬁned a general algorithm (see sec-  This paper presents a system that generates Basque equivalents to terms that describe disorders in SNOMED CT. This task has been performed using Finite-State transducers and a medical preﬁxes and sufﬁxes lexicon. This lexicon is composed of English-Basque translation pairs, and it is used both for the identiﬁcation of the afﬁxes of the English term and for the translation of them into Basque. The translated afﬁxes are composed using morphotactic rules. We evaluated the system with a Gold Standard obtaining promising results (0.93 of precision). This system is part of a more general system which aim is the translation of SNOMED CT into Basque.  tion 2) based on Natural Language Processing (NLP) resources that tries to achieve the translation with an incremental approach. The ﬁrst step of the algorithm is based on the mapping of some lexical resources and has been already developed. Considering the huge size of SNOMED CT (296,000 active concepts and around 1,000,000 descriptions in the English version dated 31-012012) the contribution of the specialized dictionaries has been limited. In the second step that is speciﬁed in this paper, we have used Finite State Machines (FSM) in the form of transducers to generate one-word-terms in Basque taking as a basis terms from the English release of SNOMED CT mentioned before. The generation is based on the translation by means of medical sufﬁxes (i.e. -dipsia, -megaly) and preﬁxes (i.e. episio-,  
 in section 4 I will present the paradigms of adjec-  tival agreement in standard German that are  Syncretism is the area of the morphologysyntax interface where morphology fails the syntax. Inadequate treatment in the design of a morphological analyzer can lead to unbalanced performance of the analyzer either at generation, or at analysis. Furthermore, adequate and consistent treatment of syncretism is needed if the analyzer is to be used for language modeling, especially modeling of the syncretism. In  heavily affected by syncretism. In section 5 I will explain on the basis of the German example and examples from other languages how with minimal changes one can tune the prototypical morphological analyzer to perform the different tasks outlined earlier in this section. In section 6 I will draw some conclusions, and in the Appendix I will show a code excerpt.  this paper I will show that it is possible to create a morphological analyzer that can be tai-  2 Syncretism  lored to various intended uses with minimal effort.  Syncretism is the identity of two or more inflected forms of the same lexeme. The identity of  
 This work presents the finite state approach to  the Kazakh nominal paradigm. The  development and implementation of a finite-  state transducer for the nominal paradigm of  the Kazakh language belonging to  agglutinative languages were undertaken. The  morphophonemic constraints that are imposed  by the Kazakh language synharmonism  (vowels and consonants harmony) on the  combinations of letters under affix joining as  well as morphotactics are considered.  Developed Kazakh finite state transducer  realizes  some  morphological  analysis/generation functions. A preliminary  testing on the use of the morphological  analyzer after OCR preprocessing for  correcting errors in the Kazakh texts was  made.  
We investigate the utility of linguistic features for automatically differentiating between children with varying combinations of two potentially comorbid neurodevelopmental disorders: autism spectrum disorder and speciﬁc language impairment. We ﬁnd that certain manual codes for linguistic errors are useful for distinguishing between diagnostic groups. We investigate the relationship between coding detail and diagnostic classiﬁcation performance, and ﬁnd that a simple coding scheme is of high diagnostic utility. We propose a simple method to automate the pared down coding scheme, and ﬁnd that these automatic codes are of diagnostic utility. 
Focusing on applications for analyzing learner language which evaluate semantic appropriateness and accuracy, we collect data from a task which models some aspects of interaction, namely a picture description task (PDT). We parse responses to the PDT into dependency graphs with an an off-the-shelf parser, then use a decision tree to classify sentences into syntactic types and extract the logical subject, verb, and object, ﬁnding 92% accuracy in such extraction. The speciﬁc goal in this paper is to examine the challenges involved in extracting these simple semantic representations from interactive learner sentences. 
We describe the NUS Corpus of Learner English (NUCLE), a large, fully annotated corpus of learner English that is freely available for research purposes. The goal of the corpus is to provide a large data resource for the development and evaluation of grammatical error correction systems. Although NUCLE has been available for almost two years, there has been no reference paper that describes the corpus in detail. In this paper, we address this need. We describe the annotation schema and the data collection and annotation process of NUCLE. Most importantly, we report on an unpublished study of annotator agreement for grammatical error correction. Finally, we present statistics on the distribution of grammatical errors in the NUCLE corpus. 
 Cambridge English Cambridge English  Streets, 62 Hills Road  
We present the ﬁrst system developed for automated grading of high school essays written in Swedish. The system uses standard text quality indicators and is able to compare vocabulary and grammar to large reference corpora of blog posts and newspaper articles. The system is evaluated on a corpus of 1 702 essays, each graded independently by the student’s own teacher and also in a blind re-grading process by another teacher. We show that our system’s performance is fair, given the low agreement between the two human graders, and furthermore show how it could improve efﬁciency in a practical setting where one seeks to identify incorrectly graded essays. 
Vector Space Models (VSM) have been widely used in the language assessment ﬁeld to provide measurements of students’ vocabulary choices and content relevancy. However, training reference vectors (RV) in a VSM requires a time-consuming and costly human scoring process. To address this limitation, we applied unsupervised learning methods to reduce or even eliminate the human scoring step required for training RVs. Our experiments conducted on data from a non-native English speaking test suggest that the unsupervised topic clustering is better at selecting responses to train RVs than random selection. In addition, we conducted an experiment to totally eliminate the need of human scoring. Instead of using human rated scores to train RVs, we used used the machine-predicted scores from an automated speaking assessment system for training RVs. We obtained VSM-derived features that show promisingly high correlations to human-holistic scores, indicating that the costly human scoring process can be eliminated. Index Terms: Vector Space Model (VSM), speech assessment, unsupervised learning, document clustering 
We developed an approach to predict the proﬁciency level of Estonian language learners based on the CEFR guidelines. We performed learner classiﬁcation by studying morphosyntactic variation and lexical richness in texts produced by learners of Estonian as a second language. We show that our features which exploit the rich morphology of Estonian by focusing on the nominal case and verbal mood are useful predictors for this task. We also show that re-formulating the classiﬁcation problem as a multi-stage cascaded classiﬁcation improves the classiﬁcation accuracy. Finally, we also studied the effect of training data size on classiﬁcation accuracy and found that more training data is beneﬁcial in only some of the cases. 
This paper presents and evaluates approaches to automatically score the content correctness of spoken responses in a new language test for teachers of English as a foreign language who are non-native speakers of English. Most existing tests of English spoken proficiency elicit responses that are either very constrained (e.g., reading a passage aloud) or are of a predominantly spontaneous nature (e.g., stating an opinion on an issue). However, the assessment discussed in this paper focuses on essential speaking skills that English teachers need in order to be effective communicators in their classrooms and elicits mostly responses that fall in between these extremes and are moderately predictable. In order to automatically score the content accuracy of these spoken responses, we propose three categories of robust features, inspired from flexible text matching, n-grams, as well as string edit distance metrics. The experimental results indicate that even based on speech recognizer output, most of the feature correlations with human expert rater scores are in the range of r = 0.4 to r = 0.5, and further, that a scoring model for predicting human rater proficiency scores that includes our content features can significantly outperform a baseline without these features (r = 0.56 vs. r = 0.33). 
We present a system for automatically identifying the native language of a writer. We experiment with a large set of features and train them on a corpus of 9,900 essays written in English by speakers of 11 different languages. our system achieved an accuracy of 43% on the test data, improved to 63% with improved feature normalization. In this paper, we present the features used in our system, describe our experiments and provide an analysis of our results. 
This paper describes MITRE’s participation in the native language identiﬁcation (NLI) task at BEA-8. Our best effort performed at an accuracy of 82.6% in the eleven-way NLI task, placing it in a statistical tie with the best performing systems. We describe the variety of machine learning approaches that we explored, including Winnow, language modeling, logistic regression and maximum-entropy models. Our primary features were word and character n-grams. We also describe several ensemble methods that we employed for combining these base systems. 
We investigate data driven natural language generation under the constraints that all words must come from a ﬁxed vocabulary and a speciﬁed word must appear in the generated sentence, motivated by the possibility for automatic generation of language education exercises. We present fast and accurate approximations to the ideal rejection samplers for these constraints and compare various sentence level generative language models. Our best systems produce output that is with high frequency both novel and error free, which we validate with human and automatic evaluations. 
Native language identification (NLI) is the task to determine the native language of the author based on an essay written in a second language. NLI is often treated as a classification problem. In this paper, we use the TOEFL11 data set which consists of more data, in terms of the amount of essays and languages, and less biased across prompts, i.e., topics, of essays. We demonstrate that even using word level n-grams as features, and support vector machine (SVM) as a classifier can yield nearly 80% accuracy. We observe that the accuracy of a binary-based word level ngram representation (~80%) is much better than the performance of a frequency-based word level n-gram representation (~20%). Notably, comparable results can be achieved without removing punctuation marks, suggesting a very simple baseline system for NLI. 
This paper investigates the use of promptbased content features for the automated assessment of spontaneous speech in a spoken language proﬁciency assessment. The results show that single highest performing promptbased content feature measures the number of unique lexical types that overlap with the listening materials and are not contained in either the reading materials or a sample response, with a correlation of r = 0.450 with holistic proﬁciency scores provided by humans. Furthermore, linear regression scoring models that combine the proposed promptbased content features with additional spoken language proﬁciency features are shown to achieve competitive performance with scoring models using content features based on prescored responses. 
We introduce a cognitive framework for measuring reading comprehension that includes the use of novel summary writing tasks. We derive NLP features from the holistic rubric used to score the summaries written by students for such tasks and use them to design a preliminary, automated scoring system. Our results show that the automated approach performs well on summaries written by students for two different passages. 
This paper reports on a study of interannotator agreement (IAA) for a dependency annotation scheme designed for learner English. Reliably-annotated learner corpora are a necessary step for the development of POS tagging and parsing of learner language. In our study, three annotators marked several layers of annotation over different levels of learner texts, and they were able to obtain generally high agreement, especially after discussing the disagreements among themselves, without researcher intervention, illustrating the feasibility of the scheme. We pinpoint some of the problems in obtaining full agreement, including annotation scheme vagueness for certain learner innovations, interface design issues, and difﬁcult syntactic constructions. In the process, we also develop ways to calculate agreements for sets of dependencies. 
Our goal is to predict the ﬁrst language (L1) of English essays’s authors with the help of the TOEFL11 corpus where L1, prompts (topics) and proﬁciency levels are provided. Thus we approach this task as a classiﬁcation task employing machine learning methods. Out of key concepts of machine learning, we focus on feature engineering. We design features across all the L1 languages not making use of knowledge of prompt and proﬁciency level. During system development, we experimented with various techniques for feature ﬁltering and combination optimized with respect to the notion of mutual information and information gain. We trained four different SVM models and combined them through majority voting achieving accuracy 72.5%. 
This paper describes an effort to perform Native Language Identiﬁcation (NLI) using machine learning on a large amount of lexical features. The features were collected from sequences and collocations of bare word forms, sufﬁxes and character n-grams amounting to a feature set of several hundred thousand features. These features were used to train a linear Support Vector Machine (SVM) classiﬁer for predicting the native language category. 
We show that it is possible to learn to identify, with high accuracy, the native language of English test takers from the content of the essays they write. Our method uses standard text classiﬁcation techniques based on multiclass logistic regression, combining individually weak indicators to predict the most probable native language from a set of 11 possibilities. We describe the various features used for classiﬁcation, as well as the settings of the classiﬁer that yielded the highest accuracy. 
In automated speech assessment, adaptation of language models (LMs) to test questions is important to achieve high recognition accuracy However, for large-scale language tests, the ordinary supervised training, which uses an expensive and time-consuming manual transcription process, is hard to utilize for LM adaptation. In this paper, several LM adaptation methods that require either no manual transcription process or just a small amount of transcriptions have been evaluated. Our experiments suggest that these LM adaptation methods can allow us to obtain considerable recognition accuracy gain with no or low human transcription cost. Index Terms: language model adaptation, unsupervised training, Web as a corpus 
We present an experiment aimed at improving interpretation robustness of a tutorial dialogue system that relies on detailed semantic interpretation and dynamic natural language feedback generation. We show that we can improve overall interpretation quality by combining the output of a semantic interpreter with that of a statistical classiﬁer trained on the subset of student utterances where semantic interpretation fails. This improves on a previous result which used a similar approach but trained the classiﬁer on a substantially larger data set containing all student utterances. Finally, we discuss how the labels from the statistical classiﬁer can be integrated effectively with the dialogue system’s existing error recovery policies. 
We present a method for automatically detecting missing hyphens in English text. Our method goes beyond a purely dictionary-based approach and also takes context into account. We evaluate our model on artiﬁcially generated data as well as naturally occurring learner text. Our best-performing model achieves high precision and reasonable recall, making it suitable for inclusion in a system that gives feedback to language learners. 
This paper discusses preliminary work investigating the application of Machine Translation (MT) metrics toward the evaluation of translations written by human novice (student) translators. We describe a study in which we apply the metric TERp (Translation Edit Rate Plus) to a corpus of student-written translations from Spanish to English and compare the judgments of TERp against assessments provided by a translation instructor. 
We present a bootstrapping algorithm to automatically learn hashtags that convey emotion. Using the bootstrapping framework, we learn lists of emotion hashtags from unlabeled tweets. Our approach starts with a small number of seed hashtags for each emotion, which we use to automatically label tweets as initial training data. We then train emotion classiﬁers and use them to identify and score candidate emotion hashtags. We select the hashtags with the highest scores, use them to automatically harvest new tweets from Twitter, and repeat the bootstrapping process. We show that the learned hashtag lists help to improve emotion classiﬁcation performance compared to an N-gram classiﬁer, obtaining 8% microaverage and 9% macro-average improvements in F-measure. 
In this paper, we detail a method for domain speciﬁc, multi-category emotion recognition, based on human computation. We create an Amazon Mechanical Turk1 task that elicits emotion labels and phrase-emotion associations from the participants. Using the proposed method, we create an emotion lexicon, compatible with the 20 emotion categories of the Geneva Emotion Wheel. GEW is the ﬁrst computational resource that can be used to assign emotion labels with such a high level of granularity. Our emotion annotation method also produced a corpus of emotion labeled sports tweets. We compared the crossvalidated version of the lexicon with existing resources for both the positive/negative and multi-emotion classiﬁcation problems. We show that the presented domain-targeted lexicon outperforms the existing general purpose ones in both settings. The performance gains are most pronounced for the ﬁne-grained emotion classiﬁcation, where we achieve an accuracy twice higher than the benchmark.2 
The topic of sentiment analysis in text has been extensively studied in English for the past 30 years. An early, inﬂuential work by Cynthia Whissell, the Dictionary of Affect in Language (DAL), allows rating words along three dimensions: pleasantness, activation and imagery. Given the lack of such tools in Spanish, we decided to replicate Whissell’s work in that language. This paper describes the Spanish DAL, a knowledge base formed by more than 2500 words manually rated by humans along the same three dimensions. We evaluated its usefulness on two sentiment analysis tasks, which showed that the knowledge base managed to capture relevant information regarding the three affective dimensions. 
To avoid a sarcastic message being understood in its unintended literal meaning, in microtexts such as messages on Twitter.com sarcasm is often explicitly marked with the hashtag ‘#sarcasm’. We collected a training corpus of about 78 thousand Dutch tweets with this hashtag. Assuming that the human labeling is correct (annotation of a sample indicates that about 85% of these tweets are indeed sarcastic), we train a machine learning classiﬁer on the harvested examples, and apply it to a test set of a day’s stream of 3.3 million Dutch tweets. Of the 135 explicitly marked tweets on this day, we detect 101 (75%) when we remove the hashtag. We annotate the top of the ranked list of tweets most likely to be sarcastic that do not have the explicit hashtag. 30% of the top-250 ranked tweets are indeed sarcastic. Analysis shows that sarcasm is often signalled by hyperbole, using intensiﬁers and exclamations; in contrast, non-hyperbolic sarcastic messages often receive an explicit marker. We hypothesize that explicit markers such as hashtags are the digital extralinguistic equivalent of nonverbal expressions that people employ in live interaction when conveying sarcasm. 
Nowadays a large number of opinion reviews are posted on the Web. Such reviews are a very important source of information for customers and companies. The former rely more than ever on online reviews to make their purchase decisions and the latter to respond promptly to their clients’ expectations. Due to the economic importance of these reviews there is a growing trend to incorporate spam on such sites, and, as a consequence, to develop methods for opinion spam detection. In this paper we focus on the detection of deceptive opinion spam, which consists of ﬁctitious opinions that have been deliberately written to sound authentic, in order to deceive the consumers. In particular we propose a method based on the PU-learning approach which learns only from a few positive examples and a set of unlabeled data. Evaluation results in a corpus of hotel reviews demonstrate the appropriateness of the proposed method for real applications since it reached a f-measure of 0.84 in the detection of deceptive opinions using only 100 positive examples for training. 
This paper describes a novel approach for sexual predator detection in chat conversations based on sequences of classiﬁers. The proposed approach divides documents into three parts, which, we hypothesize, correspond to the different stages that a predator employs when approaching a child. Local classiﬁers are trained for each part of the documents and their outputs are combined by a chain strategy: predictions of a local classiﬁer are used as extra inputs for the next local classiﬁer. Additionally, we propose a ring-based strategy, in which the chaining process is iterated several times, with the goal of further improving the performance of our method. We report experimental results on the corpus used in the ﬁrst international competition on sexual predator identiﬁcation (PAN’12). Experimental results show that the proposed method outperforms a standard (global) classiﬁcation technique for the different settings we consider; besides the proposed method compares favorably with most methods evaluated in the PAN’12 competition. 
Though much research has been conducted on Subjectivity and Sentiment Analysis (SSA) during the last decade, little work has focused on Arabic. In this work, we focus on SSA for both Modern Standard Arabic (MSA) news articles and dialectal Arabic microblogs from Twitter. We showcase some of the challenges associated with SSA on microblogs. We adopted a random graph walk approach to extend the Arabic SSA lexicon using ArabicEnglish phrase tables, leading to improvements for SSA on Arabic microblogs. We used different features for both subjectivity and sentiment classiﬁcation including stemming, part-of-speech tagging, as well as tweet speciﬁc features. Our classiﬁcation features yield results that surpass Arabic SSA results in the literature. 
This article provides an in-depth research of machine learning methods for sentiment analysis of Czech social media. Whereas in English, Chinese, or Spanish this ﬁeld has a long history and evaluation datasets for various domains are widely available, in case of Czech language there has not yet been any systematical research conducted. We tackle this issue and establish a common ground for further research by providing a large humanannotated Czech social media corpus. Furthermore, we evaluate state-of-the-art supervised machine learning methods for sentiment analysis. We explore different pre-processing techniques and employ various features and classiﬁers. Moreover, in addition to our newly created social media dataset, we also report results on other widely popular domains, such as movie and product reviews. We believe that this article will not only extend the current sentiment analysis research to another family of languages, but will also encourage competition which potentially leads to the production of high-end commercial solutions. 
We discuss a tagging scheme to tag data for training information extraction models which can extract the features of a product/service and opinions about them from textual reviews, and which can be used across different domains with minimal adaptation. A simple tagging scheme results in a large number of domain dependent opinion phrases and impedes the usefulness of the trained models across domains. We show that by using minor modiﬁcations to this simple tagging scheme the number of domain dependent opinion phrases are reduced from 36% to 17%, which leads to models more useful across domains. 
We compare the performance of two lexiconbased sentiment systems – SentiStrength (Thelwall et al., 2012) and SO-CAL (Taboada et al., 2011) – on the two genres of newspaper text and tweets. While SentiStrength has been geared speciﬁcally toward short social-media text, SO-CAL was built for general, longer text. After the initial comparison, we successively enrich the SO-CAL-based analysis with tweet-speciﬁc mechanisms and observe that in some cases, this improves the performance. A qualitative error analysis then identiﬁes classes of typical problems the two systems have with tweets. 
Up until now most of the methods published for polarity classiﬁcation are applied to English texts. However, other languages on the Internet are becoming increasingly important. This paper presents a set of experiments on English and Spanish product reviews. Using a comparable corpus, a supervised method and two unsupervised methods have been assessed. Furthermore, a list of Spanish opinion words is presented as a valuable resource. 
In this paper we propose a method that uses corpora where phrases are annotated as Positive, Negative, Objective and Neutral, to achieve new sentiment resources involving words dictionaries with their associated polarity. Our method was created to build sentiment words inventories based on sentisemantic evidences obtained after exploring text with annotated sentiment polarity information. Through this process a graph-based algorithm is used to obtain auto-balanced values that characterize sentiment polarities well used on Sentiment Analysis tasks. To assessment effectiveness of the obtained resource, sentiment classification was made, achieving objective instances over 80%. 
We describe TWITA, the ﬁrst corpus of Italian tweets, which is created via a completely automatic procedure, portable to any other language. We experiment with sentiment analysis on two datasets from TWITA: a generic collection and a topic-speciﬁc collection. The only resource we use is a polarity lexicon, which we obtain by automatically matching three existing resources thereby creating the ﬁrst polarity database for Italian. We observe that albeit shallow, our simple system captures polarity distinctions matching reasonably well the classiﬁcation done by human judges, with differences in performance across polarity values and on the two sets. 
In this work, we attempt to detect sentencelevel subjectivity by means of two supervised machine learning approaches: a Fuzzy Control System and Adaptive Neuro-Fuzzy Inference System. Even though these methods are popular in pattern recognition, they have not been thoroughly investigated for subjectivity analysis. We present a novel “Pruned ICF Weighting Coefficient,” which improves the accuracy for subjectivity detection. Our feature extraction algorithm calculates a feature vector based on the statistical occurrences of words in a corpus without any lexical knowledge. For this reason, these machine learning models can be applied to any language; i.e., there is no lexical, grammatical, syntactical analysis used in the classification process. 
Sentiment analysis means to extract opinion of users from review documents. Sentiment classification using Machine Learning (ML) methods faces the problem of high dimensionality of feature vector. Therefore, a feature selection method is required to eliminate the irrelevant and noisy features from the feature vector for efficient working of ML algorithms. Rough Set Theory based feature selection method finds the optimal feature subset by eliminating the redundant features. In this paper, Rough Set Theory (RST) based feature selection method is applied for sentiment classification. A Hybrid feature selection method based on RST and Information Gain (IG) is proposed for sentiment classification. Proposed methods are evaluated on four standard datasets viz. Movie review, product (book, DVD and electronics) review dataset. Experimental results show that Hybrid feature selection method outperforms than other feature selection methods for sentiment classification. 
This paper presents a method for sentiment analysis speciﬁcally designed to work with Twitter data (tweets), taking into account their structure, length and speciﬁc language. The approach employed makes it easily extendible to other languages and makes it able to process tweets in near real time. The main contributions of this work are: a) the pre-processing of tweets to normalize the language and generalize the vocabulary employed to express sentiment; b) the use minimal linguistic processing, which makes the approach easily portable to other languages; c) the inclusion of higher order n-grams to spot modiﬁcations in the polarity of the sentiment expressed; d) the use of simple heuristics to select features to be employed; e) the application of supervised learning using a simple Support Vector Machines linear classiﬁer on a set of realistic data. We show that using the training models generated with the method described we can improve the sentiment classiﬁcation performance, irrespective of the domain and distribution of the test sets. 
This paper discusses user study outcomes with teachers who used Language MuseSM a webbased teacher professional development (TPD) application designed to enhance teachers’ linguistic awareness, and support teachers in the development of language-based instructional scaffolding (support) for their English language learners (ELL). System development was grounded in literature that supports the notion that instruction incorporating language support for ELLs can improve their accessibility to content-area classroom texts –in terms of access to content, and improvement of language skills. Measurement outcomes of user piloting with teachers in a TPD setting indicated that application use increased teachers' linguistic knowledge and awareness, and their ability to develop appropriate language-based instruction for ELLs. Instruction developed during the pilot was informed by the application’s linguistic analysis feedback, provided by natural language processing capabilities in Language Muse. 
Persons affected by Autism Spectrum Disorders (ASD) present impairments in social interaction. A signiﬁcant percentile of them have inadequate reading comprehension skills. In the ongoing FIRST project we build a multilingual tool called Open Book that helps the ASD people to better understand the texts. The tool applies a series of automatic transformations to user documents to identify and remove the reading obstacles to comprehension. We focus on three semantic components: an Image component that retrieves images for the concepts in the text, an idiom detection component and a topic model component. Moreover, we present the personalization component that adapts the system output to user preferences. 
One of the populations that often needs some form of help to read everyday documents is non-native speakers. This paper discusses aid at the word and word string levels and focuses on the possibility of using translation and simplification. Seen from the perspective of the non-native as an ever-learning reader, we show how translation may be of more harm than help in understanding and retaining the meaning of a word while simplification holds promise. We conclude that if reading everyday documents can be considered as a learning activity as well as a practical necessity, then our study reinforces the arguments that defend the use of simplification to make documents that non-natives need to read more accessible. 
We present a computational notion of Lexical Tightness that measures global cohesion of content words in a text. Lexical tightness represents the degree to which a text tends to use words that are highly inter-associated in the language. We demonstrate the utility of this measure for estimating text complexity as measured by US school grade level designations of texts. Lexical tightness strongly correlates with grade level in a collection of expertly rated reading materials. Lexical tightness captures aspects of prose complexity that are not covered by classic readability indexes, especially for literary texts. We also present initial findings on the utility of this measure for automated estimation of complexity for poetry. 
The purpose of this paper is to motivate and describe a system that simpliﬁes numerical expression in texts, along with an evaluation study in which experts in numeracy and literacy assessed the outputs of this system. We have worked with a collection of newspaper articles with a signiﬁcant number of numerical expressions. The results are discussed in comparison to conclusions obtained from a prior empirical survey. 
Many existing approaches for measuring text complexity tend to overestimate the complexity levels of informational texts while simultaneously underestimating the complexity levels of literary texts. We present a two-stage estimation technique that successfully addresses this problem. At Stage 1, each text is classified into one or another of three possible genres: informational, literary or mixed. Next, at Stage 2, a complexity score is generated for each text by applying one or another of three possible prediction models: one optimized for application to informational texts, one optimized for application to literary texts, and one optimized for application to mixed texts. Each model combines lexical, syntactic and discourse features, as appropriate, to best replicate human complexity judgments. We demonstrate that resulting text complexity predictions are both unbiased, and highly correlated with classifications provided by experienced educators. 
 2 Background  There are cultural barriers to collaborative effort between literary scholars and computational linguists. In this work, we discuss some of these problems in the context of our ongoing research project, an exploration of free indirect discourse in Virginia Woolf’s To The Lighthouse, ultimately arguing that the advantages of taking each ﬁeld out of its “comfort zone” justiﬁes the inherent difﬁculties. 
This work presents a novel method for recognizing and extracting classical Arabic poems found in textual sources. The method utilizes the basic classical Arabic poem features such as structure, rhyme, writing style, and word usage. The proposed method achieves a precision of 96.94% while keeping a high recall value at 92.24%. The method was also used to build a prototype search engine for classical Arabic poems. 
Scholars of Chinese literature note that China’s tumultuous literary history in the 20th century centered around the uncomfortable tensions between tradition and modernity. In this corpus study, we develop and automatically extract three features to show that the classical character of Chinese poetry decreased across the century. We also ﬁnd that Taiwan poets constitute a surprising exception to the trend, demonstrating an unusually strong connection to classical diction in their work as late as the ’50s and ’60s. 
 This paper describes the usage of Natural Language Processing tools, mostly probabilistic topic modeling, to study semantics (word correlations) in a collection of Persian poems consisting of roughly 18k poems from 30 different poets. For this study, we put a lot of effort in the preprocessing and the development of a large scope lexicon supporting both modern and ancient Persian. In the analysis step, we obtained very interesting and meaningful results regarding the correlation between poets and topics, their evolution through time, as well as the correlation between the topics and the metre used in the poems. This work should thus provide valuable results to literature researchers, especially for those working on stylistics or comparative literature. 
This position paper argues the need for a comprehensive corpus of online book responses. Responses to books (in traditional reviews, book blogs, on booksellers’ sites, etc.) are important for understanding how readers understand literature and how literary works become popular. A sufficiently large, varied and representative corpus of online responses to books will facilitate research into these processes. This corpus should include context information about the responses and should remain open to additional material. Based on a pilot study for the creation of a corpus of Dutch online book response, the paper shows how linguistic tools can find differences in word usage between responses from various sites. They can also reveal response type by clustering responses based on usage of either words or their POS-tags, and can show the sentiments expressed in the responses. LSA-based similarity between book fragments and response may be able to reveal the book fragments that most affected readers. The paper argues that a corpus of book responses can be an important instrument for research into reading behavior, reader response, book reviewing and literary appreciation. 
T.S. Eliot’s modernist poem The Waste Land is often interpreted as collection of voices which appear multiple times throughout the text. Here, we investigate whether we can automatically cluster existing segmentations of the text into coherent, expert-identiﬁed characters. We show that clustering The Waste Land is a fairly difﬁcult task, though we can do much better than random baselines, particularly if we begin with a good initial segmentation. 
This work performs some basic research upon topical poetry segmentation in a pilot study designed to test some initial assumptions and methodologies. Nine segmentations of the poem titled Kubla Khan (Coleridge, 1816, pp. 55-58) are collected and analysed, producing low but comparable inter-coder agreement. Analyses and discussions of these codings focus upon how to improve agreement and outline some initial results on the nature of topics in this poem. 
This paper discusses the concept of semantic repetition in literary texts, that is, the recurrence of elements of meaning, possibly in the absence of repeated formal elements. A typology of semantic repetition is presented, as well as a framework for analysis based on the use of threaded Directed Acyclic Graphs. This model is applied to the script for the movie Groundhog Day. It is shown ﬁrst that semantic repetition presents a number of traits not found in the case of the repetition of formal elements (letters, words, etc.). Consideration of the threaded DAG also brings to light several classes of semantic repetition, between individual nodes of a DAG, between subDAGs within a larger DAG, and between structures of subDAGs, both within and across texts. The model presented here provides a basis for the detailed study of additional literary texts at the semantic level and illustrates the tractability of the formalism used for analysis of texts of some considerable length and complexity. 
We present CoocViewer, a graphical analysis tool for the purpose of quantitative literary analysis, and demonstrate its use on a corpus of crime novels. The tool displays words, their signiﬁcant co-occurrences, and contains a new visualization for significant concordances. Contexts of words and co-occurrences can be displayed. After reviewing previous research and current challenges in the newly emerging ﬁeld of quantitative literary research, we demonstrate how CoocViewer allows comparative research on literary corpora in a project-speciﬁc study, and how we can conﬁrm or enhance our hypotheses through quantitative literary analysis. 
 TOP  Stylometric analysis of prose is typically limited to classiﬁcation tasks such as authorship attribution. Since the models used are typically black boxes, they give little insight into the stylistic diﬀerences they detect. In this paper, we characterize two prose genres syntactically: chick lit (humorous novels on the challenges of being a modern-day urban female) and high literature. First, we develop a top-down computational method based on existing literary-linguistic theory. Using an oﬀ-the-shelf parser we obtain syntactic structures for a Dutch corpus of novels and measure the distribution of sentence types in chick-lit and literary novels. The results show that literature contains more complex (subordinating) sentences than chick lit. Secondly, a bottom-up analysis is made of speciﬁc morphological and syntactic features in both genres, based on the parser’s output. This shows that the two genres can be distinguished along certain features. Our results indicate that detailed insight into stylistic diﬀerences can be obtained by combining computational linguistic analysis with literary theory. 
We are interested in the task of image annotation using noisy natural text as training data. An image and its caption convey different information, but are generated by the same underlying concepts. In this paper, we learn latent mixtures of topics that generate image and product descriptions on shopping websites by adapting a topic model for multilingual data (Mimno et al., 2009). We use the trained model to annotate test images without corresponding text. We capture visual properties such as color, texture, shape, and orientation by computing low-level image features, and measure the contribution of each type of visual feature towards the accuracy of the model. Our model signiﬁcantly outperforms both a competitive baseline and a previous topic model-based system. 
We present a holistic data-driven technique that generates natural-language descriptions for videos. We combine the output of state-ofthe-art object and activity detectors with “realworld” knowledge to select the most probable subject-verb-object triplet for describing a video. We show that this knowledge, automatically mined from web-scale text corpora, enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identiﬁcation. Unlike previous methods, our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus. We evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61% of the time. 
We propose a method to learn succinct hierarchical linguistic descriptions of visual datasets, which allow for improved navigation efﬁciency in image collections. Classic exploratory data analysis methods, such as agglomerative hierarchical clustering, only provide a means of obtaining a tree-structured partitioning of the data. This requires the user to go through the images ﬁrst, in order to reveal the semantic relationship between the different nodes. On the other hand, in this work we propose to learn a hierarchy of linguistic descriptions, referred to as attributes, which allows for a textual description of the semantic content that is captured by the hierarchy. Our approach is based on a generative model, which relates the attribute descriptions associated with each node, and the node assignments of the data instances, in a probabilistic fashion. We furthermore use a nonparametric Bayesian prior, known as the tree-structured stick breaking process, which allows for the structure of the tree to be learned in an unsupervised fashion. We also propose appropriate performance measures, and demonstrate superior performance compared to other hierarchical clustering algorithms. 
In this paper we present ongoing work for the creation of a linguistically-based system for event coreference. We assume that this task requires deep understanding of text and that statistically-based methods, both supervised and unsupervised are inadequate. The reason for this choice is due to the fact that event coreference can only take place whenever argumenthood is properly computed. It is a fact that in many cases, arguments of predicates are implicit and thus linguistically unexpressed. This prevents training to produce sensible results. We also assume that spatiotemporal locations need to be taken into account and this is also very often left implicit. We used GETARUNS system to develop the coreference system which works on the basis of the discourse model and the automatically annotated markables. We present data from the analysis, both on unexpressed implicit arguments and the description of the coreference algorithm. 
This paper introduces GAF, a grounded annotation framework to represent events in a formal context that can represent information from both textual and extra-textual sources. GAF makes a clear distinction between mentions of events in text and their formal representation as instances in a semantic layer. Instances are represented by RDF compliant URIs that are shared across different research disciplines. This allows us to complete textual information with external sources and facilitates reasoning. The semantic layer can integrate any linguistic information and is compatible with previous event representations in NLP. Through a use case on earthquakes in Southeast Asia, we demonstrate GAF ﬂexibility and ability to reason over events with the aid of extra-linguistic resources. 
This paper describes an approach for investigating the representation of events and their distribution in a corpus. We collect and analyze statistics about subject-verb-object triplets and their content, which helps us compare corpora belonging to the same domain but to different genre/text type. We argue that event structure is strongly related to the genre of the corpus, and propose statistical properties that are able to capture these genre differences. The results obtained can be used for the improvement of Information Extraction. 
I present a set of functional requirements for a speculative tool informing users about events in historical discourse, in order to demonstrate what these requirements imply about how we should deﬁne and represent historical events. The functions include individuation, selection, and contextualization of events. I conclude that a tool providing these functions would need events to be deﬁned and represented as features of discourses about the world rather than objectively existing things in the world. 
Understanding the event structure of sentences and whole documents is an important step in being able to extract meaningful information from the text. Our task is the identification of phenotypes, specifically, pneumonia, from clinical narratives. In this paper, we consider the importance of identifying the change of state for events, in particular, events that measure and compare multiple states across time. Change of state is important to the clinical diagnosis of pneumonia; in the example “there are bibasilar opacities that are unchanged”, the presence of bibasilar opacities alone may suggest pneumonia, but not when they are unchanged, which suggests the need to modify events with change of state information. Our corpus is comprised of chest Xray reports, where we find many descriptions of change of state comparing the volume and density of the lungs and surrounding areas. We propose an annotation schema to capture this information as a tuple of <location, attribute, value, change-of-state, time-reference>. 
We explore improving parsing social media and other web data by altering the input data, namely by normalizing web text, and by revising output parses. We ﬁnd that text normalization improves performance, though spell checking has more of a mixed impact. We also ﬁnd that a very simple tree reviser based on grammar comparisons performs slightly but signiﬁcantly better than the baseline and well outperforms a machine learning model. The results also demonstrate that, more than the size of the training data, the goodness of ﬁt of the data has a great impact on the parser.  We advance this line of research by investigating adapting parsing to social media and other web data. Speciﬁcally, we focus on two areas: 1) We compare the impact of various text normalization techniques on parsing web data; and 2) we explore parse revision techniques for dependency parsing web data to improve the ﬁt of the grammar learned by the parser. One of the major problems in processing social media data is the common usage of non-standard terms (e.g., kawaii, a Japanese-borrowed net term for ‘cute’), ungrammatical and (intentionally) misspelled text (e.g., cuttie), emoticons, and short posts with little contextual information, as exempliﬁed in (1).1  
Does phonological variation get transcribed into social media text? This paper investigates examples of the phonological variable of consonant cluster reduction in Twitter. Not only does this variable appear frequently, but it displays the same sensitivity to linguistic context as in spoken language. This suggests that when social media writing transcribes phonological properties of speech, it is not merely a case of inventing orthographic transcriptions. Rather, social media displays inﬂuence from structural properties of the phonological system. 
Although the ideal length of summaries differs greatly from topic to topic on Twitter, previous work has only generated summaries of a pre-ﬁxed length. In this paper, we propose an event-graph based method using information extraction techniques that is able to create summaries of variable length for different topics. In particular, we extend the Pageranklike ranking algorithm from previous work to partition event graphs and thereby detect ﬁnegrained aspects of the event to be summarized. Our preliminary results show that summaries created by our method are more concise and news-worthy than SumBasic according to human judges. We also provide a brief survey of datasets and evaluation design used in previous work to highlight the need of developing a standard evaluation for automatic tweet summarization task. 
More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic Natural Language Processing resources such as news, highly social dialogue is frequent in social media, making it a challenging context for NLP. This paper tests a bootstrapping method, originally proposed in a monologic domain, to train classiﬁers to identify two different types of subjective language in dialogue: sarcasm and nastiness. We explore two methods of developing linguistic indicators to be used in a ﬁrst level classiﬁer aimed at maximizing precision at the expense of recall. The best performing classiﬁer for the ﬁrst phase achieves 54% precision and 38% recall for sarcastic utterances. We then use general syntactic patterns from previous work to create more general sarcasm indicators, improving precision to 62% and recall to 52%. To further test the generality of the method, we then apply it to bootstrapping a classiﬁer for nastiness dialogic acts. Our ﬁrst phase, using crowdsourced nasty indicators, achieves 58% precision and 49% recall, which increases to 75% precision and 62% recall when we bootstrap over the ﬁrst level with generalized syntactic patterns. 
In this paper, we describe a novel approach to automatically detecting and tracking discussion dynamics in Internet social media by focusing on attitude modeling of topics. We characterize each participant’s attitude towards topics as Topical Positioning, employ Topical Positioning Map to represent the positions of participants with respect to each other and track attitude shifts over time. We also discuss how we used participants’ attitudes towards system-detected meso-topics to reflect their attitudes towards the overall topic of conversation. Our approach can work across different types of social media, such as Twitter discussion and online chat room. In this article, we show results on Twitter data. 
We perform a series of 3-class sentiment classiﬁcation experiments on a set of 2,624 tweets produced during the run-up to the Irish General Elections in February 2011. Even though tweets that have been labelled as sarcastic have been omitted from this set, it still represents a difﬁcult test set and the highest accuracy we achieve is 61.6% using supervised learning and a feature set consisting of subjectivity-lexicon-based scores, Twitterspeciﬁc features and the top 1,000 most discriminative words. This is superior to various naive unsupervised approaches which use subjectivity lexicons to compute an overall sentiment score for a <tweet,political party> pair. 
This paper presents preliminary results of using authorship attribution methods for the detection of sockpuppeteering in Wikipedia. Sockpuppets are fake accounts created by malicious users to bypass Wikipedia’s regulations. Our dataset is composed of the comments made by the editors on the talk pages. To overcome the limitations of the short lengths of these comments, we use an voting scheme to combine predictions made on individual user entries. We show that this approach is promising and that it can be a viable alternative to the current human process that Wikipedia uses to resolve suspected sockpuppet cases. 
We investigate the task of detecting reliable statements about food-health relationships from natural language texts. For that purpose, we created a specially annotated web corpus from forum entries discussing the healthiness of certain food items. We examine a set of task-speciﬁc features (mostly) based on linguistic insights that are instrumental in ﬁnding utterances that are commonly perceived as reliable. These features are incorporated in a supervised classiﬁer and compared against standard features that are widely used for various tasks in natural language processing, such as bag of words, part-of speech and syntactic parse information. 
While the automatic translation of tweets has already been investigated in different scenarios, we are not aware of any attempt to translate tweets created by government agencies. In this study, we report the experimental results we obtained when translating 12 Twitter feeds published by agencies and organizations of the government of Canada, using a state-ofthe art Statistical Machine Translation (SMT) engine as a black box translation device. We mine parallel web pages linked from the URLs contained in English-French pairs of tweets in order to create tuning and training material. For a Twitter feed that would have been otherwise difﬁcult to translate, we report signiﬁcant gains in translation quality using this strategy. Furthermore, we give a detailed account of the problems we still face, such as hashtag translation as well as the generation of tweets of legal length. 
This paper describes our approach to managing multiword expressions in Sentitext, a linguistically-motivated, lexicon-based Sentiment Analysis (SA) system for Spanish whose performance is largely determined by its coverage of MWEs. We defend the view that multiword constructions play a fundamental role in lexical Sentiment Analysis, in at least three ways. First, a significant proportion conveys semantic orientation; second, being units of meaning, their relative weight to the calculated overall sentiment rating of texts needs to be accounted for as such, rather than the number of component lexical units; and, third, many MWEs contain individual words that carry a given polarity, which may or may not be that of the phrase as a whole. As a result, successful lexiconbased SA calls for appropriate management of MWEs.1 
This paper introduces PersPred, the ﬁrst manually elaborated syntactic and semantic database for Persian Complex Predicates (CPs). Beside their theoretical interest, Persian CPs constitute an important challenge in Persian lexicography and for NLP. The ﬁrst delivery, PersPred 11, contains 700 CPs, for which 22 ﬁelds of lexical, syntactic and semantic information are encoded. The semantic classiﬁcation PersPred provides allows to account for the productivity of these combinations in a way which does justice to their compositionality without overlooking their idiomaticity. 
Practitioners of English Natural Language Processing often feel fortunate because their tokens are clearly marked by spaces on either side. However, the spaces can be quite deceptive, since they ignore the boundaries of multi-word expressions, such as noun-noun compounds, verb particle constructions, light verb constructions and constructions from Construction Grammar, e.g., caused-motion constructions and resultatives. Correctly identifying and handling these types of expressions can be quite challenging, even from the viewpoint of manual annotation. This talk will review the pervasive nature of these constructions, touching on Arabic and Hindi as well as English. Using several illustrative examples from newswire and medical informatics, current best practices for annotation and automatic identiﬁcation will be described, with an emphasis on contributions from predicate argument structures.  
Human ratings are an important source for evaluating computational models that predict compositionality, but like many data sets of human semantic judgements, are often fraught with uncertainty and noise. However, despite their importance, to our knowledge there has been no extensive look at the effects of cleansing methods on human rating data. This paper assesses two standard cleansing approaches on two sets of compositionality ratings for German noun-noun compounds, in their ability to produce compositionality ratings of higher consistency, while reducing data quantity. We ﬁnd (i) that our ratings are highly robust against aggressive ﬁltering; (ii) Z-score ﬁltering fails to detect unreliable item ratings; and (iii) Minimum Subject Agreement is highly effective at detecting unreliable subjects. 
Cliche´s, as trite expressions, are predominantly multiword expressions, but not all MWEs are cliche´s. We conduct a preliminary examination of the problem of determining how cliche´d a text is, taken as a whole, by comparing it to a reference text with respect to the proportion of more-frequent n-grams, as measured in an external corpus. We ﬁnd that more-frequent n-grams are over-represented in cliche´d text. We apply this ﬁnding to the “Eumaeus” episode of James Joyce’s novel Ulysses, which literary scholars believe to be written in a deliberately cliche´d style. 
This paper presents a supervised machine learning approach that uses a machine learning algorithm called Random Forest for recognition of Bengali noun-noun compounds as multiword expression (MWE) from Bengali corpus. Our proposed approach to MWE recognition has two steps: (1) extraction of candidate multi-word expressions using Chunk information and various heuristic rules and (2) training the machine learning algorithm to recognize a candidate multi-word expression as Multi-word expression or not. A variety of association measures, syntactic and linguistic clues are used as features for identifying MWEs. The proposed system is tested on a Bengali corpus for identifying noun-noun compound MWEs from the corpus. 
This paper presents an algorithm that allows the user to issue a query pattern, collects multi-word expressions (MWEs) that match the pattern, and then ranks them in a uniform fashion. This is achieved by quantifying the strength of all possible relations between the tokens and their features in the MWEs. The algorithm collects the frequency of morphological categories of the given pattern on a uniﬁed scale in order to choose the stable categories and their values. For every part of speech, and for all of its categories, we calculate a normalized Kullback-Leibler divergence between the category’s distribution in the pattern and its distribution in the corpus overall. Categories with the largest divergence are considered to be the most signiﬁcant. The particular values of the categories are sorted according to a frequency ratio. As a result, we obtain morphosyntactic proﬁles of a given pattern, which includes the most stable category of the pattern, and their values. 
High frequency can convert a word sequence into a multiword expression (MWE), i.e., a collocation. In this paper, we use collocations as well as syntactically-flexible, lexicalized phrases to analyze ‘job specification documents’ (a kind of corporate technical document) for subsequent acquisition of automated knowledge elicitation. We propose the definition of structural and functional patterns of specific corporate documents by analyzing the contexts and sections in which the expression occurs. Such patterns and its automated processing are the basis for identifying organizational domain knowledge and business information which is used later for the first instances of requirement elicitation processes in software engineering. 
Based on a lexicon of Portuguese MWE, this presentation focuses on an ongoing work that aims at the creation of a typology that describes these expressions taking into account their semantic, syntactic and pragmatic properties. We also plan to annotate each MWEentry in the mentioned lexicon according to the information obtained from that typology. Our objective is to create a valuable resource, which will allow for the automatic identification MWE in running text and for a deeper understanding of these expressions in their context. 
 noun, however, se has six uses:  A challenging topic in Portuguese language processing is the multifunctional and ambiguous use of the clitic pronoun se, which impacts NLP tasks such as syntactic parsing, semantic role labeling and machine translation. Aiming to give a step forward towards the automatic disambiguation of se, our study focuses on the identiﬁcation of pronominal verbs, which correspond to one of the six uses of se as a clitic pronoun, when se is considered a CONSTITUTIVE PARTICLE of the verb lemma to which it is bound, as a multiword unit. Our strategy to identify such verbs is to analyze the results of a corpus search and to rule out all the other possible uses of se. This process evidenced the features needed in a computational lexicon to automatically perform the disambiguation task. The availability of the resulting lexicon of pronominal verbs on the web enables their inclusion in broader lexical resources, such as the Portuguese versions of Wordnet, Propbank and VerbNet. Moreover, it will allow the revision of parsers and dictionaries already in use. 
We deal with syntactic identiﬁcation of occurrences of multiword expression (MWE) from an existing dictionary in a text corpus. The MWEs we identify can be of arbitrary length and can be interrupted in the surface sentence. We analyse and compare three approaches based on linguistic analysis at a varying level, ranging from surface word order to deep syntax. The evaluation is conducted using two corpora: the Prague Dependency Treebank and Czech National Corpus. We use the dictionary of multiword expressions SemLex, that was compiled by annotating the Prague Dependency Treebank and includes deep syntactic dependency trees of all MWEs. 
We present an experimental study of how different features help measuring the idiomaticity of noun+verb (NV) expressions in Basque. After testing several techniques for quantifying the four basic properties of multiword expressions or MWEs (institutionalization, semantic non-compositionality, morphosyntactic ﬁxedness and lexical ﬁxedness), we test different combinations of them for classiﬁcation into idioms and collocations, using Machine Learning (ML) and feature selection. The results show the major role of distributional similarity, which measures compositionality, in the extraction and classiﬁcation of MWEs, especially, as expected, in the case of idioms. Even though cooccurrence and some aspects of morphosyntactic ﬂexibility contribute to this task in a more limited measure, ML experiments make beneﬁt of these sources of knowledge, allowing to improve the results obtained using exclusively distributional similarity features. 
The linguistic annotation of noun-verb complex predicates (also termed as light verb constructions) is challenging as these predicates are highly productive in Hindi. For semantic role labelling, each argument of the noun-verb complex predicate must be given a role label. For complex predicates, frame ﬁles need to be created specifying the role labels for each noun-verb complex predicate. The creation of frame ﬁles is usually done manually, but we propose an automatic method to expedite this process. We use two resources for this method: Hindi PropBank frame ﬁles for simple verbs and the annotated Hindi Treebank. Our method perfectly predicts 65% of the roles in 3015 unique noun-verb combinations, with an additional 22% partial predictions, giving us 87% useful predictions to build our annotation resource. 
Grading is a primary cognitive operation that has an important expressive function. Information on degree is grammatically relevant and constitutes what Lazard (2006) calls a primary domain of grammaticalization: According to typological studies (Cuzzolin & Lehmann, 2004), many languages of the world have in fact at their disposal multiple grammatical devices to express gradation. In Italian, the class of superlativizing structures alternative to the morphological superlative is very rich and consists, among others, of adverbs of degree, focalizing adverbs and prototypical comparisons. This contribution deals with a particular analytic structure of superlative in Italian that is still neglected in the literature. This is what we will call Constructional Intensifying Adjectives (CIAs), adjectives which modify the intensity of other adjectives on the basis of regular semantic patterns, thus giving rise to multiword superlative constructions of the type: ADJX+ADJINTENS. A comparative quantitative corpus analysis demonstrates that this strategy, though paradigmatically limited, is nonetheless widely exploited: From a distributional point of view, some of these CIAs only combine with one or a few adjectives and form MWEs that appear to be completely lexicalized, while some others modify wider classes of adjectives thus displaying a certain degree of productivity. 
Multiword expressions as they appear as nominal compounds, collocational forms, and idioms are now leveraged in educational technology in assessment and instruction contexts. The talk will focus on how multiword expression identiﬁcation is used in different kinds of educational applications, including automated essay evaluation, and teacher professional development in curriculum development for English language learners. Recent approaches developed to resolve polarity for noun-noun compounds in a sentiment system being designed to handle evaluation of argumentation (sentiment) in testtaker writing (Beigman-Klebanov, Burstein, and Madnani, to appear) will also be described.  
This paper reports our ongoing project for constructing an English multiword expression (MWE) dictionary and NLP tools based on the developed dictionary. We extracted functional MWEs from the English part of Wiktionary, annotated the Penn Treebank (PTB) with MWE information, and conducted POS tagging experiments. We report how the MWE annotation is done on PTB and the results of POS and MWE tagging experiments. 
This paper evaluates four metaphor identiﬁcation systems on the 200,000 word VU Amsterdam Metaphor Corpus, comparing results by genre and by sub-class of metaphor. The paper then compares the rate of agreement between the systems for each genre and sub-class. Each of the identiﬁcation systems is based, explicitly or implicitly, on a theory of metaphor which hypothesizes that certain properties are essential to metaphor-inlanguage. The goal of this paper is to see what the success or failure of these systems can tell us about the essential properties of metaphorin-language. The success of the identiﬁcation systems varies signiﬁcantly across genres and sub-classes of metaphor. At the same time, the different systems achieve similar success rates on each even though they show low agreement among themselves. This is taken to be evidence that there are several sub-types of metaphor-in-language and that the ideal metaphor identiﬁcation system will ﬁrst deﬁne these sub-types and then model the linguistic properties which can distinguish these sub-types from one another and from nonmetaphors. 
This article discusses metaphor annotation in a corpus of argumentative essays written by test-takers during a standardized examination for graduate school admission. The quality of argumentation being the focus of the project, we developed a metaphor annotation protocol that targets metaphors that are relevant for the writer’s arguments. The reliability of the protocol is κ=0.58, on a set of 116 essays (the total of about 30K content-word tokens). We found a moderate-to-strong correlation (r=0.51-0.57) between the percentage of metaphorically used words in an essay and the writing quality score. We also describe encouraging ﬁndings regarding the potential of metaphor identiﬁcation to contribute to automated scoring of essays.  crafting – more conventionalized ones, metaphors that we “live by” according to Lakoff and Johnson’s (1980) famous tenet – subtly organize our thinking and language production in culturally coherent ways. For an example of a vivid metaphor that helps organize the essay, consider an essay on the relationship between arts and government funding thereof (see example 1). The author’s image of a piece of art as a slippery object that escapes its captor’s grip as a parallel to the relationship between an artist and his or her patron/ﬁnancier is a powerful image that provides a framework for the author’s examples (in the preceding paragraph, Chaucer is discussed as a clever and subversive writer for his patron) and elaborations (means of “slippage”, like veiled imagery, multiple meanings, etc).  
What influences the likelihood that a word will be used metaphorically? We tested whether the likelihood of metaphorical use is related to the relationality of a word’s meaning. Relational words name relations between entities. We predicted that relational words, such as verbs (e.g., speak) and relational nouns (e.g., marriage) would be more likely to be used metaphorically than words that name entities (e.g., item). In two experiments, we collected expert ratings of metaphoricity for uses of verbs, relational nouns, and entity nouns collected from a corpus search. As predicted, uses of relational words were rated as more metaphorical than uses of entity words. We discuss how these findings could inform NLP models of metaphor. 
Metaphor is a pervasive feature of human language that enables us to conceptualize and communicate abstract concepts using more concrete terminology. Unfortunately, it is also a feature that serves to confound a computer’s ability to comprehend natural human language. We present a method to detect linguistic metaphors by inducing a domainaware semantic signature for a given text and compare this signature against a large index of known metaphors. By training a suite of binary classiﬁers using the results of several semantic signature-based rankings of the index, we are able to detect linguistic metaphors in unstructured text at a signiﬁcantly higher precision as compared to several baseline approaches. 
The paper presents an experimental algorithm to detect conventionalized metaphors implicit in the lexical data in a resource like WordNet, where metaphors are coded into the senses and so would never be detected by any algorithm based on the violation of preferences, since there would always be a constraint satisfied by such senses. We report an implementation of this algorithm, which was implemented first the preference constraints in VerbNet. We then derived in a systematic way a far more extensive set of constraints based on WordNet glosses, and with this data we reimplemented the detection algorithm and got a substantial improvement in recall. We suggest that this technique could contribute to improve the performance of existing metaphor detection strategies that do not attempt to detect conventionalized metaphors. The new WordNet-derived data is of wider significance because it also contains adjective constraints, unlike any existing lexical resource, and can be applied to any language with a semantic parser (and WN) for it. 
We present the CSF - Common Semantic Features method for metaphor detection. This method has two distinguishing characteristics: it is cross-lingual and it does not rely on the availability of extensive manually-compiled lexical resources in target languages other than English. A metaphor detecting classiﬁer is trained on English samples and then applied to the target language. The method includes procedures for obtaining semantic features from sentences in the target language. Our experiments with Russian and English sentences show comparable results, supporting our hypothesis that a CSF-based classiﬁer can be applied across languages. We obtain state-ofthe-art performance in both languages. 
A metaphor is a ﬁgure of speech that refers to one concept in terms of another, as in “He is such a sweet person”. Metaphors are ubiquitous and they present NLP with a range of challenges for WSD, IE, etc. Identifying metaphors is thus an important step in language understanding. However, since almost any word can serve as a metaphor, they are impossible to list. To identify metaphorical use, we assume that it results in unusual semantic patterns between the metaphor and its dependencies. To identify these cases, we use SVMs with tree-kernels on a balanced corpus of 3872 instances, created by bootstrapping from available metaphor lists.1 We outperform two baselines, a sequential and a vectorbased approach, and achieve an F1-score of 0.75. 
We aim to investigate cross-cultural patterns of thought through cross-linguistic investigation of the use of metaphor. As a first step, we produce a system for locating instances of metaphor in English and Spanish text. In contrast to previous work which relies on resources like syntactic parsing and WordNet, our system is based on LDA topic modeling, enabling its application even to low-resource languages, and requires no labeled data. We achieve an F-score of 59% for English. 
This article describes our novel approach to the automated detection and analysis of metaphors in text. We employ robust, quantitative language processing to implement a system prototype combined with sound social science methods for validation. We show results in 4 different languages and discuss how our methods are a significant step forward from previously established techniques of metaphor identification. We use Topical Structure and Tracking, an Imageability score, and innovative methods to build an effective metaphor identification system that is fully automated and performs well over baseline. 
This work presents the tentative version of the protocol designed for annotation of a Russian metaphor corpus using the rapid annotation tool BRAT. The first part of the article is devoted to the procedure of "shallow" annotation in which metaphor-related words are identified according to a slightly modified version of the MIPVU procedure. The paper presents the results of two reliability tests and the measures of inter-annotator agreement obtained in them. Further on, the article gives a brief account of the linguistic problems that were encountered in adapting MIPVU to Russian. The rest of the first part describes the classes of metaphorrelated words and the rules of their annotation with BRAT. The examples of annotation show how the visualization functionalities of BRAT allow the researcher to describe the multifaceted nature of metaphor related words and the complexity of their relations. The second part of the paper speaks about the annotation of conceptual metaphors (the "deep" annotation), where formulations of conceptual metaphors are inferred from the basic and contextual meanings of metaphor-related words from the "shallow" annotation, which is expected to make the metaphor formulation process more controllable. 
The statistical machine translation (SMT) system heavily depends on the sentence aligned parallel corpus and the target language model. This paper points out some of the core issues on switching a language script and its repercussion in the phrase based statistical machine translation system development. The present task reports on the outcome of EnglishManipuri language pair phrase based SMT task on two aspects – a) Manipuri using Bengali script, b) Manipuri using transliterated Meetei Mayek script. Two independent views on Bengali script based SMT and transliterated Meitei Mayek based SMT systems of the training data and language models are presented and compared. The impact of various language models is commendable in such scenario. The BLEU and NIST score shows that Bengali script based phrase based SMT (PBSMT) outperforms over the Meetei Mayek based English to Manipuri SMT system. However, subjective evaluation shows slight variation against the automatic scores. 
Selecting a set of nonterminals for the synchronous CFGs underlying the hierarchical phrase-based models is usually done on the basis of a monolingual resource (like a syntactic parser). However, a standard bilingual resource like word alignments is itself rich with reordering patterns that, if clustered somehow, might provide labels of diﬀerent (possibly complementary) nature to monolingual labels. In this paper we explore a ﬁrst version of this idea based on a hierarchical decomposition of word alignments into recursive tree representations. We identify ﬁve clusters of alignment patterns in which the children of a node in a decomposition tree are found and employ these ﬁve as nonterminal labels for the Hiero productions. Although this is our ﬁrst non-optimized instantiation of the idea, our experiments show competitive performance with the Hiero baseline, exemplifying certain merits of this novel approach. 
We describe a novel approach to combining lexicalized, POS-based and syntactic treebased word reordering in a phrase-based machine translation system. Our results show that each of the presented reordering methods leads to improved translation quality on its own. The strengths however can be combined to achieve further improvements. We present experiments on German-English and GermanFrench translation. We report improvements of 0.7 BLEU points by adding tree-based and lexicalized reordering. Up to 1.1 BLEU points can be gained by POS and tree-based reordering over a baseline with lexicalized reordering. A human analysis, comparing subjective translation quality as well as a detailed error analysis show the impact of our presented tree-based rules in terms of improved sentence quality and reduction of errors related to missing verbs and verb positions. 
We show that combining both bottom-up rule chunking and top-down rule segmentation search strategies in purely unsupervised learning of phrasal inversion transduction grammars yields significantly better translation accuracy than either strategy alone. Previous approaches have relied on incrementally building larger rules by chunking smaller rules bottomup; we introduce a complementary top-down model that incrementally builds shorter rules by segmenting larger rules. Specifically, we combine iteratively chunked rules from Saers et al. (2012) with our new iteratively segmented rules. These integrate seamlessly because both stay strictly within a pure transduction grammar framework inducing under matching models during both training and testing—instead of decoding under a completely different model architecture than what is assumed during the training phases, which violates an elementary principle of machine learning and statistics. To be able to drive induction top-down, we introduce a minimum description length objective that trades off maximum likelihood against model size. We show empirically that combining the more liberal rule chunking model with a more conservative rule segmentation model results in significantly better translations than either strategy in isolation. 
Deciding whether a synchronous grammar formalism generates a given word alignment (the alignment coverage problem) depends on ﬁnding an adequate instance grammar and then using it to parse the word alignment. But what does it mean to parse a word alignment by a synchronous grammar? This is formally undeﬁned until we deﬁne an unambiguous mapping between grammatical derivations and word-level alignments. This paper proposes an initial, formal characterization of alignment coverage as intersecting two partially ordered sets (graphs) of translation equivalence units, one derived by a grammar instance and another deﬁned by the word alignment. As a ﬁrst sanity check, we report extensive coverage results for ITG on automatic and manual alignments. Even for the ITG formalism, our formal characterization makes explicit many algorithmic choices often left underspeciﬁed in earlier work. 
We propose synchronous linear context-free rewriting systems as an extension to synchronous context-free grammars in which synchronized non-terminals span k ≥ 1 continuous blocks on each side of the bitext. Such discontinuous constituents are required for inducing certain alignment conﬁgurations that occur relatively frequently in manually annotated parallel corpora and that cannot be generated with less expressive grammar formalisms. As part of our investigations concerning the minimal k that is required for inducing manual alignments, we present a hierarchical aligner in form of a deduction system. We ﬁnd that by restricting k to 2 on both sides, 100% of the data can be covered. 
Abstract. We present how TTR (Type Theory with Records) can model both geometric perception and conceptual (world) knowledge relating to the meaning of spatial descriptions for a robotic agent.  
Using data from Reddy et al. (2011), we present a series of regression models of semantic transparency in compound nouns. The results indicate that the frequencies of the compound constituents, the semantic relation between the constituents, and metaphorical shift of a constituent or of the compound as a whole, all contribute to the overall perceived level of transparency. While not proposing an actual distributional model of transparency, we hypothesise that incorporating this information into such a model would improve its success and we suggest some ways this might be possible. 
In this position paper, I discuss some linguistic problems that computational work on lexical semantics has attempted to address in the past and the implications for alternative models which incorporate distributional information. I concentrate in particular on phenomena involving count/mass distinctions, where older approaches attempted to use lexical semantics in their models of syntax. I outline methods by which the earlier models allowed the transmission of information between lexical items (regular polysemy and inheritance) and address the possibility that similar techniques could usefully be incorporated into distributional models. 
Researchers working on distributional semantics have recently taken up the challenge of going beyond lexical meaning and tackle the issue of compositionality. Several Compositional Distributional Semantics Models (CDSMs) have been developed and promising results have been obtained in evaluations carried out against data sets of small phrases and as well as data sets of sentences. However, we believe there is the need to further develop good evaluation tasks that show whether CDSM truly capture compositionality. To this end, we present an evaluation task that highlights some differences among the CDSMs currently available by challenging them in detecting semantic differences caused by word order switch and by determiner replacements. We take as starting point simple intransitive and transitive sentences describing similar events, that we consider to be paraphrases of each other but not of the foil paraphrases we generate from them. Only the models sensitive to word order and determiner phrase meaning and their role in the sentence composition will not be captured into the foils’ trap. 
Logical metonymy combines an event-selecting verb with an entity-denoting noun (e.g., The writer began the novel), triggering a covert event interpretation (e.g., reading, writing). Experimental investigations of logical metonymy must assume a binary distinction between metonymic (i.e. eventselecting) verbs and non-metonymic verbs to establish a control condition. However, this binary distinction (whether a verb is metonymic or not) is mostly made on intuitive grounds, which introduces a potential confounding factor. We describe a corpus-based approach which characterizes verbs in terms of their behavior at the syntax-semantics interface. The model assesses the extent to which transitive verbs prefer event-denoting objects over entity-denoting objects. We then test this “eventhood” measure on psycholinguistic datasets, showing that it can distinguish not only metonymic from non-metonymic verbs, but that it can also capture more ﬁne-grained distinctions among different classes of metonymic verbs, putting such distinctions into a new graded perspective. 
We present an annotation model of modality which is (i) cross-linguistic, relying on a wide, strongly typologically motivated approach, and (ii) hierarchical and layered, accounting for both factuality and speaker’s attitude, while modelling these two aspects through separate annotation schemes. Modality is deﬁned through cross-linguistic categories, but the classiﬁcation of actual linguistic expressions is language-speciﬁc. This makes our annotation model a powerful tool for investigating linguistic diversity in the ﬁeld of modality on the basis of real language data, being thus also useful from the perspective of machine translation systems. 
Spatial and spatio-temporal information is often carried by non-textual data such as maps, diagrams, tables, or pictures, both still and moving, either embedded in a text or standalone. The annotation of nontextual data raises the following questions: (i) what are the markables and how should they be coded? (ii) how should relevant information be inferred which is implicit in the data? We answer these questions with a multilayered approach. 
This paper presents the ﬁrst description of the motion subcorpus of ISO-SpaceBank (MotionBank) and discusses how motion-events are represented in ISO-Space 1.5, a speciﬁcation language for the representation of spatial information in language. We present data from this subcorpus with examples from the pilot annotation, focusing speciﬁcally on the annotation of motion-events and their various participants. These data inform further discussion of outstanding issues concerning semantic annotation, such as quantiﬁcation and measurement. We address these questions brieﬂy as they impact the design of ISO-Space. 
The Australian National Corpus (AusNC) provides a technical infrastructure for collecting and publishing language resources representing Australian language use. As part of the project we have ingested a wide range of resource types into the system, bringing together the different meta-data and annotations into a single interoperable database. This paper describes the initial collections in AusNC and the procedures used to parse a variety of data types into a single uniﬁed annotation store. 
In this note, we look at the factors that inﬂuence veridicity judgments with factive predicates. We show that more context factors play a role than is generally assumed. We propose to use crowd sourcing techniques to understand these factors better and brieﬂy discuss the consequences for the association of lexical signatures with items in the lexicon. 
The computational processing of compound semantics poses several interesting challenges. Up to now, the processing of nominal compounds with non-noun left-hand constituents (henceforth XN compounds) has not received any attention, despite the fact that these also seem to be rather productive in Germanic languages. In our research project, we aim to ﬁll this hiatus by investigating various kinds of compounds in Afrikaans and Dutch, develop annotation protocols and data sets, and model the semantics of such compounds. In this publication we present the alpha version of an annotation protocol that was designed for both descriptive linguistic and computational linguistic purposes. We describe the protocol development and discuss the current version. 
This paper analyzes the issues that arise when trying to add annotations to the dialogues in the Switchboard corpus according to ISO standard 24617-2, exploiting the existing SWBD-DAMSL annotations. These issues relate to differences between the two tag sets; to the highly multidimensional view that underlies the ISO standard; to differences in segmenting the dialogues into functional units; to the use of in-line markups for certain phenomena in Switchboard, and to the use of intra-dialogue dependence relations as deﬁned in the ISO standard. The analysis is supplemented by a discussion of how the existing annotations may be helpful to semi-automatically create a fullyﬂedged ISO standard annotation alongside the existing SWBD-DAMSL annotation. 
The last two decades witnessed a great success of revived empiricism in NLP research. However, there are still several NLP tasks that are not successful enough. As one of many directions for going beyond the revived empiricism, this paper introduces a project for annotating annotations with annotators’ rationales behind them. As a ﬁrst step of this enterprise, the paper particularly focuses on data collection during the annotation and discusses their potential uses. Finally a preliminary experiment for data collection is described with the data analysis. 
In this paper we report on an ongoing multiinstitution effort to encode inferential patterns associated with adjective modiﬁcation in English. We focus here on a subset of intensional adjectives typically referred to as “non-subsective” predicates. This class includes adjectives such as alleged, supposed, so-called, and related modally subordinating predicates. We discuss the initial results of corpus-based investigations to discriminate the patterns of inference associated with these adjectives. Based on these studies, we have created an initial annotation speciﬁcation that we are using to create a corpus of adjectiverelated inferences in English. 
In this paper we present our ongoing work on integrating large-scale terminological information into NLP tools. We focus on the problem of selecting and generating a set of suitable terms from the resources, based on deletion, modiﬁcation and addition rules. We propose a general framework in which the raw data of the resources are ﬁrst loaded into a knowledge base (KB). The selection and generation rules are then deﬁned in a declarative way using query templates in the query language of the KB system. We illustrate the use of this framework to select and generate term sets from a UMLS dataset. 
Previous research shows that aspects of doctor-patient communication in therapy can predict patient symptoms, satisfaction and future adherence to treatment (a signiﬁcant problem with conditions such as schizophrenia). However, automatic prediction has so far shown success only when based on low-level lexical features, and it is unclear how well these can generalise to new data, or whether their effectiveness is due to their capturing aspects of style, structure or content. Here, we examine the use of topic as a higher-level measure of content, more likely to generalise and to have more explanatory power. Investigations show that while topics predict some important factors such as patient satisfaction and ratings of therapy quality, they lack the full predictive power of lower-level features. For some factors, unsupervised methods produce models comparable to manual annotation. 
Automated processing of clinical texts is commonly faced with various less exposed, and not so regularly discussed linguistically complex problems that need to be addressed. One of these issues concerns the usage of ﬁgurative language. Figurative language implies the use of words that go beyond their ordinary meaning, a linguistically complex and challenging problem and also a problem that causes great difﬁculty for the ﬁeld of natural language processing (NLP). The problem is equally prevalent in both general language and also in various sublanguages, such as clinical medicine. Therefore we believe that a comprehensive model of e.g. clinical language processing needs to account for ﬁgurative language usage, and this paper provides a description, and preliminary results towards this goal. Since the empirical, clinical data used in the study is limited in size, there is no formal distinction made between different sub-classiﬁcations of ﬁgurative language. e.g., metaphors, idioms or simile. We illustrate several types of ﬁgurative expressions in the clinical discourse and apply a rather quantitative and corpus-based level analysis. The main research questions that this paper asks are whether there are traces of ﬁgurative language (or at least a subset of such types) in patient-doctor and patient-nurse interactions, how can they be found in a convenient way and whether these are transferred in the electronic health records and to what degree. 
Converting information contained in natural language clinical text into computer-amenable structured representations can automate many clinical applications. As a step towards that goal, we present a method which could help in converting novel clinical phrases into new expressions in SNOMED CT, a standard clinical terminology. Since expressions in SNOMED CT are written in terms of their relations with other SNOMED CT concepts, we formulate the important task of identifying relations between clinical phrases and SNOMED CT concepts. We present a machine learning approach for this task and using the dataset of existing SNOMED CT relations we show that it performs well. 
A large amount of medication information resides in unstructured text in electronic medical records, which requires advanced techniques to be properly mined. In clinical notes, medication information follows certain semantic patterns (e.g., medication, dosage, frequency, mode, etc.). Some medication descriptions contain additional word(s) between medication attributes. Therefore, it is essential to understand the semantic patterns as well as the patterns of the context interspersed among them (i.e., context patterns) to effectively extract comprehensive medication information. In this paper we examined both semantic and context patterns and compared those found in Mayo Clinic and i2b2 challenge data. We found that some variations exist between the institutions but the dominant patterns are common. 
We consider the problem of extracting formal process representations of the therapies deﬁned by clinical guidelines, viz., computer interpretable guidelines (CIGs), based on UMLS and semantic and syntactic annotation. CIGs enable the application of formal methods (such as model checking, veriﬁcation, conformance assessment) to the clinical domain. We argue that, while minimally structured, correspondences among clinical guideline syntax and discourse relations and clinical process constructs should however be exploited to successfully extract CIGs. We review work on current clinical syntactic and semantic annotation, pinpointing their limitations, and discuss a CIG extraction methodology based on recent efforts on business process modelling notation (BPMN) model extraction from natural language text. 
This short paper introduces the first notes about a modality annotation system that is under development for a spontaneous speech Brazilian Portuguese corpus (C-ORALBRASIL). We indicate our methodological decisions, the points which seem to be well resolved and two issues for further discussion and investigation.  category stands for, as well as identifying linguistic elements that carry it, is of utmost relevance. Our goal in annotating modality in a spontaneous speech Brazilian Portuguese Corpus is to provide a reliable starting point for researchers that might be interested in developing methodologies associated to NLP that ensue the extraction of oral discourse reliability, certainty and factuality markers, or carrying sentiment analysis, modeling modality and similar objectives.  
This paper introduces our methodology for annotating variations in enunciative and modal commitment in a text. We first present the theoretical background of the study which puts the emphasis on the close interaction between time, aspect, modality and evidentiality (TAME) categories (and also markers). We then present our semantic resources which encompass not only lexical items, but also morphological inflections and syntactic constructions. We finally describe the first step of our global natural language processing (NLP) workflow which uses a syntactic analysis parser. 
This paper reports on a series of annotation experiments carried out on a number of English adverbials. The experiments, based on occurrences obtained from the British National Corpus, focused on the distinction of epistemic and evidential meanings from other kinds of meanings. The results led to the conclusion that many of the cases of inter-annotator disagreement were due to certain syntactic and semantic factors. Some of these factors will be described in detail, together with the decisions made in each case for prospective annotation. 
This paper reports an effort to annotate modality in the Penn Chinese Treebank. We introduce the modals and features that were annotated, and describe the phases of our working process. Along with this, we address the issues in the preparation of annotation guidelines, and present the preliminary results of the first pass. Finally, we analyze the types of disagreement, and propose directions to improve consistency. 
This paper investigates the impact of modality markers on the conditional interpretation of the German preposition ohne (‘without’). It tackles the question whether it is the preposition itself that possesses a conditional sense or whether it may be due to a modal context that the interpretation arises. The paper presents an annotation study for modality factors (e.g. mood, modal auxiliary verbs, modal adjectives, modal adverbs, modal infinitives, negation) in the context of these sentences. The statistical analysis of the data has been carried out by means of a correspondence analysis in order to identify the relevant factors for the conditional interpretation. The results suggest that primarily the verb mood has an influence. 
We present a linguistically-informed schema for annotating modal expressions and describe its application to a subset of the MPQA corpus of English texts (Wiebe et al. 2005). The annotation is fine-grained in two respects: (i) in the range of expressions that are defined as modal targets and (ii) in the amount of information that is annotated for each target expression. We use inter-annotator reliability results to support a two-way distinction between priority and nonpriority modality types. 
In this paper we describe a “distant annotation” method by which we mark up tense and modality of Chinese eventualities via a wordaligned parallel corpus. We ﬁrst map Chinese verbs to their English counterpart via word alignment, and then annotate the resulting English text spans with coarse-grained tense and modality categories that we believe apply to both English and Chinese. Because English has richer morpho-syntactic indicators for tense and modality than Chinese, we hope this distant annotation approach will yield more consistent annotation than if we annotate the Chinese side directly. We report experimental results that show this expectation is largely borne out. 
We are in the process of creating a pipeline for our HPSG grammar for Norwegian (NorSource). NorSource uses the meaning representation Minimal Recursion Semantics (MRS). We present a step for validating an MRS and a step for pre-processing an MRS. The pre-processing step connects our MRS elements to a domain ontology and it can create additional states and roles. The pipeline can be reused by other grammars from the Delph-In network. 
This paper presents an approach to the annotation of quantiﬁcation, developed in the context of an ISO project aiming to deﬁne standards for semantic annotation. The approach is illustrated for a range of quantiﬁcation phenomena, including cumulative, collective, and group quantiﬁcation. 
In this paper we present a pragmatic account of scope alternation involving universal quantiﬁers in a lexicalist framework based on CCG and DRT. This account can derive the desired reading for 96% of all cases of scope interaction involving universal quantiﬁcation mediated by prepositions in a real corpus. We show how this account allows for recasting scope resolution as a simple token classiﬁcation task, providing a simpler handle for statistical approaches to scope resolution than previous accounts. 
This paper queries which aspects of lexical semantics can reasonably be expected to be modelled by corpus-based theories such as distributional semantics or techniques such as ontology extraction. We argue that a full lexical semantics theory must take into account the extensional potential of words. We investigate to which extent corpora provide the necessary data to model this information and suggest that it may be partly learnable from text-based distributions, partly inferred from annotated data, using the insight that a concept’s features are extensionally interdependent. 
Motivated by cognitive lexical models, network-based distributional semantic models (DSMs) were proposed in [Iosif and Potamianos (2013)] and were shown to achieve state-of-the-art performance on semantic similarity tasks. Based on evidence for cognitive organization of concepts based on degree of concreteness, we investigate the performance and organization of network DSMs for abstract vs. concrete nouns. Results show a “concreteness effect” for semantic similarity estimation. Network DSMs that implement the maximum sense similarity assumption perform best for concrete nouns, while attributional network DSMs perform best for abstract nouns. The performance of metrics is evaluated against human similarity ratings on an English and a Greek corpus. 
In the ﬁnite-state temporality approach, events in natural language semantics have been characterized in regular languages, with strings representing sequences of temporal observations. We extend this approach to natural language constructions which are not regular. Context-free constructions are detailed and discussed. Superposition, the key operator in the ﬁnite-state temporality approach is investigated for context-free languages. The set of context-free languages is found to not be closed under superposition. However, as with intersection, the superposition of a context-free language and a regular language results in a context-free language. Previous work on subsumption and entailment is inapplicable to context-free languages, due to the undecidability of the subset relation for context-free languages. 
In the problem of recognizing textual entailment, the goal is to decide, given a text and a hypothesis expressed in a natural language, whether a human reasoner would call the hypothesis a consequence of the text. One approach to this problem is to use a ﬁrst-order reasoning tool to check whether the hypothesis can be derived from the text conjoined with relevant background knowledge, after expressing all of them by ﬁrst-order formulas. Another possibility is to express the hypothesis, the text, and the background knowledge in a logic programming language, and use a logic programming system. We discuss the relation of these methods to each other and to the class of effectively propositional reasoning problems. This leads us to general conclusions regarding the relationship between classical logic and answer set programming as knowledge representation formalisms.  
Since long it has been noted that cross-linguistically recurring polysemies can serve as an indicator of conceptual relations, and quite a few approaches to model and analyze such data have been proposed in the recent past. Although – given the nature of the data – it seems natural to model and analyze it with the help of network techniques, there are only a few approaches which make explicit use of them. In this paper, we show how the strict application of weighted network models helps to get more out of cross-linguistic polysemies than would be possible using approaches that are only based on item-to-item comparison. For our study we use a large dataset consisting of 1252 semantic items translated into 195 different languages covering 44 different language families. By analyzing the community structure of the network reconstructed from the data, we find that a majority of the concepts (68%) can be separated into 104 large communities consisting of five and more nodes. These large communities almost exclusively constitute meaningful groupings of concepts into conceptual fields. They provide a valid starting point for deeper analyses of various topics in historical semantics, such as cognate detection, etymological analysis, and semantic reconstruction. 
In this paper we consider the issue of answering a query with a query. Although these are common, with the exception of Clariﬁcation Requests, they have not been studied empirically. After brieﬂy reviewing different theoretical approaches on this subject, we present a corpus study of query responses in the British National Corpus and develop a taxonomy for query responses. We sketch a formal analysis of the response categories in the framework of KoS. 
This paper investigates the extraction of semantic representations from bigrams. The major obstacle to this objective is that while these word to word dependencies do contain a semantic component, other factors, e.g. syntax, play a much stronger role. An effective solution will therefore require some means of isolating semantic structure from the remainder. Here, the possibility of modelling semantic dependencies within the bigram in terms of the similarity of the two words is explored. A model based on this assumption of semantic coherence is contrasted and combined with a relaxed model lacking this assumption. The induced representations are evaluated in terms of the correlation of predicted similarities to a dataset of noun-verb similarity ratings gathered in an online experiment. The results show that the coherence assumption can be used to induce semantic representations, and that the combined model, which breaks the dependencies down into a semantic and a non-semantic component, achieves the best performance. 
This project aims to build an ontology authoring interface in which the user is engaged in a dialogue with the system in controlled natural language. To investigate what such a dialogue might be like, a layered annotation scheme is being developed for interactions between ontology authors and the Prote´ge´ ontology authoring environment. A pilot experiment has been conducted with ontology authors, which reveals the complexity of mapping between user-interface actions and acts that appear in natural language dialogues; it also suggests the addition of some unanticipated types of dialogue acts and points the way to some possible enhancements of the authoring interface. 
We propose and advocate the use of an advanced declarative programming paradigm – answer set programming – as a uniform platform for integrated approach towards syntax-semantic processing in natural language. We illustrate that (a) the parsing technology based on answer set programming implementation reaches performance sufﬁcient for being a useful NLP tool, and (b) the proposed method for incorporating semantic information from FRAMENET into syntactic parsing may prove to be useful in allowing semantic-based disambiguation of syntactic structures.  
 Abstract Obtaining gold standard data for word sense disambiguation is important but costly. We show how it can be done using a “Game with a Purpose” (GWAP) called Wordrobe. This game consists of a large set of multiple-choice questions on word senses generated from the Groningen Meaning Bank. The players need to answer these questions, scoring points depending on the agreement with fellow players. The working assumption is that the right sense for a word can be determined by the answers given by the players. To evaluate our method, we gold-standard tagged a portion of the data that was also used in the GWAP. A comparison yielded promising results, ranging from a precision of 0.88 and recall of 0.83 for relative majority agreement, to a precision of 0.98 and recall of 0.35 for questions that were answered unanimously. 
Logical metonymy interpretation (e.g. begin the book → writing) has received wide attention in linguistics. Experimental results have shown higher processing costs for metonymic conditions compared with non-metonymic ones (read the book). According to a widely held interpretation, it is the type clash between the event-selecting verb and the entity-denoting object (begin the book) that triggers coercion mechanisms and leads to additional processing effort. We propose an alternative explanation and argue that the extra processing effort is an effect of thematic ﬁt. This is a more economical hypothesis that does not need to postulate a separate type clash mechanism: entitydenoting objects simply have a low ﬁt as objects of event-selecting verbs. We test linguistic datasets from psycholinguistic experiments and ﬁnd that a structured distributional model of thematic ﬁt, which does not encode any explicit argument type information, is able to replicate all signiﬁcant experimental ﬁndings. This result provides evidence for a graded account of coercion phenomena in which thematic ﬁt accounts for both the trigger of the coercion and the retrieval of the covert event. 
Syntactic annotation is an indispensable input for many semantic NLP applications. For instance, Semantic Role Labelling algorithms almost invariably apply some form of syntactic parsing as preprocessing. The categories used for syntactic annotation in NLP generally reﬂect the formal patterns used to form the text. This results in complex annotation schemes, often tuned to one language or domain, and unintuitive to non-expert annotators. In this paper we propose a different approach and advocate substituting existing syntax-based approaches with semantics-based grammatical annotation. The rationale of this approach is to use manual labor where there is no substitute for it (i.e., annotating semantics), leaving the detection of formal regularities to automated statistical algorithms. To this end, we propose a simple semantic annotation scheme, UCCA for Universal Conceptual Cognitive Annotation. The scheme covers many of the most important elements and relations present in linguistic utterances, including verb-argument structure, optional adjuncts such as adverbials, clause embeddings, and the linkage between them. The scheme is supported by extensive typological crosslinguistic evidence and accords with the leading Cognitive Linguistics theories. 
This paper introduces distributional semantic similarity methods for automatically measuring the coherence of a set of words generated by a topic model. We construct a semantic space to represent each topic word by making use of Wikipedia as a reference corpus to identify context features and collect frequencies. Relatedness between topic words and context features is measured using variants of Pointwise Mutual Information (PMI). Topic coherence is determined by measuring the distance between these vectors computed using a variety of metrics. Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task. 
Distributional semantics has very successfully modeled semantic phenomena at the word level, and recently interest has grown in extending it to capture the meaning of phrases via semantic composition. We present experiments in adjective-noun composition which (1) show that adjectival modiﬁcation can be successfully modeled with distributional semantics, (2) show that composition models inspired by the semantics of higher-order predication fare better than those that perform simple feature union or intersection, (3) contrary to what the theoretical literature might lead one to expect, do not yield a distinction between intensional and non-intensional modiﬁcation, and (4) suggest that head noun polysemy and whether the adjective corresponds to a typical attribute of the noun are relevant factors in the distributional representation of adjective phrases. 
The area of temporal information extraction has recently focused on temporal relation classiﬁcation. This task is about classifying the temporal relation (precedence, overlap, etc.) holding between two given entities (events, dates or times) mentioned in a text. This interest has largely been driven by the two recent TempEval competitions. Even though logical constraints on the structure of possible sets of temporal relations are obvious, this sort of information deserves more exploration in the context of temporal relation classiﬁcation. In this paper, we show that logical inference can be used to improve—sometimes dramatically— existing machine learned classiﬁers for the problem of temporal relation classiﬁcation. 
There exist formal accounts of tense and aspect, such as that detailed by Reichenbach (1947). Temporal semantics for corpus annotation are also available, such as TimeML. This paper describes a technique for linking the two, in order to perform a corpus-based empirical validation of Reichenbach’s tense framework. It is found, via use of Freksa’s semi-interval temporal algebra, that tense appropriately constrains the types of temporal relations that can hold between pairs of events described by verbs. Further, Reichenbach’s framework of tense and aspect is supported by corpus evidence, leading to the ﬁrst validation of the framework. Results suggest that the linking technique proposed here can be used to make advances in the difﬁcult area of automatic temporal relation typing and other current problems regarding reasoning about time in language.  
We propose an architecture for generating natural language from Linked Data that automatically learns sentence templates and statistical document planning from parallel RDF datasets and text. We have built a proof-of-concept system (LOD-DEF) trained on un-annotated text from the Simple English Wikipedia and RDF triples from DBpedia, focusing exclusively on factual, non-temporal information. The goal of the system is to generate short descriptions, equivalent to Wikipedia stubs, of entities found in Linked Datasets. We have evaluated the LOD-DEF system against a simple generate-from-triples baseline and human-generated output. In evaluation by humans, LOD-DEF signiﬁcantly outperforms the baseline on two of three measures: non-redundancy and structure and coherence. 
Distributional representations have recently been proposed as a general-purpose representation of natural language meaning, to replace logical form. There is, however, one important difference between logical and distributional representations: Logical languages have a clear semantics, while distributional representations do not. In this paper, we propose a semantics for distributional representations that links points in vector space to mental concepts. We extend this framework to a joint semantics of logic and distributions by linking intensions of logical expressions to mental concepts. 
We describe a method for learning an incremental semantic grammar from a corpus in which sentences are paired with logical forms as predicate-argument structure trees. Working in the framework of Dynamic Syntax, and assuming a set of generally available compositional mechanisms, we show how lexical entries can be learned as probabilistic procedures for the incremental projection of semantic structure, providing a grammar suitable for use in an incremental probabilistic parser. By inducing these from a corpus generated using an existing grammar, we demonstrate that this results in both good coverage and compatibility with the original entries, without requiring annotation at the word level. We show that this semantic approach to grammar induction has the novel ability to learn the syntactic and semantic constraints on pronouns.  
This paper addresses the task of ﬁnding antecedents for locally uninstantiated arguments. To resolve such null instantiations, we develop a weakly supervised approach that investigates and combines a number of linguistically motivated strategies that are inspired by work on semantic role labeling and corefence resolution. The performance of the system is competitive with the current state-of-the-art supervised system.  
We present a hybrid natural language generation system that utilizes Discourse Representation Structures (DRSs) for statistically learning syntactic templates from a given domain of discourse in sentence “micro” planning. In particular, given a training corpus of target texts, we extract semantic predicates and domain general tags from each sentence and then organize the sentences using supervised clustering to represent the “conceptual meaning” of the corpus. The sentences, additionally tagged with domain speciﬁc information (determined separately), are reduced to templates. We use a SVM ranking model trained on a subset of the corpus to determine the optimal template during generation. The combination of the conceptual unit, a set of ranked syntactic templates, and a given set of information, constrains output selection and yields acceptable texts. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics and, for generated weather, ﬁnancial and biography texts, falls within acceptable ranges. Consequently, we argue that our DRS driven statistical and template–based method is robust and domain adaptable as, while content will be dictated by a target domain of discourse, signiﬁcant investments in sentence planning can be minimized without sacriﬁcing performance. 
We apply tree kernels to entity instantiations. An entity instantiation is an entity relationship, in which a set of entities is mentioned, and then a member or subset of this set is introduced. We present the ﬁrst reliably annotated intrasentential entity instantiation corpus, along with an extension to the intersentential annotations in McKinlay and Markert (2011). We then apply tree kernels to both inter- and intrasentential entity instantiations, showing comparable results to an extensive set of unstructured features. The combination of tree kernels and unstructured features leads to signiﬁcant improvements over either method in isolation. 
We present a corpus-based investigation of cases of clause combination that can be expressed both through coordination or with subordination. We analyse the data with a two-step computational model which ﬁrst distinguishes subordination from coordination and then determines the direction for cases of subordination. We ﬁnd that a wide range of features help with the prediction, notably frequency of predicate participants, presence of adjuncts and sharing of participants between the clause predicates. 
Relational similarity is essential to analogical reasoning. Automatically determining the degree to which a pair of words belongs to a semantic relation (relational similarity) is greatly improved by considering the selectional preferences of the relation. To determine selectional preferences, we induced semantic classes through a Latent Dirichlet Allocation (LDA) method that operates on dependency parse contexts of single words. When assigning relational similarities to pairs of words, if the agreement of selectional preferences is considered alone, a correlation of 0.334 is obtained against the manual ranking outperforming the previously best reported score of 0.229. 
In this paper, we present an approach for recognizing spatial containment relations that hold between event mentions. Event mentions refer to real-world events that have spatio-temporal properties. While the temporal aspect of event relations has been well-studied, the spatial aspect has received relatively little attention. The difﬁculty in this task is the highly implicit nature of event locations in discourse. We present a supervised method that is designed to capture both explicit and implicit spatial relation information. Our approach outperforms the only known previous method by a 14 point increase in F1-measure. 
This paper provides a corpus-based study on German particle verbs. We hypothesize that there are regular mechanisms in meaning shifts of a base verb in combination with a particle that do not only apply to the individual verb, but across a semantically coherent set of verbs. For example, the syntactically similar base verbs brummen ‘hum’ and donnern ‘rumble’ both describe an irritating, displeasing loud sound. Combined with the particle auf, they result in near-synonyms roughly meaning ‘forcefully assigning a task’ (in one of their senses). Covering 6 base verb groups and 3 particles with 4 particle meanings, we demonstrate that corpus-based information on the verbs’ subcategorization frames plus conceptual properties of the nominal complements is a sufﬁcient basis for deﬁning such meaning shifts. While the paper is considerably more extensive than earlier related work, we view it as a case study toward a more automatic approach to identify and formalize meaning shifts in German particle verbs. 
The inﬂuential idea by van der Sandt (1992) to treat presuppositions as anaphora in the framework of Discourse Representation Theory (DRT, Kamp and Reyle, 1993) has inspired a lot of debate as well as elaborations of his account. In this paper, we propose an extension of DRT, called Projective DRT, which adds pointers to all DRT referents and conditions, indicating their projection site. This means that projected content need not be moved from the context in which it is introduced, while it remains clearly discernible from asserted content. This approach inherits the attractive properties from van der Sandt’s approach to presupposition, but precludes a two-step resolution algorithm by treating projection as variable binding, which increases compositionality and computational efﬁciency. The result is a ﬂexible representational framework for a descriptive theory of projection phenomena. 
Current approaches to recognizing discourse relations rely on a combination of shallow, surfacebased features (e.g., bigrams, word pairs), and rather specialized hand-crafted features. As a way to avoid both the shallowness of word-based representations and the lack of coverage of specialized linguistic features, we use a graph-based representation of discourse segments, which allows for a more abstract (and hence generalizable) notion of syntactic (and partially of semantic) structure. Empirical evaluation on a hand-annotated corpus of German discourse relations shows that our graphbased approach not only provides a suitable representation for the linguistic factors that are needed in disambiguating discourse relations, but also improves results over a strong state-of-the-art baseline by more accurately identifying Temporal, Comparison and Reporting discourse relations. 
This paper identiﬁes features that occur frequently in coherence relations labelled CHOSEN ALTERNATIVE. This achieves two goals: (1) to identify evidence for an argument being considered an alternative excluded from further consideration, and (2) to contribute to the automatic identiﬁcation of coherence relations and their arguments. It is shown that the simplest of these features occur signiﬁcantly more often in implicit CHOSEN ALTERNATIVE relations than in explicit CHOSEN ALTERNATIVE relations, where a connective helps signal this sense. 
We present the ﬁrst freely available large German dataset for Textual Entailment (TE). Our dataset builds on posts from German online forums concerned with computer problems and models the task of identifying relevant posts for user queries (i.e., descriptions of their computer problems) through TE. We use a sequence of crowdsourcing tasks to create realistic problem descriptions through summarisation and paraphrasing of forum posts. The dataset is represented in RTE-5 Search task style and consists of 172 positive and over 2800 negative pairs. We analyse the properties of the created dataset and evaluate its difﬁculty by applying two TE algorithms and comparing the results with results on the English RTE-5 Search task. The results show that our dataset is roughly comparable to the RTE-5 data in terms of both difﬁculty and balancing of positive and negative entailment pairs. Our approach to create task-speciﬁc TE datasets can be transferred to other domains and languages. 
Practical tasks like question answering and machine translational ultimately require computing meaning representations that support inference. Standard linguistic accounts of meaning are impracticable for such purposes, both because they assume nonmonotonic operations such as quantiﬁer movement, and because they lack a representation for the meaning of content words that supports efﬁcient computation of entailment. I’ll discuss practical solutions to some of these problems within a near-context free grammar formalism for a working wide-coverage parser, in current work with Mike Lewis, and show how these solutions can be usefully applied in NLP tasks. 
The Penn Discourse Treebank (PDTB) was released to the public in 2008 and remains the largest corpus of manually annotated discourse relations — both relations that are signaled explicitly (e.g., by a coordinating or subordinating conjunction, or by a discourse adverbial or other construction) and ones that otherwise appear implicit. 
We investigate national dialect identiﬁcation, the task of classifying English documents according to their country of origin. We use corpora of known national origin as a proxy for national dialect. In order to identify general (as opposed to corpus-speciﬁc) characteristics of national dialects of English, we make use of a variety of corpora of different sources, with inter-corpus variation in length, topic and register. The central intuition is that features that are predictive of national origin across different data sources are features that characterize a national dialect. We examine a number of classiﬁcation approaches motivated by different areas of research, and evaluate the performance of each method across 3 national dialects: Australian, British, and Canadian English. Our results demonstrate that there are lexical and syntactic characteristics of each national dialect that are consistent across data sources. 
Human evaluation of machine translation quality is a key element in the development of machine translation systems, as automatic metrics are validated through correlation with human judgment. However, achievement of consistent human judgments of machine translation is not easy, with decreasing levels of consistency reported in annual evaluation campaigns. In this paper we describe experiences gained during the collection of human judgments of the ﬂuency of machine translation output using Amazon’s Mechanical Turk service. We gathered a large collection of crowd-sourced human judgments for the machine translation systems that participated in the WMT 2012 shared translation task, collected across a range of eight different assessment conﬁgurations to gain insight into possible causes of – and remedies for – inconsistency in human judgments. Overall, approximately half of the workers carry out the human evaluation to a high standard, but effectiveness varies considerably across different target languages, with dramatically higher numbers of good quality judgments for Spanish and French, and the reverse observed for German. 
This study is an investigation into the effect of sample size on a likelihood ratio (LR) based forensic voice comparison (FVC) system. In particular, we looked into how the offender and suspect sample size (or the within-speaker sample size) would affect the performance of the FVC system, using spectral feature vectors extracted from spontaneous Japanese speech. For this purpose, we repeatedly conducted Monte Carlo method based experiments with different sample size, using the statistics obtained from these feature vectors. LRs were estimated using the multivariate kernel density LR formula developed by Aitken and Lucy (2004). The derived LRs were calibrated using the logistic-regression calibration technique proposed by Brümmer and du Preez (2006). The performance of the FVC system was assessed in terms of the log-likelihood-ratio cost (Cllr) and the 95% credible interval (CI), which are the metrics of validity and reliability, respectively. We will demonstrate in this paper that 1) the validity of the system notably improves when up to six tokens are included in modelling a speaker session, and 2) the system performance converges with the relative small token number (four) in the background database, regardless of the token numbers in the test and development databases. 
In Australian healthcare, failures in information ﬂow cause over one-tenth of preventable adverse events and are tangible in clinical handover. Regardless of a good verbal handover, anything from two-thirds to all of this information is lost after 3– 5 shifts if notes are taken by hand or not taken. Speech to text (SST) and information extraction (IE) have been proposed for taking the notes and ﬁlling in a handover form with extrapolated evaluations from related studies promising over 90 per cent correctness for both STT and IE. However, this cascading evokes a fruitful methodological challenge: the severe implications that errors may have in clinical decisionmaking call for superiority in STT; the correctness percentage measured in a peaceful laboratory is decreased to 77 by noise in clinical practise; and the STT errors multiply when cascaded with IE. We provide an analysis of STT errors and discuss the feasibility of phonetic similarity for their correction in this paper. Our data consists of one hundred simulated handover records in Australian English with STT recognising 73 per cent of the 7, 277 words (1 h 8 min 5 s) correctly. In text relevant to the form, 836 unique error types are present. The most common errors include inserting and, in, are, arm, is, a, the, or am (5 ≤ n ≤ 94), deleting is (n = 17), and substituting and, obs are, 2, he with in, also, to, or and she (7 ≤ n ≤ 11), respectively. Eighteen per cent of word substitutions sound exactly the same as the correct word and 26 per cent have a similarity percentage above 75. This encourages using phonetic similarity to improve STT.  
Quote attribution is the task of identifying the speaker of each quote within a document. While recent research has established large-scale corpora for this task, these corpora are not yet consistent in the way they handle candidate speakers, and many of the reported results rely on gold standard annotations of both entities and coreference chains. In this work we evaluate three quote attribution systems with automatically produced candidate speakers and coreference chains. We perform these experiments over four separate corpora, which allows us to determine how coreference resolution effects quote attribution, and to use the task as an extrinsic evaluation of three coreference systems. 
Clustering the results of a search can help a multi-document summarizer present a summary for evidence based medicine (EBM). In this work, we introduce a clustering technique that is based on multiobjective (MOO) optimization. MOO is a technique that shows promise in the areas of machine learning and natural language processing. In our approach we show how MOO based semi-supervised clustering technique can be effectively used for EBM. 
Relating one’s research to the vast body of scientiﬁc knowledge is a difﬁcult task; the sheer volume of literature makes it difﬁcult to keep up-to-date with scientiﬁc developments. Particularly when research is on-going, keeping track of related work is especially important to avoid an unintended duplication of effort. We outline a novel approach to this problem that uses the text in an Electronic Laboratory Notebook (ELN) as a representation of an experimental context in the ﬁeld of Chemistry. The contribution of this work is to situate the literature recommendation task within the context of the user’s experimental information needs. We ﬁnd that our approach to transform the ELN text into queries for use with PubMed is able to recover a subset of user bibliographies. We ﬁnd that alternative methods for query generation that capture both scientiﬁc terminology and salient terms in the ELN complement each other. 
We compared the performances of two procedures for calculating the likelihood ratio (LR) on the same set of text data. The first procedure was a multivariate kernel density (MVKD) procedure which has been successfully applied to various types of forensic evidence, including glass fragments, handwriting, fingerprint, voice, and texts. The second procedure was a Gaussian mixture model – universal background model (GMM-UBM), which has been commonly used in forensic voice comparison (FVC) with so-called automatic features. Previous studies have applied the MVKD system to electronically-generated texts to estimate LRs, but so far no previous studies seem to have applied the GMM-UBM system to such texts. It has been reported that the latter GMM-UBM system outperforms the MVKD system in FVC. The data used for this study was chatlog messages collected from 115 authors, which were divided into test, background and development databases. Three different sample sizes of 500, 1500 and 2500 words were used to investigate how the performance is susceptible to the sample size. Results show that regardless of sample size, the performance of the GMM-UBM system was better than that of the MVKD system with respect to both validity (= accuracy) (of which the metric is the log-likelihood-ratio cost, Cllr) and reliability (= precision) (of which the metric is the 95% credible interval, CI). 
This paper presents a notiﬁcation system to identify in near-real-time Tweets describing ﬁre events in Australia. The system identiﬁes ﬁre related ‘alert words’ published on Twitter which are further processed by a classiﬁer to determine if they correspond to an actual ﬁre event. We describe how the classiﬁer has been established and report preliminary results. The original notiﬁcation system did not include a classiﬁer and could not discriminate between messages unrelated to ‘real’ ﬁre events. In the ﬁrst three months of operation, the system generated 42 ‘ﬁre’ email notiﬁcations of which 20 related to actual ﬁres and 12 of those contained Tweets that may have been of interest to ﬁre ﬁghting agencies. If the classiﬁer had been used, 21 emails would have been issued: an improvement in accuracy from 48% to 78%. However, the recall score reduced from 1 to 0.8 which is not desirable for this particular task. We propose extensions to address this short coming. 
We describe a cross-corpora evaluation of disease mention recognition for two annotated biomedical corpora: the Human Variome Project Corpus and the Arizona Disease Corpus. Our analysis of the performance of a state-of-the-art NER tool in terms of the characteristics and annotation schema of these corpora shows that these factors signiﬁcantly affect performance. 
The improvements to ad-hoc IR systems over the last decades have been recently criticized as illusionary and based on incorrect baseline comparisons. In this paper several improvements to the LM approach to IR are combined and evaluated: Pitman-Yor Process smoothing, TF-IDF feature weighting and modelbased feedback. The increases in ranking quality are signiﬁcant and cumulative over the standard baselines of Dirichlet Prior and 2-stage Smoothing, when evaluated across 13 standard ad-hoc retrieval datasets. The combination of the improvements is shown to improve the Mean Average Precision over the datasets by 17.1% relative. Furthermore, the considered improvements can be easily implemented with little additional computation to existing LM retrieval systems. On the basis of the results it is suggested that LM research for IR should move towards using stronger baseline models.  suggested to LMs over the years. It is shown that the combination of Pitman-Yor Process smoothing, TF-IDF feature weighting and Model-based Feedback produces a substantial and cumulative improvement over the common baseline LM smoothing methods. 2 Improvements to LMs for IR 2.1 LM Approach to IR The LM approach to ad-hoc IR considers documents and queries to be generated by underlying n-gram LMs. The Query Likelihood (QL) framework for LM retrieval (Hiemstra, 1998) treats queries as being generated by document models, reducing the retrieval of the most relevant documents into ranking documents by the posterior probability of each document given the query. Unigram LMs and a uniform distribution over document priors is commonly assumed, so that the QLscore for each document correspond to the conditional log-probability of the query given the document:  
We offer a supervised machine learning approach for recognizing erroneous words in the output of a speech recognizer. We have investigated several sets of features combined with two word conﬁgurations, and compared the performance of two classiﬁers: Decision Trees and Naïve Bayes. Evaluation was performed on a corpus of 400 spoken referring expressions, with Decision Trees yielding a high recognition accuracy. 
In this paper, we discuss how statements about defaults and various forms of exceptions to them can be incorporated into an existing controlled natural language. We show how these defaults and exceptions are translated and represented in the answer set programming paradigm in order to support automated reasoning. 
Since Ramus et al. (1999) a number of statistical metrics have been routinely employed by researchers (Ramus 2003, Grabe & Low 2002 etc.) in an effort to rhythmically classify languages. However, recent studies by Arvaniti (2009), Tilsen & Arvaniti (2013), Arvaniti & Rodriquez (2013) etc., have challenged both the validity of these metrics in reflecting speech rhythm, and the physical measurability of rhythm itself. The present study takes a comparative evaluative approach, and explores the applicability of the proposed metrics to a Papuan language (Urama) with a phonology quite different than traditional Western European (W.E.) languages. It is argued here that the statistical underpinning of the existing rhythm metrics is a direct outcome of an overt effort to capture the temporal durational characteristics of the phonotactics of W.E. languages. As such, these metrics are only capable of providing a crude measure of timing. 
This study investigates idiosyncrasy manifested in language use in spoken Japanese. For this purpose, we use speaker classification techniques as analytical tools. More precisely, focusing on Japanese case particles and fillers, of which the linguistic functions differ significantly, we aim to investigate 1) the extent of speaker idiosyncrasy in the selection of certain case particles/fillers over others in Japanese monologues, and 2) the differences, if any, between case particles and fillers in the degree of speaker-individualising information. We discuss what contributes to the identified differences between case particles and fillers. This study will contribute to the further development of automatic speaker recognition systems and authorship analysis studies. 
Climate type is one of the potentially most relevant pieces of metadata for identifying studies in evidence-based environmental management. In this paper, we propose a method for automatically predicting the climate type in environmental science literature using NLP techniques, relative to a pre-existing set of climate type categories. Our main approaches combine toponym detection and resolution using two different resources with support vector machines. The results show great promise, but also further challenges, for using NLP to extract information from the vast and rapidly growing collection of environmental sciences literature. 
The ClearTK-TimeML submission to TempEval 2013 competed in all English tasks: identifying events, identifying times, and identifying temporal relations. The system is a pipeline of machine-learning models, each with a small set of features from a simple morpho-syntactic annotation pipeline, and where temporal relations are only predicted for a small set of syntactic constructions and relation types. ClearTKTimeML ranked 1st for temporal relation F1, time extent strict F1 and event tense accuracy.  Thus, each classiﬁer in the ClearTK-TimeML pipeline uses only the features shared by successful models in previous work (Bethard and Martin, 2006; Bethard and Martin, 2007; Llorens et al., 2010; UzZaman and Allen, 2010) that can be derived from a simple morpho-syntactic annotation pipeline2. And each of the temporal relation classiﬁers is restricted to a particular syntactic construction and to a particular set of temporal relation labels. The following sections describe the models, classiﬁers and datasets behind the ClearTK-TimeML approach.  
In this paper, we describe our participation in the TempEval-3 challenge. With our multilingual temporal tagger HeidelTime, we addressed task A, the extraction and normalization of temporal expressions for English and Spanish. Exploiting HeidelTime’s strict separation between source code and languagedependent parts, we tuned HeidelTime’s existing English resources and developed new Spanish resources. For both languages, we achieved the best results among all participants for task A, the combination of extraction and normalization. Both the improved English and the new Spanish resources are publicly available with HeidelTime. 
In this paper we present the results of experiments comparing (a) rich syntactic and semantic feature sets and (b) big context windows, for the TempEval time expression and event segmentation and classiﬁcation tasks. We show that it is possible for models using only lexical features to approach the performance of models using rich syntactic and semantic feature sets. 
This paper describes a temporal expression identiﬁcation and normalization system, ManTIME, developed for the TempEval-3 challenge. The identiﬁcation phase combines the use of conditional random ﬁelds along with a post-processing identiﬁcation pipeline, whereas the normalization phase is carried out using NorMA, an open-source rule-based temporal normalizer. We investigate the performance variation with respect to different feature types. Speciﬁcally, we show that the use of WordNet-based features in the identiﬁcation task negatively affects the overall performance, and that there is no statistically signiﬁcant difference in using gazetteers, shallow parsing and propositional noun phrases labels on top of the morphological features. On the test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the identiﬁcation phase. Normalization accuracies are 0.84 (type attribute) and 0.77 (value attribute). Surprisingly, the use of the silver data (alone or in addition to the gold annotated ones) does not improve the performance. 
We describe FSS-TimEx, a module for the recognition and normalization of temporal expressions we submitted to Task A and B of the TempEval-3 challenge. FSS-TimEx was developed as part of a multilingual event extraction system, Nexus, which runs on top of the EMM news processing engine. It consists of ﬁnite-state rule cascades, using minimalistic text processing stages and simple heuristics to model the relations between events and temporal expressions. Although FSS-TimEx is already deployed within an IE application in the medical domain, we found it useful to customize its output to the TimeML standard in order to have an independent performance measure and guide further developments. 
We analyze the performance of SUTIME, a temporal tagger for recognizing and normalizing temporal expressions, on TempEval-3 Task A for English. SUTIME is available as part of the Stanford CoreNLP pipeline and can be used to annotate documents with temporal information. Testing on the TempEval-3 evaluation corpus showed that this system is competitive with state-of-the-art techniques. 
This paper describes a system for temporal processing of text, which participated in the Temporal Evaluations 2013 campaign. The system employs a number of machine learning classiﬁers to perform the core tasks of: identiﬁcation of time expressions and events, recognition of their attributes, and estimation of temporal links between recognized events and times. The central feature of the proposed system is temporal parsing – an approach which identiﬁes temporal relation arguments (eventevent and event-timex pairs) and the semantic label of the relation as a single decision. 
In this paper, we present a system, UTTime, which we submitted to TempEval-3 for Task C: Annotating temporal relations. The system uses logistic regression classiﬁers and exploits features extracted from a deep syntactic parser, including paths between event words in phrase structure trees and their path lengths, and paths between event words in predicateargument structures and their subgraphs. UTTime achieved an F1 score of 34.9 based on the graphed-based evaluation for Task C (ranked 2nd) and 56.45 for Task C-relationonly (ranked 1st) in the TempEval-3 evaluation. 
The measurement of phrasal semantic relatedness is an important metric for many natural language processing applications. In this paper, we present three approaches for measuring phrasal semantics, one based on a semantic network model, another on a distributional similarity model, and a hybrid between the two. Our hybrid approach achieved an Fmeasure of 77.4% on the task of evaluating the semantic similarity of words and compositional phrases.  2 Semantic Similarity of Words and Compositional Phrases The semantic similarity of words and compositional phrases is the task of evaluating the similarity of a word and a short phrase of two or more words; for example, the word Interview and the phrase Formal Meeting. In the next section we present our semantic network model for computing phrasal semantic relatedness between a word and a phrase, followed by a distributional similarity model, that we evaluate on the task of semantic similarity of words and compositional phrases.  
This paper presents a system for automatically generating a set of plausible paraphrases for a given noun compound and rank them in decreasing order of their usage represented by the confidence value provided by the human annotators. Our system implements a corpusdriven probabilistic co-occurrence based model for predicting the paraphrases, that uses a seed list of paraphrases extracted from corpus to predict other paraphrases based on their co-occurrences. The corpus study reveals that the prepositional paraphrases for the noun compounds are quite frequent and well covered but the verb paraphrases, on the other hand, are scarce, revealing the unsuitability of the model for standalone corpus-driven approach. Therefore, to predict other paraphrases, we adopt a two-fold approach: (i) Prediction based on Verb-Verb cooccurrences, in case the seed paraphrases are greater than threshold; and (ii) Prediction based on Semantic Relation of NC, otherwise. The system achieves a comparabale score of 0.23 for the isomorphic system while maintaining a score of 0.26 for the non-isomorphic system. 
This work introduces a new unsupervised approach to multilingual word sense disambiguation. Its main purpose is to automatically choose the intended sense (meaning) of a word in a particular context for different languages. It does so by selecting the correct Babel synset for the word and the various Wiki Page titles that mention the word. BabelNet contains all the output information that our system needs, in its Babel synset. Through Babel synset, we find all the possible Synsets for the word in WordNet. Using these Synsets, we apply the disambiguation method Ppr+Freq to find what we need. To facilitate the work with WordNet, we use the ISR-WN which offers the integration of different resources to WordNet. Our system, recognized as the best in the competition, obtains results around 69% of Recall. 
Automatic scoring of short text responses to educational assessment items is a challenging task, particularly because large amounts of labeled data (i.e., human-scored responses) may or may not be available due to the variety of possible questions and topics. As such, it seems desirable to integrate various approaches, making use of model answers from experts (e.g., to give higher scores to responses that are similar), prescored student responses (e.g., to learn direct associations between particular phrases and scores), etc. Here, we describe a system that uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-speciﬁc n-gram features and more general text similarity measures (Heilman and Madnani, 2012). We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. 
Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the inﬂuence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system signiﬁcantly outperforms a strong baseline. 
In this paper, we describe how we created two state-of-the-art SVM classiﬁers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a message (term-level task). Among submissions from 44 teams in a competition, our submissions stood ﬁrst in both tasks on tweets, obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. We implemented a variety of surface-form, semantic, and sentiment features. We also generated two large word–sentiment association lexicons, one from tweets with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated using freely available resources.1 
Umigon is developed since December 2012 as a web application providing a service of sentiment detection in tweets. It has been designed to be fast and scalable. Umigon also provides indications for additional semantic features present in the tweets, such as time indications or markers of subjectivity. Umigon is in continuous development, it can be tried freely at www.umigon.com. Its code is open sourced at: https://github.com/seinecle/Umigon 1. General principle of operation Umigon belongs to the family of lexicon based sentiment classifiers (Davidov et al. 2010, Kouloumpis et al. 2011). It is specifically designed to detect sentiment (positive, negative or neutral) in tweets. The “sentiment detection engine” of Umigon consists of 4 main parts, which are detailed below: - detection of semantic features in the entire tweet. Smileys and onomatopes are given special attention. - evaluation of hashtags. - decomposition of the tweet into a list of its n-grams (up to 4-grams), comparison of each n-gram with the terms in lexicons. In case of a match, a heuristic is applied. - final series of heuristics at the level of the entire tweet, taking advantage of the semantic features detected in the previous steps. A final, unique sentiment (pos, neg or neut) is ascribed to the tweet. 2. The four steps of the classification engine We refer in footnotes to the Java classes which implement the processes described here.  Smileys and onomatopes carry strong indications of sentiment, but also come in a variety of orthographic forms which require methods devoted to their treatment1. Onomatopes and exclamations often include repeated vowels and consonants, as in yeaaaaahhhh (repeated “a” and “h”), but also yeaah (repeated “a”), or yeeeeaaaaah (repeated “e” and “a”). We list the most common exclamations and use regular expressions to capture the variety of forms they can assume. If such a form is found in the tweet, the related sentiment (positive or negative) is saved, and will be evaluated at a final stage for the global sentiment of the entire tweet. Similarly, smileys are frequently spelled in multiple variations: :-) can also be found as :-)) or :-)))))))) . For this reason here also the flexibility of regular expressions is used to detect spelling variations. In addition, we consider that a smiley positioned at the very end of a tweet gives an unambiguous signal as to the sentiment of the tweet. For instance: @mydearfriend You got to see Lady Gaga live, so lucky! Hate you :))) Here, whatever the negative sentiments (Hate you) signaled in the tweet, the final smiley has an overriding effect and signals the strongest sentiment in the tweet. For this reason smileys located in final positions are recorded as such. 2.2 Evaluation of hashtags Hashtags are of special interest as they single out a semantic unit of special significance in the tweet. Exploiting the semantics in a hashtag faces the issue that a hashtag can conflate several terms, as in #greatstuff or #notveryexciting. Umigon applies a series  2.1 Global heuristics  
The paper describes experiments using grid searches over various combinations of machine learning algorithms, features and preprocessing strategies in order to produce the optimal systems for sentiment classiﬁcation of microblog messages. The approach is fairly domain independent, as demonstrated by the systems achieving quite competitive results when applied to short text message data, i.e., input they were not originally trained on. 
Sentiment Analysis in Twitter has become an important task due to the huge user-generated content published over such media. Such analysis could be useful for many domains such as Marketing, Finance, Politics, and Social. We propose to use many features in order to improve a trained classifier of Twitter messages; these features extend the feature vector of uni-gram model by the concepts extracted from DBpedia, the verb groups and the similar adjectives extracted from WordNet, the Sentifeatures extracted using SentiWordNet and some useful domain specific features. We also built a dictionary for emotion icons, abbreviation and slang words in tweets which is useful before extending the tweets with different features. Adding these features has improved the f-measure accuracy 2% with SVM and 4% with NaiveBayes. 
CERPAMID, University of Oriente DI, University of Matanzas  Ave Patricio Lumumba S/N  Autopista  a  Varadero  Km  3  
We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for automatically grading student answers. RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics. An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difﬁculty of translating them, and the presence of acts of translation involved. We view question answering as translation from the question to the answer, from the question to the reference answer, from the answer to the reference answer, or from the question and the answer to the reference answer. Each view is modeled by an RTM model, giving us a new perspective on the ternary relationship between the question, the answer, and the reference answer. We show that all RTM models contribute and a prediction model based on all four perspectives performs the best. Our prediction model is the 2nd best system on some tasks according to the ofﬁcial results of the Student Response Analysis (SRA 2013) challenge. 
Assessing student understanding by evaluating their free text answers to posed questions is a very important task. However, manually, it is time-consuming and computationally, it is difﬁcult. This paper details our shallow NLP approach to computationally assessing student free text answers when a reference answer is provided. For four out of the ﬁve test sets, our system achieved an overall accuracy above the median and mean. 
The domain of DDI identification is constantly showing a rise of interest from scientific community since it represents a decrease of time and healthcare cost. In this paper we purpose a new approach based on shallow linguistic kernel methods to identify DDIs in biomedical manuscripts. The approach outlines a first step in the usage of semantic information for DDI identification. The system obtained an F1 measure of 0.534. 
Drug name entity recognition focuses on identifying concepts appearing in the text that correspond to a chemical substance used in pharmacology for treatment, cure, prevention or diagnosis of diseases. This paper describes a system based on ontologies for identifying the chemical substances in biomedical text. The system achieves an F-1 measure of 0.529 in the task. 
Automatic relation extraction provides great support for scientists and database curators in dealing with the extensive amount of biomedical textual data. The DDIExtraction 2013 challenge poses the task of detecting drugdrug interactions and further categorizing them into one of the four relation classes. We present our machine learning system which utilizes lexical, syntactical and semantic based feature sets. Resampling, balancing and ensemble learning experiments are performed to infer the best conﬁguration. For general drugdrug relation extraction, the system achieves 70.4% in F1 score. 
The development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature. This paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models, such as tensors and matrices, can be used to simulate diﬀerent aspects of predicate logic. This paper discusses how the canonical isomorphism between tensors and multilinear maps can be exploited to simulate a full-blown quantiﬁer-free predicate calculus using tensors. It provides tensor interpretations of the set of logical connectives required to model propositional calculi. It suggests a variant of these tensor calculi capable of modelling quantiﬁers, using few non-linear operations. It ﬁnally discusses the relation between these variants, and how this relation should constitute the subject of future work. 
We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks (MLNs). We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives. We also show that distributional phrase similarity, used as textual inference rules created on the ﬂy, improves its performance. 
Wikipedia articles are annotated by volunteer contributors with numerous links that connect words and phrases to relevant titles. Links to general senses of a word are used concurrently with links to more speciﬁc senses, without being distinguished explicitly. We present an approach to training coarse to ﬁne grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to signiﬁcant improvements in disambiguation. 
We describe a number of techniques for automatically deriving lists of common and proper nouns, and show that the distinction between the two can be made automatically using a vector space model learning algorithm. We present a direct evaluation on the British National Corpus, and application based evaluations on Twitter messages and on automatic speech recognition (where the system could be employed to restore case). 
This paper describes a system for automatically measuring the semantic similarity between two texts, which was the aim of the 2013 Semantic Textual Similarity (STS) task (Agirre et al., 2013). For the 2012 STS task, Heilman and Madnani (2012) submitted the PERP system, which performed competitively in relation to other submissions. However, approaches including word and n-gram features also performed well (Ba¨r et al., 2012; Sˇ aric´ et al., 2012), and the 2013 STS task focused more on predicting similarity for text pairs from new domains. Therefore, for the three variations of our system that we were allowed to submit, we used stacking (Wolpert, 1992) to combine PERP with word and ngram features and applied the domain adaptation approach outlined by Daume III (2007) to facilitate generalization to new domains. Our submissions performed well at most subtasks, particularly at measuring the similarity of news headlines, where one of our submissions ranked 2nd among 89 from 34 teams, but there is still room for improvement. 
We approach the typed-similarity task using a range of heuristics that rely on information from the appropriate metadata ﬁelds for each type of similarity. In addition we train a linear regressor for each type of similarity. The results indicate that the linear regression is key for good performance. Our best system was ranked third in the task. 
In this paper we describe KnCe2013-CORE, a system to compute the semantic similarity of two short text snippets. The system computes a number of features which are gathered from different knowledge bases, namely WordNet, Wikipedia and Wiktionary. The similarity scores derived from these features are then fed into several multilayer perceptron neuronal networks. Depending on the size of the text snippets different parameters for the neural networks are used. The ﬁnal output of the neural networks is compared to human judged data. In the evaluation our system performed sufﬁciently well for text snippets of equal length, but the performance dropped considerably once the pairs of text snippets differ in size. 
The Semantic Textual Similarity (STS) task examines semantic similarity at a sentencelevel. We explored three representations of semantics (implicit or explicit): named entities, semantic vectors, and structured vectorial semantics. From a DKPro baseline, we also performed feature selection and used sourcespeciﬁc linear regression models to combine our features. Our systems placed 5th, 6th, and 8th among 90 submitted systems.  distributional semantics is an important research topic (Mitchell and Lapata, 2008; Erk and Pado´, 2008), we sought to evaluate a principled composition strategy: structured vectorial semantics (Wu and Schuler, 2011). The remainder of this paper proceeds as follows. Section 2 overviews our similarity metrics, and Section 3 overviews the systems that were deﬁned on these metrics. Competition results and additional analyses are in Section 4. We end with discussion on the results in Section 5.  
In this year’s Semantic Textual Similarity evaluation, we explore the contribution of models that provide soft similarity scores across spans of multiple words, over the previous year’s system. To this end, we explored the use of neural probabilistic language models and a TF-IDF weighted variant of Explicit Semantic Analysis. The neural language model systems used vector representations of individual words, where these vectors were derived by training them against the context of words encountered, and thus reﬂect the distributional characteristics of their usage. To generate a similarity score between spans, we experimented with using tiled vectors and Restricted Boltzmann Machines to identify similar encodings. We ﬁnd that these soft similarity methods generally outperformed our previous year’s systems, albeit they did not perform as well in the overall rankings. A simple analysis of the soft similarity resources over two word phrases is provided, and future areas of improvement are described. 
This paper describes the system used by the LIPN team in the Semantic Textual Similarity task at *SEM 2013. It uses a support vector regression model, combining different text similarity measures that constitute the features. These measures include simple distances like Levenshtein edit distance, cosine, Named Entities overlap and more complex distances like Explicit Semantic Analysis, WordNet-based similarity, IR-based similarity, and a similarity measure based on syntactic dependencies.  tures for the global system. These measures include simple distances like Levenshtein edit distance, cosine, Named Entities overlap and more complex distances like Explicit Semantic Analysis, WordNetbased similarity, IR-based similarity, and a similarity measure based on syntactic dependencies. The paper is organized as follows. Measures are presented in Section 2. Then the regression model, based on Support Vector Machines, is described in Section 3. Finally we discuss the results of the system in Section 4.  
This paper describes the UNIBA participation in the Semantic Textual Similarity (STS) core task 2013. We exploited three different systems for computing the similarity between two texts. A system is used as baseline, which represents the best model emerged from our previous participation in STS 2012. Such system is based on a distributional model of semantics capable of taking into account also syntactic structures that glue words together. In addition, we investigated the use of two different learning strategies exploiting both syntactic and semantic features. The former uses ensemble learning in order to combine the best machine learning techniques trained on 2012 training and test sets. The latter tries to overcome the limit of working with different datasets with varying characteristics by selecting only the more suitable dataset for the training purpose. 
We present in this paper the systems we participated with in the Semantic Textual Similarity task at SEM 2013. The Semantic Textual Similarity Core task (STS) computes the degree of semantic equivalence between two sentences where the participant systems will be compared to the manual scores, which range from 5 (semantic equivalence) to 0 (no relation). We combined multiple text similarity measures of varying complexity. The experiments illustrate the different effect of four feature types including direct lexical matching, idf-weighted lexical matching, modified BLEU N-gram matching and named entities matching. Our team submitted three runs during the task evaluation period and they ranked number 11, 15 and 19 among the 90 participating systems according to the official Mean Pearson correlation metric for the task. We also report an unofficial run with mean Pearson correlation of 0.59221 on STS2013 test dataset, ranking as the 3rd best system among the 90 participating systems. 
This article provides a detailed overview of the CPN text-to-text similarity system that we participated with in the Semantic Textual Similarity task evaluations hosted at *SEM 2013. In addition to more traditional components, such as knowledge-based and corpus-based metrics leveraged in a machine learning framework, we also use opinion analysis features to achieve a stronger semantic representation of textual units. While the evaluation datasets are not designed to test the similarity of opinions, as a component of textual similarity, nonetheless, our system variations ranked number 38, 39 and 45 among the 88 participating systems. 
This paper presents three methods to evaluate the Semantic Textual Similarity (STS). The first two methods do not require labeled training data; instead, they automatically extract semantic knowledge in the form of word associations from a given reference corpus. Two kinds of word associations are considered: cooccurrence statistics and the similarity of word contexts. The third method was done in collaboration with groups from the Universities of Paris 13, Matanzas and Alicante. It uses several word similarity measures as features in order to construct an accurate prediction model for the STS. 
We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for judging the semantic similarity between text. RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics. An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difﬁculty of translating them, and the presence of acts of translation involved. We view semantic similarity as paraphrasing between any two given texts. Each view is modeled by an RTM model, giving us a new perspective on the binary relationship between the two. Our prediction model is the 15th on some tasks and 30th overall out of 89 submissions in total according to the ofﬁcial results of the Semantic Textual Similarity (STS 2013) challenge. 
We created a dataset of syntactic-ngrams (counted dependency-tree fragments) based on a corpus of 3.5 million English books. The dataset includes over 10 billion distinct items covering a wide range of syntactic conﬁgurations. It also includes temporal information, facilitating new kinds of research into lexical semantics over time. This paper describes the dataset, the syntactic representation, and the kinds of information provided. 
We propose an unsupervised method for automatically calculating word usage similarity in social media data based on topic modelling, which we contrast with a baseline distributional method and Weighted Textual Matrix Factorization. We evaluate these methods against a novel dataset made up of human ratings over 550 Twitter message pairs annotated for usage similarity for a set of 10 nouns. The results show that our topic modelling approach outperforms the other two methods. 
This paper explores two hypotheses regarding vector space models that predict the compositionality of German noun-noun compounds: (1) Against our intuition, we demonstrate that window-based rather than syntax-based distributional features perform better predictions, and that not adjectives or verbs but nouns represent the most salient part-of-speech. Our overall best result is state-of-the-art, reaching Spearman’s ρ = 0.65 with a wordspace model of nominal features from a 20word window of a 1.5 billion word web corpus. (2) While there are no signiﬁcant differences in predicting compound–modiﬁer vs. compound–head ratings on compositionality, we show that the modiﬁer (rather than the head) properties predominantly inﬂuence the degree of compositionality of the compound. 
In this paper, we propose a simple, languageindependent and highly effective method for predicting the degree of compositionality of multiword expressions (MWEs). We compare the translations of an MWE with the translations of its components, using a range of different languages and string similarity measures. We demonstrate the effectiveness of the method on two types of English MWEs: noun compounds and verb particle constructions. The results show that our approach is competitive with or superior to state-of-the-art methods over standard datasets. 
Automatic metaphor identiﬁcation and interpretation in text have been traditionally considered as two separate tasks in natural language processing (NLP) and addressed individually within computational frameworks. However, cognitive evidence suggests that humans are likely to perform these two tasks simultaneously, as part of a holistic metaphor comprehension process. We present a novel method that performs metaphor identiﬁcation through its interpretation, being the ﬁrst one in NLP to combine the two tasks in one step. It outperforms the previous approaches to metaphor identiﬁcation both in terms of accuracy and coverage, as well as providing an interpretation for each identiﬁed expression. 
Short answer questions for reading comprehension are a common task in foreign language learning. Automatic short answer scoring is the task of automatically assessing the semantic content of a student’s answer, marking it e.g. as correct or incorrect. While previous approaches mainly focused on comparing a learner answer to some reference answer provided by the teacher, we explore the use of the underlying reading texts as additional evidence for the classiﬁcation. First, we conduct a corpus study targeting the links between sentences in reading texts for learners of German and answers to reading comprehension questions based on those texts. Second, we use the reading text directly for classiﬁcation, considering three different models: an answer-based classiﬁer extended with textual features, a simple text-based classiﬁer, and a model that combines the two according to conﬁdence of the text-based classiﬁcation. The most promising approach is the ﬁrst one, results for which show that textual features improve classiﬁcation accuracy. While the other two models do not improve classiﬁcation accuracy, they do investigate the role of the text and suggest possibilities for developing automatic answer scoring systems with less supervision needed from instructors. 
Social scientists are increasingly using the vast amount of text available on social media to measure variation in happiness and other psychological states. Such studies count words deemed to be indicators of happiness and track how the word frequencies change across locations or time. This word count approach is simple and scalable, yet often picks up false signals, as words can appear in different contexts and take on different meanings. We characterize the types of errors that occur using the word count approach, and ﬁnd lexical ambiguity to be the most prevalent. We then show that one can reduce error with a simple reﬁnement to such lexica by automatically eliminating highly ambiguous words. The resulting reﬁned lexica improve precision as measured by human judgments of word occurrences in Facebook posts. 
Implicit arguments are a discourse-level phenomenon that has not been extensively studied in semantic processing. One reason for this lies in the scarce amount of annotated data sets available. We argue that more data of this kind would be helpful to improve existing approaches to linking implicit arguments in discourse and to enable more in-depth studies of the phenomenon itself. In this paper, we present a range of studies that empirically validate this claim. Our contributions are threefold: we present a heuristic approach to automatically identify implicit arguments and their antecedents by exploiting comparable texts; we show how the induced data can be used as training data for improving existing argument linking models; ﬁnally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references.  et al., 1995), the salience of an entity in a discourse is reﬂected by linguistic factors such as choice of referring expression and syntactic form. Both extremes of salience, i.e., contexts of referential continuity (Brown, 1983) and irrelevance, can also be reﬂected by the non-realization of an entity. Altough speciﬁc instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (Sag and Hankamer, 1984; Tanenhaus and Carlson, 1990, inter alia), this phenomenon has widely been ignored in computational approaches to entitybased coherence modeling. It could, however, provide an explanation for local coherence in cases that are not covered by current models of Centering (cf. Louis and Nenkova (2010)). In this work, we propose a new model to predict whether realizing an argument contributes to local coherence in a given position in discourse. Example (1) shows a text fragment, in which argument realization is necessary in the ﬁrst sentence but redundant in the second.  
We present an approach which uses the similarity in semantic structure of bilingual parallel sentences to bootstrap a pair of semantic role labeling (SRL) models. The setting is similar to co-training, except for the intermediate model required to convert the SRL structure between the two annotation schemes used for different languages. Our approach can facilitate the construction of SRL models for resource-poor languages, while preserving the annotation schemes designed for the target language and making use of the limited resources available for it. We evaluate the model on four language pairs, English vs German, Spanish, Czech and Chinese. Consistent improvements are observed over the self-training baseline.  to other languages (McDonald et al., 2011). In this paper we consider correspondences between SRL structures in translated sentences from a different perspective. Most cross-lingual annotation projection approaches transfer the source language annotation scheme to the target language without modiﬁcation, which makes it hard to combine their output with existing target language resources, as annotation schemes may vary signiﬁcantly. We instead address the problem of information transfer between two existing annotation schemes (ﬁgure 1) for a pair of languages using an intermediate model of role correspondence (RCM). An RCM models the probability of a pair of corresponding arguments being assigned a certain pair of roles. We then use it to guide a pair of monolingual models toward compatible predictions on parallel data in order to extend the coverage and/or accuracy of one or both models.  
Existing semantic parsing research has steadily improved accuracy on a few domains and their corresponding databases. This paper introduces FreeParser, a system that trains on one domain and one set of predicate and constant symbols, and then can parse sentences for any new domain, including sentences that refer to symbols never seen during training. FreeParser uses a domain-independent architecture to automatically identify sentences relevant to each new database symbol, which it uses to supplement its manually-annotated training data from the training domain. In cross-domain experiments involving 23 domains, FreeParser can parse sentences for which it has seen comparable unannotated sentences with an F1 of 0.71. 
This survey examines the feedback in current Computer Assisted Pronunciation Training (CAPT) systems and focus on perceptual feedback. The advantages of perceptual feedback are presented, while on the other hand, the reasons why it has not been integrated into commercial CAPT systems are also discussed. This is followed by a suggestion of possible directions of future work. 
There are fewer resources for textual entailment (TE) for Arabic than for other languages, and the manpower for constructing such a resource is hard to come by. We describe here a semi-automatic technique for creating a ﬁrst dataset for TE systems for Arabic using an extension of the ‘headline-lead paragraph’ technique. We also sketch the difﬁculties inherent in volunteer annotators-based judgment, and describe a regime to ameliorate some of these. 
Ongoing research work on Question Answering using multi-document summarization has been described. It has two main sub modules, document retrieval and Multi-document Summarization. We first preprocess the documents and then index them using Nutch with NE field. Stop words are removed and NEs are tagged from each question and all remaining question words are stemmed and then retrieve the most relevant 10 documents. Now, document graph-based query focused multidocument summarizer is used where question words are used as query. A document graph is constructed, where the nodes are sentences of the documents and edge scores reflect the correlation measure between the nodes. The system clusters similar texts from the graph using this edge score. Each cluster gets a weight and has a cluster center. Next, question dependent weights are added to the corresponding cluster score. Top two-ranked sentences of each cluster is identified in order and compressed and then fused to a single sentence. The compressed and fused sentences are included into the output summary with a limit of 500 words, which is presented as answer. The system is tested on data set of INEX QA track from 2011 to 2013 and best readability score was achieved. 
The development of a multi-document summarizer using automatic key-phrase extraction has been described. This summarizer has two main parts; first part is automatic extraction of Key-phrases from the documents and second part is automatic generation of a multidocument summary based on the extracted key-phrases. The CRF based Automatic Keyphrase extraction system has been used here. A document graph-based topic/query focused automatic multi-document summarizer is used for summarization where extracted keyphrases are used as topic. The summarizer has been tested on the standard TAC 2008 test data sets of the Update Summarization Track. Evaluation using the ROUGE-1.5.5 tool has resulted in ROUGE-2 and ROUGE–SU-4 scores of 0.10548 and 0.13582 respectively. 
This paper describes about an automatic technique of evaluating summary. The standard and popular summary evaluation techniques or tools are not fully automatic; they all need some manual process. Using textual entailment (TE) the generated summary can be evaluated automatically without any manual evaluation/process. The TE system is the composition of lexical entailment module, lexical distance module, Chunk module, Named Entity module and syntactic text entailment (TE) module. The syntactic TE system is based on the Support Vector Machine (SVM) that uses twenty five features for lexical similarity, the output tag from a rule based syntactic two-way TE system as a feature and the outputs from a rule based Chunk Module and Named Entity Module as the other features. The documents are used as text (T) and summary of these documents are taken as hypothesis (H). So, if the information of documents is entailed into the summary then it will be a very good summary. After comparing with the ROUGE 1.5.5 evaluation scores, the proposed evaluation technique achieved a high accuracy of 98.25% w.r.t ROUGE-2 and 95.65% w.r.t ROUGE-SU4. 
Knowledge acquisition has been and still remains a hard problem. When it comes to eliciting knowledge from human subjects, an artificial interviewer can be of tremendous benefit. In this paper we present a discourse model for representing the explicit propositional content of a text along with question raising mechanism based on it. This feature is perfectly aligned with the purpose of acquiring more knowledge from the human respondent and acting as a self-extending knowledge base.  We embark on this idea of artificial interviewer for the purpose of knowledge acquisition as a topic or domain ontology. The proposal is to start from a syntactic and semantic analysis of text (parsing) and interpret the parsed information, through the lens of the Systemic Functional Linguistics (Halliday & Matthiessen, 2004), into a coherent and consistent discourse model. Then it can serve as basis for question generation in order to drive further the knowledge elicitation process. The system, therefore, is intended to act as a self-extending knowledge base by means of written interaction with a human respondent.  
The thesis proposed here intends to assist Natural Language Processing tasks through the negation and speculation detection. We are focusing on the biomedical and review domain in which it has been proven that the treatment of these language forms helps to improve the performance of the main task. In the biomedical domain, the existence of a corpus annotated for negation, speculation and their scope has made it possible for the development of a machine learning system to automatically detect these language forms. Although the performance for clinical documents is high, we need to continue working on it to improve the efficiency of the system for scientific papers. On the other hand, in the review domain, the absence of an annotated corpus with this kind of information has led us to carry out the annotation for negation, speculation and their scope of a set of reviews. The next step in this direction will be to adapt it to this domain for the system developed by the biomedical area. 
The present paper provides a summary on the existing approaches to plagiarism detection in multilingual context. Our aim is to organize the available data for the further research. Considering distant language pairs is of a particular interest for us. Cross-language plagiarism detection issue has acquired pronounced importance lately, since semantic contents of a document can be easily and discreetly plagiarized through the use of translation (human or machine-based). We attempt to show the development of detection approaches from the ﬁrst experiments based on machine translation pre-processing to the up-to-date knowledge-based systems that proved to obtain reliable results on various corpora. 
Currently, Text analysis techniques such as named entity recognition rely mainly on ontologies which represent the semantics of an application domain. To build such an ontology from specialized texts, this article presents a tool which detects proper names, locations and dates from texts by using manually written linguistic rules. The most challenging task is to extract not only entities but also interpret the information and adapt in a speciﬁc corpus in French. Keywords named entity extraction, information retrieval, ontology population 
Deﬁnition Extraction (DE) and terminology are contributing to help structuring the overwhelming amount of information available. This article presents KESSI (Knowledge Extraction System for Scientiﬁc Interviews), a multilingual domainindependent machine-learning approach to the extraction of deﬁnitional knowledge, speciﬁcally oriented to scientiﬁc interviews. The DE task was approached as both a classiﬁcation and a sequential labelling task. In the latter, ﬁgures of Precision, Recall and F-Measure were similar to human annotation, and suggest that combining structural, statistical and linguistic features with Conditional Random Fields can contribute signiﬁcantly to the development of DE systems. 
Newswire text is often linguistically complex and stylistically decorated, hence very difﬁcult to comprehend for people with reading disabilities. Acknowledging that events represent the most important information in news, we propose an eventcentered approach to news simpliﬁcation. Our method relies on robust extraction of factual events and elimination of surplus information which is not part of event mentions. Experimental results obtained by combining automated readability measures with human evaluation of correctness justify the proposed event-centered approach to text simpliﬁcation. 
Edit distance is not the only approach how distance between two character sequences can be calculated. Strings can be also compared in somewhat subtler geometric ways. A procedure inspired by Random Indexing can attribute an D-dimensional geometric coordinate to any character N-gram present in the corpus and can subsequently represent the word as a sum of N-gram fragments which the string contains. Thus, any word can be described as a point in a dense N-dimensional space and the calculation of their distance can be realized by applying traditional Euclidean measures. Strong correlation exists, within the Keats Hyperion corpus, between such cosine measure and Levenshtein distance. Overlaps between the centroid of Levenshtein distance matrix space and centroids of vectors spaces generated by Random Projection were also observed. Contrary to standard non-random “sparse” method of measuring cosine distances between two strings, the method based on Random Projection tends to naturally promote not the shortest but rather longer strings. The geometric approach yields finer output range than Levenshtein distance and the retrieval of the nearest neighbor of text’s centroid could have, due to limited dimensionality of Randomly Projected space, smaller complexity than other vector methods. Mèδεις ageôμετρèτος eisitô μου tèή stegèή 
Since language model (LM) is very sensitive to domain mismatch between training and test data, using a group of techniques to adapt a big LM to specific domains is quite helpful. In this paper, we, benefit from salient performance of recurrent neural network to improve domain adapted LM. In this way, we first apply an automatic data selection procedure on a limited amount of in-domain data in order to enrich the training set. After that, we train a domain specific N-gram LM and improve it by using recurrent neural network language model trained on limited in-domain data. Experiments in the framework of EUBRIDGE 1 project on weather forecast dataset show that the automatic data selection procedure improves the word error rate around 2% and RNNLM makes additional improvement over 0.3%. Keywords: Language model, automatic data selection, neural network language model, speech recognition 
This paper presents a new approach to unsupervised learning of inﬂection. The problem is deﬁned as two clusterings of the input wordlist: into lexemes and into forms. Word-Based Morphology is used to describe inﬂectional relations between words, which are discovered using string edit distance. A graph of morphological relations is built and clustering algorithms are used to identify lexemes. Paradigms, understood as sets of word formation rules, are extracted from lexemes and words belonging to similar paradigms are assumed to have the same inﬂectional form. Evaluation was performed for German, Polish and Turkish and the results were compared to conventional morphological analyzers. 
In this paper, we propose a corpus-based method for the annotation of Arabic texts with morphological information. The proposed method proceeds in two stages: the segmentation stage and the morphological analysis stage. The morphological analysis stage is based on a statistical method using an annotated corpus. In order to evaluate our method, we conducted a comparative analysis between the results generated by our system AMAS (Arabic Morphological Annotation System) and those carried out by a human expert. As input, the system accepts an Arabic text and generates as a result an annotated text with morphological information in XML format. 
This paper studies the problem of automated educational test generation. We describe a procedure for generating cloze test items from Russian-language text, which consists of three steps: sentence splitting, sentence ﬁltering, and question generation. The sentence ﬁltering issue is discussed as an application of automatic summarization techniques. We describe a simple experimental system which implements cloze question generation and takes into account grammatical features of the Russian language such as gender and number. 
Most previous research on Korean WordSense Disambiguation (WSD) were focusing on unsupervised corpus-based or knowledge-based approach because they suffered from lack of sense-tagged Korean corpora.Recently, along with great effort of constructing sense-tagged Korean corpus by government and researchers, ﬁnding appropriate features for supervised learning approach and improving its prediction accuracy became an issue. To achieve higher word-sense prediction accuracy, this paper aimed to ﬁnd most appropriate features for Korean WSD based on Conditional Random Field (CRF) approach. Also, we utilized Korean-Japanese parallel corpus to enlarge size of sensetagged corpus, and improved prediction accuracy with it. Experimental result reveals that our method can achieve 95.67% of prediction accuracy. 
This work aims to study the narrative structure of Basque greeting verses from a text classiﬁcation approach. We propose a set of thematic categories for the correct classiﬁcation of verses, and then, use those categories to analyse the verses based on Machine Learning techniques. Classiﬁcation methods such as Naive Bayes, kNN, Support Vector Machines and Decision Tree Learner have been selected. Dimensionality reduction techniques have been applied in order to reduce the term space. The results shown by the experiments give an indication of the suitability of the proposed approach for the task at hands. 
The paper describes the Modern Greek (MG) Grammar, implemented in Grammatical Framework (GF) as part of the Grammatical Framework Resource Grammar Library (RGL). GF is a special-purpose language for multilingual grammar applications. The RGL is a reusable library for dealing with the morphology and syntax of a growing number of natural languages. It is based on the use of an abstract syntax, which is common for all languages, and different concrete syntaxes implemented in GF. Both GF itself and the RGL are open-source. RGL currently covers more than 30 languages. MG is the 35th language that is available in the RGL. For the purpose of the implementation, a morphologydriven approach was used, meaning a bottomup method, starting from the formation of words before moving to larger units (sentences). We discuss briefly the main characteristics and grammatical features of MG, and present some of the major difficulties we encountered during the process of implementation and how these are handled in the MG grammar. 
This paper describes the collection, annotation and linguistic analysis of a gold standard for knowledge-rich context extraction on the basis of Russian and German web corpora as part of ongoing PhD thesis work. In the following sections, the concept of knowledge-rich contexts is refined and gold standard creation is described. Linguistic analyses of the gold standard data and their results are explained. 
We propose a new approach to improving named entity recognition (NER) in broadcast news speech data. The approach proceeds in two key steps: (1) we detect block alignments between highly similar blocks of the speech data and corresponding written news data that are easily obtainable from the Web, (2) we employ term expansion techniques commonly used in information retrieval to recover named entities that were initially missed by the speech transcriber. We show that our method is able to ﬁnd the named entities missing in the transcribed speech data, but also to correct incorrectly assigned named entity tags. Consequently, our novel approach improves state-of-the-art results of NER from speech data both in terms of recall and precision. 
Translation and translation studies rely heavily on distinctive text resources, such as comparable corpora. Comparable corpora gather greater diversity of language-dependent phrases in comparison to multilingual electronic dictionaries or parallel corpora; and present a robust language resource. Therefore, we see comparable corpora compilation as impending in this technological era and suggest an automatic approach to their gathering. The originality of the research lies within the newly-proposed methodology that is guiding the compilation process. We aim to contribute to translation and translation studies professionals’ work by suggesting an approach to obtaining comparable corpora without intermediate human evaluation. This contribution reduces time and presents such professionals with non-static text resources. In our experiment we compare the automatic compilation results to the labels, which two human evaluators have given to the relevant documents. 
In this paper, we present ASMA, a fast and efﬁcient system for automatic segmentation and ﬁne grained part of speech (POS) tagging of Modern Standard Arabic (MSA). ASMA performs segmentation both of agglutinative and of inﬂectional morphological boundaries within a word. In this work, we compare ASMA to two state of the art suites of MSA tools: AMIRA 2.1 (Diab et al., 2007; Diab, 2009) and MADA+TOKAN 3.2. (Habash et al., 2009). ASMA achieves comparable results to these two systems’ state-of-theart performance. ASMA yields an accuracy of 98.34% for segmentation, and an accuracy of 96.26% for POS tagging with a rich tagset and 97.59% accuracy with an extremely reduced tagset. 
This paper introduces a method for improving tree edit distance (TED) for textual entailment. We explore two ways of improving TED: we extend the standard TED to use edit operations that apply to subtrees as well as to single nodes; and we use the ‘artiﬁcial bee colony’ algorithm (ABC) to estimate the cost of edit operations for single nodes and subtrees and to determine thresholds. The preliminary results of the current work for checking entailment between two texts are encouraging compared with the common bag-ofwords, string edit distance and standard TED algorithms. 
Our study focuses on opinion mining of several medical forums dedicated to Hearing Loss (HL). Surgeries related to HL are the most common surgeries in North America; thus, they affect many patients and their families. We have extracted the opinions of people from these forums related to stigma of HL, consequences of HL surgeries, living with HL, failures of HL loss treatments, etc. We performed a manual annotation first with two annotators and have 93% overall agreement with kappa 0.78 and then applied Machine Learning methods to classify the data into opinionated and non-opinionated messages. Using our feature set, we achieved best F-score 0.577 and 0.585 with SVM and logistic-R classifier respectively. 
We present in this paper an unsupervised approach to recognize events, time and place expressions in Arabic texts. Arabic is a resource –scarce language and we don’t easily have at hand annotated corpora, lexicons and other needed NLP tools. We show in this work that we can recognize events, time and place expressions in Arabic texts without using a POS annotated corpus and without lexicon. We use an unsupervised segmentation algorithm then a minimalist set of rules allows us to get a partial POS annotation of our corpus. This partially annotated corpus will serve as a basis for the recognition process which implements a set of rules using specific linguistic markers to recognize events, and expressions of time and place. 
We present ASemiNER, a semisupervised algorithm for identifying Named Entities (NEs) in Arabic text. ASemiNER does not require annotated training data, or gazetteers. It also can be easily adapted to handle more than the three standard NE types (Person, Location, and Organisation). To our knowledge, our algorithm is the ﬁrst study that intensively investigates the semi-supervised pattern-based learning approach to Arabic Named Entity Recognition (NER). We describe ASemiNER and compare its performance with different supervised systems. We evaluate this algorithm by way of experiments to extract the three standard named-entity types. Ultimately, our algorithm outperforms simple supervised systems and also performs well when we evaluate its performance in order to extract three new, specialised types of NEs (Politicians, Sportspersons, and Artists). 
This paper describes a text-reading tool that makes extensive use of widelyavailable NLP tools and resources to aid non-native English speakers overcome language related hindrances while reading a text. It is a web-based tool, that can be accessed from browsers running on PCs or tablets, and provides the reader with an intelligent e-book functionality. 
Sentiment analysis is currently a very dynamic ﬁeld in Computational Linguistics. Research herein has concentrated on the development of methods and resources for different types of texts and various languages. Nonetheless, the implementation of a multilingual system that is able to classify sentiment expressed in various languages has not been approached so far. The main challenge this paper addresses is sentiment analysis from tweets in a multilingual setting. We ﬁrst build a simple sentiment analysis system for tweets in English. Subsequently, we translate the data from English to four other languages - Italian, Spanish, French and German - using a standard machine translation system. Further on, we manually correct the test data and create Gold Standards for each of the target languages. Finally, we test the performance of the sentiment analysis classiﬁers for the different languages concerned and show that the joint use of training data from multiple languages (especially those pertaining to the same family of languages) signiﬁcantly improves the results of the sentiment classiﬁcation. 
We compare two different methods in domain adaptation applied to constituent parsing: parser combination and cotraining, each used to transfer information from the source domain of news to the target domain of natural dialogs, in a setting without annotated data. Both methods outperform the baselines and reach similar results. Parser combination proﬁts most from the large amounts of training data combined with a robust probability model. Co-training, in contrast, relies on a small set of higher quality data. 
Extraction of structured information from text corpora involves identifying entities and the relationship between entities expressed in unstructured text. We propose a novel iterative pattern induction method to extract relation tuples exploiting lexical and shallow syntactic pattern of a sentence. We start with a single pattern to illustrate how the method explores additional paterns and tuples by itself with increasing amount of data. We apply frequency and correlation based ﬁltering and ranking of relation tuples to ensure the correctness of the system. Experimental evaluation compared to other state of the art open extraction systems such as Reverb, textRunner and WOE shows the effectiveness of the proposed system. 
 bothers me the most is never getting to the point of actually going thru the procedure.2  The emergence of social media (networks, blogs, web forums) has given people numerous opportunities to share their personal stories, including details of their health. Although users mostly post under assumed nicknames, state-of-the-art text analysis techniques can combine texts from different media and use that linkage to identify private details of an individual‟s health. In this study we aim to empirically examine the accuracy of identifying authors of on-line posts on a medical forum.1 Our results show a high accuracy of the authorship attribution, especially when text is represented by the orthographical features. 
Twitter is the largest source of microblog text, responsible for gigabytes of human discourse every day. Processing microblog text is difﬁcult: the genre is noisy, documents have little context, and utterances are very short. As such, conventional NLP tools fail when faced with tweets and other microblog text. We present TwitIE, an open-source NLP pipeline customised to microblog text at every stage. Additionally, it includes Twitter-speciﬁc data import and metadata handling. This paper introduces each stage of the TwitIE pipeline, which is a modiﬁcation of the GATE ANNIE open-source pipeline for news text. An evaluation against some state-of-the-art systems is also presented. 
General natural language processing and text-to-speech applications require certain (lexical level) processing steps in order to solve some frequent tasks such as lemmatization, syllabification, lexical stress prediction and phonetic transcription. These steps usually require knowledge of the word’s lexical composition (derivative morphology, inflectional affixes, etc.). For known words all applications use lexicons, but there are always out-of-vocabulary (OOV) words that impede the performance of NLP and speech synthesis applications. In such cases, either rule based or data-driven techniques are used to automatically process these OOV words and generate the desired results. In this paper we describe how the above mentioned tasks can be achieved using a Perceptron with the Margin Infused Relaxed Algorithm (MIRA) and sequence labeling. 
In opinion mining, many linguistic structures, called contextual valence shifters, may modify the prior polarity of items. Some systems of sentiment analysis have tried to take these shifters into account, but few studies have focused on the identiﬁcation of all these structures and their impact on polarized words. In this paper, we describe a method that automatically identiﬁes contextual valence shifters. It relies on a chi-square test applied to the contingency table representing the distribution of a candidate shifter in a corpus of reviews of various opinions. The system depends on two resources in French – a corpus of reviews and a lexicon of valence terms – to build a list of French contextual valence shifters. We also introduce a set of rules used to classify the extracted contextual valence shifters according to their impact on polarized words. They make use of the Pearson residuals in contingency tables to ﬁlter candidate shifters and classify them. We show that the technique reaches an F-measure of either 0.56 or 0.66, depending on how the categories of shifters are deﬁned. 
For efﬁcient diagnosis processes, the multitude of heterogeneous medical data requires seamless integration. In order to automatically align radiology reports and images based on the pathological anatomical entities they describe, a preceding sentence classiﬁcation is necessary. However, the lexical resource used has to contain semantic information about the pathological classiﬁcation of each entity. We introduce an approach to extend medical lexical resources with pathology classiﬁcation information and, at the same time, with new classiﬁed vocabulary. Our algorithm is based on a semi-supervised learning algorithm and incorporates a semantic contextfree grammar combined with a RadLexbased lexicon. 
This paper introduces a new class of temporal expression – named temporal expressions – and methods for recognising and interpreting its members. The commonest temporal expressions typically contain date and time words, like April or hours. Research into recognising and interpreting these typical expressions is mature in many languages. However, there is a class of expressions that are less typical, very varied, and difﬁcult to automatically interpret. These indicate dates and times, but are harder to detect because they often do not contain time words and are not used frequently enough to appear in conventional temporally-annotated corpora – for example Michaelmas or Vasant Panchami. Using Wikipedia and linked data, we automatically construct a resource of English named temporal expressions, and use it to extract training examples from a large corpus. These examples are then used to train and evaluate a named temporal expression recogniser. We also introduce and evaluate rules for automatically interpreting these expressions, and we observe that use of the rules improves temporal annotation performance over existing corpora. 
Current approaches to document-level sentiment analysis rely on local information, e.g., the words within the given document. We try to achieve better performance by incorporating global context of the sentiment target (e.g., a movie or a product). We assume that sentiment labels of reviews about the same target are often consistent in some way. We model this consistency by Dirichlet distribution over sentiment labels and use it together with Maximum entropy classiﬁer to gain signiﬁcant improvement. This unsupervised extension increases the classiﬁcation F-measure by almost 3% absolute on both Czech and English movie review datasets and outperforms the current state of the art. 
In this paper, we present an agglomerative hierarchical clustering algorithm for labelling morphs. The algorithm aims to capture allomorphs and homophonous morphemes for a deeper analysis of segmentation results of a morphological segmentation system. Most morphological segmentation systems focus only on segmentation rather than labelling morphs according to their roles in words, i.e. inﬂectional (cases, tenses etc.) vs. derivational. Nevertheless, it is helpful to have a better understanding of the roles of morphs in a word to be able to judge the grammatical function of that word in a sentence; i.e. the syntactic category. We believe that a good morph labelling system can also help partof-speech tagging. The proposed clustering algorithm can capture allomorphs in Turkish successfully. We obtain a recall of 86.34% for Turkish and 84.79% for English. 
In this paper we look at a task in historical linguistics and the study of language development, namely that of identifying the time when a text was written. The novelty is that we evaluate our classiﬁer and our selected features on literary texts having their action placed in the past and written so as to give off the impression of the respective epoch. We investigate several types of features and ultimately go with a very simple set of 10 features which very accurately classiﬁes the texts based on the century they were actually written in. We use random forests to obtain high performance. 
In this paper we propose a method for identifying cognates based on etymology and etymons. We employ this approach to evaluate the extent to which lexical similarity can be used for automatic detection of cognate pairs. We investigate some orthographic approaches widely used in this research area and some original metrics as well. We apply this procedure for Romanian and its most closely related languages, French and Italian, but our method is applicable to any languages. 
This paper reports on the annotation and maximum-entropy modeling of the semantics of two German prepositions, mit (‘with’) and auf (‘on’). 500 occurrences of each preposition were sampled from a treebank and annotated with syntactosemantic classes by two annotators. The classiﬁcation is guided by a perspective of information extraction, relies on linguistic tests and aims at the separation of semantically transparent and opaque meanings (that is of collocational constructions). Apart from descriptive statistical material, we present results of experiments using monolingual and multilingual evidence (the latter from informative English and Spanish translations) in order to predict the semantic classes. 
In this study, we measure the contribution of different event components and particular semantic relations to the task of event coreference resolution. First we calculate what event times, locations and participants add to event coreference resolution. Secondly, we analyze the contribution by hyponymy and granularity within the participant component. Coreference of events is then calculated from the coreference match scores of each event component. Coreferent action candidates are accordingly filtered based on compatibility of their time, locations, or participants. We report the success rates of our experiments on a corpus annotated with coreferent events. 
In this paper, we propose a novel method for generating a coarse-grained sense inventory from Wikipedia using a machine learning framework. Structural and content-based features are employed to induce clusters of articles representative of a word sense. Additionally, multilingual features are shown to improve the clustering accuracy, especially for languages that are less comprehensive than English. We show the effectiveness of our clustering methodology by testing it against both manually and automatically annotated datasets. 
This paper presents a novel approach to spell checking using dictionary clustering. The main goal is to reduce the number of times distances have to be calculated when ﬁnding target words for misspellings. The method is unsupervised and combines the application of anomalous pattern initialization and partition around medoids (PAM). To evaluate the method, we used an English misspelling list compiled using real examples extracted from the Birkbeck spelling error corpus. 
In this paper we present a case study focusing on the literature genre, in particular on Italian ﬁctional prose, aimed at identifying the features characterizing this text type. Identiﬁed features were tested in two classiﬁcation tasks, i.e. by genre and by readability, with promising results. Interestingly, the same multi–level set of linguistic features turned out to reliably capture variation within and across textual genres. 
Part-of-speech information is a pre-requisite in many NLP algorithms. However, Twitter text is difﬁcult to part-of-speech tag: it is noisy, with linguistic errors and idiosyncratic style. We present a detailed error analysis of existing taggers, motivating a series of tagger augmentations which are demonstrated to improve performance. We identify and evaluate techniques for improving English part-of-speech tagging performance in this genre. Further, we present a novel approach to system combination for the case where available taggers use different tagsets, based on voteconstrained bootstrapping with unlabeled data. Coupled with assigning prior probabilities to some tokens and handling of unknown words and slang, we reach 88.7% tagging accuracy (90.5% on development data). This is a new high in PTB-compatible tweet part-of-speech tagging, reducing token error by 26.8% and sentence error by 12.2%. The model, training data and tools are made available. 
We link the weighted maximum entropy and the optimization of the expected Fβmeasure, by viewing them in the framework of a general common multi-criteria optimization problem. As a result, each solution of the expected Fβ-measure maximization can be realized as a weighted maximum likelihood solution - a well understood and behaved problem. The speciﬁc structure of maximum entropy models allows us to approximate this characterization via the much simpler class-wise weighted maximum likelihood. Our approach reveals any probabilistic learning scheme as a speciﬁc trade-off between different objectives and provides the framework to link it to the expected Fβ-measure. 
Verbs in Romanian sometimes manifest local irregularities in the form of alternating letters. We present a sequence tagging based method for learning stem alternations and ending sequences. Supervised training is based on a morphological dictionary, with a few regular expression paradigms encoded by hand. Our best model improves upon previous machine learning approaches to Romanian verb conjugation, and can generalize to unseen paradigms that can be constructed as variations of the ones in the training set. 
The occurrence of syntactic phenomena such as coordination and subordination is characteristic of long, complex sentences. Text simpliﬁcation systems need to detect and categorise constituents in order to generate simpler sentences. These constituents are typically bounded or linked by signs of syntactic complexity, which include conjunctions, complementisers, whwords, and punctuation marks. This paper proposes a supervised tagging approach to classify these signs in accordance with their linking and bounding functions. The performance of the approach is evaluated both intrinsically, using an annotated corpus covering three different genres, and extrinsically, by evaluating the impact of classiﬁcation errors on an automatic text simpliﬁcation system. The results are encouraging. 
We propose new automatic evaluation metric to evaluate machine translation. Diﬀerent from most similar metrics, our proposed metric does not depend heavily on sentence length. In most metrics based on f-measure comparisons of reference and candidate translations, the relative weight of each mismatched word in short sentences is larger than it in long sentences. Therefore, the evaluation score becomes disproportionally low in short sentences even when only one non-matching word exists. In our metric, the weight of each mismatched word is kept small even in short sentences. We designate our metric as Automatic Evaluation Metric that is Independent of Sentence Length (AILE). Experimental results indicate that AILE has the highest correlation with human judgments among some leading metrics. 
We are presenting work on recognising acronyms of the form Long-Form (Short-Form) such as “International Monetary Fund (IMF)” in millions of news articles in twenty-two languages, as part of our more general effort to recognise entities and their variants in news text and to use them for the automatic analysis of the news, including the linking of related news across languages. We show how the acronym recognition patterns, initially developed for medical terms, needed to be adapted to the more general news domain and we present evaluation results. We describe our effort to automatically merge the numerous long-form variants referring to the same short-form, while keeping non-related long-forms separate. Finally, we provide extensive statistics on the frequency and the distribution of shortform/long-form pairs across languages. 
This paper presents a new automated method for evaluating the content of a text summary. The proposed method is based on a combination of features encompassing scores of content and others of linguistic quality. This method relies on a learning technique called linear regression. The objective of this combination is to predict the PYRAMID score from the features used. In order to evaluate the presented method, we are interested in two levels of granularity evaluation: the first is named Microevaluation and proposes an evaluation of each summary while the second is called Macro-evaluation and it is applied at the level of each system. The resulting metric shows an improvement upon standard metrics by increasing the correlation with the PYRAMID metric. 
A table-of-contents (TOC) provides a quick reference to a document’s content and structure. We present the ﬁrst study on identifying the hierarchical structure for automatically generating a TOC using only textual features instead of structural hints e.g. from HTML-tags. We create two new datasets to evaluate our approaches for hierarchy identiﬁcation. We ﬁnd that our algorithm performs on a level that is sufﬁcient for a fully automated system. For documents without given segment titles, we extend our work by automatically generating segment titles. We make the datasets and our experimental framework publicly available in order to foster future research in TOC generation. 
This paper introduces the first pattern-based Persian Temporal Relation Classifier (PTRC) that finds the type of temporal relations between pairs of events in the Persian texts. The proposed system uses support vector machines (SVMs) equipped by combinations of simple, convolution tree, and string subsequence kernels (SSK). In order to evaluate the algorithm, we have developed a Persian TimeBank (PTB) corpus. PTRC not only increases the performance of the classification by applying new features and SSK, but also alleviates the probable adverse effects of the Free Word Orderness (FWO) of Persian on temporal relation classification. We have also applied our proposed algorithm to two standard corpora on English (i.e., TimeBank and TempEval-2) to measure the efficiency of the new features and SSK. The experiments show the accuracies of 65.6%, 59.53%, 50.2%, and 62.17% on an augmented version of PTB, TimeBank, tasks E and F of TempEval-2, respectively. Consequently, we have achieved the third best result on TimeBank, and the second best result on the task F of TempEval-2. 
 In this paper we introduce an approach to lexical description which is sufﬁciently powerful to support language processing tasks such as part-of-speech tagging or sentence recognition, traditionally considered the province of external algorithmic components. We show how this approach can be implemented in the lexical description language, DATR, and provide examples of modelling extended lexical phenomena. We argue that applying a modelling approach originally designed for lexicons to a wider range of language phenomena brings a new perspective to the relationship between theory-based and empirically-based approaches to language processing. 
(2)This paper reports on an application that delivers automated formative feedback designed to help university students improve their assignments. (3)The aim of the system is to improve the conﬁdence and skills of the user by promoting selfdirected learning through metacognition. (4)The system focuses on the content of an essay by using automatic summarisation techniques, automatic structure recognition, diagrams, animations, and interactive exercises that promote reﬂection. (15)The system is currently undergoing initial exploratory rounds of testing by ex-student volunteers and will be the subject of two full-scale empirical evaluations starting in September 2013. (1)The main claims of this paper are the application and adaptation of graph-based key word and key sentence ranking methods for a novel purpose, and ensuing observations concerning the suitability of two different centrality algorithms for the purposes of key word extraction. 
The problem of answering multi-sentence questions is addressed in a number of products and services-related domains. A candidate set of answers, obtained by a keyword search, is re-ranked by matching the set of parse trees of an answer with that of the question. To do that, a graph representation and learning technique for parse structures for paragraphs of text have been developed. Parse Thicket (PT) as a set of syntactic parse trees augmented by a number of arcs for inter-sentence word-word relations such as co-reference and taxonomic relations is introduced. These arcs are also derived from other sources, including Speech Act and Rhetoric Structure theories. The proposed approach is subject to evaluation in the product search and recommendation domain, where search queries include multiple sentences. An open source plugin for SOLR is developed so that the proposed technology can be easily integrated with industrial search engines. 
In this paper we present the functional automata as a general framework for representation, training and exploring of various statistical models as LLM’s, HMM’s, CRF’s, etc. Our contribution is a new construction that allows the representation of the derivatives of a function given by a functional automaton. It preserves the natural representation of the functions and the standard product and sum operations of real numbers. In the same time it requires no additional overhead for the standard dynamic programming techniques that yield the computation of a functional value. 
Opinion analysis deals with public opinions and trends, but subjective language is highly ambiguous. In this paper, we follow a simple data-driven technique to learn ﬁne-grained opinions. We select an intersection set of Wall Street Journal documents that is included both in the Penn Discourse Tree Bank (PDTB) and in the Multi-Perspective Question Answering (MPQA) corpus. This is done in order to explore the usefulness of discourselevel structure to facilitate the extraction of ﬁne-grained opinion expressions. Here we perform shallow parsing of MPQA expressions with connective based discourse structure, and then also with Named Entities (NE) and some syntax features using conditional random ﬁelds; the latter feature set is basically a collection of NEs and a bundle of features that is proved to be useful in a shallow discourse parsing task. We found that both of the feature-sets are useful to improve our baseline at different levels of this ﬁne-grained opinion expression mining task. 
Most empirically-based approaches to NL generation elaborate on co-occurrences and frequencies observed over a corpus, which are then accommodated by learning algorithms. This method fails to capture generalities in generation subtasks, such as generating referring expressions, so that results obtained for some corpus cannot be transferred with confidence to similar environments or even to other domains. In order to obtain a more general basis for choices in referring expression generation, we formulate situational and task-specific properties, and we test to what degree they hold in a specific corpus. As a novelty, we incorporate features of the role of the underlying task, object identification, into these property specifications; these features are inherently domain-independent. Our method has the potential to enable the development of a repertoire of regularities that express generalities and differences across situations and domains, which supports the development of generic algorithms and also leads to a better understanding of underlying dependencies. 
Research in text classiﬁcation currently focuses on challenging tasks such as sentiment classiﬁcation, modality identiﬁcation, and so on. In these tasks, approaches that use a structural representation, like a tree, have shown better performance rather than a bag-of-words representation. In this paper, we propose a boosting algorithm for classifying a text that is a set of sentences represented by tree. The algorithm learns rules represented by subtrees with their frequency information. Existing boostingbased algorithms use subtrees as features without considering their frequency because the existing algorithms targeted a sentence rather than a text. In contrast, our algorithm learns how the occurrence frequency of each subtree is important for classiﬁcation. Experiments on topic identiﬁcation of Japanese news articles and English sentiment classiﬁcation shows the effectiveness of subtree features with their frequency. 
Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classiﬁcation, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus. 
This paper presents a method of lexical semantic disambiguation in multilingual corpora and describes the construction of an artiﬁcial word-aligned and lexically disambiguated gold-standard corpus from an existing multilingual resource. The suggested approach uses sets of aligned words and phrases across languages as unique semantic tags similar to WordNet synsets that can be used as a part of unsupervised natural language processing and information retrieval tasks. The approach goes beyond one-to-one word alignment, and uses an algorithm for the aggregation of results of pair-wise word alignment when the corpus contains several languages. When applied to the new corpus, this methodology has proven capable of reducing the ambiguity of a polysemous word by one third on average. 
The paper1 presents a rule-based approach to semantic relation recognition within the Polish noun phrase. A set of semantic relations, including some thematic relations, has been determined for the need of experiments. The method consists in two steps: ﬁrst the system recognizes word pairs and triples, and then it classiﬁes the relations. Evaluation was performed on random samples from two balanced Polish corpora. 
 has attracted far less interest for unsupervised learning.  We describe an approach to building a morphological analyser of Arabic by inducing a lexicon of root and pattern templates from an unannotated corpus. Using maximum entropy modelling, we capture orthographic features from surface words, and cluster the words based on the similarity of their possible roots or patterns. From these clusters, we extract root and pattern lexicons, which allows us to morphologically analyse words. Further enhancements are applied, adjusting for morpheme length and structure. Final root extraction accuracy of 87.2% is achieved. In contrast to previous work on unsupervised learning of Arabic morphology, our approach is applicable to naturally-written, unvowelled Arabic text.  In this paper we present an approach to unsupervised learning of non-concatenative morphology, applying it to Arabic. We describe an approach to learning tri-literal roots and affix template of Arabic by first inducing root and affix lexicons. Our approach uses Maximum Entropy modelling to obtain clusters1 of words based on concatenative and non-concatenative orthographic features, and induces the lexicons from these clusters. Our data is an undiacritized version of the Quranic Arabic Corpus since we assume a realistic setting of unvowelled text, as most Arabic text is written without vowels; we chose this corpus since correct roots of each word are available, facilitating the evaluation process. The fact that the  
We improve upon a previous line of work for parsing web data, by exploring the impact of different decisions regarding the training data. First, we compare training on automatically POS-tagged data vs. gold POS data. Secondly, we compare the effect of training and testing within sub-genres, i.e., whether a close match of the genre is more important than training set size. Finally, we examine different ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways (sentence length, perplexity). In general, we ﬁnd that approximating the in-domain data has a positive impact on parsing. 
In this work, we present a new task for testing compositional distributional semantic models. Recently, there has been a spate of research into how distributional representations of individual words can be combined to represent the meaning of phrases. Vecchi et al. (2011) have shown that some compositional models, including the additive and multiplicative models of Mitchell and Lapata (2008; 2010) and the linear map-based model of Baroni and Zamparelli (2010), can be applied to detect semantically anomalous adjective– noun combinations. We extend their experiments and apply these models to the combinations extracted from texts written by learners of English. Our work contributes to the ﬁeld of compositional distributional semantics by introducing a new test paradigm for semantic models and shows how these models can be used for error detection in language learners’ content word combinations. 
We present an incremental dependency parser which derives predictions about the upcoming structure in a parse-as-youtype mode. Drawing on the inherent strong anytime property of the underlying transformation-based approach, an existing system, jwcdg, has been modiﬁed to make it truly interruptible. A speed-up was achieved by means of parallel processing. In addition, MaltParser is used to bootstrap the search which increases accuracy under tight time constraints. With these changes, jwcdg can effectively utilize the full time span until the next word becomes available which results in an optimal quality-time trade-off. 
The Unit Graphs (UGs) framework is a graph-based knowledge representation (KR) formalism that is designed to allow for the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory Combinatorial Dictionary of the Meaning-Text Theory (MTT). This paper introduces the UGs framework, and overviews current published outcomes. It ﬁrst introduces rationale of this new formalism: neither semantic web formalisms nor Conceptual Graphs can represent linguistic predicates. It then overviews the foundational concepts of this framework: the UGs are deﬁned over a UG-support that contains: i) a hierarchy of unit types which is strongly driven by the actantial structure of unit types, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identiﬁers. On these foundational concepts and on the deﬁnition of UGs, this paper ﬁnally overviews current outcomes of the UGs framework: the deﬁnition of a deep-semantic representation level for the MTT, representation of lexicographic definitions of lexical units in the form of semantic graphs, and two formal semantics: one based on UGs closure and homomorphism, and one based on model semantics. 
We are interested in a graph-based Knowledge Representation formalism that would allow for the representation, manipulation, query, and reasoning over dependency structures, and linguistic knowledge of the Explanatory and Combinatorial Dictionary in the Meaning-Text Theory framework. Neither the semantic web formalisms nor the conceptual graphs appear to be suitable for this task, and this led to the introduction of the new Unit Graphs framework. This paper ﬁrst introduces the foundational concepts of this framework: Unit Graphs are deﬁned over a support that contains: i) a hierarchy of unit types which is strongly driven by their actantial structure, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identiﬁers. Then, this paper provides all of these objects with a model semantics that enables to deﬁne the notion of semantic consequence between Unit Graphs. 
Information extraction systems automatically extract structured information from machine-readable documents, such as newswire, web, and multimedia. Despite signiﬁcant improvement, the performance is far from perfect. Hence, it is useful to accurately estimate conﬁdence in the correctness of the extracted information. Using the Knowledge Base Population Slot Filling task as a case study, we propose a conﬁdence estimation model based on the Maximum Entropy framework, obtaining an average precision of 83.5%, Pearson coefﬁcient of 54.2%, and 2.3% absolute improvement in F-measure score through a weighted voting strategy. 
We look into the problem of recognizing citation functions in scientiﬁc literature, trying to reveal authors’ rationale for citing a particular article. We introduce an annotation scheme to annotate citation functions in scientiﬁc papers with coarse-to-ﬁne-grained categories, where the coarse-grained annotation roughly corresponds to citation sentiment and the ﬁnegrained annotation reveals more about citation functions. We implement a Maximum Entropy-based system trained on annotated data under this scheme to automatically classify citation functions in scientiﬁc literature. Using combined lexical and syntactic features, our system achieves the F-measure of 67%. 
Translating from English, a morphologically poor language, into morphologically rich languages such as Persian comes with many challenges. In this paper, we present an approach to rich morphology prediction using a parallel corpus. We focus on the verb conjugation as the most important and problematic phenomenon in the context of morphology in Persian. We deﬁne a set of linguistic features using both English and Persian linguistic information, and use an English-Persian parallel corpus to train our model. Then, we predict six morphological features of the verb and generate inﬂected verb form using its lemma. In our experiments, we generate verb form with the most common feature values as a baseline. The results of our experiments show an improvement of almost 2.1% absolute BLEU score on a test set containing 16K sentences. 
Many sentiment-analysis methods for the classification of reviews use training and test-data based on star ratings provided by reviewers. However, when reading reviews it appears that the reviewers’ ratings do not always give an accurate measure of the sentiment of the review. We performed an annotation study which showed that reader perceptions can also be expressed in ratings in a reliable way and that they are closer to the text than the reviewer ratings. Moreover, we applied two common sentiment-analysis techniques and evaluated them on both reader and reviewer ratings. We come to the conclusion that it would be better to train models on reader ratings, rather than on reviewer ratings (as is usually done). 
In this paper we undertake a large crossdomain investigation of sentiment domain adaptation, challenging the practical necessity of sentiment domain adaptation algorithms. We first show that across a wide set of domains, a simple “all-in-one” classifier that utilizes all available training data from all but the target domain tends to outperform published domain adaptation methods. A very simple ensemble classifier also performs well in these scenarios. Combined with the fact that labeled data nowadays is inexpensive to come by, the “kitchen sink” approach, while technically nonglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that “flip” polarity across domains is not borne out empirically. 
We report on our efforts aimed at building an Open Domain Question Answering system for Polish. Our contribution is twofold: we gathered a set of question–answer pairs from various Polish sources and we performed an empirical evaluation of two re-ranking methods. The gathered collection contains factoid, list, non-factoid and yes-no questions, which makes a challenging material for experiments. We show that using two re-ranking methods based on term proximity allows to obtain signiﬁcant improvement on simple information retrieval baseline. The improvement is observed as ﬁnding more answer-bearing documents among the top n search results. 
In this paper we cover the problem of recognition of semantic relations between proper names (PNs) in running text. We focus on the manual rule creation approach and discuss to what extent the existing tools can be used for this task. As a result of our initial research we developed a rule-based toolset for recognition of relations between PNs called WCCL Relation. The toolset is built on the top of WCCL Match — a language for text annotation, which is a part of a WCCL framework (an open source, released under the GNU LGPL 3.0). The WCCL Relation toolset is language independent and can be used for almost any natural language and language tagset. We present several use cases and sample rules for recognition of semantic relations in Polish texts. 
Wordnets are lexico-semantic resources essential in many NLP tasks. Princeton WordNet is the most widely known, and the most inﬂuential, among them. Wordnets for languages other than English tend to adopt unquestioningly WordNet’s structure and its net of lexicalised concepts. We discuss a large wordnet constructed independently of WordNet, upon a model with a small yet signiﬁcant difference. A mapping onto WordNet is under way; the large portions already linked open up a unique perspective on the comparison of similar but not fully compatible lexical resources. We also try to characterise numerically a wordnet’s aptitude for NLP applications. 
Grammar induction is a basic step in natural language processing. Based on the volume of information that is used by different methods, we can distinguish three types of grammar induction method: supervised, unsupervised, and semi-supervised. Supervised and semisupervised methods require large tree banks, which may not currently exist for many languages. Accordingly, many researchers have focused on unsupervised methods. Unsupervised Data Oriented Parsing (UDOP) is currently the state of the art in unsupervised grammar induction. In this paper, we show that the performance of UDOP in free word order languages such as Persian is inferior to that of fixed order languages such as English. We also introduce a novel approach called History-based unsupervised data oriented Parsing, and show that the performance of UDOP can be significantly improved by using some history information, especially in dealing with free word order languages. 
Automatic recognition of CORROBORATE and CONTRAST relations between citations may enhance citation analysis. We describe a system that identiﬁes these citation relations using predicate/argument and discourse structures. 
For the last decade, distributional semantics has been an active area of research to address the problem of understanding the semantics of words in natural language. The core principal of the distributional semantic approach is that the linguistic context surrounding a given word, which is represented as a vector, provides important information about its meaning. In this paper we investigate the possibility to exploit Combinatory Categorial Grammar (CCG) categories as syntactic features to be relevant for characterizing the context vector and hence the meaning of words. We ﬁnd that the CCG categories can enhance the representation of verb meaning. 
Real-word errors or context sensitive spelling errors, are misspelled words that have been wrongly converted into another word of vocabulary. One way to detect and correct real-word errors is using Statistical Machine Translation (SMT), which translates a text containing some real-word errors into a correct text of the same language. In this paper, we improve the results of mentioned SMT system by employing some discourseaware features into a log-linear reranking method. Our experiments on a real-world test data in Persian show an improvement of about 9.5% and 8.5% in the recall of detection and correction respectively. Other experiments on standard English test sets also show considerable improvement of real-word checking results. 
This paper describes a computational linguistics-based approach for providing interoperability between multi-lingual systems in order to overcome crucial issues like cross-language and cross-collection retrieval. Our proposal is a system which improves capabilities of language-technology-based information extraction. In the last few years various theories have been developed and applied for making multicultural and multilingual resources easy to access. Important initiatives, like the development of the European Library and Europeana, aim to increase the availability of digital content from various types of providers and institutions. Therefore the accessibility to these resources requires the development of environments enabling to manage multilingual complexity. In this respect, we present a methodological framework which allows mapping both the data and the metadata among the languagespecific ontologies. The feasibility of crosslanguage information extraction and semantic search will be tested by implementing an early prototype system. 
A basic task in opinion mining deals with determining the overall polarity orientation of a document about some topic. This has several applications such as detecting consumer opinions in on-line product reviews or increasing the effectiveness of social media marketing campaigns. However, the informal features of Web 2.0 texts can affect the performance of automated opinion mining tools. These are usually short and noisy texts with presence of slang, emoticons and lexical variants which make more difﬁcult to extract contextual and semantic information. In this paper we demonstrate that the use of lexical normalisation techniques can be used to enhance polarity detection results by replacing informal lexical variants with their canonical version. We have carried out several polarity classiﬁcation experiments using English texts from different Web 2.0 genres and we have obtained the best result with microblogs where normalisation contribution to the classiﬁcation model can be up to 6.4%. 
The paper focuses on requests in written forms, where we describe a novel approach to computational modelling of specific features of politeness in speech act of requesting. We examine the similarities and differences in the use of specific social and expressive factors in two languages (mother tongue and a foreign language). The requests collected from different social situations among students and their teachers in a university environment were used as data source for a research. Transaction/Sequence model for text representation was formulated and association rules analysis was applied as a research method. The findings are interesting mainly in terms of differences in the use of politeness features in foreign language and mother tongue. The results indicated that the requests written in mother tongue are less direct than in foreign language. 
Research on statistical machine translation has focused on particular translation directions, typically with English as the target language, e.g., from Arabic to English. When we reverse the translation direction, the multiple reference translations turn into multiple possible inputs, which offers both challenges and opportunities. We propose and evaluate several strategies for making use of these multiple inputs: (a) select one of the datasets, (b) select the best input for each sentence, and (c) synthesize an input for each sentence by fusing the available inputs. Surprisingly, we ﬁnd out that it is best to tune on the hardest available input, not on the one that yields the highest BLEU score. This ﬁnding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 
Cloze questions are questions containing sentences with one or more blanks and multiple choices listed to pick an answer from. In this work, we present an automatic Cloze Question Generation (CQG) system that generates a list of important cloze questions given an English article. Our system is divided into three modules: sentence selection, keyword selection and distractor selection. We also present evaluation guidelines to evaluate CQG systems. Using these guidelines three evaluators report an average score of 3.18 (out of 4) on Cricket World Cup 2011 data. 
In this paper, we report on an unsupervised greedy-style process for acquiring phrase translations from sentence-aligned parallel corpora. Thanks to innovative selection strategies, this process can acquire multiple translations without size criteria, i.e. phrases can have several translations, can be of any size, and their size is not considered when selecting their translations. Even though the process is in an early development stage and has much room for improvements, evaluation shows that it yields phrase translations of high precision that are relevant to machine translation but also to a wider set of applications including memory-based translation or multi-word acquisition. 
This article presents a feasibility study for retrieving Wikipedia articles matching patents’ topics. The long term motivation behind it is to facilitate patent search by enriching patent indexing with relevant keywords found in external (terminological) resources, with their monolingual synonyms and multilingual translations. The similarity between patents and Wikipedia articles is measured using various ﬁltering techniques and patent document sections. The most similar Wikipedia articles happen to be the closest ones to the respective patent in 33% of the cases, otherwise they are within the top 12 ranked articles. 
Our paper is concerned with investigating the impact of translationese on the novels of a bilingual writer and asking whether one could determine the authorship of a translated document. The main part of our paper will be centered on selecting a good set of lexical features that can be considered characteristic for an author. We used in our research the novels of Vladimir Nabokov, a bilingual author, who wrote his works in both Russian and English. Each text is represented by a vector of function words. We are interested in determining how the results vary across different feature sets and which feature set could be considered the most representative. In order to inspect our results we used a hierarchical clustering method and draw conclusions based on the most frequent result. 
We present PurePos, an open-source HMM-based automatic morphological annotation tool. PurePos can perform tagging and lemmatization at the same time, it is very fast to train, with the possibility of easy integration of symbolic rulebased components into the annotation process that can be used to boost the accuracy of the tool. The hybrid approach implemented in PurePos is especially beneﬁcial in the case of rich morphology, highly detailed annotation schemes and if a small amount of training data is available. Evaluation of the tool was on a Hungarian corpus revealed that its hybrid components signiﬁcantly improve overall annotation accuracy. 
Most sentiment analysis approaches rely on machine-learning techniques, using a bag-of-words (BoW) document representation as their basis. In this paper, we examine whether a more ﬁne-grained representation of documents as sequences of emotionally-annotated sentences can increase document classiﬁcation accuracy. Experiments conducted on a sentence and document level annotated corpus show that the proposed solution, combined with BoW features, offers an increase in classiﬁcation accuracy. 
The paper presents a wordnet expansion algorithm, which is based on lexicosemantic relations extracted from large text corpora. We do not assume that the extracted relation instances (i.e. word pairs) are described by probabilities. Thus, results produced by any method, including pattern-based and Distributional Semantics approaches can be used. The algorithm is based on a general spreading activation model. Support for word-to-word semantic associations is ﬁrst mapped on the existing wordnet structure. Next, the support is spread over the wordnet network in order to ﬁnd attachment areas for a new word. Evaluation and comparison with other approaches in experiments on Princeton WordNet 3.0 is presented. 
In this paper the author presents a new context independent method for bilingual term mapping using maximised character alignment maps. The method tries to particularly address mapping of multi-word terms and compound terms that are extracted from comparable corpora. The method allows integrating linguistic resources (e.g., probabilistic dictionaries and character based transliteration systems) that significantly increase the mapping recall while maintaining a stable precision. The term mapping method has been automatically evaluated using the EuroVoc thesaurus with varying availability of linguistic resources and on terms extracted from Latvian-English medical domain comparable corpus collected from the Web. The paper shows that the results significantly outperform previously reported results on the same evaluation corpus. 
The lack of labeled data always poses challenges for tasks where machine learning is involved. Semi-supervised and cross-domain approaches represent the most common ways to overcome this difﬁculty. Graph-based algorithms have been widely studied during the last decade and have proved to be very effective at solving the data limitation problem. This paper explores one of the most popular stateof-the-art graph-based algorithms - label propagation, together with its modiﬁcations previously applied to sentiment classiﬁcation. We study the impact of modiﬁed graph structure and parameter variations and compare the performance of graph-based algorithms in cross-domain and semi-supervised settings. The results provide a strategy for selecting the most favourable algorithm and learning paradigm on the basis of the available labeled and unlabeled data. 
 plays a major role in boosting the efficiency of the translation system.  Arabic is a morphologically rich and complex language, which presents significant challenges for natural language processing and machine translation. In this paper, we describe an ongoing effort to build our first Arabic-French phrase– based machine translation system using the Moses decoder among other linguistic tools. The results show an improvement in the quality of translation and a gain in terms of Bleu score after introducing a pre-processing scheme for Arabic and applying some rules based on morphological variations of the source language. The proposed approach is completed without increasing the amount of training data or changing radically the algorithms that can affect the translation or training engines. 
We present an unsupervised learning model that induces phrasal inversion transduction grammars by introducing a minimum conditional description length (CDL) principle to drive search over a space defined by two opposing extreme types of ITGs. Our approach attacks the difficulty of acquiring more complex longer rules when inducing inversion transduction grammars via unsupervised bottom-up chunking, by augmenting its model search with top-down segmentation that minimizes CDL, resulting in significant translation accuracy gains. Chunked rules tend to be relatively short; long rules are hard to learn through chunking, as the smaller parts of the long rules may not necessarily be good translations themselves. Our objective criterion is a conditional adaptation of the notion of description length, that is conditioned on a fixed preexisting model, in this case the initial chunked ITG. The notion of minimum CDL (MCDL) facilitates a novel strategy for avoiding the pitfalls of premature pruning in chunking approaches, by incrementally splitting an ITG with reference to a second ITG that conditions this search. 
This paper proposes a two-step approach to ﬁnd hypernym relations between pairs of noun phrases in Dutch text. We ﬁrst apply a pattern-based approach that combines lexical and shallow syntactic information to extract a list of candidate hypernym pairs from the input text. In a second step, distributional similarity information is used to ﬁlter the obtained list of candidate pairs. Evaluation of the system shows encouraging results and reveals that the distributional information particularly helps to improve the precision for context dependent hypernym pairs. The proposed hypernym module is considered an important step in building a semantic structure for automatically extracted terminology. As our approach does not require external lexical resources, it can be applied for any given Dutch input text and is particularly well suited for domain and user speciﬁc text. 
We report on the recent development of ParZu, a German dependency parser. We discuss the effect of POS tagging and morphological analysis on parsing performance, and present novel ways of improving performance of the components, including the use of morphological features for POS-tagging, the use of syntactic information to select good POS sequences from an n-best list, and using parsed text as training data for POS tagging and statistical parsing. We also describe our efforts towards reducing the dependency on restrictively licensed and closed-source NLP resources. 
The measurement of semantic relatedness between two words is an important metric for many natural language processing applications. In this paper, we present a novel approach for measuring semantic relatedness that is based on a weighted semantic network. This approach explores the use of a lexicon, semantic relation types as weights, and word deﬁnitions as a basis to calculate semantic relatedness. Our results show that our approach outperforms many lexicon-based methods to semantic relatedness, especially on the TOEFL synonym test, achieving an accuracy of 91.25%. 
The purpose of part-of-speech tagging is to automatically tag the words of a text, written in a certain language, with labels that usually take the form of acronyms that designate the appropriate parts-of-speech. In this paper we propose a new approach to the problem that divides it in two different tasks: a learning task and an optimization task. We tackled each of those tasks with evolutionary computation techniques: genetic algorithms and a particle swarm optimizer. We emphasize the use of swarm intelligence, not only for the good results achieved, but also because it is one of the ﬁrst applications of such algorithms to this problem. We believe that this approach is generic enough so that it can be though as an alternative approach to solve other natural language processing tasks that share some fundamental characteristics with the part-of-speech tagging problem. The results obtained in two different English corpora are among the best published ones. 
With 19%–28% of Internet users participating in online health discussions, it became imperative to be able to detect and analyze posted personal health information (PHI). In this work we introduce two semantic-based methods for mining PHI on social networks which will warn the users about potential privacy breaches. One method uses WordNet as a source of health-related knowledge, another - an ontology of personal relations. We use Twitter data to empirically evaluate our methods. We also apply Machine Learning to demonstrate advantages of our extraction procedure when tweets containing PHI have to be automatically identiﬁed among other tweets. Keywords: Text mining, Twitter, Personal Health Information, Machine Learning 
In this work we present sentiment analysis of messages posted on a medical forum. We categorize posts, written in English, into five categories: encouragement, gratitude, confusion, facts, and facts + sentiments. Our study applies a manual sentiment annotation, affective lexicons in its sentiment analysis and machine learning classification of sentiments in these texts. We report empirical results obtained from analysis of 752 posts dedicated to infertility treatments. Our best results improve multi-class sentiment classification of online messages (F-score = 0.518, AUC= 0.685). 
Analyzing how people discuss about health-related topics on dedicated forums and social networks such as Twitter, can provide valuable insight for syndromic surveillance and to predict disease outbreaks. In this paper we present a minimally trained algorithm to learn associations between technical and everyday language terms, based on pattern generalization and complete linkage clustering, and we then assess its utility on a case study of five common syndromes for surveillance purposes. 1. Introduction Infodemiology is defined as “the science of distribution and determinants of information in an electronic medium, specifically the Internet, with the ultimate aim to inform public health and public policy” (Eysenbach, 2006). A seminal work in this area is (Ginsberg et al., 2009), in which the level of influenza in the U.S. is estimated using the relative frequency of search queries related to influenza-like illness. Similarly, in (Althouse et al., 2011), the authors demonstrate that query search volumes associated to Dengue fever can predict the incidence of Dengue. Another recent study (Xu et al., 2011) analyses the problem of predicting the tendency of hand-foot-and-mouth disease (HFMD), clustering HFMD-related search queries, medical pages and news reports. Query search volumes are estimated using Google Trends (GT) 1 or Google Flu, however, forums and micro-blogs (like Twitter) appear to be a better source of information, since keywords occur in contexts. Contexts make it possible to use text mining techniques for sense disambiguation, topic filtering and mood analysis (Berendt, 2011; Corley, 2009; Von Etter et al., 2010; Cohen and Hersh, 2005; Paul and 
We describe in this paper how different learning strategies can be applied on the same NLP task, namely chunking. The reference corpus is extracted from the French Treebank, the symbolic learning strategy used is grammatical inference and the statistical one is CRFs (Conditional Random Fields). As expected, the symbolic approach allows readability but is less effective than the statistical one. We then propose two distinct ways to combine both approaches and show that in both cases they beneﬁt from one another. 
Patent search is an important information retrieval problem in scientiﬁc and business research. Semantic search would be a large improvement to current technologies, but requires some insight into the language of patents. In this article we test the ﬁt of the language of patents to the sublanguage model, focussing on closure properties. The research presented here is relevant to the topic of sublanguage identiﬁcation for different domains, and to the study of the language of patents. We investigate the hypothesis that ﬁt to the sublanguage model increases as one moves down the International Patent Classiﬁcation hierarchy. The analysis employs a general English corpus and patent documents from the MAREC corpus. It is shown that patents generally ﬁt the sublanguage model, with some variability between categories in the extent of the ﬁt. 
Sublanguages are specialized genres of language associated with speciﬁc domains and document types. When sublanguages can be recognized and adequately characterized, they are useful for a variety of types of natural language processing applications. Although there are sublanguage studies related to languages other than English, all previous work on sublanguage recognition has focused on sublanguages related to general English. This paper tests whether a sublanguage detecting technique developed for English can be applied to another language. Bulgarian clinical documents are an excellent test case, because of a number of unique linguistic properties that affect their lexical and morphological characteristics. Bulgarian clinical documents were studied with respect to their closure properties and were found to ﬁt the sublanguage model and exhibit characteristics like those noted for sublanguages related to English. It was also conﬁrmed that the clinical sublanguage phenomenon is not a coincidental phenomenon of English, but applies to other languages as well. Implications of this fact for natural language processing are proposed. 
This paper provides an analysis of character-level machine translation models used in pivot-based translation when applied to sparse and noisy datasets, such as crowdsourced movie subtitles. In our experiments, we ﬁnd that such characterlevel models cut the number of untranslated words by over 40% and are especially competitive (improvements of 2-3 BLEU points) in the case of limited training data. We explore the impact of character alignment, phrase table ﬁltering, bitext size and the choice of pivot language on translation quality. We further compare cascaded translation models to the use of synthetic training data via multiple pivots, and we ﬁnd that the latter works signiﬁcantly better. Finally, we demonstrate that neither word- nor character-BLEU correlate perfectly with human judgments, due to BLEU’s sensitivity to length. 
The performance of NLP classiﬁers largely depends on the quality of the features considered for prediction (feature engineering). However, as the number of features increases, the more likely overﬁtting becomes and performance decreases. Also, due to the very large number of features, only slimple linear classiﬁers are considered, thus disregarding potentially predictive non-linear combinations of features. Here we propose an automated method for feature induction, which selects and includes in the model features and feature combinations which are likely to be useful for the prediction.The resulting model relies on a smaller feature set, is non-linear and is more accurate than the baseline, which is the model trained on the entire feature set. The method uses a greedy ﬁltering approach based on various univariate measures of feature relevance and it is very fast in practice. Also, our feature induction method is independent of the classiﬁer used: we applied it together with Na¨ıve Bayes and Perceptron models. 
In this paper, we introduce a corpus of human-authored dialogue summaries collected through a web-experiment. The corpus features (i) one of the few existing corpora of written dialogue summaries; (ii) the only corpus available for dialogue summaries in Portuguese; and (iii) the only available corpus of summaries produced for dialogues whose participants’ politeness alignment was systematically varied. Comprising 1,808 human-authored summaries, produced by 452 summarisers, for four different dialogues, this is, to the best of our knowledge, the largest individual corpus available for dialogue summaries, with the highest number of participants involved. 
This article reports on mass experiments supporting the idea that data extracted from strongly comparable corpora may successfully be used to build statistical machine translation systems of reasonable translation quality for in-domain new texts. The experiments were performed for three language pairs: SpanishEnglish, German-English and RomanianEnglish, based on large bilingual corpora of similar sentence pairs extracted from the entire dumps of Wikipedia as of June 2012. Our experiments and comparison with similar work show that adding indiscriminately more data to a training corpus is not necessarily a good thing in SMT. 
The most-frequent-sense and the predominant domain sense play an important role in the debate on word-sensedisambiguation. This discussion is, however, biased by the way sense-tagged corpora are built. In this paper, we argue that current sense-tagged corpora neglect rare senses and contexts and, as a result, do not represent a good corpus for training and testing word-sensedisambiguation. We deﬁned three quality criteria for sense-tagged corpora and a methodology to satisfy these criteria with minimal effort. Following this method, we built a Dutch sense-tagged corpus that tried to meet these criteria. The corpus was evaluated by deriving word-sensedisambiguation systems and testing these on different subsets of the corpus in different ways. The performance of our systems and the quality of the derived data are equal to state-of-the-art English systems and corpora. Finally, we used the systems to create a Dutch corpus of over 47 million sense-tagged tokens spread over a large variety of genres, domains and usages of Dutch. The results of the project can be downloaded freely from the project website. 
Dictionaries are reference resources for learning and diffusing natural languages. Their contents must be enriched carefully due to their importance. However, such contents might contain errors and inconsistencies that are hard to detect manually. Several researches have been made in recent years in order to perform this step automatically. However, they have dealt with the problem in a superficial way. The present paper deals with the detection of anomalies in the content of LMF-standardized dictionaries that covers lexical knowledge at the morphological, syntactic and semantic levels. Thus, we are proposing an approach based on a typological study of the potential anomalies that can occur in editorial dictionaries in general. This approach takes advantage of the LMF fine structure that highlights all kinds of relationships between entries’ knowledge and distinguishes the role of each available text such as giving definitions and examples. An experiment of the proposed approach was carried out on an available LMFstandardized dictionary of the Arabic language. This experiment has been related to the morphological and syntactic levels. 
This paper aims at effective use of training data by extracting sentences from large generaldomain corpora to adapt statistical machine translation systems to domain-specific data. We regard this task as a problem of filtering training sentences with respect to the target domain1 via different similarity metrics. Thus, we give new insights into when data selection model can best benefit the in-domain translation. Based on the investigation of the state-ofthe-art similarity metrics, we propose edit distance as a new data selection criterion for this topic. To evaluate this proposal, we compare it with other methods on a large dataset. Comparative experiments are conducted on Chinese-English travel dialog domain and the results indicate that the proposed approach achieves a significant improvement over the baseline system (+4.36 BLEU) as well as the best rival model (+1.23 BLEU) using a much smaller training subset. This study may have a significant impact on mining very large corpora in a computationally-limited environment. 
The Treebanks as the sets of syntactically annotated sentences, are the most widely used language resource in the application of Natural Language Processing. The occurrence of errors in the automatically created Treebanks is one of the main obstacles limiting the using of these resources in the real world applications. This paper aims to introduce an statistical method for diminishing the amount of errors occurred in a specific English LTAG-Treebank proposed in Basirat and Faili (2013). The problem has been formulated as a classification problem and has been tackled by using several classifiers. The experiments show that by using this approach, about 95% of the errors could be detected and more than 77% of them could successfully be corrected in the case of using Adaboost classifier. In addition, it has been shown that the new treebank could reach a high of 76% F-measure which is 8% higher than the original treebank. 
In Computational Linguistics, building lexical-semantic networks and validating contained relations are paramount issues as well as adding some reasoning skills in order to enrich these knowledge bases. In this paper we devise an inference engine which aims at producing new "potential" relations from already existing ones in the JeuxDeMots network. This network is constructed with the help of a GWAP (game with a purpose) thanks to thousands of players. It handles terms and weighted relations between these terms, and currently contains over 2 million relation occurences. Polysemous terms may be reﬁned in several senses (bank may be a bank>ﬁnancial institution or a bank>river) but as the network is indeﬁnitely under construction (in the context of a Never Ending Learning approach) some senses may be missing at a given time. The approach we proposed here is founded on the triangulation method through two kinds of inference schemes: deduction (topdown from generic to speciﬁc terms) and induction (bottom-up from speciﬁc to generic terms). A blocking mechanism, whose purpose is to avoid proposing highly dubious new relations, is based on logical and statistical constraints. Automatically inferred relations are then proposed to human contributors to be validated. In case of invalidation, a reconciliation dialog is undertaken to identify the cause of the wrong inference: an exception, an error in the premises or a previously undetected confusion due to polysemy on the central term common to both premises.  
This paper proposes a combined model for POS tagging, dependency parsing and co-reference resolution for Bulgarian — a pro-drop Slavic language with rich morphosyntax. We formulate an extension of the MSTParser algorithm that allows the simultaneous handling of the three tasks in a way that makes it possible for each task to beneﬁt from the information available to the others, and conduct a set of experiments against a treebank of the Bulgarian language. The results indicate that the proposed joint model achieves state-of-theart performance for POS tagging task, and outperforms the current pipeline solution. 
Hungarian is the stereotype of morphologically rich and free word order languages. Here, we introduce magyarlanc, a natural language toolkit developed for the linguistic preprocessing – segmentation, morphological analysis, POS-tagging and dependency parsing – of Hungarian texts. We hope that the free availability of the toolkit fosters the research not just on the Hungarian language but on all the morphologically rich languages in general. The main novelties of the tool are the application of a new harmonized morphological coding system of Hungarian, the datadriven approach and the integration of a dependency parser. The system is implemented in JAVA, hence it can be used in a platform-independent way. 
We consider the construction of part-of-speech taggers for resource-poor languages. Recently, manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting. In this paper, we show that additional token constraints can be projected from a resource-rich source language to a resource-poor target language via word-aligned bitext. We present several models to this end; in particular a partially observed conditional random field model, where coupled token and type constraints provide a partial signal for training. Averaged across eight previously studied Indo-European languages, our model achieves a 25{\%} relative error reduction over the prior state of the art. We further present successful results on seven additional languages from different families, empirically demonstrating the applicability of coupled token and type constraints across a diverse set of languages.
Dependency parsing algorithms capable of producing the types of crossing dependencies seen in natural language sentences have traditionally been orders of magnitude slower than algorithms for projective trees. For 95.8{--}99.8{\%} of dependency parses in various natural language treebanks, whenever an edge is crossed, the edges that cross it all have a common vertex. The optimal dependency tree that satisfies this 1-Endpoint-Crossing property can be found with an O(n4) parsing algorithm that recursively combines forests over intervals with one exterior point. 1-Endpoint-Crossing trees also have natural connections to linguistics and another class of graphs that has been studied in NLP.
Recent work has shown that the integration of visual information into text-based models can substantially improve model predictions, but so far only visual information extracted from static images has been used. In this paper, we consider the problem of grounding sentences describing actions in visual information extracted from videos. We present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos, together with an annotation of how similar the action descriptions are to each other. Experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions.
Graph based dependency parsing is inefficient when handling non-local features due to high computational complexity of inference. In this paper, we proposed an exact and efficient decoding algorithm based on the Branch and Bound (B{\&}B) framework where non-local features are bounded by a linear combination of local features. Dynamic programming is used to search the upper bound. Experiments are conducted on English PTB and Chinese CTB datasets. We achieved competitive Unlabeled Attachment Score (UAS) when no additional resources are available: 93.17{\%} for English and 87.25{\%} for Chinese. Parsing speed is 177 words per second for English and 97 words per second for Chinese. Our algorithm is general and can be adapted to non-projective dependency parsing or other graphical models.
The context in which language is used provides a strong signal for learning to recover its meaning. In this paper, we show it can be used within a grounded CCG semantic parsing approach that learns a joint model of meaning and context for interpreting and executing natural language instructions, using various types of weak supervision. The joint nature provides crucial benefits by allowing situated cues, such as the set of visible objects, to directly influence learning. It also enables algorithms that learn while executing instructions, for example by trying to replicate human actions. Experiments on a benchmark navigational dataset demonstrate strong performance under differing forms of supervision, including correctly executing 60{\%} more instruction sets relative to the previous state of the art.
Unsupervised parsing is a difficult task that infants readily perform. Progress has been made on this task using text-based models, but few computational approaches have considered how infants might benefit from acoustic cues. This paper explores the hypothesis that word duration can help with learning syntax. We describe how duration information can be incorporated into an unsupervised Bayesian dependency parser whose only other source of information is the words themselves (without punctuation or parts of speech). Our results, evaluated on both adult-directed and child-directed utterances, show that using word duration can improve parse quality relative to words-only baselines. These results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants, and motivate the use of word duration cues in NLP tasks with speech.
We introduce a novel nonparametric Bayesian model for the induction of Combinatory Categorial Grammars from POS-tagged text. It achieves state of the art performance on a number of languages, and induces linguistically plausible lexicons.
Supervised learning methods and LDA based topic model have been successfully applied in the field of multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experimental results on DUC2007, TAC2008 and TAC2009 demonstrate the effectiveness of our approach.
We demonstrate a method of improving a seed sentiment lexicon developed on essay data by using a pivot-based paraphrasing system for lexical expansion coupled with sentiment profile enrichment using crowdsourcing. Profile enrichment alone yields up to 15{\%} improvement in the accuracy of the seed lexicon on 3-way sentence-level sentiment polarity classification of essay data. Using lexical expansion in addition to sentiment profiles provides a further 7{\%} improvement in performance. Additional experiments show that the proposed method is also effective with other subjectivity lexicons and in a different domain of application (product reviews).
In this paper, we present the first incremental parser for Tree Substitution Grammar (TSG). A TSG allows arbitrarily large syntactic fragments to be combined into complete trees; we show how constraints (including lexicalization) can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers. In addition to whole-sentence F-score, we also evaluate the partial trees that the parser constructs for sentence prefixes; partial trees play an important role in incremental interpretation, language modeling, and psycholinguistics. Unlike existing parsers, our incremental TSG parser can generate partial trees that include predictions about the upcoming words in a sentence. We show that it outperforms an n-gram model in predicting more than one upcoming word.
During the course of first language acquisition, children produce linguistic forms that do not conform to adult grammar. In this paper, we introduce a data set and approach for systematically modeling this child-adult grammar divergence. Our corpus consists of child sentences with corrected adult forms. We bridge the gap between these forms with a discriminatively reranked noisy channel model that translates child sentences into equivalent adult utterances. Our method outperforms MT and ESL baselines, reducing child error by 20{\%}. Our model allows us to chart specific aspects of grammar development in longitudinal studies of children, and investigate the hypothesis that children share a common developmental path in language acquisition.
This paper proposes a discriminative forest reranking algorithm for dependency parsing that can be seen as a form of efficient stacked parsing. A dynamic programming shift-reduce parser produces a packed derivation forest which is then scored by a discriminative reranker, using the 1-best tree output by the shift-reduce parser as guide features in addition to third-order graph-based features. To improve efficiency and accuracy, this paper also proposes a novel shift-reduce parser that eliminates the spurious ambiguity of arc-standard transition systems. Testing on the English Penn Treebank data, forest reranking gave a state-of-the-art unlabeled dependency accuracy of 93.12.
In this paper, we present Dijkstra-WSA, a novel graph-based algorithm for word sense alignment. We evaluate it on four different pairs of lexical-semantic resources with different characteristics (WordNet-OmegaWiki, WordNet-Wiktionary, GermaNet-Wiktionary and WordNet-Wikipedia) and show that it achieves competitive performance on 3 out of 4 datasets. Dijkstra-WSA outperforms the state of the art on every dataset if it is combined with a back-off based on gloss similarity. We also demonstrate that Dijkstra-WSA is not only flexibly applicable to different resources but also highly parameterizable to optimize for precision or recall.
Machine translation (MT) draws from several different disciplines, making it a complex subject to teach. There are excellent pedagogical texts, but problems in MT and current algorithms for solving them are best learned by doing. As a centerpiece of our MT course, we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key MT tasks: alignment, decoding, evaluation, and reranking. Students brought a diverse set of techniques to the problems, including some novel solutions which performed remarkably well. A surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks, demonstrating that even newcomers to the field can help improve the state-of-the-art on hard NLP problems while simultaneously learning a great deal. The problems, baseline code, and results are freely available.
We introduce a new approach to semantics which combines the benefits of distributional and formal logical semantics. Distributional models have been successful in modelling the meanings of content words, but logical semantics is necessary to adequately represent many function words. We follow formal semantics in mapping language to logical representations, but differ in that the relational constants used are induced by offline distributional clustering at the level of predicate-argument structure. Our clustering algorithm is highly scalable, allowing us to run on corpora the size of Gigaword. Different senses of a word are disambiguated based on their induced types. We outperform a variety of existing approaches on a wide-coverage question answering task, and demonstrate the ability to make complex multi-sentence inferences involving quantifiers on the FraCaS suite.
This paper introduces Logical Semantics with Perception (LSP), a model for grounded language acquisition that learns to map natural language statements to their referents in a physical environment. For example, given an image, LSP can map the statement {``}blue mug on the table{''} to the set of image segments showing blue mugs on tables. LSP learns physical representations for both categorical ({``}blue,{''} {``}mug{''}) and relational ({``}on{''}) language, and also learns to compose these representations to produce the referents of entire statements. We further introduce a weakly supervised training procedure that estimates LSP{'}s parameters using annotated referents for entire statements, without annotated referents for individual words or the parse structure of the statement. We perform experiments on two applications: scene understanding and geographical question answering. We find that LSP outperforms existing, less expressive models that cannot represent relational language. We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less annotation effort.
Due to the nature of complex NLP problems, structured prediction algorithms have been important modeling tools for a wide range of tasks. While there exists evidence showing that linear Structural Support Vector Machine (SSVM) algorithm performs better than structured Perceptron, the SSVM algorithm is still less frequently chosen in the NLP community because of its relatively slow training speed. In this paper, we propose a fast and easy-to-implement dual coordinate descent algorithm for SSVMs. Unlike algorithms such as Perceptron and stochastic gradient descent, our method keeps track of dual variables and updates the weight vector more aggressively. As a result, this training process is as efficient as existing online learning methods, and yet derives consistently better models, as evaluated on four benchmark NLP datasets for part-of-speech tagging, named-entity recognition and dependency parsing.
In this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing. The semantic role labeler predicts the full syntactic paths that connect predicates with their arguments. This process is framed as a linear assignment task, which allows to control some well-formedness constraints. For the syntactic part, we define a standard arc-factored dependency model that predicts the full syntactic tree. Finally, we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations. In experiments on the CoNLL-2009 English benchmark we observe very competitive results.
This paper introduces the problem of predicting semantic relations expressed by prepositions and develops statistical learning models for predicting the relations, their arguments and the semantic types of the arguments. We define an inventory of 32 relations, building on the word sense disambiguation task for prepositions and collapsing related senses across prepositions. Given a preposition in a sentence, our computational task to jointly model the preposition relation and its arguments along with their semantic types, as a way to support the relation prediction. The annotated data, however, only provides labels for the relation label, and not the arguments and types. We address this by presenting two models for preposition relation labeling. Our generalization of latent structure SVM gives close to 90{\%} accuracy on relation labeling. Further, by jointly predicting the relation, arguments, and their types along with preposition sense, we show that we can not only improve the relation accuracy, but also significantly improve sense prediction accuracy.
In current research, most tree-based translation models are built directly from parse trees. In this study, we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric Bayesian model. In the model, we utilize synchronous tree substitution grammars (STSG) to capture the bilingual mapping between language pairs. To train the model efficiently, we develop a Gibbs sampler with three novel Gibbs operators. The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes. Experimental results show that the string-to-tree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees.
This paper explores the use of Adaptor Grammars, a nonparametric Bayesian modelling framework, for minimally supervised morphological segmentation. We compare three training methods: unsupervised training, semi-supervised training, and a novel model selection method. In the model selection method, we train unsupervised Adaptor Grammars using an over-articulated metagrammar, then use a small labelled data set to select which potential morph boundaries identified by the metagrammar should be returned in the final output. We evaluate on five languages and show that semi-supervised training provides a boost over unsupervised training, while the model selection method yields the best average results over all languages and is competitive with state-of-the-art semi-supervised systems. Moreover, this method provides the potential to tune performance according to different evaluation metrics or downstream tasks.
Head splitting techniques have been successfully exploited to improve the asymptotic runtime of parsing algorithms for projective dependency trees, under the arc-factored model. In this article we extend these techniques to a class of non-projective dependency trees, called well-nested dependency trees with block-degree at most 2, which has been previously investigated in the literature. We define a structural property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage.
Adjectives like good, great, and excellent are similar in meaning, but differ in intensity. Intensity order information is very useful for language learners as well as in several NLP tasks, but is missing in most lexical resources (dictionaries, WordNet, and thesauri). In this paper, we present a primarily unsupervised approach that uses semantics from Web-scale data (e.g., phrases like good but not excellent) to rank words by assigning them positions on a continuous scale. We rely on Mixed Integer Linear Programming to jointly determine the ranks, such that individual decisions benefit from global information. When ranking English adjectives, our global algorithm achieves substantial improvements over previous work on both pairwise and rank correlation metrics (specifically, 70{\%} pairwise accuracy as compared to only 56{\%} by previous work). Moreover, our approach can incorporate external synonymy information (increasing its pairwise accuracy to 78{\%}) and extends easily to new languages. We also make our code and data freely available.
Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. It has been verified to be a useful constraint for word alignment. However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality.
We present a comparative study of transition-, graph- and PCFG-based models aimed at illuminating more precisely the likely contribution of CFGs in improving Chinese dependency parsing accuracy, especially by combining heterogeneous models. Inspired by the impact of a constituency grammar on dependency parsing, we propose several strategies to acquire pseudo CFGs only from dependency annotations. Compared to linguistic grammars learned from rich phrase-structure treebanks, well designed pseudo grammars achieve similar parsing accuracy and have equivalent contributions to parser ensemble. Moreover, pseudo grammars increase the diversity of base models; therefore, together with all other models, further improve system combination. Based on automatic POS tagging, our final model achieves a UAS of 87.23{\%}, resulting in a significant improvement of the state of the art.
Grounded language learning, the task of mapping from natural language to a representation of meaning, has attracted more and more interest in recent years. In most work on this topic, however, utterances in a conversation are treated independently and discourse structure information is largely ignored. In the context of language acquisition, this independence assumption discards cues that are important to the learner, e.g., the fact that consecutive utterances are likely to share the same referent (Frank et al., 2013). The current paper describes an approach to the problem of simultaneously modeling grounded language at the sentence and discourse levels. We combine ideas from parsing and grammar induction to produce a parser that can handle long input strings with thousands of tokens, creating parse trees that represent full discourses. By casting grounded language learning as a grammatical inference task, we use our parser to extend the work of Johnson et al. (2012), investigating the importance of discourse continuity in children{'}s language acquisition and its interaction with social cues. Our model boosts performance in a language acquisition task and yields good discourse segmentations compared with human annotators.
Defining the reordering search space is a crucial issue in phrase-based SMT between distant languages. In fact, the optimal trade-off between accuracy and complexity of decoding is nowadays reached by harshly limiting the input permutation space. We propose a method to dynamically shape such space and, thus, capture long-range word movements without hurting translation quality nor decoding time. The space defined by loose reordering constraints is dynamically pruned through a binary classifier that predicts whether a given input word should be translated right after another. The integration of this model into a phrase-based decoder improves a strong Arabic-English baseline already including state-of-the-art early distortion cost (Moore and Quirk, 2007) and hierarchical phrase orientation models (Galley and Manning, 2008). Significant improvements in the reordering of verbs are achieved by a system that is notably faster than the baseline, while bleu and meteor remain stable, or even increase, at a very high distortion limit.
Great writing is rare and highly admired. Readers seek out articles that are beautifully written, informative and entertaining. Yet information-access technologies lack capabilities for predicting article quality at this level. In this paper we present first experiments on article quality prediction in the science journalism domain. We introduce a corpus of great pieces of science journalism, along with typical articles from the genre. We implement features to capture aspects of great writing, including surprising, visual and emotional content, as well as general features related to discourse organization and sentence structure. We show that the distinction between great and typical articles can be detected fairly accurately, and that the entire spectrum of our features contribute to the distinction.
Distant supervision algorithms learn information extraction models given only large readily available databases and text collections. Most previous work has used heuristics for generating labeled data, for example assuming that facts not contained in the database are not mentioned in the text, and facts in the database must be mentioned at least once. In this paper, we propose a new latent-variable approach that models missing data. This provides a natural way to incorporate side information, for instance modeling the intuition that text will often mention rare entities which are likely to be missing in the database. Despite the added complexity introduced by reasoning about missing data, we demonstrate that a carefully designed local search approach to inference is very accurate and scales to large datasets. Experiments demonstrate improved performance for binary and unary relation extraction when compared to learning with heuristic labels, including on average a 27{\%} increase in area under the precision recall curve in the binary case.
Recognizing metaphors and identifying the source-target mappings is an important task as metaphorical text poses a big challenge for machine reading. To address this problem, we automatically acquire a metaphor knowledge base and an isA knowledge base from billions of web pages. Using the knowledge bases, we develop an inference mechanism to recognize and explain the metaphors in the text. To our knowledge, this is the first purely data-driven approach of probabilistic metaphor acquisition, recognition, and explanation. Our results shows that it significantly outperforms other state-of-the-art methods in recognizing and explaining metaphors.
We introduce a new approach to the machine-assisted grading of short answer questions. We follow past work in automated grading by first training a similarity metric between student responses, but then go on to use this metric to group responses into clusters and subclusters. The resulting groupings allow teachers to grade multiple responses with a single action, provide rich feedback to groups of similar answers, and discover modalities of misunderstanding among students; we refer to this amplification of grader effort as {``}powergrading.{''} We develop the means to further reduce teacher effort by automatically performing actions when an answer key is available. We show results in terms of grading progress with a small {``}budget{''} of human actions, both from our method and an LDA-based approach, on a test corpus of 10 questions answered by 698 respondents.
Greedy transition-based parsers are very fast but tend to suffer from error propagation. This problem is aggravated by the fact that they are normally trained using oracles that are deterministic and incomplete in the sense that they assume a unique canonical path through the transition system and are only valid as long as the parser does not stray from this path. In this paper, we give a general characterization of oracles that are nondeterministic and complete, present a method for deriving such oracles for transition systems that satisfy a property we call arc decomposition, and instantiate this method for three well-known transition systems from the literature. We say that these oracles are dynamic, because they allow us to dynamically explore alternative and nonoptimal paths during training {---} in contrast to oracles that statically assume a unique optimal path. Experimental evaluation on a wide range of data sets clearly shows that using dynamic oracles to train greedy parsers gives substantial improvements in accuracy. Moreover, this improvement comes at no cost in terms of efficiency, unlike other techniques like beam search.
Joint morphological and syntactic analysis has been proposed as a way of improving parsing accuracy for richly inflected languages. Starting from a transition-based model for joint part-of-speech tagging and dependency parsing, we explore different ways of integrating morphological features into the model. We also investigate the use of rule-based morphological analyzers to provide hard or soft lexical constraints and the use of word clusters to tackle the sparsity of lexical features. Evaluation on five morphologically rich languages (Czech, Finnish, German, Hungarian, and Russian) shows consistent improvements in both morphological and syntactic accuracy for joint prediction over a pipeline model, with further improvements thanks to lexical constraints and word clusters. The final results improve the state of the art in dependency parsing for all languages.
We develop two techniques for analyzing the effect of porting a machine translation system to a new domain. One is a macro-level analysis that measures how domain shift affects corpus-level evaluation; the second is a micro-level analysis for word-level errors. We apply these methods to understand what happens when a Parliament-trained phrase-based machine translation system is applied in four very different domains: news, medical texts, scientific articles and movie subtitles. We present quantitative and qualitative experiments that highlight opportunities for future research in domain adaptation for machine translation.
 Marco Baroni University of Trento marco.baroni@unitn.it  Description Features automatically extracted from images constitute a new and rich source of semantic knowledge that can complement information extracted from text. The convergence between vision- and text-based information can be exploited in scenarios where the two modalities must be combined to solve a target task (e.g., generating verbal descriptions of images, or ﬁnding the right images to illustrate a story). However, the potential applications for integrated visual features go beyond mixed-media scenarios: Because of their complementary nature with respect to language, visual features might provide perceptually grounded semantic information that can be exploited in purely linguistic domains. The tutorial will ﬁrst introduce basic techniques to encode image contents in terms of low-level features, such as the widely adopted SIFT descriptors. We will then show how these low-level descriptors are used to induce more abstract features, focusing on the well-established bags-of-visual-words method to represent images, but also brieﬂy introducing more recent developments, that include capturing spatial information with pyramid representations, soft visual word clustering via Fisher encoding and attribute-based image representation. Next, we will discuss some example applications, and we will conclude with a brief practical illustration of visual feature extraction using a software package we developed. The tutorial is addressed to computational linguists without any background in computer vision. It provides enough background material to understand the vision-and-language literature and the less technical articles on image analysis. After the tutorial, the participants should also be able to autonomously incorporate visual features in their NLP pipelines using off-the-shelf tools.  Outline  1. Why image analysis? • The grounding problem • Multimodal datasets (Pascal, SUN, Im- ageNet and ESP-Game)  2. Extraction of low-level features from images  • Challenges (viewpoint, scale, occlusion, etc.) • Feature detectors • Feature descriptors  illumination,  3. Visual words for higher-level representation of visual information • Constructing a vocabulary of visual words • Classic Bags-of-visual-words representation • Recent advances • Computer vision applications: Object recognition and emotion analysis  4. Going multimodal: Example applications of visual features in NLP • Generating image descriptions • Semantic relatedness • Modeling selectional preference  
The ﬁrst natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. This tutorial explores connections between early decipherment research and today’s NLP work. We cover classic military and diplomatic ciphers, automatic decipherment algorithms, unsolved ciphers, language translation as decipherment, and analyzing ancient writing as decipherment.  oped by Alan Turing and Warren Weaver. We describe recently published work on building automatic translation systems from non-parallel data. We also demonstrate how some of the same algorithmic tools can be applied to natural language tasks like part-of-speech tagging and word alignment. Turning back to historical ciphers, we explore a number of unsolved ciphers, giving results of initial computer experiments on several of them. Finally, we look brieﬂy at writing as a way to encipher phoneme sequences, covering ancient scripts and modern applications.  
Andrea Zielinski Fraunhofer IOSB Fraunhoferstraße 1 Karlsruhe, Germany andrea.zielinski@iosb.fraunhofer.de  Introduction Social media like Twitter and micro-blogs provide a goldmine of text, shallow markup annotations and network structure. These information sources can all be exploited together in order to automatically acquire vast amounts of up-to-date, widecoverage structured knowledge. This knowledge, in turn, can be used to measure the pulse of a variety of social phenomena like political events, activism and stock prices, as well as to detect emerging events such as natural disasters (earthquakes, tsunami, etc.). 
We present WebAnno, a general purpose web-based annotation tool for a wide range of linguistic annotations. WebAnno offers annotation project management, freely conﬁgurable tagsets and the management of users in different roles. WebAnno uses modern web technology for visualizing and editing annotations in a web browser. It supports arbitrarily large documents, pluggable import/export ﬁlters, the curation of annotations across various users, and an interface to farming out annotations to a crowdsourcing platform. Currently WebAnno allows part-ofspeech, named entity, dependency parsing and co-reference chain annotations. The architecture design allows adding additional modes of visualization and editing, when new kinds of annotations are to be supported. 
We implement a city-level geolocation prediction system for Twitter users. The system infers a user’s location based on both tweet text and user-declared metadata using a stacking approach. We demonstrate that the stacking method substantially outperforms benchmark methods, achieving 49% accuracy on a benchmark dataset. We further evaluate our method on a recent crawl of Twitter data to investigate the impact of temporal factors on model generalisation. Our results suggest that user-declared location metadata is more sensitive to temporal change than the text of Twitter messages. We also describe two ways of accessing/demoing our system. 
Given the increasing interest and development of computational and quantitative methods in historical linguistics, it is important that scholars have a basis for documenting, testing, evaluating, and sharing complex workflows. We present a novel open-source toolkit for quantitative tasks in historical linguistics that offers these features. This toolkit also serves as an interface between existing software packages and frequently used data formats, and it provides implementations of new and existing algorithms within a homogeneous framework. We illustrate the toolkit’s functionality with an exemplary workflow that starts with raw language data and ends with automatically calculated phonetic alignments, cognates and borrowings. We then illustrate evaluation metrics on gold standard datasets that are provided with the toolkit. 
This paper presents AnnoMarket, an open cloud-based platform which enables researchers to deploy, share, and use language processing components and resources, following the data-as-a-service and software-as-a-service paradigms. The focus is on multilingual text analysis resources and services, based on an opensource infrastructure and compliant with relevant NLP standards. We demonstrate how the AnnoMarket platform can be used to develop NLP applications with little or no programming, to index the results for enhanced browsing and search, and to evaluate performance. Utilising AnnoMarket is straightforward, since cloud infrastructural issues are dealt with by the platform, completely transparently to the user: load balancing, efﬁcient data upload and storage, deployment on the virtual machines, security, and fault tolerance. 
Nowadays, the importance of Social Media is constantly growing, as people often use such platforms to share mainstream media news and comment on the events that they relate to. As such, people no loger remain mere spectators to the events that happen in the world, but become part of them, commenting on their developments and the entities involved, sharing their opinions and distributing related content. This paper describes a system that links the main events detected from clusters of newspaper articles to tweets related to them, detects complementary information sources from the links they contain and subsequently applies sentiment analysis to classify them into positive, negative and neutral. In this manner, readers can follow the main events happening in the world, both from the perspective of mainstream as well as social media and the public’s perception on them. This system will be part of the EMM media monitoring framework working live and it will be demonstrated using Google Earth. 
We introduce DISSECT, a toolkit to build and explore computational models of word, phrase and sentence meaning based on the principles of distributional semantics. The toolkit focuses in particular on compositional meaning, and implements a number of composition methods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure. 
Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difﬁcult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported. 
 complex NLP applications. Combining together  U-Compare is a UIMA-based workﬂow construction platform for building natural language processing (NLP) applications from heterogeneous language resources (LRs), without the need for programming skills. U-Compare has been adopted within the context of the METANET Network of Excellence, and over 40 LRs that process 15 European languages have been added to the U-Compare component library. In line with METANET’s aims of increasing communication between citizens of different European countries, U-Compare has been extended to facilitate the development of a wider range of applications, including both multilingual and multimodal workﬂows. The enhancements exploit the UIMA Subject of Analysis (Sofa) mechanism, that allows different facets of the input data to be represented. We demonstrate how our customised extensions to U-Compare allow the construction and testing of NLP applications that transform the input data in different ways, e.g., machine translation, automatic summarisation and text-to-speech.  heterogeneous components is not, however, always straightforward. The various components used in a pipeline may be implemented using different programming languages, may have incompatible input/output formats, e.g., stand-off or inline annotations, or may require or produce incompatible data types, e.g., a particular named entity recogniser (NER) may require speciﬁc types of syntactic constituents as input, making it important to choose the right type of syntactic parser to run prior to the NER. Thus, the tools required to build a new application may not be interoperable with each other, and considerable extra work may be required to make the tools talk to each other. The Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004) was created as a means to alleviate such problems. It is a framework that facilitates the straightforward combination of LRs, i.e., tools and corpora, into workﬂow applications. UIMA is an OASIS standard that enables interoperability of LRs by deﬁning a standard workﬂow metadata format and standard input/output representations. U-Compare (Kano et al., 2011) is a graphical NLP workﬂow construction platform built on top of UIMA. It facilitates the rapid construction, testing and evaluation of NLP workﬂows using drag-  
The growing need for Chinese natural language processing (NLP) is largely in a range of research and commercial applications. However, most of the currently Chinese NLP tools or components still have a wide range of issues need to be further improved and developed. FudanNLP is an open source toolkit for Chinese natural language processing (NLP), which uses statistics-based and rule-based methods to deal with Chinese NLP tasks, such as word segmentation, part-ofspeech tagging, named entity recognition, dependency parsing, time phrase recognition, anaphora resolution and so on. 
We present ICARUS, a versatile graphical search tool to query dependency treebanks. Search results can be inspected both quantitatively and qualitatively by means of frequency lists, tables, or dependency graphs. ICARUS also ships with plugins that enable it to interface with tool chains running either locally or remotely. 
In this paper we describe a platform for embodied conversational agents with tutoring goals, which takes as input written and spoken questions and outputs answers in both forms. The platform is developed within a game environment, and currently allows speech recognition and synthesis in Portuguese, English and Spanish. In this paper we focus on its understanding component that supports in-domain interactions, and also small talk. Most indomain interactions are answered using different similarity metrics, which compare the perceived utterances with questions/sentences in the agent’s knowledge base; small-talk capabilities are mainly due to AIML, a language largely used by the chatbots’ community. In this paper we also introduce EDGAR, the butler of MONSERRATE, which was developed in the aforementioned platform, and that answers tourists’ questions about MONSERRATE. 
This paper describes the online tool PhonMatrix, which analyzes a word list with respect to the co-occurrence of sounds in a speciﬁed context within a word. The cooccurrence counts from the user-speciﬁed context are statistically analyzed according to a number of association measures that can be selected by the user. The statistical values then serve as the input for a matrix visualization where rows and columns represent the relevant sounds under investigation and the matrix cells indicate whether the respective ordered pair of sounds occurs more or less frequently than expected. The usefulness of the tool is demonstrated with three case studies that deal with vowel harmony and similar place avoidance patterns. 
We describe QUEST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efﬁcacy of features and algorithms. 
The quality of automatic translation is affected by many factors. One is the divergence between the speciﬁc source and target languages. Another lies in the source text itself, as some texts are more complex than others. One way to handle such texts is to modify them prior to translation. Yet, an important factor that is often overlooked is the source translatability with respect to the speciﬁc translation system and the speciﬁc model that are being used. In this paper we present an interactive system where source modiﬁcations are induced by conﬁdence estimates that are derived from the translation model in use. Modiﬁcations are automatically generated and proposed for the user’s approval. Such a system can reduce postediting effort, replacing it by cost-effective pre-editing that can be done by monolinguals. 
In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on EnglishJapanese machine translation, and ﬁnd that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http://phontron.com/travatar 
This paper presents PLIS, an open source Probabilistic Lexical Inference System which combines two functionalities: (i) a tool for integrating lexical inference knowledge from diverse resources, and (ii) a framework for scoring textual inferences based on the integrated knowledge. We provide PLIS with two probabilistic implementation of this framework. PLIS is available for download and developers of text processing applications can use it as an off-the-shelf component for injecting lexical knowledge into their applications. PLIS is easily conﬁgurable, components can be extended or replaced with user generated ones to enable system customization and further research. PLIS includes an online interactive viewer, which is a powerful tool for investigating lexical inference processes. 
In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual deﬁnitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated deﬁnitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for deﬁnition and hypernym extraction from user-provided sentences. 
This paper offers a new way of representing the results of automatic clustering algorithms by employing a Visual Analytics system which maps members of a cluster and their distance to each other onto a twodimensional space. A case study on Urdu complex predicates shows that the system allows for an appropriate investigation of linguistically motivated data. 
Developing sophisticated NLP pipelines composed of multiple processing tools and components available through different providers may pose a challenge in terms of their interoperability. The Unstructured Information Management Architecture (UIMA) is an industry standard whose aim is to ensure such interoperability by deﬁning common data structures and interfaces. The architecture has been gaining attention from industry and academia alike, resulting in a large volume of UIMA-compliant processing components. In this paper, we demonstrate Argo, a Web-based workbench for the development and processing of NLP pipelines/workﬂows. The workbench is based upon UIMA, and thus has the potential of using many of the existing UIMA resources. We present features, and show examples, of facilitating the distributed development of components and the analysis of processing results. The latter includes annotation visualisers and editors, as well as serialisation to RDF format, which enables ﬂexible querying in addition to data manipulation thanks to the semantic query language SPARQL. The distributed development feature allows users to seamlessly connect their tools to workﬂows running in Argo, and thus take advantage of both the available library of components (without the need of installing them locally) and the analytical tools. 
We present DKPro Similarity, an open source framework for text similarity. Our goal is to provide a comprehensive repository of text similarity measures which are implemented using standardized interfaces. DKPro Similarity comprises a wide variety of measures ranging from ones based on simple n-grams and common subsequences to high-dimensional vector comparisons and structural, stylistic, and phonetic measures. In order to promote the reproducibility of experimental results and to provide reliable, permanent experimental conditions for future studies, DKPro Similarity additionally comes with a set of full-featured experimental setups which can be run out-of-the-box and be used for future systems to built upon. 
Fluid Construction Grammar (FCG) is an open-source computational grammar formalism that is becoming increasingly popular for studying the history and evolution of language. This demonstration shows how FCG can be used to operationalise the cultural processes and cognitive mechanisms that underly language evolution and change. 
Recent research has shown progress in achieving high-quality, very ﬁne-grained type classiﬁcation in hierarchical taxonomies. Within such a multi-level type hierarchy with several hundreds of types at different levels, many entities naturally belong to multiple types. In order to achieve high-precision in type classiﬁcation, current approaches are either limited to certain domains or require time consuming multistage computations. As a consequence, existing systems are incapable of performing ad-hoc type classiﬁcation on arbitrary input texts. In this demo, we present a novel Webbased tool that is able to perform domain independent entity type classiﬁcation under real time conditions. Thanks to its efﬁcient implementation and compacted feature representation, the system is able to process text inputs on-the-ﬂy while still achieving equally high precision as leading state-ofthe-art implementations. Our system offers an online interface where natural-language text can be inserted, which returns semantic type labels for entity mentions. Further more, the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy. 
In this paper, we introduce a Web-scale linguistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and additional regular expression (RE) operators. In our approach, we incorporate inverted file indexing, PoS information from BNC, and semantic indexing based on Latent Dirichlet Allocation with Google Web 1T. The method involves parsing the query to transforming it into several keyword retrieval commands. Word chunks are retrieved with counts, further filtering the chunks with the query as a RE, and finally displaying the results according to the counts, similarities, and topics. Clusters of synonyms or conceptually related words are also provided. In addition, Linggle provides example sentences from The New York Times on demand. The current implementation of Linggle is the most functionally comprehensive, and is in principle language and dataset independent. We plan to extend Linggle to provide fast and convenient access to a wealth of linguistic information embodied in Web scale datasets including Google Web 1T and Google Books Ngram for many major languages in the world. 
Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications, it is still difﬁcult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections. We present ParaQuery, a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection, analyze its utility for a particular domain, and compare it to other popular lexical similarity resources – all within a single interface. 
This paper describes a system for navigating large collections of information about cultural heritage which is applied to Europeana, the European Library. Europeana contains over 20 million artefacts with meta-data in a wide range of European languages. The system currently provides access to Europeana content with meta-data in English and Spanish. The paper describes how Natural Language Processing is used to enrich and organise this meta-data to assist navigation through Europeana and shows how this information is used within the system. 
The use of deep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction. Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. In this system demonstration, we present a tool and a workﬂow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees. We introduce the proposed ﬁve step workﬂow for creating information extractors, the graph query based rule language, as well as the core features of the PROPMINER tool. 
 We present in this paper SEMILAR, the SEMantic simILARity toolkit. SEMILAR implements a number of algorithms for assessing the semantic similarity between two texts. It is available as a Java library and as a Java standalone ap-plication offering GUI-based access to the implemented semantic similarity methods. Furthermore, it offers facilities for manual se-mantic similarity annotation by experts through its component SEMILAT (a SEMantic simILarity Annotation Tool).  
The aim of the Tag2Blog system is to bring satellite tagged wild animals “to life” through narratives that place their movements in an ecological context. Our motivation is to use such automatically generated texts to enhance public engagement with a speciﬁc species reintroduction programme, although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags. We are working with one of the largest nature conservation charities in Europe in this regard, focusing on a single species, the red kite. We describe a system that interprets a sequence of locational ﬁxes obtained from a satellite tagged individual, and constructs a story around its use of the landscape. 
Large amount of parallel corpora is required for building Statistical Machine Translation (SMT) systems. We describe the TransDoop system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. Our system uses a Map-Reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks: (a) translation of constituent phrases of the sentence; (b) validation of quality of the phrase translations; and (c) composition of complete sentence translations from phrase translations. TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd’s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efﬁcacy of our work. 
This work presents tSEARCH, a web-based application that provides mechanisms for doing complex searches over a collection of translation cases evaluated with a large set of diverse measures. tSEARCH uses the evaluation results obtained with the ASIYA toolkit for MT evaluation and it is connected to its on-line GUI, which makes possible a graphical visualization and interactive access to the evaluation results. The search engine offers a ﬂexible query language allowing to ﬁnd translation examples matching a combination of numerical and structural features associated to the calculation of the quality metrics. Its database design permits a fast response time for all queries supported on realistic-size test beds. In summary, tSEARCH, used with ASIYA, offers developers of MT systems and evaluation metrics a powerful tool for helping translation and error analysis. 
VSEM is an open library for visual semantics. Starting from a collection of tagged images, it is possible to automatically construct an image-based representation of concepts by using off-theshelf VSEM functionalities. VSEM is entirely written in MATLAB and its objectoriented design allows a large ﬂexibility and reusability. The software is accompanied by a website with supporting documentation and examples. 
We describe Docent, an open-source decoder for statistical machine translation that breaks with the usual sentence-bysentence paradigm and translates complete documents as units. By taking translation to the document level, our decoder can handle feature models with arbitrary discourse-wide dependencies and constitutes an essential infrastructure component in the quest for discourse-aware SMT models. 
We present an open-source framework for large-scale online structured learning. Developed with the ﬂexibility to handle cost-augmented inference problems such as statistical machine translation (SMT), our large-margin learner can be used with any decoder. Integration with MapReduce using Hadoop streaming allows efﬁcient scaling with increasing size of training data. Although designed with a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing. 
Morphologically rich languages such as Turkish may beneﬁt from morphological analysis in natural language tasks. In this study, we examine the effects of morphological analysis on text categorization task in Turkish. We use stems and word categories that are extracted with morphological analysis as main features and compare them with ﬁxed length stemmers in a bag of words approach with several learning algorithms. We aim to show the effects of using varying degrees of morphological information. 
We present a way to extract links from messages published on microblogging platforms and we classify them according to the language and possible relevance of their target in order to build a text corpus. Three platforms are taken into consideration: FriendFeed, identi.ca and Reddit, as they account for a relative diversity of user proﬁles and more importantly user languages. In order to explore them, we introduce a traversal algorithm based on user pages. As we target lesser-known languages, we try to focus on non-English posts by ﬁltering out English text. Using mature open-source software from the NLP research ﬁeld, a spell checker (aspell) and a language identiﬁcation system (langid.py), our case study and our benchmarks give an insight into the linguistic structure of the considered services. 
Though there has been substantial research concerning the extraction of information from clinical notes, to date there has been less work concerning the extraction of useful information from patient-generated content. Using a dataset comprised of online support group discussion content, this paper investigates two dimensions that may be important in the extraction of patient-generated experiences from text; significant individuals/groups and medication use. With regard to the former, the paper describes an approach involving the pairing of important figures (e.g. family, husbands, doctors, etc.) and affect, and suggests possible applications of such techniques to research concerning online social support, as well as integration into search interfaces for patients. Additionally, the paper demonstrates the extraction of side effects and sentiment at different phases in patient medication use, e.g. adoption, current use, discontinuation and switching, and demonstrates the utility of such an application for drug safety monitoring in online discussion forums. 
As one of the most challenging issues in NLP, metaphor identiﬁcation and its interpretation have seen many models and methods proposed. This paper presents a study on metaphor identiﬁcation based on the semantic similarity between literal and non literal meanings of words that can appear at the same context. 
In this paper we focus on practical issues of data representation for dependency parsing. We carry out an experimental comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the inﬂuence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which conﬁgurations are easier to learn for a dependency parser. 
Current domain-speciﬁc information extraction systems represent an important resource for biomedical researchers, who need to process vaster amounts of knowledge in short times. Automatic discourse causality recognition can further improve their workload by suggesting possible causal connections and aiding in the curation of pathway models. We here describe an approach to the automatic identiﬁcation of discourse causality triggers in the biomedical domain using machine learning. We create several baselines and experiment with various parameter settings for three algorithms, i.e., Conditional Random Fields (CRF), Support Vector Machines (SVM) and Random Forests (RF). Also, we evaluate the impact of lexical, syntactic and semantic features on each of the algorithms and look at errors. The best performance of 79.35% F-score is achieved by CRFs when using all three feature types. 
In this paper, we propose a method to raise the accuracy of text classiﬁcation based on latent topics, reconsidering the techniques necessary for good classiﬁcation – for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we reﬁne the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and investigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have conﬁrmed that our proposed method provides better result among various conditions for clustering. 
This study addresses issues of Japanese language learning concerning word combinations (collocations). Japanese learners may be able to construct grammatically correct sentences, however, these may sound “unnatural”. In this work, we analyze correct word combinations using different collocation measures and word similarity methods. While other methods use well-formed text, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 
Natural language can be easily understood by everyone irrespective of their differences in age or region or qualification. The existence of a conceptual base that underlies all natural languages is an accepted claim as pointed out by Schank in his Conceptual Dependency (CD) theory. Inspired by the CD theory and theories in Indian grammatical tradition, we propose a new set of meaning primitives in this paper. We claim that this new set of primitives captures the meaning inherent in verbs and help in forming an inter-lingual and computable ontological classification of verbs. We have identified seven primitive overlapping verb senses which substantiate our claim. The percentage of coverage of these primitives is 100% for all verbs in Sanskrit and Hindi and 3750 verbs in English. 
Electronic health records (EHRs) contain important clinical information about patients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efﬁcient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classiﬁcation of CT imaging reports into binary categories. In addition to regular text classiﬁcation, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classiﬁcation approach with the assumption that each topic corresponds to a class. And, ﬁnally an aggregate topic classiﬁer was built where reports are classiﬁed based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classiﬁer system is shown to be competitive with existing text classiﬁcation techniques and provides a more efﬁcient and interpretable representation. 
For expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. In order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher conﬁdence by the pre-tagging system. The task of the annotator is to select the correct version among these two alternatives. To minimise the instances in which none of the presented pre-taggings is correct, the texts presented to the annotator are actively selected from a pool of unlabelled text, with the selection criterion that one of the presented pre-taggings should have a high probability of being correct, while still being useful for improving the result of an automatic classiﬁer. 
This paper presents work in progress towards automatic recognition and classiﬁcation of comparisons and similes. Among possible applications, we discuss the place of this task in text simpliﬁcation for readers with Autism Spectrum Disorders (ASD), who are known to have deﬁcits in comprehending ﬁgurative language. We propose an approach to comparison recognition through the use of syntactic patterns. Keeping in mind the requirements of autistic readers, we discuss the properties relevant for distinguishing semantic criteria like ﬁgurativeness and abstractness. 
This study is devoted to the problem of question analysis for a Polish question answering system. The goal of the question analysis is to determine its general structure, type of an expected answer and create a search query for ﬁnding relevant documents in a textual knowledge base. The paper contains an overview of available solutions of these problems, description of their implementation and presents an evaluation based on a set of 1137 questions from a Polish quiz TV show. The results help to understand how an environment of a Slavonic language affects the performance of methods created for English. 
Identifying complex words (CWs) is an important, yet often overlooked, task within lexical simpliﬁcation (The process of automatically replacing CWs with simpler alternatives). If too many words are identiﬁed then substitutions may be made erroneously, leading to a loss of meaning. If too few words are identiﬁed then those which impede a user’s understanding may be missed, resulting in a complex ﬁnal text. This paper addresses the task of evaluating different methods for CW identiﬁcation. A corpus of sentences with annotated CWs is mined from Simple Wikipedia edit histories, which is then used as the basis for several experiments. Firstly, the corpus design is explained and the results of the validation experiments using human judges are reported. Experiments are carried out into the CW identiﬁcation techniques of: simplifying everything, frequency thresholding and training a support vector machine. These are based upon previous approaches to the task and show that thresholding does not perform signiﬁcantly differently to the more na¨ıve technique of simplifying everything. The support vector machine achieves a slight increase in precision over the other two methods, but at the cost of a dramatic trade off in recall. 
There are some chronic critics who always complain about the entity in social media. We are working to automatically detect these chronic critics to prevent the spread of bad rumors about the reputation of the entity. In social media, most comments are informal, and, there are sarcastic and incomplete contexts. This means that it is difﬁcult for current NLP technology such as opinion mining to recognize the complaints. As an alternative approach for social media, we can assume that users who share the same opinions will link to each other. Thus, we propose a method that combines opinion mining with graph analysis for the connections between users to identify the chronic critics. Our experimental results show that the proposed method outperforms analysis based only on opinion mining techniques. 
We study substitute vectors to solve the part-of-speech ambiguity problem in an unsupervised setting. Part-of-speech tagging is a crucial preliminary process in many natural language processing applications. Because many words in natural languages have more than one part-of-speech tag, resolving part-of-speech ambiguity is an important task. We claim that partof-speech ambiguity can be solved using substitute vectors. A substitute vector is constructed with possible substitutes of a target word. This study is built on previous work which has proven that word substitutes are very fruitful for part-ofspeech induction. Experiments show that our methodology works for words with high ambiguity. 
In this work we present psycholinguistically motivated computational models for the organization and processing of Bangla morphologically complex words in the mental lexicon. Our goal is to identify whether morphologically complex words are stored as a whole or are they organized along the morphological line. For this, we have conducted a series of psycholinguistic experiments to build up hypothesis on the possible organizational structure of the mental lexicon. Next, we develop computational models based on the collected dataset. We observed that derivationally suffixed Bangla words are in general decomposed during processing and compositionality between the stem and the suffix plays an important role in the decomposition process. We observed the same phenomena for Bangla verb sequences where experiments showed noncompositional verb sequences are in general stored as a whole in the ML and low traces of compositional verbs are found in the mental lexicon. 
Machine translation (MT) evaluation aims at measuring the quality of a candidate translation by comparing it with a reference translation. This comparison can be performed on multiple levels: lexical, syntactic or semantic. In this paper, we propose a new syntactic metric for MT evaluation based on the comparison of the dependency structures of the reference and the candidate translations. The dependency structures are obtained by means of a Weighted Constraints Dependency Grammar parser. Based on experiments performed on English to German translations, we show that the new metric correlates well with human judgments at the system level. 
In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageRank algorithm. Experimenting on Reuters21578 corpus, we have conﬁrmed that our proposed methods work well for raising the accuracy of a multi-class document categorization. 
We present experiments using a new unsupervised approach to automatic text simpliﬁcation, which builds on sampling and ranking via a loss function informed by readability research. The main idea is that a loss function can distinguish good simpliﬁcation candidates among randomly sampled sub-sentences of the input sentence. Our approach is rated as equally grammatical and beginner reader appropriate as a supervised SMT-based baseline system by native speakers, but our setup performs more radical changes that better resembles the variation observed in human generated simpliﬁcations. 
In this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals. We view language as a system and linguistic features as its components whose relationships are encoded in a Directed Acyclic Graph (DAG). Taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features. Then probabilistic inference enables us to see the strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated. Our model is not restricted to using only two values of a feature. Using imputation technique and EM algorithm it can handle missing values well. Model averaging technique solves the problem of limited data. In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daumé III and Campbell (2007). 
We present a novel method of statistical morphological generation, i.e. the prediction of inﬂected word forms given lemma, part-of-speech and morphological features, aimed at robustness to unseen inputs. Our system uses a trainable classiﬁer to predict “edit scripts” that are then used to transform lemmas into inﬂected word forms. Sufﬁxes of lemmas are included as features to achieve robustness. We evaluate our system on 6 languages with a varying degree of morphological richness. The results show that the system is able to learn most morphological phenomena and generalize to unseen inputs, producing signiﬁcantly better results than a dictionarybased baseline. 
Evaluation methods for Distributional Semantic Models typically rely on behaviorally derived gold standards. These methods are difﬁcult to deploy in languages with scarce linguistic/behavioral resources. We introduce a corpus-based measure that evaluates the stability of the lexical semantic similarity space using a pseudo-synonym same-different detection task and no external resources. We show that it enables to predict two behaviorbased measures across a range of parameters in a Latent Semantic Analysis model. 
Deepﬁx is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge. 
 We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation. In contrast to previous work, we ﬁrst narrow down the gap between Egyptian and MSA by applying an automatic characterlevel transformational model that changes Egyptian to EG , which looks similar to MSA. The transformations include morphological, phonological and spelling changes. The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives a gain of 1.87 BLEU points. Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points.  
The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difﬁcult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difﬁcult to ﬁnd using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for ﬁnding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality. 
While experimenting with tuning on long sentences, we made an unexpected discovery: that PRO falls victim to monsters – overly long negative examples with very low BLEU+1 scores, which are unsuitable for learning and can cause testing BLEU to drop by several points absolute. We propose several effective ways to address the problem, using length- and BLEU+1based cut-offs, outlier ﬁlters, stochastic sampling, and random acceptance. The best of these ﬁxes not only slay and protect against monsters, but also yield higher stability for PRO as well as improved testtime BLEU scores. Thus, we recommend them to anybody using PRO, monsterbeliever or not. 
 This paper proposes a framework of supervised model learning that realizes feature grouping to obtain lower complexity models. The main idea of our method is to integrate a discrete constraint into model learning with the help of the dual decomposition technique. Experiments on two well-studied NLP tasks, dependency parsing and NER, demonstrate that our method can provide state-of-the-art performance even if the degrees of freedom in trained models are surprisingly small, i.e., 8 or even 2. This signiﬁcant beneﬁt enables us to provide compact model representation, which is especially useful in actual use. 
This paper proposes a technique to leverage topic based sentiments from Twitter to help predict the stock market. We first utilize a continuous Dirichlet Process Mixture model to learn the daily topic set. Then, for each topic we derive its sentiment according to its opinion words distribution to build a sentiment time series. We then regress the stock index and the Twitter sentiment time series to predict the market. Experiments on real-life S&P100 Index show that our approach is effective and performs better than existing state-of-the-art non-topic based methods. 
We propose a novel entity disambiguation model, based on Deep Neural Network (DNN). Instead of utilizing simple similarity measures and their disjoint combinations, our method directly optimizes document and entity representations for a given similarity measure. Stacked Denoising Auto-encoders are ﬁrst employed to learn an initial document representation in an unsupervised pre-training stage. A supervised ﬁne-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches. 
Statistical language models have successfully been used to describe and analyze natural language documents. Recent work applying language models to programming languages is focused on the task of predicting code, while mainly ignoring the prediction of programmer comments. In this work, we predict comments from JAVA source ﬁles of open source projects, using topic models and n-grams, and we analyze the performance of the models given varying amounts of background data on the project being predicted. We evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors, and show that using a comment completion tool can save up to 47% of the comment typing. 
Mismatch between queries and documents is a key issue for the web search task. In order to narrow down such mismatch, in this paper, we present an in-depth investigation on adapting a paraphrasing technique to web search from three aspects: a search-oriented paraphrasing model; an NDCG-based parameter optimization algorithm; an enhanced ranking model leveraging augmented features computed on paraphrases of original queries. Experiments performed on the large scale query-document data set show that, the search performance can be signiﬁcantly improved, with +3.28% and +1.14% NDCG gains on dev and test sets respectively. 
Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. In experiments on the multilingual GeoQuery corpus we ﬁnd that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could beneﬁt from advances in machine translation. 
Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set. 
Uncertainty text detection is important to many social-media-based applications since more and more users utilize social media platforms (e.g., Twitter, Facebook, etc.) as information source to produce or derive interpretations based on them. However, existing uncertainty cues are ineffective in social media context because of its speciﬁc characteristics. In this paper, we propose a variant of annotation scheme for uncertainty identiﬁcation and construct the ﬁrst uncertainty corpus based on tweets. We then conduct experiments on the generated tweets corpus to study the effectiveness of different types of features for uncertainty text identiﬁcation. 
We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difﬁcult dataset based on translation data with a low baseline which we beat by 17% F1. 
We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank. Our word pair features achieve signiﬁcantly higher performance than the previous formulation when evaluated without additional features. In addition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation. 
Conversational implicatures involve reasoning about multiply nested belief structures. This complexity poses signiﬁcant challenges for computational models of conversation and cognition. We show that agents in the multi-agent DecentralizedPOMDP reach implicature-rich interpretations simply as a by-product of the way they reason about each other to maximize joint utility. Our simulations involve a reference game of the sort studied in psychology and linguistics as well as a dynamic, interactional scenario involving implemented artiﬁcial agents. 
Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the beneﬁts of lexicalized features in the setting of domain-speciﬁc coreference resolution. We show that adding lexicalized features to off-the-shelf coreference resolvers yields signiﬁcant performance gains on four domain-speciﬁc data sets and with two types of coreference resolution architectures. 
Ordering texts is an important task for many NLP applications. Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method. 
We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English, Swedish, Spanish, French and Korean. To show the usefulness of such a resource, we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before. This ‘universal’ treebank is made freely available in order to facilitate research on multilingual dependency parsing.1 
Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difﬁculty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the ﬁrst empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 
This paper is concerned with the problem of heterogeneous dependency parsing. In this paper, we present a novel joint inference scheme, which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase. Different from stacked learning methods (Nivre and McDonald, 2008; Martins et al., 2008), which process the dependency parsing in a pipelined way (e.g., a second level uses the ﬁrst level outputs), in our method, multiple dependency parsing models are coordinated to exchange consensus information. We conduct experiments on Chinese Dependency Treebank (CDT) and Penn Chinese Treebank (CTB), experimental results show that joint inference can bring signiﬁcant improvements to all state-of-the-art dependency parsers. 
 In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 
We present a model for inducing sentential argument structure, which distinguishes arguments from optional modiﬁers. We use this model to study whether representing an argument/modiﬁer distinction helps in learning argument structure, and whether a linguistically-natural argument/modiﬁer distinction can be induced from distributional data alone. Our results provide evidence for both hypotheses. 
This paper presents an annotation scheme for events that negatively or positively affect entities (benefactive/malefactive events) and for the attitude of the writer toward their agents and objects. Work on opinion and sentiment tends to focus on explicit expressions of opinions. However, many attitudes are conveyed implicitly, and benefactive/malefactive events are important for inferring implicit attitudes. We describe an annotation scheme and give the results of an inter-annotator agreement study. The annotated corpus is available online. 
This paper attempts to use an off-the-shelf anaphora resolution (AR) system for Bengali. The language specific preprocessing modules of GuiTAR (v3.0.3) are identified and suitably designed for Bengali. Anaphora resolution module is also modified or replaced in order to realize different configurations of GuiTAR. Performance of each configuration is evaluated and experiment shows that the off-the-shelf AR system can be effectively used for Indic languages. 
 ent researchers choose to publish different variants  How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically signiﬁcant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in ﬁnding signiﬁcantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores ﬁnd too many signiﬁcant differences between systems which manual evaluation would deem comparable. We also test combina-  of scores. In this paper we reassess the strengths of ROUGE variants using the data from four years of Text Analysis Conference (TAC) evaluations, 2008 to 2011. To assess the performance of the automatic evaluations, we focus on determining statistical signiﬁcance1 between systems, where the gold-standard comes from comparing the systems using manual pyramid and responsiveness evaluations. In this setting, computing correlation coefﬁcients between manual and automatic scores is not applicable as it does not take into account the statistical signiﬁcance of the differences nor does it allow the use of more powerful statistical tests which use pairwise comparisons of performance on individual document sets. Instead, we report on the accuracy of decisions on pairs of systems, as well as the precision and recall of identifying pairs of systems which exhibit statistically signiﬁcant differences in content selection performance.  tions of ROUGE variants and ﬁnd that they  2 Background  considerably improve the accuracy of automatic prediction.  During 2008–2011, automatic summarization systems at TAC were required to create 100-word  
This paper tackles the problem of collecting reliable human assessments. We show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality. To reduce the cost of collecting these multiple ratings, we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings. Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example. 
The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students’ summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics. 
The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this ﬁeld. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a ﬁrst experimental evaluation using two major TREC test collections. Results show that retrieval performances tend to be better when using topics with higher semantic coherence. 
Post-retrieval clustering is the task of clustering Web search results. Within this context, we propose a new methodology that adapts the classical K-means algorithm to a third-order similarity measure initially developed for NLP tasks. Results obtained with the deﬁnition of a new stopping criterion over the ODP-239 and the MORESQUE gold standard datasets evidence that our proposal outperforms all reported text-based approaches. 
Information Retrieval (IR) and Answer Extraction are often designed as isolated or loosely connected components in Question Answering (QA), with repeated overengineering on IR, and not necessarily performance gain for QA. We propose to tightly integrate them by coupling automatically learned features for answer extraction to a shallow-structured IR model. Our method is very quick to implement, and signiﬁcantly improves IR for QA (measured in Mean Average Precision and Mean Reciprocal Rank) by 10%-20% against an uncoupled retrieval baseline in both document and passage retrieval, which further leads to a downstream 20% improvement in QA F1. 
This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model on labeled data, respectively, as the initial models. Then, the two models are constantly updated using unlabeled examples, where the learning objective is maximizing their segmentation agreements. The agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data. The segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature. 
Micro-blog is a new kind of medium which is short and informal. While no segmented corpus of micro-blogs is available to train Chinese word segmentation model, existing Chinese word segmentation tools cannot perform equally well as in ordinary news texts. In this paper we present an effective yet simple approach to Chinese word segmentation of micro-blog. In our approach, we incorporate punctuation information of unlabeled micro-blog data by introducing characters behind or ahead of punctuations, for they indicate the beginning or end of words. Meanwhile a self-training framework to incorporate conﬁdent instances is also used, which prove to be helpful. Experiments on micro-blog data show that our approach improves performance, especially in OOV-recall. 
Transliterated compound nouns not separated by whitespaces pose diﬃculty on word segmentation (WS). Ofﬂine approaches have been proposed to split them using word statistics, but they rely on static lexicon, limiting their use. We propose an online approach, integrating source LM, and/or, back-transliteration and English LM. The experiments on Japanese and Chinese WS have shown that the proposed models achieve signiﬁcant improvement over state-of-the-art, reducing 16% errors in Japanese. 
We present an efﬁcient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental results show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860. 
In this paper, we relook at the problem of pronunciation of English words using native phone set. Speciﬁcally, we investigate methods of pronouncing English words using Telugu phoneset in the context of Telugu Text-to-Speech. We compare phone-phone substitution and wordphone mapping for pronunciation of English words using Telugu phones. We are not considering other than native language phoneset in all our experiments. This differentiates our approach from other works in polyglot speech synthesis. 
This paper studies named entity translation and proposes “selective temporality” as a new feature, as using temporal features may be harmful for translating “atemporal” entities. Our key contribution is building an automatic classiﬁer to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6.1%. 
In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to signiﬁcant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can be combined using linear interpolation to achieve the best performance. The ﬁnal combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models 
Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only ﬁnds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods. 
Product reviews are now widely used by individuals and organizations for decision making (Litvin et al., 2008; Jansen, 2010). And because of the proﬁts at stake, people have been known to try to game the system by writing fake reviews to promote target products. As a result, the task of deceptive review detection has been gaining increasing attention. In this paper, we propose a generative LDA-based topic modeling approach for fake review detection. Our model can aptly detect the subtle differences between deceptive reviews and truthful ones and achieves about 95% accuracy on review spam datasets, outperforming existing baselines by a large margin. 
Ambiguity preserving representations such as lattices are very useful in a number of NLP tasks, including paraphrase generation, paraphrase recognition, and machine translation evaluation. Lattices compactly represent lexical variation, but word order variation leads to a combinatorial explosion of states. We advocate hypergraphs as compact representations for sets of utterances describing the same event or object. We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy. 
Humor generation is a very hard problem. It is difﬁcult to say exactly what makes a joke funny, and solving this problem algorithmically is assumed to require deep semantic understanding, as well as cultural and other contextual cues. We depart from previous work that tries to model this knowledge using ad-hoc manually created databases and labeled training examples. Instead we present a model that uses large amounts of unannotated data to generate I like my X like I like my Y, Z jokes, where X, Y, and Z are variables to be ﬁlled in. This is, to the best of our knowledge, the ﬁrst fully unsupervised humor generation system. Our model signiﬁcantly outperforms a competitive baseline and generates funny jokes 16% of the time, compared to 33% for human-generated jokes. 
In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 
 Each side, government and opposition, is _____  We propose discriminative methods to generate semantic distractors of ﬁll-in-the-  the other for the political crisis, and for the violence.     (a) blaming (b) accusing (c) BOTH   blank quiz for language learners using a  large-scale language learners’ corpus. Un-  Figure 1: Example of a ﬁll-in-the-blank quiz,  like previous studies, the proposed meth-  where (a) blaming is the answer and (b) accusing  ods aim at satisfying both reliability and  is a distractor.  validity of generated distractors; distrac-  tors should be exclusive against answers to avoid multiple answers in one quiz, and distractors should discriminate learners’ proﬁciency. Detailed user evaluation with 3 native and 23 non-native speakers of English shows that our methods achieve better reliability and validity than previous methods.  There are previous studies on distractor generation for automatic ﬁll-in-the-blank quiz generation (Mitkov et al., 2006). Hoshino and Nakagawa (2005) randomly selected distractors from words in the same document. Sumita et al. (2005) used an English thesaurus to generate distractors. Liu et al. (2005) collected distractor candidates that are close to the answer in terms of word-frequency,  
We propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor. We propose three types of lexical constraints as building blocks of humorous word substitution: constraints concerning the similarity of sounds or spellings of the original word and the substitute, a constraint requiring the substitute to be a taboo word, and constraints concerning the position and context of the replacement. Empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation signiﬁcantly. 
 In this paper, we study the problem of automatically annotating the factoids present in collective discourse. Factoids are information units that are shared between instances of collective discourse and may have many different ways of being realized in words. Our approach divides this problem into two steps, using a graph-based approach for each step: (1) factoid discovery, ﬁnding groups of words that correspond to the same factoid, and (2) factoid assignment, using these groups of words to mark collective discourse units that contain the respective factoids. We study this on two novel data sets: the New Yorker caption contest data set, and the crossword clues data set. 
Here, we introduce a machine learningbased approach that allows us to identify light verb constructions (LVCs) in Hungarian and English free texts. We also present the results of our experiments on the SzegedParalellFX English–Hungarian parallel corpus where LVCs were manually annotated in both languages. With our approach, we were able to contrast the performance of our method and deﬁne language-speciﬁc features for these typologically different languages. Our presented method proved to be sufﬁciently robust as it achieved approximately the same scores on the two typologically different languages. 
We present IndoNet, a multilingual lexical knowledge base for Indian languages. It is a linked structure of wordnets of 18 different Indian languages, Universal Word dictionary and the Suggested Upper Merged Ontology (SUMO). We discuss various beneﬁts of the network and challenges involved in the development. The system is encoded in Lexical Markup Framework (LMF) and we propose modiﬁcations in LMF to accommodate Universal Word Dictionary and SUMO. This standardized version of lexical knowledge base of Indian Languages can now easily be linked to similar global resources. 
This paper proposes a methodology for generating specialized Japanese data sets for textual entailment, which consists of pairs decomposed into basic sentence relations. We experimented with our methodology over a number of pairs taken from the RITE-2 data set. We compared our methodology with existing studies in terms of agreement, frequencies and times, and we evaluated its validity by investigating recognition accuracy. 
Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent topics own better adaptability and stability performance. 
Automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words. In this scheme, prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability. To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. Our evaluation shows that lexical expansion signiﬁcantly improves performance in comparison to state-of-the-art baselines. 
Current approaches for word sense disambiguation and translation selection typically require lexical resources or large bilingual corpora with rich information ﬁelds and annotations, which are often infeasible for under-resourced languages. We extract translation context knowledge from a bilingual comparable corpora of a richer-resourced language pair, and inject it into a multilingual lexicon. The multilingual lexicon can then be used to perform context-dependent lexical lookup on texts of any language, including under-resourced ones. Evaluations on a prototype lookup tool, trained on a English–Malay bilingual Wikipedia corpus, show a precision score of 0.65 (baseline 0.55) and mean reciprocal rank score of 0.81 (baseline 0.771). Based on the early encouraging results, the context-dependent lexical lookup tool may be developed further into an intelligent reading aid, to help users grasp the gist of a second or foreign language text. 
Resource scarcity along with diversity– both in dialect and script–are the two primary challenges in Kurdish language processing. In this paper we aim at addressing these two problems by (i) building a text corpus for Sorani and Kurmanji, the two main dialects of Kurdish, and (ii) highlighting some of the orthographic, phonological, and morphological differences between these two dialects from statistical and rule-based perspectives. 
As most of the world’s languages are under-resourced, projection algorithms oﬀer an enticing way to bootstrap the resources available for one resourcepoor language from a resource-rich language by means of parallel text and word alignment. These algorithms, however, make the strong assumption that the language pairs share common structures and that the parse trees will resemble one another. This assumption is useful but often leads to errors in projection. In this paper, we will address this weakness by using trees created from instances of Interlinear Glossed Text (IGT) to discover patterns of divergence between the languages. We will show that this method improves the performance of projection algorithms signiﬁcantly in some languages by accounting for divergence between languages using only the partial supervision of a few corrected trees. 
Cross-lingual projection methods can beneﬁt from resource-rich languages to improve performances of NLP tasks in resources-scarce languages. However, these methods confronted the difﬁculty of syntactic differences between languages especially when the pair of languages varies greatly. To make the projection method well-generalize to diverse languages pairs, we enhance the projection method based on word alignments by introducing target-language word representations as features and proposing a novel noise removing method based on these word representations. Experiments showed that our methods improve the performances greatly on projections between English and Chinese. 
Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 
We present an approach to mine comparable data for parallel sentences using translation-based cross-lingual information retrieval (CLIR). By iteratively alternating between the tasks of retrieval and translation, an initial general-domain model is allowed to adapt to in-domain data. Adaptation is done by training the translation system on a few thousand sentences retrieved in the step before. Our setup is time- and memory-efﬁcient and of similar quality as CLIR-based adaptation on millions of parallel sentences. 
This paper explores the use of Propositional Dynamic Logic (PDL) as a suitable formal framework for describing Sign Language (SL), the language of deaf people, in the context of natural language processing. SLs are visual, complete, standalone languages which are just as expressive as oral languages. Signs in SL usually correspond to sequences of highly speciﬁc body postures interleaved with movements, which make reference to real world objects, characters or situations. Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora. We further show how such a representation could help us with the design of computer aided SL veriﬁcation tools, which in turn would bring us closer to the development of an automatic recognition system for these languages. 
We propose the use of stacking, an ensemble learning technique, to the statistical machine translation (SMT) models. A diverse ensemble of weak learners is created using the same SMT engine (a hierarchical phrase-based system) by manipulating the training data and a strong model is created by combining the weak models on-the-ﬂy. Experimental results on two language pairs and three different sizes of training data show signiﬁcant improvements of up to 4 BLEU points over a conventionally trained SMT model. 
The quality of bilingual data is a key factor in Statistical Machine Translation (SMT). Low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance. Previous work often used supervised learning methods to ﬁlter lowquality data, but a fair amount of human labeled examples are needed which are not easy to obtain. To reduce the reliance on labeled examples, we propose an unsupervised method to clean bilingual data. The method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs, based on the observation that better sentence pairs often lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks. 
In this paper we introduce Translation Difﬁculty Index (TDI), a measure of difﬁculty in text translation. We ﬁrst deﬁne and quantify translation difﬁculty in terms of TDI. We realize that any measure of TDI based on direct input by translators is fraught with subjectivity and adhocism. We, rather, rely on cognitive evidences from eye tracking. TDI is measured as the sum of ﬁxation (gaze) and saccade (rapid eye movement) times of the eye. We then establish that TDI is correlated with three properties of the input sentence, viz. length (L), degree of polysemy (DP) and structural complexity (SC). We train a Support Vector Regression (SVR) system to predict TDIs for new sentences using these features as input. The prediction done by our framework is well correlated with the empirical gold standard data, which is a repository of < L, DP, SC > and T DI pairs for a set of sentences. The primary use of our work is a way of “binning” sentences (to be translated) in “easy”, “medium” and “hard” categories as per their predicted TDI. This can decide pricing of any translation task, especially useful in a scenario where parallel corpora for Machine Translation are built through translation crowdsourcing/outsourcing. This can also provide a way of monitoring progress of second language learners. 
We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identiﬁed using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU. 
 In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments. 
Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and afﬁxes differently. We employ stem as the atomic translation unit to alleviate data spareness. In addition, we associate each stemgranularity translation rule with a distribution of related afﬁxes, and select desirable rules according to the similarity of their afﬁx distributions with given spans to be translated. Experimental results show that our approach signiﬁcantly improves the translation performance on tasks of translating from three Turkic languages to Chinese. 
Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively. 
We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue that by preserving the meaning of the translations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent of the translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach. 
In this paper, we propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level machine translation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets. 
 We present a simple yet effective  approach to syntactic reordering for  Statistical Machine Translation (SMT).  Instead of solely relying on the top-1  best-matching rule for source sentence  preordering, we generalize fully  lexicalized rules into partially lexicalized  and unlexicalized rules to broaden the  rule coverage. Furthermore, , we consider  multiple permutations of all the matching  rules, and select the final reordering path  based on the weighed sum of reordering  probabilities of these rules. Our  experiments in English-Chinese and  English-Japanese  translations  demonstrate the effectiveness of the  proposed approach: we observe  consistent and significant improvement  in translation quality across multiple test  sets in both language pairs judged by  both humans and automatic metric.  
Machine Transliteration is an essential task for many NLP applications. However, names and loan words typically originate from various languages, obey different transliteration rules, and therefore may beneﬁt from being modeled independently. Recently, transliteration models based on Bayesian learning have overcome issues with over-ﬁtting allowing for many-to-many alignment in the training of transliteration models. We propose a novel coupled Dirichlet process mixture model (cDPMM) that simultaneously clusters and bilingually aligns transliteration data within a single uniﬁed model. The uniﬁed model decomposes into two classes of non-parametric Bayesian component models: a Dirichlet process mixture model for clustering, and a set of multinomial Dirichlet process models that perform bilingual alignment independently for each cluster. The experimental results show that our method considerably outperforms conventional alignment models. 
The phrase-based and N-gram-based SMT frameworks complement each other. While the former is better able to memorize, the latter provides a more principled model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does signiﬁcantly improve. 
In this paper we show how to automatically induce non-linear features for machine translation. The new features are selected to approximately maximize a BLEU-related objective and decompose on the level of local phrases, which guarantees that the asymptotic complexity of machine translation decoding does not increase. We achieve this by applying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in performance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks. 
An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations. In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. The features, source connectivity strength and target connectivity strength reﬂect the quality of projected alignments between the source and target phrases in the pivot phrase table. We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study. 
We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al. (2004). We compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure. Our results demonstrate that the second approach is effective in increasing the quality of translations. 
This paper presents two minimum Bayes risk (MBR) based Answer Re-ranking (MBRAR) approaches for the question answering (QA) task. The ﬁrst approach re-ranks single QA system’s outputs by using a traditional MBR model, by measuring correlations between answer candidates; while the second approach reranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model. Evaluations are performed on factoid questions selected from two different domains: Jeopardy! and Web, and signiﬁcant improvements are achieved on all data sets. 
Question answering systems have been developed for many languages, but most resources were created for English, which can be a problem when developing a system in another language such as French. In particular, for question classiﬁcation, no labeled question corpus is available for French, so this paper studies the possibility to use existing English corpora and transfer a classiﬁcation by translating the question and their labels. By translating the training corpus, we obtain results close to a monolingual setting. 
Retrieving similar questions is very important in community-based question answering(CQA). In this paper, we propose a uniﬁed question retrieval model based on latent semantic indexing with tensor analysis, which can capture word associations among diﬀerent parts of CQA triples simultaneously. Thus, our method can reduce lexical chasm of question retrieval with the help of the information of question content and answer parts. The experimental result shows that our method outperforms the traditional methods. 
Some words are more contentful than others: for instance, make is intuitively more general than produce and ﬁfteen is more ‘precise’ than a group. In this paper, we propose to measure the ‘semantic content’ of lexical items, as modelled by distributional representations. We investigate the hypothesis that semantic content can be computed using the KullbackLeibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL divergence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions. 
This paper aims at understanding what human think in textual entailment (TE) recognition process and modeling their thinking process to deal with this problem. We first analyze a labeled RTE-5 test set and find that the negative entailment phenomena are very effective features for TE recognition. Then, a method is proposed to extract this kind of phenomena from text-hypothesis pairs automatically. We evaluate the performance of using the negative entailment phenomena on both the English RTE-5 dataset and Chinese NTCIR-9 RITE dataset, and conclude the same findings. 
Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is “almost entailed” by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting. Indeed, our results show that these methods are useful for recognizing partial entailment. We also provide a preliminary assessment of how partial entailment may be used for recognizing (complete) textual entailment. 
This paper introduces a supervised approach for performing sentence level dialect identiﬁcation between Modern Standard Arabic and Egyptian Dialectal Arabic. We use token level labels to derive sentence-level features. These features are then used with other core and meta features to train a generative classiﬁer that predicts the correct label for each sentence in the given input text. The system achieves an accuracy of 85.5% on an Arabic online-commentary dataset outperforming a previously proposed approach achieving 80.9% and reﬂecting a signiﬁcant gain over a majority baseline of 51.9% and two strong baseline systems of 78.5% and 80.4%, respectively. 
Semantic parsing is a domain-dependent process by nature, as its output is deﬁned over a set of domain symbols. Motivated by the observation that interpretation can be decomposed into domain-dependent and independent components, we suggest a novel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. Our experiments show that this type of information is useful and can reduce the annotation effort signiﬁcantly when moving between domains. 
In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional conﬁgurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature. 
This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classiﬁcation from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classiﬁed by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614. 
We present an automatic method for analyzing sentiment dynamics between characters in plays. This literary format’s structured dialogue allows us to make assumptions about who is participating in a conversation. Once we have an idea of who a character is speaking to, the sentiment in his or her speech can be attributed accordingly, allowing us to generate lists of a character’s enemies and allies as well as pinpoint scenes critical to a character’s emotional development. Results of experiments on Shakespeare’s plays are presented along with discussion of how this work can be extended to unstructured texts (i.e. novels). 
In this article, we propose a novel classifier based on quantum computation theory. Different from existing methods, we consider the classification as an evolutionary process of a physical system and build the classifier by using the basic quantum mechanics equation. The performance of the experiments on two datasets indicates feasibility and potentiality of the quantum classifier. 
We present a fast method for re-purposing existing semantic word vectors to improve performance in a supervised task. Recently, with an increase in computing resources, it became possible to learn rich word embeddings from massive amounts of unlabeled data. However, some methods take days or weeks to learn good embeddings, and some are notoriously difﬁcult to train. We propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classiﬁcation with respect to several baselines, and observe that the approach is most useful when the training set is sufﬁciently small. 
We introduce LABR, the largest sentiment analysis dataset to-date for the Arabic language. It consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars. We investigate the properties of the the dataset, and present its statistics. We explore using the dataset for two tasks: sentiment polarity classiﬁcation and rating classiﬁcation. We provide standard splits of the dataset into training and testing, for both polarity and rating classiﬁcation, in both balanced and unbalanced settings. We run baseline experiments on the dataset to establish a benchmark. 
Recommendation dialog systems help users navigate e-commerce listings by asking questions about users’ preferences toward relevant domain attributes. We present a framework for generating and ranking ﬁne-grained, highly relevant questions from user-generated reviews. We demonstrate our approach on a new dataset just released by Yelp, and release a new sentiment lexicon with 1329 adjectives for the restaurant domain. 
We study subjective language in social media and create Twitter-speciﬁc lexicons via bootstrapping sentiment-bearing terms from multilingual Twitter streams. Starting with a domain-independent, highprecision sentiment lexicon and a large pool of unlabeled data, we bootstrap Twitter-speciﬁc sentiment lexicons, using a small amount of labeled data to guide the process. Our experiments on English, Spanish and Russian show that the resulting lexicons are effective for sentiment classiﬁcation for many underexplored languages in social media. 
Emotion classification can be generally done from both the writer’s and reader’s perspectives. In this study, we find that two foundational tasks in emotion classification, i.e., reader’s emotion classification on the news and writer’s emotion classification on the comments, are strongly related to each other in terms of coarse-grained emotion categories, i.e., negative and positive. On the basis, we propose a respective way to jointly model these two tasks. In particular, a cotraining algorithm is proposed to improve semi-supervised learning of the two tasks. Experimental evaluation shows the effectiveness of our joint modeling approach.* 
Quotes are used in news articles as evidence of a person’s opinion, and thus are a useful target for opinion mining. However, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. We address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic, called a position statement. Using this we construct a corpus covering 7 topics with 2,228 quotes.  Abortion: Women should have the right to choose an abortion. Carbon tax: Australia should introduce a tax on carbon or an emissions trading scheme to combat global warming. Immigration: Immigration into Australia should be maintained or increased because its beneﬁts outweigh any negatives. Reconciliation: The Australian government should formally apologise to the Aboriginal people for past injustices. Republic: Australia should cease to be a monarchy with the Queen as head of state and become a republic with an Australian head of state. Same-sex marriage: Same-sex couples should have the right to attain the legal state of marriage as it is for heterosexual couples. Work choices: Australia should introduce WorkChoices to give employers more control over wages and conditions. Table 1: Topics and their position statements.  
Bag-of-words (BOW) is now the most popular way to model text in machine learning based sentiment classification. However, the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the BOW model. In this paper, we focus on the polarity shift problem, and propose a novel approach, called dual training and dual prediction (DTDP), to address it. The basic idea of DTDP is to first generate artificial samples that are polarity-opposite to the original samples by polarity reversion, and then leverage both the original and opposite samples for (dual) training and (dual) prediction. Experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification. 
The task of review rating prediction can be well addressed by using regression algorithms if there is a reliable training set of reviews with human ratings. In this paper, we aim to investigate a more challenging task of crosslanguage review rating prediction, which makes use of only rated reviews in a source language (e.g. English) to predict the rating scores of unrated reviews in a target language (e.g. German). We propose a new coregression algorithm to address this task by leveraging unlabeled reviews. Evaluation results on several datasets show that our proposed co-regression algorithm can consistently improve the prediction results. 
In this paper we present a technique to reveal deﬁnitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classiﬁer. The results on an annotated dataset of deﬁnitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques. 
Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difﬁcult, because of high degree of polysemy, too ﬁne grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modiﬁcation to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization. 
Quality estimation models provide feedback on the quality of machine translated texts. They are usually trained on humanannotated datasets, which are very costly due to its task-speciﬁc nature. We investigate active learning techniques to reduce the size of these datasets and thus annotation effort. Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. In other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors. 
Optical Character Recognition (OCR) systems for Arabic rely on information contained in the scanned images to recognize sequences of characters and on language models to emphasize ﬂuency. In this paper we incorporate linguistically and semantically motivated features to an existing OCR system. To do so we follow an n-best list reranking approach that exploits recent advances in learning to rank techniques. We achieve 10.1% and 11.4% reduction in recognition word error rate (WER) relative to a standard baseline system on typewritten and handwritten Arabic respectively. 
Timeline summarization aims at generating concise summaries and giving readers a faster and better access to understand the evolution of news. It is a new challenge which combines salience ranking problem with novelty detection. Previous researches in this ﬁeld seldom explore the evolutionary pattern of topics such as birth, splitting, merging, developing and death. In this paper, we develop a novel model called Evolutionary Hierarchical Dirichlet Process(EHDP) to capture the topic evolution pattern in timeline summarization. In EHDP, time varying information is formulated as a series of HDPs by considering time-dependent information. Experiments on 6 different datasets which contain 3156 documents demonstrates the good performance of our system with regard to ROUGE scores. 
We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. 
The growth of the Web 2.0 technologies has led to an explosion of social networking media sites. Among them, Twitter is the most popular service by far due to its ease for realtime sharing of information. It collects millions of tweets per day and monitors what people are talking about in the trending topics updated timely. Then the question is how users can understand a topic in a short time when they are frustrated with the overwhelming and unorganized tweets. In this paper, this problem is approached by sequential summarization which aims to produce a sequential summary, i.e., a series of chronologically ordered short subsummaries that collectively provide a full story about topic development. Both the number and the content of sub-summaries are automatically identified by the proposed stream-based and semantic-based approaches. These approaches are evaluated in terms of sequence coverage, sequence novelty and sequence correlation and the effectiveness of their combination is demonstrated. 
In this paper, we investigate the problem of automatic generation of scientiﬁc surveys starting from keywords provided by a user. We present a system that can take a topic query as input and generate a survey of the topic by ﬁrst selecting a set of relevant documents, and then selecting relevant sentences from those documents. We discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids, or information units, from 47 gold standard documents (surveys and tutorials) on seven topics in Natural Language Processing. We have manually annotated 2,625 sentences with these factoids (around 375 sentences per topic) to build an evaluation corpus for this task. We present evaluation results for the performance of our system using this annotated data. 
Stanford Dependencies (SD) provide a functional characterization of the grammatical relations in syntactic parse-trees. The SD representation is useful for parser evaluation, for downstream applications, and, ultimately, for natural language understanding, however, the design of SD focuses on structurally-marked relations and under-represents morphosyntactic realization patterns observed in Morphologically Rich Languages (MRLs). We present a novel extension of SD, called Uniﬁed-SD (U-SD), which uniﬁes the annotation of structurally- and morphologically-marked relations via an inheritance hierarchy. We create a new resource composed of U-SDannotated constituency and dependency treebanks for the MRL Modern Hebrew, and present two systems that can automatically predict U-SD annotations, for gold segmented input as well as raw texts, with high baseline accuracy. 
In this paper, we propose a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets. 
This paper presents an effective algorithm of annotation adaptation for constituency treebanks, which transforms a treebank from one annotation guideline to another with an iterative optimization procedure, thus to build a much larger treebank to train an enhanced parser without increasing model complexity. Experiments show that the transformed Tsinghua Chinese Treebank as additional training data brings signiﬁcant improvement over the baseline trained on Penn Chinese Treebank only. 
In the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the ﬁrst time the use of tree-adjoining grammars (TAG). We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus, along with novel block sampling methods and approximation transformations for TAG that allow efﬁcient parsing. Our work shows performance improvements on the Penn Treebank and ﬁnds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines. 
We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We ﬁrst describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery. 
Higher-order dependency features are known to improve dependency parser accuracy. We investigate the incorporation of such features into a cube decoding phrase-structure parser. We ﬁnd considerable gains in accuracy on the range of standard metrics. What is especially interesting is that we ﬁnd strong, statistically signiﬁcant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overﬁtting the training material. 
We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 
For the cascaded task of Chinese word segmentation, POS tagging and parsing, the pipeline approach suffers from error propagation while the joint learning approach suffers from inefficient decoding due to the large combined search space. In this paper, we present a novel lattice-based framework in which a Chinese sentence is first segmented into a word lattice, and then a lattice-based POS tagger and a lattice-based parser are used to process the lattice from two different viewpoints: sequential POS tagging and hierarchical tree building. A strategy is designed to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. Experimental results on Chinese Treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks. 
Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is veriﬁed empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 
We present an unsupervised approach to part-of-speech tagging based on projections of tags in a word-aligned bilingual parallel corpus. In contrast to the existing state-of-the-art approach of Das and Petrov, we have developed a substantially simpler method by automatically identifying “good” training sentences from the parallel corpus and applying self-training. In experimental results on eight languages, our method achieves state-of-the-art results. 
Supervised NLP tools and on-line services are often used on data that is very different from the manually annotated data used during development. The performance loss observed in such cross-domain applications is often attributed to covariate shifts, with out-of-vocabulary effects as an important subclass. Many discriminative learning algorithms are sensitive to such shifts because highly indicative features may swamp other indicative features. Regularized and adversarial learning algorithms have been proposed to be more robust against covariate shifts. We present a new perceptron learning algorithm using antagonistic adversaries and compare it to previous proposals on 12 multilingual cross-domain part-of-speech tagging datasets. While previous approaches do not improve on our supervised baseline, our approach is better across the board with an average 4% error reduction. 
Automatically determining the temporal order of events and times in a text is difﬁcult, though humans can readily perform this task. Sometimes events and times are related through use of an explicit co-ordination which gives information about the temporal relation: expressions like “before” and “as soon as”. We investigate the roˆle that these co-ordinating temporal signals have in determining the type of temporal relations in discourse. Using machine learning, we improve upon prior approaches to the problem, achieving over 80% accuracy at labelling the types of temporal relation between events and times that are related by temporal signals. 
A new method for keyword extraction from conversations is introduced, which preserves the diversity of topics that are mentioned. Inspired from summarization, the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments. The method is evaluated on excerpts of the Fisher and AMI corpora, using a crowdsourcing platform to elicit comparative relevance judgments. The results demonstrate that the method outperforms two competitive baselines. 
Tabular information in text documents contains a wealth of information, and so tables are a natural candidate for information extraction. There are many cues buried in both a table and its surrounding text that allow us to understand the meaning of the data in a table. We study how natural-language tools, such as part-of-speech tagging, dependency paths, and named-entity recognition, can be used to improve the quality of relation extraction from tables. In three domains we show that (1) a model that performs joint probabilistic inference across tabular and natural language features achieves an F1 score that is twice as high as either a puretable or pure-text system, and (2) using only shallower features or non-joint inference results in lower quality. 
Distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus. However, previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases. To tackle this problem, we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with ﬁne features. We adapt the information retrieval technique of pseudorelevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique signiﬁcantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments. 
Appositions are adjacent NPs used to add information to a discourse. We propose systems exploiting syntactic and semantic constraints to extract appositions from OntoNotes. Our joint log-linear model outperforms the state-of-the-art Favre and Hakkani-Tu¨r (2009) model by ∼10% on Broadcast News, and achieves 54.3% Fscore on multiple genres. 
Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams. 
 and Denoual (2005) pioneered the application of  Analogical learning over strings is a holistic model that has been investigated by a few authors as a means to map forms of a source language to forms of a target language. In this study, we revisit this learning paradigm and apply it to the transliteration task. We show that alone, it performs worse than a statistical phrase-based machine translation engine, but the combination of both approaches outperforms each one taken separately, demonstrating the usefulness of the information captured by a so-called formal analogy.  analogical learning to Machine Translation. Different variants of the system they proposed have been tested in a number of evaluation campaigns, see for instance (Lepage et al., 2009). Langlais and Patry (2007) investigated the more speciﬁc task of translating unknown words, a problem simultaneously studied in (Denoual, 2007). Analogical learning has been applied to various other purposes, among which query expansion in information retrieval (Moreau et al., 2007), classiﬁcation of nominal and binary data, and handwritten character recognition (Miclet et al., 2008). Formal analogy has also been used for solving Raven IQ tests (Correa et al., 2012).  
 We present an efﬁcient algorithm to estimate large modiﬁed Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a ﬁxed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM.  
We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations. Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context. A signiﬁcant novelty of our adaptation technique is its incremental nature; we continuously update the topic distribution on the evolving test conversation as new utterances become available. Thus, our approach is well-suited to the causal constraint of spoken conversations. On an English-to-Iraqi CSLT task, the proposed approach gives signiﬁcant improvements over a baseline system as measured by BLEU, TER, and NIST. Interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation. 
Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system. 
We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners. The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion. 
Measuring semantic textual similarity (STS) is at the cornerstone of many NLP applications. Different from the majority of approaches, where a large number of pairwise similarity features are used to represent a text pair, our model features the following: (i) it directly encodes input texts into relational syntactic structures; (ii) relies on tree kernels to handle feature engineering automatically; (iii) combines both structural and feature vector representations in a single scoring model, i.e., in Support Vector Regression (SVR); and (iv) delivers signiﬁcant improvement over the best STS systems. 
We present results from our study of which uses syntactically and semantically motivated information to group segments of sentences into unbreakable units for the purpose of typesetting those sentences in a region of a ﬁxed width, using an otherwise standard dynamic programming line breaking algorithm, to minimize raggedness. In addition to a rule-based baseline segmenter, we use a very modest size text, manually annotated with positions of breaks, to train a maximum entropy classiﬁer, relying on an extensive set of lexical and syntactic features, which can then predict whether or not to break after a certain word position in a sentence. We also use a simple genetic algorithm to search for a subset of the features optimizing F1, to arrive at a set of features that delivers 89.2% Precision, 90.2% Recall (89.7% F1) on a test set, improving the rule-based baseline by about 11 points and the classiﬁer trained on all features by about 1 point in F1. 
 2 Regular polysemy  We present the result of an annotation task  Very often a word that belongs to a semantic type,  on regular polysemy for a series of seman-  like Location, can behave as a member of another  tic classes or dot types in English, Dan-  semantic type, like Organization, as shown by the  ish and Spanish. This article describes  following examples from the American National  the annotation process, the results in terms  Corpus (Ide and Macleod, 2001) (ANC):  of inter-encoder agreement, and the sense  a) Manuel died in exile in 1932 in England.  distributions obtained with two methods:  b) England was being kept busy with other con-  majority voting with a theory-compliant  cerns  backoff strategy, and MACE, an unsuper-  c) England was, after all, an important wine  vised system to choose the most likely  market  sense from all the annotations.  In case a), England refers to the English terri-  
Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010). However, they are also sparse, with resulting reliability and coverage problems. We address this problem by derivational smoothing, which uses knowledge about derivationally related words (oldish → old) to improve semantic similarity estimates. We develop a set of derivational smoothing methods and evaluate them on two lexical semantics tasks in German. Even for models built from very large corpora, simple derivational smoothing can improve coverage considerably. 
Although diathesis alternations have been used as features for manual verb classiﬁcation, and there is recent work on incorporating such features in computational models of human language acquisition, work on large scale verb classiﬁcation has yet to examine the potential for using diathesis alternations as input features to the clustering process. This paper proposes a method for approximating diathesis alternation behaviour in corpus data and shows, using a state-of-the-art verb clustering system, that features based on alternation approximation outperform those based on independent subcategorization frames. Our alternation-based approach is particularly adept at leveraging information from less frequent data. 
We present the ﬁrst attempt to perform full FrameNet annotation with crowdsourcing techniques. We compare two approaches: the ﬁrst one is the standard annotation methodology of lexical units and frame elements in two steps, while the second is a novel approach aimed at acquiring frames in a bottom-up fashion, starting from frame element annotation. We show that our methodology, relying on a single annotation step and on simpliﬁed role deﬁnitions, outperforms the standard one both in terms of accuracy and time. 
The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. However, there is no widely-used metric to evaluate wholesentence semantic structures. In this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. We give an efﬁcient algorithm to compute the metric and show the results of an inter-annotator agreement study. 
We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane. Previous approaches assign a constant number of bits per hyperplane. This neglects the fact that a subset of hyperplanes may be more informative than others. Our method, dubbed Variable Bit Quantisation (VBQ), provides a datadriven non-uniform bit allocation across hyperplanes. Despite only using a fraction of the available hyperplanes, VBQ outperforms uniform quantisation by up to 168% for retrieval across standard text and image datasets. 
This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches. 
Lexical resources such as WordNet and VerbNet are widely used in a multitude of NLP tasks, as are annotated corpora such as treebanks. Often, the resources are used as-is, without question or examination. This practice risks missing signiﬁcant performance gains and even entire techniques. This paper addresses the importance of resource quality through the lens of a challenging NLP task: detecting selectional preference violations. We present DAVID, a simple, lexical resource-based preference violation detector. With asis lexical resources, DAVID achieves an F1-measure of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, the F1-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given reﬁned resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements. 
The use of automatic word alignment to capture sentence-level semantic relations is common to a number of cross-lingual NLP applications. Despite its proved usefulness, however, word alignment information is typically considered from a quantitative point of view (e.g. the number of alignments), disregarding qualitative aspects (the importance of aligned terms). In this paper we demonstrate that integrating qualitative information can bring signiﬁcant performance improvements with negligible impact on system complexity. Focusing on the cross-lingual textual entailment task, we contribute with a novel method that: i) signiﬁcantly outperforms the state of the art, and ii) is portable, with limited loss in performance, to language pairs where training data are not available. 
We present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages. The monolingual component of our objective is the average mutual information of clusters of adjacent words in each language, while the bilingual component is the average mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically signiﬁcant improvement in F1 score when using bilingual word clusters instead of monolingual clusters. 
We report on the ﬁrst structured distributional semantic model for Croatian, DM.HR. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available. 
The ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision. However, the potential beneﬁts of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text. We address this challenge with contributions in two folds: ﬁrst, we introduce the new task of image caption generalization, formulated as visually-guided sentence compression, and present an efﬁcient algorithm based on dynamic beam search with dependency-based constraints. Second, we release a new large-scale corpus with 1 million image-caption pairs achieving tighter content alignment between images and text. Evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new imagetext parallel corpus with respect to a concrete application of image caption transfer. 
Identifying news stories that discuss the same real-world events is important for news tracking and retrieval. Most existing approaches rely on the traditional vector space model. We propose an approach for recognizing identical real-world events based on a structured, event-oriented document representation. We structure documents as graphs of event mentions and use graph kernels to measure the similarity between document pairs. Our experiments indicate that the proposed graph-based approach can outperform the traditional vector space model, and is especially suitable for distinguishing between topically similar, yet non-identical events. 
While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. Results over a dataset of entities from four product domains show that the proposed approach achieves signiﬁcantly above baseline F-measure of 0.96. 
Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach. 
Determining the stance expressed by an author from a post written for a twosided debate in an online debate forum is a relatively new problem. We seek to improve Anand et al.’s (2011) approach to debate stance classiﬁcation by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classiﬁcation. 
 School of thought analysis is an important yet not-well-elaborated scientific knowledge discovery task. This paper makes the first attempt at this problem. We focus on one aspect of the problem: do characteristic school-of-thought words exist and whether they are characterizable? To answer these questions, we propose a probabilistic generative School-Of-Thought (SOT) model to simulate the scientific authoring process based on several assumptions. SOT defines a school of thought as a distribution of topics and assumes that authors determine the school of thought for each sentence before choosing words to deliver scientific ideas. SOT distinguishes between two types of school-ofthought words for either the general background of a school of thought or the original ideas each paper contributes to its school of thought. Narrative and quantitative experiments show positive and promising results to the questions raised above. 
In this paper, we use Arabic natural language processing techniques to analyze Arabic debates. The goal is to identify how the participants in a discussion split into subgroups with contrasting opinions. The members of each subgroup share the same opinion with respect to the discussion topic and an opposing opinion to the members of other subgroups. We use opinion mining techniques to identify opinion expressions and determine their polarities and their targets. We opinion predictions to represent the discussion in one of two formal representations: signed attitude network or a space of attitude vectors. We identify opinion subgroups by partitioning the signed network representation or by clustering the vector space representation. We evaluate the system using a data set of labeled discussions and show that it achieves good results. 
We present a system for extracting the dates of illness events (year and month of the event occurrence) from posting histories in the context of an online medical support community. A temporal tagger retrieves and normalizes dates mentioned informally in social media to actual month and year referents. Building on this, an event date extraction system learns to integrate the likelihood of candidate dates extracted from time-rich sentences with temporal constraints extracted from eventrelated sentences. Our integrated model achieves 89.7% of the maximum performance given the performance of the temporal expression retrieval step. 
In this paper, we address the problem for predicting cQA answer quality as a classiﬁcation task. We propose a multimodal deep belief nets based approach that operates in two stages: First, the joint representation is learned by taking both textual and non-textual features into a deep learning network. Then, the joint representation learned by the network is used as input features for a linear classiﬁer. Extensive experimental results conducted on two cQA datasets demonstrate the effectiveness of our proposed approach. 
Opinion mining is often regarded as a classiﬁcation or segmentation task, involving the prediction of i) subjective expressions, ii) their target and iii) their polarity. Intuitively, these three variables are bidirectionally interdependent, but most work has either attempted to predict them in isolation or proposing pipeline-based approaches that cannot model the bidirectional interaction between these variables. Towards better understanding the interaction between these variables, we propose a model that allows for analyzing the relation of target and subjective phrases in both directions, thus providing an upper bound for the impact of a joint model in comparison to a pipeline model. We report results on two public datasets (cameras and cars), showing that our model outperforms state-ofthe-art models, as well as on a new dataset consisting of Twitter posts. 
Sentiment Word Identiﬁcation (SWI) is a basic technique in many sentiment analysis applications. Most existing researches exploit seed words, and lead to low robustness. In this paper, we propose a novel optimization-based model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words. 
Thwarting and sarcasm are two uncharted territories in sentiment analysis, the former because of the lack of training corpora and the latter because of the enormous amount of world knowledge it demands. In this paper, we propose a working definition of thwarting amenable to machine learning and create a system that detects if the document is thwarted or not. We focus on identifying thwarting in product reviews, especially in the camera domain. An ontology of the camera domain is created. Thwarting is looked upon as the phenomenon of polarity reversal at a higher level of ontology compared to the polarity expressed at the lower level. This notion of thwarting defined with respect to an ontology is novel, to the best of our knowledge. A rule based implementation building upon this idea forms our baseline. We show that machine learning with annotated corpora (thwarted/nonthwarted) is more effective than the rule based system. Because of the skewed distribution of thwarting, we adopt the Areaunder-the-Curve measure of performance. To the best of our knowledge, this is the first attempt at the difficult problem of thwarting detection, which we hope will at  least provide a baseline system to compare against. 
Syntactic features are useful for many text classiﬁcation tasks. Among these, tree kernels (Collins and Duﬀy, 2001) have been perhaps the most robust and eﬀective syntactic tool, appealing for their empirical success, but also because they do not require an answer to the diﬃcult question of which tree features to use for a given task. We compare tree kernels to diﬀerent explicit sets of tree features on ﬁve diverse tasks, and ﬁnd that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly available tools), we suggest they should always be included as baseline comparisons in tree kernel method evaluations. 
Computational models of infant word segmentation have not been tested on a wide range of languages. This paper applies a phonotactic segmentation model to Korean. In contrast to the undersegmentation pattern previously found in English and Russian, the model exhibited more oversegmentation errors and more errors overall. Despite the high error rate, analysis suggested that lexical acquisition might not be problematic, provided that infants attend only to frequently segmented items. 
We investigated the effect of word surprisal on the EEG signal during sentence reading. On each word of 205 experimental sentences, surprisal was estimated by three types of language model: Markov models, probabilistic phrasestructure grammars, and recurrent neural networks. Four event-related potential components were extracted from the EEG of 24 readers of the same sentences. Surprisal estimates under each model type formed a signiﬁcant predictor of the amplitude of the N400 component only, with more surprising words resulting in more negative N400s. This effect was mostly due to content words. These ﬁndings provide support for surprisal as a generally applicable measure of processing difﬁculty during language comprehension. 
We have elicited human quantitative judgments of semantic relatedness for 122 pairs of nouns and compiled them into a new set of relatedness norms that we call Rel-122. Judgments from individual subjects in our study exhibit high average correlation to the resulting relatedness means (r = 0.77, σ = 0.09, N = 73), although not as high as Resnik’s (1995) upper bound for expected average human correlation to similarity means (r = 0.90). This suggests that human perceptions of relatedness are less strictly constrained than perceptions of similarity and establishes a clearer expectation for what constitutes human-like performance by a computational measure of semantic relatedness. We compare the results of several WordNet-based similarity and relatedness measures to our Rel-122 norms and demonstrate the limitations of WordNet for discovering general indications of semantic relatedness. We also offer a critique of the ﬁeld’s reliance upon similarity norms to evaluate relatedness measures. 
We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efﬁcient integration of both n-gram and dependency language models. To resolve conﬂicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the merits of phrase-based and string-todependency models, it achieves signiﬁcant improvements over the two baselines on the NIST Chinese-English datasets. 
Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associated with each phrase at SMT decoding. On a Chinese–English TM database, our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Besides, the proposed models also outperform previous approaches significantly. 
We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and ﬁrst order parameters, are nondeﬁcient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted Fmeasure) for the IBM-3. For the IBM4 there are no consistent improvements. Training the nondeﬁcient IBM-5 in the regular way gives surprisingly good results. Using the resulting alignments for phrasebased translation systems offers no clear insights w.r.t. BLEU scores. 
Annotating linguistic data is often a complex, time consuming and expensive endeavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and levels of consistency. We present novel techniques for learning from the outputs of multiple annotators while accounting for annotator speciﬁc behaviour. These techniques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata speciﬁc models, while explicitly representing correlations between models which can be learned directly from data. Our experiments on two machine translation quality estimation datasets show uniform signiﬁcant accuracy gains from multi-task learning, and consistently outperform strong baselines. 
We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the beneﬁts of such marginal distribution constraints. We present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library.1 
We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video. 
Recent work on statistical quantiﬁer scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary number and type of noun phrases. No corpusbased method, however, has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation. In this paper we report early, though promising, results for automatic QSD when handling both phenomena. We also present a general model for learning to build partial orders from a set of pairwise preferences. We give an n log n algorithm for ﬁnding a guaranteed approximation of the optimal solution, which works very well in practice. Finally, we signiﬁcantly improve the performance of the previous model using a rich set of automatically generated features. 
Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classiﬁers. By contrast, we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved. In addition, we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance signiﬁcantly. Our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents. 
Temporal resolution systems are traditionally tuned to a particular language, requiring signiﬁcant human effort to translate them to new languages. We present a language independent semantic parser for learning the interpretation of temporal phrases given only a corpus of utterances and the times they reference. We make use of a latent parse that encodes a language-ﬂexible representation of time, and extract rich features over both the parse and associated temporal semantics. The parameters of the model are learned using a weakly supervised bootstrapping approach, without the need for manually tuned parameters or any other language expertise. We achieve state-of-the-art accuracy on all languages in the TempEval2 temporal normalization task, reporting a 4% improvement in both English and Spanish accuracy, and to our knowledge the ﬁrst results for four other languages. 
We propose a computationally efﬁcient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems. 
Automated annotation of social behavior in conversation is necessary for large-scale analysis of real-world conversational data. Important behavioral categories, though, are often sparse and often appear only in speciﬁc subsections of a conversation. This makes supervised machine learning difﬁcult, through a combination of noisy features and unbalanced class distributions. We propose within-instance content selection, using cue features to selectively suppress sections of text and biasing the remaining representation towards minority classes. We show the effectiveness of this technique in automated annotation of empowerment language in online support group chatrooms. Our technique is signiﬁcantly more accurate than multiple baselines, especially when prioritizing high precision. 
Efﬁciently incorporating entity-level information is a challenge for coreference resolution systems due to the difﬁculty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between speciﬁed properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efﬁcient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system. 
Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation, part-of-speech (POS) tagging and phrase-structure parsing system that integrates character-structure features. Our joint system signiﬁcantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing. 
We present a novel transition-based, greedy dependency parser which implements a ﬂexible mix of bottom-up and top-down strategies. The new strategy allows the parser to postpone difﬁcult decisions until the relevant information becomes available. The novel parser has a ∼12% error reduction in unlabeled attachment score over an arc-eager parser, with a slow-down factor of 2.8. 
Binarization of grammars is crucial for improving the complexity and performance of parsing and translation. We present a versatile binarization algorithm that can be tailored to a number of grammar formalisms by simply varying a formal parameter. We apply our algorithm to binarizing tree-to-string transducers used in syntax-based machine translation. 
This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 
In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score. 
In the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring parallel text: some users create post multilingual messages targeting international audiences while others “retweet” translations. We present an efﬁcient method for detecting these messages and extracting parallel segments from them. We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo (the Chinese counterpart of Twitter) using only their public APIs. As a supplement to existing parallel training data, our automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news commentary. The resources in described in this paper are available at http://www.cs.cmu.edu/∼lingwang/utopia. 
Supervised topic models with a logistic likelihood have two issues that potentially limit their practical use: 1) response variables are usually over-weighted by document word counts; and 2) existing variational inference methods make strict mean-ﬁeld assumptions. We address these issues by: 1) introducing a regularization constant to better balance the two parts based on an optimization formulation of Bayesian inference; and 2) developing a simple Gibbs sampling algorithm by introducing auxiliary Polya-Gamma variables and collapsing out Dirichlet variables. Our augment-and-collapse sampling algorithm has analytical forms of each conditional distribution without making any restricting assumptions and can be easily parallelized. Empirical results demonstrate signiﬁcant improvements on prediction performance and time efﬁciency. 
We present a dual decomposition framework for multi-document summarization, using a model that jointly extracts and compresses sentences. Compared with previous work based on integer linear programming, our approach does not require external solvers, is signiﬁcantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers. 
We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Google’s open source OCR system. 
We adapt discriminative reranking to improve the performance of grounded language acquisition, speciﬁcally the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. Instead, we show how the weak supervision of response feedback (e.g. successful task completion) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees. 
Syntactic structures, by their nature, reﬂect ﬁrst and foremost the formal constructions used for expressing meanings. This renders them sensitive to formal variation both within and across languages, and limits their value to semantic applications. We present UCCA, a novel multi-layered framework for semantic representation that aims to accommodate the semantic distinctions expressed through linguistic utterances. We demonstrate UCCA’s portability across domains and languages, and its relative insensitivity to meaning-preserving syntactic variation. We also show that UCCA can be effectively and quickly learned by annotators with no linguistic background, and describe the compilation of a UCCAannotated corpus. 
Many current Natural Language Processing [NLP] techniques work well assuming a large context of text as input data. However they become ineffective when applied to short texts such as Twitter feeds. To overcome the issue, we want to ﬁnd a related newswire document to a given tweet to provide contextual support for NLP tasks. This requires robust modeling and understanding of the semantics of short texts. The contribution of the paper is two-fold: 1. we introduce the Linking-Tweets-toNews task as well as a dataset of linked tweet-news pairs, which can beneﬁt many NLP applications; 2. in contrast to previous research which focuses on lexical features within the short texts (text-to-word information), we propose a graph based latent variable model that models the inter short text correlations (text-to-text information). This is motivated by the observation that a tweet usually only covers one aspect of an event. We show that using tweet speciﬁc feature (hashtag) and news speciﬁc feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show signiﬁcant improvement of our new model over baselines with three evaluation metrics in the new task. 
We propose a computational framework for identifying linguistic aspects of politeness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These ﬁndings guide our construction of a classiﬁer with domain-independent lexical and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonalization and modality. Our classiﬁer achieves close to human performance and is effective across domains. We use our framework to study the relationship between politeness and social power, showing that polite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bottom. Finally, we apply our classiﬁer to a preliminary analysis of politeness variation by gender and community. 
Recently, researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence, technical errors, and relevance to prompt, but there is relatively little work on modeling thesis clarity. We present a new annotated corpus and propose a learning-based approach to scoring essays along the thesis clarity dimension. Additionally, in order to provide more valuable feedback on why an essay is scored as it is, we propose a second learning-based approach to identifying what kinds of errors an essay has that may lower its thesis clarity score. 
We present a corpus analysis of how Italian connectives are translated into LIS, the Italian Sign Language. Since corpus resources are scarce, we propose an alignment method between the syntactic trees of the Italian sentence and of its LIS translation. This method, and clustering applied to its outputs, highlight the different ways a connective can be rendered in LIS: with a corresponding sign, by affecting the location or shape of other signs, or being omitted altogether. We translate these ﬁndings into a computational model that will be integrated into the pipeline of an existing Italian-LIS rendering system. Initial experiments to learn the four possible translations with Decision Trees give promising results. 
In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data speciﬁc for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features. To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses. The model includes features of both parses, allowing transfer between the formalisms, while preserving parsing efﬁciency. We evaluate our approach on three constituency-based grammars — CCG, HPSG, and LFG, augmented with the Penn Treebank-1. Our experiments show that across all three formalisms, the target parsers signiﬁcantly beneﬁt from the coarse annotations.1 
We propose a new variant of TreeAdjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efﬁcient grammar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to TreeSubstitution Grammars and between different variations in probabilistic model design. Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible. 
We present a fast and scalable online method for tuning statistical machine translation models with large feature sets. The standard tuning algorithm—MERT—only scales to tens of features. Recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces signiﬁcant translation quality gains by exploiting sparse features. Equally important is our analysis, which suggests techniques for mitigating overﬁtting and domain mismatch, and applies to other recent discriminative methods for machine translation. 
In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. Results on ﬁve Chinese-English NIST tasks show that our model improves the baseline system by 1.32 BLEU and 1.53 TER on average. Results of comparative study with other seven widely used reordering models will also be reported. 
Most modern machine translation systems use phrase pairs as translation units, allowing for accurate modelling of phraseinternal translation and reordering. However phrase-based approaches are much less able to model sentence level effects between different phrase-pairs. We propose a new model to address this imbalance, based on a word-based Markov model of translation which generates target translations left-to-right. Our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 
 (e.g., (Nigam et al., 2000; Mann and McCallum,  Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classiﬁcation, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today.  2010)). However, current SSL techniques have scalability limitations. Typically, for each target concept to be learned, a semi-supervised classiﬁer is trained using iterative techniques that execute multiple passes over the unlabeled data (e.g., Expectation-Maximization (Nigam et al., 2000) or Label Propagation (Zhu and Ghahramani, 2002)). This is problematic for text classiﬁcation over large unlabeled corpora like the Web: new target concepts (new tasks and new topics of interest) arise frequently, and performing even a single pass  In this paper, we show that improving  over a large corpus for each new target concept is  marginal word frequency estimates using  intractable.  unlabeled data can enable semi-supervised  In this paper, we present a new SSL text classi-  text classiﬁcation that scales to massive  ﬁcation approach that scales to large corpora. In-  unlabeled data sets. We present a novel  stead of utilizing unlabeled examples directly for  learning algorithm, which optimizes a  each given target concept, our approach is to pre-  Naive Bayes model to accord with statis-  compute a small set of statistics over the unlabeled  tics calculated from the unlabeled corpus.  data in advance. Then, for a given target class and  In experiments with text topic classiﬁca-  labeled data set, we utilize the statistics to improve  tion and sentiment analysis, we show that  a classiﬁer.  our method is both more scalable and more accurate than SSL techniques from previous work.  Speciﬁcally, we introduce a method that extends Multinomial Naive Bayes (MNB) to leverage marginal probability statistics P (w) of each  
We present two latent variable models for learning character types, or personas, in ﬁlm, in which a persona is deﬁned as a set of mixtures over latent lexical classes. These lexical classes capture the stereotypical actions of which a character is the agent and patient, as well as attributes by which they are described. As the ﬁrst attempt to solve this problem explicitly, we also present a new dataset for the text-driven analysis of ﬁlm, along with a benchmark testbed to help drive future work in this area. 
In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we ﬁrst introduce a new framework for decipherment training that is ﬂexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efﬁcient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the ﬁrst time) and large vocabulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving signiﬁcant computational speedups (several orders faster). We also report for the ﬁrst time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus). 
The English ’s possessive construction occurs frequently in text and can encode several different semantic relations; however, it has received limited attention from the computational linguistics community. This paper describes the creation of a semantic relation inventory covering the use of ’s, an inter-annotator agreement study to calculate how well humans can agree on the relations, a large collection of possessives annotated according to the relations, and an accurate automatic annotation system for labeling new examples. Our 21,938 example dataset is by far the largest annotated possessives dataset we are aware of, and both our automatic classiﬁcation system, which achieves 87.4% accuracy in our classiﬁcation experiment, and our annotation data are publicly available. 
This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or normal for a given context (e.g., number of people facing a water shortage). We ﬁrst discuss the necessity of numerical common sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numerical expressions and their context from the Web. One approach estimates the distribution of numbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distribution. Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. Experimental results demonstrate the effectiveness of both approaches. 
Generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. In contrast, vector space models of distributional semantics are trained on large corpora, but are typically applied to domaingeneral lexical disambiguation tasks. We introduce Distributional Semantic Hidden Markov Models, a novel variant of a hidden Markov model that integrates these two approaches by incorporating contextualized distributional semantic vectors into a generative model as observed emissions. Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a domain. In a subsequent extrinsic evaluation, we show that these improvements are also reﬂected in multi-document summarization. 
In this paper we present a method for extracting bilingual terminologies from comparable corpora. In our approach we treat bilingual term extraction as a classiﬁcation problem. For classiﬁcation we use an SVM binary classiﬁer and training data taken from the EUROVOC thesaurus. We test our approach on a held-out test set from EUROVOC and perform precision, recall and f-measure evaluations for 20 European language pairs. The performance of our classiﬁer reaches the 100% precision level for many language pairs. We also perform manual evaluation on bilingual terms extracted from English-German term-tagged comparable corpora. The results of this manual evaluation showed 60-83% of the term pairs generated are exact translations and over 90% exact or partial translations. 
Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classiﬁcation. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classiﬁcation for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA. 
Supervised training procedures for semantic parsers produce high-quality semantic parsers, but they have difﬁculty scaling to large databases because of the sheer number of logical constants for which they must see labeled training data. We present a technique for developing semantic parsers for large databases based on a reduction to standard supervised training algorithms, schema matching, and pattern learning. Leveraging techniques from each of these areas, we develop a semantic parser for Freebase that is capable of parsing questions with an F1 that improves by 0.42 over a purely-supervised learning algorithm. 
Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. 
Many models in NLP involve latent variables, such as unknown parses, tags, or alignments. Finding the optimal model parameters is then usually a difﬁcult nonconvex optimization problem. The usual practice is to settle for local optimization methods such as EM or gradient ascent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually ﬁnd the global maximum (up to a user-speciﬁed ) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Linearization Technique to produce convex relaxations during branch-and-bound. Although these techniques do not yet provide a practical solution to our instance of this NP-hard problem, they sometimes ﬁnd better solutions than Viterbi EM with random restarts, in the same time. 
Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efﬁcient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments. 
In spoken dialog systems, statistical state tracking aims to improve robustness to speech recognition errors by tracking a posterior distribution over hidden dialog states. Current approaches based on generative or discriminative models have different but important shortcomings that limit their accuracy. In this paper we discuss these limitations and introduce a new approach for discriminative state tracking that overcomes them by leveraging the problem structure. An ofﬂine evaluation with dialog data collected from real users shows improvements in both state tracking accuracy and the quality of the posterior probabilities. Features that encode speech recognition error patterns are particularly helpful, and training requires relatively few dialogs. 
To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models. 
We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser signiﬁcantly outperforms the stateof-the-art, often by a wide margin. 
Techniques that compare short text segments using dependency paths (or simply, paths) appear in a wide range of automated language processing applications including question answering (QA). However, few models in ad hoc information retrieval (IR) use paths for document ranking due to the prohibitive cost of parsing a retrieval collection. In this paper, we introduce a ﬂexible notion of paths that describe chains of words on a dependency path. These chains, or catenae, are readily applied in standard IR models. Informative catenae are selected using supervised machine learning with linguistically informed features and compared to both non-linguistic terms and catenae selected heuristically with ﬁlters derived from work on paths. Automatically selected catenae of 1-2 words deliver signiﬁcant performance gains on three TREC collections. 
Paratactic syntactic structures are notoriously difﬁcult to represent in dependency formalisms. This has painful consequences such as high frequency of parsing errors related to coordination. In other words, coordination is a pending problem in dependency analysis of natural languages. This paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures. We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. In addition, empirical observations on convertibility between selected styles of representations are shown too. 
We present GlossBoot, an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages. For each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns. Our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages. 
Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the ﬁeld of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation. 
This paper discusses the construction of a parallel treebank currently involving ten languages from six language families. The treebank is based on deep LFG (LexicalFunctional Grammar) grammars that were developed within the framework of the ParGram (Parallel Grammar) effort. The grammars produce output that is maximally parallelized across languages and language families. This output forms the basis of a parallel treebank covering a diverse set of phenomena. The treebank is publicly available via the INESS treebanking environment, which also allows for the alignment of language pairs. We thus present a unique, multilayered parallel treebank that represents more and different types of languages than are available in other treebanks, that represents  deep linguistic knowledge and that allows for the alignment of sentences at several levels: dependency structures, constituency structures and POS information. 
Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identiﬁcation of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identiﬁcation relies on a discriminative classiﬁer trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classiﬁer to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies. 
We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we represent by visual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classiﬁers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better ﬁt to human word association data compared to amodal models and word representations based on handcrafted norming data. 
Developing natural language processing tools for low-resource languages often requires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and evaluate how the amounts of various kinds of data affect performance of a trained POS-tagger. Our results show that annotation of word types is the most important, provided a sufﬁciently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that ﬁnitestate morphological analyzers are effective sources of type information when few labeled examples are available. 
This paper demonstrates the need and impact of subcategorization information for SMT. We combine (i) features on sourceside syntactic subcategorization and (ii) an external knowledge base with quantitative, dependency-based information about target-side subcategorization frames. A manual evaluation of an English-toGerman translation task shows that the subcategorization information has a positive impact on translation quality through better prediction of case. 
We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel corpora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decoding. Additionally, we also propose a new MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to different words according to their importance values in a document. Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline1. 
In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchanging them with a unique substitute—ﬁnding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before. 
This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high afﬁnity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while existing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data. 
This paper studies the problem of mining named entity translations from comparable corpora with some “asymmetry”. Unlike the previous approaches relying on the “symmetry” found in parallel corpora, the proposed method is tolerant to asymmetry often found in comparable corpora, by distinguishing different semantics of relations of entity pairs to selectively propagate seed entity translations on weakly comparable corpora. Our experimental results on English-Chinese corpora show that our selective propagation approach outperforms the previous approaches in named entity translation in terms of the mean reciprocal rank by up to 0.16 for organization names, and 0.14 in a low comparability case. 
Wikipedia infoboxes are a valuable source of structured knowledge for global knowledge sharing. However, infobox information is very incomplete and imbalanced among the Wikipedias in different languages. It is a promising but challenging problem to utilize the rich structured knowledge from a source language Wikipedia to help complete the missing infoboxes for a target language. In this paper, we formulate the problem of cross-lingual knowledge extraction from multilingual Wikipedia sources, and present a novel framework, called WikiCiKE, to solve this problem. An instancebased transfer learning method is utilized to overcome the problems of topic drift and translation errors. Our experimental results demonstrate that WikiCiKE outperforms the monolingual knowledge extraction method and the translation-based method. 
We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages. We support this hypothesis with experiments in crosslanguage (English-Italian) document categorization. In a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents, and investigate the performance of a model built on English data, on Italian test data (and viceversa). The results show not only statistically signiﬁcant, but a large improvement – a jump of almost 40 points in F1-score – over the raw (vanilla bag-ofwords) representation. 
Just as observing is more than just seeing, comparing is far more than mere matching. It takes understanding, and even inventiveness, to discern a useful basis for judging two ideas as similar in a particular context, especially when our perspective is shaped by an act of linguistic creativity such as metaphor, simile or analogy. Structured resources such as WordNet offer a convenient hierarchical means for converging on a common ground for comparison, but offer little support for the divergent thinking that is needed to creatively view one concept as another. We describe such a means here, by showing how the web can be used to harvest many divergent views for many familiar ideas. These lateral views complement the vertical views of WordNet, and support a system for idea exploration called Thesaurus Rex. We show also how Thesaurus Rex supports a novel, generative similarity measure for WordNet. 
Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multiword phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques. 
Metaphor is an important way of conveying the affect of people, hence understanding how people use metaphors to convey affect is important for the communication between individuals and increases cohesion if the perceived affect of the concrete example is the same for the two individuals. Therefore, building computational models that can automatically identify the affect in metaphor-rich texts like “The team captain is a rock.”, “Time is money.”, “My lawyer is a shark.” is an important challenging problem, which has been of great interest to the research community. To solve this task, we have collected and manually annotated the affect of metaphor-rich texts for four languages. We present novel algorithms that integrate triggers for cognitive, affective, perceptual and social processes with stylistic and lexical information. By running evaluations on datasets in English, Spanish, Russian and Farsi, we show that the developed affect polarity and valence prediction technology of metaphor-rich texts is portable and works equally well for different languages. 
Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, FDOOHG7LHUHG7DJJLQJ7XIL, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. 
We present a novel approach to noun phrase lemmatisation where the main phase is cast as a tagging problem. The idea draws on the observation that the lemmatisation of almost all Polish noun phrases may be decomposed into transformation of singular words (tokens) that make up each phrase. We perform evaluation, which shows results similar to those obtained earlier by a rule-based system, while our approach allows to separate chunking from lemmatisation. 
We describe a novel approach for automatically predicting the hidden demographic properties of social media users. Building on prior work in common-sense knowledge acquisition from third-person text, we ﬁrst learn the distinguishing attributes of certain classes of people. For example, we learn that people in the Female class tend to have maiden names and engagement rings. We then show that this knowledge can be used in the analysis of ﬁrst-person communication; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples. Our novel approach enables substantial improvements on the widelystudied task of user gender prediction, obtaining a 20% relative error reduction over the current state-of-the-art. 
With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality ﬂaws. We show that the distribution of these labels is topically biased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classiﬁer and overly optimistic evaluation results. We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. This approach better reﬂects the situation a classiﬁer would face in a real-life application. 
We introduce the novel task of automatically generating questions that are relevant to a text but do not appear in it. One motivating example of its application is for increasing user engagement around news articles by suggesting relevant comparable questions, such as “is Beyonce a better singer than Madonna?”, for the user to answer. We present the ﬁrst algorithm for the task, which consists of: (a) ofﬂine construction of a comparable question template database; (b) ranking of relevant templates to a given article; and (c) instantiation of templates only with entities in the article whose comparison under the template’s relation makes sense. We tested the suggestions generated by our algorithm via a Mechanical Turk experiment, which showed a signiﬁcant improvement over the strongest baseline of more than 45% in all metrics. 
Punctuations are not available in automatic speech recognition outputs, which could create barriers to many subsequent text processing tasks. This paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts. Our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right. It can exploit a global view to capture long-range dependencies for punctuation prediction with linear complexity. The experimental results on the test data sets of IWSLT and TDT4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in transcribed speech text. 
Structural information in web text provides natural annotations for NLP problems such as word segmentation and parsing. In this paper we propose a discriminative learning algorithm to take advantage of the linguistic knowledge in large amounts of natural annotations on the Internet. It utilizes the Internet as an external corpus with massive (although slight and sparse) natural annotations, and enables a classiﬁer to evolve on the large-scaled and real-time updated web text. With Chinese word segmentation as a case study, experiments show that the segmenter enhanced with the Chinese wikipedia achieves signiﬁcant improvement on a series of testing sets from different domains, even with a single classiﬁer and local features. 
This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random ﬁelds (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task. 
Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems. 
Most statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efﬁciency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks. 
Typical statistical machine translation systems are batch trained with a given training data and their performances are largely inﬂuenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efﬁcient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is signiﬁcantly reduced by training them in parallel. 
We present a new translation model integrating the shallow local multi bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we signiﬁcantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German translation task. As an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation. 
Empty categories (EC) are artiﬁcial elements in Penn Treebanks motivated by the government-binding (GB) theory to explain certain language phenomena such as pro-drop. ECs are ubiquitous in languages like Chinese, but they are tacitly ignored in most machine translation (MT) work because of their elusive nature. In this paper we present a comprehensive treatment of ECs by ﬁrst recovering them with a structured MaxEnt model with a rich set of syntactic and lexical features, and then incorporating the predicted ECs into a Chinese-to-English machine translation task through multiple approaches, including the extraction of EC-speciﬁc sparse features. We show that the recovered empty categories not only improve the word alignment quality, but also lead to signiﬁcant improvements in a large-scale state-of-the-art syntactic MT system. 
While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also describe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 BLEU over unadapted systems and single-domain adaptation. 
This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual inﬁnite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. 
Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically ﬁnd the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issue. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization. Experiments conducted on a real CQA data show that our proposed approach is promising. 
Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the ﬁrst uniﬁed framework for unsupervised learning of these three types of information. We show how to utilize Determinantal Point Processes (DPPs), elegant probabilistic models that are deﬁned over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets, for clustering. Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and utilizes the efﬁcient sampling algorithms of DPPs to cluster together verbs with similar SCFs and SPs. We evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1. 
Semantic frames are a rich linguistic resource. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from ﬁnancial news. Semantic frames help to generalize from speciﬁc sentences to scenarios, and to detect the (positive or negative) roles of speciﬁc companies. We introduce a novel tree representation, and use it to train predictive models with tree kernels using support vector machines. Our experiments test multiple text representations on two binary classiﬁcation tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have signiﬁcantly better performance across years on the polarity task. 
Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general. 
Given that structured output prediction is typically performed over entire datasets, one natural question is whether it is possible to re-use computation from earlier inference instances to speed up inference for future instances. Amortized inference has been proposed as a way to accomplish this. In this paper, ﬁrst, we introduce a new amortized inference algorithm called the Margin-based Amortized Inference, which uses the notion of structured margin to identify inference problems for which previous solutions are provably optimal. Second, we introduce decomposed amortized inference, which is designed to address very large inference problems, where earlier amortization methods become less effective. This approach works by decomposing the output structure and applying amortization piece-wise, thus increasing the chance that we can re-use previous solutions for parts of the output structure. These parts are then combined to a global coherent solution using Lagrangian relaxation. In our experiments, using the NLP tasks of semantic role labeling and entityrelation extraction, we demonstrate that with the margin-based algorithm, we need to call the inference engine only for a third of the test examples. Further, we show that the decomposed variant of margin-based amortized inference achieves a greater reduction in the number of inference calls. 
Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggravate this scarcity problem. To deal with these issues, we describe an efﬁcient semisupervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (concepts). It can efﬁciently handle semantic ambiguity by extending standard topic models with two new features. First, it encodes word n-gram features from labeled source and unlabeled target data. Second, by going beyond a bag-of-words approach, it takes into account the inherent sequential nature of utterances to learn semantic classes based on context. (ii) Retrospective Learner is a new learning technique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging. 
Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing. 
We present the ﬁrst unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches. 
It is important that the testimony of children be admissible in court, especially given allegations of abuse. Unfortunately, children can be misled by interrogators or might offer false information, with dire consequences. In this work, we evaluate various parameterizations of ﬁve classiﬁers (including support vector machines, neural networks, and random forests) in deciphering truth from lies given transcripts of interviews with 198 victims of abuse between the ages of 4 and 7. These evaluations are performed using a novel set of syntactic features, including measures of complexity. Our results show that sentence length, the mean number of clauses per utterance, and the StajnerMitkov measure of complexity are highly informative syntactic features, that classiﬁcation accuracy varies greatly by the age of the speaker, and that accuracy up to 91.7% can be achieved by support vector machines given a sufﬁcient amount of data. 
A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reﬂects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classiﬁers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classiﬁers that perform well. 
While there have been many attempts to estimate the emotion of an addresser from her/his utterance, few studies have explored how her/his utterance affects the emotion of the addressee. This has motivated us to investigate two novel tasks: predicting the emotion of the addressee and generating a response that elicits a speciﬁc emotion in the addressee’s mind. We target Japanese Twitter posts as a source of dialogue data and automatically build training data for learning the predictors and generators. The feasibility of our approaches is assessed by using 1099 utterance-response pairs that are built by ﬁve human workers. 
During real-life interactions, people are naturally gesturing and modulating their voice to emphasize speciﬁc points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques. This paper presents a method for multimodal sentiment classiﬁcation, which can identify the sentiment expressed in utterance-level visual datastreams. Using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality. 
Sentiment Similarity of word pairs reflects the distance between the words regarding their underlying sentiments. This paper aims to infer the sentiment similarity between word pairs with respect to their senses. To achieve this aim, we propose a probabilistic emotionbased approach that is built on a hidden emotional model. The model aims to predict a vector of basic human emotions for each sense of the words. The resultant emotional vectors are then employed to infer the sentiment similarity of word pairs. We apply the proposed approach to address two main NLP tasks, namely, Indirect yes/no Question Answer Pairs inference and Sentiment Orientation prediction. Extensive experiments demonstrate the effectiveness of the proposed approach. 
Social Media contain a multitude of user opinions which can be used to predict realworld phenomena in many domains including politics, ﬁnance and health. Most existing methods treat these problems as linear regression, learning to relate word frequencies and other simple features to a known response variable (e.g., voting intention polls or ﬁnancial indicators). These techniques require very careful ﬁltering of the input texts, as most Social Media posts are irrelevant to the task. In this paper, we present a novel approach which performs high quality ﬁltering automatically, through modelling not just words but also users, framed as a bilinear model with a sparse regulariser. We also consider the problem of modelling groups of related output variables, using a structured multi-task regularisation method. Our experiments on voting intention prediction demonstrate strong performance over large-scale input from Twitter on two distinct case studies, outperforming competitive baselines. 
In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety of indicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup. 
We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity. 
 This study proposes a text summarization  model that simultaneously performs sen-  tence extraction and compression. We  translate the text summarization task into  a problem of extracting a set of depen-  dency subtrees in the document cluster.  We also encode obligatory case constraints  as must-link dependency constraints in or-  der to guarantee the readability of the gen-  erated summary. In order to handle the  subtree extraction problem, we investigate  a new class of submodular maximization  problem, and a new algorithm that has  ethxepearpimpreonxtismwatiitohnthraetiNoT12C(I1R−AeC−L1I)A.  Our test  collections show that our approach outper-  forms a state-of-the-art algorithm.  
Probabilistic context-free grammars have the unusual property of not always deﬁning tight distributions (i.e., the sum of the “probabilities” of the trees the grammar generates can be less than one). This paper reviews how this non-tightness can arise and discusses its impact on Bayesian estimation of PCFGs. We begin by presenting the notion of “almost everywhere tight grammars” and show that linear CFGs follow it. We then propose three different ways of reinterpreting non-tight PCFGs to make them tight, show that the Bayesian estimators in Johnson et al. (2007) are correct under one of them, and provide MCMC samplers for the other two. We conclude with a discussion of the impact of tightness empirically. 
This paper describes a method of inducing wide-coverage CCG resources for Japanese. While deep parsers with corpusinduced grammars have been emerging for some languages, those for Japanese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method ﬁrst integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing. 
We present a novel approach, called selectional branching, which uses conﬁdence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search. 
This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency grammar for ﬁve different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a signiﬁcant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average. 
Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as ﬁxed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We introduce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines. 
In some societies, internet users have to create information morphs (e.g. “Peace West King” to refer to “Bo Xilai”) to avoid active censorship or achieve other communication goals. In this paper we aim to solve a new problem of resolving entity morphs to their real targets. We exploit temporal constraints to collect crosssource comparable corpora relevant to any given morph query and identify target candidates. Then we propose various novel similarity measurements including surface features, meta-path based semantic features and social correlation features and combine them in a learning-to-rank framework. Experimental results on Chinese Sina Weibo data demonstrate that our approach is promising and signiﬁcantly outperforms baseline methods1. 
We describe a new probabilistic model for extracting events between major political actors from news corpora. Our unsupervised model brings together familiar components in natural language processing (like parsers and topic models) with contextual political information— temporal and dyad dependence—to infer latent event classes. We quantitatively evaluate the model’s performance on political science benchmarks: recovering expert-assigned event class valences, and detecting real-world conﬂict. We also conduct a small case study based on our model’s inferences. A supplementary appendix, and replication software/data are available online, at: http://brenocon.com/irevents 
Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data. However, these solutions are impractical in complex structured prediction problems such as statistical machine translation. We present an online gradient-based algorithm for relative margin maximization, which bounds the spread of the projected data while maximizing the margin. We evaluate our optimizer on Chinese-English and ArabicEnglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve signiﬁcant improvements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 
Predicate-argument structure (PAS) has been demonstrated to be very effective in improving SMT performance. However, since a sourceside PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. In this paper, we group PAS ambiguities into two types: role ambiguity and gap ambiguity. Then we propose two novel methods to handle the two PAS ambiguities for SMT accordingly: 1) inside context integration; 2) a novel maximum entropy PAS disambiguation (MEPD) model. In this way, we incorporate rich context information of PAS for disambiguation. Then we integrate the two methods into a PASbased translation framework. Experiments show that our approach helps to achieve significant improvements on translation quality. 
Mother tongue interference is the phenomenon where linguistic systems of a mother tongue are transferred to another language. Although there has been plenty of work on mother tongue interference, very little is known about how strongly it is transferred to another language and about what relation there is across mother tongues. To address these questions, this paper explores and visualizes mother tongue interference preserved in English texts written by Indo-European language speakers. This paper further explores linguistic features that explain why certain relations are preserved in English writing, and which contribute to related tasks such as native language identiﬁcation. 
We describe a new representation of the content vocabulary of a text we call word association proﬁle that captures the proportions of highly associated, mildly associated, unassociated, and dis-associated pairs of words that co-exist in the given text. We illustrate the shape of the distirbution and observe variation with genre and target audience. We present a study of the relationship between quality of writing and word association proﬁles. For a set of essays written by college graduates on a number of general topics, we show that the higher scoring essays tend to have higher percentages of both highly associated and dis-associated pairs, and lower percentages of mildly associated pairs of words. Finally, we use word association proﬁles to improve a system for automated scoring of essays. 
Text normalization is an important ﬁrst step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly deﬁne the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normalization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations. 
This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate’s smoothed preferences. Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements. 
This paper presents a novel deterministic algorithm for implicit Semantic Role Labeling. The system exploits a very simple but relevant discursive property, the argument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data. 
Semantic Role Labeling (SRL) has become one of the standard tasks of natural language processing and proven useful as a source of information for a number of other applications. We address the problem of transferring an SRL model from one language to another using a shared feature representation. This approach is then evaluated on three language pairs, demonstrating competitive performance as compared to a state-of-the-art unsupervised SRL system and a cross-lingual annotation projection baseline. We also consider the contribution of different aspects of the feature representation to the performance of the model and discuss practical applicability of this method. 
Derivational models are still an underresearched area in computational morphology. Even for German, a rather resourcerich language, there is a lack of largecoverage derivational knowledge. This paper describes a rule-based framework for inducing derivational families (i.e., clusters of lemmas in derivational relationships) and its application to create a highcoverage German resource, DERIVBASE, mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quantitative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books. 
We report on the construction of the Webis text reuse corpus 2012 for advanced research on text reuse. The corpus compiles manually written documents obtained from a completely controlled, yet representative environment that emulates the web. Each of the 297 documents in the corpus is about one of the 150 topics used at the TREC Web Tracks 2009–2011, thus forming a strong connection with existing evaluation efforts. Writers, hired at the crowdsourcing platform oDesk, had to retrieve sources for a given topic and to reuse text from what they found. Part of the corpus are detailed interaction logs that consistently cover the search for sources as well as the creation of documents. This will allow for in-depth analyses of how text is composed if a writer is at liberty to reuse texts from a third party—a setting which has not been studied so far. In addition, the corpus provides an original resource for the evaluation of text reuse and plagiarism detectors, where currently only less realistic resources are employed. 
We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ∗) and learn the semantic classes that best ﬁt the ∗ argument. To do this, we extract all the occurrences in Wikipedia which match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach. 
In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed. 
This paper presents HEADY: a novel, abstractive approach for headline generation from news collections. From a web-scale corpus of English news, we mine syntactic patterns that a Noisy-OR model generalizes into event descriptions. At inference time, we query the model with the patterns observed in an unseen news collection, identify the event that better captures the gist of the collection and retrieve the most appropriate pattern to generate a headline. HEADY improves over a state-of-theart open-domain title abstraction method, bridging half of the gap that separates it from extractive methods using humangenerated titles in manual evaluations, and performs comparably to human-generated headlines as evaluated with ROUGE.  • Carmelo and La La Party It Up with Kim and Ciara • La La Vazquez and Carmelo Anthony: Wedding Day Bliss • Carmelo Anthony, actress LaLa Vazquez wed in NYC • Stylist to the Stars • LaLa, Carmelo Set Off Celebrity Wedding Weekend • Ciara rocks a sexy Versace Spring 2010 mini to LaLa Vasquez and Carmelo Anthony’s wedding (photos) • Lala Vasquez on her wedding dress, cake, reality tv show and ﬁance´, Carmelo Anthony (video) • VAZQUEZ MARRIES SPORTS STAR ANTHONY • Lebron Returns To NYC For Carmelo’s Wedding • Carmelo Anthony’s stylist dishes on the wedding • Paul pitching another Big Three with “Melo in NYC” • Carmelo Anthony and La La Vazquez Get Married at Star-Studded Wedding Ceremony  
Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random ﬁelds (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modiﬁed inputs and is therefore applicable to incremental surface realisation. Results from a human rating study conﬁrm that users are sensitive to this extended notion of context and assign ratings that are signiﬁcantly higher (up to 14%) than those for taking only local context into account. 
Long distance reordering remains one of the greatest challenges in statistical machine translation research as the key contextual information may well be beyond the conﬁne of translation units. In this paper, we propose Two-Neighbor Orientation (TNO) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule boundaries. We explicitly model the longest span of such chunks, referred to as Maximal Orientation Span, to serve as a global parameter that constrains underlying local decisions. We integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efﬁcacy of our proposal in a large-scale Chinese-to-English translation task. On NIST MT08 set, our most advanced model brings around +2.0 BLEU and -1.0 TER improvement. 
Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine alignments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline. 
 ular author’s or publication’s style; the word “do-  This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is ﬁrst to create a vector proﬁle for the in-domain development (“dev”) set. This proﬁle might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reﬂects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with features deﬁned in the same way, and calculate its similarity score with the vector representing the dev set. Thus, we obtain a decoding feature whose value represents the phrase pair’s closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Experiments on large scale NIST evaluation data show improvements over strong baselines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and signiﬁcant improvements in most circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.  main” is often used to indicate a particular combination of all these factors. Unless there is a perfect match between the training data domain and the (test) domain in which the SMT system will be used, one can often get better performance by adapting the system to the test domain. Domain adaptation is an active topic in the natural language processing (NLP) research community. Its application to SMT systems has recently received considerable attention. Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, instance weighting, and phrase sense disambiguation, etc. Research on mixture models has considered both linear and log-linear mixtures. Both were studied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) loglinearly. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. In transductive learning, an MT system trained on general domain data is used to translate indomain monolingual data. The resulting bilingual sentence pairs are then used as additional training data (Uefﬁng et al., 2007; Chen et al., 2008; Schwenk, 2008; Bertoldi and Federico, 2009). Data selection approaches (Zhao et al., 2004; Hildebrand et al., 2005; Lu¨ et al., 2007; Moore  
We present a method for automatically generating input parsers from English speciﬁcations of input ﬁle formats. We use a Bayesian generative model to capture relevant natural language phenomena and translate the English speciﬁcation into a speciﬁcation tree, which is then translated into a C++ input parser. We model the problem as a joint dependency parsing and semantic role labeling task. Our method is based on two sources of information: (1) the correlation between the text and the speciﬁcation tree and (2) noisy supervision as determined by the success of the generated C++ parser in reading input examples. Our results show that our approach achieves 80.0% F-Score accuracy compared to an F-Score of 66.7% produced by a state-of-the-art semantic parser on a dataset of input format speciﬁcations from the ACM International Collegiate Programming Contest (which were written in English for humans with no intention of providing support for automated processing).1 
We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method. 
Speaker identiﬁcation is the task of attributing utterances to characters in a literary narrative. It is challenging to automate because the speakers of the majority of utterances are not explicitly identiﬁed in novels. In this paper, we present a supervised machine learning approach for the task that incorporates several novel features. The experimental results show that our method is more accurate and general than previous approaches to the problem. 
Hierarchical Bayesian Models (HBMs) have been used with some success to capture empirically observed patterns of under- and overgeneralization in child language acquisition. However, as is well known, HBMs are “ideal” learning systems, assuming access to unlimited computational resources that may not be available to child language learners. Consequently, it remains crucial to carefully assess the use of HBMs along with alternative, possibly simpler, candidate models. This paper presents such an evaluation for a language acquisition domain where explicit HBMs have been proposed: the acquisition of English dative constructions. In particular, we present a detailed, empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs. 
Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level. A recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model signiﬁcantly outperforms prior word-level and topic-level models. We also release a ﬁrst context-sensitive inference rule set. 
Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a uniﬁed approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This uniﬁed representation shows state-ofthe-art performance on three tasks: semantic textual similarity, word similarity, and word sense coarsening. 
We create an open multilingual wordnet with large wordnets for over 26 languages and smaller ones for 57 languages. It is made by combining wordnets with open licences, data from Wiktionary and the Unicode Common Locale Data Repository. Overall there are over 2 million senses for over 100 thousand concepts, linking over 1.4 million words in hundreds of languages. 
We present a new bilingual FrameNet lexicon for English and German. It is created through a simple, but powerful approach to construct a FrameNet in any language using Wiktionary as an interlingual representation. Our approach is based on a sense alignment of FrameNet and Wiktionary, and subsequent translation disambiguation into the target language. We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexicalsemantic resources. The created resource is publicly available at http://www. ukp.tu-darmstadt.de/fnwkde/. 
Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and ﬁltering, the resulting data boosts translation performance across the board for ﬁve different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1 
We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efﬁciently ﬁnd highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically signiﬁcant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. 
We address the challenge of generating natural language abstractive summaries for spoken meetings in a domain-independent fashion. We apply Multiple-Sequence Alignment to induce abstract generation templates that can be used for different domains. An Overgenerateand-Rank strategy is utilized to produce and rank candidate abstracts. Experiments using in-domain and out-of-domain training on disparate corpora show that our system uniformly outperforms state-of-the-art supervised extract-based approaches. In addition, human judges rate our system summaries signiﬁcantly higher than compared systems in ﬂuency and overall quality. 
What do we want to learn from a translation competition and how do we learn it with conﬁdence? We argue that a disproportionate focus on ranking competition participants has led to lots of different rankings, but little insight about which rankings we should trust. In response, we provide the ﬁrst framework that allows an empirical comparison of different analyses of competition results. We then use this framework to compare several analytical models on data from the Workshop on Machine Translation (WMT). 
Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 
Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We deﬁne a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difﬁcult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains. 
We present BRAINSUP, an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sentences and to control the generation process across several semantic dimensions, namely emotions, colors, domain relatedness and phonetic properties. We evaluate its performance on a creative sentence generation task, showing its capability of generating well-formed, catchy and effective sentences that have all the good qualities of slogans produced by human copywriters. 
Toponym resolvers identify the speciﬁc locations referred to by ambiguous placenames in text. Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population. This paper shows that text-driven disambiguation for toponyms is far more effective. We exploit document-level geotags to indirectly generate training instances for text classiﬁers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles. 
As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 
Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and semantically typing newly emerging out-ofKB entities, thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE. Our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowdsourced user studies, show our method performing signiﬁcantly better than prior work. 
Relation Extraction (RE) is the task of extracting semantic relationships between entities in text. Recent studies on relation extraction are mostly supervised. The clear drawback of supervised methods is the need of training data: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. This is the problem of domain adaptation. In this paper, we propose to combine (i) term generalization approaches such as word clustering and latent semantic analysis (LSA) and (ii) structured kernels to improve the adaptability of relation extractors to new text genres/domains. The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation. 
Word-ﬁnal /t/-deletion refers to a common phenomenon in spoken English where words such as /wEst/ “west” are pronounced as [wEs] “wes” in certain contexts. Phonological variation like this is common in naturally occurring speech. Current computational models of unsupervised word segmentation usually assume idealized input that is devoid of these kinds of variation. We extend a non-parametric model of word segmentation by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-deﬁned joint model as a ﬁrst step towards handling variation and segmentation in a single model. We analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s. We ﬁnd that Bigram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for different contexts.1 
Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics. 
In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters. We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more ﬁne-grained phonetic distinctions. On a three-way classiﬁcation task between vowels, nasals, and nonnasal consonants, our model yields unsupervised accuracy of 89% across the same set of languages. 
We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, signiﬁcantly outperforms standard pipelines or a parallel architecture. 
Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can generalize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by incorporating cross-lingual features and knowledge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and microblogs test sets respectively. 
In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly better (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate. 
We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text. 
 Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, ﬁnd its corresponding discontinuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve signiﬁcant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation. 
We propose a method for automatically detecting low-quality Web-text translated by statistical machine translation (SMT) systems. We focus on the phrase salad phenomenon that is observed in existing SMT results and propose a set of computationally inexpensive features to effectively detect such machine-translated sentences from a large-scale Web-mined text. Unlike previous approaches that require bilingual data, our method uses only monolingual text as input; therefore it is applicable for reﬁning data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages. 
We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions. Given a large, community-authored, question-paraphrase corpus, we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions. Our approach automatically generalizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision. 
The 2011 Great East Japan Earthquake caused a wide range of problems, and as countermeasures, many aid activities were carried out. Many of these problems and aid activities were reported via Twitter. However, most problem reports and corresponding aid messages were not successfully exchanged between victims and local governments or humanitarian organizations, overwhelmed by the vast amount of information. As a result, victims could not receive necessary aid and humanitarian organizations wasted resources on redundant efforts. In this paper, we propose a method for discovering matches between problem reports and aid messages. Our system contributes to problem-solving in a large scale disaster situation by facilitating communication between victims and humanitarian organizations. 
 Aspect-based summarization is an active re-  We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that  search area for which various techniques have been developed, both statistical (Mei et al., 2007; Titov and McDonald, 2008b) and not (Hu and Liu, 2004), and relying on different types of supervision sources, such as sentiment-annotated texts or polarity lexica (Turney and Littman, 2002). Most methods rely on local information (bag-of-words, short ngrams or elementary syntactic fragments) and do not attempt to account for more complex interactions. However, these local lexical representations by themselves are often not sufﬁcient to infer a sentiment or aspect for a fragment of text. For instance, in the following example taken from a TripAdvisor1 review:  we conducted indicated that the integra-  Example 1. The room was nice but let’s not talk  tion of a discourse model increased the  about the view.  prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.  it is difﬁcult to deduce on the basis of local lexical features alone that the opinion about the view is negative. The clause let’s not talk about the view could by itself be neutral or even positive given the right context (e.g., I’ve never seen such a fancy ho-  
This paper addresses the task of ﬁnegrained opinion extraction – the identiﬁcation of opinion-related entities: the opinion expressions, the opinion holders, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. Most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the interdependencies among different extraction stages are not captured. We propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction, and seeks a globally optimal solution. Experimental results demonstrate that our joint inference approach signiﬁcantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction. 
Unbiased language is a requirement for reference sources like encyclopedias and scientiﬁc texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-speciﬁc words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective intensiﬁers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task. 
Procedural dialog systems can help users achieve a wide range of goals. However, such systems are challenging to build, currently requiring manual engineering of substantial domain-speciﬁc task knowledge and dialog management strategies. In this paper, we demonstrate that it is possible to learn procedural dialog systems given only light supervision, of the type that can be provided by non-experts. We consider domains where the required task knowledge exists in textual form (e.g., instructional web pages) and where system builders have access to statements of user intent (e.g., search query logs or dialog interactions). To learn from such textual resources, we describe a novel approach that ﬁrst automatically extracts task knowledge from instructions, then learns a dialog manager over this task knowledge to provide assistance. Evaluation in a Microsoft Ofﬁce domain shows that the individual components are highly accurate and can be integrated into a dialog system that provides effective help to users. 
Social media platforms have enabled people to freely express their views and discuss issues of interest with others. While it is important to discover the topics in discussions, it is equally useful to mine the nature of such discussions or debates and the behavior of the participants. There are many questions that can be asked. One key question is whether the participants give reasoned arguments with justifiable claims via constructive debates or exhibit dogmatism and egotistic clashes of ideologies. The central idea of this question is tolerance, which is a key concept in the field of communications. In this work, we perform a computational study of tolerance in the context of online discussions. We aim to identify tolerant vs. intolerant participants and investigate how disagreement affects tolerance in discussions in a quantitative framework. To the best of our knowledge, this is the first such study. Our experiments using real-life discussions demonstrate the effectiveness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions. 
Repeating experiments is an important instrument in the scientiﬁc toolbox to validate previous work and build upon existing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproducing results is still difﬁcult. We show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper understanding of the examined techniques. We identify ﬁve aspects that can inﬂuence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our ﬁeld. 
This work proposes a new segmentation evaluation metric, named boundary similarity (B), an inter-coder agreement coefﬁcient adaptation, and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in Fournier and Inkpen (2012). Existing segmentation metrics such as Pk, WindowDiff, and Segmentation Similarity (S) are all able to award partial credit for near misses between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also produces cosmetically high values that overestimate agreement & performance, leading this work to propose a solution. 
Query segmentation, like text chunking, is the ﬁrst step towards query understanding. In this study, we explore the effectiveness of crowdsourcing for this task. Through carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Similarly, in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees. 
In Community question answering (QA) sites, malicious users may provide deceptive answers to promote their products or services. It is important to identify and ﬁlter out these deceptive answers. In this paper, we ﬁrst solve this problem with the traditional supervised learning methods. Two kinds of features, including textual and contextual features, are investigated for this task. We further propose to exploit the user relationships to identify the deceptive answers, based on the hypothesis that similar users will have similar behaviors to post deceptive or authentic answers. To measure the user similarity, we propose a new user preference graph based on the answer preference expressed by users, such as “helpful” voting and “best answer” selection. The user preference graph is incorporated into traditional supervised learning framework with the graph regularization technique. The experiment results demonstrate that the user preference graph can indeed help improve the performance of deceptive answer prediction. 
In this paper, we explore the utility of intra- and inter-sentential causal relations between terms or clauses as evidence for answering why-questions. To the best of our knowledge, this is the ﬁrst work that uses both intra- and inter-sentential causal relations for why-QA. We also propose a method for assessing the appropriateness of causal relations as answers to a given question using the semantic orientation of excitation proposed by Hashimoto et al. (2012). By applying these ideas to Japanese why-QA, we improved precision by 4.4% against all the questions in our test set over the current state-of-theart system for Japanese why-QA. In addition, unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the conﬁdent answers only. 
In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and signiﬁcantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin. 
Mining opinion targets is a fundamental and important task for opinion mining from online reviews. To this end, there are usually two kinds of methods: syntax based and alignment based methods. Syntax based methods usually exploited syntactic patterns to extract opinion targets, which were however prone to suffer from parsing errors when dealing with online informal texts. In contrast, alignment based methods used word alignment model to fulﬁll this task, which could avoid parsing errors without using parsing. However, there is no research focusing on which kind of method is more better when given a certain amount of reviews. To ﬁll this gap, this paper empirically studies how the performance of these two kinds of methods vary when changing the size, domain and language of the corpus. We further combine syntactic patterns with alignment model by using a partially supervised framework and investigate whether this combination is useful or not. In our experiments, we verify that our combination is effective on the corpus with small and medium size. 
This paper proposes a novel two-stage method for mining opinion words and opinion targets. In the ﬁrst stage, we propose a Sentiment Graph Walking algorithm, which naturally incorporates syntactic patterns in a Sentiment Graph to extract opinion word/target candidates. Then random walking is employed to estimate conﬁdence of candidates, which improves extraction accuracy by considering conﬁdence of patterns. In the second stage, we adopt a self-learning strategy to reﬁne the results from the ﬁrst stage, especially for ﬁltering out high-frequency noise terms and capturing the long-tail terms, which are not investigated by previous methods. The experimental results on three real world datasets demonstrate the effectiveness of our approach compared with stateof-the-art unsupervised methods. 
Understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text, as seemingly objective statements often allude nuanced sentiment of the writer, and even purposefully conjure emotion from the readers’ minds. The focus of this paper is drawing nuanced, connotative sentiments from even those words that are objective on the surface, such as “intelligence”, “human”, and “cheesecake”. We propose induction algorithms encoding a diverse set of linguistic insights (semantic prosody, distributional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the ﬁrst broad-coverage connotation lexicon. 
In this paper, we present a new method based on machine translation for correcting serial grammatical errors in a given sentence in learners’ writing. In our approach, translation models are generated to translate the input into a grammatical sentence. The method involves automatically learning two translation models that are based on Web-scale n-grams. The first model translates trigrams containing serial preposition-verb errors into correct ones. The second model is a back-off model, used in the case where the trigram is not found in the training data. At run-time, the phrases in the input are matched and translated, and ranking is performed on all possible translations to produce a corrected sentence as output. Evaluation on a set of sentences in a learner corpus shows that the method corrects serial errors reasonably well. Our methodology exploits the state-of-the art in machine translation, resulting in an effective system that can deal with many error types at the same time. Keywords: Grammatical Error Correction, Serial Errors, Machine Translation, N-grams, Language Model 1. Introduction Many people are learning English as a second or foreign language: it is estimated there are 375 million English as a Second Language (ESL) and 750 million English as a Foreign Language (EFL) learners around the world, according to Graddol (2006). Three times as many people speak English as a second language as there are native speakers of English. Nevertheless, non-native speakers tend to make many kinds of errors in their writing, due to the influence of their native languages (e.g., Chinese or Japanese). Therefore, automatic grammar checkers are needed to help learners improve their writing. In the long run, automatic grammar checkers also can help non-native writers learn from the corrections and   Department of Computer Science, National Tsing Hua University E-mail: {wujc86; jim.chang.nthu; jason.jschang}@gmail.com  32  Jian-cheng Wu et al.  gradually gain better command of grammar and word choices. The grammar checkers available in popular word processors have been developed with a focus on native speaker errors, such as subject-verb agreement and pronoun reference. Therefore, these word processors (e.g., Microsoft Word) often offer little or no help with common errors causing problems for English learners (e.g., missing, unnecessary, or wrong article, preposition, and verb form) as described in The Longman Dictionary of Common Errors, second edition (LDOCE) by Heaton and Turton (1996). The LDOCE is the result of analyzing errors encoded in the Longman Learners’ Corpus. The LDOCE shows that grammatical errors in learners’ writing can either appear in isolation (e.g., the wrong proposition in “I want to improve my ability of [in] English.”) or consecutively (e.g., the unnecessary preposition immediately followed by a wrong verb form in “These machines are destroying our ability of thinking [to think].”). We refer to two or more errors appearing consecutively as serial errors. Previous works on grammar checkers either have focused on handling one common type of error exclusively or handling it independently in a sequence of errors. Nevertheless, when an error is not isolated, it is difficult to correct the error when another related error is in the immediate context. In other words, when serial errors occur in a sentence, a grammar checker needs to correct the first error in the presence of the second error (or vice-versa), making correction difficult to achieve. These errors could be corrected more effectively if the corrector recognized them as serial errors and attempted to correct the serial errors at once. Consider an erroneous sentence, “I have difficulty to understand English.” The correct sentence should be “I have difficulty in understanding English.” It is hard to correct these two errors one by one, since the errors are dependent on each other. Intuitively, by identifying “difficulty to understand” as containing serial errors and correcting it to “difficulty in understanding,” we can handle this kind of problem more effectively.  Input: I have difficulty to understand English. .  Phrase table of translation model:  difficulty of understanding ||| difficulty in understanding ||| 0.86  difficulty to understand  ||| difficulty in understanding ||| 0.86  difficulty with understanding ||| difficulty in understanding ||| 0.86  difficulty in understand  ||| difficulty in understanding ||| 0.86  difficulty for understanding ||| difficulty in understanding ||| 0.86  difficulty about understand ||| difficulty in understanding ||| 0.86  Back-off translation model:  difficulty of VERB+ing ||| difficulty in VERB+ing ||| 0.34  difficulty to VERB  ||| difficulty in VERB+ing ||| 0.34  difficulty with VERB+ing||| difficulty in VERB+ing ||| 0.34  difficulty in VERB  ||| difficulty in VERB+ing ||| 0.34  difficulty for VERB+ing ||| difficulty in VERB+ing ||| 0.34  difficulty about VERB+ing ||| difficulty in VERB+ing ||| 0.34  Output: I have difficulty in understanding English. Figure 1. Example session of correcting the sentence, “I have difficulty to understand English.”  Correcting Serial Grammatical Errors based on N-grams and Syntax  33  We present a new system that automatically generates a statistical machine translation model based on a trigram containing a word followed by preposition and verb or by an infinitive in web-scale n-gram data. At run-time, the system generates multiple possible trigrams by changing a word’s lexical form and preposition in the original trigram. Example trigrams generated for “difficulty to understand” are shown in Figure 1. The system then ranks all of these generated sentences and use the highest ranking sentence as suggestion. The rest of the paper is organized as follows. We review the related work in the next section. Then, we describe our method for automatically learning to translate a sentence that may contain preposition-verb serial errors into a grammatical sentence (Section 3). In our evaluation, we describe how to measure the precision and recall of producing grammatical sentences (Section 4) in an automatic evaluation (Section 5) over a set of marked sentences in a learner corpus. 2. Related Work Grammatical Error Detection (GED) for language learners has been an area of active research. GED involves pinpointing some words in a given sentence as ungrammatical and offering correction if necessary. Common errors in learners’ writing include misuse of articles, prepositions, noun number, and verb form. Recently, the state-of-the-art research on GED has been surveyed by Leacock et al. (2010). In our work, we address serial errors in English learners’ writing which are simultaneously related to the preposition and verb form, an aspect that has not been dealt with in most GED research. We also consider the issues of broadening the training data for better coverage and coping with data sparseness when unseen events happen. Although there are over a billion people estimated to be using or learning English as a second or foreign language, common English proofreading tools do not target specifically the most common errors made by second language learners. Many widely-used grammar checking tools are based on pattern matching and at least some linguistic analysis, based on hand-coded grammar rules (Leacock et al., 2010). In the 1990s, data-driven, statistical methods began to emerge. Statistical systems have the advantage of being more intolerant of ill-form, interlanguage, and unknown words produced by the learners than the rule-based systems. Knight and Chander (1994) proposed a method based on a decision tree classifier to correct article errors in the output of machine translation systems. Articles were selected based on contextual similarity to the same noun phrase in the training data. Atwell (1987) used a language model of a language to represent correct usage for that language. He used the language model to detect errors that tend to have a low language model score.  34  Jian-cheng Wu et al.  More recently, researchers have looked at grammatical errors related to the most common prepositions (9 to 34 prepositions, depending on the percentage of coverage). Eeg-Olofsson and Knuttson (2003) described a rule-based system to detect preposition errors for learners of Swedish. Based on part-of-speech tags assigned by a statistical trigram tagger, 31 rules were written for very specific preposition errors. Tetreault and Chodorow (2008), Gamon et al. (2008), and Gamon (2010) developed statistical classifiers for preposition error detection. De Felice and Pulman (2007) trained a voted perceptron classifier on features of grammatical relations and WordNet categories in an automatic parse of a sentence. Han et al. (2010) found that a preposition error detection model trained on correct and incorrect usage in a learner corpus works better than using well-formed text in a reference corpus. In the research area of detecting verb form errors, Heidorn (2000) and Bender et al. (2004) proposed methods based on parse tree and error templates. Lee and Seneff (2008) focused on three cases of verb form errors: subject-verb agreement, auxiliary agreement, and verb complement. The first two types are isolated verb form errors, while the third type may involve serial errors related to preposition and verb. Izumi et al. (2003) proposed a maximum entropy model, using lexical and POS features, to recognize a variety of errors, including verb form errors. Lee and Seneff (2008) used a database of irregular parsing caused by verb form misuse to detect and correct verb form errors. In addition, they also used the Google n-gram corpus to filter out improbable detections. Both Izumi et al. (2003) and Lee and Seneff (2008) obtained a high error correction rate, but they did not report serial errors separately, making comparison with our approach is impossible. In a study more closely related to our work, Alla Rozovskaya and Dan Roth (2013) introduced a joint learning scheme to jointly resolve pairs of interacting errors related to subject-verb and article-noun agreements. They showed that the overall error correction rate is improved by learning a model that jointly learns each of these interacting errors. 3. Method Correcting serial errors (e.g., “I have difficulty to understand English.”) one error at a time in the traditional way may not work very well, but previous works typically have dealt with one type of error at a time. Unfortunately, it may be difficult to correct an error in the context of another error, because an error could only be corrected successfully within the correct context. Besides, such systems need to correct a sentence multiple times, which is time-consuming and more error-prone. To handle serial errors, a promising approach is to treat serial errors together as one single error.  Correcting Serial Grammatical Errors based on N-grams and Syntax  35  3.1 Problem Statement We focus on correcting serial errors in learners’ writing using the context of trigrams in a sentence. We train a statistical machine translation model to correct learners’ errors of the types of a content word followed by a preposition and a verb using web-scale n-grams. Problem Statement: We are given a sentence S = w1, w2, …, wn, and web-scale n-gram, webgram. Our goal is to train two statistical machine translation model TM and back-off model TMbo to correct learners’ writing. At run-time, trigrams (wi , wi+1, wi+2) in S (i =1, n-2) are matched and replaced using TM and the back-off model TMbo to translate S into a correct sentence T. In the rest of this section, we describe our solution to this problem. First, we describe the strategy to train TM (Section 3.2) and TMbo (Section 3.3) using webgrams. Finally, we show how our system corrects a sentence at run-time using TM, TMbo, and a language model LM (Section 3.4). 3.2 Generating TM We attempt to identify trigrams that fit the pattern of serial errors and correction we are dealing with in webngram, and we group the selected trigrams by their content words and verb lemmas. Our learning process is shown in Figure 2. We assume that, within each group, the low frequency trigrams are probably errors that should be replaced by the most frequent trigram: a one construction per collocation constraint. For example, when expressing “difficulty” and “to understand,” any NPV constructs with low frequency (e.g., “difficulty for understanding” and “difficulty about understanding”) are erroneous forms of the most frequent trigram “difficulty in understanding”. Therefore, we generate TM with such phrase to phrase translations accordingly. (1) Select trigrams related to serial errors and corrections from webngram (Section 3.2.1) (2) Group the selected trigrams by the first and last word in the trigrams (Section 3.2.2) (3) Generate a phrase table for the statistical machine translation models for each group (Section 3.2.3) Figure 2. Outline of the process used to generate TM. 3.2.1 Select and Annotate Trigrams We select four types of trigrams (t1, t2, t3) from webngram, including noun-prep-verb (NPV), verb-prep-verb (VPV), adj-prep-verb (APV), and adverb-prep-verb (RPV). We then annotate the trigrams with types and lemmas of content words t1 and t3 (e.g., “accused of being 230633” becomes “VPV, accuse be, accused of being 230633). Figure 3 shows some sample annotated trigrams.  36  Jian-cheng Wu et al.  VPV, accuse be, accused of being  230,600  VPV, accuse kill, accused of killing  83,100  VPV, accuse have, accused of having  78,500  VPV, accuse use, accuse of using  45,200  VPV, accuse murder, accused of murdering 40,032  VPV, accuse be, accused to be  10,200  VPV, accuse prove, accused to prove  3,600  Figure 3. Sample annotated trigrams  VPV, accuse be, accused of being  230,600  VPV, accuse be, accused to be  10,200  VPV, accuse be, accused of is  2,841  VPV, accuse be, accuse of being  2,837  VPV, accuse be, accused as being  929  VPV, accuse be, accused of was  676  VPV, accuse be, accused from being 535  Figure 4. Sample trigram group  accused to be  ||| accused of being ||| 0.93  accused of is  ||| accused of being ||| 0.93  accuse of being ||| accused of being ||| 0.93  accused as being ||| accused of being ||| 0.93  accuse of was  ||| accused of being ||| 0.93  accused from being ||| accused of being ||| 0.93  Figure 5. Sample phrase translations for a trigram group  3.2.2 Group Trigrams We then group the trigrams by types, the first words, and the verb lemmas. See Figure 4 for a sample VPV group of trigrams. This step should bring together the trigrams containing serial errors and their correction. Note that we assume certain serial errors will have a correction of the same length here, which is true in most cases.  3.2.3 Generate Rules For each group of annotated trigrams, we then generate phrase and translation pairs with  Correcting Serial Grammatical Errors based on N-grams and Syntax  37  probability as follows. Recall that we assume that the higher the count of the trigram, the more likely the trigram is to be correct. So, we generate “l1, l2, l3 ||| h1, h2, h3 ||| p ,” where h1, h2, h3 is the trigram with the highest frequency count; l1, l2, l3 is one of the trigrams with lower frequency count; and p denotes the probability of l1, l2, l3 translating into h1, h2, h3. We define p=(highest frequency count)/(group frequency count).  3.3 Generating TMbo In addition to the surface-level translation model TM, we also build a back-off model as a way of coping with cases where the trigram (t1, t2, t3) is unseen in TM. The idea is to assume the complement (t2, t3) of t1 tends to be in a certain syntactic form regardless of the verb t3, as dictionaries typically would describe the usage of “accuse” in terms of “accuse somebody of doing something.” Our learning process for TMbo is shown in Figure 9.  VPV, accuse VERB, accused of VERB-ing  230,600  VPV, accuse VERB, accused of VERB-ing  83,100  VPV, accuse VERB, accused of VERB-ing  78,500  VPV, accuse VERB, accuse of VERB-ing  45,200  
In this project, we have studied Chinese noun-noun compounds (NNCs) and have found that N1 and N2 are linked either by semantic roles assigned by events (complex relations) or by static relations (simple relations), including meronymy, conjunction, and the host-attribute-value relation. Using data from the FrameNet and E-HowNet, we have found that, for NNCs of either type, the major semantic relations between the two components are limited enough to allow computational implementation. Regarding simple relations, most conjunction pairs have been listed in E-HowNet,and so are host-attribute-value sets. The E-HowNet Taxonomy also makes identification of meronymy possible. As for NNCs involving complex relations, each component’s semantic role, along with the events that assign these roles, can be restored through mappings to corresponding frame elements (FEs) in entity and to event frames and lexical units (LUs) in FrameNet’s frames, respectively, that represent the concept the NNC conveys. Keywords: Noun-noun Compounds, Automatic Interpretation, Extended HowNet (E-HowNet), FrameNet 1. Introduction Noun-noun compounds (henceforth NNC) are compounds composed of two nouns. For example: 麵包刀 mianbao-dao ‘bread knife’ 衛星城市 weixin-chengshi ‘satellite city’ 金融股 jinrong-gu ‘stocks in the financial sector’ 秋蟹 qiu-xie ‘autumn crab’   Institute of Information Science, Academia Sinica, Taipei, Taiwan E-mail: {yschung, kchen}@iis.sinica.edu.tw  46  You-shan Chung and Keh-Jiann Chen  腳踏車輪胎 jiao-ta-che luntai ‘bicycle tire’ 卵石地板 luan-shi diban ‘pebble floor’ 鐘錶 zhong-biao ‘clock and watch’ 鐵桌 tie-zhuo ‘iron table/desk’ 車速 che-su ‘car speed’ While the part-of-speech (POS) of NNCs usually is nominal, their interpretations seem so diverse that some researchers even contend that they are completely determined by context (e.g. Dowty, 1979; reviewed in Copestake & Lascarides, 1997). Nevertheless, the majority of researchers believe that there is at least some degree of regularity in NNC interpretation. This regularity is often reported to be at least partially universal as well (Levi, 1978; SØgaard, 2005). There are three popular theories along these lines, which are not mutually exclusive. First, there is a limited set of semantic relations between the two component nouns, N1 and N2 (Levi, 1978; as well as computational works that implemented her theory, e.g. Copestake & Lascarides, 1997; SØgaard, 2005; Copestake & Briscoe, 2005; Huang, 2008). Second, N1 and N2 are the arguments of an event that bridges them and by which they are assigned semantic roles (Levi, 1978; Leonard, 1984; Ryder, 1994). Third, the two component nouns sometimes are linked through similarity in some aspect, resulting in metaphorical readings. Nevertheless, these accounts generally have the following four problems. First, the semantic relations they proposed or adopted tend to be not specific enough. Levi (1978), for instance, proposed nine semantic relations between N1 and N2, which she called Recoverably Deleted Predicates (RDP), including CAUSE, HAVE, MAKE, USE, BE, IN, FOR, FROM, and ABOUT. These RDPs, however, appear to be too general to be informative, especially with prepositional ones like IN and FOR, as NNCs linked by the same preposition belong to the same semantic categories only in a very broad sense. Second, some of the studies resolve only limited or sporadic semantic categories, while others are questionable in terms of their correct prediction rate. For example, the fourteen semantic relations Li and Thompson (1981) proposed do not seem to make up a meaningful and discrete inventory of semantic relations, while Huang’s (2008) combinational patterns for three major categories of physical objects (i.e. animals, plants, and artifacts) are each based on the analysis of only six morphemes, raising concerns about generality. The third problem is that the classifying criteria mostly are left unaccounted for; thus, they appear arbitrary. For example, Levi (1978) sees the two components of lemon peel and apple seed as linked by the predicates HAVE and FROM, respectively, but such a distinction  A Semantic-Based Approach to Noun-Noun Compound Interpretation  47  between the two NNCs may not be without controversy. The last problem is that bridging does not seem to be eventive or by prepositions in the following three situations: first, the host-attribute-value relation (e.g. 鐵桌 tie-zhuo ‘iron table/desk,’ 車速 che-su ‘car speed’) with two special subclasses, where N1 denotes time (e.g. 秋 蟹 qiu-xie ‘autumn crab’) or N1 denotes space (e.g. 倫 敦 地 鐵 Lundun-ditie ‘London Underground’); second, meronymy, or part-whole relation (part-whole: e.g. 雙底船 shuang-di chuan ‘double-bottom,’; whole-part: e.g. 腳踏車輪胎 jiao-ta-che luntai ‘bicycle tire’); and third, conjunction (e.g. 鐘錶 zhong-biao ‘clock and watch,’ 禮樂 li-yue ‘manners and music’) . Before we go on, we need to explain the definition of Chinese NNCs adopted in this study. Unlike in English, formal similarity in Chinese does not entail a shared POS. For example, the first component in 希臘國歌 xila guo-ge ‘the national anthem of Greece,’ 希臘 菜 xila-cai ‘Greek dish,’ and 月費 yue-fei ‘monthly fee’ corresponds to adjective forms in their English equivalents. Nevertheless, we include these various forms in our analysis since such formal differences do not reflect conceptual differences, as Levi (1978) has argued for this at length and also included adjectives in her analysis of what she called “complex nominal,” or “NNCs” in our terms. Addressing the aforementioned four problems, we used a knowledge base that we believe could help decide the precise semantic relations for both event-linked and non-event-linked NNCs, which is FrameNet (https://framenet.icsi.berkeley.edu/fndrupal/). In essence, the theory behind FrameNet is that lexical units (LU) evoke concepts represented by “frames,” which are each composed of a set of frame elements (FE), i.e. the overtly-realized semantic roles assigned by the frame’s LUs. Some LUs evoke entity concepts, while others evoke eventive ones. Since many entities in FrameNet have frames, we think it might be possible to map more NNC-productive N2s in our database, along with the NNCs they derive, to corresponding entity frames in FrameNet. We have two research questions. First, with a corpus and FrameNet, we investigate whether there are only limited bridging verbs and semantic relations between the two component nouns of a NNC. Second, are there semantic relations between N1 and N2 that do not involve bridging events? 2. Complex Relations As mentioned in the Introduction, many researchers hold that an NNC’s component nouns are the arguments of an event that bridges them and by which they are assigned semantic roles. Levi (1978) regards all N1s and N2s as subjects and objects of nine linking predicates, with  48  You-shan Chung and Keh-Jiann Chen  one component entity doing something to the other. Below are her examples1 and their Chinese equivalents: CAUSE: e.g. malarial mosquitoes (瘧蚊 nue-wen) HAVE: e.g. picture book (圖畫書 tuhua-shu), apple cake (蘋果蛋糕 pingguo dangao), gunboat (砲艇 pao-ting), industrial area (工業區 gongye-qu), imperial bearing (貴族 氣質 guizu-qizhi) MAKE: e.g. honeybee (蜜蜂 mi-fong), daisy chains (雛菊鍊 chuju-lian) USE: e.g. steam iron (蒸氣電熨斗 zhengqi dian-yundou), solar generator (太陽能發電機 taiyang-neng fadian-ji) BE: e.g. target structure (目標結構 mubiao-jiegou), ceiling price (天價 tian-jia), queen bee (女王蜂 nu-wang fong), satellite nation (衛星國家 weixing-guojia), phantom limb (幽靈 肢 youling-zhi) IN: e.g. field mouse (田鼠 tian-shu), autumnal rains (秋雨 qiu-yu) FOR: e.g. horse doctor (馬醫 ma-yi), arms budget (武器預算 wuqi-yusuan), nasal mist (鼻腔 噴霧 bi-qiang pen-wu) FROM: e.g. olive oil (橄欖油 ganlan-you), test-tube baby (試管嬰兒 shi-guan yinger), apple seed (蘋果籽 pingguo-zi), rural visitors (鄉間訪客 xiangjian fang-ke) ABOUT: e.g. tax law (稅法 shui-fa), criminal policy (刑事政策 xingshi-zhengce) Levi says NNCs are all linked by one of the nine predicates, with the two components being their arguments; however, we believe that some NNCs simply involve more static relations and some relations are not covered by the above nine predicates. One instance that involves a missing static relation is, for example, the highly-productive shape relation, e.g. dragon boat (龍舟 long-zhou). In the following sections, we will use evidence of both language instinct and FrameNet data to support the distinction between simple and complex relations. 3. Motivating Simple Relations Besides event-bridging relations, we propose simple relations, where N1 and N2 are not interacting participants of an event. Despite their shared syntactic and semantic properties, instances of simple relations have not been recognized as a distinct category, as observed by Liu (2008) and by Chung and Chen (2010). 
Fluency and continuity properties are essential in synthesizing a high quality singing voice. In order to synthesize a smooth and continuous singing voice, the Hidden Markov Model-based synthesis approach is employed in this study to construct a Mandarin singing voice synthesis system. The system is designed to generate Mandarin songs with arbitrary lyrics and melody in a certain pitch range. In this study, a singing voice database is designed and collected, considering the phonetic converge of Mandarin singing voices. Synthesis units and a question set are defined carefully and tailored the meet the minimum requirement for Mandarin singing voice synthesis. In addition, pitch-shift pseudo data extension and vibrato creation are applied to obtain more natural synthesized singing voices. The evaluation results show that the system, based on tailored synthesis units and the question set, can improve the quality and intelligibility of the synthesized singing voice. Using pitch-shift pseudo data and vibrato creation can further improve the quality and naturalness of the synthesized singing voices. Keywords: Mandarin Singing Voice Synthesis, Hidden Markov Models, Vibrato 1. Introduction In recent years, Mandarin text-to-speech synthesis systems have been proposed and have achieved satisfactory performance (Ling, 2012; Wu, 2007). These systems are able to synthesize fluent and natural speech, even with personal characteristics (Huang, 2013). Recently, singing voice synthesis has been one of the emerging and popular research topics. Such systems enable computers to sing any song. There are two main methods in the research on corpus-based singing voice synthesis. The first one is the sample-based approach. The principle of this method is to use a large database  Department of Computer Science and Information Engineering, National Cheng Kung University, Taiwan E-mail: { carrie771221; ychin.huang; chunghsienwu}@gmail.com  64  Ju-Yun Cheng et al.  of recordings of singing voices that are further segmented into units. In the synthesis phase, based on a given score with the lyrics, the system then searches and selects appropriate sub-word units for concatenation. VOCALOID (Kenmochi, 2007) is such a singing voice synthesizer that enables the user to input lyrics and the corresponding melody. Given the score information, the system selects the necessary samples from the Singer Library and concatenates them to produce the synthesized singing voice. Finally, the system performs pitch conversion and timbre manipulation to generate smoothed concatenated samples. The software was originally only available in English and Japanese, but VOCALOID 3 has added support for Spanish, Chinese, and Korean. A Mandarin singing voice system using a unit selection method was proposed in (Zhou, 2008). Singing units in this method are chosen from a singing voice corpus with the lyrics of the song and the musical score information embedded in a MIDI file. To improve the synthesis quality, synthesis unit selection and the prosody and amplitude modification are applied. This system uses a Hanning window to smooth instances where speech segments were concatenated. Although the unit selection method is able to synthesize high quality speech at the waveform level, the concatenation-based methods suffer from the discontinuity problem at the boundaries between concatenated units. As different samples that make up the singing voice are recorded in different pitches and phonemes, discontinuity might exist in the resulting singing voice. The other method is the statistical approaches, where hidden Markov models (HMMs) (Oura, 2010; Saino, 2006) are the most widely used. Acoustic parameters are extracted from a singing voice database and modeled by the context-dependent HMMs. The acoustic parameters are generated by the concatenated HMM sequence. Finally, vocoded waveforms of the singing voice are generated from the inverse filter of the acoustic parameters. Sinsy (Oura, 2010) is a free-online HMM-based singing voice synthesis system that provides Japanese and English singing voices. Users can obtain synthesized singing voices by uploading musical scores. Synthesizing singing voices based on HMMs sound blurred due to the limitation of the current vocoding technique. Nevertheless, it can generate a smooth and stable singing voice, and its voice characteristics can be modified easily by transforming the parameters appropriately. In addition to the concatenation-based method and statistical method, there are also some other methods proposed to generate a Mandarin singing voice, e.g., Harmonic plus Noise Model (HNM) (Gu, 2008), which adopted HNM parameters of a source syllable to synthesize singing syllables of diverse pitches and durations. This method can generate singing voices with good quality. Nevertheless, the discontinuity problem occurring at the concatenation points is still a major problem. Speech-to-singing method (Saitou, 2007) is another approach. Instead of synthesizing from a singing database, the speech-to-singing method converts speech into a singing voice by a parameter control model. Similarly, text-to-singing (lyrics-to-singing)  HMM-based Mandarin Singing Voice Synthesis  65  Using Tailored Synthesis Units and Question Sets  synthesis (Li, 2011) is used to generate synthesized speech of input lyrics by a TTS system followed by a melody control model that converts speech signals into singing voices by modifying the acoustic parameters. These two methods are based mainly on conversion rules that could be patchy. Research on speech and singing synthesis has been closely linked, but there are important differences between the two methods with respect to the generated voices. The major parts of singing voices are voiced segments, whereas speech consists of a relatively large percentage of unvoiced sounds (Kim, 2003). Besides, fluency and continuity in singing voices are very important properties. In order to synthesize a smooth and continuous singing voice, an HMM-based synthesis approach is adopted in this study to build our singing voice synthesis system. To the best of our knowledge, the currently available HMM-based singing voice synthesis systems have not been applied to the Mandarin singing voice. By carefully defining and tailoring the synthesis units and the question set, a Mandarin singing voice synthesis system based on HMM-based framework has been constructed successfully in this study. The rest of the paper is organized as follows. The proposed HMM-based singing voice synthesis system is introduced in Section 2. Section 3 consists of subjective and objective evaluations of the proposed system, compared to the original HMM-based singing voice synthesis system. Concluding remarks and future work are given in Section 4. 2. Proposed Mandarin Singing Voice Synthesis System In recent years, the number of studies on HMM-based speech synthesis has grown. Some research has made progress on prosody improvement (Hsia, 2010; Huang, 2012) to obtain more natural speech. Recently, an HMM-based method has been applied to singing voice synthesis (Saino, 2006). There are more combinations of contextual factors in singing voice synthesis than that in speech synthesis. Applying a unit selection method to singing voice synthesis is quite difficult, because it needs a huge number of singing voices. On the contrary, an HMM-based system can be constructed using a relatively small amount of training data. As a result, the HMM-based approach is easier for constructing a singing voice synthesizer. The system proposed in this study is based on the HMM-based approach that was developed by the HTS working group (Zen, 2007). The proposed structure of the singing synthesis system based on HMM is shown in Figure 1.  66  Ju-Yun Cheng et al.  Figure 1. Structure of the HMM-based Singing Voice Synthesis System In the training phase of the proposed system, excitation, spectral, and aperiodic parameters are extracted from a singing voice database. Lyrics and notes of the songs in the singing corpus are considered as contextual information for generating context-dependent label sequences. Then, the sequences are split and clustered with context-dependent question sets and the context-dependent HMM models are trained based on the clustered phone segments. In the synthesis phase, a musical score and the lyrics to be synthesized also are converted into a context-dependent label sequence. Based on the label sequence, a sequence of parameters, consisting of excitation, spectral, and aperiodic parameters, corresponding to the given song is obtained from the concatenated context-dependent HMMs. Finally, the obtained parameter sequences are synthesized to generate the singing voice. 2.1 Model Definition Singing is the act of producing musical sounds with one’s voice, and one main difference between a singing voice and speech is the use of the tonality and rhythm of a song. Therefore, the contextual factors should consist of not only linguistic information but also note information. In addition, the cue information obtains the actual timing of each phone in the singing data. The details of the model definition are described in the following section.  HMM-based Mandarin Singing Voice Synthesis  67 
This research focuses on validating a Taiwanese speech corpus by using speech recognition and assessment to automatically find the potentially problematic utterances. There are three main stages in this work: acoustic model training, speech assessment and error labeling, and performance evaluation. In the acoustic model training stage, we use the ForSD (Formosa Speech Database) ,provided by Chang Gung University (CGU), to train hidden Markov models (HMMs) as the acoustic models. Monophone, biphone (right context dependent), and triphone HMMs are tested. The recognition net is based on free syllable decoding. The best syllable accuracies of these three types of HMMs are 27.20%, 43.28%, and 45.93% respectively. In the speech assessment and error labeling stage, we use the trained triphone HMMs to assess the unvalidated parts of the dataset. And then we split the dataset as low-scored dataset, mid-scored dataset, and high-score dataset by different thresholds. For the low-scored dataset, we identify and label the possible cause of having such a lower score. We then extract features from these lower-scored utterances and train an SVM classifier to further examine if each of these low-scored utterances is to be removed. In the performance evaluation stage, we evaluate the effectiveness of finding problematic utterances by using 2 subsets of ForSD, TW01, and TW02 as the  使用語音評分技術輔助台語語料的驗證  83  training dataset and one of the following: the entire unprocessed dataset, both mid-scored and high-scored dataset, and high-scored dataset only. We use these three types of joint dataset to train and to evaluate the performance. The syllable accuracies of these three types of HMMs are 40.22%, 41.21%, 44.35% respectively. From the previous result, the disparity of syllable accuracy between the HMMs trained by unprocessed dataset and processed dataset can be 4.13%. Obviously, it proves that the processed dataset is less problematic than unprocessed dataset. We can use speech assessment automatically to find the potential problematic utterances. Keywords: Taiwanese Corpus Validation, Hidden Markov Model, Speech Assessment, Support Vector Machine. 1. 緒論 傳統語料的整理往往需要耗費相當的時間以及需要具有專業知識背景的人員進行人工聽 測。本論文的研究即是針對台語語料使用語音評分輔助機器篩選掉不良的語料，如空白 音檔、文本有誤...等錯誤類型，取代傳統費時的人工聽測方法，藉此減少人工檢查的時 間，加快語料庫的建立。 本論文首先會進行基礎聲學模型的訓練，藉由基礎聲學模型對未整理的語料進行語 音評分，並依照分數門檻值將未整理的語料劃分為低分區語料、中間值區語料及高分區 語料，最後分別將未經篩選語料與經篩選語料加入原始訓練語料重新進行聲學模型的訓 練，以測試語料的辨識結果來評估語料整理的程度。 而對於低分區語料，我們特別對它進行人工標記，標記該音檔的不良類型，並依觀 測到的不良類型對其擷取特徵，使用支持向量機（Support Vector Machine, SVM）訓練分 類器。最後我們則使用該分類器對低分區語料再次檢驗為可用語料或是不良的語料，減 少需要人工檢驗的語料數目。底下圖 1 為語料整理系統流程圖：  84  李毓哲 等  
Spectral over-smoothing is still observable in the converted spectral envelope when linear multivariate regression (LMR) based spectrum mapping is adopted to convert voice. Therefore, in this paper, we study to place a histogram-equalization (HEQ) module immediately before LMR based mapping and to place a target frame selection (TFS) module immediately after LMR based mapping. These two modules are intended to promote the quality of the converted voice. Here, HEQ processing includes the two steps: (a) transform discrete cepstral coefficients (DCC) into principal component analysis (PCA) coefficients; (b) transform PCA coefficients into cumulated density function (CDF) coefficients. As to TFS, an input frame is first processed to obtain its converted DCC and its segment-class number. Then, the group of target-speaker frames corresponding to the same segment-class number is searched to find a target frame whose DCC are sufficiently close to the converted DCC. Next, the converted DCC are replaced by the DCC of the target frame found. In experimental evaluation, the outside parallel sentences (not used in model-parameter training) are used to measure average cepstral distances (ACD) between the converted DCC and the target DCC. When the HEQ module is added, the value of ACD would be increased a little. Furthermore, the value of ACD would be apparently increased when the TFS module is added. Nevertheless, according to the measured VR (variance ratio) values and the scores of subjective listening tests, the quality of the converted voice will become better when HEQ is added, and become much better when TFS is added. As to the reasons for why the measured ACD values and the perceived converted-voice qualities are inconsistent, we have found one possible cause which can explain why this inconsistency may occur. Keywords: Voice Conversion, Linear Multivariate Regression, Histogram Equalization, Target Frame Selection, Discrete Cepstral Coefficients. 1. 緒論 把一個來源語者(source speaker)的語音轉換成另一個目標語者(target speaker)的語音，這 種處理稱為語音轉換(voice conversion) (Abe et al., 1988; Valbret et al., 1992; Stylianou et al., 1998)，語音轉換可應用於銜接語音合成處理，以獲得多樣性的合成語音音色。去年 我們曾嘗試以線性多變量迴歸(linear multivariate regression, LMR)來建構一種頻譜對映 (mapping)的機制(古鴻炎等，2012)，然後用於作語音轉換，希望藉以改進傳統上基於高 斯混合模型(Gaussian mixture model, GMM)之頻譜對映機制(Stylianou et al., 1998)常遇到 的一個問題，就是轉換出的頻譜包絡(spectral envelope)會發生過度平滑(over smoothing) 的現象。我們經由實驗發現，音段式(segmental) LMR 頻譜對映機制不僅在平均轉換誤差  基於音段式LMR對映之語音轉換方法的改進  99  上可以比傳統 GMM 頻譜對映機制獲得一些改進，並且轉換出語音的音質也比傳統 GMM 對映的稍好一些。不過，整體而言音段式 LMR 對映機制所轉換出的頻譜包絡，仍然存 在有過度平滑的現象，而使得轉換出的語音仍然令人覺得有一些悶悶的，而不像真人發 音那樣清晰。前面提到的”音段式” LMR，是指我們對於訓練語料中不同的韻母、有 聲聲母(如/m, n, l, r/)的語音要分別去建立各自的 LMR 矩陣，這是為了避免發生一對多 (one to many)對映的問題(Godoy et al., 2009)，而造成某些相鄰的音框之間，相鄰音框所 轉換出的頻譜卻出現劇烈的頻譜形狀差異(即頻譜不連續)，而不連續的頻譜很可能導致 怪音(artifact sound)被合成出來。 去年我們研究的基於 LMR 頻譜對映之語音轉換系統，其主要的處理流程如圖 1 所 示，來源語者發出的語音先分割成一序列的音框，然後對各個音框去估計它的 40 階 DCC (discrete cepstral coefficients) 倒頻譜係數(Cappé & Moulines, 1996; Gu & Tsai, 2009)及偵 測出基頻值；接著，依據各音框的 DCC 係數，可作有聲聲母與韻母的音段(segment)偵測， 先前我們曾提出一種基於音段式 GMM 與最大似然率(maximum likelihood)的音段自動偵 測方法(Gu & Tsai, 2011)，實驗顯示即使挑選到錯誤但近似的音段，也仍可轉換出正確的 語音，由於在此我們把焦點放在 LMR 對映方塊，所以音段偵測方塊暫時以讀取標記(label) 檔案的方式來進行；LMR 對映就是把 LMR 矩陣乘以輸入的 DCC 向量而求得輸出的 DCC 向量，至於 LMR 矩陣的訓練方法，則可參考我們去年的論文(古鴻炎等，2012)；之後， LMR 對映出的 DCC 向量、及以平均值與標準差轉換出的基頻值，兩者就可送給 HNM (harmonic plus noise model)語音再合成方塊，以合成出轉換後的語音信號，關於使用諧波 加雜音模型(HNM)作語音信號合成的細節，可參考前人的論文(Stylianou, 1996; Gu & Tsai, 2009)。  來源 語音  音框 DCC 估計 基頻 偵測  音段 偵測  音段式 LMR 對映 基頻 轉換  HNM 語音 再合成  轉換後 語音  圖 1. 基於 LMR 頻譜對映之語音轉換的主要處理流程 為了提升轉換出語音的音質，我們開始思考在 GMM 對映與 LMR 對映之外，是否 還有其它種類的對映方法？後來我們想到一種似乎可行的頻譜對映方法，就是以直方圖 等化(histogram equalization, HEQ)來取代 LMR 對映。直方圖等化雖然起源於影像處理領 域，但是近年來被應用於語音辨識領域(Torre et al., 2005; Lin et al., 2007)，用以降低環境 噪音造成的訓練語音和測試語音之間的頻譜不匹配(mismatch)問題，而使得辨識率獲得了 明顯的改進。有鑑於此，我們覺得在語音轉換的問題上，來源與目標語者之間有著差異 的頻譜形狀而呈現出差異的音色，這可想像是因為來源語音通過了某一種特殊的通訊通 道而使得其頻譜形狀被轉換成目標語音的形狀，以致於造成來源與目標語音之間的頻譜  100  古鴻炎、張家維  不匹配。因此在觀念上應可應用直方圖等化的處理，來模仿前述的通訊通道之特性，以 把來源語音的頻譜轉變成目標語音的頻譜，所以我們構想了如圖 2 所示的基於直方圖等 化之語音轉換的處理流程。  來源 語音  DCC 音段 PCA 係 CDF 係 LMR 估計 偵測 數轉換 數轉換 對映 基頻 偵測  CDF 反 轉換  PCA 反 轉換 基頻 轉換  HNM 語音 再合成  轉換 語音  圖 2. 基於直方圖等化及 LMR 對映之語音轉換流程 在圖 2 的處理流程中，我們不直接拿 DCC 係數去作直方圖等化，即計算 CDF (cumulative density function)係數，我們的觀點是，一個音框各維度的 DCC 係數之間有明 顯的相關性存在，而直方圖等化卻是對特徵的各維度獨立去進行，這恐將降低直方圖等 化的功用，因此我們決定對各個音段類別所屬的音框 DCC 向量先進行主成分分析 (principle component analysis, PCA) (Jolliffe, 2002)，再依據主成分向量把 DCC 係數轉換 成 PCA 係數，如此將可讓一個音框各維度的 PCA 係數之間變成是獨立的。此外，圖 2 中的 LMR 對映方塊，一開始時是未被加入的，不過經由初步的測試實驗發現，當沒有 作 LMR 對映的處理時，轉換出語音的音色雖可達到部分近似目標語者的音色，但是仍 存在明顯的音色落差，因此我們遂決定把 LMR 對映方塊加上去，以提升音色相似度。 對於圖 1 處理流程會遇到的頻譜包絡過於平滑的情況，雖然前人曾經提出至少兩種 的改進方法，即全域變異數(global variance, GV)之變異數調整方法(Toda et al., 2007)、和 頻率軸校正(frequency warping)的方法(Erro et al., 2010; Godoy et al., 2012)，但是 Toda 等 人的方法(Toda et al., 2007)和 Erro 等人的方法(Erro et al., 2010)都是針對 GMM 對映所設 計的，而 Godoy 等人的方法(Godoy et al., 2012)則不是針對 GMM 對映或 LMR 對映所設 計的。因此我們就從另外一個方向去思考圖 1 流程的改進作法，在參考 Dutoit 等人的論 文(Dutoit et al., 2007)之後，我們想到的一個作法是，在圖 1”LMR 對映”方塊之後插入” 目標音框挑選”的方塊。既然經過 GMM 或 LMR 對映得到的頻譜包絡會發生過度平滑 的現象，那麼就不要直接拿 LMR 對映得到的頻譜係數去作語音再合成處理，而要改變 成依據來源音框(來源語者音框)的音段類別、及對映出的頻譜特徵係數(如 DCC)，去對 同一音段類別的目標音框(目標語者音框)群作搜尋，以找出頻譜特徵很相似(或距離很小) 的目標音框，然後把找出的目標音框的頻譜係數拿去取代對映出的頻譜係數，如此就可 免除發生頻譜包絡過度平滑的問題。由於被找出的目標音框不是經由頻譜對映而得到， 所以在此也稱它為真實音框(真實語音的音框)，此外，目標音框的音段分類與收集是在 訓練階段進行，所以轉換階段就可直接去作搜尋與挑選。當圖 1 插入”目標音框挑選” 的方塊之後，一種基於 LMR 對映及目標音框挑選之改進的語音轉換處理流程就如圖 3 所示。  基於音段式LMR對映之語音轉換方法的改進  101  DCC  音段 音段式  目標音  估計 偵測 LMR 對映 框挑選  來源  HNM 語音  轉換  語音  再合成  語音  基頻  基頻  偵測  轉換  圖 3. 基於 LMR 對映及目標音框挑選之語音轉換流程 除了分別去加入直方圖等化和目標音框挑選的處理動作，我們也考慮了另外一種處 理流程，就是同時把這兩種處理動作加入圖 1 的處理流程中，如此轉換出的語音是否可 以獲得最好的音色相似度及語音品質？這將會第四節中作實驗探討。此外，在圖 1、2、 3 裡都出現的 DCC 估計之方塊，表示我們採用離散倒頻譜係數(DCC)(Cappé & Moulines, 1996; Gu & Tsai, 2009)作為頻譜特徵參數，並且階數設為 40 階，即一個音框要計算出 c0, c1, c2, …, c40 等 41 個係數，但是只拿 c1, c2, …, c40 去作頻譜轉換的處理。當轉換出各個 音框的 DCC 係數之後，我們就可依據各音框的 DCC 係數去計算出頻譜包絡(Cappé & Moulines, 1996; Gu & Tsai, 2009)，然後再依據頻譜包絡、轉換出的基頻值，去設定該音 框的 HNM 模型之諧波參數和雜音參數(Gu & Tsai, 2009; Stylianou, 1996)，之後就可拿這 些參數去合成出語音信號(Gu & Tsai, 2009; Stylianou, 1996)。 2. PCA係數轉換與直方圖等化 若要依據圖 2 的處理流程來進行語音轉換的處理，則各音框在求取 DCC 係數之後，接著 就要作 PCA 係數轉換和 CDF 係數轉換的動作，然後在 LMR 對映之後，還要作 PCA 反 轉換和 CDF 反轉換的動作，以將頻譜特徵還原成 DCC 係數。因此，在這一節就說明 PCA 係數轉換和 CDF 係數轉換的細節。  2.1 PCA係數轉換 要能夠把一個來源音框的 DCC 係數轉換成 PCA 係數，則在訓練階段就要先對來源語者 各個音段類別所收集到的 DCC 向量作 PCA 分析，以求取來源語者各個音段類別的主成 分向量。相對地，要能夠把一個 LMR 對映後音框的 PCA 係數反轉換成 DCC 係數，則 在訓練階段也要先對目標語者各個音段類別所收集到的 DCC 向量作 PCA 分析，以求取 目標語者各個音段類別的主成分向量。然而關於 PCA 分析的作法，我們曾經思索的一個 疑問是，雖然直覺上我們會認為來源音框和目標音框應該要分開去收集，並且分開去作 PCA 分析以求取各自的主成分向量，但是，為什麼不能夠把同一音段類別的來源音框和 目標音框放在一起去作 PCA 分析？又為什麼不讓來源音框和目標音框共用一組主成分 向量呢？因此，我們將以實驗評估的方式來探討此一疑問。 PCA 分析是由 K. Pearson 於 1901 年提出，在 1933 年時再由 H. Hotelling 加以發展  102  古鴻炎、張家維  (Hotelling, 1933)。PCA 轉換是一種正交變換，它可以將原本維度間相關的原始數據轉換 成各維度獨立的新數據，再者作 PCA 轉換後的新數據，它們的總變異數(variance)與原始 數據集的總變異數相等，也就是說 PCA 轉換能保留原始數據的訊息。  2.1.1 主成分分析  對於某一音段類別的所有訓練語音作音框切割及求取 DCC 係數，以建立一個 40 維 DCC 係數的數據集，接著再對這個數據集作 PCA 分析以得到該種音段的主成分向量，詳細的 分析流程如下:  (a) 假設某一音段類別的訓練語音總共可切成 M 個音框，而每個音框經由計算可得到一個 DCC 係數的向量，然後把全部音框的 DCC 向量並列成各欄(column)的方式，表示成大 小為 L×M 的矩陣   [1, 2,, M ] ，其中 L 表示 DCC 係數的階數，M 的值大於 L。 (b) 接著求出這 M 個音框之 DCC 向量的平均向量  ，  代表著這 M 個音框共有的 DCC 向量成分。  (c) 將第 i 個音框的 DCC 向量作標準化，即減去平均向量  ，而得到一個差值向量 i 。  (d) 使用所有的差值向量 i ，來計算出一個共變異矩陣  。      M   iiT  (1)  i1  (e) 對矩陣  求其特徵值(eigen value) i 與特徵向量(eigen vector)  i 。    i  i  i , i  1, 2,, L  (2)  (f) 求得特徵向量  i 後，進一步對  i 作正規化，以取得 L 個主成分基底向量 i 。  i  ( i1)2  ( i2 )2   ( iL )2 , i  1, 2,L  i     i1  i  ,  i   2 i  ,   iL i  T   ,  i  1,2,, L  (3)  2.1.2 主成分係數轉換  當我們對某一個音段類別做完主成分分析後，就可得到該類別的 DCC 平均向量  、L  個主成分基底向量 i 。接著，要把各個音框的 DCC 係數轉換成 PCA 係數，首先把一個 音框的 DCC 向量 i 減去 DCC 平均向量  而得到差值向量 i ，再將 i 分別投影到各個 主成分基底向量  j ，投影公式為:  ij      T j  i,  j  1, 2,, L  (4)  如此就可得到 DCC 向量 i 的 L 個主成分係數(亦稱為 PCA 係數)，再用以形成 L 維度的 主成分係數(PCA 係數)之向量：  i  i1,i2,iL T  (5)  基於音段式LMR對映之語音轉換方法的改進  103  2.1.3 主成分係數反轉換  在圖 2 的處理流程中，“PCA 反轉換”方塊就是要將轉換後的 PCA 係數還原到 DCC 係 數的向量空間，以得到轉換後的 DCC 係數。假設我們取得一序列音框的 PCA 係數之向  量，則首先要知道各個音框分別所屬的音段類別，如此才能對各個音框分別去作還原，  令第 i 個音框所屬的音段類別之編號為 k，則我們就要取出訓練階段目標語者在第 k 類音  段所計算出的 DCC 平均向量 、及 L 個主成分基底向量  j，來把轉換後的 PCA 向量 i 還原成轉換後的 DCC 向量 i ，如公式(6)所示:  i          L j1    j   ij  (6)  2.2 直方圖等化 直方圖等化所指的是圖 2 流程裡“CDF 係數轉換” 與 “CDF 反轉換”兩方塊的處理。 要能夠把一個來源音框的 PCA 係數轉換成 CDF 係數，則在訓練階段就要先對來源語者 各個音段類別所收集到的 PCA 向量作 HEQ 分析，以建造來源語者各個音段類別的 HEQ 表格。相對地，要能夠把一個 LMR 對映後音框的 CDF 係數反轉換成 PCA 係數，則在訓 練階段也要先對目標語者各個音段類別所收集到的 PCA 向量作 HEQ 分析，以建造目標 語者各個音段類別的 HEQ 表格。這裡提到 HEQ 表格，意謂我們採取基本的表格法來建 立 PCA 係數和 CDF 係數之間的直方圖等化關係。  2.2.1 HEQ表格建造 選定一個來源(或目標)語者的音段類別，令該類別裡收集到的音框總數為 M，則將 M 個 維度為 L 的 PCA 係數向量作為輸入資料，依照下列步驟來建造 HEQ 表格: (a) 令區間數為 N，並且對各個維度 i，i=1, 2, …, L，分別作下列步驟的處理。 (b) 將 M 個音框中所有位於第 i 維度的 PCA 係數挑出，然後依係數值作由小到大之排序， 排序後則把 M 個 PCA 係數依順序且平均地分配到 N 個區間。 (c) 區間編號 j 從 1 變到 N，對於第 j 個區間內的 PCA 係數，挑選排序位於中間(median)  的 PCA 係數數值，然後記錄該 PCA 係數值為 Fpij，並且記錄其對應的 CDF 值為 Fcij ，  CDF 值就是該 PCA 係數在全體(M 個)係數排序中的順序值除以 M。 (d) 記錄第 i 維度 PCA 係數的最大值為 FpiN 1 ，且記錄其對應的 CDF 值為 FciN 1  1 ；此  外，記錄第  i 維度 PCA 係數的最小值為 Fpi0 ，且記錄其對應的 CDF 值為 Fci0    
1. 緒論 本論文是探討與發展降低各種外在環境存在之雜訊干擾所對應的強健性演算法。在近幾 十年來，無數的學者先進對於此雜訊干擾問題提出了豐富眾多的演算法，也都可對雜訊 環境下的語音辨識效能有所改進，我們把這些方法略分成兩大範疇： （1）強健性語音特徵參數（robust speech feature）求取 這類方法主要目的在抽取不易受到外在環境干擾下而失真的語音特徵參數，或從原始語 音特徵中儘量削減雜訊造成的效應，其中常應用的是探究語音訊號與雜訊干擾不同的特 性、藉由凸顯其差異將二者儘量分離，這類的方法可使用語音訊號的不同領域（domain） 上，分別發揮不同的效果，例如常見的時域、頻域、對數頻域與倒頻譜之時間序列域等。 比較知名的方法有：頻域上的頻譜消去法（spectral subtraction, SS）（Boll, 1979）、韋 納濾波器法（Wiener filtering, WF）（Plapous et al., 2006）、對數頻域上的對數頻譜平均 消去法（logarithmic spectral mean subtraction, LSMS）（Gelbart & Morgan, 2001）與基於 雙 聲 道 語 音 之 片 段 線 性 補 償 法 （ Stereo-based Piecewise Linear Compensation for Environments, SPLICE）（Deng et al., 2003），倒頻譜之時間序列域上的倒頻譜平均消去 法（cepstral mean subtraction, CMS）（Furui, 1981）、倒頻譜增益正規化法（cepstral gain normalization, CGN）（Yoshizawa et al., 2004）、倒頻譜平均值與變異數正規化法（cepstral mean and variance normalization, CMVN）（Tiberewala & Hermansky, 1997）、倒頻譜統 計圖正規化法（cepstral histogram normalization, CHN）（Hilger & Ney, 2006）、倒頻譜 形狀正規化法（cepstral shape normalization, CSN）（Du & Wang, 2008）、倒頻譜平均值 與 變 異 數 正 規 化 結 合 自 動 回 歸 動 態 平 均 濾 波 器 法 （ cepstral mean and variance normalization plus auto-regressive-moving average filtering, MVA）（Chen & Bilmes, 2007） 等，附帶一提的是，近幾年來本語音實驗室針對倒頻譜之時間序列域開發了許多此類的 強健型包括了：廣義對數域調變頻譜平均值正規化法（generalized-log magnitude spectrum mean normalization, GLMSMN）（Hsu et al., 2012）、調變頻譜指數權重法（modulation spectrum exponential weighting, MSEW）（Hung et al., 2012a）、調變頻譜替代法（modulation  雜訊環境下應用線性估測編碼於特徵時序列之強健性語音辨識  117  spectrum replacement, MSR）（Hung et al., 2012b）、調變頻譜濾波法（modulation spectrum filtering, MSF）（Hung et al., 2012b）、分頻帶調變頻譜補償（Sub-band modulation spectrum compensation）（Tu et al., 2009）等，這些方法跟前人所提的許多技術幾乎都可以有良好 的加成性、對於語音特徵更好的強健性加強效果。 （2）語音模型調適法（speech model adaptation） 此類的方法則是藉由少量的應用環境語料或雜訊，來對原始的語音模型中的統計參數作 調整，降低模型之訓練環境與應用環境之不匹配的情況，而它特點之一是在於無需對於 待辨識的語音或其特徵作消噪等強健的處理。較有名的語音模型調適技術包含了：最大 後機率法則調適法（maximum a posteriori adaptation, MAP）（Gauiain & Lee, 1994）、平 行模型合併法（parallel model combination, PMC）（Hung et al., 2001）、向量泰勒級數 轉換（vector Taylor series transform, VTS）（Moreno et al., 1996）與最大相似度線性回歸 法調適（maximum likelihood linear regression, MLLR）（Leggetter & Woodland, 1995）等。 本論文較集中討論與發展的是上述的第一類方法，簡單來說，我們將提出一套作用 於 倒 頻 譜 時 間 序 列 域 的 強 健 性 技 術 ， 稱 作 線 性 估 測 編 碼 濾 波 法 （ linear prediction coding-based filtering, LPCF），此方法主要是應用線性估測（linear prediction）的原理， 來擷取語音特徵隨著時間變化的特性、進而凸顯語音的成分、抑制雜訊的成分。實驗結 果將顯示，LPCF 此方法作用於原始倒頻譜特徵或經過許多上述之強健性技術預處理後 的倒頻譜特徵，都可帶來明顯進步的辨識率。 本論文其他之各章節之內容結構安排如下：第二節中，將簡介線性預估編碼（linear prediction coding, LPC）之處理的原理與應用發展，並藉其推演出所提出之基於 LPC 的 特徵強健技術。第三節包含了實驗環境設定，第四節則為辨識實驗結果與討論，主要內 容為包含所提新技術的各種強健性法在一系列雜訊環境下的語音辨識結果，以及相關討 論。最終之第五節則為結論與未來展望，其對本論文內容做一結論，並敘說未來可研究 之方向。 2. 基於線性預估編碼之特徵時間序列濾波技術 在本章節中我們將分成二個小節、分別介紹 LPC 之基本原理、及本篇論文所提出之基於 LPC 的語音特徵序列之線性濾波技術。 2.1 LPC的原理介紹 線性預估編碼（王小川，2004）技術普遍運用於訊號處理與資料分析的領域中，此方法 的基本背景假設，在於時間（或位置）相近的訊號點彼此存在著相關性，每一個訊號點 可以由相近的訊號點藉由線性組合（linear combination）加以逼近或估測，而線性組合中 各 相 鄰 訊 號 點 所 使 用 的 係 數 （ parameter ） 即 稱 為 線 性 估 測 係 數 （ linear prediction coefficients）。在語音訊號處理的應用上，由於語音訊號在短時距的範圍內有變化緩慢 的特性，相鄰訊號點之間的相關性很高，所以許多擷取語音特性的理論或技術，皆是根  118  范顥騰 等  據線性估測法則來加以發展推論，其應用範圍廣及語音訊號傳輸上的編碼、語音辨識與 語者辨識之特徵等。  在諸多的語音相關主題上，藉由 LPC 可將語音訊號中大致地分離成聲帶與聲道兩大 成分，可能是最為廣泛且為人知的應用，以下，我們就簡單地藉由數學推導方式，介紹 LPC 的分析及求取線性估測係數的步驟。  LPC 法將一段時域（time domain）上的訊號 x[n] 用以下數學式表示：  P  x[n]   ak xn  k   en ,  (1)  k 1  x[n] 為在特定時間 n 時的訊號值，其由位於時間軸上 n-1, n-2,..n-P 這串 P 點的訊號值以 線性組合（加權組合）來近似，每一點所使用的權重係數（稱做線性預估係數）分別為 a1, a2, …, aP, en 則為在近似過程中的誤差訊號，換言之，下式的訊號 xˆ[n] 為線性估測 的訊號：  P  xˆ[n]   ak xn  k   (2)  k 1  P 稱作線性估測的階數（order）。而式（1）與（2）中 x[n] 與 xˆ[n] 兩者之間的差量就是 線性估測的誤差，  en  xn  xˆn  (3)  由之前的陳述，可明顯得知，線性預估的過程是希望原始訊號與預估訊號之間的誤  差越小越好，而預估訊號與原始訊號逼近的程度，恰是由線性預估係數{ak}所決定，在 標準之線性估測理論中，{ak}是藉由最小化將誤差訊號 en 的最小均方值（mean squared value）所決定：  E    
Tonal identity and tonal variation in Mandarin have been the focus of intensive research that has long sought to bring out the underlying causes of variations in realized pitch values. Included among the variables studied in tonal variation are syntactic, contextual, emotional, and interactional influences. In the current study, we present results of our comparative research into tonal pitch variation in read speech and spontaneous Mandarin conversations. We acoustically and quantitatively characterize differences in the degree of pitch variability of these two modes of speech, and we present our results on tonal variability, as well as the influence of tone sequencing, syllable amplitude, and contextual factors on realized tonal shape. We show that, although tones are manifested in great diversity of pitch in spontaneous speech, there is a consistency of pitch shape that is dependent on tonal lexical identity. Keywords: Tone, Prosody, Mandarin, Tonal Variability, Spontaneous Speech 1. Introduction Previous research on read speech and spontaneous speech in Mandarin has demonstrated the wider variability of pitch movement in the latter, and researchers have attributed the greater variability to a number of different factors. Research on read speech (Xu, 1997; Shih, 1992; Shen, 1990) has predominantly focused on production and perception of tones in read or experimentally elicited speech. In these studies, prominent results have been the elucidation of rules for tone sandhi and the modification of tonal shapes under differing aspects, such as questioning, focus, or emphasis.  English Language Center, College of Arts, Tunghai University, Taichung Taiwan TEL: 011-886-04-2359-0121x31923 FAX:011-886-04-2359-0232 E-mail: yang_lc@thu.edu.tw The author for correspondence is Li-chiung Yang.  Spoken Language Research, USA E-mail: esposito_r@sprynet.com  22  Li-chiung Yang & Richard Esposito  Recent research has demonstrated the wide divergence between defined tonal pitch values that occur in spontaneous speech (Tseng, 2005) and values in read speech. The interactive and cognitively intense environment of spontaneous speech provides an abundance of differing factors that often lead to tonal sequences with tones that seemingly rarely reach their defined values. Researchers have suggested several avenues to explain systematic divergences from intrinsic tone shape, including tone sequence patterns, targeting of adjacent tones, and stress and metrical patterns of speech. Prior research on speech prosody in both tone and non-tonal languages (Hirst et al., 1998; Shriberg et al., 2000; Tseng, 2005; 2009; Tseng, 2010) has shown that there are a number of important influences on the prosody of natural speech, including interactive monitoring activity, emotional state, and the level of uncertainty, as well as topic organization and phrasal position. Our view is that experimental and spontaneous speech corpora are complementary and each has its important role to play in the discovery process. Experimental and read speech data are ideally suited to testing hypotheses on relationships among known variables while controlling for confounding effects of other variables. Study of spontaneous speech, on the other hand, is ideally suited to the discovery of new contributing variables and to the forming of new hypotheses. In addition, the study of spontaneous speech, as in other natural science fields, has the potential to yield valid information on underlying processes that are uncovered through the identification of systematic parallels observed across the available data. This discovery of relevant variables is especially important when a particular phenomenon could arise from more than one underlying cause or from an alternate cause. By combining the results of read or experimental studies with those of spontaneous speech, the relative robustness of experimental results in a spontaneous setting can be used as a marker or clue to the existence of other potentially important variables. With this view in focus, in the current paper, we study how tonal sequences and amplitude affect the realization of lexical tone shape and compare results obtained from a read speech corpus to results found in spontaneous Mandarin. We further investigate differences between the speech modes, and study the roles of tone sequence, speaker variability, and lexical identity in shaping realized tone values. Section 2 describes our methodology, including the nature and extent of the data corpora, and the automatic and post-processing steps taken to extract acoustic parameters from the speech signal. In Section 3, we introduce an innovative technique to facilitate consistent comparison of measures of Chinese tonal f0 contour in large speech corpora. We describe the importance of spontaneous speech to realized tonal variability and also present a benchmark comparison of  Understanding Mandarin Prosody:  23  Tonal and Contextual Variations in Spontaneous Conversation  spontaneous to read f0 shape. We explore several important factors in the determination of f0 shape in spontaneous speech, including tonal identity, word syllable number and position, and tonal sequence patterns, including tone sandhi. We present results of our comparisons grouped by lexical tone, by tonal sequence patterns, and by mono- or di-syllabic words, and we indicate the degree to which our results match prior theories on anticipatory and carryover effects of tone sequences. Section 3.3 introduces the data reduction techniques used to extract comparable measures of f0 shape and provides graphical representations of the distributions of these measures as important guides to the behavior of syllable f0 in natural spontaneous conversations. In Section 3.4, we show individual instances of tonal variation in spontaneous speech and suggest mechanisms for the specific variational tendencies revealed by the data. Section 4 provides a summarization of the key findings of the paper, the importance of tone variability in spontaneous Mandarin, and how future work on tonal variability can be enhanced through the techniques introduced. 2. Data and Methodology 2.1 Data, Participants, and Approach The data utilized in this study are part of a larger project on Mandarin conversational speech, totaling over 20 hours of speech. For this study, a subset of continuous speech from two conversations, one between two female speakers (Speaker P and Speaker S) and the other between one female (Speaker T) and one male speaker (Speaker B), totaling approximately 40 minutes in duration for spontaneous speech were selected and analyzed. In addition, to provide a baseline comparison between the read and spontaneous speech modes of the same speaker, 10 minutes of read speech, consisting of four read stories, by the same male speaker in the spontaneous conversation, were also analyzed. As previous research on Mandarin has concentrated almost exclusively on read or experimentally controlled speech (with the exception of Tseng, 2004, 2005, 2009), in this study, our goal is to concentrate on exploring tonal and prosodic variations in spontaneous Mandarin Chinese. The spontaneous conversation data were collected in informal settings, and the read speech corpus was collected in a laboratory. Speech data were recorded using a SONY PCM-M1 DAT recorder with a SONY ECM lavalier microphone at a sampling rate of 22,050 kHz. Data were manually segmented to the phrase, word, and syllable levels using Wavesurfer and ESPS/xwaves for their ease with extended speech data processing capability, and acoustic-prosodic features such as time, amplitude, and pitch (f0) values were automatically extracted from the speech files using ESPS/xwaves function get_f0. F0 values were further corrected for formant outliers (e.g. doubling and halving), and slight errors in segmentation among the different label files were adjusted automatically.  24  Li-chiung Yang & Richard Esposito  A total of 7,710 lexical syllables from the 40 continuous minutes of segmented spontaneous speech were obtained, after eliminating overlapping syllables for which f0 values were ambiguous between speakers. For read speech, there was one speaker, and a total of 1,804 syllables of speech. Tables 1-3 show the breakdown by speech mode, corpus, speaker, and tone. Altogether, the corpora investigated contained a total of 9,514 syllables over 50 minutes of speech.  In this study, we investigate the f0 contours of tones as they are realized in spontaneous speech by utilizing several measures of f0 contour to facilitate data reduction and comparisons across a large corpus of syllables. Our approach is to examine the data first, without any preconceived assumptions about specific tonal variations, show the patterns that emerge from this large corpus, and then relate our results to previous claims and findings.  The results between spontaneous and read speech are based on spontaneous and read speech corpora from one male speaker. While the sample size of the read speech is small, use of the same speaker highlights differences in syllable f0 contour while controlling for speaker variability. Focusing on finding metrics to evaluate the variations in f0 contour in spontaneous speech with respect to defined tonal shape, we present overall results on the consistency of tonal change across speakers in the wider spontaneous corpora.  Table 1. Number of syllables by tone, spontaneous conversation MC1, 2 female speakers, Speaker P and Speaker S.  Tone 1 Tone 2 Tone 3 Tone 4 Tone 0 Total  P  439 338 455 902 357 2491  S  270 222 297 579 239 1607  Total 709 560 752 1481 596 4098  Table 2. Number of syllables by tone, spontaneous conversation MC2, 1 male speaker, Speaker B and 1 female speaker, Speaker T.  Tone 1 Tone 2 Tone 3 Tone 4 Tone 0 Total  B  404  353  485  807  340 2389  T  151  182  261  449  180 1223  Total 555 535 746 1256 520 3612  Table 3. Number of syllables by tone, read speech, RS1, male speaker, Speaker B.  Tone 1 Tone 2 Tone 3 Tone 4 Tone 0 Total  B  356  300  318  502  328 1804  Understanding Mandarin Prosody:  25  Tonal and Contextual Variations in Spontaneous Conversation  3. Results: Variability of Tonal Shapes The defined lexical tones in Mandarin are commonly recognized as Tone 1, which has a high pitch level and is flat (55); Tone 2, which starts at a mid to low pitch level, and rises (35); Tone 3, which starts at a mid to high level, falls, then rises (214); Tone 4, which is high and falling (51); and a neutral tone, Tone 0 (Chao, 1968). Tone 3 has two recognized variants in spontaneous speech: a high to low fall with no final rise and a short duration, low pitch value (21). Tone sequence pitch values are also governed by generally accepted tone sandhi rules (Chao, 1968): a Tone 3 that is followed by a Tone 3 takes on a rising pitch (33-->23); Tone 3 when followed by a non-3rd tone takes on a half-Tone 3, without an ending rise; and a Tone 4 that is followed by Tone 4 takes on a less steep fall. As a simple first measure of relative concordance of syllable shapes with the defined pitch movement for tones, we used the linear slope of f0 as a simple shape indicator to approximate f0 values. Syllable f0 values were extracted automatically and corrected as described in Section 2. The resulting f0 values of each syllable were fitted using S-plus, and linear slopes and intercepts of each syllable were calculated. A linear slope would be most applicable for Tones 1 and 4, and, to a large degree, rising Tone 2. A quadratic fit would better capture the curvature of all tones, especially Tones 2 and 3, and was also produced for each syllable and used in measuring the effects of amplitude.  3.1 Tonal Values in Read and Spontaneous speech Read, controlled, or experimental speech data are frequently considered as benchmarks that preserve a number of relatively stable phonological relationships in their realization. Table 4 shows the overall measure of tonal shape for our read speech data, using averages of linear approximations to each syllable’s f0 data, restricted to monosyllables occurring in the pre-pause or at phrase end position, to avoid the influence of adjacent tonal values. This is likely to have some downward bias due to pitch declination at phrase end, and this may cause the slight negative slope of level Tone 1. As expected, Tone 4 has a large falling slope, while Tones 2 and 3 have an overall positive rise. The f0 minimum values shown in Table 4 indicate the percentage point within the syllable that attains the syllable minimum, and they provide additional information on syllable shape, as they mark the location of syllable f0 slope direction change. The f0 minimum point is very informative on the shape as well and is more robust with respect to averaging over different speakers and different speech situations, as this point is defined in percentage terms. In read speech, Tones 1 and 4 reach their minimum pitch point relatively close to the end, while Tones 2 and 3 reach their minimum pitch point very close to the midpoint of the syllable, matching the defined fall-rise shape of Tone 3, and rising Tone 2 with an initial fall.  26  Li-chiung Yang & Richard Esposito  Table 4. Mean slope in Hz change per second and percentage point of syllable f0 minimum of each token, pre-pausal monosyllables, read speech, Speaker B.  Tone 1 Tone 2 Tone 3 Tone 4 Tone 0  Slope f0 min  -39.73 0.64  36.42 0.48  58.98 0.48  -212.94 -75.49 0.72 0.55  Table 5 shows the parallel results for this same speaker as he engaged in spontaneous conversation. It is immediately clear that tone values in spontaneous speech diverge widely from their lexically defined values and from their realized values in read speech. In particular, Tone 1 exhibits a greater average fall than expected and Tone 4 displays an average fall that is just barely greater than defined level Tone 1. Notably, lexically rising Tone 2, which on average rises in read speech, also has an overall negative slope in spontaneous speech. Of the four tones, only Tone 3 has an overall rise that is similar to its read speech counterpart. Both Tables 4 and 5 are restricted to monosyllables in pre-pausal position, so the average pitch slopes for Tone 3 are independent of tone sandhi rules.  The f0 minimum points for Tones 2, 3, and 4 occur earlier than in read speech and occur at nearly the same percentage position for Tone 1. For this speaker, Tones 1 and 2 in spontaneous speech became more negatively sloped, while Tone 3 remained similar to read speech. The most striking change occurs with Tone 4, which falls much less in spontaneous speech than in read speech and has about the same average slope as spontaneous Tone 1. Neutral tone (Tone 0) for this speaker becomes significantly more neutral, that is, flatter, in spontaneous speech, with a slope near zero, indicating very little pitch change.  Table 5. Mean slope in Hz change per second and percentage point of syllable f0 minimum of each tone, pre-pausal monosyllables, spontaneous speech, Speaker B.  Tone 1 Tone 2 Tone 3 Tone 4 Tone 0  Slope -62.31 -11.86 54.54 -64.97 3.91  f0 min  0.64  0.41  0.34  0.54 0.38  The overall tone slope results, including mono-, di-, and tri-syllables, for the two participants in the current spontaneous corpus are presented in Table 6.  Table 6 indicates that the slope directions agree for each of the four tones across the two speakers, although the strength of directional changes varies, and the tones vary substantially from their defined lexical values. For both speakers, there is a striking similarity in average pitch slope for Tones 1 and 4, but there is also a consistent difference in degree between the speakers. For Speaker B, Tones 1 and 4 fall by about the same amount. For Speaker T, Tone 1 actually falls more than defined falling Tone 4, on average. Relative to each speaker’s pattern for all tones, Table 6 shows that Speaker T’s Tone 2 syllables fall relatively more,  Understanding Mandarin Prosody:  27  Tonal and Contextual Variations in Spontaneous Conversation  highlighting the importance of speaker variation in the realized tone shapes of spontaneous speech.  Table 7 further breaks out the results for the two speakers in spontaneous conversation MC2 by whether the syllable is a monosyllabic word (mono), the 1st syllable in a disyllabic word (D1), or the 2nd syllable in a disyllabic word.  Table 6. Mean slope in Hz change per second over all syllables, spontaneous speech, Speaker B and Speaker T.  Tone 1 Tone 2 Tone 3 Tone 4 Tone 0  B  -79.84 -15.72 21.47 -77.64 -104.38  T -188.39 -104.14 7.87 -138.69  1.16  Table 7. Mean slope in Hz change per second over monosyllables, 1st and 2nd syllables of disyllabic words, spontaneous speech, Speaker B and Speaker T.  Speaker Tone 1 Tone 2 Tone 3 Tone 4 Tone 0  Mono-B -62.31 -11.86  54.54 -64.97  3.91  Mono-T -192.31 -124.76  14.43 -150.49 -127.10  D1-B  -90.49 -30.22 -23.42 -105.59  -  D1-T  -189.30 -79.72 -10.39 -227.56  -  D2-B  -52.57  7.24 -16.64 -69.99 -17.70  D2-T  -118.26 -128.95  1.86 -45.33 -72.46  The most striking pattern seen in Table 7 is the evident pervasive strength of a falling pitch, with almost all values showing an average negative slope. Table 7 clearly indicates that this pattern is similar for both speakers. As the two participants differ in overall pitch level, in Table 8 we show the same data normalized to Z-score values with respect to each speaker’s overall average pitch mean and standard deviation, calculated across all f0 values, by speaker. Table 8. Mean slope in Hz change per second over monosyllables, 1st and 2nd syllables of disyllabic words, normalized to each speaker’s average syllable pitch range, spontaneous speech, Speaker B and Speaker T.  Speaker Tone 1 Tone 2 Tone 3 Tone 4 Neutral  Mono-B -2.13  -0.41  1.87  -2.22  0.13  Mono-T -4.36  -2.83  0.33  -3.41  -2.88  D1-B  -3.09  -1.03  -0.80  -3.61  -  D1-T  -4.29  -1.81  -0.24  -5.16  -  D2-B  -1.80  0.25  -0.57  -2.39  -0.61  D2-T  -2.68  -2.92  0.04  -1.03  -2.68  28  Li-chiung Yang & Richard Esposito  From Table 8, calculated from the spontaneous speech of corpus MC2, we can see that Tones 4 and 1 have the largest negative slope, but the mean values for Tone 4 do not have a consistently greater negative slope than Tone 1 for both speakers. For Speaker B, Tone 4 is marginally more falling than Tone 1, but for Speaker T, Tone 1 falls marginally more than Tone 4 for monosyllables and for the 2nd syllable of a disyllabic word. The normalized data of Table 8 indicate that Speaker T has a greater propensity for a falling pitch over all tones except Tone 3, and for Tone 4 when it is the 2nd syllable of a disyllabic word; such differences in tonal modification may form a component of a speaker’s characteristic or general speech style. The data from Tables 7 and 8 show that, except for Tone 4, the slope of syllables in spontaneous speech exhibit a divergence from the lexically defined shape, on average. As these results are consistent across syllable types, Table 8 further suggests that the results of divergence from lexically defined tonal values may not be due solely to a pattern that affects only monosyllables or that is effected through syllable position in the word, and may arise from other causes as well. 3.2 Tonal Sequencing and the Influence of Preceding and Following Lexical Tone Researchers on Mandarin tonal pitch values have proposed that local tone sequences can theoretically affect the realized target in two primary ways: through anticipatory effects of the upcoming syllable or through carryover effects of the preceding syllable. A succeeding tone value with a high f0 onset should lead to a higher offset for the previous syllable, while a succeeding syllable with a low onset should induce a low offset in the previous syllable, according to the anticipatory theory. Analogously, carryover theory predicts that a high f0 offset in a preceding syllable should lead to a higher onset for the succeeding syllable, while a preceding syllable with a low offset (half-Tone 3 and Tone 4) should induce a low onset in the succeeding syllable. For example, Xu (1997) found greater evidence for the strength of carryover effects than for anticipatory effects using balanced sequences in experimental speech for Mandarin, while Chang & Hsieh (2012) found a more balanced effect of carryover and anticipatory effects for the more complex tonal system of Eng Choon Hokkien in their experimental data. In the current study, we were interested in investigating if similar effects on the realized target tones hold in our spontaneous speech data. In Tables 9 through 12, we compare the average slope of each lexical syllable of two speakers from spontaneous conversation MC1, grouped by immediately preceding and succeeding syllable tone, using the linear regression slopes for the two speakers. The resulting slope coefficients were grouped by the subsequent lexical tone in Tables 9 and 10, and by the  Understanding Mandarin Prosody:  29  Tonal and Contextual Variations in Spontaneous Conversation  preceding tone for Tables 11 and 12.  Table 9. Mean slope in Hz change per second of each tone, by tone of the following syllable, Speaker P.  Slope of X-Tone 1 X-Tone 2 X-Tone3 X-Tone 4 X-Tone 0  Tone 1  -78.4  -76.2  -125.3  -35.6  -119.5  Tone 2  -66.1  24.7  -115.8  -16.0  -53.2  Tone 3 -185.4  -76.7  -0.4  -149.5  -97.8  Tone 4 -133.6  -254.7  -248.8  -187.1  -202.9  Table 10. Mean slope in Hz change per second of each tone, by tone of the following syllable, Speaker S.  Slope of X-Tone 1 X-Tone 2 X-Tone3 X-Tone 4 X-Tone 0  Tone 1  71.1  -114.3  -51.3  -53.4  -81.3  Tone 2  -23.7  -48.1  -58.7  52.6  -83.9  Tone 3 -211.1  -145.4  -87.8  -98.0  -205.5  Tone 4  -35.6  -233.8  -177.5  -139.9  -119.9  Table 11. Mean slope in Hz change per second of each tone, by tone of the preceding syllable, Speaker P.  Slope of Tone 1-X Tone 2-X Tone 3-X Tone 4-X X-Tone 0  Tone 1  -56.4  -35.2  -67.4  -75.2  -102.6  Tone 2 -160.0  -130.0  30.3  -18.5  17.0  Tone 3 -211.3  -163.6  -118.5  -106.2  -26.7  Tone 4 -225.5  -249.7  -122.9  -201.4  -180.1  Table 12. Mean slope in Hz change per second of each tone, by tone of the preceding syllable, Speaker S.  Slope of Tone 1-X Tone 2-X Tone 3-X Tone 4-X X-Tone 0  Tone 1 -116.5  -35.3  57.4  -104.3  -32.1  Tone 2 -124.7  -61.5  34.6  -12.3  -1.7  Tone 3 -332.8  -87.3  -149.9  -163.8  -114.4  Tone 4 -212.2 -127.7  -69.4  -151.9  -27.5  According to the anticipatory theory, high succeeding onset Tones 1 and 4 should show a relatively positive slope on the previous syllable, and conversely for low and low onset tones 2 and 3. In forward assimilation not based on the succeeding onset, a succeeding rising tone 2  30  Li-chiung Yang & Richard Esposito  can cause a rising effect. We can see from Tables 9-12 that there is considerable variability in the slopes of each tone depending on the tonal sequence; we can also see that this is in part speaker dependent. The data from Tables 9-12 suggest that different tone combinations merge contextually in different ways. For example, for both speakers, Tables 9-10 show that the anticipatory condition is true for Tone 4 followed by all tones, that is, the following tone influences the pitch slope of the previous syllable in the hypothesized direction, on average. For both speakers, succeeding Tones 3 and 4 induce a deeper fall for Tone 4 than succeeding Tones 1 and 4. A succeeding neutral tone induces a fall for Tone 4 roughly between these two cases. Similarly, high onset succeeding Tone 4 induces a relatively flatter (but still falling) pitch for all Tones except initial Tone 2 for Speaker S in Table 10. Initial Tone 2 achieves an overall average rising pitch only when it is followed by Tone 2 for Speaker P, contrary to the anticipation account, and by Tone 4 for Speaker S, in agreement with the anticipation effect. Initial Tone 2 falls the most for both speakers when it is followed by Tones 2 and 3, again consistent with the anticipation effect. A succeeding Tone 1 appears as the most problematic: for both speakers, an initial Tone 3 achieves its greatest falling pitch when followed by Tone 1. Based on our data, we suggest that the defined level nature of Tone 1 may introduce a discrete jump rather than a transformation to a gradient-sloped succeeding tone. The tone sequence values seen in these tables also show some evidence of tone sandhi effects on pitch shapes. Tables 9-12 show some evidence for the 3-3 tone sandhi rule, since, for both speakers, the first of two 3rd Tones falls the least of all 3rd Tones, especially for Speaker P. When followed by another tone, Tone 3 exhibits a strong falling contour, as predicted by tone sandhi. Similarly, comparing the 4-4 tonal sequences for both speakers in Tables 9-12 shows that the 1st of two 4th Tones will have a slightly flatter pitch slope than the 2nd. Tone sandhi rules can override assimilation, as seen when Tone 3 is followed by Tone 1, where Tone 3 remains low, rather than assimilating to high Tone 1. However, in our data, succeeding Tone 4 has a relative positive effect on Tone 3 for both speakers. When looking at the influence of the preceding tone in Tables 11 and 12, we see that a preceding Tone 1 has a negative effect on the subsequent tonal slope. When preceded by Tones 2 or 3, the tonal slope becomes relatively higher for Tone 1 and Tone 2 for both speakers, and these results are in accordance with the carryover predictions. Also, preceding Tone 1 has a negative effect on all tones for Speaker S, and for all tones except Tone 1 for Speaker P. For both speakers, a succeeding Tone 4 has the greatest fall in pitch when it is preceded by high ending Tones 1 and 2, consistent with carryover. Both anticipatory and lag effects are found in the above tables. Because of the greater consistency of anticipatory results, there may be a marginally greater influence of anticipatory effects than carryover effects, and the overall pattern provides support for Chang and Hsieh’s  Understanding Mandarin Prosody:  31  Tonal and Contextual Variations in Spontaneous Conversation  findings of more balanced effects from both anticipatory and carryover. The differences that are evident between the speakers suggest that the influence of sequencing in speech on tonal targets may be conditioned by a number of speaker factors, such as speech rate and speaking style. The results obtained confirm the results of prior researchers, who have found substantial effects of preceding and subsequent tones on the realized tonal pitch values in read and experimental speech (Xu, 1997; Chang & Hsieh, 2012). 3.3 Variability of Tonal Shape and Amplitude As seen in the above results, average results to a certain degree can be accounted for by tonal sequencing and by consistency of speaker style in producing the defined lexical tones. Nevertheless, when we look at individual tokens, our data indicate that it is much more difficult to account for the wide range of pitch shapes found in spontaneous speech for a given tone only through the factors presented in the tables above. A wide range of phonological and linguistic phenomena has been cited as also affecting the pitch values of syllables. Among the factors affecting pitch are the position in the phrase, phonemic identity, emotional and interactive effects, communicative function, and prosodic environment. 3.3.1 Comparative Measures of Tonal Shape Figures 1 through 8 present a comparison of read speech to spontaneous speech, based on the simple linear slope measure of each syllable in the read speech corpus and the corresponding spontaneous speech counterpart, in which the fitted linear syllable slope is plotted against syllable average amplitude. A further regression line of slope vs. amplitude is superimposed on each figure. Figures 1-4 show the slopes for spontaneous speech, and Figures 5-8 show the slopes for read speech. Comparison of spontaneous to read speech syllable slopes by tone shows that spontaneous speech is dispersed much more widely around the linear regression line. Furthermore, for each tone, for spontaneous speech the variation of pitch slope extends widely into both negative and positive slope regions, showing that the basic slope direction for spontaneous speech occurs with great frequency as either rising or falling. By contrast, the slope value points for read speech in Figures 5-8 are clustered more tightly around the regression line, and their slope values are more in accordance with their defined lexical pitch values. In particular, for read speech, the slope values for Tone 1 cluster around zero, consistent with Tone 1’s level pitch definition, and virtually all Tone 4 slopes are falling for read speech. Tone 3 has a preponderance of falling slopes, while Tone 2 has a somewhat greater preponderance of rising pitch values for read speech. The much greater dispersion of spontaneous speech tone pitch values into both negative and positive slope regions indicates the presence and greater influence of contextual variables in determining  32  Li-chiung Yang & Richard Esposito  pitch shapes in spontaneous speech. These figures indicate that tonal identity is still an important factor in the realized pitch shapes of syllables because of the consistencies with respect to tones that exist in the figures for both read and spontaneous speech. For example, comparing across tones for spontaneous speech shows that, like read speech, Tone 1 syllables tend to cluster relatively symmetrically around a zero level pitch slope value, in accordance with the defined flat slope of Tone 1, although, for both read and spontaneous, the overall slope average was slightly below zero. Spontaneous Tone 2 syllables generally do not have the steeper falling slopes associated with Tones 3 and Tones 4. Spontaneous Tone 4 syllables seem to be the most consistent in their adherence to falling pitch, with the great majority of syllables still having a negative slope, although not as consistently as with read speech. The overall tendency for a greater falling pitch for syllables than is predicted by the lexical identity may be the reason that Tone 4 exhibits the greatest resilience and adherence to its defined falling pitch value, as it combines both of these effects in its pitch realization. Pair-wise non-parametric Wilcoxon rank sum tests comparing read and spontaneous speech by tone indicate the existence of systematic contour differences between spontaneous and read speech at high significance levels (See Appendix A). Results shown in Figures 1-8 indicate that amplitude also correlates with changes in tonal shape. A steeper falling slope for high amplitude syllables is seen in the figures for spontaneous and read speech Tones 3 and 4, and, for Tones 1 and 2, greater amplitude is moderately associated with flatter or rising slope values. Prior research using experimental speech has found that the distribution of energy in syllables is correlated with tone identity in Mandarin tones (Whalen et al., 1992). The current finding measures across syllables and finds that in both spontaneous and read speech, higher average amplitude of a syllable is associated with a greater degree of adherence to the defined lexical tone shape for the syllable as a whole.  Understanding Mandarin Prosody:  33  Tonal and Contextual Variations in Spontaneous Conversation  Spontaneous speech  Read speech  2000  2000  1000  1000  0  Syllable F0 Slope,Tone 1  0  Syllable F0 Slope, Tone 1  -1000  -1000  -2000  -2000  Syllable F0 Slope, Tone 2  Syllable F0 Slope, Tone 3  0  2000  4000  6000  8000  Average amplitude of syllable  0  500  1000  1500  2000  Amplitude of syllable  2000  2000  1000  1000  0  Syllable F0 Slope,Tone 2  0  -1000  -1000  -2000  -2000  2000  1000  0  0  2000  4000  6000  8000  0  Average amplitude of syllable  500  1000  Amplitude of syllable  1500  2000  1000  0  Syllable F0 Slope,Tone 3  -1000  -2000  0  2000  4000  6000  8000  0  Average amplitude of syllable  500  1000  Amplitude of syllable  1500  -1000  -2000  2000  2000  1000  1000  0  Syllable F0 Slope,Tone 4  0  -1000  -1000  -2000  -2000  0  2000  4000  6000  8000  0  Average amplitude of syllable  500  1000  Amplitude of syllable  1500  Figures 1-8. Slope of f0 by amplitude for Tones 1-4, spontaneous speech shown in the left column and read speech in the right column. x-axis= syllable amplitude level; y-axis=syllable f0 slope in Hz change per second  Syllable F0 Slope, Tone 4  34  Li-chiung Yang & Richard Esposito  To achieve a more precise representation of the different tonal shapes, we fitted each syllable’s normalized f0 values to a quadratic polynomial. For all syllables in the spontaneous corpora used in this study, all f0 values output were normalized by calculating Z-score values with respect to each speaker’s mean syllable f0 and standard deviation over all f0 values for that speaker. After manual segmentation to the syllable level for all speech data, an average amplitude was calculated as the mean of all non-zero amplitude values. For each syllable, a quadratic fit of the f0 normalized values also was calculated. Within each of the 4 tones’ sub-groups, the average syllable amplitudes were categorized as low, medium, or high, such that each amplitude category had equal numbers of syllables within a tone sub-group, that is, each amplitude grouping contained 1/3 of the syllables within that tone. An average duration was also calculated over all syllables within each subgroup, resulting in 12 average syllable duration values. For each quadratic contour, the time dimension then was compressed or extended linearly to the average duration within its tone and amplitude subgroup, in order to avoid averaging across inconsistent segments of each syllable. Within each of the 12 tone and amplitude sub-groups, a model quadratic contour then was calculated as the average of the f0 values over all quadratic contours within that subgroup, which, because of the normalization to average duration, is accomplished by averaging over the quadratic coefficients. Quadratic contours provide better overall model representations of average syllable shape than simple linear slopes, especially for Tones 2 and 3, because of the varying curvatures of the lexically defined f0 values of the different tones. The resulting quadratics are plotted in Figures 9-12. The model pitch slope shapes for spontaneous speech depicted in these figures corroborate the above conclusion that higher amplitude is associated with greater conformity to the tonal target: Tone 1 becomes higher and flatter and Tones 3 and 4 fall more with higher amplitude. Only Tone 2 breaks this pattern: higher amplitude Tone 2 becomes higher in average pitch, but exhibits a relatively greater fall in slope. The average durations are similar across amplitude groups for each tone, except for a slightly shorter average duration for low and mid amplitude syllables for Tone 3 tokens, so that the amplitude effect is not due to greater duration tokens having a fuller manifestation of the lexical shape. The model shapes seen in Figures 9-12 may provide a partial explanation of the similarity of slope between different tones, as seen previously with Tone 1 and Tone 4 in Table 5. The low and mid amplitude model shapes are more similar than dissimilar across tones, and this will cause greater similarity in the overall averages.  Understanding Mandarin Prosody:  35  Tonal and Contextual Variations in Spontaneous Conversation  F0  300 250 200 150 100 50 0 0  Spontaneous : T one 1  0.05 High Amp  0.1  Mid Amp  Low Amp  0.15  F0  300 250 200 150 100 50 0 0  Spontaneous : T one 2  0.05 High Amp  0.1  Mid Amp  Low Amp  0.15  F0  300 250 200 150 100 50 0 0  Spontaneous : T one 3  0.05 High Amp  0.1  Mid Amp  Low Amp  300  250  200  F0  150  100  50  0  0.15  0  Spontaneous : T one 4  0.05 High Amp  0.1  Mid Amp  Low Amp  0.15  Figures 9-12. Slope of f0 for Tones 1-4 by amplitude, spontaneous speech, in upper left, right, lower left, right order. x-axis= normalized syllable duration; y-axis= f0 in Hz The greater overall conformity to defined shape for read shape may arise because of the relative lack of other contextual influences. However, the greater conformity for greater amplitudes f0 for both read and spontaneous speech must arise from a different cause, as it also occurs in the highly contextualized environment of interactive spontaneous conversation. Amplitude frequently is associated with emphasis, and emphasis arises naturally in both read and spontaneous speech. The above results suggest that one way to give emphasis to a lexical item in a tonal language, such as Mandarin, is to provide a more prominent and distinct lexical tone shape that marks the lexical meaning as more salient.  3.4 Visualizing Tonal Variation in Read and Spontaneous Speech When viewed without respect to amplitude level, Figures 1-8 also give us a clearer picture of the differences between read and spontaneous speech. A comparison of the read to  36  Li-chiung Yang & Richard Esposito  spontaneous speech in these figures shows that, although the pitch level varies more in spontaneous speech, the level nature of Tone 1 is similar in both read and spontaneous speech. To analyze the differences between read and spontaneous in greater detail, we compared spontaneous vs. read speech for a number of identical tokens that were among the most frequently used syllables in both the read and the spontaneous corpora. This comparison suggested that a difference in speech mode does not affect all tones equally. The contours from read speech for identical Tone 1 tokens substantiated the result seen in Figures 1 and 5 above that Tone 1 remains essentially flat in both read and spontaneous speech. For Tones 2-4, however, there were more noticeable differences between the speech modes, as seen in the following Tone 2 hai ‘still’ example.  Frequency Frequency  250  300  250 200 200 150 150 100 100  50  50  0 1  11  21  31  Time  0 1  11  21  31  Time  Figures 13-14. f0 contours of 2ndTone ‘hai’ in read (left) and spontaneous speech (right)  3.4.1 Data Example 1: Tone 2 hai The f0 contours for Tone 2 hai ‘still’ of Figure 13 for read speech show clearly the defined rising nature of Tone 2, while, in the spontaneous speech shown in Figure 14, the pitch contours, for the most part, are much flatter, after an initial drop. Similar results were found for Tone 2 with other tokens. This flattening effect for spontaneous Tone 2 can also be seen from Figures 2 and 6 above, with a greater proportion of read speech slope measures in the rising range above zero than spontaneous Tone 2. A similar result occurs for Tones 3 and 4: the read speech slopes depicted in Figures 7 and 8 reflect an overall falling slope for almost all Tone 3 and Tone 4 read syllables, while Figures 3 and 4 show that spontaneous Tones 3 and 4 have greater numbers of syllables that fall less and frequently are rising. These results (also see the results in Appendix A) suggest that, while spontaneous speech has more variability in pitch height and pitch contour, it also has a greater flattening effect on f0 adherence to the defined tonal contours. Tone 1 remains overall flatter in spontaneous speech than in read speech. Spontaneous Tone 3 is similar to read speech, while Tone 2 is  Understanding Mandarin Prosody:  37  Tonal and Contextual Variations in Spontaneous Conversation  distinctly flatter in spontaneous speech than in read speech. Spontaneous Tone 3 is less likely to have an ending rise, and Tone 4 falls less than in read speech. Our analysis suggests that it is the greater multi-functional usage of f0 variation in spontaneous speech that leads to these results. In conversation, there is a greater use of pitch to indicate topic, provide signals of emotional and cognitive state, and to signal interactive intentions, and that the ‘flattening’ or reduced adherence to the lexically defined shape may be a more efficient use of lexical pitch so that a greater proportion of f0 variability for the discourse functions is adopted. 3.4.2 Data Example 2: Tone 3 you In Figures 15 and 16, we show individual instances of you3 ‘have/has’ or ‘is’ for the male speaker as read speech in Figure 15 and in spontaneous conversation in Figure 16. You in Mandarin not only has a very high frequency of use, but frequently occurs in environments where cognitive or emotional intensity is high, such as in both questioning and in answers to questions, when uncertainty of information is present. On the other hand, you also occurs as a unremarkable syntactic object within a stream of more relevant lexical items or simple statements of facts. From Figures 15 and 16, Tone 3 you achieves its lexical full fall-rise shape or half-tone falling shape very rarely for this speaker, even in read speech. Since the read speech is a story in this case, the rise-fall pattern that is most predominant in Figure 15 reflects the communicative functions of emphasis added to the underlying lexical token. In spontaneous speech, you is reduced in pitch range in most cases to a flattened pitch contour, with a small number of exceptions that rise and fall as in read speech. In neither the read nor the spontaneous case do the pitch shapes approximate the lexical shape. In spontaneous speech, you has a very high frequency of use in interactive exchange, comparable to ‘has/have’ and ‘is’ in English, and its frequency of use in matter-of-fact statements de-emphasizes the need for a prominent signal for lexical comprehension; thus flatter pitch shape would not hinder comprehension and de-emphasis may aid in placing focus on the informative lexical target. Therefore, high frequency tokens, such as you, may rarely hit their defined shape: on one hand, de-emphasis flattens the contour, while, when communicating a cognitively or emotionally high intensity content, the commonplace nature of this token may be what allows it to take on primarily communicative prosody. Spontaneous Tone 3 is similar to read speech Tone 3, while Tone 2 is distinctly flatter in spontaneous speech than in read speech. Spontaneous Tone 3 is less likely to have an ending rise, and Tone 4 falls less than in read speech. From research on experimental data, it has been proposed that flattening effects in speech are correlated with speech rate, and that shorter duration syllables should have greater flattening. The longer duration tokens for hai in Figure 13 that approximate continuous ending rise suggest that this may be a partial factor in  38  Li-chiung Yang & Richard Esposito  flattening. However, in the corresponding spontaneous contours of Figure 14, there does not appear to be a correlation between short duration and flatness of slope. The flatter average slopes found for mid and low amplitude tones in the quadratic approximations in Figures 9-12 also do not appear to reflect the influence of duration, as, for all tones except Tone 3, all amplitude classes had approximately the same average duration. Our preliminary analysis suggests that it may be the greater multi-functional usage of f0 variation in spontaneous speech instead that leads to these results. In spontaneous speech, there is a greater use of pitch to indicate topic, provide signals of emotional and cognitive state, and to signal interactive intentions than in read speech, as exemplified in the emotional emphasis signaled in the high arching contours in Figure 16. The general ‘flattening’ or reduced adherence to the lexically defined shape may be a more efficient use of lexical pitch so that a greater proportion of f0 variability for the discourse functions is adopted. T3: Read-stories-you-B-M 250  Frequency  200 150 100 50  ZH2u001 ZH2u003 ZH2u004 ZH2u005 ZH2u007 ZH2u008 ZH2u009 ZH2u010 ZH2u011 ZH2u013 ZH2u014 ZH2u016  0  
Duanmu (2000) proposed that tonal languages, such as Chinese, follow the same Compound and Nuclear Stress Rules (Chomsky & Halle, 1968) for phrasal stress as English. This study investigates the acoustic correlates of contrastive stress between compound words and verbal phrases in Mandarin Chinese. We focused on the durational, fundamental frequency, and intensity correlates of stress within minimal pair MN modifier-head compounds and VO verb-object phrases. Our results demonstrated that (1) the final syllable was more lengthened in [VO] than in [MN] and that (2) the F0 range was larger in [VO] than in [MN]. Moreover, the duration of the pause between the two syllables seems to play a role in distinguishing between [MN] and [VO]. In contrast, we showed that intensity contributed less to this distinction. Our results confirmed the right stress pattern in [VO]; however, we failed to find the lexical stress on the Left syllable we had expected, at least with the speakers we examined. Taken together, the present acoustic study lends support to the hypothesis that principles of stress upward of word level are universal through different languages. Keywords: Morpholexical Ambiguity, Compounding, Compound versus Nuclear Stress, Acoustic Features. 1. Introduction In stress languages, such as in English, most words have stable lexical stress patterns and it is often easy to tell which syllables have stress. For a typical tonal language, e.g. Mandarin, the word stress is often less obvious. Although, lexical stress has been shown to be highly  Institute of Psychology, Paris Descartes University, France  Laboratory of Phonetics and Phonology, University Sorbonne Nouvelle Paris 3, France  Laboratoire d’Excellence EFL, PRES Paris Sorbonne, France  Phone: ++33 1 55205924  Fax: ++33 1 55205745  E-mail: frederic.isel@parisdescartes.fr  The author for correspondence is Frédéric Isel.  46  Weilin Shen et al.  language-dependent, principles of stress upward of word level (i.e. compound stress and phrasal stress) are more universal in different languages. Chomsky and Halle (1968) proposed two rules for English compound and phrasal stress. - Compound stress rule: stress is assigned to the leftmost stressable vowel in nouns, verbs, or adjectives, e.g. bláckbird. - Nuclear stress rule (NSR): stress is assigned to the rightmost stressable vowel in a major constituent, e.g. [the [black bírd]]. It has been proposed that the Compound stress rule and NSR are true for Mandarin Chinese, and they permit one to distinguish between compounds and phrases (Duanmu, 2000). Nevertheless, there is no empirical evidence supporting this hypothesis to date. The first goal of the present study was to understand to what extent morphology affects abstract stress using acoustic-phonetic evidence. Moreover, we were interested in discerning the acoustic phonetic cues, which reflect abstract stress. In Chinese, a V-N construction is sometimes ambiguous, possibly representing both a modifier-head compound [MN] and a verb-object phrase [VO]. For example, a V-N construction ‘chao-fan’ (fry-rice) may be a compound, in which the verbal constituent ‘chao’ (fry) modifies the nominal head ‘fan’ (rice); it may also represent a verb-object relation (to fry rice). The ambiguous pairs have the same segmental characteristics and are assumed to differ from each other only in the stress pattern, showing left stress for compounds and right stress for phrases. There is, however, no phonetic evidence that compound stress and phrasal stress are implemented in [MN] compounds and in [VO] phrases in Mandarin Chinese. Fraisse (1956) proposed two basic rhythmic tendencies 1) “rythmitisation intensive,” sensitive to strengthening of the initial element, and 2) “rythmitisationtemporelle,” building on the lengthening of the final element. The supposed basic rhythmic tendencies predict initial extra loudness and final lengthening. From phonetic studies on the acoustic correlates of stress since the 1950s, researchers have agreed that linguistic stress correlates with a complex configuration of events of increased duration, larger F0 range, and raised intensity (Lehiste, 1970) and that several cues may be functionally equivalent cross-linguistically (Vaissière, 2004). Duration From a series of experiments, Fry (1955, 1958) showed that duration is a consistent correlate of stress at the word level in English and that it is a more effective cue than intensity. Since then, researchers have started to give up the classical view that stress is equated to a higher degree of intensity. Studies on the neutral tone (i.e. destressed syllable) in Chinese have confirmed the crucial role of duration on the perception of a destressed syllable for Chinese. Lin (1980, 1990) and Cao (1992) showed that duration of the destressed neutral tone syllable  Acoustic Correlates of Contrastive Stress in  47  Compound Words versus Verbal Phrase in Mandarin Chinese  is systematically shorter (reduced by approximately 50%) than a syllable with full tone. Fundamental frequency The F0 has been showed to be a major acoustic manifestation of suprasegmental structures. It is claimed by some researchers to be the strongest cue of stress for stress languages (Cooper et al., 1985; Lieberman, 1960; Gussenhoven et al., 1997). Nevertheless, others have shown that F0 is not a necessary cue because stress can be identified on the basis of duration and intensity alone (Cutler & Darwin, 1981). The situation is the same for tonal languages, such as Mandarin. The pitch range has been shown wider when syllables are stressed (Shen, 1985; Liu & Xu, 2005). More specifically, when a 3rd Tone is stressed, it is dipped lower and, when a 4th Tone is stressed, it starts higher and falls lower (Chao, 1968). Moreover, computational corpus studies (Kochanski et al., 2003) have established quantitative F0 predictions in terms of the lexical tones and the prosodic strength of each word. Shen (1993), however, found that stress in Mandarin could be identified without F0 information. Intensity In literature, the role of intensity for stress is not agreed upon. Fry (1955, 1958) showed that intensity was a less effective cue than duration on the perception of linguistic stress patterns. Nevertheless, some authors have argued that the strongest cue to prominence is intensity for English (e.g., Beckman, 1986; Turk & Sawusch, 1996). For Mandarin Chinese, the effect of the intensity is only secondary. Studies on the neutral tone in Chinese showed that the intensity of the destressed neutral tone is not necessary lower than the one with full tone (Cao, 1986). Moreover, the destressed neutral tone raises its intensity after Tone 3 (Lin, 2006). Phonetic data (Cao, 1992) has illustrated that the destressing of the neutral tone syllable is not related simply to its intensity. The intensity of a neutral tone syllable is lower than that of one with full tone in general, but the situation is reversed when it is preceded by a Tone 3 syllable. The present study investigates the acoustic correlates of stress between compound and phrase in Mandarin Chinese. We focused on the durational, fundamental frequency, and intensity correlates of stress within minimal pair [MN] modifier-head compound and [VO] verb-object phrase. Our hypotheses were that 1) [MN] modifier-head compound and [VO] phrases differ phonetically with left stress in [MN] modifier-head compounds and right stress in [VO] phrases and that 2) a different prosodic pattern is reflected in acoustic features in F0, duration and intensity.  48  Weilin Shen et al.  2. Methodology 2.1 Materials One hundred thirty-five minimal pairs presenting a morpholexical ambiguity (i.e. [MN] modifier-head compound vs. [VO] phrases) were selected from the Contemporary Chinese Dictionary 5th edition (Lu & Ding, 2008). Each pair had the same segmental characteristics and was assumed to differ from each other only in the stress pattern. The target words were not recorded in isolation and were embedded in an utterance fragment: 1)我说的不是名词编号而是动词编号. [I did not say noun “bian-hao” but said verb “bian-hao”.] The critical words in each pair change their position in the utterance fragment, giving 2)我说的不是动词编号而是名词编号. [I did not say verb “bian-hao” but said noun “bian-hao”.] In all, 270 sentences were created. The order of the sentences was randomized. 2.2 Recording Procedure Before the recording session, the participants were instructed in the goal of the recording and how the recording would proceed. The material was carried out in the laboratory of Phonetics and Phonology of University Sorbonne Nouvelle Paris 3. Speakers were recorded individually in an acoustic chamber, using an attached microphone, placed at a distance of about 5 centimeters from the speaker’s mouth. Speech samples were recorded digitally at 44,100 Hz, 16-bit mono. 2.3 Subjects Three Mandarin speakers (two females) in Paris participated in the experiment. One female speaker is an international student aged 25 years that was born in Xi’an, China. Her mother tongue and language of schooling is Mandarin. The others speakers are Beijing Mandarin speakers (one female 26 years; one male 32 years).  Acoustic Correlates of Contrastive Stress in  49  Compound Words versus Verbal Phrase in Mandarin Chinese  2.4 Acoustic Measurements The first syllable, the second syllable, and the pause between them for each critical word were manually marked in Praat, yielding four marks, one at the beginning of the first syllable, a second mark at the offset of the first syllable, a third mark between the offset of the first syllable and the onset of the second syllable, and a fourth one at the offset of the second syllable. A Praat script extracted the duration and intensity value of each segment in msec. F0 onset and offset were measured at the beginning and at the end of the vowel. In the study, we divided the vowel into ten segments normalized in time, with the mean F0 of the first segment as F0 onset and the mean F0 of the last segment as F0 offset. 3. Results Three-way repeated analysis of variance ANOVA tests were performed separately for each acoustic feature (duration, F0, and intensity). Word type ([MN] modifier-head compound vs. [VO] verb-object phrase) and syllable position (left syllable: S1 vs. right syllable: S2) were the within groups factors, and word position in the utterance fragment (i.e. final vs. non-final) was the between groups factor.  3.1 Duration  3.1.1 Left Syllable vs. Right Syllable Results of the three-way ANOVA for the duration revealed a significant main effect for word type [F(1, 134) = 440.8, p<0.001; ηp2= 0.77], a significant main effect for word position [F(1, 134) = 25.6, p< 0.001; ηp2 = 0.16], a significant main effect for syllable position [F(1, 134) = 105.3, p< 0.001; ηp2 = 0.44], a significant interaction word type x syllable position [F(1, 134) = 87.4, p< 0.001; ηp2 = 0.40], and a significant interaction word position x syllable position [F(1, 134) = 440.8, p< 0.001; ηp2 = 0.77]. Word position showed no interaction with word type. In order to increase the statistical power, we token the word position out, and ran a two-way ANOVA (word type x syllable position). The two-way ANOVA showed significant main effect for word type [F(1, 269) = 846.7, p< 0.001; ηp2 = 0.64] and for syllable position [F(1, 269) = 166.3, p< 0.001; ηp2 = 0.38] and a significant interaction word position x syllable position [F(1, 269) = 217.1, p< 0.001; ηp2 = 0.45]. Post hoc analyses showed a larger effect of syllable position for [VO] [F(1, 269) = 217.0, p< 0.001; ηp2 = 0.45] than for [MN] [F(1, 269) = 28.6, p< 0.001; ηp2 = 0.10].  50  Weilin Shen et al.  Figure 1. Mean syllable durations in msec for each syllable in [MN] and [VO]. 3.1.2 Duration of the Pause As one can notice a pause between the two syllables in VO, we decided to perform measures of pause duration. The ANOVA on the average pause duration between the syllables showed a significant main effect for word type [F(1, 269) = 33.5, p< 0.001; ηp2 = 0.11]. 3.2 F0 The F0 was analyzed separately for each of the four tones. A three-way ANOVA with word type (MN vs. VO), syllable position (Left syllable vs. Right syllable), and measure point (onset vs. set) was applied to Tone 1 and Tone 4, and a two-way ANOVA with word type (MN vs. VO) and syllable position (Left syllable vs. Right syllable) was calculated on the difference between F0max and F0min for Tone 2 and Tone 3. 3.2.1 Tone 1 Results showed a significant main effect for measure point [F(1, 45) = 10.7, p<0.01; ηp2 = 0.19], a significant interaction between word type and syllable position [F(1, 45) = 4.7, p<0.05; ηp2 = 0.10], and a significant interaction between measure point and syllable position [F(1, 45) = 6.6, p<0.05; ηp2 = 0.13]. Nevertheless, neither significant interaction between word type and measure point [F< 1], nor significant interaction between word type, syllable position, and measure point [F< 1] was found.  Acoustic Correlates of Contrastive Stress in  51  Compound Words versus Verbal Phrase in Mandarin Chinese  Figure 2. F0 values on ten segments for Tone 1 for Left and Right syllable in [MN] and [VO]. 3.2.2 Tone 2 Neither significant main effect for word type and syllable position [F< 1] nor significant interaction [F< 1] was found on the difference between F0max and F0min. Figure 3. F0 values on ten segments for Tone 2 for Left and Right syllable in [MN] and [VO]. 3.2.3 Tone 3 In order to not confound tone sandhi influence for these analyses, we took out two items in our experimental material with a Tone 3-Tone 3 combination. The two-way ANOVA on the difference between F0max and F0min revealed a significant interaction word type x syllable position [F(1, 53) = 217.1, p< 0.001; ηp2 = 0.31]. Post hoc analyses showed a larger effect of syllable position for [VO] [F(1, 53) = 37.9, p< 0.001; ηp2 = 0.42] than for [MN] [F(1, 53) = 37.9, p< 0.05; ηp2 = 0.80].  52  Weilin Shen et al.  Figure 4. F0 values on ten segments for Tone 3 for Left and Right syllable in [MN] and [VO]. 3.2.4 Tone 4 Results showed a significant main effect for measure point [F(1, 91) = 339.1, p<0.001; ηp2 = 0.79] and for syllable position [F(1, 91) = 14.0, p<0.001; ηp2 = 0.13], a significant interaction between word type and syllable position [F(1, 91) = 23.5, p<0.001; ηp2 = 0.21], a significant interaction between measure point and syllable position [F(1, 91) = 14.4, p<0.001; ηp2 = 0.14], and a significant interaction of word type x measure point x syllable position [F(1, 91) = 6.8, p<0.05; ηp2 = 0.07]. Post hoc analyses showed a main effect of syllable position for the Left syllable of [VO] [F(1, 91) = 36.8, p< 0.001; ηp2 = 0.29], however, there was no main effect of syllable position for the Left syllable of [VO] [F<1]. Figure 5. F0 values on ten segments for Tone 4 for Left and Right syllable in [MN] and [VO].  Acoustic Correlates of Contrastive Stress in  53  Compound Words versus Verbal Phrase in Mandarin Chinese  3.3 Intensity A three-way ANOVA with repeated measure was performed on the average intensity. Results showed a significant main effect for word type [F(1, 134) = 139.0, p< 0.001; ηp2 = 0.64], a significant main effect for word position [F(1, 134) = 234.8, p< 0.001; ηp2 = 0.16], a significant main effect for syllable position [F(1, 134) = 58.9, p< 0.001; ηp2 = 0.31], a significant interaction word type x syllable position [F(1, 134) = 87.4, p< 0.001; ηp2 = 0.40], and a significant interaction word position x syllable position [F(1, 134) = 440.8, p< 0.001; ηp2 = 0.77]. No interaction was found.  Figure 6. Mean intensity (dB) for each syllable in [MN] and [VO]. 4. Discussion This article investigated the acoustic correlates of linguistic stress on the ambiguous structure Verb-Noun (i.e. [MN] vs. [VO]) in Mandarin Chinese. Moreover, the acoustic feature associated with this stress pattern was analyzed. As explained in the introduction, duration, F0, and intensity are the main correlates of stress. Results showed the implication of duration, F0, and intensity in the production of compound and phrasal stress in Mandarin. Our preliminary data showed that the duration was longer for the right syllable in [VO], which was consistent with previous studies on the acoustic correlates of linguistic stress for stress languages and for tone languages, such as Mandarin. Nevertheless, the ‘assumed stressed’ left syllable in [MN] was not longer than the Right syllable. We also performed measures of pause duration, and the results on the average pause duration between the Left and Right syllables showed that average pause duration is longer in [VO] than in [MN]. Nevertheless, we considered that this larger pause duration was not an acoustic manifestation of stress but a mark of the syntactic boundary in the verb-object phrase. Despite the fact that, in tone languages, F0 information should be attributed to its lexical  54  Weilin Shen et al.  usage, our results showed that F0 would be a reliable cue for the stress pattern in [MN] and [VO]. The F0 range was shown to link to the stress for Tone 3 and Tone 4, which was in line with the predictions (Chao, 1968) that pitch range is wider for stressed syllables, specifically, when a 3rd Tone is stressed, it dips lower, and, when a 4th Tone is stressed, it starts higher. Our results showed that, for Tone, 3 the right syllable in [VO] had a larger F0 range than the left syllable. For Tone 4 the left syllable in [MN] showed higher onset F0 than the right one. The analyses on the intensity were in line with previous studies, which showed a less important role of the intensity for stress. In our preliminary data, the intensity was shown to have larger amplitude in [VO] than in [MN] for the two syllables. Nevertheless, we failed to find the strengthening of the Left syllable in [MN], as proposed by Fraisse, that left-headed feet should show extra loudness on the initial syllable than the second initial. Our results showed the same pattern of intensity between [VO] and [MN]. Therefore, we considered that, unless the [VO] and [MN] were presented together, the intensity was not an effective cue for distinguishing between [VO] and [MN]. In sum, our preliminary data suggested an implementation of the final lengthening for the stressed syllable in [VO], but no initial extra loudness in [MN]. The F0 information suggested that, for Tone 3, the Right syllable was stressed in [VO] and, for Tone 4, the Left syllable was stressed in [MN]. The results confirmed the right stress pattern in [VO]; however, with the only support in Tone 4, we did not consider a lexical stress on the Left syllable in [MN]. The prosodic information, such as stress, duration, and pause was shown to be critical for the processing of the compound words (Isel et al., 2003). Once we have shown that compound word and verbal phrase present different acoustic patterns with respect to the position of stress, the next step would be to verify whether this stress pattern is used by the listeners to differentiate the two forms in cases of segmental ambiguities. For this purpose, we plan to conduct different perception and categorization experiments. At the same time, more speakers would be added to the production study. 5. Conclusion Our results showed a right stress pattern in [VO] with longer duration in the Right syllable, larger range F0, and longer pause duration between the syllables; in contrast, no initial strengthening in [MN] was found. Only the F0 range information in Tone 4 supported a lexical stress on the Left syllable in [MN]. Acknowledgments We are grateful to the anonymous reviewers whose insightful suggestions have appreciably improved the manuscript. We also thank Mr. Joe Harwood for the correction of English.  Acoustic Correlates of Contrastive Stress in  55  Compound Words versus Verbal Phrase in Mandarin Chinese  References Boersma, P. (2001). Praat, a system for doing phonetics by computer. Glot International, 5(9/10), 341-345. Cao, J. f. (1992). On Neutral-Tone Syllables in Mandarin Chinese. Canadian Acoustics, 20(3). Chao, Y. R. (1968). A Grammar of Spoken Chinese. Berkeley, CA: University of California Press. Chomsky, N., & Halle, M. (1968). The sound pattern of English. New York: Harper and Row. Cooper, W. E., Eady, S. J., & Mueller, P. R.. (1985). Acoustical aspects of contrastive stress in question-answer contexts. Journal of the Acoustical Society of America, 77, 2142-2156. Duanmu, S. (2000). The phonology of Standard Chinese. Oxford: Oxford University Press. Fraisse, P. (1956). Les structures rhythmiques. Louvain: Publication Universitaires de Louvain. Fry, D. B. (1955). Duration and intensity as physical correlates of linguistic stress. Journal of the Acoustical Society of America, 27(4), 765-768. Fry, D. B. (1958). Experiments in the perception of stress. Language and Speech, 1, 126-152. Gussenhoven, C., Repp, B. H., Rietveld, A. C. M., Rump, H. H., & Terken, J. (1997). The perceptual prominence of fundamental frequency peaks. Journal of the Acoustical Society of America, 102, 3009-3021. Isel, F., Gunter, T. C., & Friederici, A. D. (2003). Prosody-assisted head-driven access to spoken German compounds. Journal of Experimental Psychology: Learning, Memory, and Cognition, 29, 277-288. Kochanski, G., Shih, C., & Jing, H. Y. (2003) Hierarchical structure and word strength prediction of Mandarin prosody. International Journal of Speech Technology, 6(1), 33-43. Lehiste, I. (1970). Suprasegmentals. Cambridge, Mass.: M.I.T. Press. Lieberman, P. (1960). Some acoustic correlates of word stress in American English. Journal of the Acoustical Society of America, 33, 451-454. Lin, M. C., & Yan, J. Z. (1990). Putonghua qingsheng yu qingzhong yin [The neutral tone and stress in Mandarin.]. Yuyan Jiaoxue yu Yanjiu [Language Teaching and Linguistic Studies], 1990(2), 88-104. Lin, H. (2006). Mandarin Neutral Tone Is a Phonologically Low Tone. Journal of Chinese Language and Computing, 16(2), 121-134. Lin, M. C., & Yan, J. Z. (1980). The acoustic characteristics of neutral-tone syllables in Standard Chinese. Fangyan, 3, 166-178. Liu, F., & Xu, Y. (2005). Parallel Encoding of Focus and Interrogative Meaning in Mandarin Intonation. Phonetica, 62, 70-87. Lu, S. X., & Ding, S. S. (2008). Modern Chinese Dictionary (5th ed.). Beijing: Commercial Press.  56  Weilin Shen et al.  Shen, J. (1985). Beijinghua shengdiaode yinyu he yudiao. [Pitch range of tone and intonation in Beijing dialect]. In: Beijing Yuyin Shiyan Lu [Working Papers in Experimental Phonetics]. Beijing: Beijing Daxue Chubanshe. 73-130. Shen, X. S. (1993). Relative duration as a perceptual cue to stress in Mandarin. Language and Speech, 36, 415-433. Vaissière, J. (2005). Perception of intonation. In D. B. Pisoni & R. E. Remez (Eds.), Handbook of Speech Perception. Oxford: Blackwell.  Acoustic Correlates of Contrastive Stress in  57  Compound Words versus Verbal Phrase in Mandarin Chinese  Appendix  Table 1. One hundred thirty-five minimal pairs [MN] [VO] selected in the Contemporary Chinese Dictionary 5th edition (Lu & Ding, 2008).  
This study is aimed at a better understanding of the perception of syllables. As the traditional view seems to associate syllable perception with segmental cues that result from local (i.e. present only within or adjacent to the syllable) supralaryngeal events, we are particularly interested in whether non-segmental and non-local laryngeal information contribute to syllable perception as well. Existing works on Indo-European languages show that local stress patterns and global (i.e. non-local) speech rates provide perceptual cues to words and phonemes. While we believe that the effects of the global speech rate hold across languages, based on the long-developed notion of language-specific perception, we expect that lexical tones, rather than stress patterns, serve as an important local non-segmental cue in tonal languages. We conducted a perception study on Mandarin to investigate whether tonal f0 patterns and speech rates interfere with spectral information in determining the number of syllables in an utterance. F0 contours were generated using the qTA model (Prom-on, Xu & Thipakorn, 2009). Our results show that the perceptual number of syllables depends on the perception of tonal f0 patterns and speech rates to a substantial extent. Combining our findings with prior claims (Olsberg, Xu & Green, 2007), it appears that a variety of cues – segments, lexical tones, and speech rate – compete in perceiving Mandarin syllables. In relating this study to the existing works on word segmentation, lexical access, and phoneme identification, 
The lexical meaning of Chinese words is determined by syllables and lexical tones. Phonologically, there are four full tones. Empirically, however, it remains a puzzle how tones are recognized when they are reduced in natural speech. This article presents three studies on tones of reduced disyllables: (1) a corpus study on disyllabic reduction, (2) two tone categorical identification experiments on fully pronounced and reduced disyllables, and (3) an analysis of word identification responses of two disyllables. Utilizing a segment-aligned corpus, disyllables were classified by ear into four degrees of contraction (from none to full), i.e., where a disyllable is gradiently reduced towards one syllable. The results suggested that the onset of the second syllable was most likely to be shortened or deleted. For studying the lexical effect of tones, a Ganong-style word bias experiment was conducted on T1-T4 continua of three T1-T4 disyllables. Results of the fully pronounced stimuli confirmed that the lexical status of the disyllables affected the tone classification of F0 contours along a continuum from T1 to T4, showing distinct differences of tone identification in real words and nonwords. Then, this effect disappeared when the onset of the second syllable was removed to simulate a partly reduced disyllable. Insufficient segmental information seemed to deactivate the word-nonword contrast, i.e. lexical status seemed to override any acoustic information available. Tones tended to be recognized as those from a real word throughout the continua. Finally, responses to two T1-T4 disyllables from the identification experiment done by Tseng & Lee (2010) were re-analyzed. The results suggested that reduction degree, F0 shapes, word unit type, and exposure frequency seemed to play a role in the recognition of words and tones. Keywords: Taiwan Mandarin, Disyllabic Words, Tone Perception, Reduced Speech.   Institute of Linguistics, Academia Sinica, Taipei, Taiwan E-mail: {tsengsc, soemer }@gate.sinica.edu.tw; tzulun@gmail.com  82  Shu-Chuan Tseng et al.  1. Introduction How tones are recognized in natural conversation remains a puzzle, partly because a number of other factors are acting simultaneously. Some of these factors, for instance pragmatic contextual information and the degree of speech clarity, can be directly observed and manipulated when researchers analyze natural speech. The speaker- and recipient-related individuality, such as their prior experiences about the world, prior experiences about their acquired language, and their speech competence, also are important but difficult to control. This means that the substantial construction of the mental lexicon of a speaker or a group of speakers of a language cannot be investigated in-depth unless we can cope with the variability derived from discrepancies and similarities among individuals, language communities, and their concurrent interaction in speech communication. Working towards this research goal, we made an initial but significant move in this article by beginning to examine the issue of tone recognition in reduced disyllables in Mandarin Chinese. The production and the perception of lexical tones closely and dynamically correlate with the context of lexical constituency. Thus, we first focused on disyllabic words by examining their phonetic forms. Selected disyllabic content words extracted from a corpus of conversational speech were analyzed to obtain typical reduction patterns in terms of reduction degree. The reduction pattern then was applied in our later tone categorical identification experiments to simulate the contrast of fully pronounced and reduced speech. As disyllables with an internal word boundary may not have the same reduction pattern as disyllabic words, we further re-analyzed the responses of a previously conducted word identification experiment, focusing on one disyllabic word and one monosyllabic disyllable to study the role of tone as modulated by the degree of reduction. 1.1 Processing of Spoken Words The most commonly used speech form is daily face-to-face conversation. The consensus in building natural speech corpora is the belief that we should look at realistic data for studying linguistic patterns and human behavior. The fine (and complex) details in natural speech cannot be thoroughly investigated until annotated speech corpora have been made available for different languages, for instance the Kiel Corpus of Spontaneous Speech for German (IPDS, 1995), the Chinese Annotated Corpus of Spontaneous Speech (Li et al., 2000), the Spoken Dutch Corpus (Goddijn & Binnenpoorte, 2003), the Buckeye Corpus of Conversational Speech (Pitt et al., 2006), the Corpus of Interactional Data for French (Bertrand et al., 2008), the Corpus of Spontaneous Japanese (Maekawa, 2009), and the Taiwan Mandarin Conversational Corpus (Tseng, 2013). Variation of spoken words has been studied since then on different levels, including segmental, word category, word frequency, and prosodic position. Corpus-based studies are required to provide quantitative empirical description of reduction in natural speech. For instance, using the Buckeye Corpus, Jurafsky et  Tones of Reduced T1-T4 Mandarin Disyllables  83  al. (2001) studied the effect of lexical frequency and local contextual information on the probability of reduction for function and content words. They suggested that this shortening process was not necessarily correlated with vowel reduction, but probably with the lexical frequency. Similarly, Meunier & Espesser (2011) analyzed the Corpus of Interactional Data for French. Although not directly affected by the lexical frequency effect, they found that vowel reduction, including duration shortening and vowel centralization, occurred more often in monosyllabic function words than in monosyllabic content words. Both studies addressed the importance of lexical frequency on the surface forms of reduced word in natural speech. This leaves the question of how reduced spoken words are recognized by humans. A number of psycholinguistic models have been proposed to explain how humans perform the task of spoken word recognition and how words are stored in the mental lexicon, including the phonetic-acoustic form and the associated higher level lexical information. The Cohort Theory, the Interactive-Activation Model (TRACE model), and the Neighborhood Activation Model were the classical models accounting for abstract lexical representation and extraction (Marslen-Wilson, 1987; McClelland & Elman, 1986; Luce & Pisoni, 1998). The Cohort Theory proposes that spoken word recognition involves both early bottom-up processing mainly utilizing word-initial onsets and late top-down contextual information. The Interactive-Activation Model states that (once the sensory acoustic-phonetic input comes in) different levels of a lexical item are activated, including feature, phoneme, and word. The Neighborhood Activation Model stresses the importance of similarity neighborhood density between words and the relative effects of word frequency on the spoken word recognition. Different from these abstract form extraction models, Lacerda (1995) suggested that prototypes of word forms are stored in memory and discrimination extent is used to filter the best exemplar, which is most similar to the prototype. To some extent, the concept of best exemplar is similar to the concept of episodic memory traces, which may preserve the (most likely) surface phonetic details (Goldinger, 1996). In speech communication, we constantly encounter different types of phonetic forms of words, which we may classify into phonological variants. Thus, it is not surprising that production frequency of words has some influence on the phonological representation of pronunciation variants, as found by Ranbom & Connine (2007) in their studies on the nasal flap in English. While pronunciation variants are normally concerned with substitution of segments, the phonetic forms of words can deviate substantially from the canonical forms in the way that segments are omitted. Nevertheless, spoken words with a number of deleted segments still can be recognized easily, given proper contextual information. Frauenfelder & Tyler (1987) emphasized the importance of context in empirical practice of spoken language, in addition to the five typical, sequential lexical processing phases they proposed: initial lexical contact, activation, selection, word recognition, and lexical access. A number of previous studies have  84  Shu-Chuan Tseng et al.  made similar proposals. Words in semantically coherent sentences were more accurately recognized than those in isolation, and words heard in context often are recognized long before their full acoustic signal has been delivered (Grant & Seitz, 2000; Grosjean, 1980; Marslen-Wilson, 1987). Nevertheless, words with omitted segments have been studied only recently. Highly reduced words were well recognizable only when presented in their original context with semantic and syntactic information. Without context or only with limited contextual information of adjacent syllables, they cannot be properly recognized (Ernestus et al., 2002; Tseng & Lee, 2010). Thus, questions are raised. Do the same sequential lexical processing phases apply to the recognition process of reduced words without context (Frauenfelder & Tyler, 1987)? Will we observe a lexical effect of tones in reduced words without contextual information? In this study, we aim to study the reduction patterns of Mandarin words in natural speech and how tones in reduced words are recognized. 1.2 Tones Lexical tones in Mandarin Chinese constitute an abstract, contrasting phonological system of four full tones and a neutral tone (Ho, 1996; Duanmu, 2000). The four full tones include the high level tone (T1), the mid-rising tone (T2), the dipping tone (T3), and the high falling tone (T4). The neutral tone is normally noted as T5. The lexical meaning of Chinese words is determined by the syllable and tone information simultaneously. For decades, the issue of how lexical tones are produced and recognized has attracted tremendous attention and interest from linguists and psycholinguists. Tones are often illustrated by means of fundamental frequency contour (F0). F0 contour is a kind of acoustic information transmitted through the air to the ears of the listeners. Canonical forms of lexical tones in monosyllables show similar F0 contour shapes (Xu, 1997: 67; Lai & Zhang, 2008: 184). A clear high level F0 contour is observed for T1. T4 starts higher than T1 with a falling contour. T2 and T3 start lower in pitch height, with a rising and dipping contour, respectively. In addition to studies on monosyllables produced in isolation, contextual effects of tones have also been investigated. Very detailed phonetic cues have been studied or used for manipulating the stimulus tokens, such as the onset and the offset F0 values of syllables and those in the adjacent syllables, as well as the degrees of rise and fall of the F0 contours (Lin & Wang, 1984; Xu, 1997; Xu, 2004; Cutler & Chen, 1997). A number of different paradigms have been applied to test how tones are perceived or recognized (Lai & Zhang, 2008; Lee, 2007; Lee et al., 2008; Hallé et al., 2004; Ye & Connine, 1999; Fox & Unkefer, 1985). Testing monosyllables with four lexical tones in the gating experiment of Lai & Zhang (2008), T1 and T4 are confusing to the listeners in the first 40 to 80 ms after onset. After a period of 80 ms, the recognition turns to be correct in cases of both T1 and T4. The vowel and tone monitoring tasks conducted by Ye & Connine (1999) provided support for the notion that tones are activated differently (from vowels) when  Tones of Reduced T1-T4 Mandarin Disyllables  85  the listeners hear the syllable-tone stimulus tokens in isolation or in context. To more concretely involve the semantic association of experiment stimuli, Lee (2007) and Lee et al. (2008) took into account semantic and acoustic cues in their form priming and identification experiments. Their results suggested that tonal information can be used to reduce activated candidates and can be compensated for when segmental information is insufficient. To examine the lexical status effect of tones in an implicit way, the Ganong paradigm (Ganong, 1980) was adopted by Fox & Unkefer (1985). When using the Ganong paradigm, listeners are given a standard categorical perception identification task, i.e., they are played a series of phonetic stimuli that vary from one phonemic category to another in a gradient way and must identify each stimulus as belonging to one of these two phonemic categories. One end of the continuum is a real word and the other is not, usually finding a response preference for the phonemic category that forms a real word (e.g., in a tash-dash continuum, English listeners will identify the /d/ in more stimuli than /t/, since "dash" is a real word but "tash" is not). Fox & Unkefer (1985) tested the lexical status effect of tones by having native Chinese and non-native English subjects identify T1 or T2 in the stimulus tokens of T1-T2 continua of Mandarin Chinese monosyllabic words and nonwords. Differences were observed between the two groups of subjects. Nevertheless, no differences were found between the nonword/nonword continuum and the other continua involving real words. Hallé et al. (2004) conducted a tone identification task on tone continua of T1-T2, T2-T4, and T3-T4 monosyllables to native Taiwan Mandarin and non-native French listeners. Despite observable differences between the native and non-native groups, the French listeners were also sensitive to the changes of F0 shapes in perceiving the tonal contrast to a certain degree. As we speculate on the lack of an obvious semantic association of the stimulus tokens in the above experiments, we used disyllabic words instead of monosyllabic words in our tone perception experiment to concretely enhance the link to the lexical constituency. 2. Disyllabic Words in Mandarin Chinese 2.1 Why T1-T4 Disyllabic Words? The majority of words in modern Mandarin Chinese are either monosyllabic or disyllabic, according to the 5-million words of a textual corpus, the Sinica Balanced Corpus1 (Chen & Huang, 1996). In terms of word tokens, mono- and disyllabic words together make up approximately 90% of the corpus. Tri- and quadra-syllabic words normally are composed of mono- or disyllabic words. Figure 1 illustrates the frequency percentages of monosyllabic words, disyllabic words, and words of more than two syllables in the Sinica Balanced Corpus of 5 million words, in the Taiwan Mandarin Conversational Corpus of 500K transcribed 
There has been no consensus as to what constitutes a set of base concepts in the mental landscape. With the aim of exploring base concepts in Chinese, this paper proposes that frequently-occurring words in the glosses of a lexical resource such as the Chinese Wordnet can be seen as a candidate set of base concepts because the glosses use basic words. The present study identified 130 base concepts in Chinese. The Base Concepts in EuroWordNet were adopted as a reference for comparison. While only 44.6% of the base concepts identified in the present study have an equivalent in the set of Base Concepts of EuroWordNet, the other base concepts extracted by our gloss-based approach also reflect a certain degree of basicness. It is hoped that both the overlap and the difference between different sets of base concepts identified in different languages and by different approaches can deepen our understanding of the basic core in the mind. Additionally, it is also hoped that the set of base concepts identified in the present study can have computational as well as pedagogical applications in the future. Keywords: Chinese Wordnet, EuroWordNet, Base Concept, Gloss 1. Introduction For the past few decades, a large body of research has been trying to touch on the basic core in the mind. Some studies (e.g., Wierzbicka, 1996) have aimed to figure out how a large number of concepts in the mind can be neatly organized with a basic set of concepts, leading us to the realm of human cognition. Furthermore, some studies have identified a set of base concepts that have had a wide range of computational applications.1 WordNet (Miller et al., 1990), for instance, is organized around a set of base concepts (i.e., SuperSenses), with which a large number of lexical items are associated through lexical relations. There have been many ∗ Graduate Institute of Linguistics, National Taiwan University, Taiwan E-mail: chanchiah@gmail.com; shukaihsieh@ntu.edu.tw 1The term base concept should be distinguished from other terms related to the notion of basicness in the mind, such as basic level concept. See Section 2 for a more comprehensive review.  58  Chan-Chia Hsu & Shu-Kai Hsieh  approaches to exploring what is basic in the mind, but there has been no consensus as to what constitutes a set of base concepts universal to all human languages. This study aims at providing a new perspective to identify a candidate set of base concepts in Chinese. Our data consist of the glosses in the Chinese Wordnet. Since the glosses in the Chinese Wordnet use basic words, words that occur frequently in the glosses of the Chinese Wordnet can be assumed to be reflective of a candidate set of base concepts. After data extraction and introspection, the resulting set of base concepts in the present study is compared with the set of Base Concepts proposed in the EuroWordNet project (Vossen et al., 1998). In selecting a set of base concepts, our method is based on the frequencies of words used in the glosses of the Chinese Wordnet, whereas the method adopted in the EuroWordNet project is based on the relations between synsets. It is thus noted that the set of Base Concepts in EuroWordNet is not seen as de facto, but as a reference. We use the Base Concepts in EuroWordNet as our reference because on the one hand, the Chinese Wordnet and EuroWordNet both derive from the WordNet framework, and on the other hand, the set of Base Concepts from EuroWordNet is based on many European languages. It is hoped that both the overlap and the difference between different sets of base concepts identified by different approaches can deepen our understanding of the basic core in the mind. Additionally, it is also hoped that the set of base concepts identified in the present study can have computational as well as pedagogical applications in the future. This paper is organized as follows. Section 2 provides a comprehensive review of different approaches to the notion of basicness in the mind. Section 3 reviews the significance of glosses in different contexts. Section 4 introduces our experiment method and presents the set of base concepts identified in the present study. Section 5 discusses how our proposed set of base concepts in Chinese is different from that of EuroWordNet. Section 6 concludes the paper. 2. Defining the Core Lexicon in Language and the Mind Over the past few decades, there have been various approaches to the notion of basicness in the mental landscape. Some have created lists of lexical items as basic words, mainly for pedagogical purposes. Some, from a cognitive perspective, have selected different sets of basic concepts at different levels of abstraction (e.g., semantic primitives, base concepts, basic-level categories, and basic domains). The present study focuses on base concepts, which have contributed to the establishment of lexical resources (e.g., WordNet, EuroWordNet, and BalkaNet). Compared with basic words, base concepts have more computational applications than pedagogical ones. Compared with semantic primitives and basic domains, base concepts are selected in a more scientific procedure. Compared with basic-level categories, base concepts are hierarchically higher. A  Back to the Basic: Exploring Base Concepts from the Wordnet Glosses  59  comprehensive review of different approaches to the notion of basicness in the mind will be given in the following. 2.1 Basic Words One of the earliest efforts to address the notion of basicness in the lexicon is to identify a list of basic words, which is motivated by pedagogical needs.2 Many basic vocabulary lists have been proposed, ranging from 300 words to more than 2,000 words (e.g., Dolch, 1936; Gates, 1926; Hindmarsh, 1980; Lee, 2001; McCarthy, 1999; McCarthy & O’Dell, 1999; Ogden, 1930; West, 1953; Wheeler & Howell, 1930). With the rapid development of computational analyses, such lists are mostly based on frequency counts. They can serve as useful references for pedagogical purposes, such as the design of a syllabus and the development of a language proficiency test. The main problem with most basic vocabulary lists is that the raw data on which the frequency counts are based may not be representative enough. Additionally, since what counts as a word is an issue in itself, an insight is needed when it comes to word forms and lexicalized phrases (McCarthy, 1999). 2.2 Semantic Primitives In the discussion of basicness in the mind, more abstract than basic words are semantic primitives, or semantic primes, which are pursued mainly in the theory of Natural Semantic Metalanguage (Goddard, 2002; Wierzbicka, 1972, 1996).3 A semantic primitive is basic in the sense that it is lexicalized in every language and that it cannot be defined or paraphrased in simpler terms. From a cognitive perspective, it is suggested that there is an innate set of semantic primitives representing “a universal set of fundamental human concepts” (Wierzbicka, 1996:13). Such a set is argued to be sufficient to define or paraphrase the entire vocabulary of a language. For example, the word envy can be defined as what follows (Wierzbicka, 1996:161):  2 In previous studies, the terms “basic vocabulary”, “sight vocabulary”, “core vocabulary”, and the like are sometimes interchangeable. 3 For others who have adopted a similar approach in languages other than English, see Goddard (2002:12).  60  Chan-Chia Hsu & Shu-Kai Hsieh  X feels envy. = sometimes a person thinks something like this: something good happened to this other person it didn’t happen to me I want things like this to happen to me because of this, this person feels something bad X feels something like this Specifically, Goddard (2002:14) has presented 58 “atoms of meaning”, such as I, YOU, SOMEONE, PEOPLE, SOMETHING/THING, and BODY. Unfortunately, this line of research is open to valid criticisms due to a lack of a sound method of identifying semantic primitives (e.g., Riemer, 2006). 2.3 Base Concepts in WordNets The notion of basicness has played a vital role in many lexical resources, such as English WordNet (Miller et al., 1990),4 EuroWordnet (Vossen et al., 1998), and BalkaNet (Cristea et al., 2002). In the architecture of English WordNet, synonyms are assembled in a set called synset (synonymous set). During the development of WordNet, synsets are organized into 45 lexicographical files based on the criteria of syntactic category and logical groupings. The 45 names of lexicographical files (e.g., noun.feeling and verb.cognition) are also called SuperSenses, which reveal the base concepts from the developer’s perspectives.5 As an extension of the wordnet model, EuroWordnet further proposes a set of 1,024 core synsets - called Base Concepts - that are extracted from four wordnets and translated into the closest WordNet 1.5 synsets. To keep the set balanced and shared among these wordnets, 164 core base concepts of them were selected in terms of their (more) relations with other concepts and (higher) position in the hierarchy. 6 Based on the Base Concepts identified for EuroWordNet, the BalkaNet project adopts a similar approach and selects a set of Base Concepts by focusing on five Balkan languages, including Bulgarian, Greek, Romanian, 4 WordNet is open to the general public at http://wordnet.princeton.edu. 5 For the format of the lexicographical files, see wninput(5WN) at http://wordnet.princeton.edu/wordnet/man/lexnames.5WN.html. 6 The 164 Base Concepts in EuroWordnet consist of 66 concrete synsets (nouns) and 98 abstract synsets (nouns and verbs). For more details, refer to http://www.globalwordnet.org/gwa/ewn_to_bc/ConcreteInfo.html and http://www.globalwordnet.org/gwa/ewn_to_bc/AbstractInfo.htm.  Back to the Basic: Exploring Base Concepts from the Wordnet Glosses  61  Serbian, and Turkish.7 2.4 Basic-level Concepts In the context of cognitive linguistics, many experiments have shown that in taxonomies of concrete objects, there is one level of abstraction that is regarded as basic which distinguishes them from higher and lower-level categories (Cruse, 1977, 2000; Rosch et al., 1976). For instance, in answering the question what's that in the garden, most speakers choose to say a dog rather than its hypernym an animal or its hyponym an Alsatian (Cruse, 1977:153-154). Compared with the ANIMAL concept and the ALSATIAN concept, the DOG concept is seen as a basic-level concept in that both its internal homogeneity and its distinctness from neighboring concepts are greater. The presumption of basic-level concepts has been also supported by language acquisition studies, which reveal a large percentage of children’s early words are basic-level terms (Ungerer & Schmid, 2006).8 Some recent computational approaches have attempted to use algorithms to automatically extract the basic-level concepts. Izquierdo et al. (2008) automatically select basic-level concepts from WordNet based on the relations between synsets, while Lin (2010) proposes an algorithm that can automatically identify the cognitive level of a noun in WordNet based on the ability of the noun to form compounds and the position of the noun in a hierarchical chain. A relevant discussion with regard to basic conceptualization in the study of language and the mind has been focused on basic domains, which derive directly from human embodied experience (e.g., sensory and subjective experience). Cognitive Grammar argues that a concept should be understood in terms of another more general, inclusive concept (Langacker, 1987:148). For example, the concept RADIUS makes sense only when it is viewed against the concept CIRCLE. Such a relationship can form a chain (i.e., the concept CIRCLE should be understood in terms of the concept SPACE), but the chain cannot be endless. Some concepts of a general nature, such as SPACE, TIME, and QUANTITY, are basic domains because they are characterized by a high degree of inclusiveness. 3. Definitions and Glosses in Different Contexts Defining a word can be as easy as pointing to something the word refers to, but it can be as difficult as formulating “an ideal hypothetical norm which is a sort of compromise between 7 For more information about the BalkaNet project, refer to http://www.dblab.upatras.gr/balkanet/ and http://nlp.lsi.upc.edu/web/index.php?option=com_content&task=view&id=53 for similar works (e.g., Atserias et al., 2003). 8 Note that basic-level concepts should not be confused with Base Concepts. While a Base Concept occupies a high position in a hierarchy, a basic-level concept occurs in the middle of a hierarchy.  62  Chan-Chia Hsu & Shu-Kai Hsieh  the generalization of inadequate experiential reality and a projected reality which is yet to be attained in its entirety” (Bernard, 1941:510). In different contexts, definitions and glosses play different roles, which will be reviewed in the following. 3.1 Definitions in Linguistic Semantics When it comes to the meaning of a word, people may first think of looking up its definition in a dictionary. A good understanding of word meaning relies thus upon how the word can be defined. In the discussion of linguistic semantics, there are many ways to define the meaning of a word (Riemer, 2010:65-79). A definition can be ostensive, relational, or extensional, and it sometimes combines different approaches. First, perhaps the most obvious, people often define a word in terms of ostension, i.e., by pointing out the objects a word denotes. Though an ostensive definition is useful for concrete nouns, it may cause many difficulties when used to define verbs, adjectives, adverbs, and function words (e.g., prepositions). Second, a definition can place a word in relation to other words or events. For example, a word can be defined by its synonyms. However, since there are few absolute synonyms, the identity between a word and its synonyms can be challenged. A word can also be defined through an event, which is regarded as a typical context for the word. For instance, the verb scratch can be defined as “the type of thing you do when you are itchy” (Riemer, 2010:66). The weakness of such a definition is that it works only when the addressee of the definition can accurately infer the intended meaning on the basis of the given cue. That is, someone may not get the correct meaning of scratch if he or she does not scratch when feeling itchy. Third, a definition can be extensional, and one of the commonest strategies is to define by a broad class (i.e., genus) and some distinguishing features (i.e., differentia). For example, man (in the sense of “human being”) can be loosely defined as “rational animal” (Riemer, 2010:67). One of the main problems of a genus-differentia definition is that it can be too abstract to its addressee (Landau, 2001:167). In summary, there are many strategies to define the meaning of a word, and all of them have their limitations. More generally, the difficulty of a definitional approach to semantics is that defining the meaning of a piece of language with more language in the same system will inevitably end up circular (Portner, 2005:4). 3.2 Definitions in Lexicography Explaining what words mean (thus the concepts they encode) is the central function of a dictionary. While the mental lexicon is a “theoretical exercise”, a dictionary can be seen as a “practical work” (Landau, 2001:153). On the one hand, a dictionary simulates the mental  Back to the Basic: Exploring Base Concepts from the Wordnet Glosses  63  lexicon, offering the phonological, syntactic, and semantic information of a lexical item. On the other hand, a dictionary cannot be as detailed as the mental lexicon, and lexicographers need to decide what to include in a dictionary. Compiling a dictionary is seen as a craft, for lexicographers aim to make the most of their limited resources to cater for the communicative and pedagogical needs of dictionary users. One of the most challenging and contentious aspects of the compilation of a dictionary is the creation of definitions for a dictionary entry. The term ‘definition’ would be a misnomer if it implies that word’s meaning can be precisely pinned down. There are many strategies to define a word in a dictionary (Lew & Dziemianko, 2006). The most traditional definition in a dictionary is the analytical model, i.e., the genus-differentia definition. A definition composed in this way typically consists of two elements: the genus expression that locates the definiendum in the proper semantic category, and the differentia (or plural form differentiae) that indicates the information which makes the word differ from other words of the same semantic category. For example, appraisal is defined as “a statement or opinion judging the worth, value or condition of something” (taken from Longman Dictionary of Contemporary English), where ‘a statement or opinion’ is the genus expression and the postmodifying expression ‘judging the worth, value or condition of something’ is the differentia. In many cases, it is not an easy task to produce a genus-differentia definition, and such a definition can be difficult for a dictionary user to understand. Another way to define a word in a dictionary is to adopt a contextual definition. A contextual definition of ‘appraisal’, for example, is stated as “if you make an appraisal of something, you consider it carefully and form an opinion about it” (taken from Collins COBUILD Advanced Dictionary of English). Our concern here is not to deal with the issue of ‘what makes a good definition’, or search for the underlying necessary and sufficient conditions, but to evaluate the way the principle of maximal economy is reflected in a definition sentence. Zgusta (1971) proposed a list of criteria, one of which states that the lexical definition “should not contain words more difficult to understand than the word defined” (cited in Landau, 2001:157). In addition, the effectiveness of dictionary definitions can be evaluated from the user’s viewpoint (Cumming et al., 1994; Lew & Dziemianko, 2006). For example, language learners have been found to prefer contextual definitions to analytical ones (Cumming et al., 1994). An interim conclusion thus worth drawing is that a definition should contain no more words than necessary, consistent with the demands of intelligibility and information-transfer (Atkins & Rundell, 2008). 3.3 Glosses in Lexical Resources The reviews so far naturally lead us to the glosses (definitions of word senses) in lexical and ontological resources developed in recent years. Glosses and example sentences are two  64  Chan-Chia Hsu & Shu-Kai Hsieh  essential components in the construction of lexical resources like WordNet, for they have been proved to be highly useful in discovering semantic relations and word sense disambiguation tasks (Kulkarni et al., 2010). In the design of WordNet, word lemmas are grouped into synsets (synonymous sets), which are organized as a lexical network by a wide range of lexical relations (e.g., hyponymy and antonymy). The role of glosses is thus to explain explicitly the meaning of synsets which lexically encode the human concepts. Most of the lexical relations that connect synsets are conceptually inclusive relations, such as hypernymy-hyponymy and holonymy-meronymy, which make the wordnet architecture a hierarchical conceptual structure, or a lexicalized ontology.9 In connection with ontology studies, Jarrar (2006) suggests that glosses can be of great use in an ontology. For example, glosses are easier to understand than formal representations, so ontology developers from different fields can rely on glosses to a certain degree when they communicate. However, as Jarrar (2006) further suggests, a gloss in an ontology is not intended to provide some general comments about a concept, as a traditional definition in a dictionary does. Instead, a gloss in an ontology functions in an auxiliary manner, providing some factual knowledge that is critical to the understanding of a concept but can be difficult to formalize explicitly and logically. As a consequence, glosses in a wordnet as a lexical ontology are different from dictionary definitions. Jarrar (2006) provides some guidelines for writing a gloss in an ontology. First, an ontology gloss should start with the upper type of the concept being defined. Second, an ontology gloss should be in the form of a proposition. Third, an ontology gloss should emphasize the distinguishing features of the concept being defined. Fourth, an ontology gloss can include some examples. Fifth, an ontology gloss should be consistent with the formal representation of the concept being defined. Sixth, an ontology gloss should be sufficient and clear. Generally, the glosses in the Chinese Wordnet fulfill the above criteria. Here is an example taken from the Chinese Wordnet: (1) 書：有 文字 或 圖畫 的 出版品 shu you wenzi huo tuhua DE chubanpin ‘book: a publication with words or pictures’ 9 According to Gruber (1995:908), an ontology is “an explicit specification of a conceptualization”, and a wordnet can be thought of as a lexical ontology because of its lexical implementation of conceptualization, in comparison with other formal ontologies (e.g., SUMO) where the focus is put on logical constrains.  Back to the Basic: Exploring Base Concepts from the Wordnet Glosses  65  While the gloss looks like a genus-differentia definition in a dictionary, they are different in essence. The definition techniques used by lexicographers to indicate differentiation come from various conventions, while the ontology gloss aims to make a minimal commitment to conceptualization, which meets the need of logical conciseness. The study of the basic lexicon is crucially different from other tasks of lexical acquisition in that unlike the latter where the broad coverage is at issue, the former requires instead fine-grained data to be explored. In summary, we propose that glosses in lexical resources are the best source to study the core component of the basic lexicon. 4. Glosses in the Chinese Wordnet In this section, we introduce the method of how we used gloss data from the Chinese Wordnet to touch on base concepts.10 The glosses in the Chinese Wordnet can be seen as a sample corpus with fine-grained lexical information. Figure 1 shows the similar type frequency distribution of 46 part-of-speeches (proposed by the Sinica Corpus) in the Sinica Corpus and the Chinese Wordnet, respectively.  Figure 1. The POS distribution of the Sinica Corpus and the Chinese Wordnet 4.1 Extracting a Set of Frequently-occurring Words from the Glosses of the Chinese Wordnet In our first experiment, we extracted a set of frequently-occurring words from the glosses of the Chinese Wordnet. Since a gloss in the Chinese Wordnet uses basic words instead of giving a scientific definition that can be incomprehensible to the user (Huang, 2008:22), the frequently-occurring words extracted from our experiment may reflect a certain degree of basicness in Chinese and even be considered to constitute a candidate set of base concepts in 10 The Chinese Wordnet (CWN) has been released as an open-source project, and is freely available at http://lope.linguistics.ntu.edu.tw/cwn  66  Chan-Chia Hsu & Shu-Kai Hsieh  Chinese. Our method and the results will be presented in the following. Our first step was to extract all the glosses from the Chinese Wordnet. For glosses containing more than one period (i.e., the Chinese period 。), we discarded words preceding the first period because what precedes the first period in a gloss only provides grammatical properties. Next, what remained in the glosses was segmented by a segmentation system developed by Chinese Knowledge and Information Processing (CKIP). Consider the following example: (2) 學生： 普通名詞。 在 學校 系統 內 讀書 學習 的 人。 xuesheng putongmingci zai xuexiao xitong nei dushu xuexi DE ren ‘student: someone who studies and learns in a school system’ In the example (2), putong mingci ‘common noun’ would be discarded, and then the remaining part of the definition would be segmented as shown in the example. With all the glosses segmented, a frequency wordlist with 19,852 words was created. We manually checked the wordlist for meta-linguistic terms (e.g., xingrong ‘modify’) and mis-chunked words (e.g., *dedanwei ‘DE + unit’). Only the first 1,000 words on the wordlist were checked both because our resources were limited and because it was assumed that core base concepts should be at the top of the frequency wordlist. For meta-linguistic terms, we chose to exclude them because it is obvious that they do not represent base concepts. For mis-chunked words, we either manually segmented them further (*dedanwei → de danwei) or simply excluded them if they were not comprehensible (e.g., dejian ‘DE-simple’).11 In such cases as dedanwei, the resulting words together with their frequencies were added to the wordlist if they had not been listed there, or the frequencies of the resulting words were revised. Take de danwei as an example. There were 328 de danwei in the data, and both de and danwei had been on the wordlist before dedanwei was further segmented. The frequencies of de and danwei were revised to be 15,653 and 1,178, respectively.12 To demonstrate how our new approach to identifying a set of base concepts is different from others, we decided to compare the resulting set in the present study with the set from EuroWordNet. Since all the Base Concepts in EuroWordNet are nouns and verbs, we focus on only nouns and verbs in the present study.13 Therefore, words that were not tagged with V or 11 The morpheme jian does not stand alone in Modern Chinese. 12 Originally, there were 15,325 tokens of de and 850 tokens of danwei in the data. 13 For which synsets in EuroWordNet were merged in the present study, see the appendix.  Back to the Basic: Exploring Base Concepts from the Wordnet Glosses  67  N were removed from our wordlist. In the end, the frequency wordlist based on the glosses of the Chinese Wordnet contained 17,018 words. In EuroWordNet, there are 98 abstract Base Concepts and 66 concrete Base Concepts. However, as Vossen et al. (1998) have admitted, some synsets appear to represent almost the same concepts (e.g., {form 1; shape 1} and {form 6; pattern 5; shape 5}), so the number of the Base Concepts in EuroWordNet can be reduced. In such cases, we merged the two (or more) synsets into one. Finally, we retained 130 Base Concepts, i.e., 75 abstract concepts and 55 concrete concepts. Therefore, we also selected the top 130 words from our wordlist to be a candidate set of base concepts in Chinese. When we examined the 130 words high on our wordlist, we found that some words needed to be replaced. First, two proper nouns were unsurprisingly high on the wordlist based on the Chinese Wordnet, i.e., Zhongguo ‘China’ (32th) and Taiwan ‘Taiwan’ (67th). The two words were excluded from the candidate set of base concepts. Second, since we focused on typical nouns and verbs, words typically not functioning as nouns or as verbs were excluded from our wordlist, regardless of their tags. Words discarded at this stage included:  (3) 負面 多 主要 大 相同 小 容易 固定 用來 可以 所在 受到 沒有  fumian duo zhuyao da xiangtong xiao rongyi guding yonglai keyi suozai shoudao meiyou  ‘negative’ ‘numerous’ ‘primary’ ‘big’ ‘the same’ ‘small’ ‘easy’ ‘stable; fixed’ ‘use…to…’ ‘can’ ‘a place where…’ a passivization marker in Chinese ‘without’  68  Chan-Chia Hsu & Shu-Kai Hsieh  In (3), words such as da and xiao usually function as adjectives, and zhuyao and rongyi can be adjectives or adverbs. The word meiyou, originally tagged as a noun, functions as a polarity operator rather than as a noun or as a verb.14 Another issue in the selection of the top 130 words from the glosses of the Chinese Wordnet was near-synonymy. For example, both yong ‘use’ and shiyong ‘use’ were high on our wordlist, and so were wuti ‘object’ and wupin ‘object’. In deciding whether two words did represent the same concept, the present study counted on the Chinese Wordnet rather than on our own introspection or on further analyses. In the former case, yong ‘use’ and shiyong ‘use’ bear the relation of synonymy in the Chinese Wordnet. Therefore, the two words were considered to represent the same concept, and the frequencies of the two words were added together. In the latter case (i.e., wuti and wupin), the two words do not bear the relation of synonymy in the Chinese Wordnet. As a consequence, the two words were listed separately on our wordlist (cf. Table 1). Finally, five words had two tags and were listed separately. They were gaibian ‘change’, shiyong ‘use’, jisuan ‘calculate’, chansheng ‘produce, generate’, and fasheng ‘happen’. They are verbs in their literal sense, but they can be nominalized. For the five words, the frequencies of the verbal use and the nominal use were added together, and each word was listed only once in our wordlist since both the verbal use and the nominal use represent the same concept. When words were excluded or merged with another word, another word immediately lower on the wordlist went up until we got 130 words. The final set of base concepts extracted from the glosses of the Chinese Wordnet on the basis of the frequencies will be presented and discussed in the following section.  14 In the glosses of the Chinese Wordnet, a typical context where guding occurs is as follows:  職業 婦女： 有 固定 工作  的  zhiye funu you guding dongzuo de  career woman have stable job  DE  career woman: a female who has a stable job  女子。 nuzi female  In this example, guding is used to modify gongzuo ‘job’. We decided to exclude guding because it functions neither as a typical noun nor as a typical verb, but typically functions as a modifier in the glosses of the Chinese Wordnet. Additionally, the tag automatically assigned to guding (i.e., Nv) is problematic.  Back to the Basic: Exploring Base Concepts from the Wordnet Glosses  69  4.2 Results  By extracting words that occur frequently in the glosses of the Chinese Wordnet, we obtained a candidate set of words representing base concepts in Chinese. We attempted to map each word extracted in the present study to a Base Concept in EuroWordNet, either concrete or abstract. Note that if a word has no equivalent in the set of Base Concepts in EuroWordNet, we simply translated the word into English. Moreover, those without an equivalent in the set of Base Concepts in EuroWordNet were classified on the basis of their semantic characteristics. Table 1 summarizes the results. Following Table 1, each category will be presented.  Table 1. The distribution of base concepts extracted in the present study  CATEGORY  #  %  match  abstract concrete  34  26.2%  24  18.5%  positions  7  5.4%  people  6  4.6%  non-match  organizations measurement  6  4.6%  5  3.8%  other (abstract) nouns  28  21.5%  other abstract verbs  20  15.4%  TOTAL  130 100.0%  z Abstract concepts mapped to the Base Concepts of EuroWordNet  Word  Freq. Type  Synset in EuroWordNet  
In recent years, state-of-the-art cross-linguistic systems have been based on parallel corpora. Nevertheless, it is difficult at times to find translations of a certain technical term or named entity even with a very large parallel corpora. In this paper, we present a new method for learning to find translations on the Web for a given term. In our approach, we use a small set of terms and translations to obtain mixed-code snippets returned by a search engine. We then automatically annotate the data with translation tags, automatically generate features to augment the tagged data, and automatically train a conditional random fields model for identifying translations. At runtime, we obtain mixed-code webpages containing the given term and run the model to extract translations as output. Preliminary experiments and evaluation results show our method cleanly combines various features, resulting in a system that outperforms previous works. Keywords: Machine Translation, Cross-lingual Information Extraction, Wikipedia, Conditional Random Fields. 1. Introduction The phrase translation problem is critical to many cross-language tasks, including statistical machine translation, cross-lingual information retrieval, and multilingual terminology (Bian & Chen, 2000; Kupiec, 1993). Such systems typically use a bilingual lexicon or a parallel corpus to obtain phrase translations. Nevertheless, the out of vocabulary problem (OOV) is difficult to overcome, even with a very large training corpus, due to the Zipf nature of word ∗ Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, Taiwan E-mail: joseph.nthu.tw@gmail.com The author for correspondence is Joseph Z. Chang. + Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan E-mail: jason.jschang@gmail.com # Department of Computer Sciences and Information Engineering, National Taiwan University, Taiwan E-mail: jang@csie.ntu.edu.tw  20  Joseph Z. Chang et al.  distribution and the fact that new words, technical terms, and named entities arise frequently. On the other hand, the advent of the Internet has lead to an unprecedented buildup of multilingual texts. Specifically, there are an abundance of webpages consisting of mixed-code text, namely text written in more than one language. We observe that the mixed-code webpages typically are written in one language but interspersed with some sentential or phrasal translations written in another language. By retrieving and identifying such translation counterparts on the Web, we can cope with the OOV problem caused by the limited coverage of dictionaries and parallel corpora. Consider a Wikipedia title, “Named-entity recognition”. The best places to find the Chinese translations for this technical term are probably not some parallel corpus or dictionary, but rather mixed-code webpages that mention it in both Chinese and English. The following example is a snippet returned by the Bing search engine for the query “named entity recognition” requesting Chinese language webpages: http://zh.wikipedia.org/zh-hk/問答系統: 從系統內部來看，問答系統使用了大 量有別於傳統資訊檢索系統自然語言處理技術，如自然語言剖析(Natural Language Parsing)、問題分類(Question Classification)、專名辨識(Named Entity Recognition)等等。 In this snippets, the author mentioned several technical terms in Chinese (e.g., 自然語言剖析 zhiran yuyan poxi, 問題分類 wenti fenlei, and 專名辨識 zhuanming bianshi), followed by the source terms in brackets (Natural Language Parsing, Question Classification, and Named Entity Recognition, respectively). The term-translation pairs in the above example follow the parenthetical translation surface pattern in the form of “Chinese translation (English term)”. This pattern is only one of many surface patterns found on the Web that may indicate a term-translation pair. In the following examples, we show different surface patterns of translation pairs found on the Web, with Chinese translations underlined and the counterpart English terms italicized: (a) 血液學檢驗(hematology) – 白血球分類 (b) [巴黎最美的橋] 亞歷山大三世橋 Pont Alexandre III (c) 胰島素泵的臨床應用及護理進展 progress on nursing of clinical application of insulin pump (d) 國外組織美國職棒大聯盟 (Major League Baseball，簡稱: MLB，或大聯 盟)  Learning to Find Translations and Transliterations  21  on the Web based on Conditional Random Fields  (e) [食記]義美蔥油餅 Imei green onion pancake (f) [食記]義美蔥油餅 Imei green onion pancake . . .  Examples (a) and (b) show Chinese translations occurring near or next to an English phrase. There are also cases (e.g., Example (c)) where the translation (e.g.,胰島素泵 yidaoshu pang) and the English phrase (e.g., insulin pump) are far apart. Example (d) shows another form of parenthetical translation pattern, where translations are right next to the English term (Major League Baseball). Examples (e) and (f) show two term translation pairs interwoven in the same text (義美 yi-mei transliterated into Imei and 蔥油餅 cong-you-bing translated into green onion pancake). For a given English term, such translations can be extracted by classifying the Chinese characters in the snippets as either translation or otherwise. Intuitively, we can cast the problem as a sequence labeling task. To be effective, we need to associate each token (i.e., Chinese character or word) with some features to characterize the likelihood of the token being part of the translation. For example, by exploiting some external knowledge sources (e.g., bilingual dictionaries), we derive that the Chinese character “辨” (bian) in the Chinese word “辨識” (bian-shi, recognition) is likely to be part of the translation of “named entity recognition.” In this paper, we present a new method that automatically obtains such labeled data and generates features for training a conditional random fields (CRF) model that is capable of identifying translation or transliteration in mixed-code snippets returned by search engines (e.g., Google or Bing). The system uses a small set of phrase-translation pairs to obtain search engine snippets that may contain both an English term and its Chinese translation from search engines. The snippets then are tagged automatically to train a CRF sequence labeler. We describe the training process in more detail in Section 4. At run-time, we start with a given phrase (e.g., “named-entity recognition”), which is transformed into a query with a setup to retrieve webpages in the target language (e.g., Chinese). We then retrieve mixed-code snippets returned by the search engine and extract translations within the snippets. The identified translations can be used to supplement a bilingual terminology bank (e.g., adding multilingual titles to existing Wikipedia); alternatively, they can be used as additional training data for a machine translation system, as described in Lin, Zhao, Van Durme, and Paşca (2008). Most previous works focus on extracting translation pairs where the counterpart terms appear near one another in the webpage, based on a limited set of short patterns. In our approach, we extract term and translation pairs that are near or far apart, and are not limited by a set of predefined patterns. We have evaluated our method based on English-Chinese  22  Joseph Z. Chang et al.  language links in Wikipedia as the gold standard. Results show that our method produces output for 80% of the test cases with an exact match precision of 43%, outperforming previous works. The rest of the paper is organized as follows. In the next Section 2, we survey the related work that also aimed to mine translations from the Web. In Section 3, we give brief descriptions on resources we make use of. In Section 4, we describe in detail the problem statement and the proposed method. Finally, we report evaluation results and error analysis in Section 5. 2. Related Work In machine translation, a source text is typically translated one sentence at a time, while cross-lingual information retrieval involves phrasal translation. The proposed methods for phrase translation in the literature rely on either handcrafted bilingual dictionaries, transliteration tables, or bilingual corpora. For example, Knight and Graehl (1998) described and evaluated a multi-stage machine translation method for performing backwards transliteration of Japanese names and technical terms into English, while Bian and Chen (2000) described cross-language information access to multilingual collections on the Internet. Recently, Smadja, McKeown, and Hatzivassiloglou (1996) proposed an algorithm for producing collocation and translation pairs, including noun and verb phrases, in bilingual corpora. Similarly, Kupiec (1993) propose an algorithm for finding noun phrase correspondence in bilingual corpora for bilingual lexicography and machine translation. Koehn and Knight (2003) described a noun phrase translation subsystem that improves word-based statistical machine translation methods. Some methods in the literature also have aimed to exploit mixed code webpages for word and phrase translation. Nagata, Saito, and Suzuki (2001) presented a system for finding English translations for a given Japanese technical term in search engine results. Their method extracts English phrases appearing near the given Japanese term, and it scores translation candidates based on co-occurrence counts and location. Cao and Li (2002) proposed an EM algorithm for finding translation for base noun phrases on the Web. Kwok et al. (2005) focused on named entity phrases and implemented a cross-lingual name finder based on Chinese-English webpages. Wu, Lin, and Chang (2005) proposed a method for learning a set of surface patterns to find terms and translations occurring in short distance. Mixed-code webpage snippets were obtained by querying a search engine with English terms for Chinese webpages. They discovered that the most frequent pattern is where the translation immediately followed by the source term, with the coverage rate of 46%. Their results also indicate the stricter parenthetical pattern covers less than 30% of the translation instances. Researchers also have explored the hyperlinks in webpages as a source of bilingual  Learning to Find Translations and Transliterations  23  on the Web based on Conditional Random Fields  information. Lu, Chien, and Lee (2004) proposed a method for mining terms and translations from anchor text directly or transitively. In a follow-up project, Cheng et al. (2004) proposed a method for translating unknown queries with web corpora for cross-language information retrieval. Similarly, Gravano and Henzinger (2006) also proposed systems and methods for using anchor text as parallel corpora for cross-language information retrieval. In a study more closely related to our work, Lin et al. (2008) proposed a method that performs word alignment between Chinese translations and English phrases within parentheses in crawled webpages. Their paper also proposed a novel and automatic evaluation method based on Wikipedia. The main difference from our work is that the alignment process in Lin et al. (2008) is done heuristically using a competitive linking algorithm proposed by Melamed (2000), while we use a learning-based approach to align words and phrases. Moreover, in their method, only parenthetical translations are considered. With only the parenthetical pattern, their method is able to extract a significant number of translation pairs from crawled webpages without a given list of target English phrases. By restricting to parenthetical surface patterns however, many translation pairs in webpages may not be captured, including term-translation pairs that are further apart. In our work, we exploit surface patterns differently as a soft constraint in a CRF model and use an approach similar to Lin et al. (2008) to evaluate our results. In contrast to the previous work in phrase and query translation, we present a learning-based approach that uses annotated data to develop the system. Nevertheless, we do not require human intervention to prepare the training data, but instead make use of language links in Wikipedia to automatically obtain the training data. The annotated data is further augmented with features indicative of translation and transliteration relations obtained from external lexical knowledge sources publicly-available on the Web. The trained CRF sequence labeler then is used to find translations on the Web for a given term. 3. Resources In this work, we rely on several resources that are available on the Internet. These resources are used for different purposes: the seed data are used for obtaining and labeling training data, the gold standard is used for automatic evaluation, and the external knowledge sources are used for generating features. 3.1 Wikipedia Wikipedia is an online encyclopedia compiled by volunteers around the world. Anyone on the Internet can edit existing entries or create new entries to add to Wikipedia. Owing to the number of its participants, Wikipedia has achieved both high quantity and a quality comparable to traditional encyclopedias compiled by experts (Giles, 2005). Due to these  24  Joseph Z. Chang et al.  reasons, Wikipedia has become the largest and most popular reference tool. We extracted bilingual title pairs from the English and Chinese editions of Wikipedia as the gold standard for evaluation and as seeds to automatically collect and label training data from the Internet by querying search engines. The number of entries in English Wikipedia grew at an exponential rate from 2001 to 2008, with some 20,000 new articles created monthly by thousands of volunteers around the world, making it an excellent source for finding new words and terms. As of February 2, 2012, the English Wikipedia had 3,861,652 articles, making it the most well-established edition for all 285 languages. Entries on the same topic among different language editions of Wikipedia are interlinked via the so-called language links. Nevertheless, only a small percentage of English articles are linked to editions of other languages. The Chinese Wikipedia contains only 398,206 articles, making it roughly one-tenth the size of the English Wikipedia. Furthermore, only 5% of the entries in the English Wikipedia contain language links to their Chinese counterparts. The proposed method can be used to find the translations of those English terms, thus speeding up the process of building a more complete multilingual Wikipedia. As will be described in Section 4, we extracted the titles of English-Chinese article pairs connected by language links for training and testing purposes. The content of Wikipedia is freely downloadable online.1 We used the Google Freebase Wikipedia Extraction (WEX) instead of the official raw dump. The WEX is a processed version of the official dump, with the Wikipedia syntax transformed into XML. The WEX database can be freely downloaded online.2 3.2 WordNet WordNet is a freely available, handcrafted lexical semantic database for English.3 Starting its development in 1985 at Princeton University by a team of cognition scientists, WordNet was originally intended to support psycho-linguistic research. Over the years, WordNet has become increasingly popular in the fields of information retrieval, natural language processing, and artificial intelligent. Through each release, WordNet has grown into a comprehensive database of concepts in the English language. As of today, the stable 3.0 version of WordNet contains 207,000 semantic relations between 150,000 words organized in over 115,000 senses. Senses inWordNet are represented as synonym sets (synsets). A synset with a definition contains one or more words, or lemmas, that express the same meaning. In addition, WordNet 
The term Machine Translation is a standard name for computerized systems responsible for the production of translations from one natural language into another with or without human assistance. It is a sub-field of computational linguistics that investigates the use of computer software to translate text or speech from one natural language to another. Many attempts are being made all over the world to develop machine translation systems for various languages using rule-based as well as statistically based approaches. Development of a full-fledged bilingual machine translation (MT) system for any two natural languages with limited electronic resources and tools is a challenging and demanding task. In order to achieve reasonable translation quality in open source tasks, corpus based machine translation approaches require large amounts of parallel corpora that are not always available, especially for less resourced language pairs. On the other hand, the rule-based machine translation process is extremely time consuming, difficult, and fails to analyze accurately a large corpus of unrestricted text. Even though there has been effort towards building English to Indian language and Indian language to Indian language translation system, unfortunately, we do not have an efficient translation system as of today. The literature shows that there have been many attempts in MT for English to Indian languages and Indian languages to Indian languages. At present, a number of government and private sector projects are working towards developing a full-fledged MT for Indian languages. This paper gives a brief description of the various approaches and major machine translation developments in India. Keywords: Corpus, Computational Linguistics, Statistical Approach, Interlingua Approach, Dravidian Languages0.  ∗ Professor and Head, Department of ISE, St. Joseph Engineering College, Mangalore, VTU. E-mail: antonypjohn@gmail.com  48  Antony P. J.  1. Introduction MT refers to the use of computers to automate some of the tasks or the entire task of translating between human languages. Development of a full-fledged bilingual MT system for any two natural languages with limited electronic resources and tools is a challenging and demanding task. Many attempts are being made all over the world to develop MT systems for various languages using rule-based as well as statistical-based approaches. MT systems can be designed either specifically for two particular languages, called a bilingual system, or for more than a single pair of languages, called a multilingual system. A bilingual system may be either unidirectional, from one Source Language (SL) into one Target Language (TL), or may be bidirectional. Multilingual systems are usually designed to be bidirectional, but most bilingual systems are unidirectional. MT methodologies are commonly categorized as direct, transfer, and Interlingua. The methodologies differ in the depth of analysis of the SL and the extent to which they attempt to reach a language independent representation of meaning or intent between the source and target languages. Barriers in good quality MT output can be attributed to ambiguity in natural languages. Ambiguity can be classified into two types: structural ambiguity and lexical ambiguity. India is a linguistically rich area. It has 18 constitutional languages, which are written in 10 different scripts. Hindi is the official language of the Union. Many of the states have their own regional language, which is either Hindi or one of the other constitutional languages. In addition, English is very widely used for media, commerce, science and technology, and education only about 5% of the world's population speaks English as a first language. In such a situation, there is a large market for translation between English and the various Indian languages. Even though MT in India started more than two decades ago, it is still an ongoing process. The third section of this paper discusses various approaches used in English to Indian languages and Indian language to Indian language MT systems. The fourth section gives a brief explanation of different MT attempts for English to Indian languages and Indian languages to Indian languages. 2. History of MT The major changeovers in MT systems are as shown in Figure 1. The theory of MT pre-dates computers, with philosophers ’Leibniz and Descartes’ ideas of using code to relate words between languages in the seventeenth century (Hutchins et al., 1993). The early 1930s saw the first patents for ’translating machines’. Georges Artsrouni was issued a patent in France in July 1933. He developed a device, which he called a ’cerveau mécanique’ (mechanical brain) that could translate between languages using four components: memory, a keyboard for input,  Machine Translation Approaches and Survey for Indian Languages  49  1949 Weaver Memo 1946 ENIAC  1966 Alpac report  IBM CANDIDE System 1988 Candide  2001 BLEU  1950  1975  2000  Dictionary MT systems  Rule-based MT systems  Empirical and Hybrid MT systems  Figure 1. Major changeovers in MT Systems. a search method, and an output mechanism. The search method was basically a dictionary look-up in the memory; therefore, Hutchins is reluctant to call it a translation system. The proposal Russian Petr Petrovich Troyanskii patented in September 1933 bears a resemblance  50  Antony P. J.  to the Apertium system, using a bilingual dictionary and a three-staged process, i.e. first a native speaking human editor of the SL (SL) pre-processed the text, then the machine performed the translation, and finally a native-speaking human editor of the TL post-edited the text (Hutchins et al., 1993; Hutchins et al., 2000). After the birth of computers Electrical Numerical Integrator and Calculator (ENIAC) in 1947, research began on using computers as aids for translating natural languages (Hutchins et al., 2005). The first public demonstration of MT in the Georgetown-IBM experiment, which proved deceptively promising, encouraged financing of further research in the field. In 1949, Weaver wrote a memorandum, putting forward various proposals (based on the wartime successes in code breaking) on the developments in information theory and speculation about universal principles underlying natural languages (Weaver et al., 1999). In the decade of optimism, from 1954-1966, researchers encountered many predictions of imminent ’breakthroughs’. In 1966, the Automated Language Processing Advisory Committee (ALPAC) report was submitted, which said that, for ’semantic barriers’, there are no straightforward solutions. The ALPAC report committee could not find any "pressing need for MT" nor "an unfulfilled need for translation (ALPAC et al., 1996)". This report brought MT research to its knees, suspending virtually all research in the United States of America (USA) while some research continued in Canada, France, and Germany (Hutchins et al., 2005). After the ALPAC report, MT almost was ignored from 1966-1980. In the year 1988, Georgetown-IBM experiment launched “IBM CANDIDE System,” where over 60 Russian sentences were translated smoothly into English using 6 rules and a bilingual dictionary consisting of 250 Russian words, with rule-signs assigned to words with more than one meaning. Although Professor Leon Dostert cautioned that this experimental demonstration was only a scientific sample, or "a Kitty Hawk of electronic translation (Kitty Hawk1)," a wide variety of MT systems emerged after 1980 from various countries and research continued on more advanced methods and techniques. Those systems mostly were comprised of indirect translations or used an ’interlingua’ as an intermediary. In the 1990s, Statistical Machine Translation (SMT) and what is now known as Example-based Machine Translation (EBMT) saw the light of day (IBM, 1954). At this time the focus of MT began to shift somewhat from pure research to practical application using a hybrid approach. Moving towards the change of the millennium, MT became more readily available to individuals via online services and software for their personal computers.  
This paper proposes a novel scheme that enhance the modulation spectrum of speech features in noise speech recognition via non-negative matrix factorization (NMF). In the presented approach, we apply NMF to obtain a set of non-negative basis spectra vectors which derived from the clean speech to represent the important components for speech recognition. The difference compared to the conventional NMF-based scheme that leverages iterative search to update the full-band modulation spectra is two: first, we apply the orthogonal projection to update the low sub-band modulation spectra. Second, we process the low half-band of the 22  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) modulation spectrum rather than the full-band. The presented new process improves the computation efficiency without the cost of degarded recognition performance. In the Aurora-2 database and task, the presented new NMF-based approach can achieve the average error reduction rate of over 58% relative to the baseline MFCC. 關鍵詞：非負矩陣分解法、強健性、調變頻譜、語音辨識。 Keywords: nonnegative matrix factorization, modulation spectrum, speech recognition, noise robustness. 一、緒論 當一套語音辨識系統[1]應用在實際環境下時，環境的不匹配、語者變異性及發音的變 異性通常會造成辨識效能的低落，為了降低這些變異性所造成的影響而發展的技術，一 般而言統稱為強健性技術(robustness techniques)。 常見的語音特徵(speech features)強健性技術中，有一大類別是對於語音特徵的統計 值 做 正 規 化 (statistics normalization) ， 如 倒 頻 譜 平 均 值 正 規 化 法 (cepstral mean normalization, CMN)[2]、倒頻譜平均值與變異數正規化法(cepstral mean and variance normalization, CMVN)[3]、倒頻譜增益正規化法(cepstral gain normalization, CGN)[4]、相 對頻譜法(RelAtive SpecTra, RASTA)[5]、倒頻譜平均值與變異數正規化結合自回歸動態 平 均 濾 波 器 法 (cepstral mean and variance normalization plus auto-regressive-moving average filtering, MVA)[6]、統計圖等化法(histogram equalization, HEQ)[7][8]、時間序列 結構正規化法(temporal structure normalization, TSN)[9]等，這些正規化通常是作用於語 音特徵的時間序列(temporal sequence)上。 本篇論文中，與上述正規化法主要不同的是，我們對於特徵時間序列的傅立葉轉 換、即其調變頻譜(modulation spectrum)作強健性更新，其它常見的調變頻譜處理技術 有 頻 譜 統 計 圖 等 化 法 (spectral histogram equalization, SHE)[10] 、 強 度 比 率 等 化 法 (magnitude ratio equalization, MRE)[10] 、 調 變 頻 譜 替 代 法 (modulation spectrum replacement, MSR)與調變頻譜濾波法(modulation spectrum filtering, MSF)[11]等。作用於 調變頻譜上其可能的好處是，我們可以直接針對不同的頻率成分加以處理。由 N.Kanedera 的研究[12]發現，大部份的語音辨識資訊分布在 1 Hz 及 16 Hz 的中低調變 頻率之間，因此如果若著重於處理此段調變頻率成分，而非整體頻帶，預期將可在不影 響原始全頻帶方法之辨識精確度的前提下，有效減低計算上的複雜度、提升強健性技術 的即時(real-time)性。 在分析多維資料的特性時，非負矩陣分解法(non-negative matrix factorization, NMF) [13-16]是近十幾年來相當新穎且有用的技術，起初，NMF 常運用在影像處理上，近年 來，已有許多的相關 NMF 的應用及研究於語音辨識上。例如在文獻[17]中，利用了 NMF 對於語音特徵之全頻帶的調變頻譜作分解與更新，而達到了提升語音特徵強健性的效 果。在本篇論文中，我們延伸了文獻[17][18]的觀念與方法，提出了兩種提升運算效能 的新步驟： 1. 藉由正交投影(orthogonal projection)的方式取代原先 NMF 中求取新調變頻譜強度之 迭代法(iteration approach)，如此可避免迭代法中費時的迭代運算及不確定的迭代數目， 進而改進整體的運算速度。 2. 我們採取分頻帶分解的方式取代原先全頻帶分解，可只對於重要的中低頻帶作 NMF 的分解與更新，或分別對中低頻帶與高頻帶作 NMF 的分解與更新，藉此強調中低頻帶 23  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013)  的重要性、並可減少計算量。 除了上述採用 NMF 更新及分解調變頻譜之外，我們另行使用文獻[18]中的方法來 進行討論，以主軸成分分析(principal component analysis, PCA)求取調變頻譜基底矩陣， 由於主軸成分分析其主要目的在於針對一群資料找出其最佳投影方向，使得其投影後的 資料點能夠獲得對大之變異量，因此，同樣以投影方法更新調變頻譜，並利用上述分頻 帶分解進行更新的方法降低其運算複雜度。 在 Aurora-2 的數字資料庫的辨識實驗上，我們驗證了上述新方法的良好效果、可有效 降低原始 NMF 處理法的複雜度，有時可附加提升辨識精確度的功能。  二、非負矩陣分解調變頻譜投影法 在本章節中我們將分成四個小節介紹基本的非負矩陣概念、及本篇論文所提出的方 法之原理與步驟。首先第一小節介紹非負矩陣分解法，接著第二小節說明之前學者所提 出的迭代式頻譜更新法，第三小節則說明本論文所提出的調變頻譜投影法，最後一小節 則是藉由功率頻譜密度圖來討論其效能。  （一）非負矩陣分解法之介紹  非負矩陣分解法(nonnegative matrix factorization, NMF)主要目的是將一個內容皆為  非負實數的矩陣，分解為元素皆為非負實數的兩個基底矩陣之乘積。假設欲分解的非負  矩陣表示為 V = [v1, ...vM ]，其中 vj 為矩陣 V 之第 j 行，而矩陣 V 的尺寸為 N×M。藉由 NMF 分解 V ，得到 W 及 H 兩個非負矩陣，如下式表示：  V » WN´r Hr´M  (1)  W 及 H 尺寸分別為 N×r 與 r×M，r 可決定 W 及 H 兩矩陣尺寸(一般而言 r 遠小於 N 與  M)，其中 W 帶有 v 的行向量的綜合資訊，當我們改寫成 v » Wh 時，則 v 、 h 和 V 、H  將呈現行相關的關係，換句話說，v 可視為 W 之行向量的線性組合而 h 為其權重比，然  而最主要的目的在於使 W 與 H 兩矩陣的乘積逼近於 V ，如此一來才能得到一組可靠的  基底，這個過程我們以下式表示：  å( ( ) ) (W, H) = min W ,H i,m  Vi,m -  W H i,m  2  (2)  （二）NMF 使用於全頻帶調變頻譜之迭代式更新 無論是此小節將要介紹的迭代式更新法或是下一小節的投影法，所採用的皆是與壓 縮感知(compressed sensing)[16]相同的概念，簡單來說，所謂的壓縮感知並不直接對訊 號直接做採集，而是經由將信號投影至一組波形上，得到一組壓縮數據後，再藉由最佳 化的方式進行解碼，進而估計出原始訊號的重要訊息。 在文獻[18]中，首先提及利用NMF於語音倒頻譜特徵調變頻譜的更新上。在此，我 們簡要地介紹其更新步驟： 步驟 I. 對於特定項之語音特徵(如第一維倒頻譜特徵)而言，將用以訓練聲學模型的每一 句乾淨語音特徵時間序列作作離散傅立葉轉換(discrete Fourier transform, DFT)，得到其 調變頻譜序列，將這些不同語句所對應之調變頻譜序列的強度(magnitude)排成一個矩陣 V 的每一行(column)，因此若 V 的尺寸為 N×M，代表了我們共有 M 句語音，而其頻率  24  
劉昭麟  Wei-Jie Huang  Po-Cheng Lin 國立政治大學資訊科學系  Chao-Lin Liu  Department of Computer Science, National Chengchi University  {100753014, 101753028, chaolin}@nccu.edu.tw  摘要 文字蘊涵是研究文字敘述之間的邏輯關係的工作，本文利用詞彙、語法、詞彙語意的相關 語文資訊，建構經驗法則公式與機器學習模型，檢驗自動推測文字蘊涵關係的效果。本文 所報告的效果在NTCIR-10的RITE競賽中的簡體與繁體中文的文字蘊涵都有相當好的表 現。我們同時延伸文字蘊涵的推論技術，企圖以語文處理技術自動回答國小及國中的中英 文閱讀測驗試題，這一部份的工作仍在發展之中，對於比較簡易的四選一的試題，如果相 關的基礎技術成熟，可以達到超過五成的答對率。 Research on text entailment studies the logical relationships between statements. We employed linguistic information at the lexical, syntactic, and semantic levels to build heuristics and machine-learning based models for algorithmic judgment of text entailment relationships. Methods proposed in this paper achieved relatively very good performances in the RITE task for both traditional and simplified Chinese entailment problems in NTCIR-10. We extended our work and attempted to automatically answer questions in reading comprehension tests in Chinese and English used in elementary and middle schools. To make the automatic answering more feasible, we manually selected statements which were relevant to the test items before we ran the text entailment component. Experimental results indicated that it was then possible to find the answers better than 50% of the time for one out of four multiple-choice items. 關鍵詞：文字蘊涵、經驗法則公式、機器學習模型 
s99323904@mail1.ncnu.edu.tw, s100323553@mail1.ncnu.edu.tw, jwhung@ncnu.edu.tw  摘要  近幾十年來，無數的學者先進對於此雜訊干擾問題提出了豐富眾多的演算法，略分成兩  大類別：強健性語音特徵參數表示法(robust speech feature representation)與語音模型調適  法(speech model adaptation)，第一類別之方法主要目的在抽取不易受到外在環境干擾下  而失真的語音特徵參數，或從原始語音特徵中儘量削減雜訊造成的效應，比較知名的方  法 有 ： 倒 頻 譜 平 均 值 與 變 異 數 正 規 化 法 (cepstral mean and variance normalization,  CMVN)[1]、倒頻譜統計圖正規化法(cepstral histogram normalization, CHN)[2]、倒頻譜平  均 值 與 變 異 數 正 規 化 結 合 自 動 回 歸 動 態 平 均 濾 波 器 法(cepstral mean and variance  normalization plus auto-regressive-moving average filtering, MVA)[3]等；第二類別之方  法，則藉由少量的應用環境語料或雜訊，來對原始的語音模型中的統計參數作調整，降 
Opinion analysis has grown to be one of the most active research areas in natural language processing. If we can classify reviews and messages of blogs correctly, it will help to analyze product and service competition and to realize the opinion orientations of the people on public issues. In this paper, we propose an opinion orientation estimation approach based on target finding and opinion modifying relations in microblog reviews. First, it collects reviews from microblog and preprocesses the source data. Then, by extracting any entity or aspect of the entity about which an opinion has been expressed according to opinion modifying relations, we calculate the overall score of opinion orientation. In our experiment on the 1000 movie reviews of 50 movies from Twitter, the average   3URFHHGLQJVRIWKH7ZHQW\)LIWK&RQIHUHQFHRQ&RPSXWDWLRQDO/LQJXLVWLFVDQG6SHHFK3URFHVVLQJ52&/,1* accuracy of the proposed method is 84.44%, and the highest precision is 88.89%, which is better than SVM and Naive Bayes. This validates the higher precision from modifying relation identification for opinion orientation classification. 關鍵詞：情感分析，意見傾向，微網誌，意見詞，修飾關係 Keywords: opinion analysis, opinion orientation, microblog, sentiment word, modifying relation 一、緒論 情緒偵測 (emotion detection)的發展對於商業與科技的互動具有高度的應用價值， 包括依照使用者情緒推薦相符合的文章、音樂等商品。本研究透過知名微網誌 Twitter 的英文短句中的情緒詞彙進行推文 (tweet) 情緒分類，因為短篇文件所包含的語境和詞 彙通常比較不足夠，所以短篇文件的文件分類效果通常會比長篇的文件分類效果不佳。 有別於傳統文件分類，我們分析情緒詞彙與修飾關係進行以句子為基礎的情緒偵測 (sentence-based emotion detection) 問題。 本研究方法使用英文文法的修飾關係，主要是 tweet 中的內容評論目標與意見詞之 間的修飾關係，找到修飾關係即能判斷評論者藉此內容抒發某種情緒。根據評論主題以 及意見詞的修飾關係，發掘出主題相關的評論目標以判斷其意見傾向來預測未知情緒類 別的文章之可能情緒。 二、相關研究 常見的情緒偵測方法所適用的範圍可分為為句子層次的推論，段落層次和全篇文章 層次的情緒偵測方法 [1][2]。因為微網誌的字數限制，本篇研究專注在句子層次的情緒 偵測方法，包括評論目標 (target)、意見詞 (opinion word)等。接下來將探討一些使用文 件分類相關技術於情緒偵測的文獻，彼此最大的差異在於偵測方法上的差異。 （一）、評論目標發掘 Kim 和 Hovy[3]針對各類主題的新聞進行找出內文中的 opinion holders 及 opinion topics。首先以動詞及形容詞為主建立情緒辭典，接著然後使用剖析器解析句子，並將 FrameNet 的 frame element 及範例句子來進行 Maximum Entropy 訓練以找到句子中的 opinion holder 及 opinion topic。最後實驗結果的準確度為 64%，說明 FrameNet 中的 frame 及 frame element 有限，只能找到部分的句子結構，因此準確率並不高。 Popescu 和 Etzioni[4] 針 對 商 品 的 使 用 者 評 論 ， 採 用 PMI (Point-wise Mutual Information) 來獲得與主題共同出現機率最高的詞來當作評論目標 (opinion targets)，與 本研究分法類似。我們除了使用 PMI 以外，有鑑於不同使用者所使用的字詞會有所不 同，所以也將 PMI 所獲得詞的同義字來擴增我們的評論目標。   3URFHHGLQJVRIWKH7ZHQW\)LIWK&RQIHUHQFHRQ&RPSXWDWLRQDO/LQJXLVWLFVDQG6SHHFK3URFHVVLQJ52&/,1* Qiu 等人[5]使用消費者評論資料集[20]裡的評論辨識出評論目標(target)及意見詞 (opinion word)。首先使用 POS tagging 標註字的詞性，他們定義 target 為名詞，opinion words 為形容詞。再透過 Minipar[21]解析句子的結構。最後利用句子的結構及資料已標 記好的目標 (商品相關屬性等等)和意見詞來找到句子中未知而可能的目標。例 如”Canon G3 has a great lens.”，句子經過解析後得到 G3 subj，lens obj，G3 為動詞”has”的主詞，lens 為動詞”has”受詞，若已知的目標為”lens”，透過句子的結構關 係與已知的目標得知”lens”與”G3”為相同主題的目標。例如”iPod is the best mp3 player”，句子經過解析後得到 iPod subj，best 修飾 player，而”best”為已知的意見 詞，透過句子的結構與已知的意見詞得知”player”與”iPod”為相同主題的目標。 （二）、修飾關係辨識 Zhuang 等人[6]針對電影評論進行情緒的分類。他們使用 Stanford Parser 工具[22] 來解析句子結構並找出字與字的修飾關係，進一步定義意見詞的情緒傾向。因為 tweet 句子結構複雜並包含許多口語化用字，若使用 Stanford Parser 來解析 tweet 並不能準確 分析字與字間的修飾關係，因此本研究方法定義意見詞與主題之間修飾關係的方法，來 克服 tweet 不規則的句子結構。 Qiu 等人[5]透過資料集[20]中已標註的目標及意見詞以及句子的結構關係，來擴增 與主題相關的目標及意見詞。在句子中結構使用語意相依法則 (semantic dependency grammars)，可分為直接相依性 (Direct Dependency, DD) 及非直接相依性 (Indirect Dependency, IDD)兩種字詞結構[23]。透過剖析器將句子，剖析出樹狀的詞性架構，並 標記出中心詞(head)所在的位置。以中心詞為基準，考慮其它詞與中心詞的關係。他們 針對結構化且單純的評論文章進行實驗，且每篇文章的評論目標較為明確，因此使用剖 析工具來解析句子結構較為適合，且能利用資料集已提供與主題相關的目標及意見詞來 進一步擴增與主題相關的目標及意見詞。但社群上的訊息結構複雜且訊息內容無特定主 題，因此使用剖析工具無法正確解析句子，且因訊息內容主題不明確，不易利用句子結 構及修飾關係來找到其他與主題相關的目標及意見詞。因此本論文提出利用統計方法找 到可能與主題相關的目標，我們定義意見詞為形容詞及動詞，增加了修飾規則的判斷。 並且利用意見詞及目標的距離來判斷修飾關係的可能性，因此不需要考慮句子複雜的結 構問題。 （三）、Twitter 與意見分析 Go 等人[7]對 Twitter 進行情緒分析，利用 SVM、Naïve Bayes 及 Maximum Entropy 等分類方法進行比較，並使用 n-grams 當作特徵進行分類器的訓練。他們訓練及測試的 資料是根據 tweet 中的表情符號來當作情緒分類的標準(正和負)。Pak 及 Paroubek[8]等 人根據形容詞的頻率和 Naïve Bayes Classifier 分類訊息，他們亦是使用訊息中的表情符 號當作參考答案。 根據以上的研究，因為 Twitter 並沒有提供情緒分類的語料庫，所以往往使用訊息 中的表情符號來當作訓練及測試資料的分類標準答案[2][9][10][11]。在本論文中，我們   3URFHHGLQJVRIWKH7ZHQW\)LIWK&RQIHUHQFHRQ&RPSXWDWLRQDO/LQJXLVWLFVDQG6SHHFK3URFHVVLQJ52&/,1* 利用人工標註方法來標記每筆資料的類別 (positive、negative 及 neutral)，如此能針對評 論者針對目標 (target)的意見傾向 (opinion orientation)。使用 n-grams 及字詞頻率當作訓 練特徵常常無法更進一步知道句子結構及語意，本研究方法專注在意見詞及評論目標的 修飾關係，能有效知道評論者真正想要評論的事件。 三、研究方法 （一）、系統架構 每則推文 (tweet)的內容可能包含心情、興趣以及對時事的評論等。本論文主要收 集使用者在 Twitter 中對主題發表的意見評論，根據意見詞與主題相關目標 (target)之間 的字詞修飾關係，精確計算出該主題之總評價。本方法共分為三個主要部分:資料收集、 前處理、意見詞修飾關係辨識，架構如圖一所示。 圖一、方法架構圖 如圖一所示，首先透過 Crawler 收集內容包含主題的 tweet (Twitter API[24])，並做 前置處理，如: 句子簡化，另外根據查詢主題的不同，我們利用主題相關資源 (topic-specific resource)進行目標發掘 (target finding)。以電影為例，我們從全球最大的 電影查詢資料庫 IMDB (The Internet Movie Database)中收集電影的相關作者、導演、演 員及類型等資訊。接著透過意見分析模組分析每則 tweet 內容是否含有對該主題相關目 標的評論，進而計算意見分數來判斷評價的正負面。 （二）、前處理 為了更容易找到訊息中的意見詞 (opinion word)及評論主題 (topic)，我們將與主題 相關的 tweet 做前置處理來達到簡化句子的目的。   3URFHHGLQJVRIWKH7ZHQW\)LIWK&RQIHUHQFHRQ&RPSXWDWLRQDO/LQJXLVWLFVDQG6SHHFK3URFHVVLQJ52&/,1* 1、 拼字檢查 在 Twitter 中，使用者往往不會太留意拼字的正確性，有時也會藉由單字來強調他 欲評論的事件，例如: “I lovvvvvvvvve this movie.”。然而錯誤的拼字可能會影響字詞修 飾關係的判斷，因此本論文使用 Google Spell Check[25]對每則 tweet 進行拼字的檢查， 在此部份我們是做純拼字錯誤的檢查，對於文法及字義使用不當並無做檢查。 2、 Stemming 由於名詞的單複數 (如 movie 和 movies)、詞性的變化 (如 good 和 goodness，動 詞的時態 (如 see 和 seeing)，導致語意大致相同的詞或字卻有不同的呈現方式，為了要 簡化句子的複雜程度，本研究使用 Porter Stemming algorithm 來進行字根還原的處理。 希望將這些後綴去除同時並不影響文字本身的意義，而且對於檢索查全率的提升也更有 幫助。 3、 特徵過濾 Tweet除了內容本文之外，還包含以下幾點特徵:  Username: 給一個Tweet的回覆或留言。用法為在 @ 符號後加對方的 Twitter ID ，一個空格或冒號後寫上回覆內容。例如，“@disc the tall man is such a good movie.”。  Links (url): 使用者常會在tweet中分享鏈結，例如:“That Blade Runner sequel is still happening. After seeing Prometheus, I was hoping everyone had forgotten about it. http://www.deadline.com/hollywood.”。  Retweet (RT): 就是轉推的意思，當你在Twitter上看到一個有意思的tweet，就可以 RT一下，以幫助傳播這條信息。用法為：RT @原始發布者Twitter ID: 被轉推的原 文。例如 : “RT if you like Titanic, Harry Potter, Twilight, Pitch Perfect, Skyfall, Life of Pi, Transformers, Les Miserables & etc.. :)”。 以上特徵並不影響使用者在tweet中欲表達的敘述內容，但會使得訊息內容複雜而影響 到意見分析的準確率，所以我們將這些特徵予以刪除，只留下敘述內容。 4、 詞性標註 (POS Tagging) 因為研究中須找出詞與詞中的修飾及對等關係，任何語言處理的系統都必須先分辨 文本中的詞才能進行進一步的處理，我們使用 Stanford POS Tagger[26]進行詞性標註。 （三）、意見分析 本章節說明意見分析的方法，主要分為三個步驟: 發掘相關的評論目標 (target expansion)，意見詞與評論目標修飾關係 (opinion words modification relation)，最後計算 句子的意見分數 (opinion Score estimation)來判斷句子的意見傾向 (opinion orientation identification)。   3URFHHGLQJVRIWKH7ZHQW\)LIWK&RQIHUHQFHRQ&RPSXWDWLRQDO/LQJXLVWLFVDQG6SHHFK3URFHVVLQJ52&/,1*  1. 評論目標發掘  雖然我們從Twitter中依照主題收集tweet，但tweet內容仍可能包含發文者對無關主 題的評論或敘述，例如:I went to theater to watch Argo yesterday. Ring Ring Ring! I was so humiliated when my phone rang out.，例子中屬於負面的情緒抒發，但評論的是因為電影 中電話響起而感到丟臉，此內容並沒有針對Argo這部電影做任何的評價及論述。 為了能精確計算出使用者在Twitter中對主題的評價，首先我們要找到tweet中使用 者可能在評論的事件，並且要確認此事件是否跟主題相關。例如: “I watched battleship last night, Rihanna’s acting is amazing.” 例子中，在講述battleship這部電影中演員的演技 很不錯，由此例可發現，發文者並不直接評論battleship，而是對演員的演技做好的評價， 這是一篇對battleship正面評價的tweet，因為演員的名字也是電影的屬性之一。目標 (target)為與主題高度相關的字詞，可能是同義詞或評論主題使用的字詞，本小節將介紹 我們找target的方法。  （1）、命名實體辨識  在本研究中以電影為例，因為演員、導演、編劇等人物都極可能是評論電影的網友 可能評論的目標，專有名詞的標記，可以解決詞庫涵蓋不足的問題，也因其牽涉到人、 事、時、地、物等重要內容，我們使用 Stanford Named Entity Recognizer[27]來做專有名 詞標記，主要是要找出 tweet 中可能出現跟電影有關的專有名詞，例如 : 演員、導演、 編劇、其他電影專業術語等。  （2）、共同出現關係 (Co-Occurrence)  我們在 Movie Review Data 文集[30]使用 PMI (Point-wise Mutual Information)處理詞 彙共同出現關係 (word collocation)，進而了解文集中使用者評論電影時最常提及的名 詞。我們利用此文集所有的名詞單獨出現次數 (term frequency, tf) 和與單字”movie” 共 同出現的詞彙組合 (emotion-words collocation pairs)出現次數，分別計算 PMI score 並依 照降冪排列，再從中取 PMI 最高的前 k 個詞彙共同出現的組合作為特徵子集合。最後 在實驗中評估 k 應該取幾個字來當作我們的 target。  PMI(w1,w2) =  (1)  如公式 1 所示， P(w1)和 P(w2)可以透過計算 w1 和 w2 個別出現的次數作為機率估 計值；而 P(w1,w2)代表 w1 和 w2 兩個字共同出現 (co-occurrence)的機率，可以透過計 算兩個字在文章中共同出現的次數作為機率估計值。  （3）、同義詞  在 英 文中，同一事物卻有很多 單字可以表達，例如 : 與 ”movie”同義的單字 有”film”、”show” 、”flick” 、”motion picture” 、” moving picture”等等。為了增加與主 題相關的 target 數量，我們使用 WordNet 來找出先前找到的 target 的同義字。    3URFHHGLQJVRIWKH7ZHQW\)LIWK&RQIHUHQFHRQ&RPSXWDWLRQDO/LQJXLVWLFVDQG6SHHFK3URFHVVLQJ52&/,1* 2. 詞性修飾關係 在找到可能的評論目標 (target)後，接下來要找到在句子中評論這些目標的修飾關 係，並根據情緒詞典比對，計算出對評論目標的情緒分數: （1）、修飾關係辨識 本方法在搜尋修飾關係時都是以句子為單位，多個單字放在一起可以表示出完整的 意思，且通常以標點符號將句子間做區隔。 一個句子的基本結構包含兩個重要的部分：主詞部分（subject group）和述語部分 （predicate group），亦即一個句子必須要有主詞和述語動詞。而意見詞常是動詞以及 形容詞[3]，所以本論文僅找句子中動詞及形容詞與目標的修飾關係[12][13]。為了要辨 別目標的評論與其他敘述，我們訂定以下修飾規則，若句子中包含以下的修飾關係，則 此句子可能含有對目標的評論。我們將意見詞的意見傾向及分數作為此修飾關係的意見 傾向及分數: 1. VB/ VBD/ VBG/ VBN/ VBP/ VBZ (意見詞，動詞) + T: T 為 target，也是動詞之後的 補語，顧名思義，就是針對動詞，再多作描述，補充動詞不足之處，表達出句子完 整的意思，補足方式通常是以名詞或代名詞作為動詞的受詞。例如:I love battleship.， ”love”是句子中的 VB，”battleship”是電影名字也是我們的 target，在此句子中就是 ”love”的受詞，所以符合我們的此項規則;因為”love”是屬於正面情緒詞，所以此句是 對電影”Battleship”是屬於正面的評價。 2. T + VB/ VBD/ VBG/ VBN/ VBP/ VBZ (意見詞，動詞): T 為 target，是句子中的主詞。 主詞之後若是動詞，那此動詞可能在描述主詞的行為或狀態。例如: The film bored me to death.，”film”是我們找到的 target，”bored”是句子中的 VB，因為” bored”是屬於 負面情緒詞，所以此句是屬於對電影的負面的評價。 3. T + VB/ VBD/ VBG/ VBN/ VBP/ VBZ + JJ (意見詞，形容詞): T 為 target，是句子中 的主詞。例如: This movie is worth seeing.，”movie”是我們找到的 target，”is”是句子 中的 VBZ，”worth”在句子中為 JJ，因為” worth”是屬於正面情緒詞，所以此句是屬 於對電影的正面的評價。 4. JJ (意見詞，形容詞) + T: 此項規則主要是找到 target 及修飾 target 的修飾詞。例如:It’s my favorite movie.，例子中，”movie”是 target，”favorite”是 JJ，也是修飾 target 的形 容詞，因為” favorite”是屬於正面情緒詞，所以此句是屬於對電影的正面的評價。 以上的修飾關係，都是尋找句子中距離最近的單字，例如: In the first movie Tony Curtis’s acting is amazing.，例子中找到第 3 種特徵 T + VBZ + JJ，但在 target finding 時 我們找到”movie”及”acting”兩個 target，”is”是 VBZ，這時會尋找與修飾詞 JJ ”amazing” 最近的字，也就是”acting”，最後找到的特徵就是”acting + is + amazing”。在此特徵中會 因為字與字在句子中的距離而影響正負面評價的分數，我們將在後面章節作介紹。 （2）、比較句 所謂「比較級」就是在雙方或兩者間做比較的表達方式，比較的內容當然就不外是   3URFHHGLQJVRIWKH7ZHQW\)LIWK&RQIHUHQFHRQ&RPSXWDWLRQDO/LQJXLVWLFVDQG6SHHFK3URFHVVLQJ52&/,1*  「形容詞」或「副詞」了，使用者在評論某一物件時常會以相似的物件來做比較，例如 : The picture quality of Camera-x is better than that of Camera-y.在例子中比較兩台相機的相 片畫質，這是最常見的比較關係。我們將常見的比較關係分為下列兩項[14][15]: 1. 非對等比較 (non-equal comparisons) : 物件間比較屬性的優劣，例如 : The VIA chip is faster than that of AMD.，例子中是最常見的比較物件屬性的優劣關係。又例如 : I prefer VIA to AMD.，此例子也是表達優劣關係。我們利用 POS tagger 來標記比較 級，標記為”JJR”、”JJS”、”RBR”、”RBS”，為形容詞及副詞的比較級，例如: Life was harder then because neither of us had a job.，例子中”harder”經過 POS tagger 標註 為”JJR”。再來尋找 target 與標記的相對位置:  Target + JJR / RBR (意見詞): 若 JJR / RBR 是正面情緒詞，則 tweet 對此 Target 是屬 於正面評價，若 JJR / RBR 是負面情緒詞，則 tweet 對此 Target 是屬於負面評價。  JJR / RBR(意見詞) + than + Target : 若 JJR / RBR 是正面情緒詞，則 tweet 對此 Target 是屬於負面評價，若 JJR / RBR 是負面情緒詞，則 tweet 對此 Target 是屬於 正面評價。例如: Why are books always better than the movie versions? ，例子中，若 movie 為 target，句中找到”better than”為形容詞比較級，屬於正面評價，但在此比 較關係中，”book”優於”movie”，所以對於”movie”是屬於負面的評價 2. 對等比較 (equal comparisons) : 比較的關係的程度或強弱是相等的，例如 : The picture quality of Camera-x is as good as that of Camera-y.，例子中是說明 Camera-x 的 相片品質與 Camera-y 一樣好。此用法是英文文法中的常用特定模式。若 tweet 內容 找到”as + 原級形容詞 + as”規則，若此原級形容詞為正面情緒詞，則此 tweet 對此 target 有可能是正面評價;反之，若此原級形容詞為負面情緒詞，則此 tweet 對此 target 有可能是負面評價。  （3）、否定詞  根據 Tottie[16]，英文的否定標記 (negative marker)主要分為三大類： (1) not 否定（not-negation） (2) no 否定（no-negation） (3) 詞綴否定（affixal negation） 否定標記的範例如表一所示： 表一、否定標記 ＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿  not-negation  no-negation  affixal negation  not  No  (im)perfect  nor  (ir)respective  none  (in)dependent  never  (un)able  neither  (non)functional  nowhere, nothing, nobody  meaning(less)    3URFHHGLQJVRIWKH7ZHQW\)LIWK&RQIHUHQFHRQ&RPSXWDWLRQDO/LQJXLVWLFVDQG6SHHFK3URFHVVLQJ52&/,1*  ＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿_______  由 Tottie 的定義中可以發現，not 否定和 no 否定基本上屬於語法的範疇，而詞綴否定  （affixal negation）則是在詞彙的範疇。在詞綴否定的部分在 SentiWordNet 能作適當的  辨別，例如 : perfect  positive ， imperfect  negative。  not-negation 與 no-negation 的處理[17]，先依照先前介紹的方法找到修飾關係及正負面  情緒，若是句子中含有 not-negation 與 no-negation 的字，則會反轉正負面結果，例如: I  don’t like this movie, the plot is so boring. 例子中，依照之前介紹的規則在” I don’t like  this movie,”句子中的”like this movie”找到 VB + Target 的修飾關係，屬於正面評價的句  子，但在句中找到”n’t”的否定詞，所以原屬正面評價的句子在最後會反轉成負面評價。  3. 意見評分  Tweets 在經過前章節的修飾關係特徵的搜尋，這些修飾關係會因為字與字之間的 距離而影響到情緒分數，例如: In the first movie Tony Curtis’s acting is amazing.，例子 中，找到”T + VBZ + JJ”的修飾關係”acting + is + amazing”，我們會計算意見詞與 target 之間的距離來調整修飾的權重。一則 tweet 中可能存在許多修飾關係的特徵，所以需要 經過正負面情緒分數的加總來判斷此 tweet 是屬於正面情緒或負面情緒，亦或是客觀論 述的 tweet。針對某句子 s，其情緒分數的計算如下:  score(s) =  ，  (2)  公式中，opj是句子 s 中的意見詞，T 為經由 target finding 所找到 target 的集合，d(opj,ti) 是在句子 s 中意見詞 opj 及 ti 的距離，so 是修飾字 opj 的情緒面向分數，由 SentiWordNet 得知。公式的 multiplicative inverse 是為了判斷修飾字在 修飾 target 的可能性，若距離 越遠則計算出的情緒分數越低。整篇 tweet 的分數即為所有句子情緒分數的總和。最後 依據分數將 tweet 分成三類:  正面(positive): score > 0。  負面(negative): score < 0。  客觀(objectivity): score = 0。  四、實驗與討論 （一）、測試資料收集 隨機挑選在 2013 年 2 月至 3 月上映的五十部電影收集其相關評論 tweet。因為不想 使資料過於集中在某一天，所以每隔 5 天收集一次。收集日期分別為 2013/3/20、 2013/3/25、2013/3/30、2013/4/5 及 2013/4/10，每日的收集量為 200 則 tweets，測試資料 總共 1000 則 tweets，接著使用人工標註每則 tweet 的情緒面向作為實驗的標準答案，由 5 人進行情緒標註，標註有三類:正面、負面、客觀。若是正面為+1，負面為-1，客觀為 0。最後依三種分數的個別加總，採多數決的方式來決定每則 tweet 的情緒面向。    3URFHHGLQJVRIWKH7ZHQW\)LIWK&RQIHUHQFHRQ&RPSXWDWLRQDO/LQJXLVWLFVDQG6SHHFK3URFHVVLQJ52&/,1*  （二）、實驗結果與討論  我們在進行研究方法中的 target expansion 的實驗與討論，最後比較本論文的方法與 SVM 及 Naive Bayes 分類方法的效果。分別會計算出正負面及主觀(subjectivity)評論的 精確率(precision)、查全率(recall)、F1 及準確率(accuracy)等數值來衡量方法效果。 其中 baseline 為未經過評論目標發掘，所以 target 只包含電影名字，藉此來比較評 論目標發掘方法的效果。  1. 命名實體辨識  Baseline 因為 target 過於稀少，使得 recall 都過於偏低，無法有效的辨識大部分有 關電影的評論。電影評論的 target 可能也包含人名及專有名詞，我們使用 Stanford Named Entity Recognition 工具擴增評論目標的數量。加入 NER 前後的分類效果如表二、表三 所示。 表二、加入 Named Entity Recognition 前後的主客觀評論分類效果比較  主觀分類  Baseline  with Named Entity  Improvement (%)  Recognition  Recall  0.38173  0.40315  5.6%  Precision  0.89247  0.90671  1.6%  F score  0.53474  0.55814  4.4%  Accuracy  0.57438  0.59594  3.8%  雖然使用 Named Entity Recognition 會增加電影相關的導演、演員等的名字及專有名詞， 但也會使與電影無關的名字及專有名詞也會納入 target，但因為我們根據電影名稱去收 集 tweet，所以大部份的 tweet 內容都是在評論電影，如表二所示，在判斷主客觀評論會 因為 target 的增加在 precision、recall 及 accuracy 都有提升。  表三、加入 Named Entity Recognition 前後的正負面評論分類效果比較  正負面評論  
This paper analyzes synonym groups appearing in fixed frames containing Chinese locative phrases such as [zái noun phrase (yi/zhi) shàng/xià/etc. bian/miàn/etc.] by using statistical methods. We collected locative phrases from Sketch Engine using 11 monosyllabic locative words and 5 locative compound-formation patterns, and we aligned these compounds with Chinese Synonym Forest [1] before clustering. Different noun phrases were mapped to their collocating synonym groups to as to enable mutual information comparisons between different combinations. When analyzing concept combinations, we used point-wise mutual information to compare two synonym groups, and adopt multivariate mutual information 198  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013)  (MMI)[2] to examine three groups. The results showed that behaviors of using suffixes and prefixes to forming locative nouns in different context (combination of 1 or 2 top level synonym groups), and the statistic results can be used in further analyzing locative nouns in different fields. 關鍵詞：中文方位詞，同義詞詞林，互斥資訊，多變數互斥資訊 Keywords: Chinese Locative Nouns, Chinese Synonym Forest, PMI, MMI.  一、緒論 方 位 名 詞 表 達 了 從 某 個 參 考 物 件 或 事 項 而 產 生 的 方 向 資 訊 。 在 中 文 裡 ， Li 與 Thompson[3]指出方位名詞主要是以下列的方式出現： 在 名詞片語 ~ (方位名詞單元) 在這個結構之中，方位名詞可以是單音字或是雙音字的組合。單音字如上/下、前/後、 左/右、裡/外、東/西、南/北及內/中等；雙音字組合則是前述的單音字搭配以與之作為 前飾詞，或是邊、面與頭做為後綴詞。然而，並非所有的組合在表達方向時都會被使用 到。依據盛玉麒在《現代漢語網絡課程》[4]中的方位詞分析(如下表一)，在 14 個方位 詞與五種前飾/後綴詞的組合並非經常被用到(或不會被用到)。  表 1 中文方位多音詞組合表  後綴詞  前飾詞  ~邊  ~面  ~頭  以 ~  之 ~  上  上邊  上面  上頭  以上  之上  下  下邊  下面  下頭  以下  之下  前  前邊  前面  前頭  以前  之前  後  後邊  後面  後頭  以後  之後  左  左邊  左面  N/A  N/A  N/A  右  右邊  右面  N/A  N/A  N/A  裡  裡邊  裡面  裡頭  N/A  N/A  外  外邊  外面  外頭  以外  之外  東  東邊  東面  東頭  以東  之東  西  西邊  西面  西頭  以西  之西  南  南邊  南面  南頭  以南  之南  北  北邊  北面  北頭  以北  之北  內  N/A  N/A  N/A  以內  之內  中  N/A  N/A  N/A  N/A  之中  許多研究也從不同的觀點對中文方位進行討論。如從參照框架(Frames of Reference)的概 念進分析「上」[5]與「前」[6]方位詞的特性，及依意象圖式(Image Schema)來探討《詩  199  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 經》中的方位詞「下」[7]與足部動作詞的空間隱喻[8]。上述的研究都只局限在單一方 位名詞的探討，並不能將前述表一之中各項方位詞組合進行綜合比較，因此無法較全面 了解各方位詞的組合之間有何差異。 本研究中，我們接續先前的研究[9]從 Sketch Engine 裡收集在中文十億字語料庫(Chinese Giga-Word Corpus1)[10]中包括出現在表一裡的各種組合的短語(在此，我們稱為方位短 語)，並且將所收集得到的短語切段(segmentation)為詞組後，再透過同義詞詞林[1]轉成 其知識架構中的同義詞組代號。在先前的研究之中，我們希望透過視覺化的查詢工具， 以呈現較為明顯的詞群，其中包括了較高出現率(High Frequency)與分群鑒別率(Cluster Discrimination)。在此我們採取不同於先前研究的分析策略，旨在了解詞組所轉換詞群 關係間的相依關係。首先依詞組多寡使用不同互斥資訊(Mutual Information)的計算原 則，以分析在語料庫中每一方位短語裡所存在的知識概念組合模式。在計算互斥資訊 時，因多變數(由三個詞組所構成的三個概念間)計算不能直接使用兩變數(兩個概念)的 互斥資訊計算原則，從而我們使用多變數互斥資訊[2]來計算。研究結果除了可以提供 在同義詞詞林中，知識層級較高的同義詞組在不同的方位短語裡的常見出現模式，亦可 擷取常見的中間層級同義詞組在方位短語的使用情況。為了避免混淆，在本文之中所使 用的字句單元大小關係，我們定義為如下：「方位詞」(如上下、左右…等)，是組成「方 位詞組」的重要單元；「方位(名)詞組」可以是單音詞的「方位詞」，或是與前飾/後綴詞 組成的多音詞；「方位詞短語」則是符合 Li 與 Thompson[3]所指結構的短語；最後「方 位短句」則是由 Sketch Engine 所取得的符合搜尋結果，其句中雖然包括「方位詞短語」， 但因受系統限制無法取得完整句子。 本篇論文的架構如下：在第二節，我們先回顧互斥資訊計算的相關原則與方法，並說明 同義詞詞林的知識概念架構；接著我們報導整個研究過程，其中包括資料收集、處理、 同義詞概念轉換與互斥資訊相關計算；在第四節中，我們將研究結果則以高階層知識概 念進行報告與討論；最後是結論與討論。 二、文獻探討 在這節中，我們說明互斥資訊計算的相關原則與同義詞詞林的內容。互斥資訊計算是本 研究中用來評估概念之間的相互關係計算原則，而同義詞詞林則是參考其具系統架構知 識分類，以協助我們了解在方位詞短語之中的概念組成原則。 （一）互斥資訊 Mutual Information （1）PMI, Point-wise Mutual Information 從訊息理論(Information Theory)所延用而來的互斥資訊計算原則，是指兩發生事件之間 的相關性參考指標。在此事件則是指某單一詞或是單一知識概念(於同義詞詞林之中的 同義詞代號)出現在句子之中的情況。例如從 Chinese Giga-word Corpus 中取得的例子“在 /P21 國家_Na 的_DE 邊界/Ncb 之外/Ng”，我們則稱在句子中可以「找到邊界一詞出現 在句子中」的事件發生。接著我們將所有在語料庫中，包括“之外”方位詞組的 41612 條 短句進行統計(此數字為 Sketch Engine 回傳的符合搜尋條件的資料總數)，且逐一計次 後了解“邊界”出現的次數共計有 25 次，我們便可使用條件機率概念表示此事件 -Cb14A01(Cb14A01, 為同義詞詞林裡的同義詞群代號，於下 2.2 節中說明。)發生在包 
Spelling error is broadly classified in two categories namely non word error and real word error. In this paper a localized real word error detection and correction method is proposed where the scores of bigrams generated by immediate left and right neighbour of the candidate word and the trigram of these three words are combined. A single character position error model is assumed so that if a word W is erroneous then the correct word belongs to the set of real words S generated by single character edit operation on W. The above combined score is calculated also on all members of S. These words are ranked in the decreasing order of the score. By observing the rank and using a rule based approach, the error decision and correction candidates are simultaneously selected. The approach gives comparable accuracy with other existing approaches but is computationally attractive. Since only left and right neighbor are involved, multiple errors in a sentence can also be detected ( if the error occurs in every alternate words ). Keywords: Real word error, Local context. 1. Introduction Word error is a major hindrance to the real world applications of Natural Language Processing. In textual documents, word-error can be of two types. One is non-word error which has no meaning and other is real word error which is meaningful but not the intended word in the context of the sentence. Of these, non-word has been widely studied and algorithms to detect and suggest correction word for the error have been proposed. These algorithms are generally termed as spell-checker, which are integrated in various wordprocessing software like Microsoft Word1, LibreOffice Writer2, Ispell3, Aspell4 etc. For error occurring at two positions of a word, the commercial spell checkers work fairly well. Some studies on spell checking approaches are found in [1-5], that include English and non-English language like Bangla. However, the problem of real-word error is a more complex one. Usually, such error disturbs the syntax and semantics of the whole sentence, which requires human-being to detect it. However, an automatic syntactic/semantic analysis of a 'correct' sentence itself is a difficult 
This paper aims to seek approaches in investigating the relationships within emotion words under linguistic aspect, rather than figuring out new algorithms or so in processing emotion detection. It is noted that emotion words could be categorized into two groups: emotion-inducing words and emotion-describing words, and emotion-inducing words would be able to trigger emotions expressed via emotion-describing words. Hence, this paper takes the social network Plurk, the emotion words are from the study on Standard Stimuli and Normative Responses of Emotions (SSNRE) in Taiwan and the National Taiwan University Sentiment Dictionary (NTUSD) as corpus, combining with Principle Component Analysis (PCA) and followed collocation approach, in order to make a preliminary exploration in observing the interactions between emotion-inducing and emotion-describing words. From the results, it is found that though the retrieved Plurk posts containing emotion-inducing words, polarities of the induced emotion-describing words contained within the posts are not consistent. In addition, the polarities of posts would not only be influenced by emotion words, but negation words, modal words and certain content words within context. Keywords: sentiment analysis, emotion word, collocation. 1. Introduction Sentiment analysis has recently become a prevalent trend in the field of natural language processing, and has wide applications for industry, policy making, sociology, psychology and so on. Various approaches have been proposed with impressive experimental or computational evidence, from document-level analysis to sentence-level or even phrasal-level analysis [1]. Among most studies, the Sentiment/Emotion-labeled Lexicon is taken as an indispensible lexical resource for the improvement of emotion classification accuracy. However, by assuming the static correspondence of word-emotion, most studies have neglected the fact that emotion words are not fixed with specific valence but are influenced under diverse contexts. On account of contextual effects of emotion, [2-4] have firstly introduced a notion inspired by cognitive linguistics - emotion cause event - that refers to “the explicitly expressed arguments or events that trigger the presence of the corresponding emotions.” A set of 236  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) linguistic cues is proposed to detect the cause events, resulting a valuable corpus resource for the task of emotion classification. Despite that there are some explicit causers that might trigger emotions via context, a recent large-scaled interdisciplinary emotion research project [5, 6] has focused on the emotion words and found that they do help capture the emotion perceptions [7], and can thus be employed in emotion-related processing tasks. As designed in [5, 6] emotion words can be further grouped into emotion-inducing (情緒誘發詞) emotion-describing words (情緒描述 詞 ). Emotions are mainly divided into two polarities: positive and negative. Emotion-inducing words encode the underlying repository knowledge to be able to elicit emotion-describing words. Therefore, in this study, we assume that the emotion-inducing word can be treated as the pivot in emotion detection of the sentences, and the way the emotion-inducing word interacts with its collocational context would be the key to a deeper understanding of emotional processing in texts. Instead of seeking new approaches and algorithms in emotion detection, this paper aims to emphasize on seeking other possibilities in context-based emotion detection through investigating the relationships of emotion polarity between emotion-inducing and collocated content words. We carry out an exploratory data analysis with the assistances of programming technique and linguistic resolution on data inspection, in order to make prediction on the potential underlying linguistic cues within emotions embedded in context. Since taking web as corpus is convenient for its easy access and availability of voluminous data, one of the popular social network in Taiwan, Plurk, is considered in our study. 2. Literature Review 2.1 Emotion Classes Constructing a gold standard emotion classification has long been an unsolved issue among various research fields, such as philosophy [8, 9] biology [10], linguistics [11, 12], neuropsychology [13] and computer science [14, 15]. Regardless of the disagreement and not having consensus on one emotion class, some parts of emotions are widely shared amid diverse emotion classes proposed by previous studies [16-18], which are happiness, sadness, fear, and anger. However, since our study is based on the approach of [2], we simply follow the five emotion classes adopted in the paper, which the emotion classification is firstly presented by [18] happiness, sadness, fear, anger, and surprise. 237  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 2.2 Emotion Words in Context In sentiment analysis, the fact that emotion of a word changes based on contexts has been mostly neglected, which might lead to diverse polarities. Thus, in recent studies, researchers started to take this issue into consideration while exploring word sentiments. In addition, since words may contain various senses and further evoke diverse emotions based on contexts, the need of a list of emotion lexicon would be practical and could be applied to a number of purposes. [19] introduced the approach of using Mechanical Turk provided by Amazon's online service of crowdsourcing platform for a large amount of human annotation on numerous linguistic tasks [20, 21]. To be more specific, emotion lexicon or also known as emotion words that are covered in sentiment detection and classification (for example, happy, sad, angry and so on) are mostly emotion-describing words, which are words that directly express and describe emotions. On the other hand, for words that have the potential to evoke or arouse emotions under context, are grouped as emotion-inducing words, such as holiday, homework, weekend, Monday and so on. Since emotion-inducing words contain certain underlying implicit linguistic cues to evoke emotions, many studies work on different approaches to inspect the context-based emotion words. For example, [22] uses the technique of crowdsourcing and Mechanical Turk method to help annotate the lexicon that have the possibility to evoke emotions, and evaluate the results with inter-annotator agreement. Other studies take the emotion cause event to help figure out the causers of emotions within context. As mentioned by [23], a cause of an emotion is suggested to be one event. Therefore, a cause event could be referred to a cause that could immediately trigger an event, as stated by [4]. [3] expresses in the paper that emotion cause detection is one of cause event detection, therefore some typical patterns that are used in cause event detection, such as because and thus, could be applied to emotion cause detection. Additionally, they have included some manually and automatically generalized linguistic cues to further explore emotion cause detection. In this study, the experimental results of Chinese emotion word list in [5] are included, which obtain the valences (from 1 to 9) of word polarities in both emotion-describing and emotion-inducing words, in order to investigate whether given an emotion-inducing word along with context could the sentiment prediction model envision its possible evoked emotions presented via emotion-describing words. 238  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 3. Methodology In order to investigate the implicit linguistic cues that might shift the polarities of emotion words, the analysis by applying Principle Component Analysis (PCA) and collocation of emotion-inducing words are considered. Through PCA, the distribution and relationships between emotion-inducing and emotion-describing words could be revealed and presented visually via the powerful plots in R. In addition, since PCA tends to exhibit the groups of emotion words that might have strong interactions between emotion-inducing and emotion-describing words, the approach by inspecting the collocations of emotion-inducing words would help figure out the linguistic cues that might lead to the interactions in context. Three materials taken in this study include Plurk corpus, emotion words from the study on Standard Stimuli and Normative Responses of Emotions (SSNRE) in Taiwan, and National Taiwan University Sentiment Dictionary (NTUSD, [24]). 3.1 Material Like Twitter, Plurk is one of most popular social networks and micro-blogging service in Taiwan. Since Plurk can be easily and freely accessed through Plurk API 2.01 and along with its enriched emoticon information, a total of 43959 posts has been retrieved and used in this study. Regarding the emotion words adapted from the project SSNRE, these words are categorized into two groups: emotion-inducing words and emotion-describing words. While the emotion of emotion-inducing words is recessive and needs to be triggered by the context, the emotion of emotion-describing words is dominant and exists in its semantic sense. That is, although emotion-inducing words have explicit polarities in experimental results, its polarities will be affected by the context, such as emotion-describing words in the same sentences. Based on the changeable polarities of emotion-inducing words, the paper treats emotion-inducing words as the target of observation. In the study of SSNRE, 395 emotion-inducing words and 218 emotion-describing words has been underwent three psychological experiments with a 9-point likert scale, which includes four to six perception parameters. In the 9-point likert scale, the number 9 refers to the greatest positive emotion; whereas, the number 1 indicated the most awful negative emotion. That is, emotion words that are more than five points would belong to positive emotion and those lower than five points would be assigned as negative emotion. Within the 395 emotion-inducing words, 140 words are with positive emotion and 255 words are with negative emotion; as to the 218 emotion-describing words, there are 58 words tagged as positive emotion and 160 words tagged as negative emotion. Since emotion-inducing words are to induce and trigger emotions, we assume that if a sentence 
PTT (批踢踢) is one of the largest web forums in Taiwan. In the last few years, its importance has been growing rapidly because it has been widely mentioned by most of the mainstream media. It is observed that its influence reflects not only on the society but also on the language novel use in Taiwan. In this research, a pipeline processing system in Python was developed to collect the data from PTT, and the n-gram model with proposed linguistic filter are adopted with the attempt to capture two-character neologisms emerged in PTT. Evaluation task with 25 subjects was conducted against the system's performance with the calculation of Fleiss’ kappa measure. Linguistic discussion as well as the comparison with time series analysis of frequency data are provided. It is hoped that the detection of neologisms in PTT can be improved by observing the features, which may even facilitate the prediction of the neologisms in the future. Keywords: PTT, Neologisms, n-gram, Fleiss’ kappa, Time series analysis 1. Introduction A neologism in general refers to “a newly coined term, word, or phrase, that may be in the process of entering common use, but has not yet been accepted into mainstream language” (Levchenko, 2010)1. It is closely related to the unknown words or out-of-vocabulary in the field of Speech and Natural Language Processing, but with the nuance that the latter is often formally defined by its non-existence in a given vocabulary repository. With the emergence of voluminous data on the web and fast-developing technologies, never before has our world been facing with such an overwhelming mass of neologisms. Therefore, the description and 
Cerebral palsy (CP) is a developmental motor disorder and the study of the speech characteristics and developmental speech patterns may provide valuable information on early speech development. Vowels appear early in speech development and they are central to the understanding of the acoustic properties of speech. Therefore, the current study aimed to examine the differences of vowel formant frequencies among five children with cerebral palsy in different severity ranging from ages 3 to 7. First and second vowel formants (F1 and F2) were measured to investigate: 1) the changes of the F1 and F2 values, 2) vowel space, and 3) the vowel space area in CP children of different ages and severity. The major findings are: 1) There was no obvious decline in F2 values from 3 to 7 years old, which indicated delayed speech development; 2) The overlapping ellipses of all vowel spaces illustrated unstable motor control in all the five children; and 3) The five CP children had centralized corner vowels and there was no expansion of vowel spaces at different ages. This indicated their limited motor control. Keywords: vowel formant frequencies, vowel space, cerebral palsy, Mandarin-speaking 
The aim of this paper is to propose a system, which can automatically infer entailment relations of textual pairs. SVM is utilized as a prediction model of the system and seven features of textual pairs are employed to be input of the prediction model. The performance of this system is evaluated by dataset in CT-MC task held by RITE-2 of NTCIR. Macro-F1 of the proposed method is 46.35%. 關鍵詞：蘊涵關係，自動推論，支援向量機 Keywords: Textual Entailment, Automatic Inference, Support Vector Machine. 一、緒論 文本對蘊涵關係判讀是自然語言處理領域一個有趣且有意義的問題，其可能的應用也十 
In the beginning, search engines provide placements next to the original search results for advertisers on specific keywords. Since users often search for their interests or purchasing decision, timely presenting proper advertisements to users will encourage them to click on search ads. With the rapid growth of advertising, there is a bidding mechanism that advertisers need to bid keywords on their ads. They should carefully compose keywords in order to enhance the opportunity for their ads to be clicked. Until now, how to efficiently improve the ad performance to earn more clicks remains a main task. In this paper, we focus on the scope of smart phone and produce a social intentional model with advertising based features to forecast future trend on ads’ click-through rate (CTR). In terms of social intentional model, we analyze Chinese text content of technology forum to derive social intentional factors which are Hotness, Sentiment, Promotion, and Event. Our results indicate that with knowing public opinions or occurring events beforehand can efficiently enhance click prediction. This will be very helpful for advertisers on adjusting bidding keywords to improve ad performance via social intention. Keywords: Advertising, Sponsored Search, Click-Through Rate, Social Intention. 1. Introduction For online search advertising, the well-known search engines such as Bing, Google, and Yahoo! enable ads to be shown on the top banner or alongside the search results. This generates most of the revenue for search engines. The most common mechanism is cost-per-click (CPC), which means the advertiser bid on keywords but only be charged for each user click on the ad. Both search engines and advertisers look forward to enhancing the ad’s click-through rate (CTR), which indicates the probability of the number of ad clicks divided by the number of ad impression. The ad position is on the basis of the ranking score which is computed by the multiplication of CPC and ad quality score. The ad quality depends on plenty of factors that cause an ad to be clicked like ad’s keywords, historical CTR, title, description, display URL, landing page, etc. Moreover, CTR is an important and direct metric 278  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) for measuring advertised performance. This paper will focus on forecasting ad keyword’s CTR trend, since different bidding keywords in the same ad have various CTR values. Target on the popular 3C products: smart phones, we use the public information from technology forum to predict ups and downs of the next day CTR. As [1] statistics, the top three most important factors influencing consumer choice of mobile phones are: innovative features, recommendation and price. We extend these criteria as following factors: Hotness, Sentiment, Promotion, and Event. All these factors may affect ad’s future CTR as Figure 1 depicted. For example the releasing news, a kind of events, may trigger users search on search engine or forum to look for product comments in detail. Users may click more on ads while the ads containing promotion terms or the promotion news is releasing. Figure 1. The impact on CTR from releasing news to people reaction The purpose of this study is to predict and analyze which factors that affect ad keywords’ CTR in the next day. This work could previously inform advertisers of user intention on product keywords and assist them to judge whether to change ad strategy or not. It appears that research has not yet been available concerning the effect on search advertising from forum opinions. We expect that this work could significantly aid advertisers in advertising production. 279  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 2. Related Work Even now, there are still lots of research on improving advertising performance in order to verify which features could probably affect ad clicks. We will introduce some related researches that predicted clicks on search advertising; moreover, for the same predicting task but in different research domain, there exist some studies that use public mood to predict the stock trend in the stock market. 2.1 Traditional click prediction problem Regelson and Fain [2] claimed that historical click information provides tangible examples of user behavior. To predict future click-through rate by term level, for those terms with low frequent or completely novel terms, they use hierarchical clusters of related terms to compute. Apart from terms, Richardson et al. [3] suggested that adding features of ads, and advertisers can accurately predicts the click-through rate for new ads. The collected information of ads contained landing page, bid terms, title, body, display URL, clicks, and impressions (views). User intentions may significantly vary in the same query. Guo et al. [4] develop a fine-grained user interaction model for inferring searcher receptiveness to advertising. They modified the Firefox version of the OpenSource LibX toolbar to instrument mouse movements and other user action events on search result pages. Cheng and Cantú-Paz [5] develop demographic-based and user-specific features that reflect the click behavior of groups and individuals. To strengthen the relation between query and ad, Dave and Varma [6] proposed a similarity method to give prediction. Especially for those rare/new ads, they used cosine similarity between two queries or two ads. Xiong et al. [7] designed a continuous conditional random fields (CRF) based model, which considered both features of an ad and its similarity to the surrounding ads. 2.2 Using social media for prediction The prediction problem on trend is analogous to click prediction. Bollen et al. [8] first used six dimensions of mood(tension, depression, anger, vigor, fatigue, confusion) from Profile of Mood States(POMS), a well-established psychometric instrument to observe the relation between moods and socio-economic phenomena. After that, Bollen et al. [9] expanded terms of POMS from Google webpages, named it GPOMS. GPOMS contained six different mood dimensions: Calm, Alert, Sure, Vital, Kind, and Happy. They used Granger causality analysis to investigate the hypothesis of public mood states and a Self-Organizing Fuzzy Neural Network to predict the daily up and down changes of Dow Jones Industrial Average (DJIA) in the stock market by the OpinionFinder and GPOMS mood time series. 280  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 3. Method In this section, we present our proposed framework as shown in Figure 2 to address the problem of predicting ad keyword CTR via adding social phenomena. In brief, given an ad keyword as an input, our system returns the direction of movement in the next day based on previous advertising data and social intention effects. First, in advertising-based part, we do the CTR filtering to be basic information on an ad keyword. Next, before running the main process, the social intentional factors have been built from historical public behaviors on technology forum. After that, we crawl the related articles on technology forum in recent time duration to calculate social intentional scores. Thus, with these two-part values, we can run the prediction model in the last process. The results are produced from Linear Regression model and SVM classification model. Figure 1. The Dynamite Figure 2. Proposed framework According to user preference on purchasing, we propose four extracting methods to produce Hotness, Sentiment, Promotion, and Event that may be sufficient to affect user click on ads. The data used for methods in this part is Mobile01 articles from November 1, 2012 to January 31, 2013 which contains 21,674 articles. In the following parts, we will introduce these methods with Mobile01 articles in detail. - Hotness The “Hot” means feverish, to become lively or exciting1 that can informal arouse 
In fact, most people have had the experience that they haven’t made detailed itinerary in advance before a journey, and as a result they don’t know what place or what kind of activity is suitable as the next visit location and activity after they engage in an activity in a certain place. To alleviate such problem, in this paper, we proposed the Consecutive Itinerary Matching Model to help mobile users find next locations and activities in line with their leisure needs. This model effectively utilizes time, location, user, and activity as features to find the most possible “Consecutive Itinerary” and then recommend mobile users next locations and activities. In this preliminary study, although our approach achieved only about 30% top-1 inclusion rate, however, to our knowledge, this work is novel for the recommendation of location and activity based on consecutive itinerary discovery from check-in data. 關鍵詞：地點推薦，活動推薦 Keywords: Location Recommendation, Activity Recommendation 1. Introduction In fact, most people have had the experience that they haven’t made detailed itinerary in advance before a journey, and as a result they don’t know what place or what kind of activity is suitable as the next visit location and activity after they engage in an activity in a certain 288  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) place. To alleviate such problem, in this paper, we intend to propose an effective method to help mobile users find next locations and activities in line with their leisure needs. For example, after somebodies watches a film in the cinema, we can recommend them to go bowling next. In the last few years, when people go places, the common thing for them to do is that using Facebook to check in to the places and let their friends know exactly where they are and what they’re doing. Check-in data is a new and useful resource for our work of finding next locations and activities for mobile users. In addition, many bloggers describe travel itineraries in their blog articles which are really worth discovering. Thus, to recommend effective next potential location-activity pair to mobile users, our idea is to utilize these two kinds of resources, check-in data and travel blogs. In this study, we collect and analyze a large number of travel itineraries from these two resources, and then use these data to train the Consecutive Itinerary Matching Model (CIMM). This model uses time, location, activity, and user information in check-ins and travel blogs as features to find the most possible consecutive itinerary associated to a user’s current location and activity, and then recommends him next locations and activities. The example of recommending mobile users next location and activity based on the consecutive itinerary discovery from check-ins and travel blogs is shown in Figure 1. We collected a large number of check-ins and travel blogs, and extracted a little check-in information to make useful consecutive itineraries. A consecutive itinerary includes arrival time, user gender, user age, current location, current activity, next location and next activity. Based on the extracted consecutive itineraries, we can train a CIMM to effectively recommend mobile users next locations and activities associated with their current locations and activities. To our knowledge, this method may be an innovative and useful technique. Figure 1. The example of recommending mobile users next location and activity based on the consecutive itinerary discovery from check-ins and travel blogs. 289  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 2. Related Work Location recommendation always is a popular topic. Conventional location recommendations usually collected user's past GPS path and used data mining techniques to find user's moving trajectory, and then give the corresponding location recommendations. Based on multiple users’ GPS trajectories [1], Zheng et al. aimed to mine interesting locations and classical travel sequences in a given geospatial region. Morzy [2] used the past trajectory of the object and combines it with movement rules discovered in the moving objects database for predicting the location. Furthermore, he also proposed a probabilistic model of object location prediction [3]. Monreale et al. [4] proposed a location predictor WhereNext, which uses the locations visited by users to build a decision tree, and finds users’ trajectory patterns, and then uses the trajectory patterns to predict the next location. Location-based Social Networks (LSNs) have become extremely popular. Recently, people try to use the location records of LSNs to recommend location. Berjani and Strufe [5] proposed a recommendation scheme based on regularized matrix factorization (RMF). They collected locations and users, and mapped them to an n-dimensional space, and then calculate the inner-product between user and location to recommend location. This study is characterized by mapped users and locations to the same two-dimensional space. The difference between our paper and this study is that we also consider the time factor and the user's personal characteristics. Ye et al. [6] developed a friend-based collaborative filtering (FCF) approach for location recommendation based on collaborative ratings of places made by social friends. Moreover, they proposed a variant of FCF technique, namely Geo-Measured FCF (GM-FCF), based on some heuristics derived from observed geospatial characteristics. First, they use the distance between a user and his friends to calculate similarity. Second, according to the score of location given by a friend and the friend’s similarity, they can calculate the recommend score between a user and location. Finally, they select top n locations to recommend. Using friendship to recommend locations is the main idea in this study. Wei [7] used data mining techniques based on the location information of LSNs to find user’s trajectory patterns, and then utilized the trajectory patterns to recommend locations. The advantage of this study is to explore a few useful features of location. The difference between our work and conventional location recommendation work is that we not only recommend activities as well as locations. Besides it, mining useful information like consecutive itinerary from check-in data is a new and important research direction. 3. Method 3.1 Observation of Consecutive Itinerary  Check-in Data As mentioned before, many mobile users usually tend to leave check-in records when arriving tourist attractions or interesting spots. Therefore, we collected a large number of different mobile users’ check-in data from Facebook, and then listed a sequence of check-ins for each user according to their arrival time. As a result, we often can find a number of interesting itineraries for the user based on his sequential check-ins. The observation of sequential check-ins inspired us to discover interesting itineraries from 290  Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) 
Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings. These ambiguities can still be found when language is restricted to a particular domain, such as biomedicine. Word Sense Disambiguation (WSD) systems attempt to resolve these ambiguities but are often only able to identify the meanings for a small set of ambiguous terms. DALE (Disambiguation using Automatically Labeled Examples) is a supervised WSD system that can disambiguate a wide range of ambiguities found in biomedical documents. DALE uses the UMLS Metathesaurus as both a sense inventory and as a source of information for automatically generating labeled training examples. DALE is able to disambiguate biomedical documents with the coverage of unsupervised approaches and accuracy of supervised methods. 
Effectively exploring and analyzing large text corpora requires visualizations that provide a high level summary. Past work has relied on faceted browsing of document metadata or on natural language processing of document text. In this paper, we present a new web-based tool that integrates topics learned from an unsupervised topic model in a faceted browsing experience. The user can manage topics, ﬁlter documents by topic and summarize views with metadata and topic graphs. We report a user study of the usefulness of topics in our tool. 
TMTprime is a recommender system that facilitates the effective use of both translation memory (TM) and machine translation (MT) technology within industrial language service providers (LSPs) localization workﬂows. LSPs have long used Translation Memory (TM) technology to assist the translation process. Recent research shows how MT systems can be combined with TMs in Computer Aided Translation (CAT) systems, selecting either TM or MT output based on sophisticated translation quality estimation without access to a reference. However, to date there are no commercially available frameworks for this. TMTprime takes conﬁdence estimation out of the lab and provides a commercially viable platform that allows for the seamless integration of MT with legacy TM systems to provide the most effective (least effort/cost) translation options to human translators, based on the TMTprime conﬁdence score. 
Anafora is a newly-developed open source web-based text annotation tool built to be lightweight, ﬂexible, easy to use and capable of annotating with a variety of schemas, simple and complex. Anafora allows secure web-based annotation of any plaintext ﬁle with both spanned (e.g. named entity or markable) and relation annotations, as well as adjudication for both types of annotation. Anafora offers automatic set assignment and progress-tracking, centralized and humaneditable XML annotation schemas, and ﬁlebased storage and organization of data in a human-readable single-ﬁle XML format. 
This paper presents a web application and a web service for the diagnostic evaluation of Machine Translation (MT). These web-based tools are built on top of DELiC4MT, an opensource software package that assesses the performance of MT systems over user-deﬁned linguistic phenomena (lexical, morphological, syntactic and semantic). The advantage of the web-based scenario is clear; compared to the standalone tool, the user does not need to carry out any installation, conﬁguration or maintenance of the tool. 
 Hand gesture-based input systems have been in active research, yet most of them focus only on single character recognition. We propose KooSHO: an environment for Japanese input based on aerial hand gestures. The system provides an integrated experience of character input, Kana-Kanji conversion, and search result visualization. To achieve faster input, users only have to input consonant, which is then converted directly to Kanji sequences by direct consonant decoding. The system also shows suggestions to complete the user input. The comparison with voice recognition and a screen keyboard showed that KooSHO can be a more practical solution compared to the existing system. 
UMLS::Similarity is freely available open source software that allows a user to measure the semantic similarity or relatedness of biomedical terms found in the Uniﬁed Medical Language System (UMLS). It is written in Perl and can be used via a command line interface, an API, or a Web interface. 
We present KELVIN, an automated system for processing a large text corpus and distilling a knowledge base about persons, organizations, and locations. We have tested the KELVIN system on several corpora, including: (a) the TAC KBP 2012 Cold Start corpus which consists of public Web pages from the University of Pennsylvania, and (b) a subset of 26k news articles taken from English Gigaword 5th edition. Our NAACL HLT 2013 demonstration permits a user to interact with a set of searchable HTML pages, which are automatically generated from the knowledge base. Each page contains information analogous to the semi-structured details about an entity that are present in Wikipedia Infoboxes, along with hyperlink citations to supporting text. 
We introduce an efﬁcient, interactive framework—Argviz—for experts to analyze the dynamic topical structure of multi-party conversations. Users inject their needs, expertise, and insights into models via iterative topic reﬁnement. The reﬁned topics feed into a segmentation model, whose outputs are shown to users via multiple coordinated views. 
In this paper we revisit the task of quantitative evaluation of coreference resolution systems. We review the most commonly used metrics (MUC, B3, CEAF and BLANC) on the basis of their evaluation of coreference resolution in ﬁve texts from the OntoNotes corpus. We examine both the correlation between the metrics and the degree to which our human judgement of coreference resolution agrees with the metrics. In conclusion we claim that loss of information value is an essential factor, insufﬁciently adressed in current metrics, in human perception of the degree of success or failure of coreference resolution. We thus conjecture that including a layer of mention information weight could improve both the coreference resolution and its evaluation. 
Annotated corpora play a signiﬁcant role in many NLP applications. However, annotation by humans is time-consuming and costly. In this paper, a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes. We demonstrate the effectiveness of our approach in the context of one form of unbalanced task: annotation of transcribed human-human dialogues for presence/absence of uncertainty. In two data sets, our cost-matrix based method of uncertainty annotation achieved high levels of recall while maintaining acceptable levels of accuracy. The method is able to reduce human annotation effort by about 80% without a signiﬁcant loss in data quality, as demonstrated by an extrinsic evaluation showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations. 
In this paper we propose an automatic term extraction approach that uses machine learning incorporating varied and rich features of candidate terms. In our preliminary experiments, we also tested different attribute selection methods to verify which features are more relevant for automatic term extraction. We achieved state of the art results for unigram extraction in Brazilian Portuguese. 
We present our work in generating Karmina, an old Malay poetic form for Indonesian language. Karmina is a poem with two lines that consists of a hook (sampiran) on the first line and a message on the second line. One of the unique aspects of Karmina is in the absence of discourse relation between its hook and message. We approached the problem by generating the hooks and the messages in separate processes using predefined schemas and a manually built knowledge base. The Karminas were produced by randomly pairing the messages with the hooks, subject to the constraints imposed on the rhymes and on the structure similarity. Syllabifications were performed on the cue words of the hooks and messages to ensure the generated pairs have matching rhymes. We were able to generate a number of positive examples while still leaving room for improvement, particularly in the generation of the messages, which currently are still limited, and in filtering the negative results. 
Revealing an anonymous author’s traits from text is a well-researched area. In this paper we aim to identify the native language and language family of a non-native English author, given his/her English writings. We extract features from the text based on prior work, and extend or modify it to construct different feature sets, and use support vector machines for classiﬁcation. We show that native language identiﬁcation accuracy can be improved by up to 6.43% for a 9-class task, depending on the feature set, by introducing a novel method to incorporate language family information. In addition we show that introducing grammarbased features improves accuracy of both native language and language family identiﬁcation. 
Our research investigates the translation of ontology labels, which has applications in multilingual knowledge access. Ontologies are often deﬁned only in one language, mostly English. To enable knowledge access across languages, such monolingual ontologies need to be translated into other languages. The primary challenge in ontology label translation is the lack of context, which makes this task rather different than document translation. The core objective therefore, is to provide statistical machine translation (SMT) systems with additional context information. In our approach, we ﬁrst extend standard SMT by enhancing a translation model with context information that keeps track of surrounding words for each translation. We compute a semantic similarity between the phrase pair context vector from the parallel corpus and a vector of noun phrases that occur in surrounding ontology labels. We applied our approach to the translation of a ﬁnancial ontology, translating from English to German, using Europarl as parallel corpus. This experiment showed that our approach can provide a slight improvement over standard SMT for this task, without exploiting any additional domain-speciﬁc resources. 
Morphological tokenization has been used in machine translation for morphologically complex languages to reduce lexical sparsity. Unfortunately, when translating into a morphologically complex language, recombining segmented tokens to generate original word forms is not a trivial task, due to morphological, phonological and orthographic adjustments that occur during tokenization. We review a number of detokenization schemes for Arabic, such as rule-based and table-based approaches and show their limitations. We then propose a novel detokenization scheme that uses a character-level discriminative string transducer to predict the original form of a segmented word. In a comparison to a stateof-the-art approach, we demonstrate slightly better detokenization error rates, without the need for any hand-crafted rules. We also demonstrate the effectiveness of our approach in an English-to-Arabic translation task. 
My thesis will explore ways to improve the performance of statistical machine translation (SMT) in low resource conditions. Specifically, it aims to reduce the dependence of modern SMT systems on expensive parallel data. We deﬁne low resource settings as having only small amounts of parallel data available, which is the case for many language pairs. All current SMT models use parallel data during training for extracting translation rules and estimating translation probabilities. The theme of our approach is the integration of information from alternate data sources, other than parallel corpora, into the statistical model. In particular, we focus on making use of large monolingual and comparable corpora. By augmenting components of the SMT framework, we hope to extend its applicability beyond the small handful of language pairs with large amounts of available parallel text. 
We examine the application of data-driven paraphrasing to natural language understanding. We leverage bilingual parallel corpora to extract a large collection of syntactic paraphrase pairs, and introduce an adaptation scheme that allows us to tackle a variety of text transformation tasks via paraphrasing. We evaluate our system on the sentence compression task. Further, we use distributional similarity measures based on context vectors derived from large monolingual corpora to annotate our paraphrases with an orthogonal source of information. This yields signiﬁcant improvements in our compression system’s output quality, achieving state-of-the-art performance. Finally, we propose a reﬁnement of our paraphrases by classifying them into natural logic entailment relations. By extending the synchronous parsing paradigm towards these entailment relations, we will enable our system to perform recognition of textual entailment. 
Automatically describing visual content is an extremely difﬁcult task, with hard AI problems in Computer Vision (CV) and Natural Language Processing (NLP) at its core. Previous work relies on supervised visual recognition systems to determine the content of images. These systems require massive amounts of hand-labeled data for training, so the number of visual classes that can be recognized is typically very small. We argue that these approaches place unrealistic limits on the kinds of images that can be captioned, and are unlikely to produce captions which reﬂect human interpretations. We present a framework for image caption generation that does not rely on visual recognition systems, which we have implemented on a dataset of online shopping images and product descriptions. We propose future work to improve this method, and extensions for other domains of images and natural text. 
Review mining and summarization has been a hot topic for the past decade. A lot of effort has been devoted to aspect detection and sentiment analysis under the assumption that every review has the same utility for related tasks. However, reviews are not equally helpful as indicated by user-provided helpfulness assessment associated with the reviews. In this thesis, we propose a novel review summarization framework which summarizes review content under the supervision of automated assessment of review helpfulness. This helpfulness-guided framework can be easily adapted to traditional review summarization tasks, for a wide range of domains. 
Entrainment is the phenomenon of the speech of conversational partners becoming more similar to each other. This thesis proposal presents a comprehensive look at entrainment in human conversations and how entrainment may be incorporated into the design of spoken dialogue systems in order to improve system performance and user satisfaction. We compare different kinds of entrainment in both classic and novel dimensions, provide experimental results on the utility of entrainment, and show that entrainment can be used to improve a system’s ASR performance and turntaking decisions. 
In this paper, a Maximum Entropy Markov Model (MEMM) for dialog state tracking is proposed to efﬁciently handle user goal evolvement in two steps. The system ﬁrst predicts the occurrence of a user goal change based on linguistic features and dialog context for each dialog turn, and then the proposed model could utilize this user goal change information to infer the most probable dialog state sequence which underlies the evolvement of user goal during the dialog. It is believed that with the suggested various domain independent feature functions, the proposed model could better exploit not only the intra-dependencies within long ASR N-best lists but also the inter-dependencies of the observations across dialog turns, which leads to more efﬁcient and accurate dialog state inference.  best list when the top ASR hypothesis is incorrect. Furthermore, reasoning over different ASR N-best lists is also difﬁcult since it is hard to decide when to detect commonality (when user repeats) and when to look for differences (when user changes her or his mind) among multiple ASR N-best lists. Another challenge is how to handle more complex user actions such as negotiating alternative choices or seeking out other potential solutions when interacting with the system. This proposal presents a probabilistic framework for modeling the evolvement of user goal during the dialog (focusing on the shaded component Dialog State Tracking in Figure 1 that shows a typical diagram for a spoken dialog system), which aims to endow the system with the ability to model natural negotiation strategies, in the hope of leading to more accurate and efﬁcient dialog state tracking performance.  
N-gram-based models co-exist with their phrase-based counterparts as an alternative SMT framework. Both techniques have pros and cons. While the N-gram-based framework provides a better model that captures both source and target contexts and avoids spurious phrasal segmentation, the ability to memorize and produce larger translation units gives an edge to the phrase-based systems during decoding, in terms of better search performance and superior selection of translation units. In this paper we combine N-grambased modeling with phrase-based decoding, and obtain the beneﬁts of both approaches. Our experiments show that using this combination not only improves the search accuracy of the N-gram model but that it also improves the BLEU scores. Our system outperforms state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems by a signiﬁcant margin on German, French and Spanish to English translation tasks. 
Standard phrase-based translation models do not explicitly model context dependence between translation units. As a result, they rely on large phrase pairs and target language models to recover contextual eﬀects in translation. In this work, we explore n-gram models over Minimal Translation Units (MTUs) to explicitly capture contextual dependencies across phrase boundaries in the channel model. As there is no single best direction in which contextual information should ﬂow, we explore multiple decomposition structures as well as dynamic bidirectional decomposition. The resulting models are evaluated in an intrinsic task of lexical selection for MT as well as a full MT system, through n-best reranking. These experiments demonstrate that additional contextual modeling does indeed beneﬁt a phrase-based system and that the direction of conditioning is important. Integrating multiple conditioning orders provides consistent beneﬁt, and the most important directions diﬀer by language pair. 
There have been many recent investigations into methods to tune SMT systems using large numbers of sparse features. However, there have not been nearly so many examples of helpful sparse features, especially for phrasebased systems. We use sparse features to address reordering, which is often considered a weak point of phrase-based translation. Using a hierarchical reordering model as our baseline, we show that simple features coupling phrase orientation to frequent words or wordclusters can improve translation quality, with boosts of up to 1.2 BLEU points in ChineseEnglish and 1.8 in Arabic-English. We compare this solution to a more traditional maximum entropy approach, where a probability model with similar features is trained on wordaligned bitext. We show that sparse decoder features outperform maximum entropy handily, indicating that there are major advantages to optimizing reordering features directly for BLEU with the decoder in the loop. 
Current word alignment models for statistical machine translation do not address morphology beyond merely splitting words. We present a two-level alignment model that distinguishes between words and morphemes, in which we embed an IBM Model 1 inside an HMM based word alignment model. The model jointly induces word and morpheme alignments using an EM algorithm. We evaluated our model on Turkish-English parallel data. We obtained signiﬁcant improvement of BLEU scores over IBM Model 4. Our results indicate that utilizing information from morphology improves the quality of word alignments. 
Identifying documents that describe a speciﬁc type of event is challenging due to the high complexity and variety of event descriptions. We propose a multi-faceted event recognition approach, which identiﬁes documents about an event using event phrases as well as deﬁning characteristics of the event. Our research focuses on civil unrest events and learns civil unrest expressions as well as phrases corresponding to potential agents and reasons for civil unrest. We present a bootstrapping algorithm that automatically acquires event phrases, agent terms, and purpose (reason) phrases from unannotated texts. We use the bootstrapped dictionaries to identify civil unrest documents and show that multi-faceted event recognition can yield high accuracy. 
Different languages contain complementary cues about entities, which can be used to improve Named Entity Recognition (NER) systems. We propose a method that formulates the problem of exploring such signals on unannotated bilingual text as a simple Integer Linear Program, which encourages entity tags to agree via bilingual constraints. Bilingual NER experiments on the large OntoNotes 4.0 Chinese-English corpus show that the proposed method can improve strong baselines for both Chinese and English. In particular, Chinese performance improves by over 5% absolute F1 score. We can then annotate a large amount of bilingual text (80k sentence pairs) using our method, and add it as uptraining data to the original monolingual NER training corpus. The Chinese model retrained on this new combined dataset outperforms the strong baseline by over 3% F1 score. 
We propose a minimally supervised method for multilingual paraphrase extraction from deﬁnition sentences on the Web. Hashimoto et al. (2011) extracted paraphrases from Japanese deﬁnition sentences on the Web, assuming that deﬁnition sentences deﬁning the same concept tend to contain paraphrases. However, their method requires manually annotated data and is language dependent. We extend their framework and develop a minimally supervised method applicable to multiple languages. Our experiments show that our method is comparable to Hashimoto et al.’s for Japanese and outperforms previous unsupervised methods for English, Japanese, and Chinese, and that our method extracts 10,000 paraphrases with 92% precision for English, 82.5% precision for Japanese, and 82% precision for Chinese. 
Traditional relation extraction predicts relations within some ﬁxed and ﬁnite target schema. Machine learning approaches to this task require either manual annotation or, in the case of distant supervision, existing structured sources of the same schema. The need for existing datasets can be avoided by using a universal schema: the union of all involved schemas (surface form predicates as in OpenIE, and relations in the schemas of preexisting databases). This schema has an almost unlimited set of relations (due to surface forms), and supports integration with existing structured data (through the relation types of existing databases). To populate a database of such schema we present matrix factorization models that learn latent feature vectors for entity tuples and relations. We show that such latent models achieve substantially higher accuracy than a traditional classiﬁcation approach. More importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms stateof-the-art distant supervision. 
We develop a method for effective extraction of linguistic patterns that are differentially expressed based on the native language of the author. This method uses multiple corpora to allow for the removal of data set speciﬁc patterns, and addresses both feature relevancy and redundancy. We evaluate different relevancy ranking metrics and show that common measures of relevancy can be inappropriate for data with many rare features. Our feature set is a broad class of syntactic patterns, and to better capture the signal we extend the Bayesian Tree Substitution Grammar induction algorithm to a supervised mixture of latent grammars. We show that this extension can be used to extract a larger set of relevant features. 
The frequency of words and syntactic constructions has been observed to have a substantial effect on language processing. This begs the question of what causes certain constructions to be more or less frequent. A theory of grounding (Phillips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. This paper studies how current theories of working memory ﬁt into theories of language processing and what inﬂuence memory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing. 
We propose a new approach to identifying semantically similar words across languages. The approach is based on an idea that two words in different languages are similar if they are likely to generate similar words (which includes both source and target language words) as their top semantic word responses. Semantic word responding is a concept from cognitive science which addresses detecting most likely words that humans output as free word associations given some cue word. The method consists of two main steps: (1) it utilizes a probabilistic multilingual topic model trained on comparable data to learn and quantify the semantic word responses, (2) it provides ranked lists of similar words according to the similarity of their semantic word response vectors. We evaluate our approach in the task of bilingual lexicon extraction (BLE) for a variety of language pairs. We show that in the cross-lingual settings without any language pair dependent knowledge the response-based method of similarity is more robust and outperforms current state-of-the art methods that directly operate in the semantic space of latent cross-lingual concepts/topics. 
Humans identify word boundaries in continuous speech by combining multiple cues; existing state-of-the-art models, though, look at a single cue. We extend the generative model of Goldwater et al (2006) to segment using syllable stress as well as phonemic form. Our new model treats identiﬁcation of word boundaries and prevalent stress patterns in the language as a joint inference task. We show that this model improves segmentation accuracy over purely segmental input representations, and recovers the dominant stress pattern of the data. Additionally, our model retains high performance even without single-word utterances. We also demonstrate a discrepancy in the performance of our model and human infants on an artiﬁcial-language task in which stress cues and transition-probability information are pitted against one another. We argue that this discrepancy indicates a bound on rationality in the mechanisms of human segmentation. 
We consider the problem of training a statistical parser in the situation when there are multiple treebanks available, and these treebanks are annotated according to different linguistic conventions. To address this problem, we present two simple adaptation methods: the ﬁrst method is based on the idea of using a shared feature representation when parsing multiple treebanks, and the second method on guided parsing where the output of one parser provides features for a second one. To evaluate and analyze the adaptation methods, we train parsers on treebank pairs in four languages: German, Swedish, Italian, and English. We see signiﬁcant improvements for all eight treebanks when training on the full training sets. However, the clearest beneﬁts are seen when we consider smaller training sets. Our experiments were carried out with unlabeled dependency parsers, but the methods can easily be generalized to other featurebased parsers.  to different syntactic theories. Apart from German, Swedish, and Italian, which will be considered in this paper, there are important examples among the world’s major languages, such as Arabic and Chinese. To exemplify how syntactic annotation conventions may differ in even such a simple case as unlabeled dependency annotation, consider the Italian sentence fragment la sospensione o l’interruzione (’the suspension or the interruption’) in Figure 1. As we will see in detail in §3.1.3, there are two Italian treebanks: the ISST and TUT. If annotating as in the ISST treebank (drawn above the sentence) determiners (la, l’) are annotated as dependents of the following nouns (sospensione, interruzione); in TUT (drawn below the sentence), we have the reverse situation. There are also differences in how coordinate structures are represented: in ISST, the two conjuncts are directly conjoined and the conjunction attached to the ﬁrst of them, while in TUT the conjunction acts as a link between the conjuncts.  
Most work on weakly-supervised learning for part-of-speech taggers has been based on unrealistic assumptions about the amount and quality of training data. For this paper, we attempt to create true low-resource scenarios by allowing a linguist just two hours to annotate data and evaluating on the languages Kinyarwanda and Malagasy. Given these severely limited amounts of either type supervision (tag dictionaries) or token supervision (labeled sentences), we are able to dramatically improve the learning of a hidden Markov model through our method of automatically generalizing the annotations, reducing noise, and inducing word-tag frequency information. 
Latent-variable PCFGs (L-PCFGs) are a highly successful model for natural language parsing. Recent work (Cohen et al., 2012) has introduced a spectral algorithm for parameter estimation of L-PCFGs, which—unlike the EM algorithm—is guaranteed to give consistent parameter estimates (it has PAC-style guarantees of sample complexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efﬁcient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 
Topics generated automatically, e.g. using LDA, are now widely used in Computational Linguistics. Topics are normally represented as a set of keywords, often the n terms in a topic with the highest marginal probabilities. We introduce an alternative approach in which topics are represented using images. Candidate images for each topic are retrieved from the web by querying a search engine using the top n terms. The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image and features extracted from the images themselves. We show that the proposed approach signiﬁcantly outperforms several baselines and can provide images that are useful to represent a topic. 
Multi-dimensional latent text models, such as factorial LDA (f-LDA), capture multiple factors of corpora, creating structured output for researchers to better understand the contents of a corpus. We consider such models for clinical research of new recreational drugs and trends, an important application for mining current information for healthcare workers. We use a “three-dimensional” f-LDA variant to jointly model combinations of drug (marijuana, salvia, etc.), aspect (effects, chemistry, etc.) and route of administration (smoking, oral, etc.) Since a purely unsupervised topic model is unlikely to discover these speciﬁc factors of interest, we develop a novel method of incorporating prior knowledge by leveraging user generated tags as priors in our model. We demonstrate that this model can be used as an exploratory tool for learning about these drugs from the Web by applying it to the task of extractive summarization. In addition to providing useful output for this important public health task, our prior-enriched model provides a framework for the application of fLDA to other tasks. 
We propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic. We build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases. We then aggregate those selected phrases by means of phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance signiﬁcantly over baseline algorithms. 
We present a new hierarchical Bayesian model for unsupervised topic segmentation. This new model integrates a point-wise boundary sampling algorithm used in Bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents. We develop an MCMC inference algorithm to split/merge segment(s). Experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on Choi’s datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets. 
 0HUJLQJ,QFRPSOHWH&DSWLRQV  The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear. In this paper, we describe an improved method for combining partial captions into a ﬁnal output based on weighted A∗ search and multiple sequence alignment (MSA). In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds. Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%). The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem. 
Automatically assessing the ﬁdelity of a retelling to the original narrative – a task of growing clinical importance – is challenging, given extensive paraphrasing during retelling along with cascading automatic speech recognition (ASR) errors. We present a word tagging approach using conditional random ﬁelds (CRFs) that allows a diversity of features to be considered during inference, including some capturing acoustic confusions encoded in word confusion networks. We evaluate the approach under several scenarios, including both supervised and unsupervised training, the latter achieved by training on the output of a baseline automatic word-alignment model. We also adapt the ASR models to the domain, and evaluate the impact of error rate on performance. We ﬁnd strong robustness to ASR errors, even using just the 1-best system output. A hybrid approach making use of both automatic alignment and CRFs trained tagging models achieves the best performance, yielding strong improvements over using either approach alone. 
The study presented in this work is a ﬁrst effort at real-time speech translation of TED talks, a compendium of public talks with different speakers addressing a variety of topics. We address the goal of achieving a system that balances translation accuracy and latency. In order to improve ASR performance for our diverse data set, adaptation techniques such as constrained model adaptation and vocal tract length normalization are found to be useful. In order to improve machine translation (MT) performance, techniques that could be employed in real-time such as monotonic and partial translation retention are found to be of use. We also experiment with inserting text segmenters of various types between ASR and MT in a series of real-time translation experiments. Among other results, our experiments demonstrate that a good segmentation is useful, and a novel conjunction-based segmentation strategy improves translation quality nearly as much as other strategies such as comma-based segmentation. It was also found to be important to synchronize various pipeline components in order to minimize latency. 
Treebanks are not large enough to adequately model subcategorization frames of predicative lexemes, which is an important source of lexico-syntactic constraints for parsing. As a consequence, parsers trained on such treebanks usually make mistakes when selecting the arguments of predicative lexemes. In this paper, we propose an original way to correct subcategorization errors by combining subparses of a sentence S that appear in the list of the n-best parses of S. The subcategorization information comes from three different resources, the ﬁrst one is extracted from a treebank, the second one is computed on a large corpora and the third one is an existing syntactic lexicon. Experiments on the French Treebank showed a 15.24% reduction of erroneous subcategorization frames (SF) selections for verbs as well as a relative decrease of the error rate of 4% Labeled Accuracy Score on the state of the art parser on this treebank. 
We introduce a new large-scale discriminative learning algorithm for machine translation that is capable of learning parameters in models with extremely sparse features. To ensure their reliable estimation and to prevent overﬁtting, we use a two-phase learning algorithm. First, the contribution of individual sparse features is estimated using large amounts of parallel data. Second, a small development corpus is used to determine the relative contributions of the sparse features and standard dense features. Not only does this two-phase learning approach prevent overﬁtting, the second pass optimizes corpus-level BLEU of the Viterbi translation of the decoder. We demonstrate signiﬁcant improvements using sparse rule indicator features in three different translation tasks. To our knowledge, this is the ﬁrst large-scale discriminative training algorithm capable of showing improvements over the MERT baseline with only rule indicator features in addition to the standard MERT features. 
Measuring term informativeness is a fundamental NLP task. Existing methods, mostly based on statistical information in corpora, do not actually measure informativeness of a term with regard to its semantic context. This paper proposes a new lightweight feature-free approach to encode term informativeness in context by leveraging web knowledge. Given a term and its context, we model contextaware term informativeness based on semantic similarity between the context and the term’s most featured context in a knowledge base, Wikipedia. We apply our method to three applications: core term extraction from snippets (text segment), scientiﬁc keywords extraction (paper), and back-of-the-book index generation (book). The performance is state-of-theart or close to it for each application, demonstrating its effectiveness and generality. 
We here present and compare two unsupervised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages. We evaluate the two approaches in two different information extraction settings: monolingual and cross-lingual information extraction. The extraction systems are trained on auto-annotated summaries (containing the induced concepts) and evaluated on humanannotated documents. Extraction results are promising, being close in performance to those achieved when the system is trained on human-annotated summaries. 
Language variations are generally known to have a severe impact on the performance of Human Language Technology Systems. In order to predict or improve system performance, a thorough investigation into these variations, similarities and dissimilarities, is required. Distance measures have been used in several applications of speech processing to analyze different varying speech attributes. However, not much work has been done on language distance measures, and even less work has been done involving South African languages. This study explores two methods for measuring the linguistic distance of six South African languages. It concerns a text based method, (the Levenshtein Distance), and an acoustic approach using extracted mean pitch values. The Levenshtein distance uses parallel word transcriptions from all six languages with as little as 144 words, whereas the pitch method is text-independent and compares mean language pitch differences. Cluster analysis resulting from the distance matrices from both methods correlates closely with human perceptual distances and existing literature about the six languages. 
We present a new variant of the SyntaxAugmented Machine Translation (SAMT) formalism with a category-coarsening algorithm originally developed for tree-to-tree grammars. We induce bilingual labels into the SAMT grammar, use them for category coarsening, then project back to monolingual labeling as in standard SAMT. The result is a “collapsed” grammar with the same expressive power and format as the original, but many fewer nonterminal labels. We show that the smaller label set provides improved translation scores by 1.14 BLEU on two Chinese– English test sets while reducing the occurrence of sparsity and ambiguity problems common to large label sets. 
Multi-Sentence Compression (MSC) is the task of generating a short single sentence summary from a cluster of related sentences. This paper presents an N-best reranking method based on keyphrase extraction. Compression candidates generated by a word graph-based MSC approach are reranked according to the number and relevance of keyphrases they contain. Both manual and automatic evaluations were performed using a dataset made of clusters of newswire sentences. Results show that the proposed method signiﬁcantly improves the informativity of the generated compressions. 
This paper describes the annotation process and linguistic properties of the Persian syntactic dependency treebank. The treebank consists of approximately 30,000 sentences annotated with syntactic roles in addition to morpho-syntactic features. One of the unique features of this treebank is that there are almost 4800 distinct verb lemmas in its sentences making it a valuable resource for educational goals. The treebank is constructed with a bootstrapping approach by means of available tagging and parsing tools and manually correcting the annotations. The data is splitted into standard train, development and test set in the CoNLL dependency format and is freely available to researchers. 
Recent work has shown that word aligned data can be used to learn a model for reordering source sentences to match the target order. This model learns the cost of putting a word immediately before another word and ﬁnds the best reordering by solving an instance of the Traveling Salesman Problem (TSP). However, for efﬁciently solving the TSP, the model is restricted to pairwise features which examine only a pair of words and their neighborhood. In this work, we go beyond these pairwise features and learn a model to rerank the n-best reorderings produced by the TSP model using higher order and structural features which help in capturing longer range dependencies. In addition to using a more informative set of source side features, we also capture target side features indirectly by using the translation score assigned to a reordering. Our experiments, involving Urdu-English, show that the proposed approach outperforms a state-of-theart PBSMT system which uses the TSP model for reordering by 1.3 BLEU points, and a publicly available state-of-the-art MT system, Hiero, by 3 BLEU points. 
Translation models in statistical machine translation can be scaled to large corpora and arbitrarily-long phrases by looking up translations of source phrases “on the ﬂy” in an indexed parallel corpus using sufﬁx arrays. However, this can be slow because on-demand extraction of phrase tables is computationally expensive. We address this problem by developing novel algorithms for general purpose graphics processing units (GPUs), which enable sufﬁx array queries for phrase lookup and phrase extraction to be massively parallelized. Compared to a highly-optimized, state-of-the-art serial CPU-based implementation, our techniques achieve at least an order of magnitude improvement in terms of throughput. This work demonstrates the promise of massively parallel architectures and the potential of GPUs for tackling computationallydemanding problems in statistical machine translation and language processing. 
Until recently, the application of discriminative training to log linear-based statistical machine translation has been limited to tuning the weights of a limited number of features or training features with a limited number of parameters. In this paper, we propose to scale up discriminative training of (He and Deng, 2012) to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning. The experimental results conﬁrm the effectiveness of our proposals on NIST MT06 set over a strong baseline. 
In Statistical Machine Translation we often have to combine different sources of parallel training data to build a good system. One way of doing this is to build separate translation models from each data set and linearly interpolate them, and to date the main method for optimising the interpolation weights is to minimise the model perplexity on a heldout set. In this work, rather than optimising for this indirect measure, we directly optimise for BLEU on the tuning set and show improvements in average performance over two data sets and 8 language pairs. 
Modern Standard Arabic (MSA) has a wealth of natural language processing (NLP) tools and resources. In comparison, resources for dialectal Arabic (DA), the unstandardized spoken varieties of Arabic, are still lacking. We present ELISSA, a machine translation (MT) system for DA to MSA. ELISSA employs a rule-based approach that relies on morphological analysis, transfer rules and dictionaries in addition to language models to produce MSA paraphrases of DA sentences. ELISSA can be employed as a general preprocessor for DA when using MSA NLP tools. A manual error analysis of ELISSA’s output shows that it produces correct MSA translations over 93% of the time. Using ELISSA to produce MSA versions of DA sentences as part of an MSA-pivoting DA-to-English MT solution, improves BLEU scores on multiple blind test sets between 0.6% and 1.4%. 
The rise of social media has brought computational linguistics in ever-closer contact with bad language: text that deﬁes our expectations about vocabulary, spelling, and syntax. This paper surveys the landscape of bad language, and offers a critical review of the NLP community’s response, which has largely followed two paths: normalization and domain adaptation. Each approach is evaluated in the context of theoretical and empirical work on computer-mediated communication. In addition, the paper presents a quantitative analysis of the lexical diversity of social media text, and its relationship to other corpora. 
Online learning algorithms such as perceptron and MIRA have become popular for many NLP tasks thanks to their simpler architecture and faster convergence over batch learning methods. However, while batch learning such as CRF is easily parallelizable, online learning is much harder to parallelize: previous efforts often witness a decrease in the converged accuracy, and the speedup is typically very small (∼3) even with many (10+) processors. We instead present a much simpler architecture based on “mini-batches”, which is trivially parallelizable. We show that, unlike previous methods, minibatch learning (in serial mode) actually improves the converged accuracy for both perceptron and MIRA learning, and when combined with simple parallelization, minibatch leads to very signiﬁcant speedups (up to 9x on 12 processors) on stateof-the-art parsing and tagging systems. 
We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the ﬁrst POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data. 
We describe a new self-learning framework for parser lexicalisation that requires only a plain-text corpus of in-domain text. The method ﬁrst creates augmented versions of dependency graphs by applying a series of modiﬁcations designed to directly capture higherorder lexical path dependencies. Scores are assigned to each edge in the graph using statistics from an automatically parsed background corpus. As bilexical dependencies are sparse, a novel directed distributional word similarity measure is used to smooth edge score estimates. Edge scores are then combined into graph scores and used for reranking the topn analyses found by the unlexicalised parser. The approach achieves signiﬁcant improvements on WSJ and biomedical text over the unlexicalised baseline parser, which is originally trained on a subset of the Brown corpus. 
Advances in sentiment analysis have enabled extraction of user relations implied in online textual exchanges such as forum posts. However, recent studies in this direction only consider direct relation extraction from text. As user interactions can be sparse in online discussions, we propose to apply collaborative ﬁltering through probabilistic matrix factorization to generalize and improve the opinion matrices extracted from forum posts. Experiments with two tasks show that the learned latent factor representation can give good performance on a relation polarity prediction task and improve the performance of a subgroup detection task. 
Feature and context aggregation play a large role in current NER systems, allowing significant opportunities for research into optimizing these features to cater to different domains. This work strives to reduce the noise introduced into aggregated features from disparate and generic training data in order to allow for contextual features that more closely model the entities in the target data. The proposed approach trains models based on only a part of the training set that is more similar to the target domain. To this end, models are trained for an existing NER system using the top documents from the training set that are similar to the target document in order to demonstrate that this technique can be applied to improve any pre-built NER system. Initial results show an improvement over the University of Illinois NE tagger with a weighted average F1 score of 91.67 compared to the Illinois tagger’s score of 91.32. This research serves as a proof-of-concept for future planned work to cluster the training documents to produce a number of more focused models from a given training set, thereby reducing noise and extracting a more representative feature set. 
 Language can describe our visual world at many levels, including not only what is literally there but also the sentiment that it invokes. In this paper, we study visual language, both literal and sentimental, that describes the overall appearance and style of virtual characters. Sentimental properties, including labels such as “youthful” or “country western,” must be inferred from descriptions of the more literal properties, such as facial features and clothing selection. We present a new dataset, collected to describe Xbox avatars, as well as models for learning the relationships between these avatars and their literal and sentimental descriptions. In a series of experiments, we demonstrate that such learned models can be used for a range of tasks, including predicting sentimental words and using them to rank and build avatars. Together, these results demonstrate that sentimental language provides a concise (though noisy) means of specifying low-level visual properties. 
The many differences between Dialectal Arabic and Modern Standard Arabic (MSA) pose a challenge to the majority of Arabic natural language processing tools, which are designed for MSA. In this paper, we retarget an existing state-of-the-art MSA morphological tagger to Egyptian Arabic (ARZ). Our evaluation demonstrates that our ARZ morphology tagger outperforms its MSA variant on ARZ input in terms of accuracy in part-of-speech tagging, diacritization, lemmatization and tokenization; and in terms of utility for ARZ-toEnglish statistical machine translation. 
We present a novel, structured language model - Supertagged Dependency Language Model to model the syntactic dependencies between words. The goal is to identify ungrammatical hypotheses from a set of candidate translations in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features. We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination. 
We report the results of our work on automating the transliteration decision of named entities for English to Arabic machine translation. We construct a classiﬁcation-based framework to automate this decision, evaluate our classiﬁer both in the limited news and the diverse Wikipedia domains, and achieve promising accuracy. Moreover, we demonstrate a reduction of translation error and an improvement in the performance of an English-to-Arabic machine translation system.  
This paper describes an approach to improve summaries for a collection of Twitter posts created using the Phrase Reinforcement (PR) Algorithm (Shariﬁ et al., 2010a). The PR algorithm often generates summaries with excess text and noisy speech. We parse these summaries using a dependency parser and use the dependencies to eliminate some of the excess text and build better-formed summaries. We compare the results to those obtained using the PR Algorithm. 
This paper presents a general, statistical framework for modeling phrase translation via Markov random fields. The model allows for arbituary features extracted from a phrase pair to be incorporated as evidence. The parameters of the model are estimated using a large-scale discriminative training approach that is based on stochastic gradient ascent and an N-best list based expected BLEU as the objective function. The model is easy to be incoporated into a standard phrase-based statistical machine translation system, requiring no code change in the runtime engine. Evaluation is performed on two Europarl translation tasks, GermanEnglish and French-English. Results show that incoporating the Markov random field model significantly improves the performance of a state-of-the-art phrase-based machine translation system, leading to a gain of 0.8-1.3 BLEU points. 
In this paper, we study the problem of automatic enrichment of a morphologically underspeciﬁed treebank for Arabic, a morphologically rich language. We show that we can map from a tagset of size six to one with 485 tags at an accuracy rate of 94%-95%. We can also identify the unspeciﬁed lemmas in the treebank with an accuracy over 97%. Furthermore, we demonstrate that using our automatic annotations improves the performance of a state-of-the-art Arabic morphological tagger. Our approach combines a variety of techniques from corpus-based statistical models to linguistic rules that target speciﬁc phenomena. These results suggest that the cost of treebanking can be reduced by designing underspeciﬁed treebanks that can be subsequently enriched automatically. 
Social media texts are written in an informal style, which hinders other natural language processing (NLP) applications such as machine translation. Text normalization is thus important for processing of social media text. Previous work mostly focused on normalizing words by replacing an informal word with its formal form. In this paper, to further improve other downstream NLP applications, we argue that other normalization operations should also be performed, e.g., missing word recovery and punctuation correction. A novel beam-search decoder is proposed to effectively integrate various normalization operations. Empirical results show that our system obtains statistically signiﬁcant improvements over two strong baselines in both normalization and translation tasks, for both Chinese and English. 
LDA-frames is an unsupervised approach for identifying semantic frames from semantically unlabeled text corpora, and seems to be a useful competitor for manually created databases of selectional preferences. The most limiting property of the algorithm is such that the number of frames and roles must be predeﬁned. In this paper we present a modiﬁcation of the LDA-frames algorithm allowing the number of frames and roles to be determined automatically, based on the character and size of training data. 
We provide an approximation algorithm for PCFG parsing, which asymptotically improves time complexity with respect to the input grammar size, and prove upper bounds on the approximation quality. We test our algorithm on two treebanks, and get signiﬁcant improvements in parsing speed. 
The rising inﬂuence of user-generated online reviews (Cone, 2011) has led to growing incentive for businesses to solicit and manufacture DECEPTIVE OPINION SPAM—ﬁctitious reviews that have been deliberately written to sound authentic and deceive the reader. Recently, Ott et al. (2011) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews. However, the complementary problem of negative deceptive opinion spam, intended to slander competitive offerings, remains largely unstudied. Following an approach similar to Ott et al. (2011), in this work we create and study the ﬁrst dataset of deceptive opinion spam with negative sentiment reviews. Based on this dataset, we ﬁnd that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges. Finally, in conjunction with the aforementioned positive review dataset, we consider the possible interactions between sentiment and deception, and present initial results that encourage further exploration of this relationship. 
We present a method for improving the perceived naturalness of corpus-based speech synthesizers. It consists in removing pronounced pitch peaks in the original recordings, which typically lead to noticeable discontinuities in the synthesized speech. We perceptually evaluated this method using two concatenative and two HMM-based synthesis systems, and found that using it on the source recordings managed to improve the naturalness of the synthesizers and had no effect on their intelligibility. 
We show that existing methods for training preposition error correction systems, whether using well-edited text or error-annotated corpora, do not generalize across very different test sets. We present a new, large errorannotated corpus and use it to train systems that generalize across three different test sets, each from a different domain and with different error characteristics. This new corpus is automatically extracted from Wikipedia revisions and contains over one million instances of preposition corrections. 
Prior research into learning translations from source and target language monolingual texts has treated the task as an unsupervised learning problem. Although many techniques take advantage of a seed bilingual lexicon, this work is the ﬁrst to use that data for supervised learning to combine a diverse set of signals derived from a pair of monolingual corpora into a single discriminative model. Even in a low resource machine translation setting, where induced translations have the potential to improve performance substantially, it is reasonable to assume access to some amount of data to perform this kind of optimization. Our work shows that only a few hundred translation pairs are needed to achieve strong performance on the bilingual lexicon induction task, and our approach yields an average relative gain in accuracy of nearly 50% over an unsupervised baseline. Large gains in accuracy hold for all 22 languages (low and high resource) that we investigate. 
Bilingual dictionaries are expensive resources and not many are available when one of the languages is resource-poor. In this paper, we propose algorithms for creation of new reverse bilingual dictionaries from existing bilingual dictionaries in which English is one of the two languages. Our algorithms exploit the similarity between word-concept pairs using the English Wordnet to produce reverse dictionary entries. Since our algorithms rely on available bilingual dictionaries, they are applicable to any bilingual dictionary as long as one of the two languages has Wordnet type lexical ontology. 
This paper examines the efficacy of the application of a pre-existing technique in the area of event-event temporal relationship identification. We attempt to both reproduce the results of said technique, as well as extend the previous work with application to a newlycreated domain of biographical data. We find that initially the simpler feature sets perform as expected, but that the final improvement to the feature set underperforms. In response, we provide an analysis of the individual features and identify differences existing between two corpora. 
We examine predicative adjectives as an unsupervised criterion to extract subjective adjectives. We do not only compare this criterion with a weakly supervised extraction method but also with gradable adjectives, i.e. another highly subjective subset of adjectives that can be extracted in an unsupervised fashion. In order to prove the robustness of this extraction method, we will evaluate the extraction with the help of two different state-of-the-art sentiment lexicons (as a gold standard).  phrase, but which predicate a property of the referent of a noun phrase to which they are linked via a copula or a control predicate (3). We show that adjectives that frequently occur as predicative adjectives are more likely to convey subjectivity (in general) than adjectives that occur nonpredicatively, such as the pre-nominal (attributive) adjectives (4). A subjective adjective may occur both as a predicative (3) and a non-predicative (5) adjective and also convey subjectivity in both contexts. However, a large fraction of non-subjective adjectives do not occur as predicative adjectives (6).  
Incorporating semantic structure into a linguistics-free translation model is challenging, since semantic structures are closely tied to syntax. In this paper, we propose a two-level approach to exploiting predicate-argument structure reordering in a hierarchical phrase-based translation model. First, we introduce linguistically motivated constraints into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. Second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. Experiments on Chinese-to-English translation demonstrate that both advances signiﬁcantly improve translation accuracy. 
This paper discusses the extension of a system developed for automatic discovery of treebank annotation inconsistencies over an entire corpus to the particular case of evaluation of inter-annotator agreement. This system makes for a more informative IAA evaluation than other systems because it pinpoints the inconsistencies and groups them by their structural types. We evaluate the system on two corpora - (1) a corpus of English web text, and (2) a corpus of Modern British English. 
Word sense disambiguation aims to identify which meaning of a word is present in a given usage. Gathering word sense annotations is a laborious and difﬁcult task. Several methods have been proposed to gather sense annotations using large numbers of untrained annotators, with mixed results. We propose three new annotation methodologies for gathering word senses where untrained annotators are allowed to use multiple labels and weight the senses. Our ﬁndings show that given the appropriate annotation task, untrained workers can obtain at least as high agreement as annotators in a controlled setting, and in aggregate generate equally as good of a sense labeling. 
To solve data sparsity problem, recently there has been a trend in discriminative methods of NLP to use representations of lexical items learned from unlabeled data as features. In this paper, we investigated the usage of word representations learned by neural language models, i.e. word embeddings. The direct usage has disadvantages such as large amount of computation, inadequacy with dealing word ambiguity and rare-words, and the problem of linear non-separability. To overcome these problems, we instead built compound features from continuous word embeddings based on clustering. Experiments showed that the compound features not only improved the performances on several NLP tasks, but also ran faster, suggesting the potential of embeddings. 
In the ﬁeld of Intelligent User Interfaces, Spoken Dialogue Systems (SDSs) play a key role as speech represents a true intuitive means of human communication. Deriving information about its quality can help rendering SDSs more user-adaptive. Work on automatic estimation of subjective quality usually relies on statistical models. To create those, manual data annotation is required, which may be performed by actual users or by experts. Here, both variants have their advantages and drawbacks. In this paper, we analyze the relationship between user and expert ratings by investigating models which combine the advantages of both types of ratings. We explore two novel approaches using statistical classiﬁcation methods and evaluate those with a preexisting corpus providing user and expert ratings. After analyzing the results, we eventually recommend to use expert ratings instead of user ratings in general. 
Large unsupervised latent variable models (LVMs) of text, such as Latent Dirichlet Allocation models or Hidden Markov Models (HMMs), are constructed using parallel training algorithms on computational clusters. The memory required to hold LVM parameters forms a bottleneck in training more powerful models. In this paper, we show how the memory required for parallel LVM training can be reduced by partitioning the training corpus to minimize the number of unique words on any computational node. We present a greedy document partitioning technique for the task. For large corpora, our approach reduces memory consumption by over 50%, and trains the same models up to three times faster, when compared with existing approaches for parallel LVM training. 
In cases in which there is no standard orthography for a language or language variant, written texts will display a variety of orthographic choices. This is problematic for natural language processing (NLP) because it creates spurious data sparseness. We study the transformation of spontaneously spelled Egyptian Arabic into a conventionalized orthography which we have previously proposed for NLP purposes. We show that a two-stage process can reduce divergences from this standard by 69%, making subsequent processing of Egyptian Arabic easier. 
Bibliometric measures are commonly used to estimate the popularity and the impact of published research. Existing bibliometric measures provide “quantitative” indicators of how good a published paper is. This does not necessarily reﬂect the “quality” of the work presented in the paper. For example, when hindex is computed for a researcher, all incoming citations are treated equally, ignoring the fact that some of these citations might be negative. In this paper, we propose using NLP to add a “qualitative” aspect to biblometrics. We analyze the text that accompanies citations in scientiﬁc articles (which we term citation context). We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation. 
Most NLP tools are applied to text that is different from the kind of text they were evaluated on. Common evaluation practice prescribes signiﬁcance testing across data points in available test data, but typically we only have a single test sample. This short paper argues that in order to assess the robustness of NLP tools we need to evaluate them on diverse samples, and we consider the problem of ﬁnding the most appropriate way to estimate the true effect size across datasets of our systems over their baselines. We apply meta-analysis and show experimentally – by comparing estimated error reduction over observed error reduction on held-out datasets – that this method is signiﬁcantly more predictive of success than the usual practice of using macro- or micro-averages. Finally, we present a new parametric meta-analysis based on nonstandard assumptions that seems superior to standard parametric meta-analysis. 
We present a systematic study of the effect of crowdsourced translations on Machine Translation performance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazon’s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation. 
Dependency analysis relies on morphosyntactic evidence, as well as semantic evidence. In some cases, however, morphosyntactic evidence seems to be in conﬂict with semantic evidence. For this reason dependency grammar theories, annotation guidelines and tree-to-dependency conversion schemes often differ in how they analyze various syntactic constructions. Most experiments for which constituent-based treebanks such as the Penn Treebank are converted into dependency treebanks rely blindly on one of four-ﬁve widely used tree-to-dependency conversion schemes. This paper evaluates the down-stream effect of choice of conversion scheme, showing that it has dramatic impact on end results. 
A discourse typically involves numerous entities, but few are mentioned more than once. Distinguishing discourse entities that die out after just one mention (singletons) from those that lead longer lives (coreferent) would beneﬁt NLP applications such as coreference resolution, protagonist identiﬁcation, topic modeling, and discourse coherence. We build a logistic regression model for predicting the singleton/coreferent distinction, drawing on linguistic insights about how discourse entity lifespans are affected by syntactic and semantic features. The model is effective in its own right (78% accuracy), and incorporating it into a state-of-the-art coreference resolution system yields a signiﬁcant improvement. 
A respelling is an alternative spelling of a word in the same writing system, intended to clarify pronunciation. We introduce the task of automatic generation of a respelling from the word’s phonemic representation. Our approach combines machine learning with linguistic constraints and electronic resources. We evaluate our system both intrinsically through a human judgment experiment, and extrinsically by passing its output to a letterto-phoneme converter. The results show that the respellings generated by our system are better on average than those found on the Web, and approach the quality of respellings designed by an expert. 
We present a simple log-linear reparameterization of IBM Model 2 that overcomes problems arising from Model 1’s strong assumptions and Model 2’s overparameterization. Efﬁcient inference, likelihood evaluation, and parameter estimation algorithms are provided. Training the model is consistently ten times faster than Model 4. On three large-scale translation tasks, systems built using our alignment model outperform IBM Model 4. An open-source implementation of the alignment model described in this paper is available from http://github.com/clab/fast align . 
We present a novel approach for translation model (TM) adaptation using phrase training. The proposed adaptation procedure is initialized with a standard general-domain TM, which is then used to perform phrase training on a smaller in-domain set. This way, we bias the probabilities of the general TM towards the in-domain distribution. Experimental results on two different lectures translation tasks show signiﬁcant improvements of the adapted systems over the general ones. Additionally, we compare our results to mixture modeling, where we report gains when using the suggested phrase training adaptation method. 
We propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora. The motivation is that, given a certain query term, it is often possible for a user to specify one or more synonyms. Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term’s context vector does not always reliably represent a terms meaning due to the context vector’s sparsity. Our proposed method uses a weighted average of the synonyms’ context vectors, that is derived by inferring the mean vector of the von Mises-Fisher distribution. We evaluate our method, using the synsets from the cross-lingually aligned Japanese and English WordNet. The experiments show that our proposed method signiﬁcantly improves translation accuracy when compared to a previous method for smoothing context vectors. 
We consider the task of tagging Arabic nouns with WordNet supersenses. Three approaches are evaluated. The ﬁrst uses an expertcrafted but limited-coverage lexicon, Arabic WordNet, and heuristics. The second uses unsupervised sequence modeling. The third and most successful approach uses machine translation to translate the Arabic into English, which is automatically tagged with English supersenses, the results of which are then projected back into Arabic. Analysis shows gains and remaining obstacles in four Wikipedia topical domains. 
Inspired by robust generalization and adversarial learning we describe a novel approach to learning structured perceptrons for part-ofspeech (POS) tagging that is less sensitive to domain shifts. The objective of our method is to minimize average loss under random distribution shifts. We restrict the possible target distributions to mixtures of the source distribution and random Zipﬁan distributions. Our algorithm is used for POS tagging and evaluated on the English Web Treebank and the Danish Dependency Treebank with an average 4.4% error reduction in tagging accuracy. 
We adapt the popular LDA topic model (Blei et al., 2003) to the representation of stylistic lexical information, evaluating our model on the basis of human-interpretability at the word and text level. We show, in particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions. 
The topic of a document can prove to be useful information for Word Sense Disambiguation (WSD) since certain meanings tend to be associated with particular topics. This paper presents an LDA-based approach for WSD, which is trained using any available WSD system to establish a sense per (Latent Dirichlet allocation based) topic. The technique is tested using three unsupervised and one supervised WSD algorithms within the SPORT and FINANCE domains giving a performance increase each time, suggesting that the technique may be useful to improve the performance of any available WSD system. 
Multi-Domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains. However, real-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classiﬁcation. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 
This opinion piece proposes that recent advances in opinion detection are limited in the extent to which they can detect important categories of opinion because they are not designed to capture some of the pragmatic aspects of opinion. A component of these is the perspective of the user of an opinion-mining system as to what an opinion really is, which is in itself a matter of opinion (metasubjectivity). We propose a way to deﬁne this component of opinion and describe the challenges it poses for corpus development and sentence-level detection technologies. Finally, we suggest that investment in techniques to handle metasubjectivity will likely bear costs but bring beneﬁts in the longer term. 
Social media users who post bullying related tweets may later experience regret, potentially causing them to delete their posts. In this paper, we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted. We then conduct exploratory analysis in order to isolate factors associated with deleted posts. Finally, we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret. 
We investigate two systems for automatic disﬂuency detection on English and Mandarin conversational speech data. The ﬁrst system combines various lexical and prosodic features in a Conditional Random Field model for detecting edit disﬂuencies. The second system combines acoustic and language model scores for detecting ﬁlled pauses through constrained speech recognition. We compare the contributions of different knowledge sources to detection performance between these two languages. 
Atypical semantic and pragmatic expression is frequently reported in the language of children with autism. Although this atypicality often manifests itself in the use of unusual or unexpected words and phrases, the rate of use of such unexpected words is rarely directly measured or quantiﬁed. In this paper, we use distributional semantic models to automatically identify unexpected words in narrative retellings by children with autism. The classiﬁcation of unexpected words is sufﬁciently accurate to distinguish the retellings of children with autism from those with typical development. These techniques demonstrate the potential of applying automated language analysis techniques to clinically elicited language data for diagnostic purposes. 
Automatic assessment of reading ability builds on applying speech recognition tools to oral reading, measuring words correct per minute. This work looks at more ﬁne-grained analysis that accounts for effects of prosodic context using a large corpus of read speech from a literacy study. Experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be ﬁnal in a prosodic phrase, i.e. in less appropriate locations. The results have implications for automatic assessment of text difﬁculty in that locations of atypical prosodic lengthening are indicative of difﬁcult lexical items and syntactic constructions. 
In this paper we leverage methods from submodular function optimization developed for document summarization and apply them to the problem of subselecting acoustic data. We evaluate our results on data subset selection for a phone recognition task. Our framework shows signiﬁcant improvements over random selection and previously proposed methods using a similar amount of resources. 
One way to improve the accuracy of automatic speech recognition (ASR) is to use discriminative language modeling (DLM), which enhances discrimination by learning where the ASR hypotheses deviate from the uttered sentences. However, DLM requires large amounts of ASR output to train. Instead, we can simulate the output of an ASR system, in which case the training becomes semisupervised. The advantage of using simulated hypotheses is that we can generate as many hypotheses as we want provided that we have enough text material. In typical scenarios, transcribed in-domain data is limited but large amounts of out-of-domain (OOD) data is available. In this study, we investigate how semi-supervised training performs with OOD data. We ﬁnd out that OOD data can yield improvements comparable to in-domain data. 
Word Sense Disambiguation (WSD) approaches have reported good accuracies in recent years. However, these approaches can be classiﬁed as weak AI systems. According to the classical deﬁnition, a strong AI based WSD system should perform the task of sense disambiguation in the same manner and with similar accuracy as human beings. In order to accomplish this, a detailed understanding of the human techniques employed for sense disambiguation is necessary. Instead of building yet another WSD system that uses contextual evidence for sense disambiguation, as has been done before, we have taken a step back - we have endeavored to discover the cognitive faculties that lie at the very core of the human sense disambiguation technique. In this paper, we present a hypothesis regarding the cognitive sub-processes involved in the task of WSD. We support our hypothesis using the experiments conducted through the means of an eye-tracking device. We also strive to ﬁnd the levels of difﬁculties in annotating various classes of words, with senses. We believe, once such an in-depth analysis is performed, numerous insights can be gained to develop a robust WSD system that conforms to the principle of strong AI. 
Sentence Similarity [SS] computes a similarity score between two sentences. The SS task differs from document level semantics tasks in that it features the sparsity of words in a data unit, i.e. a sentence. Accordingly it is crucial to robustly model each word in a sentence to capture the complete semantic picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 
We propose a novel semantic annotation type of assigning truth values to predicate occurrences, and present TruthTeller, a standalone publiclyavailable tool that produces such annotations. TruthTeller integrates a range of semantic phenomena, such as negation, modality, presupposition, implicativity, and more, which were dealt only partly in previous works. Empirical evaluations against human annotations show satisfactory results and suggest the usefulness of this new type of tool for NLP. 
We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff. 
This paper presents an approach that exploits the scope of negation cues for relation extraction (RE) without the need of using any specifically annotated dataset for building a separate negation scope detection classiﬁer. New features are proposed which are used in two different stages. These also include non-target entity speciﬁc features. The proposed RE approach outperforms the previous state of the art for drug-drug interaction (DDI) extraction. 
Iterative bootstrapping methods are widely employed for relation extraction, especially because they require only a small amount of human supervision. Unfortunately, a phenomenon known as semantic drift can affect the accuracy of iterative bootstrapping and lead to poor extractions. This paper proposes an alternative bootstrapping method, which ranks relation tuples by measuring their distance to the seed tuples in a bipartite tuple-pattern graph. In contrast to previous bootstrapping methods, our method is not susceptible to semantic drift, and it empirically results in better extractions than iterative methods. 
Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. In this paper, we show that a signiﬁcant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious ﬂaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 
In this paper, we propose a novel Rhetorical Structure Index (RSI) to measure the structural importance of a word or a phrase. Unlike TF-IDF and other content-driven measurements, RSI identiﬁes words or phrases that are structural cues in an unstructured document. We show structurally motivated features with high RSI values are more useful than content-driven features for applications such as segmenting unstructured lecture transcripts into meaningful segments. Experiments show that using RSI signiﬁcantly improves the segmentation accuracy compared to TF-IDF, a traditional content-based feature weighting scheme. 
Twitter has been shown to be a fast and reliable method for disease surveillance of common illnesses like inﬂuenza. However, previous work has relied on simple content analysis, which conﬂates ﬂu tweets that report infection with those that express concerned awareness of the ﬂu. By discriminating these categories, as well as tweets about the authors versus about others, we demonstrate signiﬁcant improvements on inﬂuenza surveillance using Twitter. 
Wizard-of-Oz experimental setup in a dialogue system is commonly used to gather data for informing an automated version of that system. Previous work has exposed dependencies between user behavior towards systems and user belief about whether the system is automated or human-controlled. This work examines whether user behavior changes when user belief is held constant and the system’s operator is varied. We perform a posthoc experiment using generalizable prosodic and lexical features of user responses to a dialogue system backed with and without a human wizard. Our results suggest that user responses are different when communicating with a wizarded and an automated system, indicating that wizard data may be less reliable for informing automated systems than generally assumed. 
We present a method of improving the performance of dialog act tagging in identifying minority classes by using per-class feature optimization and a method of choosing the class based not on conﬁdence, but on a cascade of classiﬁers. We show that it gives a minority class F-measure error reduction of 22.8%, while also reducing the error for other classes and the overall error by about 10%. 
Document-level sentiment analysis can beneﬁt from ﬁne-grained subjectivity, so that sentiment polarity judgments are based on the relevant parts of the document. While ﬁnegrained subjectivity annotations are rarely available, encouraging results have been obtained by modeling subjectivity as a latent variable. However, latent variable models fail to capitalize on our linguistic knowledge about discourse structure. We present a new method for injecting linguistic knowledge into latent variable subjectivity modeling, using discourse connectors. Connector-augmented transition features allow the latent variable model to learn the relevance of discourse connectors for subjectivity transitions, without subjectivity annotations. This yields significantly improved performance on documentlevel sentiment analysis in English and Spanish. We also describe a simple heuristic for automatically identifying connectors when no predeﬁned list is available. 
This study focuses on modeling discourse coherence in the context of automated assessment of spontaneous speech from non-native speakers. Discourse coherence has always been used as a key metric in human scoring rubrics for various assessments of spoken language. However, very little research has been done to assess a speaker's coherence in automated speech scoring systems. To address this, we present a corpus of spoken responses that has been annotated for discourse coherence quality. Then, we investigate the use of several features originally developed for essays to model coherence in spoken responses. An analysis on the annotated corpus shows that the prediction accuracy for human holistic scores of an automated speech scoring system can be improved by around 10% relative after the addition of the coherence features. Further experiments indicate that a weighted FMeasure of 73% can be achieved for the automated prediction of the coherence scores. 
In this paper, we propose a multi-step stacked learning model for disﬂuency detection. Our method incorporates reﬁned n-gram features step by step from different word sequences. First, we detect ﬁller words. Second, edited words are detected using n-gram features extracted from both the original text and ﬁller ﬁltered text. In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection. We use Max-Margin Markov Networks (M3Ns) as the classiﬁer with the weighted hamming loss to balance precision and recall. Experiments on the Switchboard corpus show that the reﬁned n-gram features from multiple steps and M3Ns with weighted hamming loss can significantly improve the performance. Our method for disﬂuency detection achieves the best reported F-score 0.841 without the use of additional resources.1  
We consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. However, a given regular expression can be written in many semantically equivalent forms, and we exploit this ﬂexibility to facilitate translation by ﬁnding a form which more directly corresponds to the natural language. We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a stateof-the-art semantic parsing baseline, yielding a 29% absolute improvement in accuracy.1 
In natural-language discourse, related events tend to appear near each other to describe a larger scenario. Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difﬁcult to diagnose or extend. In this paper, we propose the ﬁrst probabilistic approach to frame induction, which incorporates frames, events, and participants as latent topics and learns those frame and event transitions that best explain the text. The number of frame components is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produces state-of-the-art results while substantially reducing engineering effort. 
In this paper we explore the potential of quantum theory as a formal framework for capturing lexical meaning. We present a novel semantic space model that is syntactically aware, takes word order into account, and features key quantum aspects such as superposition and entanglement. We deﬁne a dependency-based Hilbert space and show how to represent the meaning of words by density matrices that encode dependency neighborhoods. Experiments on word similarity and association reveal that our model achieves results competitive with a variety of classical models. 
Our goal is to extract answers from preretrieved sentences for Question Answering (QA). We construct a linear-chain Conditional Random Field based on pairs of questions and their possible answer sentences, learning the association between questions and answer types. This casts answer extraction as an answer sequence tagging problem for the ﬁrst time, where knowledge of shared structure between question and source sentence is incorporated through features based on Tree Edit Distance (TED). Our model is free of manually created question and answer templates, fast to run (processing 200 QA pairs per second excluding parsing time), and yields an F1 of 63.3% on a new public dataset based on prior TREC QA evaluations. The developed system is open-source, and includes an implementation of the TED model that is state of the art in the task of ranking QA pairs. 
Traditional relation extraction seeks to identify pre-speciﬁed semantic relations within natural language text, while open Information Extraction (Open IE) takes a more general approach, and looks for a variety of relations without restriction to a ﬁxed relation set. With this generalization comes the question, what is a relation? For example, should the more general task be restricted to relations mediated by verbs, nouns, or both? To help answer this question, we propose two levels of subtasks for Open IE. One task is to determine if a sentence potentially contains a relation between two entities? The other task looks to conﬁrm explicit relation words for two entities. We propose multiple SVM models with dependency tree kernels for both tasks. For explicit relation extraction, our system can extract both noun and verb relations. Our results on three datasets show that our system is superior when compared to state-of-the-art systems like REVERB and OLLIE for both tasks. For example, in some experiments our system achieves 33% improvement on nominal relation extraction over OLLIE. In addition we propose an unsupervised rule-based approach which can serve as a strong baseline for Open IE systems. 
In natural language question answering (QA) systems, questions often contain terms and phrases that are critically important for retrieving or ﬁnding answers from documents. We present a learnable system that can extract and rank these terms and phrases (dubbed mandatory matching phrases or MMPs), and demonstrate their utility in a QA system on Internet discussion forum data sets. The system relies on deep syntactic and semantic analysis of questions only and is independent of relevant documents. Our proposed model can predict MMPs with high accuracy. When used in a QA system features derived from the MMP model improve performance signiﬁcantly over a state-of-the-art baseline. The ﬁnal QA system was the best performing system in the DARPA BOLT-IR evaluation. 
In a meeting, it is often desirable to extract keywords from each utterance as soon as it is spoken. Thus, this paper proposes a just-intime keyword extraction from meeting transcripts. The proposed method considers two major factors that make it different from keyword extraction from normal texts. The ﬁrst factor is the temporal history of preceding utterances that grants higher importance to recent utterances than old ones, and the second is topic relevance that forces only the preceding utterances relevant to the current utterance to be considered in keyword extraction. Our experiments on two data sets in English and Korean show that the consideration of the factors results in performance improvement in keyword extraction from meeting transcripts. 
Coreference resolution systems rely heavily on string overlap (e.g., Google Inc. and Google), performing badly on mentions with very different words (opaque mentions) like Google and the search giant. Yet prior attempts to resolve opaque pairs using ontologies or distributional semantics hurt precision more than improved recall. We present a new unsupervised method for mining opaque pairs. Our intuition is to restrict distributional semantics to articles about the same event, thus promoting referential match. Using an English comparable corpus of tech news, we built a dictionary of opaque coreferent mentions (only 3% are in WordNet). Our dictionary can be integrated into any coreference system (it increases the performance of a state-of-the-art system by 1% F1 on all measures) and is easily extendable by using news aggregators. 
We present the ﬁrst work on antecedent selection for bridging resolution without restrictions on anaphor or relation types. Our model integrates global constraints on top of a rich local feature set in the framework of Markov logic networks. The global model improves over the local one and both strongly outperform a reimplementation of prior work. 
We examine the task of temporal relation classiﬁcation. Unlike existing approaches to this task, we (1) classify an event-event or eventtime pair as one of the 14 temporal relations deﬁned in the TimeBank corpus, rather than as one of the six relations collapsed from the original 14; (2) employ sophisticated linguistic knowledge derived from a variety of semantic and discourse relations, rather than focusing on morpho-syntactic knowledge; and (3) leverage a novel combination of rule-based and learning-based approaches, rather than relying solely on one or the other. Experiments with the TimeBank corpus demonstrate that our knowledge-rich, hybrid approach yields a 15–16% relative reduction in error over a state-of-the-art learning-based baseline system. 
Inferring the information structure of scientiﬁc documents is useful for many downstream applications. Existing feature-based machine learning approaches to this task require substantial training data and suffer from limited performance. Our idea is to guide feature-based models with declarative domain knowledge encoded as posterior distribution constraints. We explore a rich set of discourse and lexical constraints which we incorporate through the Generalized Expectation (GE) criterion. Our constrained model improves the performance of existing fully and lightly supervised models. Even a fully unsupervised version of this model outperforms lightly supervised feature-based models, showing that our approach can be useful even when no labeled data is available. 
Previous research on domain adaptation (DA) for statistical machine translation (SMT) has mainly focused on the translation model (TM) and the language model (LM). To the best of our knowledge, there is no previous work on reordering model (RM) adaptation for phrasebased SMT. In this paper, we demonstrate that mixture model adaptation of a lexicalized RM can signiﬁcantly improve SMT performance, even when the system already contains a domain-adapted TM and LM. We ﬁnd that, surprisingly, different training corpora can vary widely in their reordering characteristics for particular phrase pairs. Furthermore, particular training corpora may be highly suitable for training the TM or the LM, but unsuitable for training the RM, or vice versa, so mixture weights for these models should be estimated separately. An additional contribution of the paper is to propose two improvements to mixture model adaptation: smoothing the in-domain sample, and weighting instances by document frequency. Applied to mixture RMs in our experiments, these techniques (especially smoothing) yield signiﬁcant performance improvements. 
This paper examines tuning for statistical machine translation (SMT) with respect to multiple evaluation metrics. We propose several novel methods for tuning towards multiple objectives, including some based on ensemble decoding methods. Pareto-optimality is a natural way to think about multi-metric optimization (MMO) and our methods can effectively combine several Pareto-optimal solutions, obviating the need to choose one. Our best performing ensemble tuning method is a new algorithm for multi-metric optimization that searches for Pareto-optimal ensemble models. We study the effectiveness of our methods through experiments on multiple as well as single reference(s) datasets. Our experiments show simultaneous gains across several metrics (BLEU, RIBES), without any signiﬁcant reduction in other metrics. This contrasts the traditional tuning where gains are usually limited to a single metric. Our human evaluation results conﬁrm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 
We propose a new algorithm to approximately extract top-scoring hypotheses from a hypergraph when the score includes an N –gram language model. In the popular cube pruning algorithm, every hypothesis is annotated with boundary words and permitted to recombine only if all boundary words are equal. However, many hypotheses share some, but not all, boundary words. We use these common boundary words to group hypotheses and do so recursively, resulting in a tree of hypotheses. This tree forms the basis for our new search algorithm that iteratively reﬁnes groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 
The dominant yet ageing IBM and HMM word alignment models underpin most popular Statistical Machine Translation implementations in use today. Though beset by the limitations of implausible independence assumptions, intractable optimisation problems, and an excess of tunable parameters, these models provide a scalable and reliable starting point for inducing translation systems. In this paper we build upon this venerable base by recasting these models in the non-parametric Bayesian framework. By replacing the categorical distributions at their core with hierarchical Pitman-Yor processes, and through the use of collapsed Gibbs sampling, we provide a more ﬂexible formulation and sidestep the original heuristic optimisation techniques. The resulting models are highly extendible, naturally permitting the introduction of phrasal dependencies. We present extensive experimental results showing improvements in both AER and BLEU when benchmarked against Giza++, including signiﬁcant improvements over IBM model 4. 
We present a novel approach to automatic metaphor identiﬁcation, that discovers both metaphorical associations and metaphorical expressions in unrestricted text. Our system ﬁrst performs hierarchical graph factorization clustering (HGFC) of nouns and then searches the resulting graph for metaphorical connections between concepts. It then makes use of the salient features of the metaphorically connected clusters to identify the actual metaphorical expressions. In contrast to previous work, our method is fully unsupervised. Despite this fact, it operates with an encouraging precision (0.69) and recall (0.61). Our approach is also the ﬁrst one in NLP to exploit the cognitive ﬁndings on the differences in organisation of abstract and concrete concepts in the human brain. 
We present three approaches to lexical chaining based on the LDA topic model and evaluate them intrinsically on a manually annotated set of German documents. After motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains, we describe our new two-level chain annotation scheme, which rooted in the concept of cohesive harmony. Also, we propose a new measure for direct evaluation of lexical chains. Our three LDA-based approaches outperform two knowledge-based state-of-the art methods to lexical chaining by a large margin, which can be attributed to lacking coverage of the knowledge resource. Subsequent analysis shows that the three methods yield a different chaining behavior, which could be utilized in tasks that use lexical chaining as a component within NLP applications. 
Hidden properties of social media users, such as their ethnicity, gender, and location, are often reﬂected in their observed attributes, such as their ﬁrst and last names. Furthermore, users who communicate with each other often have similar hidden properties. We propose an algorithm that exploits these insights to cluster the observed attributes of hundreds of millions of Twitter users. Attributes such as user names are grouped together if users with those names communicate with other similar users. We separately cluster millions of unique ﬁrst names, last names, and userprovided locations. The efﬁcacy of these clusters is then evaluated on a diverse set of classiﬁcation tasks that predict hidden users properties such as ethnicity, geographic location, gender, language, and race, using only proﬁle names and locations when appropriate. Our readily-replicable approach and publiclyreleased clusters are shown to be remarkably effective and versatile, substantially outperforming state-of-the-art approaches and human accuracy on each of the tasks studied. 
Information extraction from microblog posts is an important task, as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world. As the core component of information extraction, we consider the task of Twitter entity linking in this paper. In the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems. However, in our task, we ﬁnd that mention detection is often the performance bottleneck. The reason is that messages on micro-blogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings. To rigorously address the Twitter entity linking problem, we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task. By combining structural learning and a variety of ﬁrstorder, second-order, and context-sensitive features, our system is able to outperform existing state-of-the art entity linking systems by 15% F1. 
Threaded discussion forums provide an important social media platform. Its rich user generated content has served as an important source of public feedback. To automatically discover the viewpoints or stances on hot issues from forum threads is an important and useful task. In this paper, we propose a novel latent variable model for viewpoint discovery from threaded forum posts. Our model is a principled generative latent variable model which captures three important factors: viewpoint speciﬁc topic preference, user identity and user interactions. Evaluation results show that our model clearly outperforms a number of baseline models in terms of both clustering posts based on viewpoints and clustering users with different viewpoints. 
This paper proposes to study the problem of identifying intention posts in online discussion forums. For example, in a discussion forum, a user wrote “I plan to buy a camera,” which indicates a buying intention. This intention can be easily exploited by advertisers. To the best of our knowledge, there is still no reported study of this problem. Our research found that this problem is particularly suited to transfer learning because in different domains, people express the same intention in similar ways. We then propose a new transfer learning method which, unlike a general transfer learning algorithm, exploits several special characteristics of the problem. Experimental results show that the proposed method outperforms several strong baselines, including supervised learning in the target domain and a recent transfer learning method. 
We describe a novel approach to detecting empty categories (EC) as represented in dependency trees as well as a new metric for measuring EC detection accuracy. The new metric takes into account not only the position and type of an EC, but also the head it is a dependent of in a dependency tree. We also introduce a variety of new features that are more suited for this approach. Tested on a subset of the Chinese Treebank, our system improved signiﬁcantly over the best previously reported results even when evaluated with this more stringent metric. 
We study multi-source transfer parsing for resource-poor target languages; speciﬁcally methods for target language adaptation of delexicalized discriminative graph-based dependency parsers. We ﬁrst show how recent insights on selective parameter sharing, based on typological and language-family features, can be applied to a discriminative parser by carefully decomposing its model features. We then show how the parser can be relexicalized and adapted using unlabeled target language data and a learning method that can incorporate diverse knowledge sources through ambiguous labelings. In the latter scenario, we exploit two sources of knowledge: arc marginals derived from the base parser in a self-training algorithm, and arc predictions from multiple transfer parsers in an ensemble-training algorithm. Our ﬁnal model outperforms the state of the art in multi-source transfer parsing on 15 out of 16 evaluated languages. 
Grice characterized communication in terms of the cooperative principle, which enjoins speakers to make only contributions that serve the evolving conversational goals. We show that the cooperative principle and the associated maxims of relevance, quality, and quantity emerge from multi-agent decision theory. We utilize the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) model of multi-agent decision making which relies only on basic deﬁnitions of rationality and the ability of agents to reason about each other’s beliefs in maximizing joint utility. Our model uses cognitively-inspired heuristics to simplify the otherwise intractable task of reasoning jointly about actions, the environment, and the nested beliefs of other actors. Our experiments on a cooperative language task show that reasoning about others’ belief states, and the resulting emergent Gricean communicative behavior, leads to signiﬁcantly improved task performance. 
We present open dialogue management and its application to relational databases. An open dialogue manager generates dialogue states, actions, and default strategies from the semantics of its application domain. We define three open dialogue management tasks. First, vocabulary selection finds the intelligible attributes in each database table. Second, focus discovery selects candidate dialogue foci, tables that have the most potential to address basic user goals. Third, a focus agent is instantiated for each dialogue focus with a default dialogue strategy governed by efficiency. We demonstrate the portability of open dialogue management on three very different databases. Evaluation of our system with simulated users shows that users with realistically limited domain knowledge have dialogues nearly as efficient as those of users with complete domain knowledge. 
This paper explores the relationship between explicit and predictive models of incremental speech understanding in a dialogue system that supports a ﬁnite set of user utterance meanings. We present a method that enables the approximation of explicit understanding using information implicit in a predictive understanding model for the same domain. We show promising performance for this method in a corpus evaluation, and discuss its practical application and annotation costs in relation to some alternative approaches. 
In this paper we propose a new approach to the generation of pseudowords, i.e., artiﬁcial words which model real polysemous words. Our approach simultaneously addresses the two important issues that hamper the generation of large pseudosense-annotated datasets: semantic awareness and coverage. We evaluate these pseudowords from three different perspectives showing that they can be used as reliable substitutes for their real counterparts. 
In this paper we consider the problem of labeling the languages of words in mixed-language documents. This problem is approached in a weakly supervised fashion, as a sequence labeling problem with monolingual text samples for training data. Among the approaches evaluated, a conditional random ﬁeld model trained with generalized expectation criteria was the most accurate and performed consistently as the amount of training data was varied. 
Non-expert annotation services like Amazon’s Mechanical Turk (AMT) are cheap and fast ways to evaluate systems and provide categorical annotations for training data. Unfortunately, some annotators choose bad labels in order to maximize their pay. Manual identiﬁcation is tedious, so we experiment with an item-response model. It learns in an unsupervised fashion to a) identify which annotators are trustworthy and b) predict the correct underlying labels. We match performance of more complex state-of-the-art systems and perform well even under adversarial conditions. We show considerable improvements over standard baselines, both for predicted label accuracy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most conﬁdent in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1.  
We propose a supervised lexical substitution system that does not use separate classiﬁers per word and is therefore applicable to any word in the vocabulary. Instead of learning word-speciﬁc substitution patterns, a global model for lexical substitution is trained on delexicalized (i.e., non lexical) features, which allows to exploit the power of supervised methods while being able to generalize beyond target words in the training set. This way, our approach remains technically straightforward, provides better performance and similar coverage in comparison to unsupervised approaches. Using features from lexical resources, as well as a variety of features computed from large corpora (n-gram counts, distributional similarity) and a ranking method based on the posterior probabilities obtained from a Maximum Entropy classiﬁer, we improve over the state of the art in the LexSub Best-Precision metric and the Generalized Average Precision measure. Robustness of our approach is demonstrated by evaluating it successfully on two different datasets. 
In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use our method to model the composition of subject verb object triples. The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce a latent model of three-way subject verb object interactions. Our model has been evaluated on a similarity task for transitive phrases, in which it exceeds the state of the art. 
Twitter offers an unprecedented advantage on live reporting of the events happening around the world. However, summarizing the Twitter event has been a challenging task that was not fully explored in the past. In this paper, we propose a participant-based event summarization approach that “zooms-in” the Twitter event streams to the participant level, detects the important sub-events associated with each participant using a novel mixture model that combines the “burstiness” and “cohesiveness” properties of the event tweets, and generates the event summaries progressively. We evaluate the proposed approach on different event types. Results show that the participantbased approach can effectively capture the sub-events that have otherwise been shadowed by the long-tail of other dominant sub-events, yielding summaries with considerably better coverage than the state-of-the-art. 
This paper presents G-FLOW, a novel system for coherent extractive multi-document summarization (MDS).1 Where previous work on MDS considered sentence selection and ordering separately, G-FLOW introduces a joint model for selection and ordering that balances coherence and salience. G-FLOW’s core representation is a graph that approximates the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference, and more. This graph enables G-FLOW to estimate the coherence of a candidate summary. We evaluate G-FLOW on Mechanical Turk, and ﬁnd that it generates dramatically better summaries than an extractive summarizer based on a pipeline of state-of-the-art sentence selection and reordering components, underscoring the value of our joint model. 
We introduce a novel algorithm for generating referring expressions, informed by human and computer vision and designed to refer to visible objects. Our method separates absolute properties like color from relative properties like size to stochastically generate a diverse set of outputs. Expressions generated using this method are often overspeciﬁed and may be underspeciﬁed, akin to expressions produced by people. We call such expressions identifying descriptions. The algorithm outperforms the well-known Incremental Algorithm (Dale and Reiter, 1995) and the GraphBased Algorithm (Krahmer et al., 2003; Viethen et al., 2008) across a variety of images in two domains. We additionally motivate an evaluation method for referring expression generation that takes the proposed algorithm’s non-determinism into account. 
We describe a supervised approach to predicting the set of all inﬂected forms of a lexical item. Our system automatically acquires the orthographic transformation rules of morphological paradigms from labeled examples, and then learns the contexts in which those transformations apply using a discriminative sequence model. Because our approach is completely data-driven and the model is trained on examples extracted from Wiktionary, our method can extend to new languages without change. Our end-to-end system is able to predict complete paradigms with 86.1% accuracy and individual inﬂected forms with 94.9% accuracy, averaged across three languages and two parts of speech. 
In this paper we introduce the task of unlabeled, optimal, data set selection. Given a large pool of unlabeled examples, our goal is to select a small subset to label, which will yield a high performance supervised model over the entire data set. Our ﬁrst proposed method, based on the rank-revealing QR matrix factorization, selects a subset of words which span the entire word-space effectively. For our second method, we develop the concept of feature coverage which we optimize with a greedy algorithm. We apply these methods to the task of grapheme-to-phoneme prediction. Experiments over a data-set of 8 languages show that in all scenarios, our selection methods are effective at yielding a small, but optimal set of labelled examples. When fed into a state-of-the-art supervised model for grapheme-to-phoneme prediction, our methods yield average error reductions of 20% over randomly selected examples. 
We present a morphology-aware nonparametric Bayesian model of language whose prior distribution uses manually constructed ﬁnitestate transducers to capture the word formation processes of particular languages. This relaxes the word independence assumption and enables sharing of statistical strength across, for example, stems or inﬂectional paradigms in different contexts. Our model can be used in virtually any scenario where multinomial distributions over words would be used. We obtain state-of-the-art results in language modeling, word alignment, and unsupervised morphological disambiguation for a variety of morphologically rich languages. 
I was of course delighted to learn of this award. The most we can hope for in life is to take part in the conversation, and an award like this means that you’ve taken part in the conversation. 
Transition-based parsing is a widely used approach for dependency parsing that combines high efﬁciency with expressive feature models. Many different transition systems have been proposed, often formalized in slightly different frameworks. In this article, we show that a large number of the known systems for projective dependency parsing can be viewed as variants of the same stack-based system with a small set of elementary transitions that can be composed into complex transitions and restricted in different ways. We call these systems divisible transition systems and prove a number of theoretical results about their expressivity and complexity. In particular, we characterize an important subclass called efﬁcient divisible transition systems that parse planar dependency graphs in linear time. We go on to show, ﬁrst, how this system can be restricted to capture exactly the set of planar dependency trees and, secondly, how the system can be generalized to k-planar trees by making use of multiple stacks. Using the ﬁrst known efﬁcient test for k-planarity, we investigate the coverage of k-planar trees in available dependency treebanks and ﬁnd a very good ﬁt for 2-planar trees. We end with an experimental evaluation showing that our 2-planar parser gives signiﬁcant improvements in parsing accuracy over the corresponding 1-planar and projective parsers for data sets with non-projective dependency trees and performs on a par with the widely used arc-eager pseudo-projective parser. 1. Introduction Syntactic parsing using dependency-based representations has attracted considerable interest in computational linguistics in recent years, both because it appears to provide a useful interface to downstream applications of parsing and because many dependency parsers combine competitive parsing accuracy with highly efﬁcient processing. Among the most efﬁcient systems available are transition-based dependency parsers, which perform a greedy search through a transition system, or abstract state machines, that map sentences to dependency trees, guided by statistical models trained on treebank ∗ Departamento de Computacio´ n, Universidade da Corun˜ a, Facultad de Informtica, Campus de Elvin˜ a s/n, 15071 A Corun˜ a, Spain. E-mail: cgomezr@udc.es. ∗∗ Department of Linguistics and Philology, Uppsala University, Box 635, 75126 Uppsala, Sweden. E-mail: joakim.nivre@lingfil.uu.se. Submission received: 13 October 2011; revised submission received: 29 August 2012; accepted for publication: 7 November 2012. doi:10.1162/COLI a 00150 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 4  data (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Zhang and Clark 2008). Transition systems for dependency parsing come in many different varieties, and our aim in the ﬁrst part of this article is to deepen our understanding of these systems by analyzing them in a uniform framework. More precisely, we demonstrate that a number of well-known systems from the literature can all be viewed as variants of a stack-based system with ﬁve elementary transitions, where different variants are obtained by composing elementary transitions into complex transitions and by adding restrictions on their applicability. We call such systems divisible transition systems and prove a number of theoretical results about their expressivity (which classes of dependency graphs they can handle) and their complexity (what upper bounds exist on the length of transition sequences). In particular, we show that an important subclass called efﬁcient divisible transition systems derive planar dependency graphs in time that is linear in the length of the sentence using standard inference methods for transition-based dependency parsing. Even though many of these results were already known for particular systems, the general framework allows us to derive these results from more general principles and thereby to establish connections between previously unrelated systems. We then go on to show that there are interesting cases of efﬁcient divisible transition systems that have not yet been explored, notably a system that is sound and complete for planar dependency trees, a mild extension to the class of projective trees that are assumed in most existing systems. In the second part of the article, we take the planar parsing system as our point of departure for addressing the problem of non-projective dependency parsing. Despite the impressive results obtained with dependency parsers limited to strictly projective dependency trees—that is, trees where every subtree has a contiguous yield—it is clear that most if not all languages have syntactic constructions whose analysis requires nonprojective trees. It is also clear, however, that allowing arbitrary non-projective trees makes parsing computationally hard (McDonald and Satta 2007) and does not seem justiﬁed by the data in available treebanks (Kuhlmann and Nivre 2006; Nivre 2006a; Havelka 2007). This suggests that we should try to ﬁnd a superset of projective trees that is permissive enough to encompass constructions found in natural language yet restricted enough to permit efﬁcient parsing. Proposals for such a set include trees with bounded arc degree (Nivre 2006a, 2007), well-nested trees with bounded gap degree (Kuhlmann and Nivre 2006; Kuhlmann and Mo¨ hl 2007), as well as trees parsable by a particular transition system such as that proposed by Attardi (2006). In the same vein, Yli-Jyra¨ (2003) introduced the concept of multiplanarity, which generalizes the simple notion of planarity by saying that a dependency tree is k-planar if it can be decomposed into at most k planar subgraphs, a proposal that remains largely unexplored because an efﬁcient test for k-planarity has been lacking. In this article, we construct a test for k-planarity by reducing it to a graph coloring problem. Applying this test to a wide range of dependency treebanks, we show that, although simple planarity (or 1-planarity) is clearly insufﬁcient (Kuhlmann and Nivre 2006), the set of 2-planar dependency trees gives a very good ﬁt with the available data, better than many of the previously proposed superclasses of projective trees. We then demonstrate how the transition system for planar dependency parsing can be generalized to k-planarity by introducing additional stacks. In particular, we deﬁne a two-stack system for 2-planar dependency parsing that is provably correct and has linear complexity. Finally, we show that the 2-planar parser, when evaluated on data sets with a non-negligible proportion of non-projective trees, gives signiﬁcant improvements in parsing accuracy over the corresponding 1-planar and projective parsers, and provides comparable accuracy to the widely used arc-eager pseudo-projective parser. 800  Go´ mez-Rodr´ıguez and Nivre  Divisible Transition Systems and Multiplanarity  The remainder of the article is structured as follows. Section 2 reviews basic concepts of dependency parsing and in particular the formalization of stack-based transition systems from Nivre (2008). Section 3 introduces our system of elementary transitions, uses it to analyze a number of parsing algorithms from the literature as divisible transition systems, proves a number of theoretical results about the expressivity and complexity of such systems, and ﬁnally introduces a divisible transition system for 1-planar dependency parsing. Section 4 reviews the notion of multiplanarity, introduces an efﬁcient procedure for determining the smallest k for which a dependency tree is k-planar, and uses this procedure in an empirical investigation of available dependency treebanks. Section 5 shows how the divisible transition system framework and the 1-planar parser can be generalized to handle k-planar trees by introducing additional stacks, presents proofs of correctness and complexity for the 2-planar case, and reports the results of an experimental evaluation of projective, pseudo-projective, 1-planar and 2-planar dependency parsing. Section 6 reviews related work, and Section 7 concludes and makes suggestions for future research. Part of the contributions in this article (namely, the test for multiplanarity and the 1-planar and 2-planar parsers) have been published previously by Go´ mez-Rodr´ıguez and Nivre (2010); this article substantially revises and extends the ideas presented in that paper. The framework of divisible transition systems and all the derived theoretical results, including the properties and proofs regarding the 1-planar and 2-planar parsers, are entirely new contributions of this article. 2. Dependency Parsing Dependency parsing is based on the idea that syntactic structure can be analyzed in terms of binary, asymmetric relations between the words of a sentence, an idea that has a long tradition in descriptive and theoretical linguistics (Tesnie`re 1959; Sgall, Hajicˇova´, and Panevova´ 1986; Mel’cˇuk 1988; Hudson 1990). In computational linguistics, dependency structures have become increasingly popular in the interface to downstream applications of parsing, such as information extraction (Culotta and Sorensen 2004; Stevenson and Greenwood 2006; Buyko and Hahn 2010), question answering (Shen and Klakow 2006; Bikel and Castelli 2008), and machine translation (Quirk, Menezes, and Cherry 2005; Xu et al. 2009). And although dependency structures can easily be extracted from other syntactic representations, such as phrase structure trees, this has also led to an increased interest in statistical parsers that speciﬁcally produce dependency trees (Eisner 1996; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005). Current approaches to statistical dependency parsing can be broadly grouped into graph-based and transition-based techniques (McDonald and Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transition-based parsers have relied on local optimization and 801  Computational Linguistics  Volume 39, Number 4  ✞ ✞ ROOT1  AuxK  ✞  AuxP  ¤  ✞  AuxP  Pred  ¤✞  Sb  ¤  ✞ Atr ¤  ✞ AuxZ ¤  ❄ Z2 (Out-of  ❄ nich3 them  ❄  ❄  ❄  je4  jen5  jedna6  is  only one-FEM-SG  (“Only one of them concerns quality.”)  ¤  ¤  ✞ Adv ¤  ❄  ❄❄  na7 kvalitu8 .9  to quality .)  Figure 1 Dependency graph for a Czech sentence from the Prague Dependency Treebank.  greedy, deterministic parsing (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Nivre 2008), but globally trained models and non-greedy parsing methods such as beam search are increasingly used (Johansson and Nugues 2006; Titov and Henderson 2007; Zhang and Clark 2008; Huang, Jiang, and Liu 2009; Huang and Sagae 2010; Zhang and Nivre 2011). In empirical evaluations, the two main approaches to dependency parsing often achieve very similar accuracy, but transition-based parsers tend to be more efﬁcient. In this article, we will be concerned exclusively with transitionbased models. In the remainder of this background section, we ﬁrst introduce the syntactic representations used by dependency parsers, starting from a general characterization of dependency graphs and discussing a number of different restrictions of this class that will be relevant for the analysis later on. We then go on to review the formalization of transition systems proposed by Nivre (2008), and in particular the class of stack-based systems that provides the framework for our discussion of existing and novel transitionbased models. Finally, we discuss the implementation of efﬁcient parsers based on these transition systems. 2.1 Dependency Graphs In dependency parsing, the syntactic structure of a sentence is modeled by a dependency graph, which represents each token and its syntactic dependents through labeled, directed arcs. This is exempliﬁed in Figure 1 for a Czech sentence taken from the Prague Dependency Treebank (Hajicˇ et al. 2001; Bo¨ hmova´ et al. 2003), and in Figure 2 for an English sentence taken from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993; Marcus et al. 1994).1 In the former case, an artiﬁcial token ROOT has been inserted at the beginning of the sentence, serving as the unique root of the graph and ensuring that the graph is a tree even if more than one token is independent of all other tokens. In the latter case, no such device has been used, and we will not in general assume the existence of an artiﬁcial root node preﬁxed to the sentence, although all our models will be compatible with such a device.  
Yves Peirsman∗∗ University of Leuven Nathanael Chambers† United States Naval Academy Mihai Surdeanu‡ University of Arizona Dan Jurafsky§ Stanford University We propose a new deterministic approach to coreference resolution that combines the global information and precise features of modern machine-learning models with the transparency and modularity of deterministic, rule-based systems. Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision, where each model builds on the previous model’s cluster output. The two stages of our sieve-based architecture, a mention detection stage that heavily favors recall, followed by coreference sieves that are precision-oriented, offer a powerful way to achieve both high precision and high recall. Further, our approach makes use of global information through an entity-centric model that encourages the sharing of features across all mentions that point to the same real-world entity. Despite its simplicity, our approach gives state-of-the-art performance on several corpora and genres, and has also been incorporated into hybrid state-of-the-art coreference systems for Chinese and ∗ Stanford University, 450 Serra Mall, Stanford, CA 94305. E-mail: heeyoung@stanford.edu, angelx@cs.stanford.edu. ∗∗ University of Leuven, Blijde-Inkomststraat 21 PO Box 03308, B-3000 Leuven, Belgium. E-mail: yves.peirsman@arts.kuleuven.be. † United States Naval Academy, 121 Blake Road, Annapolis, MD 21402. E-mail: nchamber@usna.edu. ‡ University of Arizona, PO Box 210077, Tucson, AZ 85721-0077. E-mail: msurdeanu@email.arizona.edu. § Stanford University, 450 Serra Mall, Stanford, CA 94305. E-mail: jurafsky@stanford.edu. Submission received: 27 May 2012; revised submission received: 22 October 2012; accepted for publication: 20 November 2012. doi:10.1162/COLI a 00152 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 4  Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of ﬁnding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difﬁcult to apply to new problems and genres as well. Rule-based models like Lappin and Leass (1994) were a popular early solution to the subtask of pronominal anaphora resolution. Rules are easy to create and maintain and error analysis is more transparent. But early rule-based systems relied on hand-tuned weights and were not capable of global inference, two factors that led to poor performance and replacement by machine learning. We propose a new approach that brings together the insights of these modern supervised and unsupervised models with the advantages of deterministic, rule-based systems. We introduce a model that performs entity-centric coreference, where all mentions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules. Our work is inspired both by the seminal early work of Baldwin (1997), who ﬁrst proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for named entity recognition (Chiticariu et al. 2010). Figure 1 illustrates the two main stages of our new deterministic model: mention detection and coreference resolution, as well as a smaller post-processing step. In the mention detection stage, nominal and pronominal mentions are identiﬁed using a high-recall algorithm that selects all noun phrases (NPs), pronouns, and named entity mentions, and then ﬁlters out non-mentions (pleonastic it, i-within-i, numeric entities, partitives, etc.). The coreference resolution stage is based on a succession of ten independent coreference models (or ”sieves”), applied from highest to lowest precision. Precision can be informed by linguistic intuition, or empirically determined on a coreference corpus (see Section 4.4.3). For example, the ﬁrst (highest precision) sieve links ﬁrst-person pronouns inside a quotation with the speaker of a quotation, and the tenth sieve (i.e., low precision but high recall) implements generic pronominal coreference resolution. 886  Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules Figure 1 The architecture of our coreference system. Crucially, our approach is entity-centric—that is, our architecture allows each coreference decision to be globally informed by the previously clustered mentions and their shared attributes. In particular, each deterministic rule is run on the entire discourse, using and extending clusters (i.e., groups of mentions pointing to the same real-world entity, built by models in previous tiers). Thus, for example, in deciding whether two mentions i and j should corefer, our system can consider not just the local features of i and j but also any information (head word, named entity type, gender, or number) about the other mentions already linked to i and j in previous steps. Finally, the architecture is highly modular, which means that additional coreference resolution models can be easily integrated. The two stage architecture offers a powerful way to balance both high recall and precision in the system and make use of entity-level information with rule-based architecture. The mention detection stage heavily favors recall, and the following sieves favor precision. Our results here and in our earlier papers (Raghunathan et al. 2010; Lee et al. 2011) show that this design leads to state-of-the-art performance despite the simplicity of the individual components, and that the lack of language-speciﬁc lexical features makes the system easy to port to other languages. The intuition is not new; in addition to the prior coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solutions combines the intuitions of “shaping” or “successive approximations” ﬁrst proposed for learning by Skinner (1938), and widely used in NLP (e.g., the successively trained IBM MT models of Brown et al. [1993]) and the “islands of reliability” approaches to parsing and speech recognition [Borghesi and Favareto 1982; Corazza et al. 1991]). The idea of beginning with a high-recall list of candidates that are followed by a series of high-precision ﬁlters dates back to one of the earliest architectures in natural language processing, the part of speech tagging algorithm of the Computational Grammar Coder (Klein and Simmons 887  Computational Linguistics  Volume 39, Number 4  1963) and the TAGGIT tagger (Greene and Rubin 1971), which begin with a high-recall list of all possible tags for words, and then used high-precision rules to ﬁlter likely tags based on context. In the next section we walk through an example of our system applied to a simple made-up text. We then describe our model in detail and test its performance on three different corpora widely used in previous work for the evaluation of coreference resolution. We show that our model outperforms the state-of-the-art on each corpus. Furthermore, in these sections we describe analytic and ablative experiments demonstrating that both aspects of our algorithm (the entity-centric aspect that allows the global sharing of features between mentions assigned to the same cluster and the precision-based ordering of sieves) independently offer signiﬁcant improvements to coreference, perform an error analysis, and discuss the relationship of our work to previous models and to recent hybrid systems that have used our algorithm as a component to resolve coreference in English, Chinese, and Arabic.  2. Walking Through a Sample Coreference Resolution  Before delving into the details of our method, we illustrate the intuition behind our  approach with the simple pedagogical example listed in Table 1.  In the mention detection step, the system extracts mentions by inspecting all noun  phrases (NP) and other modiﬁer pronouns (PRP) (see Section 3.1 for details). In Table 1,  this step identiﬁes 11 different mentions and assigns them initially to distinct entities  (Entity id and mention id in each step are marked by superscript and subscript).  This component also extracts mention attributes—for example, John:{ne:person}, and  A girl:{gender:female, number:singular}. These mentions form the input for the  following sequence of sieves.  The ﬁrst coreference resolution sieve (the speaker or quotation sieve) matches  pronominal mentions that appear in a quotation block to the corresponding speaker.  In general, in all the coreference resolution sieves we traverse mentions left-to-right in  a given merged  document (see Section 3.2.1). The ﬁrst match with John1100 into the same entity (entity id: 9).  for this model is my99, which is This illustrates the advantages  of our incremental approach: by assigning a higher priority to the quotation sieve, we  avoid linking my99 with A girl55, a common mistake made by generic coreference models,  since anaphoric candidates (especially in subject position) are generally preferred to  cataphoric ones (Hobbs 1978).  The next sieve searches for anaphoric antecedents that have the exact same string as the mention under consideration. This component resolves the tenth mention, John910, by linking it with John11. When searching for antecedents, we sort candidates in the same sentential clause from left to right, and we prefer sentences that are closer to the mention  under consideration (see Section 3.2.2 for details). Thus, the sorted list of candidates for John910 is It77, My favorite88, My99, A girl55, the song66, He33, a new song44, John11, a musician22. The algorithm stops as soon as a matching antecedent is encountered. In this case, the algorithm ﬁnds John11 and does not inspect a musician22. The relaxed string match sieve searches for mentions satisfying a looser set of  string matching constraints than exact match (details in Section 3.3.3), but makes no  change because there are no such mentions. The precise constructs sieve searches for  several high-precision syntactic constructs, such as appositive relations and predicate  nominatives. In this example, there are two predicate nominative relations in the ﬁrst and fourth sentences, so this component clusters together John11 and a musician22, and It77 and my favorite88.  888  Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules  Table 1 A sample run-through of our approach, applied to a made-up sentence. In each step we mark in bold the affected mentions; superscript and subscript indicate entity id and mention id.  Input: Mention Detection: Speaker Sieve: String Match: Relaxed String Match: Precise Constructs: Strict Head Match A: Strict Head Match B,C: Proper Head Noun Match: Relaxed Head Match: Pronoun Match: Post Processing: Final Output:  John is a musician. He played a new song. A girl was listening to the song. “It is my favorite,” John said to her. [John]11 is [a musician]22. [He]33 played [a new song]44. [A girl]55 was listening to [the song]66. “[It]77 is [[my]99 favorite]88,” [John]1100 said to [her]1111. [John]11 is [a musician]22. [He]33 played [a new song]44. [A girl]55 was listening to [the song]66. “[It]77 is [[my]99 favorite]88,” [John]910 said to [her]1111. [John]11 is [a musician]22. [He]33 played [a new song]44. [A girl]55 was listening to [the song]66. “[It]77 is [[my]19 favorite]88,” [John]110 said to [her]1111. [John]11 is [a musician]22. [He]33 played [a new song]44. [A girl]55 was listening to [the song]66. “[It]77 is [[my]19 favorite]88,” [John]110 said to [her]1111. [John]11 is [a musician]12. [He]33 played [a new song]44. [A girl]55 was listening to [the song]66. “[It]77 is [[my]19 favorite]78,” [John]110 said to [her]1111. [John]11 is [a musician]12. [He]33 played [a new song]44. [A girl]55 was listening to [the song]46. “[It]77 is [[my]19 favorite]78,” [John]110 said to [her]1111. [John]11 is [a musician]12. [He]33 played [a new song]44. [A girl]55 was listening to [the song]46. “[It]77 is [[my]19 favorite]78,” [John]110 said to [her]1111. [John]11 is [a musician]12. [He]33 played [a new song]44. [A girl]55 was listening to [the song]46. “[It]77 is [[my]19 favorite]78,” [John]110 said to [her]1111. [John]11 is [a musician]12. [He]33 played [a new song]44. [A girl]55 was listening to [the song]46. “[It]77 is [[my]19 favorite]78,” [John]110 said to [her]1111. [John]11 is [a musician]12. [He]13 played [a new song]44. [A girl]55 was listening to [the song]46. “[It]47 is [[my]19 favorite]48,” [John]110 said to [her]511. [John]11 is a musician. [He]13 played [a new song]44. [A girl]55 was listening to [the song]46. “[It]47 is [my]19 favorite,” [John]110 said to [her]511. [John]11 is a musician. [He]13 played [a new song]44. [A girl]55 was listening to [the song]46. “[It]47 is [my]19 favorite,” [John]110 said to [her]511.  The next four sieves (strict head match A–C, proper head noun match) cluster mentions that have the same head word with various other constraints. a new song44 and the song66 are linked in this step. The last resolution component in this example addresses pronominal coreference resolution. The three pronouns in this text, He33, It77, and her1111 are linked to their  889  Computational Linguistics  Volume 39, Number 4  compatible antecedents based on their attributes, such as gender, number, and animacy.  In It77  this step to entity  we assign He33 and her1111 to entities 1 and 5, 4, which represents an inanimate concept.  respectively  (same  gender),  and  The system concludes with a post-processing component, which implements  corpus-speciﬁc rules. For example, to align our output with the OntoNotes annotation  standard, we remove mentions assigned to singleton clusters (i.e., entities with a single  mention in text) and links obtained through predicate nominative patterns. Note that  even though we might remove some coreference links in this step, these links serve an  important purpose in the algorithm ﬂow, as they allow new features to be discovered for  the corresponding entity and shared between its mentions. See Section 3.2.3 for details  on feature extraction.  3. The Algorithm We ﬁrst describe our mention detection stage, then introduce the general architecture of the coreference stage, followed by a detailed examination of the coreference sieves. In describing the architecture, we will sometimes ﬁnd it helpful to discuss the precision of individual components, drawn from our later experiments in Section 4. 3.1 Mention Detection As we suggested earlier, the recall of our mention detection component is more important than its precision. This is because for the OntoNotes corpus and for many practical applications, any missed mentions are guaranteed to affect the ﬁnal score by decreasing recall, whereas spurious mentions may not impact the overall score if they are assigned to singleton clusters, because singletons are deleted during post-processing. Our mention detection algorithm implements this intuition via a series of simple yet broad-coverage heuristics that take advantage of syntax, named entity recognition and manually written patterns. Note that those patterns are built based on the OntoNotes annotation guideline because mention detection in general depends heavily on the annotation policy. We start by marking all NPs, pronouns, and named entity mentions (see the named entity tagset in Appendix A) that were not previously marked (i.e., they appear as modiﬁers in other NPs) as candidate mentions. From this set of candidates we remove the mentions that match any of the following exclusion rules: 1. We remove a mention if a larger mention with the same head word exists (e.g., we remove The ﬁve insurance companies in The ﬁve insurance companies approved to be established this time). 2. We discard numeric entities such as percents, money, cardinals, and quantities (e.g., 9%, $10, 000, Tens of thousands, 100 miles). 3. We remove mentions with partitive or quantiﬁer expressions (e.g., a total of 177 projects, none of them, millions of people).1  
M. Anto` nia Mart´ı‡ Universitat de Barcelona Paolo Rosso§ Universitat Polite`cnica de Vale`ncia Although paraphrasing is the linguistic mechanism underlying many plagiarism cases, little attention has been paid to its analysis in the framework of automatic plagiarism detection. Therefore, state-of-the-art plagiarism detectors ﬁnd it difﬁcult to detect cases of paraphrase plagiarism. In this article, we analyze the relationship between paraphrasing and plagiarism, paying special attention to which paraphrase phenomena underlie acts of plagiarism and which of them are detected by plagiarism detection systems. With this aim in mind, we created the P4P corpus, a new resource that uses a paraphrase typology to annotate a subset of the PAN-PC-10 corpus for automatic plagiarism detection. The results of the Second International Competition on Plagiarism Detection were analyzed in the light of this annotation. The presented experiments show that (i) more complex paraphrase phenomena and a high density of paraphrase mechanisms make plagiarism detection more difﬁcult, (ii) lexical substitutions are the paraphrase mechanisms used the most when plagiarizing, and (iii) paraphrase mechanisms tend to shorten the plagiarized text. For the ﬁrst time, the paraphrase mechanisms behind plagiarism have been analyzed, providing critical insights for the improvement of automatic plagiarism detection systems. ∗ TALP Research Center, Jordi Girona Salgado 1-3, 08034 Barcelona, Spain. E-mail: albarron@lsi.upc.es. ∗∗ CLiC, Department of Linguistics, Gran Via 585, 08007 Barcelona, Spain. E-mail: marta.vila@ub.edu. † Both authors contributed equally to this work. ‡ CLiC, Department of Linguistics, Gran Via 585, 08007 Barcelona, Spain. E-mail: amarti@ub.edu. § NLE Lab-ELiRF, Department of Information Systems and Computation, Camino de Vera s/n, 46022 Valencia, Spain. E-mail: prosso@dsic.upv.es. Submission received: 13 March 2012; revised submission received: 17 October 2012; accepted for publication: 7 November 2012. doi:10.1162/COLI a 00153 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 4  1. Introduction Plagiarism is the re-use of someone else’s prior ideas, processes, results, or words without explicitly acknowledging the original author and source (IEEE 2008). Although plagiarism may occur incidentally, it is often the outcome of a conscious process. Independently from the vocabulary or channel through which an idea is communicated, a person who fails to provide its corresponding source is suspected of plagiarism. The amount of text available in electronic media nowadays has caused cases of plagiarism to increase. In the academic domain, some surveys estimate that around 30% of student reports include plagiarism (Association of Teachers and Lecturers 2008), and a more recent study increases this percentage to more than 40% (Comas et al. 2010). As a result, its manual detection has become infeasible. Models for automatic plagiarism detection are being developed as a countermeasure. Their main objective is assisting people in the task of detecting plagiarism—as a side effect, plagiarism is discouraged. The linguistic phenomena underlying plagiarism have barely been analyzed in the design of these systems, which we consider to be a key issue for their improvement. Martin (2004) identiﬁes different kinds of plagiarism: of ideas, of references, of authorship, word by word, and paraphrase plagiarism. In the ﬁrst case, ideas, knowledge, or theories from another person are claimed without proper citation. In plagiarism of references and authorship, citations and entire documents are included without any mention of their authors. Word by word plagiarism, also known as copy–paste or verbatim copy, consists of the exact copy of a text (fragment) from a source into the plagiarized document. Regarding paraphrase plagiarism, in order to conceal the plagiarism act, a different form expressing the same content is often used. Paraphrasing, generally understood as sameness of meaning between different wordings, is the linguistic mechanism underlying many plagiarism acts and the linguistic process on which plagiarism is based. In this article, the relationship between plagiarism and paraphrasing, which consists of a largely unexplored problem, is analyzed, and the potential of such a relationship in automatic plagiarism detection is set out. We aim not only to investigate how difﬁcult detecting paraphrase cases for state-of-the-art plagiarism detectors is, but to understand which types of paraphrases underlie plagiarism acts and which are the most difﬁcult to detect. For this purpose, we created the Paraphrase for Plagiarism corpus (P4P) annotating a portion of the PAN-PC-10 corpus for plagiarism detection (Potthast et al. 2010) on the basis of a paraphrase typology, and we mapped the annotation results with those of the Second International Competition on Plagiarism Detection (Pan-10 competition, hereafter).1 The results obtained provide critical insights for the improvement of automatic plagiarism detection systems. The rest of the article is structured as follows. Section 2 sets out the paraphrase typology used in this research work. Section 3 describes the construction of the P4P corpus. Section 4 gives an overview of the state of the art in automatic plagiarism detection; special attention is given to the systems participating in the Pan-10 competition. Section 5 discusses our experiments and the ﬁndings derived from mapping the P4P corpus and the Pan-10 competition results. Section 6 draws some conclusions and offers insights for future research.  
Shuly Wintner† University of Haifa, Israel Translation models used for statistical machine translation are compiled from parallel corpora that are manually translated. The common assumption is that parallel texts are symmetrical: The direction of translation is deemed irrelevant and is consequently ignored. Much research in Translation Studies indicates that the direction of translation matters, however, as translated language (translationese) has many unique properties. It has already been shown that phrase tables constructed from parallel corpora translated in the same direction as the translation task outperform those constructed from corpora translated in the opposite direction. We reconﬁrm that this is indeed the case, but emphasize the importance of also using texts translated in the “wrong” direction. We take advantage of information pertaining to the direction of translation in constructing phrase tables by adapting the translation model to the special properties of translationese. We explore two adaptation techniques: First, we create a mixture model by interpolating phrase tables trained on texts translated in the “right” and the “wrong” directions. The weights for the interpolation are determined by minimizing perplexity. Second, we deﬁne entropy-based measures that estimate the correspondence of target-language phrases to translationese, thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation. We show that incorporating these measures as features in the phrase tables of statistical machine translation systems results in consistent, statistically signiﬁcant improvement in the quality of the translation. ∗ Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: gennadi.lembersky@nice.com. ∗∗ Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: noam.ordan@gmail.com. † Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il. Submission received: 23 June 2012; revised submission received: 13 November 2012; accepted for publication: 18 January 2013. doi:10.1162/COLI a 00159 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 4  1. Introduction Much research in translation studies indicates that translated texts have unique characteristics that set them apart from original texts (Toury 1980; Gellerstam 1986; Toury 1995). Known as translationese, translated texts (in any language) constitute a genre, or a dialect, of the target language, which reﬂects both artifacts of the translation process and traces of the original language from which the texts were translated. Among the better-known properties of translationese are simpliﬁcation and explicitation (Blum-Kulka and Levenston 1983; Blum-Kulka 1986; Baker 1993): Translated texts tend to be shorter, to have lower type/token ratio, and to use certain discourse markers more frequently than original texts. Interestingly, translated texts are so markedly different from original ones that automatic classiﬁcation can identify them with very high accuracy (Baroni and Bernardini 2006; van Halteren 2008; Ilisei et al. 2010; Koppel and Ordan 2011). Contemporary statistical machine translation (SMT) systems use parallel corpora to train translation models that reﬂect source- and target-language phrase correspondences. Typically, SMT systems ignore the direction of translation of the parallel corpus. Given the unique properties of translationese, which operate asymmetrically from source to target language, it is reasonable to assume that this direction may affect the quality of the translation. Recently, Kurokawa, Goutte, and Isabelle (2009) showed that this is indeed the case. They trained a system to translate between French and English (and vice versa) using a French-translated-to-English parallel corpus, and then an Englishtranslated-to-French one. They ﬁnd that in translating into French the latter parallel corpus yields better results (in terms of higher BLEU scores), whereas for translating into English it is better to use the former. Typically, however, parallel corpora are not marked for direction. Therefore, Kurokawa, Goutte, and Isabelle (2009) trained an SVM-based classiﬁer to predict which side of a bi-text is the origin and which one is the translation, and trained a translation model by utilizing only the subset of the corpus that corresponds to the direction of the task. We use these results as our departure point, but improve them in two major ways. First, we demonstrate that the other subset of the corpus, reﬂecting translation in the “wrong” direction, is also important for the translation task, and must not be ignored; second, we show that explicit information on the direction of translation of the parallel corpus, whether manually annotated or machine-learned, is not mandatory. This is achieved by casting the problem in the framework of domain adaptation: We use domain-adaptation techniques to direct the SMT system toward producing output that better reﬂects the properties of translationese. We show that SMT systems adapted to translationese produce better translations than vanilla systems trained on exactly the same resources. We conﬁrm these ﬁndings using automatic evaluation metrics, as well as through a qualitative analysis of the results. After reviewing related work in Section 2, we begin by replicating the results of Kurokawa, Goutte, and Isabelle (2009) in Section 3. We then (Section 4) explain why translation quality improves when the parallel corpus is translated in the “right” direction. We do so by showing that the subset of the corpus that was translated in the direction of the translation task (the “right” direction, henceforth source-to-target, or S → T) yields phrase tables that are better suited for translation of the original language than the subset translated in the reverse direction (the “wrong” direction, henceforth target-to-source, or T → S). We use several statistical measures that indicate the better quality of the phrase tables in the former case. 1000  Lembersky, Ordan, and Wintner  Adapting Translation Models to Translationese  We then show (Section 5) that using the entire parallel corpus, including texts that are translated both in the “right” and in the “wrong” direction, improves the quality of the results. Next, we investigate several ways to improve the translation quality by adapting a translation model to the nature of translationese, thereby making the output of machine translation more similar to actual, human translation. Speciﬁcally, we create two phrase tables, one for the S → T portion of the corpus, and one for the T → S portion, and combine them into a mixture model using perplexity minimization (Sennrich 2012) to set the model weights. We show that this combination signiﬁcantly outperforms a simple union of the two portions of the parallel corpus. Furthermore, we show that the direction of translation used for producing the parallel corpus can be approximated by deﬁning several entropy-based measures that correlate well with translationese, and, consequently, with translation quality. We use the entire corpus, create a single, uniﬁed phrase table, and then use these measures, and in particular cross-entropy, as a clue for selecting phrase pairs from this table. The beneﬁt of this method is that not only does it improve the translation quality, but it also eliminates the need to directly predict the direction of translation of the parallel corpus. The main contribution of this work, therefore, is a methodology that improves the quality of SMT by building translation models that are adapted to the nature of translationese.1 To demonstrate the contribution of our methodology, we conduct in Section 6 a thorough analysis of our results, both quantitatively and qualitatively. We show that translations produced by our best-performing system indeed reﬂect some well-known properties of translationese better than the output of baseline systems. Furthermore, we provide several examples of SMT outputs that demonstrate in what ways our adapted system generates better results. 2. Related Work Kurokawa, Goutte, and Isabelle (2009) were the ﬁrst to address the direction of translation in the context of SMT. They found that a translation model based on the S → T portion of the parallel corpus results in much better translation quality than a translation model based on the T → S portion. We replicate these results here (Section 3), and view them as a baseline. In taking direction into account, we are faced with two major challenges. First, using only the “right” portion of the corpus results in discarding potentially very useful data. In real-world scenarios this can be crucial, because the proportion between the two portions of the corpus can vary greatly. In the Hansard corpus, used by Kurokawa, Goutte, and Isabelle (2009), only 20% of the corpus is S → T. We show that the T → S portion is also important for machine translation and thus should not be discarded. Using information-theoretic measures, and in particular crossentropy, we gain statistically signiﬁcant improvements in translation quality beyond the results of Kurokawa, Goutte, and Isabelle (2009). The second challenge is to overcome the need to (manually or automatically) classify parallel corpora according to direction. We face this challenge by using an adaptation technique. In previous work, we investigated the relations between translationese and machine translation, focusing on the language model (LM) (Lembersky, Ordan, and Wintner  
Alexander Koller† University of Potsdam Psycholinguistic research shows that key properties of the human sentence processor are incrementality, connectedness (partial structures contain no unattached nodes), and prediction (upcoming syntactic structure is anticipated). There is currently no broad-coverage parsing model with these properties, however. In this article, we present the ﬁrst broad-coverage probabilistic parser for PLTAG, a variant of TAG that supports all three requirements. We train our parser on a TAG-transformed version of the Penn Treebank and show that it achieves performance comparable to existing TAG parsers that are incremental but not predictive. We also use our PLTAG model to predict human reading times, demonstrating a better ﬁt on the Dundee eyetracking corpus than a standard surprisal model. 1. Introduction Evidence from psycholinguistic research suggests that human language comprehension is incremental. Comprehenders do not wait until the end of the sentence before they build a syntactic representation for the sentence; rather, they construct a sequence of partial representations for sentence preﬁxes. Experimental results indicate that each new word that is read or heard triggers an update of the representation constructed so far (Tanenhaus et al. 1995; Konieczny 2000). There is also evidence for connectedness in human language processing (Sturt and Lombardo 2005). Connectedness means that all input words are attached to the ∗ Cluster of Excellence Multimodal Computing and Interaction (MMCI), Postfach 151150, 66041 Saarbru¨ cken, Germany. E-mail: vera@coli.uni-saarland.de. ∗∗ Institute for Language, Cognition, and Computation, School of Informatics, 10 Crichton Street, Edinburgh EH8 9AB, UK. E-mail: keller@inf.ed.ac.uk. † Department of Linguistics, Karl-Liebknecht-Straße 24–25, 14476 Potsdam, Germany. E-mail: koller@ling.uni-potsdam.de. Submission received: 4 July 2011; revised submission received: 22 December 2012; accepted for publication: 22 January 2013. doi:10.1162/COLI a 00160 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 4  same syntactic structure (though connected structures can be constructed in parallel); comprehenders build no unconnected tree fragments, even for the sentence preﬁxes that arise during incremental processing. Furthermore, a range of studies show that comprehenders make predictions about upcoming material on the basis of sentence preﬁxes. There is experimental evidence that listeners predict complements of verbs based on their selectional restrictions (Altmann and Kamide 1999); readers predict a phrase introduced by or on encountering the word either (Staub and Clifton 2006); also the subcategorization frame of a verb can be used for prediction (Arai and Keller 2013). These studies ﬁnd processing facilitation if predictions can be veriﬁed successfully, compared with sentences where predictions cannot be made or turn out to be incorrect. Presumably, the human sentence processor uses prediction mechanisms to enable efﬁcient comprehension in real time. The three concepts of incrementality, connectedness, and prediction are fundamentally interrelated: Maintaining connected partial analyses is only nontrivial if the parsing process is incremental, and prediction means that a connected analysis is required also for words the parser has not yet seen. In this article, we exploit the interrelatedness of incrementality, connectedness, and prediction to develop a parsing model for psycholinguistically motivated TAG (PLTAG; Demberg and Keller 2008b). This formalism augments standard tree-adjoining grammar (TAG; Joshi, Levy, and Takahashi 1975) with a predictive lexicon and a veriﬁcation operation for validating predicted structures. As we show in Section 2, these operations are motivated by psycholinguistic ﬁndings. We argue that our PLTAG parser can form the basis for a new model of human sentence processing. We successfully evaluate the predictions of this model against reading time data from an eye-tracking corpus, showing that it provides a better ﬁt with the psycholinguistic data than the standard surprisal model of human sentence processing. Crucially, this evaluation relies on the broad-coverage nature of our PLTAG parser, that is, the fact that it achieves high coverage and good parsing accuracy on corpus data. Only a broad-coverage parser can be used to model naturalistic data such as reading times from an eye-tracking corpus; this sets our approach apart from most other psycholinguistic models, for which only small-scale implementations for restricted data sets are available. On the technical side, our key contribution is a novel parsing algorithm for probabilistic PLTAG. Incremental fully connected parsing is fundamentally more difﬁcult than non-incremental parsing or parsing without connectedness: Explicit hypotheses about how the words in a sentence are connected have to be made before all of the relevant evidence has been encountered in the input. The number of connected analyses grows quickly with the length of the sentence, and this problem gets worse in the presence of predicted structure. Our parsing algorithm addresses this by grouping equivalent analyses together by only considering the fringes of trees, and by controlling the prediction process via supertagging. We evaluate our parser on a TAG-converted version of the Penn Treebank, achieving a coverage of 98.09% and an F-score of 79.41. These results approach the performance of previous (non-predictive) incremental TAG parsers. We present a formalization of PLTAG in Section 3, introduce the PLTAG parsing algorithm and probability model in Section 4, show how a PLTAG lexicon can be induced from an augmented version of the Penn Treebank in Section 5, test parsing performance in Section 6, and ﬁnally provide a psycholinguistic evaluation on an eyetracking corpus in Section 7. 1026  Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar 2. Background and Related Work This section situates the current work with respect to the experimental literature on human parsing, and with respect to prior work on incremental parsing. 2.1 Prediction, Incrementality, and Connectedness in Human Parsing We start with a short review of the experimental evidence for incremental, predictive, and connected processing in human parsing. In a classic study, Altmann and Kamide (1999) showed that listeners can predict the complement of a verb based on its selectional restrictions. Participants heard sentences such as: (1) a. The boy will eat the cake. b. The boy will move the cake. while viewing images that depicted sets of relevant objects—in this example, a cake, a train set, a ball, and a model car. Altmann and Kamide (1999) monitored participants’ eye-movements while they heard the sentences and found an increased number of looks to the cake during the word eat compared with the control condition, that is, during the word move (only the cake is edible, but all depicted objects are movable). This indicates that selectional preference information provided by the verb is not only used as soon as it is available (i.e., incremental processing takes place), but this information also triggers the prediction of upcoming arguments of the verb. Subsequent work has generalized this effect, demonstrating that syntactic information such as case marking is also used for prediction (Kamide, Scheepers, and Altmann 2003). More recently, Arai and Keller (2013) used the same experimental paradigm to show that verb subcategorization information is used for prediction. They compared transitive and intransitive verbs in sentences such as: (2) a. The inmate offended the judge. b. The inmate frowned at the judge. Participants’ eye-movements indicate which subcategorization frame they assume when they process the verb. While hearing offended, listeners predict upcoming patient information and look at the judge. While hearing frowned, no such prediction is possible, and there is no increase of looks at the judge (this increase is observable later, during at). This shows that the human parser uses the subcategorization frame of the verb to anticipate upcoming syntactic structure, working out whether this structure contains a noun phrase argument or not. Selectional restrictions, case marking, and subcategorization arguably are all encoded as part of lexical items, which raises the question whether the prediction of larger structural units is also possible. This was addressed by a study of Staub and Clifton (2006), who investigated prediction in coordinate structures. They compared sentences such as: (3) a. Peter read either a book or an essay in the school magazine. b. Peter read a book or an essay in the school magazine. By monitoring eye-movements during reading, they found that the presence of either leads to shorter reading times on or and on the noun phrase that follows it in Example (3a), compared to the control condition Example (3b). This suggests that the word either makes it possible to anticipate an upcoming noun phrase conjunction, ruling out 1027  Computational Linguistics  Volume 39, Number 4  verb phrase conjunction (which remains possible in Example (3b)). This result can be taken as evidence for structural prediction, that is, prediction that goes beyond the lexical information (case marking, subcategorization, etc.) encoded in the word either. Let us now turn to the evidence for connectedness in human parsing. Connectedness means that the parser only generates syntactic trees that cover all of the input received so far. The claim is that comprehenders do not build unconnected tree fragments, even when the syntactic heads needed to build a connected structure are not available yet during incremental processing. Evidence for this claim comes from an experiment by Sturt and Lombardo (2005), who investigated the binding of pronouns in sentences such as: (4) a. The pilot embarrassed Mary and put herself in an awkward situation. b. The pilot embarrassed Mary and put her in an awkward situation. They found increased reading times on the word herself in Example (4a), but not on her in Example (4b). They attribute this to a gender mismatch between herself and its antecedent pilot, whose stereotypical gender is masculine. No such mismatch occurs in Example (4b), as the antecedent of her is Mary. Crucially, this gender mismatch can only be detected if the anaphor is c-commanded by its antecedent. The c-command relationship can only be established if the parser builds a fully connected structure, which includes a path from the anaphor to its antecedent. A parser that operates on unconnected sentence fragments therefore is unable to predict the contrast in Example (4); Sturt and Lombardo (2005) use this to argue for TAG as the basis for a model of human sentence processing, and against formalisms with a weaker notion of connectedness, such as Combinatory Categorial Grammar (CCG; Steedman 2000). Subsequent work has provided evidence for connectedness in a range of other phenomena, including sluicing and ellipsis (Aoshima, Yoshida, and Phillips 2009; Yoshida, Walsh-Dickey, and Sturt 2013). 2.2 Incremental Parsing Models In the previous section, we identiﬁed incrementality, connectedness, and prediction as key desiderata for computational models of human parsing. In what follows, we will review work on parsing in computational linguistics in the light of these desiderata. Incremental parsers for a range of grammatical formalisms have been proposed in the literature. An example is the work of Shen and Joshi (2005), who propose an efﬁcient incremental parser for a variant of TAG, spinal TAG. This approach, however, allows multiple unconnected subtrees for a sentence preﬁx and uses a look-ahead of two words, that is, it does not build connected structures. An example of a TAG parser that is both incremental and builds connected structures is the work of Kato, Matsubara, and Inagaki (2004). This comes at the price of strong simplifying assumptions with respect to the TAG formalism, such as not distinguishing modiﬁers and arguments. (We will return to a discussion of other TAG parsers in Section 6.1.) An example of an incremental parser based on context-free grammars is the one proposed by Roark (2001). That parser uses a top–down algorithm to build fully connected structures; it is also able to compute probabilities for sentence preﬁxes, which makes it attractive for psycholinguistic modeling, where preﬁx probabilities are often used to predict human processing difﬁculty (see Section 7 for details). The Roark parser has been shown to successfully model psycholinguistic data from eye-tracking corpora (Demberg and Keller 2008a; Frank 2009) and other reading time data (Roark et al. 2009). It therefore is a good candidate for a broad-coverage model of human parsing, and 1028  Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar will serve as a standard of comparison for the model proposed in the current article in Section 7. The Roark parser has been extended with discriminative training (Collins and Roark 2004), resulting in a boost in parsing accuracy. Preﬁx probabilities cannot be computed straightforwardly in a discriminative framework, however, making this approach less interesting from a psycholinguistic modeling point of view. Wu et al. (2010) propose another approach based on preﬁx probabilities over context-free structures. These are generated in their approach using a bottom–up parsing algorithm based on hierarchical hidden Markov models (HMMs) (Schuler et al. 2010). They show that preﬁx probabilities, as well as a new measure based on the embedding depth of the HMM, successfully predicts human reading time data. In the dependency parsing literature, Nivre (2004) proposes a parser that builds dependency structures word-by-word, based on a shift-reduce algorithm. This approach is highly efﬁcient, but has two disadvantages from a psycholinguistic point of view: Firstly, it cannot guarantee that only connected structures are built, as the stack potentially contains unconnected words (though Nivre [2004] shows that 68.9% of all parse conﬁgurations contain only connected components, rising to 87.1% if only valid dependency graphs are considered). Secondly, Nivre (2004) uses a discriminative probability model over parser actions, which means that preﬁx probabilities cannot be computed directly. It is, however, possible to predict reading times using probabilities over parser actions rather than preﬁx probabilities, as Boston et al. (2008) have shown by using the Nivre (2004) parser to model reading times in an eye-tracking corpus of German sentences. An interesting alternative to Nivre’s approach has been proposed by Beuck, Ko¨ hn, and Menzel (2011), who introduce an incremental version of Weighted Constraint Dependency Grammar (WCDG). The proposed parsing framework is able to produce structures that are both connected and predictive; this is achieved by the introduction of virtual nodes in the dependency tree, an idea akin to our use of prediction trees in TAG (detailed subsequently). WCDG parsing is non-monotonic, that is, it uses a mechanism by which the current analysis can be revised if it becomes incompatible with the input. This contrasts with the fully monotonic approach we use in the present article. In terms of evaluation, Beuck, Ko¨ hn, and Menzel (2011) present a comparison of their incremental WCDG parser with the model of Nivre (2004) for parsing German. What is common to all of these approaches it that they lack an explicit prediction and veriﬁcation mechanism (WCDG includes prediction, but not veriﬁcation), which means that they cannot be used to model psycholinguistic results that involve veriﬁcation cost.1 A simple form of prediction can be achieved in a chart parser (incomplete edges in the chart can be seen as predictive), but in order to maintain psycholinguistic plausibility, an arc-eager left-corner parsing strategy needs to be used. Other parsing strategies fail to predict human processing difﬁculty that arises in certain cases, such as for center embedding (Thompson, Dixon, and Lamping 1991; Resnik 1992a). This is an argument against using a top–down parser such as Roark’s for psycholinguistic modeling. Furthermore, it is important to emphasize that a full model of human parsing needs to not only model prediction, but also account for processing difﬁculty associated with the veriﬁcation of predictions (we will return to this point in Section 7). None of the existing incremental parsing models includes an explicit veriﬁcation component. 
Lars Ahrenberg Linko¨ ping University† In this article we investigate statistical machine translation (SMT) into Germanic languages, with a focus on compound processing. Our main goal is to enable the generation of novel compounds that have not been seen in the training data. We adopt a split-merge strategy, where compounds are split before training the SMT system, and merged after the translation step. This approach reduces sparsity in the training data, but runs the risk of placing translations of compound parts in non-consecutive positions. It also requires a postprocessing step of compound merging, where compounds are reconstructed in the translation output. We present a method for increasing the chances that components that should be merged are translated into contiguous positions and in the right order and show that it can lead to improvements both by direct inspection and in terms of standard translation evaluation metrics. We also propose several new methods for compound merging, based on heuristics and machine learning, which outperform previously suggested algorithms. These methods can produce novel compounds and a translation with at least the same overall quality as the baseline. For all subtasks we show that it is useful to include part-of-speech based information in the translation process, in order to handle compounds. 1. Introduction In many languages including most of the Germanic (German, Swedish, etc.) and Uralic (Finnish, Hungarian, etc.) language families, so-called closed compounds are used ∗ Department of Linguistics and Philology, Uppsala University, Box 635, 751 26 Uppsala, Sweden. E-mail: sara.stymne@lingfil.uu.se. ∗∗ Xerox Research Centre Europe, 6 chemin de Maupertuis, 38240 Meylan, France. E-mail: nicola.cancedda@xrce.xerox.com. † Department of Computer and Information Science, Linko¨ ping University, 58183 Linko¨ ping, Sweden. E-mail: lars.ahrenberg@liu.se. Submission received: 25 April 2012; revised submission received: 30 November 2012; accepted for publication: 8 January 2013. doi:10.1162/COLI a 00162 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 4  productively. Closed compounds are written as single words without spaces or other word boundaries, as in German Goldring. We will refer to these languages as compounding languages. In English, on the other hand, compounds are generally open, that is, written as two words as in gold ring. This difference in compound orthography leads to problems for statistical machine translation (SMT). For translation into a compounding language, often fewer compounds than in normal texts are produced. This can be due to the fact that the desired compounds are missing in the training data or that they have not been aligned correctly. When a compound is the idiomatic word choice in the translation, systems often produce separate words, genitive or other alternative constructions, or translate only one part of the compound. For an SMT system to cope with the productivity of the phenomenon, any effective strategy should be able to correctly process compounds that have never been seen in the training data as such, although possibly their components have, either in isolation or within a different compound. Previous work (e.g., Koehn and Knight 2003) has shown that compound splitting improves translation from compounding languages into English. In this article we explore several aspects of the less-researched area of compound treatment for translation into such languages, using three Germanic languages (German, Swedish, and Danish) as examples.1 The assumption is that splitting compounds will also improve translation for this translation direction and lead to more natural translations. The strategy we adopt is to split compounds in the training data, and to merge them in the translation output. Our overall goal is to improve translation quality by productively generating compounds in SMT systems. The main contributions of the article are as follows: r Demonstrating improved coalescence (adjacency and order) of compound parts in translation through the use of sequence models based on customized part-of-speech sets and count features r Designing and evaluating several heuristic methods for compound merging that outperforms previous heuristic merging methods r Designing and evaluating a novel method for compound merging based on sequence labeling r Demonstrating the ability of these merging methods to generate novel unseen compounds In addition, we report effects on translation performance from a number of variations in the methods for compound splitting and merging. The rest of the article is structured as follows. Section 2 gives an overview of compound formation in the three target languages used in this work. Section 3 reports related work on compound processing for machine translation. Section 4 describes the compound processing strategy we use and Section 5 describes compound splitting. Section 6 addresses compound coalescence, followed by compound merging in Section 7. In Section 8 we present experimental results and in Section 9 we state our conclusions.  
USC Information Sciences Institute Paraphrases are sentences or phrases that convey the same meaning using different wording. Although the logical deﬁnition of paraphrases requires strict semantic equivalence, linguistics accepts a broader, approximate, equivalence—thereby allowing far more examples of “quasiparaphrase.” But approximate equivalence is hard to deﬁne. Thus, the phenomenon of paraphrases, as understood in linguistics, is difﬁcult to characterize. In this article, we list a set of 25 operations that generate quasi-paraphrases. We then empirically validate the scope and accuracy of this list by manually analyzing random samples of two publicly available paraphrase corpora. We provide the distribution of naturally occurring quasi-paraphrases in English text. 1. Introduction Sentences or phrases that convey the same meaning using different wording are called paraphrases. For example, consider sentences (1) and (2): (1) The school said that their buses seat 40 students each. (2) The school said that their buses accommodate 40 students each. Paraphrases are of interest for many current NLP tasks, including textual entailment, machine reading, question answering, information extraction, and machine translation. Whenever the text contains multiple ways of saying “the same thing,” but the application requires the same treatment of those various alternatives, an automated paraphrase recognition mechanism would be useful. One reason why paraphrase recognition systems have been difﬁcult to build is because paraphrases are hard to deﬁne. Although the strict interpretation of the term “paraphrase” is quite narrow because it requires exactly identical meaning, in linguistics literature paraphrases are most often characterized by an approximate equivalence of meaning across sentences or phrases. De Beaugrande and Dressler (1981, page 50) deﬁne paraphrases as “approximate conceptual equivalence among ∗ 24515 SE 46th Terrace Issaquah, WA 98029. E-mail: me@rahulbhagat.net. ∗∗ 24515 SE 46th Terrace Issaquah, WA 98029. E-mail: hovy@isi.edu. Submission received: 5 July 2012; revised submission received: 21 January 2013; accepted for publication: 6 March 2013. doi:10.1162/COLI a 00166 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 3  outwardly different material.” Hirst (2003, slide 9) deﬁnes paraphrases as “talk(ing) about the same situation in a different way.” He argues that paraphrases aren’t fully synonymous: There are pragmatic differences in paraphrases, namely, difference of evaluation, connotation, viewpoint, and so forth. According to Mel’cuk (2012, page 7) “An approximate synonymy of sentences is considered as sufﬁcient for them to be produced from the same SemS.” He further adds that approximate paraphrases include implications (not in the logical sense, but in the everyday sense). Taking an extreme view, Clark (1992, page 172) rejects the idea of absolute synonymy by saying “Every two forms (in language) contrast in meaning.” Overall, there is a large body of work in the linguistics literature that argues that paraphrases are not restricted to strict synonymy. In this article, we take a broad view of paraphrases. To avoid the conﬂict between the notion of strict paraphrases as understood in logic and the broad notion in linguistics, we use the term quasi-paraphrases to refer to the paraphrases that we deal with here. In the context of this article, the term “paraphrases” (even without the preﬁx “quasi”) means “quasi-paraphrases.” We deﬁne quasi-paraphrases as ‘sentences or phrases that convey approximately the same meaning using different words.’ We ignore the ﬁne grained distinctions of meaning between sentences and phrases, introduced due to the speaker’s evaluation of the situation, connotation of the terms used, change of modality, and so on. For example, consider sentences (3) and (4). (3) The school said that their buses seat 40 students each. (4) The school said that their buses cram in 40 students each. Here, seat and cram in are not synonymous: They carry different evaluations by the speaker about the same situation. We, however, consider sentences (3) and (4) to be (quasi) paraphrases. Similarly, consider sentences (5) and (6). (5) The school said that their buses seat 40 students each. (6) The school is saying that their buses might accommodate 40 students each. Here, said and is saying have different tenses. Also, might accommodate and seat are not synonymous, due to the modal verb might. We consider sentences (5) and (6) to be quasi-paraphrases, however. Note that this article focuses on deﬁning quasi-paraphrases. It does not provide direct implementation/application results of using them. We believe, however, that this work will allow computation-oriented researchers to focus their future work more effectively on a subset of paraphrase types without concern for missing important material, and it will provide linguistics-oriented researchers with a blueprint of the overall distribution of the types of paraphrase. 2. Paraphrasing Phenomena Classiﬁed Although approximate equivalence is hard to characterize, it is not a completely unstructured phenomenon. By studying various existing paraphrase theories—Mel’cuk (2012), Harris (1981), Honeck (1971)—and through an analysis of paraphrases obtained from two different corpora, we have discovered that one can identify a set of 25 classes of quasi-paraphrases, with each class having its own speciﬁc way of relaxing the requirement of strict semantic equivalence. In this section, we deﬁne and describe these classes. 464  Bhagat and Hovy  What Is a Paraphrase?  The classes described here categorize quasi-paraphrases from the lexical perspective. The lexical perspective deﬁnes paraphrases in terms of the kinds of lexical changes that can take place in a sentence/phrase resulting in the generation of its paraphrases. 1. Synonym substitution: Replacing a word/phrase by a synonymous word/phrase, in the appropriate context, results in a paraphrase of the original sentence/phrase. This category covers the special case of genitives, where the clitic ’s is replaced by other genitive indicators like of, of the, and so forth. This category also covers near-synonymy, that is, it allows for changes in evaluation, connotation, and so on, of words or phrases between paraphrases. Example: (a) Google bought YouTube. ⇔ Google acquired YouTube. (b) Chris is slim. ⇔ Chris is slender. ⇔ Chris is skinny. 2. Antonym substitution: Replacing a word/phrase by its antonym accompanied by a negation or by negating some other word, in the appropriate context, results in a paraphrase of the original sentence/phrase. This substitution may be accompanied by the addition/deletion of appropriate function words. Example: (a) Pat ate. ⇔ Pat did not starve. 3. Converse substitution: Replacing a word/phrase with its converse and inverting the relationship between the constituents of a sentence/phrase, in the appropriate context, results in a paraphrase of the original sentence/phrase, presenting the situation from the converse perspective. This substitution may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. Example: (a) Google bought YouTube. ⇔ YouTube was sold to Google. 4. Change of voice: Changing a verb from its active to passive form and vice versa results in a paraphrase of the original sentence/phrase. This change may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. This often generates the most strictly meaning-preserving paraphrase. Example: (a) Pat loves Chris. ⇔ Chris is loved by Pat. 5. Change of person: Changing the grammatical person of a referenced object results in a paraphrase of the original sentence/phrase. This change may be accompanied by the addition/deletion of appropriate function words. Example: (a) Pat said, “I like football.” ⇔ Pat said that he liked football. 6. Pronoun/Co-referent substitution: Replacing a pronoun by the noun phrase it co-refers with results in a paraphrase of the original sentence/phrase. This also often generates the most strictly meaning-preserving paraphrase. Example: (a) Pat likes Chris, because she is smart. ⇔ Pat likes Chris, because Chris is smart. 465  Computational Linguistics  Volume 39, Number 3  7. Repetition/Ellipsis: Ellipsis or elliptical construction results in a paraphrase of the original sentence/phrase. Similarly, this often generates the most strictly meaningpreserving paraphrase. Example: (a) Pat can run fast and Chris can run fast, too. ⇔ Pat can run fast and Chris can, too. 8. Function word variations: Changing the function words in a sentence/phrase without affecting its semantics, in the appropriate context, results in a paraphrase of the original sentence/phrase. This can involve replacing a light verb by another light verb, replacing a light verb by copula, replacing certain prepositions with other prepositions, replacing a determiner by another determiner, replacing a determiner by a preposition and vice versa, and addition/removal of a preposition and/or a determiner. Example: (a) Results of the competition have been declared. ⇔ Results for the competition have been declared. (b) Pat showed a nice demo. ⇔ Pat’s demo was nice. 9. Actor/Action substitution: Replacing the name of an action by a word/phrase denoting the person doing the action (actor) and vice versa, in the appropriate context, results in a paraphrase of the original sentence/phrase. This substitution may be accompanied by the addition/deletion of appropriate function words. Example: (a) I dislike rash drivers. ⇔ I dislike rash driving. 10. Verb/“Semantic-role noun” substitution: Replacing a verb by a noun corresponding to the agent of the action or the patient of the action or the instrument used for the action or the medium used for the action, in the appropriate context, results in a paraphrase of the original sentence/phrase. This substitution may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. Example: (a) Pat teaches Chris. ⇔ Pat is Chris’s teacher. (b) Pat teaches Chris. ⇔ Chris is Pat’s student. (c) Pat tiled his bathroom ﬂoor. ⇔ Pat installed tiles on his bathroom ﬂoor. 11. Manipulator/Device substitution: Replacing the name of a device by a word/ phrase denoting the person using the device (manipulator) and vice versa, in the appropriate context, results in a paraphrase of the original sentence/phrase. This substitution may be accompanied by the addition/deletion of appropriate function words. Example: (a) The pilot took off despite the stormy weather. ⇔ The plane took off despite the stormy weather. 12. General/Speciﬁc substitution: Replacing a word/phrase by a more general or more speciﬁc word/phrase, in the appropriate context, results in a paraphrase of the original 466  Bhagat and Hovy  What Is a Paraphrase?  sentence/phrase. This substitution may be accompanied by the addition/deletion of appropriate function words. Hypernym/hyponym substitution is a part of this category. This often generates a quasi-paraphrase. Example: (a) I dislike rash drivers. ⇔ I dislike rash motorists. (b) Pat is ﬂying in this weekend. ⇔ Pat is ﬂying in this Saturday. 13. Metaphor substitution: Replacing a noun by its standard metaphorical use and vice versa, in the appropriate context, results in a paraphrase of the original sentence/ phrase. This substitution may be accompanied by the addition/deletion of appropriate function words. Example: (a) I had to drive through fog today. ⇔ I had to drive through a wall of fog today. (b) Immigrants have used this network to send cash. ⇔ Immigrants have used this network to send stashes of cash. 14. Part/Whole substitution: Replacing a part by its corresponding whole and vice versa, in the appropriate context, results in a paraphrase of the original sentence/ phrase. This substitution may be accompanied by the addition/deletion of appropriate function words. Example: (a) American airplanes pounded the Taliban defenses. ⇔ American airforce pounded the Taliban defenses. 15. Verb/Noun conversion: Replacing a verb by its corresponding nominalized noun form and vice versa, in the appropriate context, results in a paraphrase of the original sentence/phrase. This substitution may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. Example: (a) The police interrogated the suspects. ⇔ The police subjected the suspects to an interrogation. (b) The virus spread over two weeks. ⇔ Two weeks saw a spreading of the virus. 16. Verb/Adjective conversion: Replacing a verb by the corresponding adjective form and vice versa, in the appropriate context, results in a paraphrase of the original sentence/phrase. This substitution may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. Example: (a) Pat loves Chris. ⇔ Chris is lovable to Pat. 17. Verb/Adverb conversion: Replacing a verb by its corresponding adverb form and vice versa, in the appropriate context, results in a paraphrase of the original sentence/ phrase. This substitution may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. Example: (a) Pat boasted about his work. ⇔ Pat spoke boastfully about his work. 467  Computational Linguistics  Volume 39, Number 3  18. Noun/Adjective conversion: Replacing a verb by its corresponding adjective form and vice versa, in the appropriate context, results in a paraphrase of the original sentence/phrase. This substitution may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. Example: (a) I’ll ﬂy by the end of June. ⇔ I’ll ﬂy late June. 19. Verb-preposition/Noun substitution: Replacing a verb and a preposition denoting location by a noun denoting the location and vice versa, in the appropriate context, results in a paraphrase of the original sentence/phrase. This substitution may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. Example: (a) The ﬁnalists will play in Giants stadium. ⇔ Giants stadium will be the playground for the ﬁnalists. 20. Change of tense: Changing the tense of a verb, in the appropriate context, results in a paraphrase of the original sentence/phrase. This change may be accompanied by the addition/deletion of appropriate function words. This often generates a quasiparaphrase, although it might be semantically less accurate than many other quasiparaphrases. Example: (a) Pat loved Chris. ⇔ Pat loves Chris. 21. Change of aspect: Changing the aspect of a verb, in the appropriate context, results in a paraphrase of the original sentence/phrase. This change may be accompanied by the addition/deletion of appropriate function words. Example: (a) Pat is ﬂying in today. ⇔ Pat ﬂies in today. 22. Change of modality: Addition/deletion of a modal or substitution of one modal by another, in the appropriate context, results in a paraphrase of the original sentence/phrase. This change may be accompanied by the addition/deletion of appropriate function words. This often generates a quasi-paraphrase, although it might be semantically less accurate than many other quasi-paraphrases. Example: (a) Google must buy YouTube. ⇔ Google bought YouTube. (b) The government wants to boost the economy. ⇔ The government hopes to boost the economy. 23. Semantic implication: Replacing a word/phrase denoting an action, event, and so forth, by a word/phrase denoting its possible future effect, in the appropriate context, results in a paraphrase of the original sentence/phrase. This may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. This often generates a quasi-paraphrase. Example: (a) Google is in talks to buy YouTube. ⇔ Google bought YouTube. (b) The Marines are ﬁghting the terrorists. ⇔ The Marines are eliminating the terrorists. 468  Bhagat and Hovy  What Is a Paraphrase?  24. Approximate numerical equivalences: Replacing a numerical expression (a word/ phrase denoting a number, often with a unit) by an approximately equivalent numerical expression (even perhaps with change of unit), in the appropriate context, results in a paraphrase of the original sentence/phrase. This often generates a quasiparaphrase. Example: (a) At least 23 U.S. soldiers were killed in Iraq last month. ⇔ About 25 U.S. soldiers were killed in Iraq last month. (b) Disneyland is 32 miles from here. ⇔ Disneyland is around 30 minutes from here. 25. External knowledge: Replacing a word/phrase by another word/phrase based on extra-linguistic (world) knowledge, in the appropriate context, results in a paraphrase of the original sentence/phrase. This may be accompanied by the addition/deletion of appropriate function words and sentence restructuring. This often generates a quasiparaphrase, although in some cases preserves meaning exactly. Example: (a) We must work hard to win this election. ⇔ The Democrats must work hard to win this election. (b) The government declared victory in Iraq. ⇔ Bush declared victory in Iraq. 3. Analysis of Paraphrases In Section 2, we presented a list of lexical changes that deﬁne quasi-paraphrases. In this section, we seek to validate the scope and accuracy of this list. Our analysis uses two criteria: 1. Distribution: What is the distribution of each of these lexical changes in a paraphrase corpus? 2. Human judgment: If one uses each of the lexical changes, on applicable sentences, how often do each of these changes generate acceptable quasi-paraphrases? 3.1 Distribution We used the following procedure to measure the distribution of the lexical changes: 1. We downloaded paraphrases from two publicly available data sets containing sentence-level paraphrases: the Multiple-Translations Corpus (MTC) (Huang, Graff, and Doddington 2002) and the Microsoft Research (MSR) paraphrase corpus (Dolan, Quirk, and Brockett 2004). The paraphrase pairs come with their equivalent parts manually aligned (Cohn, Callison-Burch, and Lapata 2008). 2. We selected 30 sentence-level paraphrase pairs from each of these corpora at random and extracted the corresponding aligned and unaligned phrases.1 This resulted in 210 phrase pairs for the MTC corpus and 145 phrase pairs for the MSR corpus.  
Fine-grained opinion analysis methods often make use of linguistic features but typically do not take the interaction between opinions into account. This article describes a set of experiments that demonstrate that relational features, mainly derived from dependency-syntactic and semantic role structures, can signiﬁcantly improve the performance of automatic systems for a number of ﬁne-grained opinion analysis tasks: marking up opinion expressions, ﬁnding opinion holders, and determining the polarities of opinion expressions. These features make it possible to model the way opinions expressed in natural-language discourse interact in a sentence over arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously, which makes the search for the optimal analysis intractable. However, a reranker can be used as a sufﬁciently accurate and efﬁcient approximation. A number of feature sets and machine learning approaches for the rerankers are evaluated. For the task of opinion expression extraction, the best model shows a 10-point absolute improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local contextual features, while precision decreases only slightly. Signiﬁcant improvements are also seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall, respectively. In addition, the systems outperform previously published results for unlabeled (6 F-measure points) and polarity-labeled (10–15 points) opinion expression extraction. Finally, as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical opinion mining tasks. In all scenarios considered, the machine learning features derived from the opinion expressions lead to statistically signiﬁcant improvements. 1. Introduction Automatic methods for the analysis of opinions (textual expressions of emotions, beliefs, and evaluations) have attracted considerable attention in the natural language ∗ Spra˚kbanken, Department of Swedish, University of Gothenburg, Box 100, SE-40530 Gothenburg, Sweden. E-mail: richard.johansson@gu.se. The work described here was mainly carried out at the University of Trento. ∗∗ DISI – Department of Information Engineering and Computer Science, University of Trento, Via Sommarive 14, 38123 Trento (TN), Italy. E-mail: moschitti@disi.unitn.it. Submission received: 11 January 2012; revised version received: 11 May 2012; accepted for publication: 11 June 2012. doi:10.1162/COLI a 00141 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 3  processing community in recent years (Pang and Lee 2008). Apart from their interest from a linguistic and psychological point of view, the technologies emerging from this research have obvious practical uses, either as stand-alone applications or supporting other tools such as information retrieval or question answering systems. The research community initially focused on high-level tasks such as retrieving documents or passages expressing opinion, or classifying the polarity of a given text, and these coarse-grained problem formulations naturally led to the application of methods derived from standard retrieval or text categorization techniques. The models underlying these approaches have used very simple feature representations such as purely lexical (Pang, Lee, and Vaithyanathan 2002; Yu and Hatzivassiloglou 2003) or low-level grammatical features such as part-of-speech tags and functional words (Wiebe, Bruce, and O’Hara 1999). This is in line with the general consensus in the information retrieval community that very little can be gained by complex linguistic processing for tasks such as text categorization and search (Moschitti and Basili 2004). There are a few exceptions, such as Karlgren et al. (2010), who showed that construction features added to a bag-ofwords representation resulted in improved performance on a number of coarse-grained opinion analysis tasks. Similarly, Greene and Resnik (2009) argued that a speaker’s attitude can be predicted from syntactic features such as the selection of a transitive or intransitive verb frame. In contrast to the early work, recent years have seen a shift towards more detailed problem formulations where the task is not only to ﬁnd a piece of opinionated text, but also to extract a structured representation of the opinion. For instance, we may determine the person holding the opinion (the holder) and towards which entity or fact it is directed (the topic), whether it is positive or negative (the polarity), and the strength of the opinion (the intensity). The increasing complexity of representation leads us from retrieval and categorization deep into natural language processing territory; the methods used here have been inspired by information extraction and semantic role labeling, combinatorial optimization, and structured machine learning. For such tasks, deeper representations of linguistic structure have seen more use than in the coarsegrained case. Syntactic and shallow-semantic relations have repeatedly proven useful for subtasks of opinion analysis that are relational in nature, above all for determining the holder or topic of a given opinion, in which case there is considerable similarity to tasks such as semantic role labeling (Kim and Hovy 2006; Ruppenhofer, Somasundaran, and Wiebe 2008). There has been no systematic research, however, on the role played by linguistic structure in the relations between opinions expressed in text, despite the fact that the opinion expressions in a sentence are not independent but organized rhetorically to achieve a communicative effect intended by the speaker. We therefore expect that the interplay between opinion expressions can be exploited to derive information useful for the analysis of opinions expressed in text. In this article, we start from this intuition and propose several novel features derived from the interdependencies between opinion expressions on the syntactic and shallow-semantic levels. Based on these features, we devised structured prediction models for (1) extraction of opinion expressions, (2) joint expression extraction and holder extraction, and (3) joint expression extraction and polarity labeling. The models were trained using a number of discriminative machine learning methods. Because the interdependency features required us to consider more than one opinion expression at a time, the inference steps carried out at training and prediction time could not rely on commonly used opinion expression mark-up methods based on Viterbi search, but we show that an approximate search method using reranking sufﬁces for this purpose: In a ﬁrst step a base system  474  Johansson and Moschitti  Relational Features in Fine-Grained Opinion Analysis  using local features and efﬁcient search generates a small set of hypotheses, and in a second step a classiﬁer using the complex features selects the ﬁnal output from the hypothesis set. This approach allows us to make use of arbitrary features extracted from the complete set of opinion expressions in a sentence, without having to impose any restriction on the expressivity of the features. An additional advantage is that it is fairly easy to implement as long as the underlying system is able to generate k-best output. The interaction-based reranking systems were evaluated on a test set extracted from the MPQA corpus, and compared to strong baselines consisting of stand-alone systems for opinion expression mark-up, opinion holder extraction, and polarity classiﬁcation. Our evaluations showed that (1) the best opinion expression mark-up system we evaluated achieved a 10-point absolute improvement in soft recall, and a 5-point improvement in F-measure, over the baseline sequence labeler. Our system outperformed previously described opinion expression mark-up tools by six points in overlap F-measure. (2) The recall was boosted by almost 10 points for the holder extraction task (over three points in F-measure) by modeling the interaction of opinion expressions with respect to holders. (3) We saw an improvement for the extraction of polaritylabeled expression of four F-measure points. Our result for opinion extraction and polarity labeling is especially striking when compared with the best previously published end-to-end system for this task: 10–15 points in F-measure improvement. In addition to the performance evaluations, we studied the impact of features on the subtasks, and the effect of the choice of the machine learning method for training the reranker. As a ﬁnal extrinsic evaluation of the system, we evaluated the usefulness of its output in a number of applications. Although there have been several publications detailing the extraction of MPQA-style opinion expressions, as far as we are aware there has been no attempt to use them in an application. In contrast, we show that the opinion expressions as deﬁned by the MPQA corpus may be used to derive machine learning features that are useful in two practical opinion mining tasks; the addition of these features leads to statistically signiﬁcant improvements in all scenarios we evaluated. First, we develop a system for the extraction of evaluations of product attributes from product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov and McDonald 2008), and we show that the features derived from opinion expressions lead to signiﬁcant improvement. Secondly, we show that ﬁne-grained opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classiﬁcation of reviews (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2004). After the present introduction, Section 2 gives a linguistic motivation and an overview of the related work; Section 3 describes the MPQA opinion corpus and its underlying representation; Section 4 illustrates the baseline systems: a sequence labeler for the extraction of opinion expressions and classiﬁers for opinion holder extraction and polarity labeling; Section 5 reports on the main contribution: the description of the interaction models and their features; ﬁnally, Section 7 presents the experimental results and Section 8 derives the conclusions. 2. Motivation and Related Work Intuitively, interdependency features could be useful in the process of locating and disambiguating expressions of opinion. These expressions tend to occur in patterns, and the presence of one opinionated piece of text may inﬂuence our interpretation of another as opinionated or not. Consider, for instance, the word said in sentences (a) and (b) in Example (1), where the presence or non-presence of emotionally loaded words in the  475  Computational Linguistics  Volume 39, Number 3  complement clause provides evidence for the interpretation as a subjective opinion or an objective speech event. (In the example, opinionated expressions are marked S for subjective and the non-opinionated speech event O for objective.) Example (1) (a) “We will identify the [culprits]S of these clashes and [punish]S them,” he [said]S. (b) On Monday, 80 Libyan soldiers disembarked from an Antonov transport plane carrying military equipment, an African diplomat [said]O. Moreover, opinions expressed in a sentence are interdependent when it comes to the resolution of their holders—the person or entity having the attitude expressed in the opinion expression. Clearly, the structure of the sentence is also inﬂuential for this task because certain linguistic constructions provide evidence for opinion holder correlation. In the most obvious case, shown in the two sentences in Example (2), pejorative words share the opinion holder with the communication and categorization verbs dominating them. (Here, opinions are marked S and holders H.) Example (2) (a) [Domestic observers]H [tended to side with]S the MDC, [denouncing]S the election as [fraud-tainted]S and [unfair]S. (b) [Bush]H [labeled]S North Korea, Iran and Iraq an “[axis of evil]S.” In addition, interaction is important when determining opinion polarity. Here, relations that inﬂuence polarity interpretation include coordination, verb–complement, as well as other types of discourse relations. In particular, the presence of a COMPARISON discourse relation, such as contrast or concession (Prasad et al. 2008), may allow us to infer that opinion expressions have different polarities. In Example (3), we see how contrastive discourse connectives (underlined) are used when there are contrasting polarities in the surrounding opinion expressions. (Positive opinions are tagged ‘+’, negative ‘-’.) Example (3) (a) “[This is no blind violence but rather targeted violence]-,” Annemie Neyts [said]-. “However, the movement [is more than that]+.” (b) “[Trade alone will not save the world]-,” Neyts [said]-, but it constitutes an [important]+ factor for economic development. The problems we focus on in this article—extracting opinion expressions with holders and polarity labeling—have naturally been studied previously, especially since the release of the MPQA corpus (Wiebe, Wilson, and Cardie 2005). For the ﬁrst subtask, because the MPQA corpus uses span-based annotation to represent opinions, it is natural to apply straightforward sequence labeling techniques to extract them. This idea has resulted in a number of publications (Choi, Breck, and Cardie 2006; Breck, Choi, and Cardie 2007). Such systems do not use any features describing the interaction between opinions, and it would not be possible to add interaction features because a Viterbibased sequence labeler by construction is restricted to using local features in a small contextual window. Works using syntactic features to extract topics and holders of opinions are numerous (Bethard et al. 2005; Kobayashi, Inui, and Matsumoto 2007; Joshi and Penstein-Rose´ 476  Johansson and Moschitti  Relational Features in Fine-Grained Opinion Analysis  2009; Wu et al. 2009). Semantic role analysis has also proven useful: Kim and Hovy (2006) used a FrameNet-based semantic role labeler to determine holder and topic of opinions. Similarly, Choi, Breck, and Cardie (2006) successfully used a PropBank-based role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for extracting holders, but did not consider their relations to the opinion expressions. Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniques are useful but not completely sufﬁcient for holder and topic identiﬁcation, and that other linguistic phenomena must be studied as well. Choi, Breck, and Cardie (2006) built a joint model of opinion expression extraction and holder extraction and applied integer linear programming to carry out the optimization step. While the tasks of opinion expression detection and polarity classiﬁcation of opinion expressions (Wilson, Wiebe, and Hoffmann 2009) have mostly been studied in isolation, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion expressions and assigned them polarity values and this is so far the only published result on joint opinion segmentation and polarity classiﬁcation. Their experiment, however, lacked the obvious baseline: a standard pipeline consisting of an expression tagger followed by a polarity classiﬁer. In addition, although their model is the ﬁrst end-to-end system for opinion expression extraction and polarity classiﬁcation, it is still based on sequence labeling and thus by construction limited in feature expressivity. On a conceptual level, discourse-oriented approaches (Asher, Benamara, and Mathieu 2009; Somasundaran et al. 2009; Zirn et al. 2011) applying interaction features for polarity classiﬁcation are arguably the most related because they are driven by a vision similar to ours: Individual opinion expressions interplay in discourse and thus provide information about each other. On a practical level there are obvious differences, since our features are extracted from syntactic and shallow-semantic linguistic representations, which we argue are reﬂections of discourse structure, while they extract features directly from a discourse representation. It is doubtful whether automatic discourse representation extraction in text is currently mature enough to provide informative features, whereas syntactic parsing and shallow-semantic analysis are today fairly reliable. Another related line of work is represented by Choi and Cardie (2008), where interaction features based on compositional semantics were used in a joint model for the assignment of polarity values to pre-segmented opinion expressions in a sentence. 3. The MPQA Corpus and its Annotation of Opinion Expressions The most detailed linguistic resource useful for developing automatic systems for opinion analysis is the MPQA corpus (Wiebe, Wilson, and Cardie 2005). In this article, we use the word opinion in its broadest sense, equivalent to the word private state used by the MPQA annotators (page 2): “opinions, emotions, sentiments, speculations, evaluations, and other private states (Quirk et al. 1985), i.e., internal states that cannot be directly observed by others.” The central building block in the MPQA annotation is the opinion expression (or subjective expression): A text piece that expresses a private state, allowing us to draw the conclusion that someone has a particular emotion or belief about some topic. Identifying these units allows us to carry out further analysis, such as the determination of opinion holder and the polarity of the opinion. The annotation scheme deﬁnes two types of opinion expressions: direct subjective expressions (DSEs), which are explicit  477  Computational Linguistics  Volume 39, Number 3  mentions of attitude or evaluation, and expressive subjective elements (ESEs), which signal the attitude of the speaker by the choice of words. The prototypical example of a DSE would be a verb of statement, feeling, or categorization such as praise or disgust. ESEs, on the other hand, are less easy to categorize syntactically; prototypical examples would include simple value-expressing adjectives such as beautiful and strongly charged words like appeasement or big government. In addition to DSEs and ESEs, the corpus also contains annotation for non-subjective statements, which are referred to as objective speech events (OSEs). Some words such as say may appear as DSEs or OSEs depending on the context, so for an automatic system there is a need for disambiguation. Example (4) shows a number of sentences from the MPQA corpus where DSEs and ESEs have been manually annotated. Example (4) (a) He [made such charges]DSE [despite the fact]ESE that women’s political, social, and cultural participation is [not less than that]ESE of men. (b) [However]ESE, it is becoming [rather fashionable]ESE to [exchange harsh words]DSE with each other [like kids]ESE. (c) For instance, he [denounced]DSE as a [human rights violation]ESE the banning and seizure of satellite dishes in Iran. (d) This [is viewed]DSE as the [main impediment]ESE to the establishment of political order in the country. Every expression in the corpus is connected to an opinion holder,1 an entity that experiences the sentiment or utters the evaluation that appears textually in the opinion expression. For DSEs, it is often fairly straightforward to ﬁnd the opinion holders because they tend to be realized as direct semantic arguments ﬁlling semantic roles such as SPEAKER or EXPERIENCER—and the DSEs tend to be verbs or nominalizations. For ESEs, the connection between the expression and the opinion holder is typically less clear-cut than for DSEs; the holder is more frequently implicit or located outside the sentence for ESEs than for DSEs. The MPQA corpus does not annotate links directly from opinion expressions to particular mentions of a holder entity. Instead, the opinions are connected to holder coreference chains that may span the whole text. Some opinion expressions are linked to entities that are not explicitly mentioned in the text. If this entity is the author of the text, it is called the writer, otherwise implicit. Example (5) shows sentences annotated with expressions and holders. Example (5) (a) For instance, [he]H1 [denounced]DSE/H1 as a [human rights violation]ESE/H1 the banning and seizure of satellite dishes in Iran. (b) [(writer)]H1: [He]H2 [made such charges]DSE/H2 [despite the fact]ESE/H1 that women’s political, social, and cultural participation is [not less than that]ESE/H1 of men. (c) [(implicit)]H1: This [is viewed]DSE/H1 as the [main impediment]ESE/H1 to the establishment of political order in the country.  
Graeme Hirst† University of Toronto Peter D. Turney‡ National Research Council Canada Knowing the degree of semantic contrast between words has widespread application in natural language processing, including machine translation, information retrieval, and dialogue systems. Manually created lexicons focus on opposites, such as hot and cold. Opposites are of many kinds such as antipodals, complementaries, and gradable. Existing lexicons often do not classify opposites into the different kinds, however. They also do not explicitly list word pairs that are not opposites but yet have some degree of contrast in meaning, such as warm and cold or tropical and freezing. We propose an automatic method to identify contrasting word pairs that is based on the hypothesis that if a pair of words, A and B, are contrasting, then there is a pair of opposites, C and D, such that A and C are strongly related and B and D are strongly related. (For example, there exists the pair of opposites hot and cold such that tropical is related to hot, and freezing is related to cold.) We will call this the contrast hypothesis. We begin with a large crowdsourcing experiment to determine the amount of human agreement on the concept of oppositeness and its different kinds. In the process, we ﬂesh out key features of different kinds of opposites. We then present an automatic and empirical measure of lexical contrast that relies on the contrast hypothesis, corpus statistics, and the structure of a Roget-like thesaurus. We show how, using four different data sets, we evaluated our approach on two different tasks, solving “most contrasting word” questions and distinguishing synonyms from opposites. The results are analyzed across four parts of speech and across ﬁve different kinds of opposites. We show that the proposed measure of lexical contrast obtains high precision and large coverage, outperforming existing methods. ∗ National Research Council Canada. E-mail: saif.mohammad@nrc-cnrc.gc.ca. ∗∗ Department of Computer Science and Institute of Advanced Computer Studies, University of Maryland. E-mail: bonnie@umiacs.umd.edu. † Department of Computer Science, University of Toronto. E-mail: gh@cs.toronto.edu. ‡ National Research Council Canada. E-mail: peter.turney@nrc-cnrc.gc.ca. Submission received: 14 January 2010; revised submission received: 26 June 2012; accepted for publication: 16 July 2012. doi:10.1162/COLI a 00143 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 3  1. Introduction Native speakers of a language intuitively recognize different degrees of lexical contrast— for example, most people will agree that hot and cold have a higher degree of contrast than cold and lukewarm, and cold and lukewarm have a higher degree of contrast than penguin and clown. Automatically determining the degree of contrast between words has many uses, including: r Detecting and generating paraphrases (Marton, El Kholy, and Habash 2011) (The dementors caught Sirius Black / Black could not escape the dementors). r Detecting certain types of contradictions (de Marneffe, Rafferty, and Manning 2008; Voorhees 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). This is in turn useful in effectively reranking target language hypotheses in machine translation, and for reranking query responses in information retrieval. r Understanding discourse structure and improving dialogue systems. Opposites often indicate the discourse relation of contrast (Marcu and Echihabi 2002). r Detecting humor (Mihalcea and Strapparava 2005). Satire and jokes tend to have contradictions and oxymorons. r Distinguishing near-synonyms from word pairs that are semantically contrasting in automatically created distributional thesauri. Measures of distributional similarity typically fail to do so. Detecting lexical contrast is not sufﬁcient by itself to solve most of these problems, but it is a crucial component. Lexicons of pairs of words that native speakers consider opposites have been created for certain languages, but their coverage is limited. Opposites are of many kinds, such as antipodals, complementaries, and gradable (summarized in Section 3). Existing lexicons often do not classify opposites into the different kinds, however. Further, the terminology is inconsistent across different sources. For example, Cruse (1986) deﬁnes antonyms as gradable adjectives that are opposite in meaning, whereas the WordNet antonymy link connects some verb pairs, noun pairs, and adverb pairs too. In this article, we will follow Cruse’s terminology, and we will refer to word pairs connected by WordNet’s antonymy link as opposites, unless referring speciﬁcally to gradable adjectival pairs. Manually created lexicons also do not explicitly list word pairs that are not opposites but yet have some degree of contrast in meaning, such as warm and cold or tropical and cold. Further, contrasting word pairs far outnumber those that are commonly considered opposites. In our own experiments described later in this article, we ﬁnd that more than 90% of the contrasting pairs in GRE “most contrasting word” questions are not listed as antonyms in WordNet. We should not infer from this that WordNet or any other lexicographic resource is a poor source for detecting opposites, but rather that identifying the large number of contrasting word pairs requires further computation, possibly relying on other semantic relations stored in the lexicographic resource. Even though a number of computational approaches have been proposed for semantic closeness (Curran 2004; Budanitsky and Hirst 2006), and some for hypernymy– 556  Mohammad et al.  Computing Lexical Contrast  hyponymy (Hearst 1992), measures of lexical contrast have been less successful. To some extent, this is because lexical contrast is not as well understood as other classical lexical– semantic relations. Over the years, many deﬁnitions of semantic contrast and opposites have been proposed by linguists (Lehrer and Lehrer 1982; Cruse 1986), cognitive scientists (Kagan 1984), psycholinguists (Deese 1965), and lexicographers (Egan 1984), which differ from each other in various respects. Cruse (1986, page 197) observes that even though people have a robust intuition of opposites, “the overall class is not a well-deﬁned one.” He points out that a deﬁning feature of opposites is that they tend to have many common properties, but differ saliently along one dimension of meaning. We will refer to this semantic dimension as the dimension of opposition. For example, giant and dwarf are both living beings; they both eat, they both walk, they are both capable of thinking, and so on. They are most saliently different, however, along the dimension of height. Cruse also points out that sometimes it is difﬁcult to identify or articulate the dimension of opposition (for example, city–farm). Another way to deﬁne opposites is that they are word pairs with a “binary incompatible relation” (Kempson 1977, page 84). That is to say that one member entails the absence of the other, and given one member, the identity of the other member is obvious. Thus, night and day are good examples of opposites because night is best paraphrased by not day, rather than the negation of any other term. On the other hand, blue and yellow make poor opposites because even though they are incompatible, they do not have an obvious binary relation such that blue is understood to be a negation of yellow. It should be noted that there is a relation between binary incompatibility and difference along just one dimension of meaning. For this article, we deﬁne opposites to be term pairs that clearly satisfy either the property of binary incompatibility or the property of salient difference across a dimension of meaning. Word pairs may satisfy the two properties to different degrees, however. We will refer to all word pairs that satisfy either of the two properties to some degree as contrasting. For example, daylight and darkness are very different along the dimension of light, and they satisfy the binary incompatibity property to some degree, but not as strongly as day and night. Thus we will consider both daylight and darkness as well as day and night as semantically contrasting pairs (the former pair less so than the latter), but only day and night as opposites. Even though there are subtle differences in the meanings of the terms contrasting, opposite, and antonym, they have often been used interchangeably in the literature, dictionaries, and common parlance. Thus, we list here what we use these terms to mean in this article: r Opposites are word pairs that have a strong binary incompatibility relation with each other and/or are saliently different across a dimension of meaning. r Contrasting word pairs are word pairs that have some non-zero degree of binary incompatibility and/or have some non-zero difference across a dimension of meaning. Thus, all opposites are contrasting, but not all contrasting pairs are opposites. r Antonyms are opposites that are also gradable adjectives.1  
Claire Gardent† CNRS - LORIA, Nancy Joseph Le Roux‡ LIPN - Universite´ Paris Nord Yannick Parmentier§ LIFO - Universite´ d’Orle´ans In this article, we introduce eXtensible MetaGrammar (XMG), a framework for specifying tree-based grammars such as Feature-Based Lexicalized Tree-Adjoining Grammars (FB-LTAG) and Interaction Grammars (IG). We argue that XMG displays three features that facilitate both grammar writing and a fast prototyping of tree-based grammars. Firstly, XMG is fully declarative. For instance, it permits a declarative treatment of diathesis that markedly departs from the procedural lexical rules often used to specify tree-based grammars. Secondly, the XMG language has a high notational expressivity in that it supports multiple linguistic dimensions, inheritance, and a sophisticated treatment of identiﬁers. Thirdly, XMG is extensible in that its computational architecture facilitates the extension to other linguistic formalisms. We explain how this architecture naturally supports the design of three linguistic formalisms, namely, FB-LTAG, IG, and Multi-Component Tree-Adjoining Grammar (MC-TAG). We further show how it permits a straightforward integration of additional mechanisms such as linguistic and formal principles. To further illustrate the declarativity, notational expressivity, and extensibility of XMG, we describe the methodology used to specify an FB-LTAG for French augmented with a ∗ UFR de Linguistique, Universite´ Paris Diderot-Paris 7, Case 7003, 2, F-75205 Paris Cedex 13, France. E-mail: bcrabbe@linguist.jussieu.fr. ∗∗ Laboratoire d’Informatique Fondamentale d’Orle´ans, Baˆtiment IIIA, Rue Le´onard de Vinci, B.P. 6759, F-45067 Orle´ans Cedex 2, France. E-mail: denys.duchier@univ-orleans.fr. † Laboratoire LORIA - CNRS, Projet Synalp, Baˆtiment B, BP 239, Campus Scientiﬁque, F-54506 Vandœuvre-Le`s-Nancy Cedex, France. E-mail: gardent@loria.fr. ‡ Laboratoire d’Informatique de Paris Nord, UMR CNRS 7030, Institut Galile´e - Universite´ Paris-Nord, 99, avenue Jean-Baptiste Cle´ment, F-93430 Villetaneuse, E-mail: leroux@univ-paris13.fr. § Laboratoire d’Informatique Fondamentale d’Orle´ans, Baˆtiment IIIA, Rue Le´onard de Vinci, B.P. 6759, F-45067 Orle´ans Cedex 2, France. E-mail: yannick.parmentier@univ-orleans.fr. Submission received: 27 March 2009; revised version received: 2 July 2012; accepted for publication: 11 August 2012. doi:10.1162/COLI a 00144 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 3  uniﬁcation-based compositional semantics. This illustrates both how XMG facilitates the modeling of the tree fragment hierarchies required to specify tree-based grammars and of a syntax/semantics interface between semantic representations and syntactic trees. Finally, we brieﬂy report on several grammars for French, English, and German that were implemented using XMG and compare XMG with other existing grammar speciﬁcation frameworks for tree-based grammars. 1. Introduction In the late 1980s and early 1990s, many grammar engineering environments were developed to support the speciﬁcation of large computational grammars for natural language. One may, for instance, cite XLE (Kaplan and Newman 1997) for specifying Lexical-Functional Grammars (LFG), LKB (Copestake and Flickinger 2000) for specifying Head-driven Phrase Structure Grammars (HPSG), and DOTCCG (Baldridge et al. 2007) for specifying Combinatory Categorial Grammars (CCG). Concretely, such environments usually rely on (i) a formal language used to describe a target computational grammar, and (ii) a processor for this language, which aims at generating the actual described grammar (and potentially at checking it, e.g., by feeding it to a parser). Although these environments were tailored for speciﬁc grammar formalisms, they share a number of features. Firstly, they are expressive enough to characterize subsets of natural language. Following Shieber (1984), we call this feature weak completeness. Secondly, they are notationally expressive enough to relatively easily formalize important theoretical notions. Thirdly, they are rigorous, that is, the semantics of their underlying language is well deﬁned and understood. Additionally, for an environment to be useful in practice, it should be simple to use (by a linguist), and make it possible to detect errors in the described target grammar. If we consider a particular type of computational grammar, namely, tree-based grammars—that is, grammars where the basic units are trees (or tree descriptions) of arbitrary depth, such as Tree-Adjoining Grammar (TAG; Joshi, Levy, and Takahashi 1975), D-Tree Grammar (DTG; Rambow, Vijay-Shanker, and Weir 1995), Tree Description Grammars (TDG; Kallmeyer 1999) or Interaction Grammars (IG; Perrier 2000)— environments sharing all of the listed features are lacking. As we shall see in Section 7 of this article, there have been some proposals for grammar engineering environments for tree-based grammar (e.g., Candito 1996; Xia, Palmer, and Vijay-Shanker 1999, but these lack notational expressivity. This is partly due to the fact that tree-based formalisms offer an extended domain of locality where one can encode constraints between remote syntactic constituents. If one wants to deﬁne such constraints while giving a modular and incremental speciﬁcation of the grammar, one needs a high level of notational expressivity, as we shall see throughout the article (and especially in Section 4). In this article, we present XMG (eXtensible MetaGrammar), a framework for specifying tree-based grammars. Focusing mostly on Feature-Based Lexicalized TreeAdjoining Grammars (FB-LTAG) (but using Interaction Grammars [IG] and MultiComponent Tree-Adjoining Grammars [MC-TAG] to illustrate ﬂexibility), we argue that XMG departs from other existing computational frameworks for designing tree-based grammars in three main ways: r First, XMG is a declarative language. In other words, grammaticality is deﬁned in an order-independent fashion by a set of well-formedness 592  Crabbe´ et al.  XMG: eXtensible MetaGrammar  constraints rather than by procedures. In particular, XMG permits a fully declarative treatment of diathesis that markedly departs from the procedural rules (called meta-rules or lexical rules) previously used to specify tree-based grammars. r Second, XMG is notationally expressive. The XMG language supports full disjunction and conjunction of grammatical units, a modular treatment of multiple linguistic dimensions, multiple inheritance of units, and a sophisticated treatment of identiﬁers. We illustrate XMG’s notational expressivity by showing (i) how it facilitates the modeling of the tree fragment hierarchies required to specify tree-based grammars and (ii) how it permits a natural modeling of the syntax/semantics interface between semantic representations and syntactic trees as can be used in FB-LTAG. r Third, XMG is extensible in that its computational architecture facilitates (i) the integration of an arbitrary number of linguistic dimensions (syntax, semantics, etc.), (ii) the modeling of different grammar formalisms (FB-LTAG, MC-TAG, IG), and (iii) the speciﬁcation of general linguistic principles (e.g., clitic ordering in French). The article is structured as follows. Section 2 starts by giving a brief introduction to FB-LTAG, the grammar formalism we used to illustrate most of XMG’s features. The next three sections then go on to discuss and illustrate XMG’s three main features— namely, declarativity, notational expressivity, and ﬂexibility. In Section 3, we focus on declarativity and show how XMG’s generalized disjunction permits a declarative encoding of diathesis. We then contrast the XMG approach with the procedural methods previously resorted to for specifying FB-LTAG. Section 4 addresses notational expressivity. We present the syntax of XMG and show how the sophisticated identiﬁer handling it supports or permits a natural treatment (i) of identiﬁers in tree based hierarchies and (ii) of the uniﬁcation-based syntax/semantics interface often used in FB-LTAG. In Section 5, we concentrate on extensibility. We ﬁrst describe the operational semantics of XMG and the architecture of the XMG compiler. We then show how these facilitate the adaptation of the basic XMG language to (i) different grammar formalisms (IG, MC-TAG, FB-LTAG), (ii) the integration of speciﬁc linguistic principles such as clitic ordering constraints, and (iii) the speciﬁcation of an arbitrary number of linguistic dimensions. In Section 6, we illustrate the usage of XMG by presenting an XMG speciﬁcation for the verbal fragment of a large scale FB-LTAG for French augmented with a uniﬁcation-based semantics. We also brieﬂy describe the various other treebased grammars implemented using XMG. Section 7 discusses the limitations of other approaches to the formal speciﬁcation of tree-based grammars, and Section 8 concludes with pointers for further research. 2. Tree-Adjoining Grammar A Tree-Adjoining Grammar (TAG) consists of a set of auxiliary or initial elementary trees and of two tree composition operations, namely, substitution and adjunction. Initial trees are trees whose leaves are either substitution nodes (marked with ↓) or terminal symbols (words). Auxiliary trees are distinguished by a foot node (marked with ) whose category must be the same as that of the root node. Substitution inserts a tree onto a substitution node of some other tree and adjunction inserts an auxiliary tree 593  Computational Linguistics  Volume 39, Number 3  S  S  N Marie Mary  V VV  N↓ V N↓ vu seen  N  ⇒∗  N  V  N  Marie V V Jean  Jean John  Mary  John  a vu  has seen  a has Figure 1 Sample derivation of Marie a vu Jean ‘Mary has seen John’ in a TAG.  into a tree. Figure 1 shows a toy TAG generating the sentence Marie a vu Jean ‘Mary has seen John’ and sketches its derivation.1 Among existing variants of TAG, one commonly used in practice is Lexicalized FB-LTAG (Vijay-Shanker and Joshi 1988). A lexicalized TAG is such that each elementary tree has at least one leaf labeled with a lexical item (word), whereas in an FB-LTAG, tree nodes are additionally decorated with two feature structures (called top and bottom). These feature structures are uniﬁed during derivation as follows. On substitution, the top features of the substitution node are uniﬁed with the top features of the root node of the tree being substituted in. On adjunction, the top features of the root of the auxiliary tree are uniﬁed with the top features of the node where adjunction takes place; and the bottom features of the foot node of the auxiliary tree are uniﬁed with the bottom features of the node where adjunction takes place. At the end of a derivation, the top and bottom feature structures of all nodes in the derived tree are uniﬁed.  Implementation of Tree-Adjoining Grammars. Most existing implementations of TAGs follow the three-layer architecture adopted for the XTAG grammar (XTAG Research Group 2001), a feature-based lexicalized TAG for English. Thus the grammar consists of (i) a set of so-called tree schemas (i.e., elementary trees having a leaf node labeled with a referring to where to anchor lexical items2), (ii) a morphological lexicon associating words with lemmas, and (iii) a syntactic lexicon associating lemmas with tree schemas (these are gathered into families according to syntactic properties, such as the subcategorization frame for verbs). Figure 2 shows some of the tree schemas associated with transitive verbs in the XTAG grammar. The tree corresponds (a) to a declarative sentence, (b) to a WH-question on the subject, (c) to a passive clause with a BY-agent, and (d) to a passive clause with a WH-object. As can be seen, each tree schema contains an anchor node (marked with ). During parsing this anchor node can be replaced by any word morphologically related to a lemma listed in the syntactic lexicon as anchoring the transitive tree family. This concept of tree family allows us to share structural information (tree schemas) between words having common syntactic properties (e.g., sub-categorization frames). There still remains a large redundancy within the grammar because many elementary tree schemas share common subtrees (large coverage TAGs usually consist of hundreds, sometimes thousands, of tree schemas). An important issue when specifying  
Llu´ıs Ma`rquez† Universitat Polite`cnica de Catalunya Mihai Surdeanu‡ University of Arizona This paper focuses on a well-known open issue in Semantic Role Classiﬁcation (SRC) research: the limited inﬂuence and sparseness of lexical features. We mitigate this problem using models that integrate automatically learned selectional preferences (SP). We explore a range of models based on WordNet and distributional-similarity SPs. Furthermore, we demonstrate that the SRC task is better modeled by SP models centered on both verbs and prepositions, rather than verbs alone. Our experiments with SP-based models in isolation indicate that they outperform a lexical baseline with 20 F1 points in domain and almost 40 F1 points out of domain. Furthermore, we show that a state-of-the-art SRC system extended with features based on selectional preferences performs signiﬁcantly better, both in domain (17% error reduction) and out of domain (13% error reduction). Finally, we show that in an end-to-end semantic role labeling system we obtain small but statistically signiﬁcant improvements, even though our modiﬁed SRC model affects only approximately 4% of the argument candidates. Our post hoc error analysis indicates that the SP-based features help mostly in situations where syntactic information is either incorrect or insufﬁcient to disambiguate the correct role. ∗ Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: benat.zapirain@ehu.es. ∗∗ Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: e.agirre@ehu.es. † UPC Campus Nord (Omega building), Jordi Girona 1–3, 08034 Barcelona, Catalonia. E-mail: lluism@lsi.upc.edu. ‡ 1040 E. 4th Street, Tucson, AZ 85721. E-mail: msurdeanu@arizona.edu. Submission received: 14 November 2011; revised submission received: 31 May 2012; accepted for publication: 15 August 2012. doi:10.1162/COLI a 00145 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 3  1. Introduction Semantic Role Labeling (SRL) is the problem of analyzing clause predicates in text by identifying arguments and tagging them with semantic labels indicating the role they play with respect to the predicate. Such sentence-level semantic analysis allows the determination of who did what to whom, when and where, and thus characterizes the participants and properties of the events established by the predicates. For instance, consider the following sentence, in which the arguments of the predicate to send have been annotated with their respective semantic roles.1 (1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal. Recognizing these event structures has been shown to be important for a broad spectrum of NLP applications. Information extraction, summarization, question answering, machine translation, among others, can beneﬁt from this shallow semantic analysis at sentence level, which opens the door for exploiting the semantic relations among arguments (Boas 2002; Surdeanu et al. 2003; Narayanan and Harabagiu 2004; Melli et al. 2005; Moschitti et al. 2007; Higashinaka and Isozaki 2008; Surdeanu, Ciaramita, and Zaragoza 2011). In Ma`rquez et al. (2008) the reader can ﬁnd a broad introduction to SRL, covering several historical and deﬁnitional aspects of the problem, including also references to the main resources and systems. State-of-the-art systems leverage existing hand-tagged corpora (Fillmore, Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised machine learning systems, and typically perform SRL in two sequential steps: argument identiﬁcation and argument classiﬁcation. Whereas the former is mostly a syntactic recognition task, the latter usually requires semantic knowledge to be taken into account. The semantic knowledge that most current systems capture from text is basically limited to the predicates and the lexical units contained in their arguments, including the argument head. These “lexical features” tend to be sparse, especially when the training corpus is small, and thus SRL systems are prone to overﬁt the training data and generalize poorly to new corpora (Pradhan, Ward, and Martin 2008). As a simpliﬁed example of the effect of sparsity, consider the following sentences occurring in an imaginary training data set for SRL: (2) [JFK]Patient was assassinated [in Dallas]Location (3) [John Lennon]Patient was assassinated [in New York]Location (4) [JFK]Patient was assassinated [in November]Temporal (5) [John Lennon]Patient was assassinated [in winter]Temporal All four sentences share the same syntactic structure, so the lexical features (i.e., the words Dallas, New York, November, and winter) represent the most relevant knowledge for discriminating between the Location and Temporal adjunct labels in learning.  
Roberto Navigli∗ Sapienza University of Rome In 2004 we published in this journal an article describing OntoLearn, one of the ﬁrst systems to automatically induce a taxonomy from documents and Web sites. Since then, OntoLearn has continued to be an active area of research in our group and has become a reference work within the community. In this paper we describe our next-generation taxonomy learning methodology, which we name OntoLearn Reloaded. Unlike many taxonomy learning approaches in the literature, our novel algorithm learns both concepts and relations entirely from scratch via the automated extraction of terms, deﬁnitions, and hypernyms. This results in a very dense, cyclic and potentially disconnected hypernym graph. The algorithm then induces a taxonomy from this graph via optimal branching and a novel weighting policy. Our experiments show that we obtain high-quality results, both when building brand-new taxonomies and when reconstructing sub-hierarchies of existing taxonomies. 1. Introduction Ontologies have proven useful for different applications, such as heterogeneous data integration, information search and retrieval, question answering, and, in general, for fostering interoperability between systems. Ontologies can be classiﬁed into three main types (Sowa 2000), namely: i) formal ontologies, that is, conceptualizations whose categories are distinguished by axioms and formal deﬁnitions, stated in logic to support complex inferences and computations; ii) prototype-based ontologies, which are based on typical instances or prototypes rather than axioms and deﬁnitions in logic; iii) lexicalized (or terminological) ontologies, which are speciﬁed by subtype-supertype relations and describe concepts by labels or synonyms rather than by prototypical instances. Here we focus on lexicalized ontologies because, in order to enable natural language applications such as semantically enhanced information retrieval and question answering, we need a clear connection between our formal representation of the ∗ Dipartimento di Informatica, Sapienza Universita` di Roma, Via Salaria, 113, 00198 Roma Italy. E-mail: {velardi,faralli,navigli}@di.uniroma1.it. Submission received: 17 December 2011; revised submission received: 28 July 2012; accepted for publication: 10 October 2012. doi:10.1162/COLI a 00146 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 3  domain and the language used to express domain meanings within text. And, in turn, this connection can be established by producing full-ﬂedged lexicalized ontologies for the domain of interest. Manually constructing ontologies is a very demanding task, however, requiring a large amount of time and effort, even when principled solutions are used (De Nicola, Missikoff, and Navigli 2009). A quite recent challenge, referred to as ontology learning, consists of automatically or semi-automatically creating a lexicalized ontology using textual data from corpora or the Web (Gomez-Perez and Manzano-Mancho 2003; Biemann 2005; Maedche and Staab 2009; Petasis et al. 2011). As a result of ontology learning, the heavy requirements of manual ontology construction can be drastically reduced. In this paper we deal with the problem of learning a taxonomy (i.e., the backbone of an ontology) entirely from scratch. Very few systems in the literature address this task. OntoLearn (Navigli and Velardi 2004) was one of the earliest contributions in this area. In OntoLearn taxonomy learning was accomplished in four steps: terminology extraction, derivation of term sub-trees via string inclusion, disambiguation of domain terms using a novel Word Sense Disambiguation algorithm, and combining the subtrees into a taxonomy. The use of a static, general-purpose repository of semantic knowledge, namely, WordNet (Miller et al. 1990; Fellbaum 1998), prevented the system from learning taxonomies in technical domains, however. In this paper we present OntoLearn Reloaded, a graph-based algorithm for learning a taxonomy from the ground up. OntoLearn Reloaded preserves the initial step of our 2004 pioneering work (Navigli and Velardi 2004), that is, automated terminology extraction from a domain corpus, but it drops the requirement for WordNet (thereby avoiding dependence on the English language). It also drops the term compositionality assumption that previously led to us having to use a Word Sense Disambiguation algorithm—namely, SSI (Navigli and Velardi 2005)—to structure the taxonomy. Instead, we now exploit textual deﬁnitions, extracted from a corpus and the Web in an iterative fashion, to automatically create a highly dense, cyclic, potentially disconnected hypernym graph. An optimal branching algorithm is then used to induce a full-ﬂedged treelike taxonomy. Further graph-based processing augments the taxonomy with additional hypernyms, thus producing a Directed Acyclic Graph (DAG). Our system provides a considerable advancement over the state of the art in taxonomy learning: r First, excepting for the manual selection of just a few upper nodes, this is the ﬁrst algorithm that has been experimentally shown to build from scratch a new taxonomy (i.e., both concepts and hypernym relations) for arbitrary domains, including very technical ones for which gold-standard taxonomies do not exist. r Second, we tackle the problem with no simplifying assumptions: We cope with issues such as term ambiguity, complexity of hypernymy patterns, and multiple hypernyms. r Third, we propose a novel algorithm to extract an optimal branching from the resulting hypernym graph, which—after some recovery steps—becomes our ﬁnal taxonomy. Taxonomy induction is the main theoretical contribution of the paper. r Fourth, the evaluation is not limited, as it is in most papers, to the number of retrieved hypernymy relations that are found in a reference taxonomy. 666  Velardi, Faralli, and Navigli  OntoLearn Reloaded  Instead, we also analyze the extracted taxonomy in its entirety; furthermore, we acquire two “brand new” taxonomies in the domains of ARTIFICIAL INTELLIGENCE and FINANCE. r Finally, our taxonomy-building workﬂow is fully implemented and the software components are either freely available from our Web site,1 or reproducible. In this paper we extend our recent work on the topic (Navigli, Velardi, and Faralli 2011) as follows: i) we describe in full detail the taxonomy induction algorithm; ii) we enhance our methodology with a ﬁnal step aimed at creating a DAG, rather than a strict tree-like taxonomical structure; iii) we perform a large-scale multi-faceted evaluation of the taxonomy learning algorithm on six domains; and iv) we contribute a novel methodology for evaluating an automatically learned taxonomy against a reference gold standard. In Section 2 we illustrate the related work. We then describe our taxonomyinduction algorithm in Section 3. In Section 4 we present our experiments, and discuss the results. Evaluation is both qualitative (on new ARTIFICIAL INTELLIGENCE and FINANCE taxonomies), and quantitative (on WordNet and MeSH sub-hierarchies). Section 5 is dedicated to concluding remarks. 2. Related Work Two main approaches are used to learn an ontology from text: rule-based and distributional approaches. Rule-based approaches use predeﬁned rules or heuristic patterns to extract terms and relations. These approaches are typically based on lexico-syntactic patterns, ﬁrst introduced by Hearst (1992). Instances of relations are harvested from text by applying patterns aimed at capturing a certain type of relation (e.g., X is a kind of Y). Such lexico-syntactic patterns can be deﬁned manually (Berland and Charniak 1999; Kozareva, Riloff, and Hovy 2008) or obtained by means of bootstrapping techniques (Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006). In the latter case, a number of term pairs in the wanted relation are manually picked and the relation is sought within text corpora or the Web. Other rule-based approaches learn a taxonomy by applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci, and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computational lexicons such as WordNet (Ponzetto and Navigli 2009). Distributional approaches, instead, model ontology learning as a clustering or classiﬁcation task, and draw primarily on the notions of distributional similarity (Pado and Lapata 2007; Cohen and Widdows 2009), clustering of formalized statements (Poon and Domingos 2010), or hierarchical random graphs (Fountain and Lapata 2012). Such approaches are based on the assumption that paradigmatically-related concepts2 appear in similar contexts and their main advantage is that they are able to discover relations that do not explicitly appear in the text. They are typically less accurate, however, and the selection of feature types, notion of context, and similarity metrics vary considerably depending on the speciﬁc approach used.  
Web search result clustering aims to facilitate information search on the Web. Rather than the results of a query being presented as a ﬂat list, they are grouped on the basis of their similarity and subsequently shown to the user as a list of clusters. Each cluster is intended to represent a different meaning of the input query, thus taking into account the lexical ambiguity (i.e., polysemy) issue. Existing Web clustering methods typically rely on some shallow notion of textual similarity between search result snippets, however. As a result, text snippets with no word in common tend to be clustered separately even if they share the same meaning, whereas snippets with words in common may be grouped together even if they refer to different meanings of the input query. In this article we present a novel approach to Web search result clustering based on the automatic discovery of word senses from raw text, a task referred to as Word Sense Induction. Key to our approach is to ﬁrst acquire the various senses (i.e., meanings) of an ambiguous query and then cluster the search results based on their semantic similarity to the word senses induced. Our experiments, conducted on data sets of ambiguous queries, show that our approach outperforms both Web clustering and search engines. 1. Introduction The Web is by far the largest information archive available worldwide. This vast pool of text contains information of the most wildly disparate kinds, and is potentially capable of satisfying virtually any conceivable user need. Unfortunately, however, in this setting retrieving the precise item of information that is relevant to a given user search can be like looking for a needle in a haystack. State-of-the-art search engines such as Google and Yahoo! generally do a good job at retrieving a small number of relevant results from such an enormous collection of data (i.e., retrieving with high precision, low recall). Such systems today, however, still ﬁnd themselves up against the lexical ambiguity issue ∗ Dipartimento di Informatica, Sapienza Universita` di Roma, Via Salaria, 113, 00198 Roma Italy. E-mail: {dimarco,navigli}@di.uniroma1.it. Submission received: 25 April 2012; revised submission received: 26 July 2012; accepted for publication: 12 September 2012. doi:10.1162/COLI a 00148 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 3  (Furnas et al. 1987; Navigli 2009), that is, the linguistic property due to which a single word may convey different meanings. Recently, the degree of ambiguity of Web queries has been studied using WordNet (Miller et al. 1990; Fellbaum 1998) and Wikipedia1 as sources of ambiguous words.2 It has been estimated that around 4% of Web queries and 16% of the most frequent queries are ambiguous (Sanderson 2008), as also conﬁrmed in later studies (Clough et al. 2009; Song et al. 2009). An example of an ambiguous query is Butterﬂy effect, which could refer to either chaos theory, a ﬁlm, a band, an album, a novel, or a collection of poetry. Similarly, black spider could refer to either an arachnid, a car, or a frying pan, and so forth. Lexical ambiguity is often the consequence of the low number of query words that Web users, on average, tend to type (Kamvar and Baluja 2006). This issue could be solved by expanding the initial query with unequivocal cue words. Interestingly, the average query length is continually growing. The average number of words per query is now estimated around three words per query,3 a number that is still too low to eradicate polysemy. The fact that there may be different informative needs for the same user query has been tackled by diversifying search results, an approach whereby a list of heterogenenous results is presented, and Web pages that are similar to ones already near the top are prevented from ranking too highly in the list (Agrawal et al. 2009; Swaminathan, Mathew, and Kirovski 2009). Today even commercial search engines are starting to rerank and diversify their results. Unfortunately, recent work suggests that diversity does not yet play a primary role in ranking algorithms (Santamar´ıa, Gonzalo, and Artiles 2010), but it undoubtedly has the potential to do so (Chapelle, Chang, and Liu 2011). Another mainstream approach to the lexical ambiguity issue is that of Web clustering engines (Carpineto et al. 2009), such as Carrot4 and Yippy.5 These systems group search results by providing a cluster for each speciﬁc meaning of the input query. Users can then select the cluster(s) and the pages therein that best answer their information needs. These approaches, however, do not perform any semantic analysis of search results, clustering them solely on the basis of their lexical similarity. For instance, given the query snow leopard, Google search returns, among others, the snippets reported in Table 1.6 In the third column of the table we provide the correct meanings associated with each snippet (i.e., either the operating system or the animal sense). Although snippets 2, 4, and 5 all refer to the same meaning, they have no content word in common apart from our query words. As a result, a traditional Web clustering engine would most likely assign these snippets to different clusters. Moreover, snippet 6 shares words with snippets referring to both query meanings (i.e., snippets 1, 2, and 3 in Table 1), thus making it even harder for Web clustering engines to group search  
Cornelis Koster† Radboud University Nijmegen Lou Boves‡ Radboud University Nijmegen With the increasing rate of patent application ﬁlings, automated patent classiﬁcation is of rising economic importance. This article investigates how patent classiﬁcation can be improved by using different representations of the patent documents. Using the Linguistic Classiﬁcation System (LCS), we compare the impact of adding statistical phrases (in the form of bigrams) and linguistic phrases (in two different dependency formats) to the standard bag-of-words text representation on a subset of 532,264 English abstracts from the CLEF-IP 2010 corpus. In contrast to previous ﬁndings on classiﬁcation with phrases in the Reuters-21578 data set, for patent classiﬁcation the addition of phrases results in signiﬁcant improvements over the unigram baseline. The best results were achieved by combining all four representations, and the second best by combining unigrams and lemmatized bigrams. This article includes extensive analyses of the class models (a.k.a. class proﬁles) created by the classiﬁers in the LCS framework, to examine which types of phrases are most informative for patent classiﬁcation. It appears that bigrams contribute most to improvements in classiﬁcation accuracy. Similar experiments were performed on subsets of French and German abstracts to investigate the generalizability of these ﬁndings. 1. Introduction Around the world, the patent ﬁling rates in the national patent ofﬁces have been increasing year after year, creating an enormous volume of texts, which patent examiners ∗ Center for Language Studies, PO Box 9103, 6500 HD Nijmegen, the Netherlands. E-mail: e.dhondt@let.ru.nl. ∗∗ Center for Language Studies / Institute for Computing and Information Sciences, PO Box 9103, 6500 HD Nijmegen, the Netherlands. E-mail: s.verberne@let.ru.nl. † Institute for Computing and Information Sciences, PO Box 9010, 6500 HD Nijmegen, the Netherlands. E-mail: kees@cs.ru.nl. ‡ Center for Language Studies, PO Box 9103, 6500 HD Nijmegen, the Netherlands. E-mail: l.boves@let.ru.nl. Submission received: 19 March 2012; revised submission received: 8 August 2012; accepted for publication: 19 September 2012. doi:10.1162/COLI a 00149 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 3  are struggling to manage (Benzineb and Guyot 2011). To speed up the examination process, a patent application needs to be directed to patent examiners specialized in the subﬁeld(s) of that particular patent as quickly as possible (Smith 2002). This preclassiﬁcation is done automatically in most patent ofﬁces, but substantial additional manual labor is still necessary. Furthermore, since 2010, the International Patent Classiﬁcation1 (IPC) is revised every year to keep track of recent developments in the various subdomains. Such a revision is followed by a reclassiﬁcation of portions of the existing patent corpus, which is currently done mainly by hand by the national patent ofﬁces (Held, Schellner, and Ota 2011). Both preclassiﬁcation and reclassiﬁcation could be improved, and a higher consistency of the classiﬁcations of the documents in the patent corpus could be obtained, if more reliable and precise automatic text classiﬁcation algorithms were available (Benzineb and Guyot 2011). Most approaches to text classiﬁcation use the bag-of-words (BOW) text representation, which represents each document by the words that occur in it, irrespective of their ordering in the original document. In the last decades much research has gone into expanding this representation with additional information, such as statistical phrases2 (n-grams) or some forms of syntactic or semantic knowledge. Even though (statistical) phrases are more representative units for classes than single words (Caropreso, Matwin, and Sebastiani 2001), they are so sparsely distributed that they have limited impact during the classiﬁcation process. Therefore, it is not surprising that the best scoring multi-class, multi-label3 classiﬁcation results for the well-known Reuters-21578 data set have been obtained using a BOW representation (Bekkerman and Allan 2003). But the limited contribution of phrases in addition to the BOW-representation does not seem to hold for all classiﬁcation tasks: O¨ zgu¨ r and Gu¨ ngo¨ r (2010) found signiﬁcant differences in the impact of linguistic phrases between short newswire texts (Reuters-21578), scientiﬁc abstracts (NSF), and informal posts in usenet groups (MiniNg): Especially the classiﬁcation of scientiﬁc abstracts could be improved by using phrases as index terms. In a follow-up study, O¨ zgu¨ r and Gu¨ ngo¨ r (2012) found that for the three different data sets, different types of linguistic phrases have most impact. The authors conclude that more formal text types beneﬁt from more complex syntactic dependencies. In this article, we investigate if similar improvements can be found for patent classiﬁcation and, more speciﬁcally, which types of phrases are most effective for this particular task. In this article we investigate the value of phrases for classiﬁcation by comparing the improvements that can be gained from extending the BOW representation with (1) statistical phrases (in the form of bigrams); (2) linguistic phrases originating from the Stanford parser (see Section 3.2.2); (3) aboutness-based4 linguistic phrases from the AEGIR parser (Section 3.2.3); and (4) a combination of all of these. Furthermore, we will investigate the importance of different syntactic relations for the classiﬁcation task,  
Any textbook on computational linguistics today must position itself relative to Jurafsky and Martin (2009), which, by virtue of its depth and comprehensiveness, is the standard introduction and reference for speech and language processing. Readers without substantial background may ﬁnd that book too demanding, however, and then Language and Computers, by Dickinson, Brew, and Meurers, may be a better choice. Roughly, this is how Language and Computers distinguishes itself from Jurafsky and Martin (2009): 1. It does not presuppose any computing background. More speciﬁcally, as stated in the Overview for Instructors, the book assumes “no mathematical or linguistic background beyond normal high-school experience.” The book certainly has the ambition to explain some of the inner workings of the technology, but does so starting from a general level. 2. It is structured according to major applications (treated in six chapters), with the theoretical material being introduced along the way as it is needed. Jurafsky and Martin (2009) is instead structured according to linguistic description levels, starting with words and gradually proceeding through higher levels (albeit with a ﬁnal part on applications). The idea in Language and Computers is that readers without a background in computing will at least be familiar with real-world tasks in which computers deal with language, and will ﬁnd a textbook structured in this way more accessible. It also gives the authors an opportunity to show how the underlying techniques recur across different applications. 3. Its topic is processing of text. There is some discussion in the ﬁrst chapter of the nature and representation of speech, but the applications (with the possible exception of dialogue) are text-oriented. 4. It is introductory, typically aimed at a quarter-length course according to the Overview for Instructors. At 232 pages, it is about one-quarter of the length of Jurafsky and Martin (2009), which has enough material for a full-year course. doi:10.1162/COLI r 00165 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 3  Chapter 1 is a prologue with a refreshingly broad perspective, dealing with how written and spoken language are represented in computers. First, the basic writing systems are described: alphabetical (roughly, one character–one phoneme), syllabic (roughly, one character–one syllable), logographic (roughly, one character–one morpheme/word), and major encodings are addressed (in particular, Unicode). A description of the nature of speech and its representation in waveforms and spectrograms follows, including sections on how to read spectrograms and on language modeling using n-grams. The two latter sections come under the heading of “Under the Hood.” Sections so headed (typically, one or two per chapter) provide digressions into selected technical material that provide bonuses for the interested reader, but which can be omitted without losing the gist of each chapter. Chapter 2 brings up the ﬁrst application, “Writer’s Aids,” which consists of two parts: checking spelling and checking grammar. The ﬁrst part includes an overview of different kinds of spelling errors, and methods for detection and correction of errors. These include a description of dynamic programming for calculating the minimum edit distance between a misspelled word and a set of candidate corrections. The second part deals with grammar checking. To set the scene for this, there is ﬁrst an exposition of context-free grammar for the purpose of specifying the norm (the well-formed sentences) of the language. (The possibility of enriching the formalism with features is mentioned later in the same chapter.) It is then stated that “[w]hen a parser fails to parse a sentence, we have an indication that something is grammatically wrong with the sentence” (page 58). At the same time, it is recognized that the most one can do is to build grammar fragments. So what about undergeneration, and the challenges of maintaining a large, manually encoded grammar? Instead of elaborating on this, the chapter goes on with a brief description of methods for correcting errors. These include relaxation-based techniques that discharge grammar rules, and special-purpose rules or n-grams that trigger directly on erroneous sequences of words. The reader is left wondering what one could expect from a grammar fragment. Although this may not be relevant to grammar checking, it would have been instructive to have a mention somewhere of the possibility of augmenting context-free rules or other formalisms with probabilities, and of how this has affected wide-coverage parsing (Clark and Curran 2007). Chapter 3 deals with language tutoring systems and how to make them linguistically aware. To this end, the concepts (but not the inner workings) of tokenization, part-of-speech tagging, and syntactic parsing are described. An example language tutoring system for learners of Portuguese is then addressed. Compared with traditional workbook exercises, the system gives immediate feedback on orthographic, syntactic, and semantic errors, and also contains audio. The topic of Chapter 4, “Searching,” is mainly related to information retrieval. Most of the chapter is contained in two comprehensive sections that deal with searching in unstructured data (typically the Web) and semi-structured data (such as Wikipedia). The former section contains a relatively detailed description of search engines and PageRank, as well as an overview of HTML. Evaluation of search results is mentioned, but the measures are described in more detail in the next chapter. The section on semistructured data contains a description of regular expressions, Unix grep, and ﬁnitestate automata. The chapter also contains brief sections on searching of structured data (databases, using Boolean expressions) and of text corpora (mainly discussing corpus annotation). Given that many of the readers of this book will be linguists, it is perhaps surprising that the book has so little material related to corpus linguistics, but it could be argued that this topic would require a text of its own (and that it is not an “application”). 778  Book Review Anyway, the “Further Reading” section of the chapter includes some good suggestions on this topic. Chapter 5 brings up document classiﬁcation, and begins with an overview of concepts in machine learning. Then there is a detailed description of how to measure success in classiﬁcation, with precision/recall, true/false positives/negatives, and the related measures of sensitivity and speciﬁcity used in medicine. After this, two examples of document classiﬁers are described in some detail: naive Bayes and the perceptron. Finally, there is a short section on sentiment analysis. This chapter has a good balance between applications-oriented and theoretical material, and gives a good grasp of each of them. Chapter 6 deals with dialogue systems. After some motivation, the chapter examines in detail an example spoken dialogue transcript from the Carnegie Mellon University Let’s Go! Bus Information System, followed by a thorough description of dialogue moves, speech acts, and Grice’s conversational maxims. After this ambitious background, one would expect some (even superﬁcial) material on the anatomy of a dialogue system founded on these principles, but instead there is a detailed description of Eliza. The motivation is that “[Let’s Go!] is . . . too complicated to be explained fully in this textbook” (page 167). The ambition to explain a working system in full is admirable, but seems misdirected here. Why not try to work out the basic principles of a simpler question-answering system? As it stands, the applications-oriented material (Eliza) is not connected to the theoretical parts of the chapter. It is also potentially misleading when the authors say: “[Eliza] works reasonably well using simple means, and this can be useful if your application calls for a straightforward but limited way of creating the illusion of an intelligent being at the other end of the wire” (page 170). Here, the authors must be referring to chatbots, but for applications in general, it would have been valuable to have a pointer to principles of user interface design, such as trying to maintain consistency (Cohen, Giangola, and Balogh 2004). On a different note, this chapter would proﬁt from picking up the thread on speech from Chapter 1. For example, it might be instructive to have an explanation of word-error rate and the degradation of input typically caused by a speech recognizer. This would also give an opportunity to elaborate on one of the problems mentioned at the outset of the chapter: “[f]ixing confusions and misunderstandings before they cause the conversation to break down” (page 154). Chapter 7 brings up the ﬁnal application, machine translation. This is a chapter that gives a good grasp of both applications-oriented and theoretical material. Examplebased translation and translation memories are brieﬂy discussed, and, after some background, word alignment, IBM Model 1, the noisy channel model, and phrase-based statistical translation are explained. Commercial translation is also addressed. In connection with the translation triangle, the possibility of an interlingua is discussed, the conclusion being that “[d]espite great efforts, nobody has ever managed to design or build a suitable interlingua” (page 189) and “we probably cannot have one any time soon” (page 190). This is true for a universal interlingua in the sense of a fully abstract language-independent representation, but what might be more relevant is domainspeciﬁc interlinguas, which have proved to be feasible in recent years (Ranta 2011). Interlinguas are also mentioned later in the chapter, at the end of “Under the Hood 12” (page 205) on phrase-based statistical translation, where it is stated that “[t]here is certainly no interlingua in sight.” Although this is different, Google Translate actually uses English as an interlingua for a large majority of its language pairs (Ranta 2010). Surely, the reason for this is that Google typically has much more parallel data for language pairs where English is one of the members than for language pairs where this is not the case. 
Keh-Yih Su† Behavior Design Corporation In this article, an integrated model is derived that jointly identiﬁes and aligns bilingual named entities (NEs) between Chinese and English. The model is motivated by the following observations: (1) whether an NE is translated semantically or phonetically depends greatly on its entity type, (2) entities within an aligned pair should share the same type, and (3) the initially detected NEs can act as anchors and provide further information while selecting NE candidates. Based on these observations, this article proposes a translation mode ratio feature (deﬁned as the proportion of NE internal tokens that are semantically translated), enforces an entity type consistency constraint, and utilizes additional new NE likelihoods (based on the initially detected NE anchors). Experiments show that this novel method signiﬁcantly outperforms the baseline. The typeinsensitive F-score of identiﬁed NE pairs increases from 78.4% to 88.0% (12.2% relative improvement) in our Chinese–English NE alignment task, and the type-sensitive F-score increases from 68.4% to 83.0% (21.3% relative improvement). Furthermore, the proposed model demonstrates its robustness when it is tested across different domains. Finally, when semi-supervised learning is conducted to train the adopted English NE recognition model, the proposed model also signiﬁcantly boosts the English NE recognition type-sensitive F-score. ∗ No. 95, Zhongguancun East Road, Haidian District, Beijing 100190, China. E-mail: chenyf@nlpr.ia.ac.cn. ∗∗ No. 95, Zhongguancun East Road, Haidian District, Beijing 100190, China. E-mail: cqzong@nlpr.ia.ac.cn. † Hsinchu, Taiwan. E-mail: bdc.kysu@gmail.com. Submission received: 9 October 2010; revised submission received: 15 February 2012; accepted for publication: 27 March 2012. doi:10.1162/COLI a 00122 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law.  Computational Linguistics  Volume 39, Number 2  1. Introduction Named entities (NEs), especially person names (PER), location names (LOC), and organization names (ORG), deliver essential context and meaning in human languages. Therefore, NE translation plays a critical role in trans-lingual language processing tasks, such as machine translation (MT) and cross-lingual information retrieval. To learn NE translation knowledge, bilingual NE alignment (which links source NEs and target NEs to generate desired NE pairs) is the ﬁrst step in producing the NE translation table (which can then be used to train the NE translation model). Furthermore, with additional alignment constraints from the other language, the alignment module can also reﬁne those initially recognized NEs, and thus can be adopted to conduct semisupervised learning to learn monolingual NE recognition models from a large untagged bilingual corpus. Because NE alignment can only be conducted after its associated NEs have been identiﬁed, the NE recognition errors propagate into the alignment stage. The typeinsensitive inclusion rate1 of the initial recognition stage thus signiﬁcantly limits the ﬁnal alignment performance. One way to alleviate this error propagation problem is to jointly perform NE recognition and alignment. Such a combined approach is usually infeasible, however, due to the high computational cost of evaluating alignment scores for a large number2 of NE pair candidates. In order to make the problem computationally tractable, a sequential approach is usually used to ﬁrst identify NEs and then align them. Two such kinds of sequential strategies that alleviate the error propagation problem have been proposed. The ﬁrst strategy, named asymmetry alignment (Al-Onaizan and Knight 2002; Moore 2003; Feng, Lv, and Zhou 2004; Lee, Chang, and Jang 2006), identiﬁes NEs only on the source side and then ﬁnds their corresponding NEs on the target side. Although this approach avoids the NE recognition errors resulting from the target side, which would otherwise be brought into the alignment process, the NE recognition errors from the source side continue to affect alignment. To further reduce the errors from the source side, the second strategy, denoted symmetry alignment (Huang, Vogel, and Waibel 2003), expands the NE candidate sets in both languages before conducting the alignment. This is achieved by using the original results as anchors, and enlarging or shrinking the boundaries of the anchors to generate new candidates. This strategy fails to work if the NE anchor has already been missed in the initial NE recognition stage, however. In our data set (1,000 Chinese– English sentence pairs randomly selected from the Chinese News Translation Text corpus [LDC2005T06]), this strategy signiﬁcantly improves the type-insensitive NE pair inclusion rate from 83.9% to 96.1%;3 in the meantime, the type-insensitive Chinese NE (CNE) recognition inclusion rate rises from 88.7% to 95.9%, and that of English NE (ENE) from 92.8% to 97.2%. This strategy is thus adopted in this article. Although the symmetric expansion strategy has substantially alleviated the problem of error propagation, the ﬁnal alignment accuracy, in terms of type-sensitive F-score 
The most widely adopted approaches for evaluation of summary content follow some protocol for comparing a summary with gold-standard human summaries, which are traditionally called model summaries. This evaluation paradigm falls short when human summaries are not available and becomes less accurate when only a single model is available. We propose three novel evaluation techniques. Two of them are model-free and do not rely on a gold standard for the assessment. The third technique improves standard automatic evaluations by expanding the set of available model summaries with chosen system summaries. We show that quantifying the similarity between the source text and its summary with appropriately chosen measures produces summary scores which replicate human assessments accurately. We also explore ways of increasing evaluation quality when only one human model summary is available as a gold standard. We introduce pseudomodels, which are system summaries deemed to contain good content according to automatic evaluation. Combining the pseudomodels with the single human model to form the gold-standard leads to higher correlations with human judgments compared to using only the one available model. Finally, we explore the feasibility of another measure—similarity between a system summary and the pool of all other system summaries for the same input. This method of comparison with the consensus of systems produces impressively accurate rankings of system summaries, achieving correlation with human rankings above 0.9. 1. Introduction In this work, we present evaluation metrics for summary content which make use of little or no human involvement. Evaluation methods such as manual pyramid scores (Nenkova, Passonneau, and McKeown 2007) and automatic ROUGE scores (Lin and Hovy 2003) rely on multiple human summaries as a gold standard (model) against which they compare a summary to assess how informative the candidate summary is. It is desirable that evaluation of similar quality be done quickly and cheaply ∗ E-mail: lannie@seas.upenn.edu. ∗∗ University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut St., Philadelphia, PA 19104. E-mail: nenkova@seas.upenn.edu. Submission received: 18 June 2011; revised submission received: 23 March 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00123 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 2  on non-standard test sets that have few or no human summaries, or on large test sets for which creating human model summaries is infeasible. In our work, we aim to identify indicators of summary content quality that do not make use of human summaries but can replicate scores based on comparison with a gold standard very accurately. Such indicators would need to be easily computable from existing resources and to provide rankings of systems that agree with rankings obtained through human judgments. There have been some early proposals for alternative methods. Donaway, Drummey, and Mather (2000) propose that a comparison of the source text with a summary can tell us how good the summary is. A summary that has higher similarity with the source text can be considered better than one with lower similarity. Radev and Tam (2003) perform a large scale evaluation with thousands of test documents. Their work is set up in a search engine scenario. They ﬁrst rank the test documents using the search engine. Then they perform the same experiment now substituting the summaries from one system in place of the original documents. The system whose summaries have the most similar ranking as that generated for the full documents is considered the best system because not much information loss is introduced by the summarization process. But these methods did not gain much popularity and their performance was never compared to human evaluations. Part of the reason is that only in the last decade have several large data sets with system summaries and their ratings from human judges become available for performing such studies. Our work is the ﬁrst to provide a comprehensive report of the strengths of such approaches and we show that human ratings can be reproduced by these fully automatic metrics with high accuracy. Our results are based on data for multi-document news summarization. The key insights of our approach can be summarized as follows: Input–summary similarity: Good summaries are representative of the input and so one would expect that the more similar a summary is to the input, the better its content. Identifying a suitable input–summary similarity metric will provide a means for fully automatic evaluation of summaries. We present a quantitative analysis of this hypothesis and show that input–summary similarity is highly predictive of scores assigned by humans for the summaries. The choice of an appropriate metric to measure similarity is critical, however, and we show that information-theoretic measures turn out to be the most powerful for this task (Section 4). Addition of pseudomodels: Having a larger number of model summaries has been shown to give more stable evaluation results, but for some data sets only a single model summary is available. We test the utility of pseudomodels, which are system summaries that are chosen to be added to the human summary pool and that are used as additional models. We ﬁnd that augmenting the gold standard with pseudomodels helps obtain better correlations with human judgments than if a single model is used (Section 5). System summaries as models: Most current summarization systems perform content selection reasonably well. We examine an approach to evaluation that exploits system output and considers all system summaries for a given input as a gold standard (Section 6). We ﬁnd that similarity between a summary and such a gold standard constitutes a powerful automatic evaluation measure. The correlation between this measure and human evaluations is over 0.9. We analyze a number of similarity metrics to identify the ones that perform best for automatic evaluation. The tool we developed, SIMetrix (Summary Input similarity  268  Louis and Nenkova  Automatic Content Evaluation  Metrics), is freely available.1 We test these resource-poor approaches to predict summary content scores assigned by human assessors. We evaluate the results on data from the Text Analysis Conferences.2 We ﬁnd that our automatic methods to estimate summary quality are highly predictive of human judgments. Our best result is 0.93 correlation with human rankings using no model summaries and this is on par with automatic evaluation methods that do use human summaries. Our study provides some direction towards alternative methods of evaluation on non-standard test sets. The goal of our methods is to aid system development and tuning on new, especially large, data sets using little resources. Our metrics complement but are not intended to replace existing manual and automatic approaches to evaluation wherein the latter’s strength and reliability are important for high conﬁdence evaluations. Some of our ﬁndings are also relevant for system development as we identify desirable properties of automatic summaries that can be computed from the input (see Section 4). Our results are also strongly suggestive that system combination has the potential for improving current summarization systems (Section 6). We start out with an outline of existing evaluation methods and the potential shortcomings of these approaches which we wish to address. 2. Current Content Evaluation Methods Summary quality is deﬁned by two key aspects—content and linguistic quality. A good summary should contain the most important content in the input and also structure the content and present it as well-written text. Several methods have been proposed for evaluating system-produced summaries; some only assess content, others only linguistic quality, and some combine assessment of both. Some of these approaches are manual and others can be performed automatically. In our work, we consider the problem of automatic evaluation of content quality. To establish the context for our work, we provide an overview of current content evaluation methods used at the annual evaluations run by NIST. The Text Analysis Conference (TAC, previously called the Document Understanding Conference [DUC]3) conducts large scale evaluation of automatic systems on different summarization tasks. These conferences have been held every year since 2001 and the test sets and evaluation methods adopted by TAC/DUC have become the standard for reporting results in publications. TAC has employed a range of manual and automatic metrics over the years. Manual evaluations of the systems are performed at NIST by trained assessors. The assessors score the summaries either a) by comparing with a gold-standard summary written by humans, or b) by providing a direct rating on a scale (1 to 5 or 1 to 10). The human summaries against which other summaries are compared are interchangeably called models, gold standards, and references. Within TAC, they are typically called models.  
Anna Korhonen∗ University of Cambridge Metaphor is highly frequent in language, which makes its computational processing indispensable for real-world NLP applications addressing semantic tasks. Previous approaches to metaphor modeling rely on task-speciﬁc hand-coded knowledge and operate on a limited domain or a subset of phenomena. We present the ﬁrst integrated open-domain statistical model of metaphor processing in unrestricted text. Our method ﬁrst identiﬁes metaphorical expressions in running text and then paraphrases them with their literal paraphrases. Such a text-to-text model of metaphor interpretation is compatible with other NLP applications that can beneﬁt from metaphor resolution. Our approach is minimally supervised, relies on the state-of-the-art parsing and lexical acquisition technologies (distributional clustering and selectional preference induction), and operates with a high accuracy. 1. Introduction Our production and comprehension of language is a multi-layered computational process. Humans carry out high-level semantic tasks effortlessly by subconsciously using a vast inventory of complex linguistic devices, while simultaneously integrating their background knowledge, to reason about reality. An ideal computational model of language understanding would also be capable of performing such high-level semantic tasks. With the rapid advances in statistical natural language processing (NLP) and computational lexical semantics, increasingly complex semantic tasks can now be addressed. Tasks that have received much attention so far include, for example, word sense disambiguation (WSD), supervised and unsupervised lexical classiﬁcation, selectional preference induction, and semantic role labeling. In this article, we take a step further and show that state-of-the-art statistical NLP and computational lexical semantic techniques can be used to successfully model complex meaning transfers, such as metaphor. ∗ Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue, Cambridge CB3 0FD, UK. E-mail: {Ekaterina.Shutova, Simone.Teufel, Anna.Korhonen}@cl.cam.ac.uk. Submission received: 28 July 2011; revised submission received: 21 April 2012; accepted for publication: 31 May 2012. doi:10.1162/COLI a 00124 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 2  Metaphors arise when one concept is viewed in terms of the properties of another. Humans often use metaphor to describe abstract concepts through reference to more concrete or physical experiences. Some examples of metaphor include the following. (1) How can I kill a process? (Martin 1988) (2) Hillary brushed aside the accusations. (3) I invested myself fully in this research. (4) And then my heart with pleasure ﬁlls, And dances with the daffodils. (“I wandered lonely as a cloud,” William Wordsworth, 1804) Metaphorical expressions may take a great variety of forms, ranging from conventional metaphors, which we produce and comprehend every day, for example, those in Examples (1)–(3), to poetic and novel ones, such as Example (4). In metaphorical expressions, seemingly unrelated features of one concept are attributed to another concept. In Example (1), a computational process is viewed as something alive and, therefore, its forced termination is associated with the act of killing. In Example (2) Hillary is not literally cleaning the space by sweeping accusations. Instead, the accusations lose their validity in that situation, in other words Hillary rejects them. The verbs brush aside and reject both entail the resulting disappearance of their object, which is the shared salient property that makes it possible for this analogy to be lexically expressed as a metaphor. Characteristic of all areas of human activity (from poetic to ordinary to scientiﬁc) and thus of all types of discourse, metaphor becomes an important problem for NLP. As Shutova and Teufel (2010) have shown in an empirical study, the use of conventional metaphor is ubiquitous in natural language text (according to their data, on average every third sentence in general-domain text contains a metaphorical expression). This makes metaphor processing essential for automatic text understanding. For example, an NLP application which is unaware that a “leaked report” is a “disclosed report” and not, for example, a “wet report,” would fail further semantic processing of the piece of discourse in which this phrase appears. A system capable of recognizing and interpreting metaphorical expressions in unrestricted text would become an invaluable component of any real-world NLP application that needs to access semantics (e.g., information retrieval [IR], machine translation [MT], question answering [QA], information extraction [IE], and opinion mining). So far, these applications have not used any metaphor processing techniques and thus often fail to interpret metaphorical data correctly. Consider an example from MT. Figure 1 shows metaphor translation from English into Russian by a state-of-the-art statistical MT system (Google Translate1). For both sentences the MT system produces literal translations of metaphorical terms in English, rather than their literal interpretations. This results in otherwise grammatical sentences being semantically infelicitous, poorly formed, and barely understandable to a native speaker of Russian. The meaning of stir in Figure 1 (1) and spill in Figure 1 (2) would normally be realized in Russian only via their literal interpretation in the given context (provoke and tell), as shown under CORRECT TRANSLATION in Figure 1. A metaphor processing component could help to  
Syntactic representations based on word-to-word dependencies have a long-standing tradition in descriptive linguistics. Since the seminal work of Tesnie`re (1959), they have become the basis for several linguistic theories, such as Functional Generative Description (Sgall, Hajicˇova´, and Panevova´ 1986), Meaning–Text Theory (Mel’cˇuk 1988), and Word Grammar (Hudson 2007). In recent years they have also been used for a wide range of practical applications, such as information extraction, machine translation, and question answering. We ascribe the widespread interest in dependency structures to their intuitive appeal, their conceptual simplicity, and in particular to the availability of accurate and efﬁcient dependency parsers for a wide range of languages (Buchholz and Marsi 2006; Nivre et al. 2007). Although there exist both a considerable practical interest and an extensive linguistic literature, dependency syntax has remained something of an island from a formal point of view. In particular, there are relatively few results that bridge between dependency syntax and other traditions, such as phrase structure or categorial syntax. ∗ Department of Linguistics and Philology, Box 635, 751 26 Uppsala, Sweden. E-mail: marco.kuhlmann@lingfil.uu.se. Submission received: 17 December 2009; revised submission received: 3 April 2012; accepted for publication: 24 May 2012. doi:10.1162/COLI a 00125 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 2  Figure 1 Nested dependencies and cross–serial dependencies.  This makes it hard to gauge the similarities and differences between the paradigms, and hampers the exchange of linguistic resources and computational methods. An overarching goal of this article is to bring dependency grammar closer to the mainland of formal study. One of the few bridging results for dependency grammar is thanks to Gaifman (1965), who studied a formalism that we will refer to as Hays–Gaifman grammar, and proved it to be weakly equivalent to context-free phrase structure grammar. Although this result is of fundamental importance from a theoretical point of view, its practical usefulness is limited. In particular, Hays–Gaifman grammar is restricted to projective dependency structures, which is similar to the familiar restriction to contiguous constituents. Yet, non-projective dependencies naturally arise in the analysis of natural language. One classic example of this is the phenomenon of cross–serial dependencies in Dutch. In this language, the nominal arguments of verbs that also select an inﬁnitival complement occur in the same order as the verbs themselves:  (i) dat Jan1 Piet2 Marie3 zag1 helpen2 lezen3 that Jan Piet Marie saw help read ‘that Jan saw Piet help Marie read’  (Dutch)  In German, the order of the nominal arguments instead inverts the verb order:  (ii) dass Jan1 Piet2 Marie3 lesen3 helfen2 sah1 that Jan Piet Marie read help saw  (German)  Figure 1 shows dependency trees for the two examples.1 The German linearization gives rise to a projective structure, where the verb–argument dependencies are nested within each other, whereas the Dutch linearization induces a non-projective structure with crossing edges. To account for such structures we need to turn to formalisms more expressive than Hays–Gaifman grammars. In this article we present a formalism for non-projective dependency grammar based on linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987; Weir 1988). This framework was introduced to facilitate the comparison of various  
Dan Klein† University of California, Berkeley Suppose we want to build a system that answers a natural language question by representing its semantics as a logical form and computing the answer given a structured database of facts. The core part of such a system is the semantic parser that maps questions to logical forms. Semantic parsers are typically trained from examples of questions annotated with their target logical forms, but this type of annotation is expensive. Our goal is to instead learn a semantic parser from question–answer pairs, where the logical form is modeled as a latent variable. We develop a new semantic formalism, dependency-based compositional semantics (DCS) and deﬁne a log-linear distribution over DCS logical forms. The model parameters are estimated using a simple procedure that alternates between beam search and numerical optimization. On two standard semantic parsing benchmarks, we show that our system obtains comparable accuracies to even state-of-the-art systems that do require annotated logical forms. 1. Introduction One of the major challenges in natural language processing (NLP) is building systems that both handle complex linguistic phenomena and require minimal human effort. The difﬁculty of achieving both criteria is particularly evident in training semantic parsers, where annotating linguistic expressions with their associated logical forms is expensive but until recently, seemingly unavoidable. Advances in learning latent-variable models, however, have made it possible to progressively reduce the amount of supervision ∗ Computer Science Division, University of California, Berkeley, CA 94720, USA. E-mail: pliang@cs.stanford.edu. ∗∗ Computer Science Division and Department of Statistics, University of California, Berkeley, CA 94720, USA. E-mail: jordan@cs.berkeley.edu. † Computer Science Division, University of California, Berkeley, CA 94720, USA. E-mail: klein@cs.berkeley.edu. Submission received: 12 September 2011; revised submission received: 19 February 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00127 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law.  Computational Linguistics  Volume 39, Number 2  required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan et al. 2009; Liang, Jordan, and Klein 2009; Clarke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as LUNAR (Woods, Kaplan, and Webber 1972), CHAT-80 (Warren and Pereira 1982), and many others (see Androutsopoulos, Ritchie, and Thanisch [1995] for an overview). We believe NLIDBs provide an appropriate starting point for semantic parsing because they lead directly to practical systems, and they allow us to temporarily sidestep intractable philosophical questions on how to represent meaning in general. Early NLIDBs were quite successful in their respective limited domains, but because these systems were constructed from manually built rules, they became difﬁcult to scale up, both to other domains and to more complex utterances. In response, against the backdrop of a statistical revolution in NLP during the 1990s, researchers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the CHILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcient in some formal language, which drastically reduces the size of the annotator pool, dampening any hope of acquiring enough data to fulﬁll the vision of learning highly accurate systems. In response to these concerns, researchers have recently begun to explore the possibility of learning a semantic parser without any annotated logical forms (Clarke et al.  Figure 1 The concrete objective: A system that answers natural language questions given a structured database of facts. An example is shown in the domain of U.S. geography. 390  Liang, Jordan, and Klein  Learning Dependency-Based Compositional Semantics  Figure 2 Our statistical methodology consists of two steps: (i) semantic parsing (p(z | x; θ)): an utterance x is mapped to a logical form z by drawing from a log-linear distribution parametrized by a vector θ; and (ii) evaluation ([[z]]w): the logical form z is evaluated with respect to the world w (database of facts) to deterministically produce an answer y. The ﬁgure also shows an example conﬁguration of the variables around the graphical model. Logical forms z are represented as labeled trees. During learning, we are given w and (x, y) pairs (shaded nodes) and try to infer the latent logical forms z and parameters θ. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011; Liang, Jordan, and Klein 2011). It is in this vein that we develop our present work. Speciﬁcally, given a set of (x, y) example pairs, where x is an utterance (e.g., a question) and y is the corresponding answer, we wish to learn a mapping from x to y. What makes this mapping particularly interesting is that it passes through a latent logical form z, which is necessary to capture the semantic complexities of natural language. Also note that whereas the logical form z was the end goal in much of earlier work on semantic parsing, for us it is just an intermediate variable—a means towards an end. Figure 2 shows the graphical model which captures the learning setting we just described: The question x, answer y, and world/database w are all observed. We want to infer the logical forms z and the parameters θ of the semantic parser, which are unknown quantities. Although liberating ourselves from annotated logical forms reduces cost, it does increase the difﬁculty of the learning problem. The core challenge here is program induction: On each example (x, y), we need to efﬁciently search over the exponential space of possible logical forms (programs) z and ﬁnd ones that produce the target answer y, a computationally daunting task. There is also a statistical challenge: How do we parametrize the mapping from utterance x to logical form z so that it can be learned from only the indirect signal y? To address these two challenges, we must ﬁrst discuss the issue of semantic representation. There are two basic questions here: (i) what 391  Computational Linguistics  Volume 39, Number 2  should the formal language for the logical forms z be, and (ii) what are the compositional mechanisms for constructing those logical forms? The semantic parsing literature has considered many different formal languages for representing logical forms, including SQL (Giordani and Moschitti 2009), Prolog (Zelle and Mooney 1996; Tang and Mooney 2001), a simple functional query language called FunQL (Kate, Wong, and Mooney 2005), and lambda calculus (Zettlemoyer and Collins 2005), just to name a few. The construction mechanisms are equally diverse, including synchronous grammars (Wong and Mooney 2007), hybrid trees (Lu et al. 2008), Combinatory Categorial Grammars (CCG) (Zettlemoyer and Collins 2005), and shiftreduce derivations (Zelle and Mooney 1996). It is worth pointing out that the choice of formal language and the construction mechanism are decisions which are really more orthogonal than is often assumed—the former is concerned with what the logical forms look like; the latter, with how to generate a set of possible logical forms compositionally given an utterance. (How to score these logical forms is yet another dimension.) Existing systems are rarely based on the joint design of the formal language and the construction mechanism; one or the other is often chosen for convenience from existing implementations. For example, Prolog and SQL have often been chosen as formal languages for convenience in end applications, but they were not designed for representing the semantics of natural language, and, as a result, the construction mechanism that bridges the gap between natural language and formal language is generally complex and difﬁcult to learn. CCG (Steedman 2000) is quite popular in computational linguistics (for example, see Bos et al. [2004] and Zettlemoyer and Collins [2005]). In CCG, logical forms are constructed compositionally using a small handful of combinators (function application, function composition, and type raising). For a wide range of canonical examples, CCG produces elegant, streamlined analyses, but its success really depends on having a good, clean lexicon. During learning, there is often a great amount of uncertainty over the lexical entries, which makes CCG more cumbersome. Furthermore, in real-world applications, we would like to handle disﬂuent utterances, and this further strains CCG by demanding either extra type-raising rules and disharmonic combinators (Zettlemoyer and Collins 2007) or a proliferation of redundant lexical entries for each word (Kwiatkowski et al. 2010). To cope with the challenging demands of program induction, we break away from tradition in favor of a new formal language and construction mechanism, which we call dependency-based compositional semantics (DCS). The guiding principle behind DCS is to provide a simple and intuitive framework for constructing and representing logical forms. Logical forms in DCS are tree structures called DCS trees. The motivation is twofold: (i) DCS trees are meant to parallel syntactic dependency trees, which facilitates parsing; and (ii) a DCS tree essentially encodes a constraint satisfaction problem, which can be solved efﬁciently using dynamic programming to obtain the denotation of a DCS tree. In addition, DCS provides a mark–execute construct, which provides a uniform way of dealing with scope variation, a major source of trouble in any semantic formalism. The construction mechanism in DCS is a generalization of labeled dependency parsing, which leads to simple and natural algorithms. To a linguist, DCS might appear unorthodox, but it is important to keep in mind that our primary goal is effective program induction, not necessarily to model new linguistic phenomena in the tradition of formal semantics. Armed with our new semantic formalism, DCS, we then deﬁne a discriminative probabilistic model, which is depicted in Figure 2. The semantic parser is a log-linear distribution over DCS trees z given an utterance x. Notably, z is unobserved, and we instead observe only the answer y, which is obtained by evaluating z on a world/database  392  Liang, Jordan, and Klein  Learning Dependency-Based Compositional Semantics  w. There are an exponential number of possible trees z, and usually dynamic programming can be used to efﬁciently search over trees. However, in our learning setting (independent of the semantic formalism), we must enforce the global constraint that z produces y. This makes dynamic programming infeasible, so we use beam search (though dynamic programming is still used to compute the denotation of a ﬁxed DCS tree). We estimate the model parameters with a simple procedure that alternates between beam search and optimizing a likelihood objective restricted to those beams. This yields a natural bootstrapping procedure in which learning and search are integrated. We evaluated our DCS-based approach on two standard benchmarks, GEO, a U.S. geography domain (Zelle and Mooney 1996), and JOBS, a job queries domain (Tang and Mooney 2001). On GEO, we found that our system signiﬁcantly outperforms previous work that also learns from answers instead of logical forms (Clarke et al. 2010). What is perhaps a more signiﬁcant result is that our system obtains comparable accuracies to state-of-the-art systems that do rely on annotated logical forms. This demonstrates the viability of training accurate systems with much less supervision than before. The rest of this article is organized as follows: Section 2 introduces DCS, our new semantic formalism. Section 3 presents our probabilistic model and learning algorithm. Section 4 provides an empirical evaluation of our methods. Section 5 situates this work in a broader context, and Section 6 concludes. 2. Representation In this section, we present the main conceptual contribution of this work, dependencybased compositional semantics (DCS), using the U.S. geography domain (Zelle and Mooney 1996) as a running example. To do this, we need to deﬁne the syntax and semantics of the formal language. The syntax is deﬁned in Section 2.2 and is quite straightforward: The logical forms in the formal language are simply trees, which we call DCS trees. In Section 2.3, we give a type-theoretic deﬁnition of worlds (also known as databases or models) with respect to which we can deﬁne the semantics of DCS trees. The semantics, which is the heart of this article, contains two main ideas: (i) using trees to represent logical forms as constraint satisfaction problems or extensions thereof, and (ii) dealing with cases when syntactic and semantic scope diverge (e.g., for generalized quantiﬁcation and superlative constructions) using a new construct which we call mark–execute. We start in Section 2.4 by introducing the semantics of a basic version of DCS which focuses only on (i) and then extend it to the full version (Section 2.5) to account for (ii). Finally, having fully speciﬁed the formal language, we describe a construction mechanism for mapping a natural language utterance to a set of candidate DCS trees (Section 2.6). 2.1 Notation Operations on tuples will play a prominent role in this article. For a sequence1 v = (v1, . . . , vk), we use |v| = k to denote the length of the sequence. For two sequences u and v, we use u + v = (u1, . . . , u|u|, v1, . . . , v|v|) to denote their concatenation.  
In computational modeling of natural language phenomena, there are at least three modes of research. The currently dominant statistical paradigm typically prioritizes instance coverage: Data-driven methods seek to use as much information observed in data as possible in order to generalize linguistic analyses to unseen instances. A second approach prioritizes detailed description of grammatical phenomena, that is, forming and defending theories with a focus on a small number of instances. A third approach might be called integrative: Rather than addressing phenomena in isolation, different approaches are brought together to address multiple challenges in a uniﬁed framework, and the behavior of the system is demonstrated with a small number of instances. Design Patterns in Fluid Construction Grammar (DPFCG) exempliﬁes the third approach, introducing a linguistic formalism called Fluid Construction Grammar (FCG) that addresses parsing, production, and learning in a single computational framework. The book emphasizes grammar-engineering, following broad-coverage descriptive paradigms that can be traced back to Generalized Phrase Structure Grammar (GPSG) (Gazdar et al. 1985), Lexical Functional Grammar (LFG) (Bresnan 2000), HeadDriven Phrase-Structure Grammar (HPSG) (Sag and Wasow 1999), and Combinatory Categorial Grammar (CCG) (Steedman 1996). In all of these cases, a formal metaframework allows computational linguists to formalize their hypotheses and intuitions about a language’s grammatical behavior and then explore how these representational choices affect the processing of natural language utterances. Many of the aforementioned approaches have engendered large-scale platforms that can be used and reused to provide formal description of grammars for different languages, such as Par-Gram for LFG (Butt et al. 2002) and the LinGO Grammar Matrix for HPSG (Bender, Flickinger, and Oepen 2002). FCG offers a similar grammar engineering framework that follows the principles of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG treats constructions as the basic units of grammatical organization in language. The constructions are viewed as learned associations between form (e.g., sounds, morphemes, syntactic phrases) and function (semantics, pragmatics, discourse meaning, etc.). CxG does not impose a strict separation between lexicon and grammar—indeed, it is perhaps best known as treating semi-productive idioms like “the X-er, the Y-er” and “X let alone Y” on equal footing with lexemes and “core” syntactic patterns (Fillmore, doi:10.1162/COLI r 00154 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 2  Kay, and O’Connor 1988; Kay and Fillmore 1999). FCG, like other CxG formalisms— namely, Embodied Construction Grammar (Bergen and Chang 2005; Feldman, Dodge, and Bryant 2009) and Sign-Based Construction Grammar (Boas and Sag 2012)—is uniﬁcation-based.1 The studies in this book describe constructions and how they can be combined in order to model natural language interpretation or generation as feature structure uniﬁcation in a general search procedure. The book has ﬁve parts, covering the groundwork, basic linguistic applications, processing matters, advanced case studies, and, ﬁnally, features of FCG that make it ﬂuid and robust. Each chapter identiﬁes general strategies (design patterns) that might merit reuse in new FCG grammars, or perhaps in other computational frameworks. Part I: Introduction lays the groundwork for the rest of the book. “Introducing Fluid Construction Grammar” (by Luc Steels) presents the aims of the FCG formalism. FCG was designed as a framework for describing linguistic units (constructions—their form and meaning), with an emphasis on language variation and evolution (“ﬂuidity”). The constructionist approach to language is described and the argument for applying it to study language variation and change is defended. Psychological validity is explicitly ruled out as a modeling goal (“The emphasis is on getting working systems, and this is difﬁcult enough”; page 4). The architects of FCG set out to include both sides of the processing coin, however—parsing (interpretation) and production (generation). The concept of search in processing is emphasized, though some of the explanations of processing steps are too abstract for the reader to comprehend at this point. A further desideratum—robustness to noisy input containing disﬂuencies, fragments, and errors—is given as motivating a constructionist approach. The next chapter, “A First Encounter with Fluid Construction Grammar” (by Steels), describes the mechanisms of FCG in detail. In FCG, a working analysis hypothesized in processing is known as transient structure; the transduction of form to meaning (and vice versa) selects a sequence of constructions that apply to the transient structure to gradually expand it until reaching a ﬁnal analysis. Identifying constructions that may apply to a transient structure presents a non-trivial search problem, also addressed by the architects of FCG. The sheer number of technical details make this chapter somewhat overwhelming. Most of the chapter is devoted to the low-level feature structures and the operations manipulating them. Templates—a practical means of avoiding boilerplate code when deﬁning constructions—are then introduced, and do most of the heavy lifting in the rest of the book. In Part II: Grammatical Structures, we begin to see how constructions are deﬁned in practice. “A Design Pattern for Phrasal Constructions” (by Steels) illustrates how constructions are used to describe the combination of multiple units into higher-level, typed phrases. Skeletal constructions compose with smaller units to form hierarchical structures (essentially similar to the Immediate Constituents Analysis of Bloomﬁeld [1933] and follow-up work in structuralist linguistics [Harris 1946]), and a range of additional constructions impose form (e.g., ordering) constraints and add new meaning to the newly created phrases. This chapter is of a tutorial nature, illustrating the step-by-step application of four kinds of noun phrase constructions to expand transient structures in processing. Over twenty templates are introduced in this chapter; they encapsulate design patterns dealing with hierarchical structure, agreement, and feature percolation.  
Inderjeet Mani and James Pustejovsky present a documentation of the state of the art with respect to the formal and computational representation of motion concepts expressed in language (mostly English). Starting from the conceptual properties represented in the linguistic repertory of motion, they provide an overview of existing formalisms and annotation approaches, ultimately moving towards automatic approaches and computational applications. The book is timely in its representation of the current understanding of motion concepts in language, and will therefore be of great interest in the computational and cognitive linguistics communities. Why, one may ask, do we need a better understanding of motion? It might seem that motion is just one of many human concepts expressed by a number of linguistic terms, which are adequately described by their lexical entries in anybody’s dictionary. Mani and Pustejovsky’s analysis of the linguistic representation of motion suggests a very different idea, however. Far from representing just one marginal aspect of human language, the conceptualization and verbalization of motion turns out to be central to human life—and, as a consequence, central to communication. Motion combines the two fundamental human concepts of space and time. Space without time is, for humans, as meaningless as time without space. Both are inextricably linked—and this link is most notably and systematically represented in language via expressions of motion. Motion is represented whenever aspects of life are described, reﬂecting its deep relevance for human thinking. Any computational approach towards interpreting natural language representation will, sooner or later, need to deal with motion concepts. Interpreting motion, therefore, turns out to be one of the most fundamental research issues for a variety of purposes both in basic (or cognitive) and applied (or computational) research. Strangely, so far, research on the human representation of these fundamental conceptual domains is characteristically divided into two fairly distinct communities dealing with either space or time in language and cognition. The combination of both, adding dynamic aspects, appears to pose too many challenges, adding too many complexities to the already puzzling diversity with respect to human representations of space and time. With their book Interpreting Motion, Mani and Pustejovsky are at the forefront of research that aims to bridge this gap by systematically bringing together ﬁndings and formalisms from both directions. The effort, as such, is laudable. The formal detail provided to explicate the representational patterns considerably adds to the value of this book. Formalization serves computational purposes just as well doi:10.1162/COLI r 00155 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 2  as providing a more precise conceptual grasp of the identiﬁed linguistic phenomena. Nevertheless, it is noted that more elaborate explanations and consistent informal glosses might have supported the general audience of the Oxford University Press “Language and Space” series. Some readers may be more interested in the identiﬁed concepts than in the abundance of formalizations, which may be felt to hamper readability. Against the background of the current state of research, it becomes clear why readers may, at particular points, feel slightly less than satisﬁed with the mechanisms provided for dealing with motion in language. The book is unusually clear and honest in highlighting limitations in the current understanding of crucially relevant concepts, including existing formalization techniques. Consequently, the book is not only a valuable summary of the currently available tools for interpreting motion, but also a useful starting point for further research that aims to ﬁll various gaps identiﬁed by Mani and Pustejovsky’s exploration of the ﬁeld. One major gap that pervades much of the book concerns the mapping of nonlinguistic formalisms such as ontologies or calculi to linguistic expressions. Such a mapping is essential, because language reﬂects how humans naturally represent just those concepts that formalisms attempt to capture. The lack of systematic mapping mechanisms between linguistic structures and widely used formalization categories is by no means the authors’ fault, but reﬂects a longstanding research desideratum well-known in the relevant research communities—with scattered attempts to provide solutions here and there, several of which are represented in this book. Mani and Pustejovsky contribute to this urgently needed research by laying out the available tools in an accessible way and in many cases going several steps further ahead, suggesting mapping solutions wherever and to the extent possible. The book starts with an insightful and fairly detailed introduction section that mainly serves to motivate the complexity involved in interpreting motion, highlighting key insights taken from cognitive linguistic theory as well as earlier psycholinguistic experimentation. Subsequent chapters outline linguistic observations supplemented by non-linguistic calculi, ontologies, and representations, dealing with space and time separately. The main innovative contribution of the book emerges with a proposal for the formal representation of motion in Chapter 4. Here, previous approaches and mechanisms are combined to model the topological changes over time introduced by motion verbs. The remaining two chapters provide annotation speciﬁcations and application prospects. Extraction of motion information from natural language descriptions is proposed in terms of manual annotation; computational implementations are currently still very limited. Nevertheless, these chapters set the stage for subsequent machine learning and other automatic approaches, adopting methodologies already successfully established for other formalisms, to which the newly proposed motion formalism is a successor. In general, the described actual applications concern mostly other related work; the book describes the relevance of the current framework to such applications and represents their goals. Concerning the formal interpretation of motion, the aim in Mani and Pustejovsky’s approach is to capture the spatial implications carried by lexical items in terms of their consequences in the real world. For instance, the verb to ﬂy implies a disconnection between the ﬂying ﬁgure and the ground below it. Although this is an essential condition for ﬂying, in other cases implications can be context dependent, which is why corpusbased investigation is essential. For instance, to establish the spatial situation conveyed by the verb to cross it is necessary to consider what exactly is being crossed. In the case of a ﬁeld, there is constant contact with the ground, whereas in the case of a river being crossed via a bridge, there is no such contact—in fact, contact with the river will be  
Miller was not only a witness but a key player in the major paradigm shift of the 20th century that came to be known as the cognitive revolution. Incredible as it may seem today, his teachers at Harvard followed the behaviorist dogma and recognized neither the autonomy nor the signiﬁcance of the human mind. It took two courageous young scientists—George Miller and Jerome Bruner—to assert, each in their own domain of investigation, that the mind is a worthwhile subject of study. They started out by teaching a course boldly entitled “Cognition” and eventually established the Center for Cognitive Studies, ultimately making behaviorism obsolete. Miller drew an analogy between the human mind and a computer, noting that both store and process huge amounts of information. At the same time, human shortterm memory is limited, as his most celebrated paper on the “Magical Number Seven” demonstrates (Miller 1956/1994). Miller showed that chunking information into meaningful units helps recall, though the number of units that can be memorized seems to hover around seven. For example, U.S. telephone numbers are broken down into three groups of three, three, and four digits (area code, local exchange, and individual number). Chunk parsers (Abney 1991) build on the idea that sentence processing proceeds in phrases, reﬂected in prosodic patterns. Among our cognitive faculties, it was language in particular that fascinated George, a gifted writer. One attraction was that linguistic behavior could be observed, tested, and evaluated quantitatively with the experimental paradigms available to psycholinguists at a time when brain imaging techniques had not yet been developed. The rules of language, with their recursive aspects, could be seen as a kind of program. Although he collaborated with Noam Chomsky on the formal aspects of language, Miller in later life harbored a suspicion of highly abstract theories of syntax. His interest lay primarily in the lexicon, not only because of his authorial love of words, but also because of its size, open-endedness, and dynamic aspects. Moreover, the growth of children’s lexicons offered a window into their cognitive development. Miller is probably best known to readers of Computational Linguistics for his creation of the large lexical database WordNet (Miller 1995). WordNet’s use as a resource for natural language processing was in fact unintended, and its rapid adoption by the NLP community came as a surprise. George was interested in human semantic organization and wanted to test the then-fashionable concept of semantic networks, which allowed for plausible and elegant models of semantic representation and seemed supported by experiments testing lexical access and retrieval (Collins and Quillian 1969). Miller wondered whether a semantic network could in fact be built for the bulk of the English lexicon. In the mid 1980s, he recruited a group of colleagues, students, and his wife Kitty and, without much further instruction, asked them to cluster nouns, verbs, and © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 1  adjectives into “synsets” that could be interrelated with a handful of semantic relations. Relying on conventional lexical resources and intuition, the WordNet team created tens of thousands of entries manually, a fact that provokes head-shaking among the new generation of WordNet builders, who proceed fully or semi-automatically. Each senior member was assigned a different part of speech; Kitty was the “Adjective Lady,” and George took charge of the noun lexicon. Both would come to the Cognitive Science Laboratory every day and patiently perform their lexicographic labor. Once or twice a week, George would leave in the afternoon for the golf course. Well into this eighties, he participated in tournaments and, more often than not, his team was among the winners. A government sponsor’s requirement that the database be publicly released was duly followed, but the response from the budding NLP community was entirely unanticipated by the WordNet team, which was unaware of the challenge of word sense disambiguation. WordNet turned out to be a tool that promised help with the vexing task of word sense discrimination, and its graph structure became the basis for a number of algorithms that measured semantic similarity among words in terms of their distance in the network. Being unique in coverage and design, WordNet not only survived but continued to grow, though its claims to modeling human semantic memory were largely abandoned. George was proud to say that WordNet deﬁned a new kind of electronic lexicography, and WordNet became a common noun. WordNets have been built for dozens of genetically and typologically unrelated languages (http://www.globalwordnet.org). While working on WordNet, George became interested in children’s literacy. He believed that children did not learn words and their meanings from dictionaries, as they were instructed to do in school. To test this hypothesis, he asked children to look up unfamiliar words in a dictionary and write sentences using the new words. As George had guessed, the results were both appalling and amusing. For example, when asked to write a sentence with the word meticulous, children would produce sentences like she was meticulous about falling off the cliff after having seen a dictionary entry that deﬁned this adjective as careful, scrupulous, fastidious. Moreover, George guessed that children enjoy reading but, when encountering an unfamiliar word, are generally disinclined to put their books down and consult a dictionary. His idea was to present new words in their contexts, based on evidence that context-based learning was both natural and efﬁcient (Miller and Gildea 1987). He and his team began to manually annotate a digitized book with entries from WordNet; an interface would allow the children to read the book on the screen, click on unfamiliar words and be presented with the context-appropriate WordNet sense. It should be remembered that George’s idea of reading books on a screen was long before the invention of e-readers! The text-to-WordNet link gave birth to the idea of the semantic concordance. A large team of Princeton students manually annotated nouns, verbs, and adjectives from texts in the Brown Corpus against the corresponding WordNet senses. George thought that this would be a straightforward task: Just as lexicographers create dictionary entries based on tokens in a text, their entries should be mappable back to words in texts in a one-to-one fashion. We learned that dictionaries with enumerative, discrete word senses are in fact not a particularly good way of modeling speakers’ lexicons. Today, research into semantic annotation and measurements of inter-annotator agreement is a lively area of investigation. Although he had to transfer to emeritus status at the then-mandatory retirement age, George did not give up teaching. He organized an informal course on the lexicon and for each of twelve weekly meetings prepared beautifully written lectures that 
Sandra Ku¨ bler† Indiana University Joakim Nivre‡ Uppsala University Parsing is a key task in natural language processing. It involves predicting, for each natural language sentence, an abstract representation of the grammatical entities in the sentence and the relations between these entities. This representation provides an interface to compositional semantics and to the notions of “who did what to whom.” The last two decades have seen great advances in parsing English, leading to major leaps also in the performance of applications that use parsers as part of their backbone, such as systems for information extraction, sentiment analysis, text summarization, and machine translation. Attempts to replicate the success of parsing English for other languages have often yielded unsatisfactory results. In particular, parsing languages with complex word structure and ﬂexible word order has been shown to require non-trivial adaptation. This special issue reports on methods that successfully address the challenges involved in parsing a range of morphologically rich languages (MRLs). This introduction characterizes MRLs, describes the challenges in parsing MRLs, and outlines the contributions of the articles in the special issue. These contributions present up-to-date research efforts that address parsing in varied, cross-lingual settings. They show that parsing MRLs addresses challenges that transcend particular representational and algorithmic choices. 1. Parsing MRLs Parsing is a central task in natural language processing, where a system accepts a sentence in a natural language as input and provides a syntactic representation of the ∗ Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden. E-mail: tsarfaty@stp.lingfil.uu.se. ∗∗ Inria’s Alpage project & Universite´ Paris Sorbonne, Maison de la Recherche, 28 rue Serpentes, 75006 Paris, France. E-mail: djame.seddah@paris-sorbonne.fr. † Indiana University, Department of Linguistics, Memorial Hall 322, Bloomington IN-47405, USA. E-mail: skuebler@indiana.edu. ‡ Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden. E-mail: joakim.nivre@lingfil.uu.se. © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 1  entities and grammatical relations in the sentence as output. The input sentences to a parser reﬂect language-speciﬁc properties (in terms of the order of words, the word forms, the lexical items, and so on), whereas the output abstracts away from these properties in order to yield a structured, formal representation that reﬂects the functions of the different elements in the sentence. The best broad-coverage parsing systems to date use statistical models, possibly in combination with hand-crafted grammars. They use machine learning techniques that allow the system to generalize the syntactic patterns characterizing the data. These machine learning methods are trained on a treebank, that is, a collection of natural language sentences which are annotated with their correct syntactic analyses. Based on the patterns and frequencies observed in the treebank, parsing algorithms are designed to suggest and score novel analyses for unseen sentences, and search for the most likely analysis. The release of a large-scale annotated corpus for English, the Wall Street Journal Penn Treebank (PTB) (Marcus, Santorini, and Marcinkiewicz 1993), led to a signiﬁcant leap in the performance of statistical parsing for English (Magerman 1995; Collins 1997; Charniak 2000; Charniak and Johnson 2005; Petrov et al. 2006; Huang 2008; Finkel, Kleeman, and Manning 2008; Carreras, Collins, and Koo 2008). At the time of their publication, each of these models improved the state-of-the-art of English parsing, bringing constituency-based parsing performance on the standard test set of the PTB to the level of 92% F1-score using the PARSEVAL evaluation metrics (Black et al. 1991). 
Most morphologically rich languages with free word order use case systems to mark the grammatical function of nominal elements, especially for the core argument functions of a verb. The standard pipeline approach in syntactic dependency parsing assumes a complete disambiguation of morphological (case) information prior to automatic syntactic analysis. Parsing experiments on Czech, German, and Hungarian show that this approach is susceptible to propagating morphological annotation errors when parsing languages displaying syncretism in their morphological case paradigms. We develop a different architecture where we use case as a possibly underspeciﬁed ﬁltering device restricting the options for syntactic analysis. Carefully designed morpho-syntactic constraints can delimit the search space of a statistical dependency parser and exclude solutions that would violate the restrictions overtly marked in the morphology of the words in a given sentence. The constrained system outperforms a state-of-the-art data-driven pipeline architecture, as we show experimentally, and, in addition, the parser output comes with guarantees about local and global morpho-syntactic wellformedness, which can be useful for downstream applications. 1. Introduction In statistical parsing, many of the ﬁrst models were developed and optimized for English. This is not surprising, given that English is the predominant language for research in both computational linguistics and linguistics proper. By design, the statistical parsing approach avoids language-speciﬁc decisions built into the model architecture; models should in principle be trainable on any data following the general treebank representation scheme. At the same time, it is well known from theoretical and typological work in linguistics that there is a broad multi-dimensional spectrum of language types, and that English is in a rather “extreme” area in that it marks grammatical relations (subject, object, etc.) strictly with phrase-structural conﬁgurations. There are only residues of an inﬂectional morphology left. In other words, one ∗ Institut fu¨ r Maschinelle Sprachverarbeitung, Universita¨t Stuttgart, Pfaffenwaldring 5b, D-70569 Stuttgart, Germany. E-mail: seeker@ims.uni-stuttgart.de. ∗∗ Institut fu¨ r Maschinelle Sprachverarbeitung, Universita¨t Stuttgart, Pfaffenwaldring 5b, D-70569 Stuttgart, Germany. E-mail: jonas@ims.uni-stuttgart.de. Submission received: 30 September 2011; revised submission received: 20 May 2012; accepted for publication: 3 August 2012. © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 1  cannot exclude that architectural or representational modeling decisions established as empirically useful on English data may be favoring the speciﬁc language type of English. Indeed, carrying over successful model architectures from English to typologically different languages mostly leads to a substantial drop in parsing accuracy. Linguistically aware representational adjustments can help reduce the problem signiﬁcantly, as Collins et al. (1999) showed in their pivotal study adjusting a statistical (constituent) parsing model to a highly inﬂectional language with free word order, Czech in that case, pushing the results more than seven percentage points up to a ﬁnal 80% dependency accuracy (as compared with 91% accuracy for the English “source” parser on the Wall Street Journal). Even in recent years, however, a clear gap has remained between the top parsing architecture for English and morphologically rich(er) languages.1 The relative hardness of the parsing task, compared with English, cuts across statistical parsing approaches (constituent or dependency parsing) and across morphological subtypes, such as languages with a moderately sized remaining inﬂectional system (like German), highly inﬂected languages (like Czech), and languages in which interactions with derivational morphology make the segmentation question non-trivial (such as Turkish or Arabic, compare, for example, Eryigˇ it, Nivre, and Oﬂazer [2008]). Still, it remains hard to pinpoint systematic architectural or representational factors that explain the empirical picture, although there is a collection of “recipes” one can try to tune an approach to a “hard language.” Of course, there are good reasons for adjusting a well-proven system rather than developing a more general one from scratch—given that part of the success of statistical parsing in general lies in subtle ways of exploiting statistical patterns that reﬂect inaccessible levels of information in an indirect way. This article attempts to do justice to the special status of mature data-driven systems and still contribute to a systematic clariﬁcation, by (1) focusing on a clear-cut aspect of morphological marking relevant to syntactic parsing (namely, case marking of core arguments); (2) comparing a selection of languages covering part of the typological spectrum (Czech, German, and Hungarian); (3) using a state-of-the-art data-driven parser (Bohnet 2009, 2010) to establish how far the technique of representational adjustments may take us; and (4) performing a problem-oriented comparison with an alternative architecture, which allows us to add constraints motivated from linguistic considerations. In a ﬁrst experiment, we vary the morphological information available to the parser and examine the errors of the parser with respect to the case-related functions. It turns out that although the parser is indeed able to learn the case-function mapping for all three languages, it is susceptible to errors that are propagated through the pipeline model when parsing languages that show syncretism2 in their morphological paradigms, in our case Czech and German (e. g., for neuter nouns, nominative and accusative case have the same surface form). In contrast, due to its mostly unambiguous case system, we ﬁnd a much smaller effect for Hungarian. Although the parser itself proﬁts much from morphological information as our experiments with gold standard morphology show, errors in automatically predicted morphological information frequently cause errors in the syntactic analysis. 
Richa´rd Farkas† Institute for NLP, University of Stuttgart Renjing Wang‡ Institute for NLP, University of Stuttgart Hinrich Schu¨ tze§ Institute for NLP, University of Stuttgart We study constituent parsing of German, a morphologically rich and less-conﬁgurational language. We use a probabilistic context-free grammar treebank grammar that has been adapted to the morphologically rich properties of German by markovization and special features added to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding with lessons learned, which apply to parsing other morphologically rich and less-conﬁgurational languages. ∗ Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart, Germany. E-mail: fraser@ims.uni-stuttgart.de. ∗∗ Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart, Germany. E-mail: schmid@ims.uni-stuttgart.de. † Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart, Germany. E-mail: farkas@ims.uni-stuttgart.de. ‡ Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart, Germany. § Institute for Natural Language Processing, University of Stuttgart, Pfaffenwaldring 5b, 70569 Stuttgart, Germany. Submission received: October 1, 2011; revised submission received: May 30, 2012; accepted for publication: August 3, 2012 © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 1  1. Introduction A large part of the methodology for parsing in natural language processing has been developed for English and a majority of publications on parsing are about parsing of English. English is a strongly conﬁgurational language. Nearly all of the syntactic information needed by any NLP application can be obtained by conﬁgurational analysis (e.g., by having a correct constituent parse). Many other languages of the world are fundamentally different from English in this respect. At the other end of the conﬁgurational–nonconﬁgurational spectrum we ﬁnd a language like Hungarian that has very little ﬁxed structure on the level of the sentence. Leaving aside the issue of the internal structure of NPs, most sentence-level syntactic information in Hungarian is conveyed by morphology, not by conﬁguration. In this paper, we address German, a third type of language that is intermediate between English and Hungarian. German has strong conﬁgurational constraints (e.g., main clauses are verb-second) as well as rich derivational and inﬂectional morphology, all of which must be modeled for high-quality parsing. German’s intermediate status raises a number of interesting issues in parsing that are of particular prominence for a mixed conﬁgurational/morphological language, but are—as we will argue—of general relevance for morphologically rich languages. Partly this is the case because there are few (if any) languages archetypical of being purely conﬁgurational and purely nonconﬁgurational (e.g., morphology is also important for English and even Hungarian has conﬁgurational constraints). For lack of a better term we refer to intermediate languages as typiﬁed by German as MR&LC for morphologically rich and less-conﬁgurational. Part of the motivation for this special issue is that most work on parsing to date has been done on English, a morphologically simple language. As computational linguistics broadens its focus beyond English it becomes important to take a more general approach to parsing that can handle languages that are typologically very different from English. Rich morphology (RM) is one very salient characteristic of a language that affects the design of parsing methods. We argue that there are two other properties of languages that are relevant in a discussion of parsing RM languages: syncretism and conﬁgurationality. These two properties are correlated typologically with RM and should therefore be taken into account when we address parsing RM languages.1 We ﬁrst deﬁne the three properties and explain their relevance for parsing. The large number of languages for which this correlation holds can be ordered along a single dimension that can be interpreted as degree of morphological complexity. We give examples for a number of languages that are positioned at different points on this scale. Finally, we argue that just as languages that are at the opposite end of the spectrum from English (prototypical examples of morphological richness like Hungarian) require parsing methods that can be quite different from those optimal for English, the same is true for a language like German that is in the middle of the spectrum—and what is required is in some respects different from what is optimal for one extreme (English) or the other (Hungarian). The three correlated properties are rich morphology, syncretism, and conﬁgurationality. Morphological richness can be roughly measured by the number of different morphological forms a word of a particular syntactic category can have; for example, 
 This paper presents the ﬁrst efﬁcient implementation of a weighted deductive CYK parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRSs). LCFRS, an extension of CFG, can describe discontinuities in a straightforward way and is therefore a natural candidate to be used for data-driven parsing. To speed up parsing, we use different context-summary estimates of parse items, some of them allowing for A∗ parsing. We evaluate our parser with grammars extracted from the German NeGra treebank. Our experiments show that data-driven LCFRS parsing is feasible and yields output of competitive quality.  1. Introduction  Recently, the challenges that a rich morphology poses for data-driven parsing have received growing interest. A direct effect of morphological richness is, for instance, data sparseness on a lexical level (Candito and Seddah 2010). A rather indirect effect is that morphological richness often relaxes word order constraints. The principal intuition is that a rich morphology encodes information that otherwise has to be conveyed by a particular word order. If, for instance, the case of a nominal complement is not provided by morphology, it has to be provided by the position of the complement relative to other complements in the sentence. Example (1) provides an example of case marking and free word order in German. In turn, in free word order languages, word order can encode information structure (Hoffman 1995).  (1) a. der kleine Jungenom schickt seiner Schwesterdat den Briefacc  the little boy sends his sister  the letter  b. Other possible word orders:  (i) der kleine Jungenom schickt den Briefacc seiner Schwesterdat (ii) seiner Schwesterdat schickt der kleine Jungenom den Briefacc (iii) den Briefacc schickt der kleine Jungenom seiner Schwesterdat  ∗ Institut fu¨ r Sprache und Information, Universita¨tsstr. 1, D-40225 Du¨ sseldorf, Germany. E-mail: kallmeyer@phil.uni-duesseldorf.de. ∗∗ Institut fu¨ r Sprache und Information, Universita¨tsstr. 1, D-40225 Du¨ sseldorf, Germany. E-mail: maierw@hhu.de. Submission received: September 29, 2011; revised submission received: May 20, 2012; accepted for publication: August 3, 2012.  © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 1  It is assumed that this relation between a rich morphology and free word order does not hold in both directions. Although it is generally the case that languages with a rich morphology exhibit a high degree of freedom in word order, languages with a free word order do not necessarily have a rich morphology. Two examples for languages with a very free word order are Turkish and Bulgarian. The former has a very rich and the latter a sparse morphology. See Mu¨ ller (2002) for a survey of the linguistics literature on this discussion. With a rather free word order, constituents and single parts of them can be displaced freely within the sentence. German, for instance, has a rich inﬂectional system and allows for a free word order, as we have already seen in Example (1): Arguments can be scrambled, and topicalizations and extrapositions underlie few restrictions. Consequently, discontinuous constituents occur frequently. This is challenging for syntactic description in general (Uszkoreit 1986; Becker, Joshi, and Rambow 1991; Bunt 1996; Mu¨ ller 2004), and for treebank annotation in particular (Skut et al. 1997). In this paper, we address the problem of data-driven parsing of discontinuous constituents on the basis of German. In this section, we inspect the type of data we have to deal with, and we describe the way such data are annotated in treebanks. We brieﬂy discuss different parsing strategies for the data in question and motivate our own approach.  1.1 Discontinuous Constituents  Consider the sentences in Example (2) as examples for discontinuous constituents (taken from the German NeGra [Skut et al. 1997] and TIGER [Brants et al. 2002] treebanks). Example (2a) shows several instances of discontinuous VPs and Example (2b) shows a discontinuous NP. The relevant constituent is printed in italics.  (2) a. Fronting:  (i) Daru¨ ber muss nachgedacht werden. (NeGra) Thereof must thought be “One must think of that”  (ii) Ohne internationalen Schaden ko¨ nne sich Bonn von dem Denkmal nicht Without international damage could itself Bonn from the monument not distanzieren, ... (TIGER) distance “Bonn could not distance itself from the monument without international damage.”  (iii) Auch wu¨ rden durch die Regelung nur “sta¨ndig neue Altfa¨lle Also would through the regulation only “constantly new old cases entstehen”. (TIGER) emerge” “Apart from that, the regulation would only constantly produce new old cases.”  b. Extraposed relative clauses:  (i) . . . ob  auf deren Gela¨nde der Typ von Abstellanlage gebaut  . . . whether on their terrain the type of parking facility built  werden ko¨ nne, der ... (NeGra)  get could, which . . .  “. . . whether one could build on their premises the type of parking facility,  which . . . ”  88  Kallmeyer and Maier  PLCFRS Parsing  Examples of other such languages are Bulgarian and Korean. Both show discontinuous constituents as well. Example (3a) is a Bulgarian example of a PP extracted out of an NP, taken from the BulTreebank (Osenova and Simov 2004), and Example (3b) is an example of fronting in Korean, taken from the Penn Korean Treebank (Han, Han, and Ko 2001). (3) a. Na kyshtata toi popravi pokriva. Of house-DET he repaired roof. “It is the roof of the house he repairs.” b. Gwon.han-u˘ l nu.ga ka.ji.go iss.ji? Authority-OBJ who has not? “Who has no authority?” Discontinuous constituents are by no means limited to languages with freedom in word order. They also occur in languages with a rather ﬁxed word order such as English, resulting from, for instance, long-distance movements. Examples (4a) and (4b) are examples from the Penn Treebank for long extractions resulting in discontinuous S categories and for discontinuous NPs arising from extraposed relative clauses, respectively (Marcus et al. 1994). (4) a. Long Extraction in English: (i) Those chains include Bloomingdale’s, which Campeau recently said it will sell. (ii) What should I do. b. Extraposed nominal modiﬁers (relative clauses and PPs) in English: (i) They sow a row of male-fertile plants nearby, which then pollinate the malesterile plants. (ii) Prices fell marginally for fuel and electricity. 1.2 Treebank Annotation and Data-Driven Parsing Most constituency treebanks rely on an annotation backbone based on Context-Free Grammar (CFG). Discontinuities cannot be modeled with CFG, because they require a larger domain of locality than the one offered by CFG. Therefore, the annotation backbone based on CFG is generally augmented with a separate mechanism that accounts for the non-local dependencies. In the Penn Treebank (PTB), for example, trace nodes and co-indexation markers are used in order to establish additional implicit edges in the tree beyond the overt phrase structure. In Tu¨ Ba-D/Z (Telljohann et al. 2012), a German Treebank, non-local dependencies are expressed via an annotation of topological ﬁelds (Ho¨ hle 1986) and special edge labels. In contrast, some other treebanks, among them NeGra and TIGER, give up the annotation backbone based on CFG and allow annotation with crossing branches (Skut et al. 1997). In such an annotation, non-local dependencies can be expressed directly by grouping all dependent elements under a single node. Note that both crossing branches and traces annotate long-distance dependencies in a linguistically meaningful way. A difference is, however, that crossing branches are less theory-dependent because they do not make any assumptions about the base positions of “moved” elements. Examples for the different approaches of annotating discontinuities are given in Figures 1 and 2. Figure 1 shows the NeGra annotation of Example (2a-i) (left), and an 89  Computational Linguistics  Volume 39, Number 1  Figure 1 A discontinuous constituent. Original NeGra annotation (left) and a Tu¨ Ba-D/Z-style annotation (right).  SBARQ  SBARQ  SQ SBJ VP  SQ SBJ VP  WHNP  NP  NP  *T*  WHNP  NP  What should  I  WP  MD  PRP  do  *T*  ?  VB -NONE-  .  What should I  WP  MD  PRP  do  ?  VB  .  Figure 2 A discontinuous wh-movement. Original PTB annotation (left) and NeGra-style annotation (right).  annotation of the same sentence in the style of the Tu¨ Ba-D/Z treebank (right). Figure 2 shows the PTB annotation of Example (4a-ii) (on the left, note that the directed edge from the trace to the WHNP element visualizes the co-indexation) together with a NeGra-style annotation of the same sentence (right). In the past, data-driven parsing has largely been dominated by Probabilistic Context-Free Grammar (PCFG). In order to extract a PCFG from a treebank, the trees need to be interpretable as CFG derivations. Consequently, most work has excluded non-local dependencies; either (in PTB-like treebanks) by discarding labeling conventions such as the co-indexation of the trace nodes in the PTB, or (in NeGra/TIGER-like treebanks) by applying tree transformations, which resolve the crossing branches (e.g., Ku¨ bler 2005; Boyd 2007). Especially for the latter treebanks, such a transformation is problematic, because it generally is non-reversible and implies information loss. Discontinuities are no minor phenomenon: Approximately 25% of all sentences in NeGra and TIGER have crossing branches (Maier and Lichte 2011). In the Penn Treebank, this holds for approximately 20% of all sentences (Evang and Kallmeyer 2011). This shows that it is important to properly treat such structures.  1.3 Extending the Domain of Locality In the literature, different methods have been explored that allow for the use of nonlocal information in data-driven parsing. We distinguish two classes of approaches. The ﬁrst class consists of approaches that aim at using formalisms which produce trees without crossing branches but provide a larger domain of locality than CFG— for instance, through complex labels (Hockenmaier 2003) or through the derivation  90  Kallmeyer and Maier  PLCFRS Parsing  CFG:  LCFRS:  •  A  A  •  •  γ1  γ2  γ3  γ  Figure 3 Different domains of locality.  mechanism (Chiang 2003). The second class, to which we contribute in this paper, consists of approaches that aim at producing trees which contain non-local information. Some methods realize the reconstruction of non-local information in a post- or preprocessing step to PCFG parsing (Johnson 2002; Dienes 2003; Levy and Manning 2004; Cai, Chiang, and Goldberg 2011). Other work uses formalisms that accommodate the direct encoding of non-local information (Plaehn 2004; Levy 2005). We pursue the latter approach. Our work is motivated by the following recent developments. Linear Context-Free Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-ﬁrst parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German have recently attracted the interest of the parsing community (Ku¨ bler and Penn 2008; Seddah, Ku¨ bler, and Tsarfaty 2010). We bring together these developments by presenting a parser for Probabilistic LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser produces trees with crossing branches and thereby accounts for syntactic long-distance dependencies while not making any additional assumptions concerning the position of hypothetical traces. We have implemented a CYK parser and we present several methods for context summary estimation of parse items. The estimates either act as ﬁgures-of-merit in a best-ﬁrst parsing context or as estimates for A∗ parsing. A test on a real-world-sized data set shows that our parser achieves competitive results. To our knowledge, our parser is the ﬁrst for the entire class of PLCFRS that has successfully been used for data-driven parsing.1 The paper is structured as follows. Section 2 introduces probabilistic LCFRS. Sections 3 and 4 present the binarization algorithm, the parser, and the outside estimates which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from a treebank and we present grammar reﬁnement methods for these speciﬁc treebank grammars. Finally, Section 6 presents evaluation results and Section 7 compares our work to other approaches.  
We present a constituency parsing system for Modern Hebrew. The system is based on the PCFG-LA parsing method of Petrov et al. (2006), which is extended in various ways in order to accommodate the speciﬁcities of Hebrew as a morphologically rich language with a small treebank. We show that parsing performance can be enhanced by utilizing a language resource external to the treebank, speciﬁcally, a lexicon-based morphological analyzer. We present a computational model of interfacing the external lexicon and a treebank-based parser, also in the common case where the lexicon and the treebank follow different annotation schemes. We show that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY lattice parsing. Performing the tasks jointly is effective, and substantially outperforms a pipelinebased model. We suggest modeling grammatical agreement in a constituency-based parser as a ﬁlter mechanism that is orthogonal to the grammar, and present a concrete implementation of the method. Although the constituency parser does not make many agreement mistakes to begin with, the ﬁlter mechanism is effective in ﬁxing the agreement mistakes that the parser does make. These contributions extend outside of the scope of Hebrew processing, and are of general applicability to the NLP community. Hebrew is a speciﬁc case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English. The lattice-based parsing methodology is useful in any case where the input is uncertain. Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with a small treebank. 1. Introduction Different languages have different syntactic properties. In English, word order is relatively ﬁxed, whereas in other languages word order is much more ﬂexible (in Hebrew, the subject may appear either before or after a verb). In languages with a ﬂexible word order, the meaning of the sentence is realized using other structural elements, like word ∗ Computer Science Department, Ben Gurion University of the Negev, Israel. E-mail: yoav.goldberg@gmail.com. ∗∗ Computer Science Department, Ben Gurion University of the Negev, Israel. E-mail: elhadad@cs.bgu.ac.il. Submission received: 30 September 2011; revised submission received: 19 May 2012; accepted for publication: 3 August 2012. © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 1  inﬂections or markers, which are referred to as morphology (in Hebrew, the marker ‫את‬ is used to mark deﬁnite objects, distinguishing them from subjects in the same position. In addition, verbs and nouns are marked for gender and number, and subject and verb must share the same gender and number). A limited form of morphology also exists in English: the -s and -ed sufﬁxes are examples of English morphological markings. In other languages, morphological processes may be much more involved. The lexical units (words) in English are always separated by white space. In Chinese, such separation is not available. In Hebrew (and Arabic), most words are separated by white space, but many of the function words (determiners like the, conjunctions such as and, and prepositions like in or of ) do not stand on their own but are instead attached to the following words. A large part of the parsing literature is devoted to automatic parsing of English, a language with a relatively simple morphology, relatively ﬁxed word order, and a large treebank. Data-driven English parsing is now at the state where naturally occurring text in the news domain can be automatically parsed with accuracies of around 90% (according to standard parsing evaluation measures). When moving from English to languages with richer morphologies and less-rigid word orders, however, the parsing algorithms developed for English exhibit a large drop in accuracy. In addition, whereas English has a large treebank, containing over one million annotated words, many other languages have much smaller treebanks, which also contribute to the drop in the accuracies of the data-driven parsers. A similar drop in parsing accuracy is also exhibited in English when moving from the news domain, on which parsers have traditionally been trained, to other genres such as prose, blogs, poetry, product reviews, or biomedical texts, which use different vocabularies and, to some extent, different syntactic rules. This work focuses on constituency parsing of Modern Hebrew, a Semitic language with a rich and productive morphology, relatively free word order,1 and a small treebank. Several natural questions arise: Can the small size of the treebank be compensated for using other available resources or sources of information? How should the word segmentation issue (that function words do not appear in isolation but attach to the next word, forming ambiguous letter patterns) be handled? Can morphological information be used effectively in order to improve parsing accuracy? We present a system which is based on a state-of-the-art model for constituency parsing, namely, the probabilistic context-free grammar (PCFG) with latent annotations (PCFG-LA) model of Petrov et al. (2006), as implemented in the BerkeleyParser. After evaluating the out-of-the-box performance of the BerkeleyParser on the Hebrew treebank, we discuss some of its limitations and then go on to extend the PCFG-LA parsing model in several directions, making it more suitable for parsing Hebrew and related languages. Our extensions are based on the following themes. Separation of lexical and syntactic knowledge. There are two kinds of knowledge inherent in a parsing system. One of them is syntactic knowledge governing the way in which words can be combined to form structures, which, in turn, can be combined to form ever larger structures. The other is lexical knowledge about the identities of individual words, the word classes they belong to, and the kinds of syntactic structures they can participate in. We argue that the amount of syntactic knowledge needed for a parsing system is relatively limited, and that sufﬁciently large parts of it can be captured also  
Owen Rambow† Center for Computational Learning Systems, Columbia University We explore the contribution of lexical and inﬂectional morphology features to dependency parsing of Arabic, a morphologically rich language with complex agreement patterns. Using controlled experiments, we contrast the contribution of different part-of-speech (POS) tag sets and morphological features in two input conditions: machine-predicted condition (in which POS tags and morphological feature values are automatically assigned), and gold condition (in which their true values are known). We ﬁnd that more informative (ﬁne-grained) tag sets are useful in the gold condition, but may be detrimental in the predicted condition, where they are outperformed by simpler but more accurately predicted tag sets. We identify a set of features (deﬁniteness, person, number, gender, and undiacritized lemma) that improve parsing quality in the predicted condition, whereas other features are more useful in gold. We are the ﬁrst to show that functional features for gender and number (e.g., “broken plurals”), and optionally the related rationality (“humanness”) feature, are more helpful for parsing than form-based gender and number. We ﬁnally show that parsing quality in the predicted condition can dramatically improve by training in a combined gold+predicted condition. We experimented with two transition-based parsers, MaltParser and Easy-First Parser. Our ﬁndings are robust across parsers, models, and input conditions. This suggests that the contribution of the linguistic knowledge in the tag sets and features we identiﬁed goes beyond particular experimental settings, and may be informative for other parsers and morphologically rich languages. 1. Introduction For Arabic—as for other morphologically rich languages—the role of morphology is often expected to be essential in syntactic modeling, and the role of word order is less important than in morphologically poorer languages such as English. Morphology ∗ Nuance Communications, 505 First Ave. S, Suite 700, Seattle, WA 98104. E-mail: yuvalmarton@gmail.com. ∗∗ Center for Computational Learning, Columbia University. E-mail: habash@ccls.columbia.edu. † Center for Computational Learning, Columbia University. E-mail: rambow@ccls.columbia.edu. Submission received: October 1, 2011; revised submission received: June 16, 2012; accepted for publication: August 3, 2012. © 2013 Association for Computational Linguistics  Computational Linguistics  Volume 39, Number 1  interacts with syntax in two ways: agreement and assignment. In agreement, there is  coordination between the morphological features of two words in a sentence based  on their syntactic conﬁguration (e.g., subject–verb or noun–adjective agreement in  GENDER and/or NUMBER). In assignment, speciﬁc morphological feature values are  assigned in certain syntactic conﬁgurations (e.g., CASE assignment for the subject or  direct object of a verb).1  Parsing model design aims to come up with features that best help parsers learn  the syntax and choose among different parses. The choice of optimal linguistic features  depends on three factors: relevance, redundancy, and accuracy. A feature has relevance  if it is useful in making an attachment (or labeling) decision. A particular feature may  or may not be relevant to parsing. For example, the GENDER feature may help parse  the Arabic phrase  /  bAb AlsyAr Aljdyd/Aljdyd (‘door the-car  the-newmasc.sg/fem.sg [lit.]),2 using syntactic agreement: if the-new is masculine (  ),  it should attach to the masculine door, resulting in the meaning ‘the car’s new door’;  if the-new is feminine (  ), it should attach to the feminine the-car, resulting in ‘the  door of the new car.’ Conversely, the ASPECT feature does not constrain any syntactic  decision. Even if relevant, a feature may not necessarily contribute to optimal perfor-  mance because it may be redundant with other features that surpass it in relevance. For  example, as we will see, the DET and STATE features alone both help parsing because  they help identify the idafa construction, but they are redundant with each other and the  DET feature is more helpful because it also helps with adjectival modiﬁcation of nouns.  Finally, the accuracy of automatically predicting the feature values (ratio of correct  predictions out of all predictions) of course affects the value of a feature on unseen text.  Even if relevant and non-redundent, a feature may be hard to predict with sufﬁcient  accuracy by current technology, in which case it will be of little or no help for parsing,  even if helpful when its gold values are provided. As we will see, the CASE feature is  very relevant and not redundant, but it cannot be predicted with high accuracy and  overall it is not useful.  Different languages vary with respect to which features may be most helpful given  various tradeoffs among these three factors. In the past, it has been shown that if we  can recognize the relevant morphological features in assignment conﬁgurations well  enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech  improves Czech parsing (Collins et al. 1999): CASE is relevant, not redundant, and can  be predicted with sufﬁcient accuracy. It has been more difﬁcult showing that agreement  morphology helps parsing, however, with negative results for dependency parsing in  several languages (Eryigit, Nivre, and Oﬂazer 2008; Nivre, Boguslavsky, and Iomdin  2008; Nivre 2009).  In this article we investigate morphological features for dependency parsing of  Modern Standard Arabic (MSA). For MSA, the space of possible morphological features  is fairly large. We determine which morphological features help and why. We further  determine the upper bound for their contribution to parsing quality. Similar to previous  
We demonstrate a web-based, languageindependent annotation framework used for manual correction of a large Arabic corpus. Our framework provides intuitive interfaces for annotating text and managing the annotation process. We describe the details of both the annotation and the administration interfaces as well as the back-end engine. We also show how this framework is able to speed up the annotation process by employing automated annotators to ﬁx basic Arabic spelling errors. 
We introduce -SmartReader- an English reading tool for non-native English readers to overcome language related hindrances while reading a text. It makes extensive use of widely-available NLP tools and resources. SmartReader is a web-based application that can be accessed from standard browsers running on PCs or tablets. A user can choose a text document from the system’s library they want to read or can upload a new document of their own and the system will display an interactive version of such text, that provides the reader with an intelligent e-book functionality. 
Dashboard is a tool for integration, validation, and visualization of Natural Language Processing (NLP) systems. It provides infrastructural facilities using which individual NLP modules may be evaluated and refined, and multiple NLP modules may be combined to build a large end-user NLP system. It helps system integration team to integrate and validate NLP systems. The tool provides a visualization interface that helps developers to profile (time and memory) for each module. It helps researchers to evaluate and compare their module with the earlier versions of same module. The tool promotes reuse of existing modules to build new NLP systems. Dashboard supports execution of modules that are distributed on heterogeneous platforms. It provides a powerful notation to specify runtime properties of NLP modules. It provides an easy-touse graphical interface that is developed using Eclipse RCP. Users can choose an I/O perspective (view) that allows him better visualization of intermediate outputs. Additionally, Eclipse RCP provides plugin architecture; hence customized end-user specific functionality can be easily added to the tool. 
DIRA is a query expansion tool that generates search terms in Standard Arabic and/or its dialects when provided with queries in English or Standard Arabic. The retrieval of dialectal Arabic text has recently become necessary due to the increase of dialectal content on social media. DIRA addresses the challenges of retrieving information in Arabic dialects, which have signiﬁcant linguistic differences from Standard Arabic. To our knowledge, DIRA is the only tool in existence that automatically generates dialect search terms with relevant morphological variations from English or Standard Arabic query terms. 
The need to navigate through massive document sets is getting common due to the abundant data available around us. To alleviate navigation, tools that are able to grasp the most relevant aspects of document subsets and their relations to other parts of the corpus can be highly beneﬁcial. In this paper, we shall introduce an application1 that processes and visualizes corpora to reveal the main topics and their relative roles to each other. Our suggested solution combines natural language processing and graph theoretic techniques for the visualization of documents based on their automatically detected keyphrases. Furthermore keyphrases that describe thematically related subcorpora are also extracted based on information-theoretic grounds. As for demonstration purposes our application currently deals with papers published at ACL workshops. 
News headlines exhibit stylistic peculiarities. The goal of our translation engine ‘Making Headlines in Hindi’ is to achieve automatic translation of English news headlines to Hindi while retaining the Hindi news headline styles. There are two central modules of our engine: the modiﬁed translation unit based on Moses and a co-occurrencebased post-processing unit. The modiﬁed translation unit provides two machine translation (MT) models: phrase-based and factor-based (both using in-domain data). In addition, a co-occurrence-based post-processing option may be turned on by a user. Our evaluation shows that this engine handles some linguistic phenomena observed in Hindi news headlines. 
Transition-based dependency parsers are widely used in the Natural Language Processing community but they are normally treated as black boxes, assuming that they provide the dependency parsing of a set of examples. We present MaltDiver, a tool developed to visualize the transitions performed by the transition-based parsers included in MaltParser and to show how the parsers interact with the sentences and the data structures within. During the demo session, we will run MaltDiver on several sentences and we will explain the potentialities of such a system.1 
Immediately after the 2011 Great East Japan Earthquake, the Internet was ﬂooded by a huge amount of information concerning the damage and problems caused by the earthquake, the tsunami, and the nuclear disaster. Many reports about aid efforts and advice to victims were also transmitted into cyberspace. However, since most people were overwhelmed by the massive amounts of information, they could not make proper decisions, and much confusion was caused. Furthermore, false rumors spread on the Internet and fanned such confusion. We demonstrate NICT’s prototype disaster information analysis system, which was designed to properly organize such a large amount of disaster-related information on social media during future large-scale disasters to help people understand the situation and make correct decisions. We are going to deploy it using a large-scale computer cluster in ﬁscal year 2014. 
 In this paper we present a demo of our system: Social Interaction Network Extractor from Text (SINNET). SINNET is able to extract a social network from unstructured text. Nodes in the network are people and links are social events.  
Various news sites exist today where internet users can read the most recent news and people’s opinions about. However, usually these sites do not organize comments well and do not ﬁlter irrelevant content. Due to this limitation, readers who wonder about people’s opinion regarding some speciﬁc topic, have to manually follow relevant comments, reading and ﬁltering a lot of irrelevant text. In this work1, we introduce a publicly available software implementing our approach, previously introduced in (Litvak and Matz, 2013), for retrieving and ranking the relevant comments for a given paragraph of news article and vice versa. We use Topic-Sensitive PageRank for ranking comments/paragraphs relevant for a userspeciﬁed paragraph/comment. 
We demonstrate an online application to explore lexical networks. Tmuse displays a 3D interactive graph of similar words, whose layout is based on the proxemy between vertices of synonymy and translation networks. Semantic themes of words related to a query are outlined, and projected across languages. The application is useful as, for example, a writing assistance. It is available, online, for Mandarin Chinese, English and French, as well as the corresponding language pairs, and can easily be ﬁtted to new resources. 
We demonstrate our large-scale web information analysis system called WISDOM2013, which consists of several deep semantic analysis systems such as a factoid QA, a non-factoid QA and a sentiment analyzer, and a software platform on which its semantic analysis systems can be applied to a billion-page-scale web archive. The software platform has an extendable architecture, and we are planning to enhance WISDOM2013 in the future by adding more semantic analysis systems and inference mechanisms. 
Mining online discussions to extract answers is an important research problem. Methods proposed in the past used supervised classiﬁers trained on labeled data. But, collecting training data for each target forum is labor intensive and time consuming, thus limiting their deployment. A recent approach had proposed to extract answers in an unsupervised manner, by taking cues from their repetitions. This assumption however, does not hold true in many cases. In this paper, we propose two semi-supervised methods for extracting answers from discussions, which utilize the large amount of unlabeled data available, alongside a very small training set to obtain improved accuracies. We show that it is possible to boost the performance by introducing a related, but parallel task of identifying acknowledgments to the answers. The accuracy achieved by our approaches surpass the baselines by a wide margin, as shown by our experiments. 
Automatic keyphrase extraction aims to pick out a set of terms as a representation of a document without manual assignment efforts. Supervised and unsupervised graph-based ranking methods have been studied for this task. However, previous methods usually computed importance scores of words under the assumption of single relation between words. In this work, we propose WordTopic-MultiRank as a new method for keyphrase extraction, based on the idea that words relate with each other via multiple relations. First we treat various latent topics in documents as heterogeneous relations between words and construct a multi-relational word network. Then, a novel ranking algorithm, named Biased-MultiRank, is applied to score the importance of words and topics simultaneously, as words and topics are considered to have mutual inﬂuence on each other. Experimental results on two different data sets show the outstanding performance and robustness of our proposed approach in automatic keyphrase extraction task. 
We explore the feasibility of contextual healthiness classiﬁcation of food items. We present a detailed analysis of the linguistic phenomena that need to be taken into consideration for this task based on a specially annotated corpus extracted from web forum entries. For automatic classiﬁcation, we compare a supervised classiﬁer and rule-based classiﬁcation. Beyond linguistically motivated features that include sentiment information we also consider the prior healthiness of food items. 
Query segmentation is to split a query into a sequence of non-overlapping segments that completely cover all tokens in the query. The majority of methods are unsupervised, however, they are usually not as accurate as supervised methods due to the lack of guidance from labeled data. In this paper, we propose a new paradigm of learning a replacement model with consistency (LRMC), to enable unsupervised training with guidance from search log data. In LRMC, we ﬁrst assume the existence of a base segmenter (an implementation of any existing approach). Then, we utilize a key observation that queries with a similar intent tend to have consistent segmentations, to automatically collect a set of labeled data from the outputs of the base segmenter by leveraging search log data. Finally, we employ the auto-collected data to train a replacement model for selecting the correct segmentation of a new query from the outputs of the base segmenter. The results show LRMC can improve state-of-the-art methods by an F-Score of around 7%. 
A concept can be linguistically expressed in various syntactic constructions. Such syntactic variations spoil the effectiveness of incorporating dependencies between words into information retrieval systems. This paper presents an information retrieval method for normalizing syntactic variations via predicate-argument structures. We conduct experiments on standard test collections and show the effectiveness of our approach. Our proposed method signiﬁcantly outperforms a baseline method based on word dependencies. 
The main challenge in hierarchical multilabel text classiﬁcation is how to leverage hierarchically organized labels. In this paper, we propose to exploit dependencies among multiple labels to be output, which has been left unused in previous studies. To do this, we ﬁrst formalize this task as a structured prediction problem and propose (1) a global model that jointly outputs multiple labels and (2) a decoding algorithm for it that ﬁnds an exact solution with dynamic programming. We then introduce features that capture inter-label dependencies. Experiments show that these features improve performance while reducing the model size. 
The objective of the present contribution is to give a survey of the annotation of information structure in the Czech part of the Prague Czech-English Dependency Treebank. We report on this first step in the process of building a parallel annotation of information structure in this corpus, and elaborate on the automatic pre-annotation procedure for the Czech part. The results of the pre-annotation are evaluated, based on the comparison of the automatic and manual annotation. 
Animacy is an inherent property of entities that nominals refer to in the physical world. This semantic property of a nominal has received much attention in both linguistics and computational linguistics. In this paper, we present a robust unsupervised technique to infer the animacy of nominals in languages with rich morphological case. The intuition behind our method is that the control/agency of a noun depicted by case marking can approximate its animacy. A higher control over an action implies higher animacy. Our experiments on Hindi show promising results with Fβ and P urity scores of 89 and 86 respectively. 
We present a simple, logic-based architecture for solving math problems written in natural language. A problem is ﬁrstly translated to a logical form. It is then rewritten into the input language of a solver algorithm and ﬁnally the solver ﬁnds an answer. Such a clean decomposition of the task however does not come for free. First, despite its formality, math text still exploits the ﬂexibility of natural language to convey its complex logical content succinctly. We propose a mechanism to ﬁll the gap between the simple form and the complex meaning while adhering to the principle of compositionality. Second, since the input to the solver is derived by strictly following the text, it may require far more computation than those derived by a human, and may go beyond the capability of the current solvers. Empirical study on Japanese university entrance examination problems showed positive results indicating the viability of the approach, which opens up a way towards a true end-to-end problem solving system through the synthesis of the advances in linguistics, NLP, and computer math. 
Automated lexicon acquisition from corpora represents one way that large datasets can be leveraged to provide resources for a variety of NLP tasks. Our work applies techniques popularized in sentiment lexicon acquisition and topic modeling to the broader task of creating a stylistic lexicon. A novel aspect of our approach is a focus on multiple related styles, ﬁrst extracting initial independent estimates of style based on co-occurrence with seeds in a large corpus, and then reﬁning those estimates based on the relationship between styles. We compare various promising implementation options, including vector space, Bayesian, and graph-based representations, and conclude that a hybrid approach is indeed warranted. 
We present the Prague Discourse Treebank 1.0, a collection of Czech texts annotated for various discourse-related phenomena "beyond the sentence boundary". The treebank contains manual annotations of (1), discourse connectives, their arguments and senses, (2), textual coreference, and (3), bridging anaphora, all carried out on 50k sentences of the treebank. Contrary to most similar projects, the annotation was performed directly on top of syntactic trees (from the previous project of the Prague Dependency Treebank 2.5), benefiting thus from the linguistic information already existing on the same data. In this article, we present our theoretical background, describe the annotations in detail, and offer evaluation numbers and corpus statistics. 
This paper proposes a novel algorithm for multilingual mention detection: we extract mentions from parse trees via kernelbased SVM learning. Our approach allows for straightforward mention detection for any language where (not necessary perfect) parsing resources are available, without any complex language-speciﬁc rule engineering. We also investigate possibilities for incorporating automatically acquired mentions into an end-to-end coreference resolution system. We evaluate our approach on the Arabic and Chinese portions of the CoNLL-2012 dataset, showing a signiﬁcant improvement over the system with the baseline mention detection. 
Social streams have proven to be the most up-to-date and inclusive information on current events. In this paper we propose a novel probabilistic modelling framework, called violence detection model (VDM), which enables the identiﬁcation of text containing violent content and extraction of violence-related topics over social media data. The proposed VDM model does not require any labeled corpora for training, instead, it only needs the incorporation of word prior knowledge which captures whether a word indicates violence or not. We propose a novel approach of deriving word prior knowledge using the relative entropy measurement of words based on the intuition that low entropy words are indicative of semantically coherent topics and therefore more informative, while high entropy words indicates words whose usage is more topical diverse and therefore less informative. Our proposed VDM model has been evaluated on the TREC Microblog 2011 dataset to identify topics related to violence. Experimental results show that deriving word priors using our proposed relative entropy method is more effective than the widely-used information gain method. Moreover, VDM gives higher violence classiﬁcation results and produces more coherent violence-related topics compared to a few competitive baselines. 
As the popularity of Community Question Answering(CQA) increases, spamming activities also picked up in numbers and variety. On CQA sites, spammers often pretend to ask questions, and select answers which were published by their partners or themselves as the best answers. These fake best answers cannot be easily detected by neither existing methods nor common users. In this paper, we address the issue of detecting spammers on CQA sites. We formulate the task as an optimization problem. Social information is incorporated by adding graph regularization constraints to the text-based predictor. To evaluate the proposed approach, we crawled a data set from a CQA portal. Experimental results demonstrate that the proposed method can achieve better performance than some state-of-the-art methods. 
We study the linguistic phenomenon of informal words in the domain of Chinese microtext and present a novel method for normalizing Chinese informal words to their formal equivalents. We formalize the task as a classiﬁcation problem and propose rule-based and statistical features to model three plausible channels that explain the connection between formal and informal pairs. Our two-stage selection-classiﬁcation model is evaluated on a crowdsourced corpus and achieves a normalization precision of 89.5% across the different channels, signiﬁcantly improving the state-of-the-art. 
Event recognition and event type classification are among the important areas in text mining. A state-of-the-art approach utilizing deep-level lexical semantics and syntactic dependencies suffers from a limitation of requiring too large feature space. In this paper, we propose a novel feature selection method using a semantic hierarchy of features based on WordNet relations and syntactic dependencies. Compared to the well-known feature selection methods, our proposed method reduces the feature space significantly while keeping the same level of effectiveness. For noun events, it improves effectiveness as well as efficiency. Moreover, we expect the proposed feature selection can be applied to the other types of text classification using hierarchically organized semantic resources such as WordNet. 
In this research, we suggest an approach to retrieval-related tasks for Korean SMS text. Most of the previous approaches to such text used morphological analysis as the routine stage of the preprocessing workflow, functionally equivalent to POS tagging. However, such approaches suffer difficulties since Short Message Service language usually contains irregular orthography, atypically spelled words, unspaced segments, etc. Two experiments were conducted to measure how well these problems can be avoided with the transliteration of Korean to Roman letters. In summary, we will argue that such a Romanization-based retrieval method has several advantages since it provides an easier way to preprocess the data with a variety of linguistic rules. 
This paper investigates the importance of a word lattice generation algorithm in joint word segmentation and POS tagging. We conducted experiments on three Japanese data sets to demonstrate that the previously proposed pruning-based algorithm is in fact not efﬁcient enough, and that the pipeline algorithm, which is introduced in this paper, achieves considerable speedup without loss of accuracy. Moreover, the compactness of the lattice generated by the pipeline algorithm was investigated from both theoretical and empirical perspectives. 
A major problem in the field of Chinese word segmentation is the identification of out-ofvocabulary words. We propose a simple yet effective approach for extracting maximized substrings, which provide good estimations of unknown word boundaries. We also develop a new semi-supervised segmentation technique that incorporates retrieved substrings using discriminative learning. The effectiveness of this novel approach is demonstrated through experiments using both in-domain and out-ofdomain data. 1. Introduction Chinese sentences are written without explicit word boundaries, which makes Chinese word segmentation (CWS) an initial and important step in Chinese language processing. Recent advances in machine learning techniques have boosted the performance of CWS systems. On the other hand, a major difficulty in CWS is the problem of identifying out-of-vocabulary (OOV) words, as the Chinese language is continually and rapidly evolving, particularly with the rapid growth of the internet. A recent line of research to overcome this difficulty is through exploiting characteristics of frequent substrings in unlabeled data. Statistical criteria for measuring the likelihood of a substring being a word have been proposed in previous studies of unsupervised segmentation, such as accessor variety (Feng et al., 2004) and branching entropy (Jin and Tanaka-Ishii, 2006). This kind of criteria has been applied to enhance the performance of supervised segmentation systems (Zhao and Kit, 2007; Zhao and Kit, 2008;  Substring  Freq  一致  3  界限数的期望值  2  一致认定界限  2  的期望值  3  认定界限数的  2  值  4  Table 1. A particular type of substrings with mul-  tiple occurrences in the Chinese sentence: “使一致  认定界限数的期望值近似于一致正确界限数的期望  值，求得一致认定界限的期望值/认定界限数的  值。”  Sun and Xu, 2011) by identifying unknown word boundaries. In this paper, instead of investigating statistical characteristics of batched substrings, we propose a novel method that extracts substrings as reliable word boundary estimations. The technique uses large-scale unlabeled data, and processes it on the fly. To illustrate the idea, we first consider the following example taken from a scientific text:  “使一致认定界限数的期望值近似于一致正确界限 数的期望值，求得一致认定界限的期望值/认定界 限数的值。”  Without any knowledge of the Chinese language one may still notice that some substrings like “一致” and “的期望值”, occur multiple times in the sentence and are likely to be valid words or chains of words. Consider a particular type of frequent substring that cannot be simultaneously extended by its surrounding characters while still being equal (Table 1). We can observe that the boundaries of such substrings can be used as perfect word delimiters. We can segment the sentence by simply treating the boundaries of each occurrence of a substring in Table 1 as word  171 International Joint Conference on Natural Language Processing, pages 171–179, Nagoya, Japan, 14-18 October 2013.  Word-level Character-level  Nodes  Nodes  Sentence: 陈德铭答记者问 (Chen Deming answers to journalists’ questions)  陈[S-noun] 陈[B-noun] 陈[S-verb] 陈[B-verb]  德[S-noun] 德[B-noun] 德[B2-noun] 德[E-noun] 德[S-verb] 德[B-verb]  铭[S-noun] 铭[B-noun] 铭[B2-noun] 铭[B3-noun] 铭[E-noun] 铭[S-verb]  答(answer) [verb] 答[S-noun] 答[B-noun] 答[B2-noun] 答[B3-noun] 答[M-noun] 答[E-noun]  记者(journalist) [noun]  记(record) [verb] 记[S-noun] 记[B-noun] 记[B2-noun] 记[B3-noun] 记[M-noun] 记[E-noun]  者(person) [noun] 者[S-noun] 者[B-noun] 者[B2-noun] 者[B3-noun] 者[M-noun] 者[E-noun]  问 (question) [noun] 问(ask) [verb] 问[S-noun] 问[E-noun] 问[S-verb] 问[E-verb]  Figure 1. A Word-character hybrid lattice of a Chinese sentence. Correct path is represented by bold lines.  Word Length Tags  
This paper is concerned with capturing long-distance dependencies in sequence models. We propose a two-step strategy. First, the stacked learning technique is applied to integrate sequence models that are good at exploring local information and other high complexity models that are good at capturing long-distance dependencies. Second, the structure compilation technique is employed to transfer the predictive power of hybrid models to sequence models via large-scale unlabeled data. To investigate the feasibility of our idea, we study Chinese POS tagging. Experiments on the Chinese Treebank data demonstrate the effectiveness of our methods. The re-compiled models not only achieve high accuracy with respect to per token classiﬁcation, but also serve as a front-end to a parser well. 
In this paper, we present our efforts towards incorporating external knowledge from Hindi WordNet to aid dependency parsing. We conduct parsing experiments on Hindi, an Indo-Aryan language, utilizing the information from concept ontologies available in Hindi WordNet to complement the morpho-syntactic information already available. The work is driven by the insight that concept ontologies capture a speciﬁc real world aspect of lexical items, which is quite distinct and unlikely to be deduced from morpho-syntactic information such as morph, POS-tag and chunk. This complementing information is encoded as an additional feature for data driven parsing and experiments are conducted. We perform experiments over datasets of different sizes. We achieve an improvement of 1.1% (LAS) when training on 1,000 sentences and 0.2% (LAS) on 13,371 sentences over the baseline. The improvements are statistically signiﬁcant at p<0.01. The higher improvements on 1,000 sentences suggest that the semantic information could address the data sparsity problem. 
We investigate the robustness of domain adaptation (DA) representations and methods across target domains using part-ofspeech (POS) tagging as a case study. We ﬁnd that there is no single representation and method that works equally well for all target domains. In particular, there are large differences between target domains that are more similar to the source domain and those that are less similar. 
Light verb constructions (LVCs) are verb and noun combinations in which the verb has lost its meaning to some degree and the noun is used in one of its original senses. They often share their syntactic pattern with other constructions (e.g. verbobject pairs) thus LVC detection can be viewed as classifying certain syntactic patterns as light verb constructions or not. In this paper, we explore a novel way to detect LVCs in texts: we apply a dependency parser to carry out the task. We present our experiments on a Hungarian treebank, which has been manually annotated for dependency relations and light verb constructions. Our results outperformed those achieved by state-of-the-art techniques for Hungarian LVC detection, especially due to the high precision and the treatment of long-distance dependencies. 
Dialog behavior is affected by power relations among the discourse participants. We show that four different types of power relations (hierarchical power, situational power, inﬂuence, and power over communication) affect written dialog behavior in different ways. We also present a system that can identify power relations given a written dialog. 
We present a performance evaluation framework for Spoken Language Understanding (SLU) modules, focusing on three elements: (1) characterization of spoken utterances, (2) experimental design, and (3) quantitative evaluation metrics. We then describe the application of our framework to Scusi?— our SLU system that focuses on referring expressions. 
We offer a noisy channel approach for recognizing and correcting erroneous words in referring expressions. Our mechanism handles three types of errors: it removes noisy input, inserts missing prepositions, and replaces mis-heard words (at present, they are replaced by generic words). Our mechanism was evaluated on a corpus of 295 spoken referring expressions, improving interpretation performance. 
We study the problem of natural language query generation for decision support systems (DSS) in the problem resolution domain. In this domain, a user has a task he is unable to accomplish (eg. bluetooth headphone not playing music), which we capture using language structures. We show how important units that deﬁne a problem can robustly and automatically be extracted from large noisy online forum data, with no labeled data or query logs. We also show how these units can be selected to reduce the number of interactions and how they can be used to generate natural language interactions for query reﬁnement. 
State-of-the-art statistical machine translation systems rely heavily on training data and insufﬁcient training data usually results in poor translation quality. One solution to alleviate this problem is triangulation. Triangulation uses a third language as a pivot through which another sourcetarget translation system can be built. In this paper, we dynamically create multiple such triangulated systems and combine them using a novel approach called ensemble decoding. Experimental results of this approach show signiﬁcant improvements in the BLEU score over the direct sourcetarget system. Our approach also outperforms a strong linear mixture baseline. 
We present a high-precision, languageindependent transliteration framework applicable to bilingual lexicon extraction. Our approach is to employ a bilingual topic model to enhance the output of a state-of-the-art graphemebased transliteration baseline. We demonstrate that this method is able to extract a high-quality bilingual lexicon from a comparable corpus, and we extend the topic model to propose a solution to the out-of-domain problem. 
We present a novel system combination of machine translation and text summarization which provides high quality summary translations superior to the baseline translation of the entire document. We ﬁrst use supervised learning and build a classiﬁer that predicts if the translation of a sentence has high or low translation quality. This is a reference-free estimation of MT quality which helps us to distinguish the subset of sentences which have better translation quality. We pair this classiﬁer with a stateof-the-art summarization system to build an MT-aware summarization system. To evaluate summarization quality, we build a test set by summarizing a bilingual corpus. We evaluate the performance of our system with respect to both MT and summarization quality and, demonstrate that we can balance between improving MT quality and maintaining a decent summarization quality. 
In this paper, we consider the tuning of statistical machine translation (SMT) models employing a large number of features. We argue that existing tuning methods for these models suffer serious sparsity problems, in which features appearing in the tuning data may not appear in the testing data and thus those features may be over tuned in the tuning data. As a result, we face an over-ﬁtting problem, which limits the generalization abilities of the learned models. Based on our analysis, we propose a novel method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efﬁcient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method signiﬁcantly outperforms the state of the art tuning methods. 
Discovering parallel data in comparable corpora is a promising approach for overcoming the lack of parallel texts in statistical machine translation and other NLP applications. In this paper we propose an alternative to comparable corpora of texts as resources for extracting parallel data: a multimodal comparable corpus of audio and texts. We present a novel method to detect parallel phrases from such corpora based on splitting comparable sentences into fragments, called phrases. The audio is transcribed by an automatic speech recognition system, split into fragments and translated with a baseline statistical machine translation system. We then use information retrieval in a large text corpus in the target language, split also into fragments, and extract parallel phrases. We compared our method with parallel sentences extraction techniques. We evaluate the quality of the extracted data on an English to French translation task and show signiﬁcant improvements over a state-ofthe-art baseline. 
Automatically mining named entities (NE) is an important but challenging task, pattern-based and bootstrapping strategy is the most widely accepted solution. In this paper, we propose a novel method for NE mining using web document titles. In addition to the traditional text patterns, we propose to use url-text hybrid patterns that introduce url criterion to better pinpoint high-quality NEs. We also design a multiclass collaborative learning mechanism in bootstrapping, in which different patterns and different classes work together to determine better patterns and NE instances. Experimental results show that the precision of NEs mined with the proposed method is 0.96 and 0.94 on Chinese and English corpora, respectively. Comparison result also shows that the proposed method signiﬁcantly outperforms a representative method that mines NEs from large-scale query logs. 
 Abstract Traditional Event Extraction mainly focuses on event type identification and event participants extraction based on pre-specified event type annotations. However, different domains have different event type paradigms. When transferring to a new domain, we have to build a new event type paradigm. It is a costly task to discover and annotate event types manually. To address this problem, this paper proposes a novel approach of building an event type paradigm by clustering event triggers. Based on the trigger clusters, the event type paradigm can be built automatically. Experimental results on three different corpora – ACE (small, homogeneous, open corpus), Financial News and Musical News (large scale, specific domain, web corpus) indicate that our method can effectively build an event type paradigm and can be easily adapted to new domains. 
Normalizing named entity abbreviations to their standard forms is an important preprocessing task for question answering, entity retrieval, event detection, microblog processing, and many other applications. Along with the quick expansion of microblogs, this task has received more and more attentions in recent years. In this paper, we propose a novel entity abbreviation generation method using ﬁrst-order logic to model long distance constraints. In order to reduce the human effort of manual annotating corpus, we also introduce an automatically training data construction method with simple strategies. Experimental results demonstrate that the proposed method achieves better performance than state-of-the-art approaches. 
The identiﬁcation of light verb constructions (LVC) is an important task for several applications. Previous studies focused on some limited set of light verb constructions. Here, we address the full coverage of LVCs. We investigate the performance of different candidate extraction methods on two English full-coverage LVC annotated corpora, where we found that less severe candidate extraction methods should be applied. Then we follow a machine learning approach that makes use of an extended and rich feature set to select LVCs among extracted candidates. 
We propose using proﬁle compatibility to differentiate genuine and fake product reviews. For each product, a collective proﬁle is derived from a separate collection of reviews. Such a proﬁle contains a number of aspects of the product, together with their descriptions. For a given unseen review about the same product, we build a test proﬁle using the same approach. We then perform a bidirectional alignment between the test and the collective proﬁle, to compute a list of aspect-wise compatible features. We adopt Ott et al. (2011)’s op spam v1.3 dataset for identifying truthful vs. deceptive reviews. We extend the recently proposed N-GRAM+SYN model of Feng et al. (2012a) by incorporating proﬁle compatibility features, showing such an addition signiﬁcantly improves upon their state-ofart classiﬁcation performance. 
Using large corpora of chronologically ordered language, it is possible to explore diachronic phenomena, identifying previously unknown correlations between language usage and time periods, or epochs. We focused on a statistical approach to epoch delimitation and introduced the task of epoch characterization. We investigated the signiﬁcant changes in the distribution of terms in the Google N-gram corpus and their relationships with emotion words. The results show that the method is reliable and the task is feasible. 
While various claims have been made about text in social media text being noisy, there has never been a systematic study to investigate just how linguistically noisy or otherwise it is over a range of social media sources. We explore this question empirically over popular social media text types, in the form of YouTube comments, Twitter posts, web user forum posts, blog posts and Wikipedia, which we compare to a reference corpus of edited English text. We ﬁrst extract out various descriptive statistics from each data type (including the distribution of languages, average sentence length and proportion of out-ofvocabulary words), and then investigate the proportion of grammatical sentences in each, based on a linguistically-motivated parser. We also investigate the relative similarity between different data types. 
In this paper, we present an automatic system to rank participants of an interaction in terms of their relative power. We ﬁnd several linguistic and structural features to be effective in predicting these rankings. We conduct our study in the domain of political debates, speciﬁcally the 2012 Republican presidential primary debates. Our dataset includes textual transcripts of 20 debates with 4-9 candidates as participants per debate. We model the power index of each candidate in terms of their relative poll standings in the state and national polls. We ﬁnd that the candidates’ power indices affect the way they interact with others and the way others interact with them. We obtained encouraging results in our experiments and we expect these ﬁndings to carry across to other genres of multi-party conversations. 
This paper addresses the problem of automatic evaluation of text simpliﬁcation systems for Spanish. We test whether already-existing readability formulae would be suitable for this task. We adapt three existing readability indices (two measuring lexical complexity and one measuring syntactic complexity) to be computed automatically, which are then applied to a corpus of original news texts and their manual simpliﬁcations aimed at people with cognitive disabilities. We show that there is a signiﬁcant correlation between each of the three readability indices and several linguistically motivated features which might be seen as reading obstacles for various target populations. Furthermore, we show that there is a signiﬁcant correlation between the two readability indices which measure lexical complexity. 
Uncertainty is an important linguistic phenomenon that is relevant in many areas of language processing. While earlier research mostly concentrated on the semantic aspects of uncertainty, here we focus on discourse- and pragmaticsrelated aspects of uncertainty. We present a classiﬁcation of such linguistic phenomena and introduce a corpus of Wikipedia articles in which the presented types of discourse-level uncertainty – weasel, hedge and peacock – have been manually annotated. We also discuss some experimental results on discourse-level uncertainty detection. 
This paper presents a methodology to exploit the potential of Arabic Wikipedia to assist in the automatic development of a large Fine-grained Named Entity (NE) corpus and gazetteer. The corner stone of this approach is efﬁcient classiﬁcation of Wikipedia articles to target NE classes. The resources developed were thoroughly evaluated to ensure reliability and a high quality. Results show the developed gazetteer boosts the performance of the NE classiﬁer on a news-wire domain by at least 2 points F-measure. Moreover, by combining a learning NE classiﬁer with the developed corpus the score achieved is a high F-measure of 85.18%. The developed resources overcome the limitations of traditional Arabic NE tasks by more ﬁne-grained analysis and providing a beneﬁcial route for further studies. 
Domain-speciﬁc bilingual lexicons extracted from domain-speciﬁc comparable corpora provide for one term a list of ranked translation candidates. This study proposes to re-rank these translation candidates. We suggest that a term and its translation appear in comparable sentences that can be extracted from domainspeciﬁc comparable corpora. For a source term and a list of translation candidates, we propose a method to identify and align the best source and target sentences that contain the term and its translation candidates. We report results with two language pairs (French-English and FrenchGerman) using domain-speciﬁc comparable corpora. Our method signiﬁcantly improves the top 1, top 5 and top 10 precisions of a domain-speciﬁc bilingual lexicon, and thus, provides a better useroriented results. 
We introduce a novel modality scheme where triggers are words and phrases that convey modality meanings and subcategorize for clauses and verbal phrases. This semanticsyntactic working definition of modality enables us to design practical and replicable annotation guidelines and procedures that alleviate some shortcomings of current purely semantic modality annotation schemes and yield high inter-annotator agreement rates. We use this scheme to annotate a tweet-based Arabic corpus for modality information. This novel language resource, being the first, initiates NLP research on Arabic modality. 
Nowadays in tunisia, the arabic Tunisian Dialect (TD) has become progressively used in interviews, news and debate programs instead of Modern Standard Arabic (MSA). Thus, this gave birth to a new kind of language. Indeed, the majority of speech is no longer made in MSA but alternates between MSA and TD. This situation has important negative consequences on Automatic Speech Recognition (ASR): since the spoken dialects are not officially written and do not have a standard orthography, it is very costly to obtain adequate annotated corpora to use for training language models and building vocabulary. There are neither parallel corpora involving Tunisian dialect and MSA nor dictionaries. In this paper, we describe a method for building a bilingual dictionary using explicit knowledge about the relation between TD and MSA. We also present an automatic process for creating Tunisian Dialect (TD) corpora. 
Phrase-based machine translation like other data driven approaches, are often plagued by irregularities in the translations of words in morphologically rich languages. The phrase-pairs and the language models are unable to capture the long range dependencies which decide the inﬂection. This paper makes the ﬁrst attempt at learning constraints between the language-pair where, the target language lacks rich linguistic resources, by automatically learning classiﬁers that prevent implausible phrases from being part of decoding and at the same time adds consistent phrases. The paper also shows that this approach improves translation quality on the English-Hindi language pair. 
We present a Variational-Bayes model for learning rules for the Hierarchical phrasebased model directly from the phrasal alignments. Our model is an alternative to heuristic rule extraction in hierarchical phrase-based translation (Chiang, 2007), which uniformly distributes the probability mass to the extracted rules locally. In contrast, in our approach the probability assigned to a rule is globally determined by its contribution towards all phrase pairs and results in a sparser rule set. We also propose a distributed framework for efﬁciently running inference for realistic MT corpora. Our experiments translating Korean, Arabic and Chinese into English demonstrate that they are able to exceed or retain the performance of baseline hierarchical phrase-based models. 
Language model is an essential part in statistical machine translation, but traditional n-gram language models can only capture a limited local context in the translated sentence, thus lacking the global information for prediction. This paper describes a novel topic-triggered language model, which takes into account the topical context by estimating the n-gram probability under the given topics and online adjusts language model score according to diﬀerent topic distributions. Experimental results show that our method provides a average improvement of +0.76 Bleu on NIST Chinese-to-English translation task and a reduction in word perplexity of the test-document. 
The imbalanced sentiment distribution of microblogs induces bad performance of binary classiﬁers on the minority class. To address this problem, we present a semisupervised method for sentiment classiﬁcation of Chinese microblogs. This method is similar to self-training, except that, a set of labeled samples is reserved for a conﬁdence scores computing process through which samples that are less than a predeﬁned conﬁdence score threshold are incorporated into training set for retraining. By doing this, the classiﬁer is able to boost the performance on the minority class samples. Experiments on the NLP&CC2012 Chinese microblog evaluation data set demonstrated that reserved self-training outperforms the best run by 2.06% macro-averaged and 2.30% microaveraged F-measure, respectively. 
This paper presents a method of improving lexicon-based review classification by merging multiple sentiment dictionaries, and selectively removing and switching the contents of merged dictionaries. First, we compare the positive/negative book review classification performance of eight individual sentiment dictionaries. Then, we select the seven dictionaries with greater than 50% accuracy and combine their results using (1) averaging, (2) weighted-averaging, and (3) majority voting. We show that the combined dictionaries perform only slightly better than the best single dictionary (65.8%) achieving (1) 67.8%, (2) 67.7%, and (3) 68.3% respectively. To improve this, we combine seven dictionaries at a deeper level by merging the dictionary entry words and averaging the sentiment scores. Moreover, we leverage the skewed distribution of positive/negative threshold setting data to update the merged dictionary by selectively removing the dictionary entries that do not contribute to classification while switching the polarity of selected sentiment scores that hurts the classification performance. We show that the revised dictionary achieves 80.9% accuracy and outperforms both the individual dictionaries and the shallow dictionary combinations in the book review classification task. 
The inherent morphological complexity of languages such as Arabic entails the exploration of language traits that could be valuable to the task of detecting and classifying sentiment within text. This paper investigates the relevance of using the roots of words as input features into a sentiment analysis system under two distinct domains, in order to tailor the task more suitably to morphologically-rich languages such as Arabic. Different wordrooting solutions are employed in conjunction with a basic sentiment classiﬁer, in order to demonstrate the potential of mapping Arabic words to basic roots for a language-speciﬁc development to the sentiment analysis task, showing a noteworthy improvement to baseline performance. 
 Concept extraction is a primary subtask of ontology construction. It is difficult to extract new concepts from traditional text corpus. Moreover, building a single ontology for multiple-topic corpus may lead to misconception. To deal with these problems, this paper proposes a novel framework to extract topical key concepts from folksonomy. Folksonomy is a valuable data source due to real-time update and rich user-generated contents. We first identify topics from folksonomy using topic models. Next the tags are ranked according to their importance for a certain topic by applying topic-specific random walk methods. The top-ranking tags are extracted as topical key concepts. Especially, a novel link weight function which combines the local structure information and global semantic similarity is proposed in importance score propagation. From the perspectives of qualitative and quantitative investigation, our method is feasible and effective.  is much more difficult to find one for a specific topic such as “cult”. Since “cult” movies often do not follow traditional standards of mainstream movies. Moreover, it is not easy for them to find a formal definition and description in text corpus. However, we can more easily find some tags (arbitrary words assigned by people to the resources of interest) to describe this kind of movie, such as cult, non-mainstream, small budgets and so on. Motivated by the fact that social tags give us flexibility and ease to describe a topic, we try to use folksonomy (Trant, 2009) as a new data source. The word ‘folksonomy’ is a blend of the words ‘folk’ and ‘taxonomy’. It is the achievement of collective wisdom derived from the practice of collaboratively creating tags to annotate and categorize web resource.  Website  Resource  
For many NLP applications such as Information Extraction and Sentiment Detection, it is of vital importance to distinguish between synonyms and antonyms. While the general assumption is that distributional models are not suitable for this task, we demonstrate that using suitable features, differences in the contexts of synonymous and antonymous German adjective pairs can be identiﬁed with a simple word space model. Experimenting with two context settings (a simple windowbased model and a ‘co-disambiguation model’ to approximate adjective sense disambiguation), our best model significantly outperforms the 50% baseline and achieves 70.6% accuracy in a synonym/antonym classiﬁcation task. 
We present three approaches to word sense disambiguation that use Wikipedia as a source of sense annotations. Starting from a basic monolingual approach, we develop two multilingual systems: one that uses a machine translation system to create multilingual features, and one where multilingual features are extracted primarily through the interlingual links available in Wikipedia. Experiments on four languages conﬁrm that the Wikipedia sense annotations are reliable and can be used to construct accurate monolingual sense classiﬁers. The experiments also show that the multilingual systems obtain on average a substantial relative error reduction when compared to the monolingual systems. 
Work on information retrieval has shown that language model smoothing leads to more accurate estimation of document models and hence is crucial for achieving good retrieval performance. Several smoothing methods have been proposed in the literature, using either semantic or positional information. In this paper, we propose a uniﬁed proximity-based framework to smooth language models, leveraging semantic and positional information simultaneously in combination. The key idea is to project terms to positions where they originally do not exist (i.e., zero count), which is actually a word count propagation process. We achieve this projection through two proximity-based density functions indicating semantic association and positional adjacency. We balance the effects of semantic and positional smoothing, and score a document based on the smoothed language model. Experiments on four standard TREC test collections show that our smoothing model is effective for information retrieval and generally performs better than the state of the art. 
 We design a language model based on a generative dependency structure for sentences. The parameter of the model is the probability of a dependency N-gram, which is composed of lexical words with four kinds of extra tags used to model the dependency relation and valence. We further propose an unsupervised expectationmaximization algorithm for parameter estimation, in which all possible dependency structures of a sentence are considered. As the algorithm is language-independent, it can be used on a raw corpus from any language, without any part-of-speech annotation, tree-bank or trained parser. We conducted experiments using four languages: English, German, Spanish and Japanese. The results illustrate the applicability and the properties of the proposed approach.  Figure 2: The constituency-based parsing (A) and the dependency-based parsing (B)1 for the English sentence “all things pass”. On the other hand, revealing the structure of a sentence is the task of parsing, which is based on linguistically oriented formulations and focuses on generating the likeliest structure for a given sentence. For this purpose, there are constituencybased and dependency-based formulations (Fig. 2). The former organizes continuous word sequences in a hierarchy of small range to large range groups with linguistically oriented labels; the latter links words with dependency relations2.  
Discriminative models such as logistic regression proﬁt from the ability to incorporate arbitrary rich features; however, complex dependencies among overlapping features can often result in weight undertraining. One popular method that attempts to mitigate this problem is logarithmic opinion pools (LOP), which is a specialized form of product of experts model that automatically adjusts the weighting among experts. A major problem with LOP is that it requires signiﬁcant amounts of domain expertise in designing effective experts. We propose a novel method that learns to induce experts — not just the weighting between them — through the use of a mixed 2 1 norm as previously seen in elitist lasso. Unlike its more popular sibling 1 2 norm (used in group lasso), which seeks feature sparsity at the group-level, 2 1 norm encourages sparsity within feature groups. We demonstrate how this property can be leveraged as a competition mechanism to induce groups of diverse experts, and introduce a new formulation of elitist lasso MaxEnt in the FOBOS optimization framework (Duchi and Singer, 2009). Results on Named Entity Recognition task suggest that this method gives consistent improvements over a standard logistic regression model, and is more effective than conventional induction schemes for experts. 
From an efﬁciency viewpoint, information extraction means to ﬁlter the relevant portions of natural language texts as fast as possible. Given an extraction task, different pipelines of algorithms can be devised that provide the same precision and recall but that vary in their run-time due to different pipeline schedules. While recent research investigated how to determine the run-time optimal schedule for a collection or a stream of texts, this paper goes one step beyond: we analyze the run-times of efﬁcient schedules as a function of the heterogeneity of the texts and we show how this heterogeneity is characterized from a data perspective. For extraction tasks on heterogeneous big data, we present a selfsupervised online adaptation approach that learns to predict the optimal schedule depending on the input text. Our evaluation suggests that the approach will signiﬁcantly improve efﬁciency on collections and streams of texts of high heterogeneity. 
Keyphrase extraction is the task of identifying single or multi-word expressions that represent the main topics of a document. In this paper we present TopicRank, a graph-based keyphrase extraction method that relies on a topical representation of the document. Candidate keyphrases are clustered into topics and used as vertices in a complete graph. A graph-based ranking model is applied to assign a signiﬁcance score to each topic. Keyphrases are then generated by selecting a candidate from each of the topranked topics. We conducted experiments on four evaluation datasets of different languages and domains. Results show that TopicRank signiﬁcantly outperforms state-of-the-art methods on three datasets. 
Queries asked on search engines nowadays increasingly fall in full natural language, which refer to Natural Language queries (NL queries). Parsing that kind of queries for the purpose of understanding user’s query intent is an essential factor to search engine. To this end, a hierarchical structure is introduced to represent the semantic intent of NL query and then we focus on the problem of mapping NL queries to the corresponding semantic intents. We propose a parsing method by conducting two steps as follows: (1) predicting semantic tags for a given input query; (2) building an intent representation for the query using the sequence of semantic tags based on a structured SVM classiﬁcation model. Experimental results on a manually labeled corpus show that our method achieved a sufﬁciently high result in term of precision and F1. 
Sentiment classification is able to help people automatically analyze customers’ opinions from the large corpus. In this paper, we collect some Chinese movie reviews from Bulletin Board System and aim at making sentiment classification so as to extract several frequent opinion words in some movie elements such as plots, actors/actresses, special effects, and so on. Moreover, we result in a general recommendation grade for users. Focusing on the movie reviews in Chinese, we propose a novel procedure which can extract the pairs of opinion words and feature words according to dependency grammar graphs. This parsing-based approach is more suitable for review articles with plenty of words. The grading results will be presented by a 5-grade scoring system. The experimental results show that the accuracy of our system, with the deviation of grades less than 1, is 70.72%, and the Mean Reciprocal Rank (MRR) value is 0.61. When we change the 5-grade scoring system into producing two values: one for recommendation and the other for non-recommendation, we get precision rates 71.23% and 55.88%, respectively. The result shows an exhilarating performance and indicates that our system can reach satisfied expectancy for movie recommendation. 
Sentiment analysis of reviews traditionally ignored the association between the features of the given product domain. The hierarchical relationship between the features of a product and their associated sentiment that influence the polarity of a review is not dealt with very well. In this work, we analyze the influence of the hierarchical relationship between the product attributes and their sentiments on the overall review polarity. ConceptNet is used to automatically create a product specific ontology that depicts the hierarchical relationship between the product attributes. The ontology tree is annotated with feature-specific polarities which are aggregated bottom-up, exploiting the ontological information, to find the overall review polarity. We propose a weakly supervised system that achieves a reasonable performance improvement over the baseline without requiring any tagged training data. 
We propose a novel method to detect cyberbullying entries on the Internet. “Cyberbullying” is deﬁned as humiliating and slandering behavior towards other people through Internet services, such as BBS, Twitter or e-mails. In Japan members of Parent-Teacher Association (PTA) perform manual Web site monitoring called “net-patrol” to stop such activities. Unfortunately, reading through the whole Web manually is an uphill task. We propose a method of automatic detection of cyberbullying entries. In the proposed method we ﬁrst use seed words from three categories to calculate semantic orientation score PMI-IR and then maximize the relevance of categories. In the experiment we checked the cases where the test data contains 50% (laboratory condition) and 12% (real world condition) of cyberbullying entries. In both cases the proposed method outperformed baseline settings. 
Event factuality is information about whether events mentioned in natural language correspond to either actual events that have occurred in the real world or events that are of uncertain interpretation. Factuality analysis is useful for information extraction and textual entailment recognition, among others, but sufﬁcient performance has not yet been achieved by the machine learning-based approach. It is now important to take a closer look at the linguistics phenomena involved in factuality analysis and identify the technical research issues more precisely. In this paper, we discuss issues regarding lexical knowledge through error analysis of a Japanese factuality analyzer based on lexical knowledge and compositionality. 
The context type and similarity calculation are two essential features of a distributional similarity scheme (DSS). In this paper, we propose a hierarchical semanticaware DSS that exploits semantic relation words as extra context information to guide the similarity calculation. First, we deﬁne and extract ﬁve types of semantic relations, and then develop relation-based similarities from the distributional similarities among the top-ranked relation words. Finally, we integrate various similarities using learning-to-rank technique. Experiments show that semantic relations are beneﬁcial to predicting accurate similarity. On 6904 pairwise similarity comparisons, the predictive accuracy of our approach reaches 83.9%, which signiﬁcantly outperforms the baseline approaches. We also conduct intrinsic analysis by varying the quality of semantic relations and the usage of individual similarities. 
Recognizing Textual Entailment (RTE) is to predict whether one text fragment can semantically infer another, which is required across multiple applications of natural language processing. The conventional alignment scheme, which is developed for machine translation, only marks the paraphrases and hyponyms to justify the entailment pairs, while provides less support for the non-entailment ones. This paper proposes a novel alignment scheme, named labeled alignment, to address this problem, which introduces negative links to explicitly mark the contradictory expressions to justify the non-entailment pairs. Thus the alignment-based RTE method employing the proposed scheme, compared with those employing the conventional one, can gain accuracy improvement through actively detecting the signals of non-entailment. The experimental results on the data sets of two shared RTE tasks indicate the implemented system signiﬁcantly outperforms both the baseline system and all the other submitted systems. 
This paper presents a new machine-learning Chinese word segmentation (CWS) approach, which defines CWS as a break-point classification problem; the break point is the boundary of two subsequent words. Further, this paper exploits a support vector machine (SVM) classifier, which learns the segmentation rules of the Chinese language from a context model of break points in a corpus. Additionally, we have designed an effective feature set for building the context model, and a systematic approach for creating the positive and negative samples used for training the classifier. Unlike the traditional approach, which requires the assistance of large-scale known information sources such as dictionaries or linguistic tagging, the proposed approach selects the most frequent words in the corpus as the learning sources. In this way, CWS is able to execute in any novel corpus without proper assistance sources. According to our experimental results, the proposed approach can achieve a competitive result compared with the Chinese knowledge and information processing (CKIP) system from Academia Sinica. 
Feature augmentation is a well-known method for domain adaptation and has been shown to be effective when tested on several NLP tasks (Daume III, 2007). However, a limitation of the method is that it requires labeled data from the target domain and very often such data is unavailable. In this paper, we propose to use training data selection to divide the source domain training data into two parts, pseudo target data (the selected part) and source data (the unselected part), and then apply feature augmentation on the two parts of the training data. This approach has two advantages: ﬁrst, feature augmentation can be applied even when there is no labeled data from the target domain; second, the approach can take advantage of all the training data including the part that is not selected by training data selection. We evaluate the approach on Chinese word segmentation and part-of-speech tagging and show that it outperforms the baseline where no feature augmentation is applied. 
This paper presents a methodology to identify polysemous German prepositions by exploring their vector spatial properties. We apply two cluster evaluation metrics (the Silhouette Value (Kaufman and Rousseeuw, 1990) and a fuzzy version of the V-Measure (Rosenberg and Hirschberg, 2007)) as well as various correlations, to exploit hard vs. soft cluster analyses based on Self-Organising Maps. Our main hypothesis is that polysemous prepositions are outliers, and thus represent either (i) singletons or (ii) marginals of the clusters within a cluster analysis. Our analyses demonstrate that (a) in a subset of the clusterings, singletons have a tendency to contain polysemous prepositions; and (b) misclassiﬁcation and cluster membership rate exhibit a moderate correlation with ambiguity rate. 
In Chinese abbreviation prediction, prior studies are limited on positive full forms. This lab assumption is problematic in realworld applications, which have a large portion of negative full forms (NFFs). We propose solutions to solve this problem of generalized abbreviation prediction. Experiments show that the proposed uniﬁed method outperforms baselines, with the full-match accuracy of 79.4%. Moreover, we apply generalized abbreviation prediction for improving web search quality. Experimental results on web search demonstrate that our method can signiﬁcantly improve the search results, with the search F-score increasing from 35.9% to 64.9%. To our knowledge, this is the ﬁrst study on generalized abbreviation prediction and its application on web search. 
This paper presents a graph-based model that integrates prosodic features into an unsupervised speech summarization framework without any lexical information. In particular it builds on previous work using mutually reinforced random walks, in which a two-layer graph structure is used to select the most salient utterances of a conversation. The model consists of one layer of utterance nodes and another layer of prosody nodes. The random walk algorithm propagates scores between layers to use shared information for selecting utterance nodes with highest scores as summaries. A comparative evaluation of our prosody-based model against several baselines on a corpus of academic multi-party meetings reveals that it performs competitively on very short summaries, and better on longer summaries according to ROUGE scores as well as the average relevance of selected utterances. 
The problem of text summarization for a collection of documents is deﬁned as the problem of selecting a small subset of sentences so that the contents and meaning of the original document set are preserved in the best possible way. In this paper we present a linear model for the problem of text summarization1, where a summary preserves the information coverage as much as possible in comparison to the original document set. We reduce the problem of ﬁnding the best summary to the problem of ﬁnding the point on a convex polytope closest to the given hyperplane, and solve it efﬁciently with the help of fractional (polynomial-time) linear programming. The experimental results show the superiority of our approach over most of the systems participating in the generic multi-document summarization task (MultiLing) of the TAC 2011 competition. 
There are many examples in which a word changes its polarity from domain to domain. For example, unpredictable is positive in the movie domain, but negative in the product domain. Such words cannot be entered in a “universal sentiment lexicon” which is supposed to be a repository of words with polarity invariant across domains. Rather, we need to maintain separate domain speciﬁc sentiment lexicons. The main contribution of this paper is to present an effective method of generating a domain speciﬁc sentiment lexicon. For a word whose domain speciﬁc polarity needs to be determined, the approach uses the Chi-Square test to detect if the difference is signiﬁcant between the counts of the word in positive and negative polarity documents. We extract 274 words that are polar in the movie domain, but are not present in the universal sentiment lexicon. Our overall accuracy is around 60% in detecting movie domain speciﬁc polar words. 
Text mining studies have started to investigae relations between positive and negative opinions and patients’ physical health. Several studies linked the personal lexicon with health and the health-related behavior of the individual. However, few text mining studies were performed to analyze opinions expressed in a large volume of user-written Web content. Our current study focused on performing sentiment analysis on several medical forums dedicated to Hearing Loss (HL). We categorized messages posted on the forums as positive, negative and neutral. Our study had two stages: first, we applied manual annotation of the posts with two annotators and have 82.01% overall agreement with kappa 0.65 and then we applied Machine Learning techniques to classify the posts. 
Emotion is an instinctive state of mind aroused by some specific objects or situation. Exchange of textual information is an important medium for communication and contains a rich set of emotional expressions. The computational approaches to emotion analysis in textual data require annotated lexicons with polarity tags. In this paper we propose a novel method for constructing emotion lexicon annotated with Ekman‟s six basic emotion classes (anger, disgust, fear, happy, sad and surprise). We adopt the Potts model for the probability modeling of the lexical network. The lexical network has been constructed by connecting each pair of words in which one of the two words appears in the gloss of the other. Starting with a small number of emotional seed words, the emotional categories of other words have been determined. With manual checking of top 200 words from each class an average precision of 85.41% has been achieved. 
Can natural language processing be used to predict the likelihood that a musician will commit or has committed suicide? In order to explore this idea, we built a corpus of songs that includes a development set, a training set, and a test set, all consisting of different lyricists. Various vocabulary and syntactic features were then calculated in order to create a suicide/nonsuicide song classiﬁer. The features were input into the Weka machine learning suite and tested with an array of algorithms. We were able to achieve up to a 70.6% classiﬁcation rate with the SimpleCart algorithm, a 12.8% increase over the majority-class baseline. Our ﬁndings suggest that syntactic and vocabulary features are useful indicators of the likelihood that a lyricist will commit or has committed suicide. 
In this study we investigate how we can learn both: (a) syntactic classes that capture the range of predicate argument structures (PASs) of a word and the syntactic alternations it participates in, but ignore large semantic differences in the component words; and (b) syntactico-semantic classes that capture PAS and alternation properties, but are also semantically coherent (a la Levin classes). We focus on Indonesian as our case study, a language that is spoken by more than 165 million speakers, but is nonetheless relatively under-resourced in terms of NLP. In particular, we focus on the syntactic variation that arises with the afﬁxing of the Indonesian sufﬁx -kan, which varies according to the kind of stem it attaches to. 
Supervised training of models for semantic relation extraction has yielded good performance, but at substantial cost for the annotation of large training corpora. Active learning strategies can greatly reduce this annotation cost. We present an efficient active learning framework that starts from a better balance between positive and negative samples, and boosts training efficiency by interleaving self-training and co-testing. We also studied the reduction of annotation cost by enforcing argument type constraints. Experiments show a substantial speed-up by comparison to the previous state-of-the-art pure co-testing active learning framework. We obtain reasonable performance with only 150 labels for individual ACE 2004 relation types. 
Paraphrase extraction relying on a single factor such as distribution similarity or translation similarity might lead to the loss of some linguistic properties. In this paper, we propose a paraphrase extraction framework, which accommodates various linguistically motivated factors to optimize the quality of paraphrase extraction. The major contributions of this study lie in the augmentable paraphrasing framework and the three kinds of factors conducive to both semantic and syntactic correctness. A manual evaluation showed that our model achieves more successful results than the state-of-the-art methods. 1. Introduction Paraphrasing provides an alternative way to express an idea using different words. Early work on paraphrase acquisition has been mainly based on either distributional similarity (e.g., Lin and Pantel, 2001) or the pivot-based approach (e.g., Bannard and Callison-Burch, 2005). Both methods have their strengths and limitations. Distributional similarity is capable of extracting syntactically correct paraphrases, but may risk including antonymous phrases as paraphrases. On the other hand, the pivot approach has the advantage of preserving semantic similarity among the generated paraphrases; however, the quality and quantity of the paraphrases closely correlates with the techniques of bilingual phrase alignment. Considering single factors, existing paraphrasing methods could lose some linguistic properties. In view of this, we attempt to differentiate the importance of the paraphrase  candidates based on various factors. In this paper, we take a graphical view of the paraphrasing issue. To achieve the goal mentioned above, we adopt the Weighted PageRank Algorithm (Xing and Ghorbani, 2004). English phrases are treated as nodes. The edge weights are determined by various factors such as semantic similarity or syntactic similarity between nodes. It means that the performance of the ranked paraphrase candidates depends on the factors we selected and added. In other words, our framework is augmentable and is able to accommodate various factors to optimize the quality of paraphrase extraction. In this case, we propose three linguistically motivated factors to improve the performance of the paraphrase extraction. Lexical distributional similarity is used to ensure that the contexts in which the generated paraphrases appear are similar whereas syntactic distributional similarity is adopted for the purpose of maintaining the syntactic correctness. Translation similarity, one more factor, is capable of preserving semantic equivalence. These three selected factors adopted together effectively achieve better performance on paraphrase extraction. The evaluation shows that our model achieves more satisfactory results than the state-of-the-art pivot-based methods and graph-based methods. 2. Related Work Several approaches have been proposed to extract paraphrases. Earlier studies have focused on extracting paraphrases from monolingual corpora. Barzilay and Mckeown (2001) determine that the phrases in a monolingual parallel corpus are paraphrases of one another only if they appear in similar contexts. Lin and Pantel (2001) derive  706 International Joint Conference on Natural Language Processing, pages 706–711, Nagoya, Japan, 14-18 October 2013.  paraphrases using parse tree paths to compute distributional similarity. Another prominent approach to paraphrase extraction is based on bilingual parallel corpora. For example, Bannard and Callison-Burch (2005) propose the pivot approach to extract phrasal paraphrases from an English-German parallel corpus. With the advantage of its parallel and bilingual natures of such a corpus, the output paraphrases preserve semantic equivalence. Callison-Burch (2008) further places syntactic constraints on extracted paraphrases to improve the quality of the paraphrases. Chan et al. (2011) use monolingual distributional similarity to rank paraphrases generated by the syntactically-constrained pivot method. Recently, some studies take a graphical view of the pivot-based approach. Kok and Brockett (2010) propose the Hitting Time Paraphrase algorithm (HTP) to measure the similarities between phrases. Chen et al. (2012) adopt the PageRank algorithm to find more relevant paraphrases that preserve both meaning and grammaticality for language learners. In this paper, we, similarly, present the state-of-the-art approach as a graph. However, unlike Kok and Brockett (2010), we treat English phrases (instead of multilingual phrases) as nodes. On the other hand, different from Chen et al. (2012), our model is augmentable by involving varied linguistic information or domain knowledge. 3. Method Typically, the state-of-the-art paraphrase extraction models only deal with single factors such as distribution similarity or translation similarity. However, different linguistic factors could facilitate the paraphrase extraction in various ways. With this in mind, we propose an augmentable paraphrase extraction framework based on a graph-based method, which can be modeled with multiple linguistically motivated factors. In the following section, we describe the graph construction (Section 3.1). Then the paraphrase extraction framework is outlined in Section 3.2. Section 3.3 introduces the three factors we proposed for optimizing the quality of paraphrase extraction. Finally, we utilize the grid search method to fine-tune the parameters of our model.  3.1 Graph Construction We transform the paraphrase generation problem into a graph-based problem. First, we generate a graph G≡(V,E), in which an English phrase is a node v ∈ V and two nodes are connected by an edge e ∈ E. A set of paraphrase candidates CP={𝑐𝑝!, 𝑐𝑝!, … , 𝑐𝑝!} is generated for a query phrase q from a bilingual corpus based on the pivot method (Bannard and Callison-Burch, 2005). We further generate a set of transitive paraphrases CP’={ 𝑐𝑝′!, 𝑐𝑝′!, … , 𝑐𝑝′! } of the phrase q, namely, paraphrases 𝑐𝑝! and their paraphrases 𝑐𝑝′! in the same manner. We truncate the paraphrase candidates whose translation similarities are smaller than the threshold ε1; we also exclude 𝑐𝑝! that consists only of a stopword or contains q or is contained in q. Thus, some noisy paraphrases are easily eliminated. Consider the example graph for the query phrase “on the whole” shown in Figure 1. We first find its set of candidate paraphrases CP, including “generally speaking”, “in general”, “in a nutshell”, using the pivot-based method mentioned above. Then for each phrase in CP, we extract the corresponding paraphrases respectively. For example, “in brief”, “broadly speaking”, “in general” are paraphrases of the first phrase “generally speaking” in CP. During the process, we keep the extracted paraphrases whose translation similarities are larger than δ2. By linking the phrases with their transitive paraphrases, the graph G is created. 3.2 Augmentable Paraphrase Extraction Framework In this sub-section, we propose an augmentable paraphrase extraction framework, which can be modeled by multiple factors. Considering a graph G ≡ (V,E), the PageRank algorithm assigns a value PR to each node as their importance measurement. We further adopt the Weighted PageRank algorithm (Xing and Ghorbani, 2004) to state the relatedness between nodes. We calculate the weight 𝑊 of the edge which links node v to node u using various factor functions ℱ!, the weight function is described as follow, ! 𝑊 𝑢, 𝑣 = 𝜆!ℱ! 𝑢, 𝑣, 𝑞 !!!  
We propose a supervised classiﬁcation approach for automatically determining the polarities of medical sentences. Our polarity classiﬁcation approach is contextsensitive, meaning that the same sentence may have differing polarities depending on the context. Using a set of carefully selected features, we achieve 84.7% accuracy, which is signiﬁcantly better than current state-of-the-art for the polarity classiﬁcation task. Our analyses and experiments on a specialised corpus indicate that automatic polarity classiﬁcation of key sentences can be utilised to generate evidence-based recommendations. 
The popularity of microblogging systems has resulted in a new form of Web data – microtext – which is very different from conventional well-written text. Microtext often has the characteristics of informality, brevity, and varied grammar, which poses new challenges in applying traditional clustering algorithms to analyze microtext. In this paper, we propose a novel two-phase approach for clustering streaming microtext, in particular Twitter messages, into event-based clusters. In the online phase, an incremental process is applied to discover base clusters and maintain detailed summary statistics. Upon demand for any user-speciﬁed time horizons, an ofﬂine phase is triggered to merge related clusters together. We demonstrate that our proposed approach can achieve better clustering accuracy than state-ofthe-art methods. Introduction Microtext is a newly emerging type of Web data which is generated in enormous volumes with the proliferation of online microblogging systems. These systems, such as Twitter and Facebook, provide a light-weight, easy form of communication that enables individuals around the globe to share information and express their opinions in ﬂuid and less formal ways. Microtext streams generated from these sites offer a rich source of realtime information about a wide variety of realworld events, ranging from planned occurrences such as political campaigns or sports games, to unexpected incidents such as earthquakes or terrorist riots. To provide insight into user-generated content broadcast in microtext streams, clustering approaches have demonstrated great potential for  identifying what topics people are talking about and tracking how events unfold over time. 
Since the machines become more and more intelligent, it is reasonable to expect the automatic construction of text classiﬁers by given just the objective categories. As trade-off solutions, existing researches usually provide additional information to the category terms to enhance the performance of a classiﬁer. Unique from them, in this paper, we construct the standard corpora from the web by just providing text categories. Since there are millions of manually constructed websites, it is hopeful to ﬁnd out proper text categorization (TC) knowledge. So we directly go to the web and use the hierarchies implied in navigation bars to extract and verify TC resources. By addressing the issues of navigation bar recognition and text ﬁltering, the corpora are constructed for given text categories and the classiﬁers are trained based on them. We conduct our experiments on the large scale of webpages collected from the 500 top English websites on Alexa. The Open Directory Project (ODP) is used as testing corpus. Experimental results show that, being compared with the classiﬁer based on manually labeled corpus, the classiﬁer trained on auto constructed corpora reaches comparable performance for the categories that are well covered by the training corpus. 
The wealth of information present in the World Wide Web has made internet search a de-facto medium for obtaining any required information. Users typically specify short and/or ambiguous queries and expect the answer to appear at the top. Hence, it can be extremely important to produce a diverse but relevant set of results in the precious top k positions. This calls for addressing two types of needs: (i) producing relevant results for queries that are often short and ambiguous and (ii) selecting a set of k diverse results to satisfy different classes of information needs. In this paper, we present a novel technique using a Biconvex optimization formulation as well as adaptations of existing techniques from other areas, for addressing these two problems simultaneously. We propose a graph based iterative method to choose diversiﬁed results. We evaluate these approaches on the QRU (Query Representation and Understanding) dataset used in SIGIR 2011 workshop as well as on the AMBIENT (Ambiguous Entities) dataset and present results on generating diversiﬁed query interpretations. We also compare these approaches against other online systems such as Surf Canyon, Carrot2, Exalead and DBpedia and empirically demonstrate that our system produces competitive results. 
This paper presents our research on automatic question classification for Vietnamese using machine learning approaches. We have experimented with several machine learning algorithms utilizing two kinds of feature groups: bag-of-words and keywords. Our research focuses on two most important tasks which are corpus building and features extraction by crawling data from the Web to build a keyword corpus. The performance of our approach is promising where our system’s precision outperforms the state-of-the-art Tree Kernel approach (Collins and Duffy, 2001) on a Vietnamese question corpus. Keywords keyword collection, machine learning, Vietnamese question classification corpus. 
Ubuntu’s Internet Relay Chat technical support channel has bots that output speciﬁc messages in response to command words from other channel users. These messages can be used to answer frequently-asked questions instead of requiring an expert to (repeatedly) type a lengthy reply. We describe an approach to automatically distinguish bot-answerable questions, which would mitigate this problem. To the best of our knowledge, this is the ﬁrst work on investigating question answering in a multiparticipant chat domain. Our results indicate that for some types of questions, supervised learning algorithms perform well on this task and, in addition, that character n-grams are a better representation than traditional bag-of-words for this task and domain. 
This work aims at constructing a corpus to satisfy such requirements to support research towards professional writing assistance. Our corpus is a collection of scientiﬁc work written by non-native speakers that has been proofread by native English experts. A new annotation scheme, which is based on word-alignments, is then proposed that is used to capture all types of inarticulations and their corrections including both spelling/grammatical error corrections and paraphrases made by proofreaders. The resulting corpus contains 3,485 pairs of original and revised sentences, of which, 2,516 pairs contain at least one articulation. 
The metalinguistic facilities of natural language are crucial to our ability to communicate, but the patterns behind the appearance of metalanguage—and thus the clues for how we may instruct computers to detect it—have remained relatively unknown. This paper describes the first results on the feasibility of automatically identifying metalanguage in English text. A core metalinguistic vocabulary has been identified, supporting intuitions about the phenomenon and aiding in its detection and delineation. These results open the door to applications that can extract the direct, salient information that metalanguage encodes. 
This paper is concerned with the problem of automatic essay grading, where the task is to grade student written essays given course materials and a set of humangraded essays as training data. Latent Semantic Analysis (LSA) has been used extensively over the years to accomplish this task. However, the major limitation of LSA is that it only retains the frequency of words by disregarding the word sequence, and the syntactic and semantic structure of texts. As a remedy, we propose the use of syntactic and shallow semantic tree kernels for grading essays. Experiments suggest that syntactic and semantic structural information can signiﬁcantly improve the performance of the state-of-the-art LSAbased models for automatic essay grading. 
In this paper we take an important step towards completely unsupervised stemming by giving a scheme for semi supervised stemming. The input to the system is a list of word forms and sufﬁxes. The motivation of the work comes from the need to create a root or stem identiﬁer for a language that has electronic corpora and some elementary linguistic work in the form of, say, sufﬁx list. The scope of our work is sufﬁx based morphology, (i.e., no preﬁx or inﬁx morphology). We give two greedy algorithms for stemming. We have performed extensive experimentation with four languages: English, Hindi, Malayalam and Marathi. Accuracy ﬁgures ranges from 80% to 88% are reported for all languages.  This work is applicable to languages with concatenative morphology where sufﬁxes stack one after another. However, problems arise when there are phonemic changes in the boundaries of the stems and sufﬁxes (sandhi). In such situation, existence of fused, composite sufﬁxes in the sufﬁx list is assumed. The roadmap of the paper is as follows. Related work in morphology learning is explained in section 2. Notations and terminologies used in this paper are deﬁned in section 3. In section 4 we deﬁned the stemming problem addressed in this work. Two models for this stemming are proposed in section 5 and section 6. In section 7, we described various experiments conducted. The conclusions and future works are presented in section 8. 2 Related Work  
Semantic Role Labeling (SRL) is an important task since it beneﬁts a wide range of natural language processing applications. Given a sentence, the task of SRL is to identify arguments for a predicate (target verb or noun) and assign semantically meaningful labels to them. Dependency parsing based methods have achieved much success in SRL. However, due to errors in dependency parsing, there remains a large performance gap between SRL based on oracle parses and SRL based on automatic parses in practice. In light of this, this paper investigates what additional information is necessary to close this gap. Is it worthwhile to introduce additional dependency information in the form of N-best parse features, or is it better to incorporate orthogonal nondependency information (base chunk constituents)? We compare the above features in a SRL system that achieves state-of-theart results on the CoNLL 2009 Chinese task corpus. Our ﬁndings suggest that orthogonal information in the form of constituents is much more helpful in improving dependency based SRL in practice. 
Natural language generation systems rely on taxonomic thesauri for tasks such as lexical choice and aggregation. WordNet is one such taxonomy, but it is limited in size. Motivated by the needs of a generation system in the scientiﬁc literature domain, we present a method for building a taxonomic thesaurus from Wikipedia articles, where each article represents a potential concept in the taxonomy. We propose framing the problem of creating a taxonomy as a classiﬁcation task of the potential relations between individual Wikipedia article pairs, and show that a supervised algorithm can achieve high precision in this task with very little training data. 
Chinese time entity is quite complex. In this paper, we give a comprehensive linguistic study on it. Based on the analysis, we present a rule system which only considers the inner structure of Chinese time entities for the recognition. Experiments on Sinica and TempEval-2 corpus show that the rule system performs much better than the CRFs model. When using the rules as features within a CRFs model, the performance could be further improved. 
This paper attempts to identify the importance of sentiment words in ﬁnancial reports on ﬁnancial risk. By using a ﬁnancespeciﬁc sentiment lexicon, we apply regression and ranking techniques to analyze the relations between sentiment words and ﬁnancial risk. The experimental results show that, based on the bag-of-words model, models trained on sentiment words only result in comparable performance to those on origin texts, which conﬁrms the importance of ﬁnancial sentiment words on risk prediction. Furthermore, the learned models suggest strong correlations between ﬁnancial sentiment words and risk of companies. As a result, these ﬁndings are of great value for providing us more insight and understanding into the impact of ﬁnancial sentiment words in ﬁnancial reports. 
This paper addresses the open problem of mathematical term sense disambiguation. We introduce a method that uses a MathML parallel markup corpus to generate relevant training and testing datasets. Based on the dataset generated, we use Support Vector Machine classiﬁer to disambiguate the sense of mathematical terms. Experimental results indicate we can generate such data automatically and with reasonable accuracy. 
Given the relatively small amount of work on event coreference resolution, our understanding of the task is arguably fairly limited. This makes it difficult to determine how to improve an event coreference resolver. We seek to gain a better understanding of the state of the art in event coreference resolution by performing the first publicly available analysis of a Chinese event coreference resolver. 
Named entity recognition in queries is the task of identifying sequences of terms in search queries that refer to a unique concept. This problem is catching increasing attention, since the lack of context in short queries makes this task difﬁcult for full-text off-the-shelf named entity recognizers. In this paper, we propose to deal with this problem in a two-step fashion. The ﬁrst step classiﬁes each query term as token or part of a named entity. The second step takes advantage of these binary labels for categorizing query terms into a pre-deﬁned set of 28 named entity classes. Our results show that our two-step strategy is promising by outperforming a one-step traditional baseline by more than 10%. 
In this paper, we present and compare various centrality measures for graphbased keyphrase extraction. Through experiments carried out on three standard datasets of different languages and domains, we show that simple degree centrality achieve results comparable to the widely used TextRank algorithm, and that closeness centrality obtains the best results on short documents. 
We address the task of improving the quality of lexicon bootstrapping, i.e., of expanding a semantic lexicon on a given corpus. A main problem of iterative bootstrapping techniques is the fact that lexicon quality degrades gradually as more and more false terms are added. We propose to exploit linguistic variation between languages to reduce this problem of semantic drift with a knowledge-lean and language-independent ensemble method. Our results on English and German show that lexicon bootstrapping beneﬁts signiﬁcantly from the multilingual symbiosis. 
Mining compound words and their pronunciations is essential for Japanese input method editors (IMEs). We propose to use a chunk-based dependency parser to mine new words, collocations and predicate-argument phrases from largescale Japanese Web pages and tweets. The pronunciations of the compound words are automatically rewritten by a statistical machine translation (SMT) model. Experiments on applying the mined lexicon to a state-of-the-art Japanese IME system1 show that the precision of Kana-Kanji conversion is signiﬁcantly improved. 
In this paper, we describe a Turkish factoid QA system which uses surface level patterns called answer patterns in order to extract the answers from the documents that are retrieved from the web. The answer patterns are learned using five different answer pattern extraction methods with the help of the web. Our novel approach to extract named entity tagged answer patterns and our new confidence factor assignment approach have an important role in the successful performance of our QA system. We also describe a novel query expansion technique to improve the performance. The evaluation results show that the named entity tagging in answer patterns and the query expansion leads to significant performance improvement. The scores of our QA system are comparable with the results of the best factoid QA systems in the literature. 
People are generating more and more short texts. There is an urgent demand to classify short texts into different domains. Due to the shortness and sparseness of short texts, conventional methods based on Vector Space Model (VSM) have limitations. To tackle the data scarcity problem, we propose a new model to directly measure the correlation between a short text instance and a domain instead of representing short texts as vectors of weights. We firstly draw domain knowledge for each user-defined domain using an external corpus of longer documents. Secondly, the correlation is calculated by measuring the proportion of the overlapping part of the instance and the domain knowledge. Finally, if the correlation is greater than a threshold, the instance will be classified into the domain. Experimental results show that the classifier based on the proposed model outperforms the state-of-the-art baselines based on VSM. 
This paper proposes a keyword extraction process, based on the PageRank algorithm, to reduce noise of input data for measuring semantic similarity. This paper will introduce several features related to implementation and discuss their effects. It will also discuss experimental results which showed significantly improved document retrieval performance with this extraction process in place. 
In this paper, we probe the problem of organization name disambiguation on twitter messages. This task is challenging due to the fact of lacking sufficient information in a tweet message. Instead of conventional methods based on mining external information from web sources to enrich information about organization, we propose to mine the relationship among tweets in data set to utilize context information for disambiguation. With a small scale of labeled tweets, we propose LP-based and TSVM-based semi-supervised methods to classify tweets. We aim to mine both related and non-related information for a given organization. The experiments on WePS-3 show that proposed methods are effective. 
Dictionary editing requires enormous time to dis-cuss whether a word should be listed in a dictionary or not. So as to define a dictionary word, this study employs the number of word users as a novel metrics for selecting a dictionary word. In order to obtain the word user, we used about 0.25 billion tweets of approximately 100,000 people published for five months. This study compared the classification performance of various measures. The result of the experiments revealed that a word in a dictionary is used by numerous users. 
A fundamental issue in opinion mining is to search a corpus for opinion units, which typically comprise the evaluation by an author for a target object from an aspect, such as “This hotel is in a good location”. However, no attempt has been made to address cases where the validity of an evaluation is restricted on a condition in the source text, such as “for traveling with small kids”. In this paper, we propose a method to extract such conditions, namely evaluative conditions, from sentences including opinion units. Our method uses supervised machine learning to determine whether each phrase is a constituent of an evaluative condition. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally. 
Cognates are words in different languages that are associated with each other by language learners. Thus, cognates are important indicators for the prediction of the perceived difﬁculty of a text. We introduce a method for automatic cognate production using character-based machine translation. We show that our approach is able to learn production patterns from noisy training data and that it works for a wide range of language pairs. It even works across different alphabets, e.g. we obtain good results on the tested language pairs English-Russian, English-Greek, and English-Farsi. Our method performs signiﬁcantly better than similarity measures used in previous work on cognates. 
This paper demonstrates that combination of multiple models achieves better classiﬁcation performance than that obtained by existing individual models for question classiﬁcation task in Bengali. We have exploited state of the art multiple model combination techniques, i.e., ensemble, stacking and voting on lexical, syntactical and semantic features of Bengali question for the question classiﬁcation task. Bagging and boosting have been experimented as ensemble techniques. Na¨ıve Bayes, kernel Na¨ıve Bayes, Rule Induction and Decision Tree classiﬁers have been used as base learners. The experimental results show that classiﬁer combination models outperform existing single model approaches. Overall voting approach has achieved maximum classiﬁcation accuracy of 91.65% and outperformed the existing single model approaches (maximum accuracy of 87.63%). 
In this paper, we present a study applying reject option to build a two-stage sentiment polarity classiﬁcation system. We construct a Naive Bayes classiﬁer at the ﬁrst stage and a Support Vector Machine at the second stage, in which documents rejected at the ﬁrst stage are forwarded to be classiﬁed at the second stage. The obtained accuracies are comparable to other state-of-the-art results. Furthermore, experiments show that our classiﬁer requires less training data while still maintaining reasonable classiﬁcation accuracy. 
This work proposes to semantically classify question-like search queries (e.g., “oil based heel creams”) based on the context yielded by preceding search queries in the same user session. Our novel approach is promising as our initial results show that the classiﬁcation accuracy improved in congruence with the number of previous queries used to model the question context. 
Tree-to-tree Statistical Machine Translation models require the use of syntactic tree structures of both the source and target side in learning rules to guide the translation process. In order to accomplish the task, available treebanks for different languages are used as the main resources to collect necessary information to handle the translation task. However, since each treebank has its own deﬁned tags, a barrier is inherently created in highlighting alignment relationships at different syntactic levels for different tag-sets. Moreover, these models are typically over constrained. This paper presents a uniﬁed tagset for all languages at Part-of-Speech and Phrasal Category level in tree-to-tree models. Different experiments are conducted to study for its feasibility, efﬁciency, and translation quality. 
We introduce a method for learning to predict reader interest. In our approach, social interaction content and both syntactic and semantic features of words are utilized. The proposed method involves estimating topical interest preferences and determining the informativity between articles and their social content. In interest prediction, we integrate articles’ quality social feedback representing readers’ opinions into articles to get information which may identify readers’ interests. In addition, semantic aware PageRank is used to find reader interest with the help of word interestingness scores. Evaluations show that PageRank benefits from proposed features and interest preferences inferred across articles. Moreover, results conclude that social interaction content and the proposed selection process help to accurately cover more span of reader interest. 
 News and twitter are sometimes closely correlated, while sometimes each of them has quite independent ﬂow of information, due to the difference of the concerns of their information sources. In order to effectively capture the nature of those two text streams, it is very important to model both their correlation and their difference. This paper ﬁrst models their correlation by applying a time series topic model to the document stream of the mixture of time series news and twitter. Next, we divide news streams and twitter into distinct two series of document streams, and then we apply our model of bursty topic detection based on the Kleinberg’s burst detection model. This approach successfully models the difference of the two time series topic models of news and twitter as each having independent information source and its own concern. 
With the overabundance of online usergenerated content, the ability to ﬁlter based on relevant perspectives is becoming increasingly important. Identifying the perspective of the authors with the review text would enhance the retrieval of pertinent information. This problem can be traditionally formulated as a text classiﬁcation task and solved by annotating the data and building a supervised learning system. However, rare classes might render annotation even more difﬁcult and expensive. Here, we used a distant supervision approach to identify restaurant reviews that were written from the perspective of a vegetarian, and we achieved a macro-average F1 score of 79.40% with minimal annotation effort. 
This paper focuses on the novel task of automatic extraction of phrases related to causes of emotions. The analysis of emotional causes in sentences, where emotions are explicitly indicated through emotion keywords can provide the foundation for research on challenging task of recognition of implicit affect from text. We developed a corpus of emotion causes specific for 22 emotions. Based on the analysis of this corpus we introduce a method for the detection of the linguistic relations between an emotion and its cause and the extraction of the phrases describing the emotion causes. The method employs syntactic and dependency parser and rules for the analysis of eight types of the emotion-cause linguistic relations. The results of evaluation showed that our method performed with high level of accuracy (82%). 
We introduce a novel technique that uses hierarchical phrase-based statistical machine translation (SMT) for grammar correction. SMT systems provide a uniform platform for any sequence transformation task. Thus grammar correction can be considered a translation problem from incorrect text to correct text. Over the years, grammar correction data in the electronic form (i.e., parallel corpora of incorrect and correct sentences) has increased manifolds in quality and quantity, making SMT systems feasible for grammar correction. Firstly, sophisticated translation models like hierarchical phrase-based SMT can handle errors as complicated as reordering or insertion, which were difﬁcult to deal with previously throuh the mediation of rule based systems. Secondly, this SMT based correction technique is similar in spirit to human correction, because the system extracts grammar rules from the corpus and later uses these rules to translate incorrect sentences to correct sentences. We describe how to use Joshua, a hierarchical phrase-based SMT system for grammar correction. An accuracy of 0.77 (BLEU score) establishes the efﬁcacy of our approach. 
This paper studies the performance of different parsers over a large Spanish treebank. The aim of this work is to assess the limitations of state-of-the-art parsers. We want to select the most appropriate parser for Subcategorization Frame acquisition, and we focus our analysis on two aspects: the accuracy drop when parsing out-of-domain data, and the performance over speciﬁc labels relevant to our task. 
Many NLP tasks such as question answering and knowledge acquisition are tightly dependent on dependency parsing. Dependency parsing accuracy is always decisive for the performance of subsequent tasks. Therefore, reducing dependency parsing errors or selecting high quality dependencies is a primary issue. In this paper, we present a supervised approach for automatically selecting high quality dependencies from automatic parses. Experimental results on three different languages show that our approach can effectively select high quality dependencies from the result analyzed by a dependency parser. 
This paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora. We study the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors and augment the standard approach by a Word Sense Disambiguation process. Our aim is to identify the translations of words that are more likely to give the best representation of words in the target language. On two specialized French-English and RomanianEnglish comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach. 
We present a novel scheme of predicate argument structure analysis that can be trained from partially annotated corpora. In order to allow partial annotation, this semantic role labeler does not require word dependency information. The advantage of partial annotation is that it allows for smooth domain adaptation of training data and improves the adaptability to a variety of domains. 
We present a method of statistical dialogue management using a directed intention dependency graph (IDG) in a partially observable Markov decision process (POMDP) framework. The transition probabilities in this model involve information derived from a hierarchical graph of intentions. In this way, we combine the deterministic graph structure of a conventional rule-based system with a statistical dialogue framework. The IDG also provides a reasonable constraint on a user simulation model, which is used when learning a policy function in POMDP and dialogue evaluation. Thus, this method converts a conventional dialogue manager to a statistical dialogue manager that utilizes task domain knowledge without annotated dialogue data. 
This paper proposes an example driven approach to improve the quality of MT system outputs. Specifically, We extend the system combination method in SMT to combine the examples by two strategies: 1) estimating the confidence of examples by the similarity between source input and the source part of examples; 2) approximating target word posterior probability by the word alignments of the bilingual examples. Experimental results show a significant improvement of 0.64 BLEU score as compared to one online translation service (Google Translate). 
We present a phrase-based method to extract parallel fragments from the comparable corpora. We do this by introducing a force decoder based on the hierarchical phrase-based (HPB) translation model to detect the alignments in comparable sentence pairs. This method enables us to extract useful training data for statistical machine translation (SMT) system. We evaluate our method by fragment detection and large-scale translation tasks, which show that our method can effectively extract parallel fragments and improve the performance of the state-of-the-art SMT system.  al., 2010), which embed non-parallel fragments or even lack translations. Consider the comparable sentence pair from Chinese to English in Figure 1. Methods for extracting parallel sentences will bring in noise when these bilingual sentences are retained. But discarding them is also not a wise choice, as there are still some useful parallel fragments as the underlines shown in the ﬁgure. Figure 1: Example of comparable sentence pairs. The parallel fragments are marked by underlines.  
In this paper we present a hybrid approach to resolve Entity-pronoun references in Hindi. While most of the existing approaches, syntactic as well as data-driven, use phrase-structure syntax for anaphora resolution, we explore use of dependency structures as a source of syntactic information. In our approach, dependency structures are used by a rule-based module to resolve simple anaphoric references, while a decision tree classiﬁer is used to resolve more ambiguous instances, using grammatical and semantic features. Our results show that, use of dependency structures provides syntactic knowledge which helps to resolve some speciﬁc types of references. Semantic information such as animacy and Named Entity categories further helps to improve the resolution accuracy. 
We propose a structure cognizant framework for pseudo relevance feedback (PRF). This has an application, for example, in selecting expansion terms for general search from subsets such as Wikipedia, wherein documents typically have a minimally ﬁxed set of ﬁelds, viz., Title, Body, Infobox and Categories. In existing approaches to PRF based expansion, weights of expansion terms do not depend on their ﬁeld(s) of origin. This, we feel, is a weakness of current PRF approaches. We propose a per ﬁeld EM formulation for ﬁnding the importance of the expansion terms, in line with traditional PRF. However, the ﬁnal weight of an expansion term is found by weighting these importance based on whether the term belongs to the title, the body, the infobox or the category ﬁeld(s). In our experiments with four languages, viz., English, Spanish, Finnish and Hindi, we ﬁnd that this structure-aware PRF yields a 2% to 30% improvement in performance (MAP) over the vanilla PRF. We conduct ablation tests to evaluate the importance of various ﬁelds. As expected, results from these tests emphasize the importance of ﬁelds in the order of title, body, categories and infobox. 
We consider the problem of learning how to rank answers across domains in community question answering using stylistic features. Our main contribution is an importance sampling technique for selecting training data per answer thread. Our approach is evaluated across 30 community sites and shown to be signiﬁcantly better than random sampling. We show that the most useful features in our model relate to answer length and overlap with question. 
In this paper, we address the problem of the morphological analysis of an Arabic dialect. We propose a method to adapt an Arabic morphological analyzer for the Tunisian dialect (TD). In order to do that, we create a lexicon for the TD. The creation of the lexicon is done in two steps. The first step consists in adapting a Modern Standard Arabic (MSA) lexicon. We adapted a list of MSA derivation patterns to TD. The second step consists in improving the resulting lists of patterns and roots by using TD specific roots and patterns. The proposed method has been tested and has achieved an Fmeasure performance of 88%. 
Deciding whether a word serves a discourse function in context is a prerequisite for discourse processing, and the performance of this subtask bounds performance on subsequent tasks. Pitler and Nenkova (2009) report 96.29% accuracy (F1 94.19%) relying on features extracted from gold-standard parse trees. This ﬁgure is an average over several connectives, some of which are extremely hard to classify. More importantly, performance drops considerably in the absence of an oracle providing gold-standard features. We show that a very simple model using only lexical and predicted part-of-speech features actually performs slightly better than Pitler and Nenkova (2009) and not signiﬁcantly different from a state-of-the-art model, which combines lexical, part-ofspeech, and parse features. 
Rare term vector replacement (RTVR) is a novel technique for dimensionality reduction. In this paper, we introduce an updating algorithm for RTVR. It is capable of updating both the projection matrix for the reduction and the reduced corpus matrix directly, without having to recompute the expensive projection operation. We introduce an effective batch updating algorithm, and present performance measurements on a subset of the Reuters newswire corpus that show that a 12.5% to 50% split of the documents into corpus and update vectors leads to a three to four fold speedup over a complete rebuild. Thus, we have enabled optimized updating for rare term vector replacement. 
Morphology is the study of internal structure of words and is an essential early step in many NLP applications such as parsing and machine translation. Researchers working in Hindi NLP have either used the widely popular paradigm based analyzer (PBA) or extensions of it. In this work, we undertook a comprehensive evaluation of PBA using the data from the Hindi Treebank (HTB) and presented a new morphological analyzer trained on the HTB. Our morphological analyzer has better coverage and accuracy when compared to the existing analyzers for Hindi. An oracle system that takes the best values from the PBA’s output achieves only 63.41% for lemma, gender, number, person and case. Our statistical analyzer has an accuracy of 84.16% for these morphological attributes when evaluated on the test section of the Hindi Treebank. 
We propose an unsupervised approach to learning non-concatenative morphology, which we apply to induce a lexicon of Arabic roots and pattern templates. The approach is based on the idea that roots and patterns may be revealed through mutually recursive scoring based on hypothesized pattern and root frequencies. After a further iterative refinement stage, morphological analysis with the induced lexicon achieves a root identification accuracy of over 94%. Our approach differs from previous work on unsupervised learning of Arabic morphology in that it is applicable to naturally-written, unvowelled text. 
Finding contradiction text is a fundamental problem in natural language understanding. Previous work on ﬁnding contradiction in text incorporate information derived from predicate-argument structures as features in supervised machine learning frameworks. In contrast to previous work, we combine shallow semantic representations derived from semantic role labeling with binary relations extracted from sentences in a rule-based framework. Evaluation experiments conducted on standard data sets indicated that our system achieves better recall and F1 score for contradiction detection than most of baseline methods, and the same recall as a state of the art supervised method for the task. 
Bilingual lexicons of proper names play a vital role in machine translation and cross-language information retrieval. Word alignment approaches are generally used to construct bilingual lexicons automatically from parallel corpora. Aligning proper names is a task particularly difficult when the source and target languages of the parallel corpus do not share a same written script. We present in this paper a system to transliterate automatically proper names from Arabic to Latin script, and a tool to align single and compound words from English-Arabic parallel texts. We particularly focus on the impact of using transliteration to improve the performance of the word alignment tool. We have evaluated the word alignment tool integrating transliteration of proper names from Arabic to Latin script using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the open source statistical machine translation system Moses. Experiments show that integrating transliteration of proper names into the alignment process improves the Fmeasure of word alignment from 72% to 81% and the translation BLEU score from 20.15% to 20.63%. 
In this paper, we propose a new semisupervised approach for Arabic word sense disambiguation. Using the corpus and Arabic Wordnet1, we define a method to cluster the sentences containing ambiguous words. For each sense, we generate a cluster that we use to construct a semantic tree. Furthermore, we construct a weighted directed graph by matching the tree of the original sentence with semantic trees of each sense candidate. To find the correct sense, we use a similarity score based on three collocation measures that will be classified using a novel voting procedure. The proposed method gives a high rate of recall and precision. 
Simultaneous translation is the challenging task of listening to source language speech, and at the same time, producing target language speech. Human interpreters achieve this task routinely and effortlessly, using different strategies in order to minimize the latency in producing target language. Toward modeling the human interpretation process, we propose a novel input segmentation method using the phrase alignment structure of the language pair. We compare and contrast three incremental decoding and two different input segmentation strategies, including our proposed method, for simultaneous translation. We present accuracy and latency tradeoffs for each of the decoding strategies when applied to audio lectures from the TED collection. 
We focus on improving the translation of the English pronoun it and English reﬂexive pronouns in an English-Czech syntaxbased machine translation framework. Our evaluation both from intrinsic and extrinsic perspective shows that adding specialized syntactic and coreference-related features leads to an improvement in translation quality. 
Beside the word order problem, word choice is another major obstacle for machine translation. Though phrase-based statistical machine translation (SMT) has an advantage of word choice based on local context, exploiting larger context is an interesting research topic. Recently, there have been a number of studies on integrating word sense disambiguation (WSD) into phrase-based SMT. The WSD score has been used as a feature of translation. In this paper, we will show that by bootstrapping WSD models using unlabeled data, we can bootstrap an SMT system. Our experiments on English-Vietnamese translation showed that BLEU scores have been improved signiﬁcantly. 
 2 Related Work  In statistical machine translation, data sparsity is a challenging problem especially for languages with rich morphology and inconsistent orthography, such as Persian. We show that orthographic preprocessing and morphological segmentation of Persian verbs in particular improves the translation quality of Persian-English by 1.9 BLEU points on a blind test set. 
Integrating language resources is a critical part in building natural language processing applications. Processing pipeline and service composition are two approaches for sharing and combining language resources. However, each approach has its drawback. While the former lacks consideration about property rights of language resources, the later is not efﬁcient to process and transfer huge amount of data through web services. In this paper we address the issue of interoperability between two approaches to mutually complement their disadvantages. We show an integration of service composition and processing pipeline, and how the integration can be used to help developers seamlessly build NLP applications. We then present a case study that adopts the integration to integrate two representative frameworks: the Language Grid and UIMA. 
A novel method is proposed for measuring contextual similarity by “weighted overlapping ratio (WOR)” to construct a bilingual dictionary of a new language pair from two bilingual dictionaries sharing one language. The WOR alleviates the effect of a noisy seed dictionary resulting from merger of two bilingual dictionaries via a third language. Combined use of two word-association measures for extracting contexts from corpora is also proposed to compensate their weaknesses. Experiments on constructing a Japanese-Chinese dictionary via English show that the proposed method outperforms the conventional method based on cosine similarity. 
We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES. 
This paper proposes a novel approach to resolve the English article error correction problem, which accounts for a large proportion in grammatical errors. Most previous machine learning based researches empirically collected features which may bring about noises and increase the computational complexity. Meanwhile, the predicted result is largely affected by the threshold setting of a classifier which can easily lead to low performance but hasn’t been well developed yet. To address these problems, we employ genetic algorithm for feature selection and confidence tuning to reinforce the motivation of correction. Comparative experiments on the NUCLE corpus show that our approach could efficiently reduce feature dimensionality and enhance the final F1 value for the article error correction problem. 
We introduce an online framework for discriminative learning problems over hidden structures, where we learn both the latent structure and the classiﬁer for a supervised learning task. Previous work on leveraging latent representations for discriminative learners has used batch algorithms that require multiple passes though the entire training data. Instead, we propose an online algorithm that efﬁciently jointly learns the latent structures and the classiﬁer. We further extend this to include multiple views on the latent structures with different representations. Our proposed online algorithm with multiple views signiﬁcantly outperforms batch learning for latent representations with a single view on a grammaticality prediction task. 
Various successful methods for synonym acquisition are based on comparing context vectors acquired from a monolingual corpus. However, a domain-speciﬁc corpus might be limited in size and, as a consequence, a query term’s context vector can be sparse. Furthermore, even terms in a domain-speciﬁc corpus are sometimes ambiguous, which makes it desirable to be able to ﬁnd the synonyms related to only one word sense. We introduce a new method for enriching a query term’s context vector by using the context vectors of a query term’s translations which are extracted from a comparable corpus. Our experimental evaluation shows, that the proposed method can improve synonym acquisition. Furthermore, by selecting appropriate translations, the user is able to prime the query term to one sense. 
Systems processing on natural language text encounters fatal problems due to long and complex sentences. Their performance degrades as the complexity of the sentence increases. This paper addresses the task of simplifying complex sentences in Hindi into multiple simple sentences, using a rule based approach. Our approach utilizes two linguistic resources viz. verb demand frames and conjuncts’ list. We performed automatic as well as human evaluation of our system. 
This paper presents a joint model for learning morphology and part-of-speech (PoS) tags simultaneously. The proposed method adopts a ﬁnite mixture model that groups words having similar contextual features thereby assigning the same PoS tag to those words. While learning PoS tags, words are analysed morphologically by exploiting similar morphological features of the learned PoS tags. The results show that morphology and PoS tags can be learned jointly in a fully unsupervised setting. 
We report on experiments designed to investigate the role of syntactic features in the task of quality estimation for machine translation, focusing on the effect of parser accuracy. Tree kernels are used to predict the segment-level BLEU score of EnglishFrench translations. In order to examine the effect of the accuracy of the parse tree on the accuracy of the quality estimation system, we experiment with various parsing systems which differ substantially with respect to their Parseval f-scores. We ﬁnd that it makes very little difference which system we choose to use in the quality estimation task – this effect is particularly apparent for source-side English parse trees. 
A variety of methods have been proposed for attribute-value extraction from semistructured text with consistent templates (strict semi-text). However, when the templates in semi-structured text are inconsistent (weak semi-text), these methods will work poorly. To overcome the templateinconsistent problem, in this paper, we proposed a novel method to leverage sitelevel knowledge for attribute-value extraction. First, we use a graph-based random walk model to acquire site-level knowledge. Then we utilize such knowledge to identify weak semi-text in each page and extract attribute-value pairs. The experiments show that, comparing to the baseline method which does not utilize sitelevel knowledge, our method can improve the extraction performance signiﬁcantly. 
We examine several quantitative techniques of authorship attribution that have gained importance over the time and compare them with the current state of the art Z-score based technique. In this paper we show how comparable the existing techniques can be to the Z-score based method, simply by tuning the parameters. We try to ﬁnd the optimum values for number of terms, smoothing parameter value and the minimum number of texts required for creating an author proﬁle. 
Classical approaches to sentiment classiﬁcation exploit only textual features in a given review and are not aware of the personality of the user or the public sentiment toward the target product. In this paper, we propose a model that can accurately estimate the sentiment polarity by referring to the user leniency and product popularity computed during testing. For decoding with this model, we adopt an approximate strategy called “two-stage decoding.” Preliminary experimental results on two realworld datasets show that our method signiﬁcantly improves classiﬁcation accuracy over existing state-of-the-art methods. 
The paper addresses the problem of text variation which often hinders interoperable use or reuse of corpora and annotations. A systematic solution is presented based on a variation of Longest Common Sequence algorithm. An empirical experiment with 20 full text articles shows it works well with a real world application. 
In this paper, we propose a semantic naïve Bayes classifier (SNBC) to improve the conventional naïve Bayes classifier (NBC) by incorporating “document-level” semantic information for document classification (DC). To capture the semantic information from each document, we develop semantic feature extraction and modeling algorithms. For semantic feature extraction, we first apply a log-Bilinear document modeling (LBDM) algorithm to transform each word into a semantic vector, and then apply principal component analysis (PCA) to the semantic space formed by the word vectors to extract a set of semantic features for each document. For semantic modeling, a semantic model is constructed using the semantic features of the training documents. In the testing phase, SNBC systematically integrates the semantic model and the conventional NBC to perform DC. The results of experiments on the 20 News-groups and WebKB datasets confirm that, with the semantic score, SNBC consistently outperforms NBC with various language modeling approaches. 
We propose a novel approach to abstractive Web summarization based on the observation that summaries for similar URLs tend to be similar in both content and structure. We leverage existing URL clusters and construct per-cluster word graphs that combine known summaries while abstracting out URL-speciﬁc attributes. The resulting topology, conditioned on URL features, allows us to cast the summarization problem as a structured learning task using a lowest cost path search as the decoding step. Early experimental results on a large number of URL clusters show that this approach is able to outperform previously proposed Web summarizers. 
Existing methods for collecting texts from endangered languages are not creating the quantity of data that is needed for corpus studies and natural language processing tasks. This is because the process of transcribing and translating from audio recordings is too onerous. A more effective method, we argue, is to involve local speakers in the ﬁeld location, using an audio-only translation interface that is portable and easy to use. We present encouraging early results of an experimental investigation of the efﬁciency of creating translations using this method, and report on the quality of the resulting content.  2010). In addition, the typical workﬂow necessitates the creation of transcriptions before any annotations can be made, for example in ELAN,1 a popular software tool for linguistic annotation (Berez, 2007). We propose to add a new path to this workﬂow (see Figure 1) to facilitate crowdsourcing of translations, whether by local (typically village-based) speakers or by geographically distributed speakers from the diaspora (Reiman, 2010; Bird, 2010; Zaidan and Callison-Burch, 2011). To this end, we have developed mobile phone software with easyto-use interfaces for collecting oral translations.  
Recognizing Chinese Named entities from the Web is challenging, due to the lack of labeled data and differences between Chinese and English. We propose a semisupervised approach which leverages seed entities and the large unlabeled data to learn templates. Some high-quality templates are generated iteratively to extract new named entities based on the model of quality metrics. Experimental results show that our approach signiﬁcantly outperforms the baseline method and it is robust against the changes of the Web. 
Although parallel sentences rarely exist in quasi–comparable corpora, there could be parallel fragments that are also helpful for statistical machine translation (SMT). Previous studies cannot accurately extract parallel fragments from quasi–comparable corpora. To solve this problem, we propose an accurate parallel fragment extraction system that uses an alignment model to locate the parallel fragment candidates, and uses an accurate lexicon ﬁlter to identify the truly parallel ones. Experimental results indicate that our system can accurately extract parallel fragments, and our proposed method signiﬁcantly outperforms a state–of–the–art approach. Furthermore, we investigate the factors that may affect the performance of our system in detail. 
We propose a simple and effective method to build a meta-level Statistical Machine Translation (SMT), called meta-SMT, for system combination. Our approach is based on the framework of Stacked Generalization, also known as Stacking, which is an ensemble learning algorithm, widely used in machine learning tasks. First, a collection of base-level SMTs is generated for obtaining a meta-level corpus. Then a meta-level SMT is trained on this corpus. In this paper we address the issue of how to adapt stacked generalization to SMT. We evaluate our approach on Englishto-Persian machine translation. Experimental results show that our approach leads to significant improvements in translation quality over a phrase-based baseline by about 1.1 BLEU points. 
We present a novel approach to learning phrasal inversion transduction grammars via Bayesian MAP (maximum a posteriori) or information-theoretic MDL (minimum description length) model optimization so as to incorporate simultaneously the choices of model structure as well as parameters. In comparison to most current SMT approaches, the model learns phrase translation lexicons that (a) do not require enormous amounts of run-time memory, (b) contain significantly less redundancy, and (c) provide an obvious basis for generalization to abstract translation schemas. Model structure choice is biased by a description length prior, while parameter choice is driven by data likelihood biased by a parameter prior. The search over possible model structures is made feasible by a novel top-down rule segmenting heuristic which efficiently incorporates estimates of the posterior probabilities. Since the priors reward model parsimony, the learned grammar is very concise and still performs significantly better than the maximum likelihood driven bottom-up rule chunking baseline. 
Previous research on quality estimation for machine translation has demonstrated the possibility of predicting the translation quality of well-formed data. We present a ﬁrst study on estimating the translation quality of user-generated content. Our dataset contains English technical forum comments which were translated into French by three automatic systems. These translations were rated in terms of both comprehensibility and ﬁdelity by human annotators. Our experiments show that tried-and-tested quality estimation features work well on this type of data but that extending this set can be beneﬁcial. We also show that the performance of particular types of features depends on the type of system used to produce the translation. 
In this paper, we propose a selective combination approach of pivot and direct statistical machine translation (SMT) models to improve translation quality. We work with Persian-Arabic SMT as a case study. We show positive results (from 0.4 to 3.1 BLEU on different direct training corpus sizes) in addition to a large reduction of pivot translation model size. 
Incorporating semantic information in the Statistical Machine Translation (SMT) framework is starting to gain some popularity in both the semantics and translation communities. In this paper, we present encouraging results obtained from experiments conducted on English to Arabic SMT system using static, dynamic, and hybrid integration of fine-grained Multiword Expression (MWE). We achieve an improvement up to 0.82 absolute BLEU score by integrating MWEs over a vanilla SMT system. We empirically show that different MWE types require different integration methods in the SMT framework. 
In this paper we investigate the application of uncertainty detection to text watermarking, a problem where the aim is to produce individually identiﬁable copies of a source text via small manipulations to the text (e.g. synonym substitutions). As previous attempts showed, accurate paraphrasing is challenging in an open vocabulary setting, so we propose the use of the closed word class of uncertainty cues. We demonstrate that these words are promising for text watermarking as they can be accurately disambiguated (from the noncue uses of the same words) and their substitution with other cues has marginal impact to the meaning of the text. 
Chinese Input Method Engine (IME) plays an important role in Chinese language processing. However, it has been subjected to lacking a proper evaluation metric for a long time. The natural metric for IME is user experience, which is a rather vague goal for research purpose. We propose a novel approach of quantifying user experience by using keystroke count and then correspondingly develop a framework of IME evaluation, which is fast and accurate. With the underlying linguistic background, the proposed evaluation framework can properly model the user behavior as Chinese is input through English keyboard. It is helpful to point out a way to improve the current Chinese IME performance. 1 
In this paper we present results for two tasks: social event detection and social network extraction from a literary text, Alice in Wonderland. For the ﬁrst task, our system trained on a news corpus using tree kernels and support vector machines beats the baseline systems by a statistically signiﬁcant margin. Using this system we extract a social network from Alice in Wonderland. We show that while we achieve an F-measure of about 61% on social event detection, our extracted unweighted network is not statistically distinguishable from the un-weighted gold network according to popularly used network measures. 
This paper describes the construction of a Japanese Input Method Editor (IME) system for mobile devices, using the largescale Web pages. We provide the training process of our IME model, n-pos model for local Kana-Kanji conversion and ngram model for online cloud service. Especially, we propose an online algorithm of mining new compound words, together with the detailed post-ﬁltering process to prune the billion level entries to be used in mobile services. Experiments show that our IME system outperforms two state-ofthe-art Japanese IME baselines. We have released our system in a completely free form1 and the system is currently used by millions of users. 
This paper presents a novel approach to categorize, model and identify contextual information in natural language interface to database (NLIDB) systems. The interactions between user and system are categorized and modeled based on the way in which the contextual information is utilized in the interactions. A relationship schema among the responses (user and system responses) is proposed. We present a novel method to identify contextual information in one speciﬁc type of usersystem interaction. We report on results of experiments with the university related queries. 
We show that an agent with fairly good social conversational abilities can be built based on a limited number of topics and dialogue strategies if it is tailored to its intended users through a high degree of user involvement during an iterative development process. The technology used is pattern matching of question-answer pairs, coupled with strategies to handle: followup questions, utterances not understood, abusive utterances, repetitive utterances, and initiation of new topics. Introduction Social aspects of conversations with agents, such as small talk and narrative storytelling, can have a positive effect on peoples general interest in interacting with it and help build rapport (Bickmore, 2003). It can also be utilised to develop a relationship and establishing trust or the expertise of the agent (Bickmore and Cassell, 1999). We are interested in exploring if and how these and other effects transfer to an educational setting where children and teenagers interact with pedagogical agents in virtual learning environments. We see several reasons to incorporate social conversation with such agents, for example, it allows for cognitive rest, it can increase overall engagement and receptivity and it can make students feel more at ease with a learning task or topic (Silvervarg et al., 2010). There has, however, been few attempts to understand the users’ behaviour in social conversations with pedagogical agents (Veletsianos and Russell, 2013) and embodied conversational agents (Robinson et al., 2008). In this paper we report on how we iteratively have worked with addressing the questions of 1) what do users talk about during social conversation with a pedagogical agent, 2) how do users  talk during social conversation with a pedagogical agent, 3) how does the answers to 1) and 2) affect the dialogue functions needed to implement social conversation with a pedagogical agent. A social conversational pedagogical agent Our work extends a virtual learning environment with an educational math game named ”The Squares Family” (Pareto et al., 2009). A crucial part of the environment is a pedagogical agent, or more speciﬁcally a teachable agent (Biswas et al., 2001). While the student is playing the game, the agent ”learns” the rules of the game in two ways, by observation or through on-task multiple choice questions answered by the user. A teachable agent is independent and can act on its own, yet is dependent on the student to learn rules and strategies. The intended users are 12-14-year-old students, and the teachable agent is designed as having the same age or slightly younger. The conversational module for off-task conversations has been developed as a rather independent module of the learning environment. Off-task conversation is based on a character description of the teachable agent that is consistent with the overall role of the agent as a peer in the environment. The challenge can be seen as a question of managing the students’ expectations on the agent’s abilities. Our approach was to frame and guide the interaction with the student in such a way that, ideally, the shortcomings and knowledge gaps of the agent never become a critical issue for achieving a satisfying communication. We have therefore chosen to work with user-centred agile system development methods to be able to capture the users’ behaviour and tailor the agent’s conversational capabilities to meet their expectations. This includes combining focus group interviews and Wizard-ofOz role-play (Dahlba¨ck et al., 1993) with development and evaluation of prototypes, surveys and analyses of natural language interaction logs.  1223 International Joint Conference on Natural Language Processing, pages 1223–1229, Nagoya, Japan, 14-18 October 2013.  The off-task conversation is implemented using a slightly extended version of AIML, Artiﬁcial Intelligence Markup Language (Wallace, 2010). AIML works on the surface level and map user utterances to system responses. User utterances can consist of words, which in turn consist of letters, numerals, and the wildcards and *, which function like words. Synonyms are handled using substitutions and grammatical variants through several different patterns for the same type of question and topic. Responses consist in their simplest form of only plain text. It is also possible to set or get data in variables and predicates, give conditional responses, choose a random response from a set of responses, and combinations of these. AIML also allows for handling a limited context by either referring to the systems last utterance or a topic that span multiple exchanges. Prototype 1 In the ﬁrst iteration an agent persona was developed through focus groups with 20 target users. The persona sketch formed the basis for WOzstyle role play, in which students simulated offtask conversations in the game. Three students played the part of the agent, and four students played the role of the user. The resulting 12 dialogues were analysed according to topics, linguistic style and dialogue phenomenon. A number of new topics emerged that had not been brought up in the focus groups. The linguistic style of the utterances could be characterised as grammatical, short sentences, with the use of smileys and ”chatexpressions”. The dialogue mostly consisted of unconnected question and answer pairs, but some instances of connected dialogue with 3-4 turns occurred. The initiative was evenly distributed between user and system. There were frequent use of elliptical expressions, mostly questions of the type ”what about you”, but no anaphora. Based on these ﬁndings the ﬁrst prototype implemented basic question-answer pairs, a strategy for follow-up questions from the agent and user, a topic model with 6 topics that could be initiated by the agent (to allow for mixed-initiative dialogue), and a very simple strategy to handle utterances that the agent could not understand. To handle variations in user input (choice of words and grammatical variations) the system used substitutions where, for example, synonyms and hyponyms  were substituted for a ”normalised” word, and variations of patterns that used the ”normalised” keywords. The agent’s replies were sometimes randomly chosen from a set of 3-5 variants to get some variation if the user asked the same question several times. Follow-up questions where randomly attached to half of the answers the agent gave to questions from the user. When the agent did not understand a user utterance it said so three out of four times, but in one out of four it instead initiated a new topic and posed a question to the user. The agent also initiated a new topic when the user gave an acknowledgement, such as ok, after an answer from the agent. To evaluate the system a total of 27 students tested the prototype. After a short introduction to the project and the system they played the game for 10 minutes, chatted with the agent for 5 minutes, then played the game for 5 minutes and chatted for 5 minutes again. Analysis of the corpus showed that failed interpretations had to be dealt with. Many of the failed interpretations were due to linguistic variations on known topics, and most of all acknowledgments, but also greetings and follow-up questions. Topics also needed to be expanded, both new topics, for example age, food, pets, favourite colour and cars, but also more subtopics related to, for example, computer games, school subjects, and TV. Topics initiated by the agent were proved to be a good strategy and implied that the model should be expanded with new topics and more sub-topics. If the agent could initiate more topics it would both make the dialogue more fun but also help the user to know what type of topics the agent could talk about. A recovery strategy to deal with utterances that the system still were not able to interpret correctly, for example utterances out of scope, was also needed to help the user understand what they could talk about and avoid losing track of the conversation. The recovery strategy should also include ways of breaking long sequences of utterances that included, for example, random letters and ”Ok” sub-dialogues, and steer the dialogue back to a topic. Evaluation of the prototype also showed that hazing, testing and ﬂaming occurred rather frequently, for example comments and questions regarding the agent’s appearance, intellectual capacities, and sexual orientation and activities. To fend this off, the agent also needed to recognise such utterances and have a strategy to direct the dialogue  1224  in a more productive direction. Prototype 2 The second prototype expanded the agent’s dialogue functionality with a recovery strategy for utterances not understood by the agent, a strategy to handle abusive utterances, a strategy for delayed follow-up questions by the user (e.g. Agent: How are u? User: Fine. Agent: Ok. User: wbu?), an extended topic model, a better model for agent initiative and ﬂow of the dialogue, and extended vocabulary. The recovery strategy for utterances not understood by the agent was implemented similar to the one used by SGT Star (Artstein et al., 2009); ﬁrst a simple ”clariﬁcation”, utterance 2 in Figure 1, then, a ”stall”, followed by a ”move on”, and ﬁnally a ”prompt” that directs the user to a new topic. In our system the agent stall is a general prompt for the user to initiate a new topic, utterance 4 in Figure 1, if this fails the agent proposes a topic, utterance 6 in Figure 1, and as a last resort takes the initiative and asks a question, utterance 8 in Figure 1. 
In this paper, we propose a morphological disambiguation method for Turkish, which is an agglutinative language. We use a hybrid method, which combines statistical information with handcrafted rules and learned rules. Five different steps are applied for disambiguation. In the first step, the most likely tags of words are selected. In the second step, we use handcrafted rules to constrain possible parses or select the correct parse. Next, the most likely tags are selected for still ambiguous words according to the suffixes of the words that are unseen in the training corpus. Then, we use transformation-based rules that are learned by a variation of Brill tagger. If the word is still ambiguous, we use some heuristics for the disambiguation. We constructed a hand-tagged dataset for training and applied a ten-fold cross validation with this dataset. We obtained 93.4% accuracy on the average when whole morphological parses are considered in calculation. The accuracy increased to 94.1% when only part-of-speech tags and inflections of last derivations are considered. Our accuracy is 96.9% in terms of part-of-speech tagging. 
In this paper we propose an approach to dynamically compute a confusion score for dependency arc labels, in typed dependency parsing framework. This score accompanies the parsed output and aims to administer an informed account of parse correctness, detailed down to each edge of the parse. The methodology explores the confusion encountered by the oracle of a data driven parser, in predicting an arc label. We support our hypothesis by empirically illustrating, for 20 languages, that the labels with a high confusion score are notably the predominant parsing errors. 
Bilingual corpora offer a promising bridge between resource-rich and resource-poor languages, enabling the development of natural language processing systems for the latter. English is often selected as the resource-rich language, but another choice might give better performance. In this paper, we consider the task of unsupervised cross-lingual POS tagging, and construct a model that predicts the best source language for a given target language. In experiments on 9 languages, this model improves on using a single ﬁxed source language. We then show that further improvements can be made by combining information from multiple source languages. 
Information Structure (IS) determines the “communicative” segmentation of the meaning of an utterance, which makes it central to the semantics–syntax– intonation interface and therefore also to NLP. Despite this relevance, IS has not received much attention in the context of the majority of the reference treebanks for data-driven NLP that already contain a semantic and syntactic layers of annotation. We present our work in progress on the annotation of the Penn TreeBank with the thematicity dimension of the IS as deﬁned in the Meaning-Text Theory. We experiment with tagging and transitionbased parsing techniques. Especially the latter achieve acceptable accuracy with even very small training samples, which is promising for languages with scarce resources. 
This paper gives an Abstract Categorial Grammar (ACG) account of (Kallmeyer and Kuhlmann, 2012)’s process of transformation of the derivation trees of Tree Adjoining Grammar (TAG) into dependency trees. We make explicit how the requirement of keeping a direct interpretation of dependency trees into strings results into lexical ambiguity. Since the ACG framework has already been used to provide a logical semantics from TAG derivation trees, we have a uniﬁed picture where derivation trees and dependency trees are related but independent equivalent ways to account for the same surface–meaning relation. 
Named entities (NE) are important information carrying units within documents. Named Entity extraction (NEX) task consists of automatic construction of a list of phrases belonging to each NE of interest. NEX is important for domains which lack a corpus with tagged NEs. We present an enhanced version and improved results of our unsupervised (bootstrapping) NEX technique (Patil et al., 2013) and establish its domain independence using experimental results on corpora from two different domains: agriculture and mechanical engineering (IC engine1 parts). We use a new variant of Multiword Expression Distance (MED) (Bu et al., 2010) to quantify proximity of a candidate phrase with a given NE type. MED itself is an approximation of the information distance (Bennett et al., 1998). Efﬁcacy of our method is shown using experimental comparison with pointwise mutual information (PMI), BASILISK and KNOWITALL. Our method discovered 8 new plant diseases which are not found in Wikipedia. To the best of our knowledge, this is the ﬁrst use of NEX techniques for agriculture and mechanical engineering (engine parts) domains. 
In this paper we introduce a feature-based neural language model, which is trained to estimate the probability of an element given its previous context features. In this way our feature-based language model can learn representation for more sophisticated features. We introduced the deep neural architecture into the Chinese Word Segmentation task. We got a signiﬁcant improvement on segmenting performance by sharing the pre-learned representation of character features. The experimental result shows that, while using the same feature sets, our neural segmentation model has a better segmenting performance than CRF-based segmentation model. 
Previous research shows that Kalman filter based human-computer interactive Chinese word segmentation achieves an encouraging effect in reducing user interventions, but suffers from the drawback of incompetence in distinguishing segmentation ambiguities. This paper proposes a novel approach to handle this problem by using an adaptive Dirichlet process mixture model. By adjusting the hyperparameters of the model, ideal classifiers can be generated to conform to the interventions provided by the users. Experiments reveal that our approach achieves a notable improvement in handling segmentation ambiguities. With knowledge learnt from users, our model outperforms the baseline Kalman filter model by about 0.5% in segmenting homogeneous texts. 
If we compare the widely used Conditional Random Fields (CRF) with newly proposed “deep architecture” sequence models (Collobert et al., 2011), there are two things changing: from linear architecture to non-linear, and from discrete feature representation to distributional. It is unclear, however, what utility nonlinearity offers in conventional featurebased models. In this study, we show the close connection between CRF and “sequence model” neural nets, and present an empirical investigation to compare their performance on two sequence labeling tasks – Named Entity Recognition and Syntactic Chunking. Our results suggest that non-linear models are highly effective in low-dimensional distributional spaces. Somewhat surprisingly, we ﬁnd that a nonlinear architecture offers no beneﬁts in a high-dimensional discrete feature space. 
Many NLP tools are released as programs that include statistical models. Unfortunately, the models do not always match the documents that the tool user is interested in, which forces the user to update the models. In this paper, we investigate model adaptation under the condition that users cannot access the data used in creating the original model. Transfer learning and online learning are investigated as adaptation strategies. We test them on the category classiﬁcation of Japanese newspaper articles. Experiments show that both transfer and online learning can appropriately adapt the original model if the dataset for adaptation contains all data, not just the data that cannot be well handled by the original model. In contrast, we conﬁrmed that the adaptation fails if the dataset contains only erroneous data as indicated by the original model. 
Recently, translation scholars have made some general claims about translation properties. Some of these are source language independent while others are not. Koppel and Ordan (2011) performed empirical studies to validate both types of properties using English source texts and other texts translated into English. Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties. In this paper, we are validating both types of translation properties using original and translated texts from six European languages. 
This paper describes a new method, based on information theory, for measuring sentence similarity. The method ﬁrst computes the information content (IC) of dependency triples using corpus statistics generated by processing the Open American National Corpus (OANC) with the Stanford Parser. We deﬁne the similarity of two sentences as a function of (1) the similarity of their constituent dependency triples, and (2) the position of the triples in their respective dependency trees. We apply the algorithm to 15 pairs of sentences that were also given to human subjects to assign a similarity score. The human- and computer-generated scores are compared; the results are promising, but point to the need for further reﬁnement. 
Unsupervised Relation Extraction (URE) methods automatically discover semantic relations in text corpora of unknown content and extract for each discovered relation a set of relation instances. Due to the sparsity of the feature space, URE is vulnerable to ambiguities and underspeciﬁcation in patterns. In this paper, we propose to increase the discriminative power of patterns in URE using selectional restrictions (SR). We propose a method that utilizes a Web-derived soft clustering of n-grams to model selectional restrictions in the open domain. We comparatively evaluate our method against a baseline without SR, a setup in which standard 7class Named Entity types are used as SR and a setup that models SR using a ﬁnegrained entity type system. Our results indicate that modeling SR into patterns signiﬁcantly improves the ability of URE to discover relations and enables the discovery of more ﬁne-granular relations. 
We address the task of bootstrapping a semantic lexicon from a list of seed terms and a large corpus. By restricting to a small subset of semantically strong patterns, i.e., coordinations, we improve results signiﬁcantly. We show that the restriction to coordinations has several additional beneﬁts, such as improved extraction of multiword expressions, and the possibility to scale up previous efforts. 
This paper proposes state-of-the-art models for time-event relation extraction (TERE). The models are speciﬁcally designed to work effectively with relations that span multiple sentences and paragraphs, i.e., inter-sentence TERE. Our main idea is: (i) to build a computational representation of the context of the two target relation arguments, and (ii) to encode it as structural features in Support Vector Machines using tree kernels. Results on two data sets – Machine Reading and TimeBank – with 3-fold crossvalidation show that the combination of traditional feature vectors and the new structural features improves on the state of the art for inter-sentence TERE by about 20%, achieving a 30.2 F1 score on intersentence TERE alone, and 47.2 F1 for all TERE (inter and intra sentence combined). 
This paper describes an unsupervised method for extracting product attributes and their values from an e-commerce product page. Previously, distant supervision has been applied for this task, but it is not applicable in domains where no reliable knowledge base (KB) is available. Instead, the proposed method automatically creates a KB from tables and itemizations embedded in the product’s pages. This KB is applied to annotate the pages automatically and the annotated corpus is used to train a model for the extraction. Because of the incompleteness of the KB, the annotated corpus is not as accurate as a manually annotated one. Our method tries to ﬁlter out sentences that are likely to include problematic annotations based on statistical measures and morpheme patterns induced from the entries in the KB. The experimental results show that the performance of our method achieves an average F score of approximately 58.2 points and that ﬁlters can improve the performance. 
Determining the stance expressed in a post written for a two-sided debate in an online debate forum is a relatively new and challenging problem in opinion mining. We seek to gain a better understanding of how to improve machine learning approaches to stance classiﬁcation of ideological debates, speciﬁcally by examining how the performance of a learning-based stance classiﬁcation system varies with the amount and quality of the training data, the complexity of the underlying model, the richness of the feature set, as well as the application of extra-linguistic constraints. 
Virtually all the commonly-used evaluation metrics for entity coreference resolution are linguistically agnostic, treating the mentions to be clustered as generic rather than linguistic objects. We argue that the performance of an entity coreference resolver cannot be accurately reﬂected when it is evaluated using linguistically agnostic metrics. Consequently, we propose a framework for incorporating linguistic awareness into commonly-used coreference evaluation metrics. 
Social networking sites such as Facebook and Twitter have become favorite portals for users to discuss and express opinions. Research shows that topical discussions around events tend to evolve socially on microblogs. However, sources like Twitter have no explicit discussion thread which will link semantically similar posts. Moreover, the discussion may be evolving in multiple different threads (like Facebook). Researchers have proposed the use of online contemporary documents to act as external corpus to connect pairs of contextually related semantic topics. This motivates the question: given a signiﬁcant social event, what is a good choice of external corpus to identify evolution of discussion topics around the event’s context? In this work, we compare the effectiveness of contemporary blog posts, online news media and forum discussions in creating adhoc external corpus. Using social propensity of evolution of topical discussions on Twitter to assess the goodness of the creation, we ﬁnd online news media as most effective. We evaluate on three large reallife Twitter datasets to afﬁrm our ﬁndings. 
There is a need of matching text difﬁculty to the expected reading skill of the audience. Readability measures were developed with this objective in mind, ﬁrst by psycholinguists, and more recently, by practitioners of natural language processing. A common strategy was to extract linguistic features that are good predictors of readability, and then train discriminative classiﬁcation or regression models that correlate well with human judgment. But correlation does not imply causality, which is a necessary property to explain why documents are not readable. Our objective is to provide mechanisms for text producers to adjust the readability of their content. We propose the use of generative models to diagnose causes of reading difﬁculty, and bring closer the realization of automatic readability optimization. 
Methods dealing with bilingual lexicon extraction from comparable corpora are often based on word co-occurrence observation and are by essence more effective when using large corpora. In most cases, specialized comparable corpora are of small size, and this particularity has a direct impact on bilingual terminology extraction results. In order to overcome insufﬁcient data coverage and to make word co-occurrence statistics more reliable, we propose building a predictive model of word co-occurrence counts. We compare different predicting models with the traditional Standard Approach (Fung, 1998) and show that once we have identiﬁed the best procedures, our method increases signiﬁcantly the performance of extracting word translations from comparable corpora. 
The work presented in this paper attempts to evaluate and quantify the use of discourse relations in the context of blog summarization and compare their use to more traditional and factual texts. Specifically, we measured the usefulness of 6 discourse relations - namely comparison, contingency, illustration, attribution, topic-opinion, and attributive for the task of text summarization from blogs. We have evaluated the effect of each relation using the TAC 2008 opinion summarization dataset and compared them with the results with the DUC 2007 dataset. The results show that in both textual genres, contingency, comparison, and illustration relations provide a signiﬁcant improvement on summarization content; while attribution, topic-opinion, and attributive relations do not provide a consistent and signiﬁcant improvement. These results indicate that, at least for summarization, discourse relations are just as useful for informal and affective texts as for more traditional news articles. 
We detect errors in Korean post-positional particle usage, focusing on optimizing omission detection, as omissions are the single-biggest factor in particle errors for learners of Korean. We also develop a system for predicting the correct choice of a particle. For omission detection, we model the task largely on English grammatical error detection, but employ Korean-speciﬁc features and ﬁlters; likewise, output analysis and the omission correction system illustrate how unique properties of Korean, such as the distinct types of particles used, need to be accounted for in adapting the system, thereby moving the ﬁeld one step closer to robust multi-lingual methods. 
The goal of this study is to investigate whether learners’ written data in highly inflectional Czech can suggest a consistent set of clues for automatic identification of the learners’ L1 background. For our experiments, we use texts written by learners of Czech, which have been automatically and manually annotated for errors. We define two classes of learners: speakers of Indo-European languages and speakers of non-Indo-European languages. We use an SVM classifier to perform the binary classification. We show that non-content based features perform well on highly inflectional data. In particular, features reflecting errors in orthography are the most useful, yielding about 89% precision and the same recall. A detailed discussion of the best performing features is provided. 
Mining a Bilingual Lexicon of MultiWord Expressions : A Statistical Machine Translation Evaluation Perspective This paper describes a method aiming to construct a bilingual lexicon of MultiWord Expressions (MWES) from a French-English parallel corpus. We ﬁrst extract monolingual MWES from each part of the parallel corpus. The second step consists in acquiring bilingual correspondences of MWEs. In order to assess the quality of the mined lexicon, a Statistical Machine Translation (SMT) task-based evaluation is conducted. We investigate the performance of three dynamic strategies and of one static strategy to integrate the mined bilingual MWES lexicon in a SMT system. Experimental results show that such a lexicon signiﬁcantly improves the quality of translation. MOTS-CLÉS : Expression polylexicale, alignement bilingue, traduction automatique statistique. KEYWORDS: MultiWord expression, bilingual alignment, statistical machine translation.  
Clustering for categorial grammar induction In this article, we describe the way we use hierarchical clustering to learn an AB grammar from partial derivation trees. We describe AB grammars and the derivation trees we use as input for the clustering, then the way we extract information from Treebanks for the clustering. The uniﬁcation algorithm, based on the information extracted from our cluster, will be explained and the results discussed. MOTS-CLÉS : grammaires catégorielles, clustering hiérarchique, inférence grammaticale. KEYWORDS: categorial grammars, hierarchical clustering, grammatical inference.  
 @'>( 8#>&H%'( =#$?':>'( ":'( A$>PKIK%KN&'( ='#A'>>8:>( IJ'X>#8&#'( '>( I'( I$H#&#'( I'?( %KH">&K:?( 9'#Y8%'?( 9&?;Z;9&?( I'( %'"#( HKA=K#>'A':>( >#8:?[K#A8>&K::'%Q( -%"?&'"#?( KYO'H>&[?( ?K:>( H&Y%$?(\(/3('X>#8&#'(8">KA8>&L"'A':>(%'?('X=#'??&K:?(=P#8?$K%KN&L"'?('>(':(=8#>&H"%&'#(%'?( 'X=#'??&K:?( [&N$'?0( 13( I$H#&#'( %&:N"&?>&L"'A':>( %'( HKA=K#>'A':>( I'?( =P#8?$K%KN&?A'?( C3( HKA=8#'#(%'?(A$>PKI'?(?>8>&?>&L"'?('>(:K>#'(8==#KHP'('>(':[&:(D3(AK:>#'#(%J&A=K#>8:H'(I'( H'?('X=#'??&K:?(I8:?(":(K">&%(I'(H%8??&[&H8>&K:(I'(>'X>'?Q(  !<+GS!@G(WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW((  9/:-/.();6$,%<&'0)9+2'(11%&/1)9+,'-.,%&/)-/0)=(1.'%2,%&/)<%,:)>'-/18&'"-,%&/-$) ?6$(1)  GP&?(=8='#(=#'?':>?(8(A'>PKIK%KN](>K('X>#8H>(8:I(I'?H#&Y'(9'#Y8%(A"%>&^K#I('X=#'??&K:?( "?&:N( >P'&#( >#8:?[K#A8>&K:8%( Y'P89&K#Q( +'9'#8%( KYO'H>&9'?( 8#'( >8#N'>'I\( /3( 8">KA8>&H8%%]( 'X>#8H>&:N(V_,(8:I('?='H&8%%]([#K`':('X=#'??&K:0(13(I'?H#&Y&:N(%&:N"&?>&H8%%](>P'&#(V_,( Y'P89&K#0(C3(HKA=8#&:N(?>8>&?>&H8%(A'>PKI?(8:I(K"#(8==#K8HP0(8:I([&:8%%](D3(?PK^&:N(>P'( &A=K#>8:H'(K[(V_,(&:(8(>'X>(H%8??&[&H8>&K:(>KK%Q(  ( ( V*G+;@4T+\ 'X=#'??&K:?( =K%]%'X&H8%'?0( 'X=#'??&K:?( [&N$'?0( %KH">&K:( 9'#Y8%'0( 'X>#8H>&K:0( >#8:?[K#A8>&K:0(H%8??&[&H8>&K:(I'(>'X>'?( a,b_*S5+(\( A"%>&^K#I( 'X=#'??&K:0( 9'#Y8%( =P#8?'0( 'X>#8H>&K:0( >#8:?[K#A8>&K:0( >'X>( H%8??&[&H8>&K:(  (  )  42  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne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`/,' )"&' ".#0"&&%2+&' @%5,"&' 6a*=%&14' LMGL9' "+' @*:"$0' 3"&'  43  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne !"##"!$%&"'()*+##,(*'-!,((&%,'%*-.$#,/,'%*0,(*!"123(*4"#3/&',35)*  !"! #$%&'(()*+,$%&,-.)/0$%&  67/,* (&* !,1%$&'(* !8,1!8,31(* 1,93(,'%* #,(* 1,(("31!,(* #&'.3&(%&:3,(* ;* !$3(,* 0,* (,(* &'!"'4-'&,'%(* <0-2,'0$'%,(* 0,* #$* #$'.3,* -%30&-,=* ("34,'%* #"'.3,(* ;* !"'(%13&1,* ,%* ;* /$&'%,'&1>=* #,(* /-%8"0"#".&,(* #,(* 2#3(* 2,19"1/$'%,(* ,%* #,(* 2#3(* 3%&#&(-,(=* !"/?&','%* (%$%&(%&:3,* ,%* 0,(* 9&#%1,(* #&'.3&(%&:3,()* @,(* 9&#%1,(* 2,34,'%* 7%1,* 0,(* ',%%"A$.,(* 0,* /"%(* .1$//$%&!$35* <6$''&'.* ,%* B!8C%D,=* EFFF>=* 0,(* (-#,!%&"'(* 0,* (%13!%31,(* (A'%$5&:3,(* 21"03!%&4,(*<G$%1&'=*HIIJ>)*K3,#:3,9"&(*#,(*!8,1!8,31(*&'%1"03&(,'%*#,(* %1$'(9"1/$%&"'(* 2"31*-%,'01,*#,31*,5%1$!%&"'*<L$&##,=*EFFM>*"3*2"31*4-1&9&,1*#$*4$#&0&%-*0,(*!$'0&0$%(*<N#* O$P* ,%* G&'%',1=* HIEI*Q* N?,&##-* ,%* B!8$?,(=* EFRF>)* ST"3%&#* /U,%""#V&%* <W$/&(!8* HIEH>* 21"2"(,*0,*'"/?1,3(,(*2"((&?&#&%-(*2"31*,5%1$&1,*!,(*,521,((&"'(*(,#"'*!,1%$&'(*9&#%1,()*  !"1 #$%&'(()*+,$%&2345/4/673$%&  X1Y(* ?1&Y4,/,'%* 1,4,'"'(* (31* #,(* %1$4$35* 0,* Z&##$0$* 6"&1['* ,%* $#)* <HIIM>* "3* N1!8,1* <HIIM>* :3&* 21"2"(,'%* 3',* ,5%1$!%&"'* ?$(-,* (31* #$* !"/2$1$&("'* 0,* !"123(* 2$1$##Y#,()* \#(* 2$1%,'%* 03* 2"(%3#$%* :3,* !,1%$&',(* +]* ',* ("'%* 2$(* %1$03&(&?#,(* /"%* ;* /"%)* +'* 0T$3%1,(* %,1/,(=* #$* %1$03!%&"'* 0,* !8$:3,* %,1/,* ',* 2,3%* /,',1* ;* #$* %1$03!%&"'* 0,* #T,521,((&"'* ,'%&Y1,*0$'(*#$*#$'.3,*!&?#,)**  !"8 #$%&'(()*+,$%&.'%9$%&%3)&0$%&)$%%*3)+$%&4/673/%5/:3$%&  N#"1(* :3,* 0,(* #&(%,(* 1-2,1%"1&$'%* #,(* ,521,((&"'(* 9&.-,(* 2,34,'%* 7%1,* 3%&#&(-,(* <^1,DV$* ,%* ]"30$%=* HIEH>=* 2"31* 1,!"''$_%1,* #,(* ,521,((&"'(* (,/&`9&.-,(* :3&* ("'%* #,(* +]* #,(* 2#3(* 91-:3,'%,(* 0$'(* #,(* !"123(=* #$* /,&##,31,* 1,(("31!,* ,(%* !,##,* :3&* 0-!1&%* #,(* 4$1&$%&"'(* 0,(* !"/2"($'%(* ,%* #,(* %1$'(9"1/$%&"'(* 2"((&?#,()* S,* #,5&:3,`.1$//$&1,* <&'&%&-* 2$1* 6)* ^1"((>* ,%* #,(* 1,(("31!,(* 03* SL\* <a,'`O,'&$* NA$%=* HIIM*Q* @$1%&,1=* HIEI*Q* a34,%=* HIIR>* 0-!1&4,'%* !8$:3,* ,521,((&"'* 0,* !,%%,* /$'&Y1,)* 6$&(* #$* 0,(!1&2%&"'* ,(%* %1Y(* !"b%,3(,* ,'* %,/2(* 0,* 1-$#&($%&"')* @T,(%* 2"31:3"&* '"3(* 4"3#"'(* $/-#&"1,1* !,%%,* $221"!8,* ,'* &'%1"03&($'%* 0,(* 0,(!1&2%&"'(*$3%"/$%&:3,()* ]#3(* 1-!,//,'%=* #,(* /-%8"0,(* ?$(-,(* (31* #,* G,?=* "'%* -/,1.-)* @,1%$&',(* 3%&#&(,'%* #,(* /"%,31(*0,*1,!8,1!8,*<@"#("'=*HIEI*Q*@$1%&,1*,%*c"(,28=*HIEE>=*0T$3%1,(*3%&#&(,'%*^"".#,* '.1$/(*<d1$'e"&(=*HIEE>*"3*G&V&2-0&$*<^$1!&$`d,1'$'0,D*,%*$#)=*HIEE>)* 
Multilingual document clustering : state of the art Multilingual corpora are extensively exploited in several branches of natural language processing. This paper presents an overview of works in the automatic construction of such corpora. We address this topic by ﬁrst providing an overview of different perceptions of comparability. We then examine the main approaches to similarity computation, construction and evaluation developed in the ﬁeld. We notice that the measurement of the textual similarity is usually based on corpus statistics or the structure of ontological resources or on a combination of these two approaches. In a multilingual framework, with the use of a multilingual dictionary or a machine translator, many problems arise. The exploitation of a multilingual ontological ressource seems to be a worthy option. In clustering, the problem of adding documents to the initial base without affecting the quality of clusters remains open. MOTS-CLÉS : corpus multilingues, comparabilité, similarité textuelle translingue, classiﬁcation. KEYWORDS: multilingual corpora, comparability, crosslingual textual similarity, classiﬁcation.  
NOPAEO%QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ% % R(;% #:;"M)(% <8M:";% $?;:(% M?$;:"4.;"?$% /.:% )#% <8;(M;"?$% <(% 9?)#:";8% <=?9"$"?$/% ($% )#$H.(% #:#4(% 9#:% #99:($;"//#H(% /.9(:B"/8S% T$% (>>(;% )(% /K/;U'(% 9:?9?/8% M?'9:($<% ;:?"/% 95#/(/L% )(% 9:8;:#";('($;% <.% M?:9./+% )=(V;:#M;"?$% <(/% M#:#M;8:"/;"@.(/% (;% )#% M)#//">"M#;"?$S% W?.:% )#% <(.V"U'(%95#/(+%$?./%.;")"/?$/%B"$H;%M#:#M;8:"/;"@.(/%<?$;%)(/%9:"$M"9#)(/%/?$;%)=8'?;"B";8+%)#% :8>)(V"B";8+%)=#<:(//#H(%(;%)#%9?)#:";8S%J#%95#/(%<(%M)#//">"M#;"?$%:(9:8/($;(%<#$/%$?;:(%;:#B#")% )#% M?'4"$#"/?$% <(/% 9)./"(.:/% M)#//">"(.:/% PXE/% GE#M5"$(% Y% X(M;(.:% <(% P.99?:;I% 9?.:% :8/?.<:(% )(% 9:?4)U'(% '.);"% M)#//(/S% 3?./% #B?$/% <?$M% #$#)K/8/% )(/% <(.V% /;:#;8H"(/% <(% PXE% '.);"%M)#//(/%@."%/?$;%L%Z%.$%M?$;:(%;?./%[%(;%Z%.$%M?$;:(%.$%[%#>"$%<(%M?'9#:(:%)(/%:8/.);#;/%(;% #'8)"?:(:%)#%9(:>?:'#$M(%<.%/K/;U'(%H)?4#)S%% &CP,N&R,%QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ% % Polarity Opinion Detection in Arabic Forums by Fusing Multiple SVMs ,5"/% #:;"M)(% <(/M:"4(/% ?.:% M?$;:"4.;"?$% ?$% ;5(% 9?)#:";K=/% <(;(M;"?$% ?>% ?9"$"?$/% "$% &:#4"#$% )#$H.#H(% 4K% /.9(:B"/(<% ;:#"$"$HS% \$<((<% ;5(% 9:?9?/(<% /K/;('% M?$/"/;/% ?>% ;5:((% 95#/(/L% ;5(% 9:(;:(#;'($;% ?>% ;5(% M?:9./+% ;5(% (V;:#M;"?$% ?>% ;5(% >(#;.:(/% #$<% ;5(% M)#//">"M#;"?$S% ]?:% ;5(% /(M?$<% 95#/(+% ^(% ./(% ;^($;K% >(#;.:(/% ?>% ^5"M5% ;5(% '#"$% #:(% ('?;"?$#)"/'+% ;5(% :(>)(V"B";K+% ;5(% #<:(//#H(% #$<% ;5(% 9?)#:";KS% ,5(% 95#/(% ?>% M)#//">"M#;"?$% :(9:(/($;/% "$% ?.:% ^?:F% ;5(% M?'4"$#;"?$%?>%;5(%/(B(:#)%PXE/%GP.99?:;%X(M;?:%E#M5"$(I+;?%/?)B(%;5(%'.);"%M)#//%9:?4)('S% _(% #$#)K2(<% ;5(% ;^?% /;:#;(H"(/% ?>% ;5(% PXE/% '.);"% M)#//% ;5#;% #:(L% `?$(% #H#"$/;% #))`% #$<% `?$(% #H#"$/;% ?$(`% "$% ?:<(:% ;?% M?'9#:(% ;5(% :(/.);/% #$<% ;?% "'9:?B(% ;5(% 9(:>?:'#$M(% ?>% ;5(% H)?4#)% /K/;('S% 
State of the Art of Automatic Keyphrase Extraction Methods This article presents the state of the art of the automatic keyphrase extraction methods. The aim of the automatic keyphrase extraction task is to extract the most representative terms of a document. Automatic keyphrase extraction methods can be divided into two categories : supervised methods and unsupervised methods. For supervised methods, the task is reduced to a binary classiﬁcation where terms are classiﬁed as keyphrases or non keyphrases. This classiﬁcation requires a learning step which is not required by unsupervised methods. The unsupervised methods use features extracted from the analysed document (sometimes a document collection) to check properties which allow keyphrase identiﬁcation. MOTS-CLÉS : extraction de termes-clés ; méthodes supervisées ; méthodes non-supervisées ; état de l’art . KEYWORDS: keyphrase extraction ; supervised methods ; unsupervised methods ; state of the art .  
On the Effect of Head Tagging on Parsing Discontinuous Dependencies in French In this paper we want to show the strong impact of syntactic tagging on syntactic dependency parsing. The rules of categorial dependency grammar used to parse French deal with discontinuous dependencies and long distance syntactic relations. Such parsing method produces a substantial number of dependency structures and takes too much parsing time. We want to show that a local tagging method can reduce these problems and help to solve the global problem of dependency parsing disambiguation. Then we adapt a tagging method to types of the categorial dependency grammar. We obtain a dependency-head pre-selection allowing to reduce parsing ambiguity and to see that we can ﬁnd distant relation of dependencies through local results of such method. MOTS-CLÉS : Analyse syntaxique en dépendances discontinues, Étiquetage syntaxique. KEYWORDS: Discontinuous Dependency Parsing, Syntactic Tagging.  110  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Towards an automatic identiﬁcation of chiasmus of words This article summarises the study of the rhetorical ﬁgure “chiasmus” (e.g : “Quitters never win and winners never quit.”). We address the problem of its computational identiﬁcation. How can a computer identify this automatically ? For this purpose this article will provide a formal description of the phenomenon. First, we put together an annotated text for testing our hypothesis. At the end we demonstrate that the use of stopword lists and the identiﬁcation of the punctuation improve the precision of the results with very little impact on the recall. We discover also that using lemmatization improves the results but stemming doesn’t. Finally we see that a French thesaurus provided us with good results on the most elaborate form of chiasmus. MOTS-CLÉS : chiasme, rhétorique, antimétabole, ﬁgure de style. KEYWORDS: chiasmus, rhetoric, antimetabole, stylistic device.  
ECD Knowledge Representation : Fundamental Concepts of the Unit Graphs Framework In this paper we are interested in the choice of a knowledge representation formalism that enables the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory and Combinatorial Dictionary (ECD) of the Meaning-Text Theory. We show that neither the semantic web formalisms nor the Conceptual Graphs Formalism suit our needs, and justify the introduction of a new formalism denoted Unit Graphs. We introduce the core of this formalism which is the Unit Types hierarchy, and present Unit Graphs and how one may use them to represent aspects of the ECD. MOTS-CLÉS : Représentation de Connaissances Linguistiques, Théorie Sens-Texte, Graphes d’Unités, Dictionnaire Explicatif et Combinatoire. KEYWORDS: Linguistic Knowledge Representation, Meaning-Text Theory, Unit Graphs, Explana- tory and Combinatorial Dictionary.  164  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Automatic search session detection exploiting results similarity from an external document collection Search engines users apply complex search behaviours such as query reformulation and multitasking search to satisfy their information needs. These search behaviours may be observed through query logs, and constitute clues allowing a better understanding of users’ needs. In this perspective, it is decisive to group queries related to the same information need into a unique search session. We propose an automatic session detection method exploiting the WIKIPEDIA documents collection, based on the similarity between the results returned for each query pair to estimate the similarity between queries. This method shows better performance than both temporal and lexical approaches traditionally used for successive session detection, and can be applied as well to multitasking search session detection. These experiments were conducted on a dataset originating from the OpenEdition Web portal. MOTS-CLÉS : Recherche d’information, détection automatique de sessions de recherche, analyse de journal de requêtes. KEYWORDS: Information retrieval, automatic search session detection, query log analysis.  217  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Améliorer la traduction des langages morphologiquement riches While statistical techniques for machine translation have made signiﬁcant progress in the last 20 years, results for translating to morphologically rich languages are still mixed versus previous generation rule-based systems. Current research in statistical techniques for translating to morphologically rich languages varies greatly in the amount of linguistic knowledge used and the form of this linguistic knowledge. This varies most strongly by target language (e.g., the resources used for linguistically-aware statistical machine translation to Arabic, French, German are very different). The talk will discuss state-of-the-art techniques for statistical translation tasks involving translating to a target language which is morphologically richer than the source language. MOTS-CLÉS : traduction statistique, langages morphologiquement riches, connaissances linguis- tiques. KEYWORDS: statistical translation, morphologically rich languages, linguistic knowledge.  
DAnIEL, parsimonious yet high-coverage multilingual epidemic surveillance DAnIEL is a multilingual epidemic surveillance system. DAnIEL relies on a parsimonious scheme making it possible to process new languages at small cost. 
;:9UEGVIT'=&)*$)$+,&.)')"#$0.)'-.*.L./"C'&+$7"1/.)'/"(.+*$0'?$"1.10?FC'(.0?$+"'*1.+/).*$2+P' O2&/' 516/"+*2+/' )"' 02(52/.+*' /6(.+*$%&"' -&' (2-3)"' )$+,&$/*$%&"' 42(51"+2P' 4"' (2-3)"' 02(51"+-'W'(2-&)"/'$+*"1-65"+-.+*/'T'(215?2)2,$%&"C'/6(.+*$%&"C'/F+*.#$%&"C'/*.*$/*$%&"C' "*' -$/52/"'' +2+' /"&)"("+*' -"/' (60.+$/("/' -"' -6/.(L$,&Xǡ   ǯ  ±ǯǯ± Y=.+$0?"7.' "*'.)PC'DZ[D\P'A2&1')"'     ǯ ሺͻͻͲͲͲ  ሻ    ሺͺ͹ͲͲͲ  )"#$0.)"/\' "/*' 51"/%&"' *"1($+6"']' ).' -"/01$5*$2+' -&' M1.+^.$/' Y[[_ZZ' 0).//"/' )"#$0.)"/\C' -"' ǯ ሺͳ͵ͲͲͲ  ሻ    ሺͺͷͲͲ  $0.)"/\' "/*' "+' 02&1/P' `' 516/"+*')"'/F/*3("'.//&1"').'*1.-&0*$2+'-"'?.&*"'%&.)$*6'ǯYB+$/$(27$0?'"*' .)C'DZ[D\P'!"/'-$1"0*$2+/'>:aSbGJ'"*'cGaSb'GJ'2+*'6*6'6,.)"("+*'*"/*6"/'"+'7"1/$2+'.)5?.P'' 
An Interface for Validating and Evaluating Thematic Timelines This demo paper presents a graphical interface for the visualization and evaluation of event timelines built automatically from a search query on a newswire article corpus provided by the Agence France Pressse (AFP). This interface also enables journalists to validate chronologies by editing and modifying them. MOTS-CLÉS : chronologie événementielle, évaluation, validation. KEYWORDS: event timeline, evaluation, validation.  
TALN Archives : a digital archive of French research articles in Natural Language Processing Scientiﬁc research is an incremental process. Reviewing the literature is the ﬁrst step to do before starting a new research project. The French Natural Language Processing (NLP) community produces numerous scientiﬁc publications which are scattered across different sources and for which no metadata is available. This paper presents the construction of TALN Archives, a digital archive of French research articles whose aim is to provide efﬁcient access to articles in the NLP ﬁeld. We also present an analysis of the collaboration network constructed from the metadata and disclose the identity of the Kevin Bacon of the TALN Archives, i.e. the most central author in the collaboration network. MOTS-CLÉS : TALN Archives, archive numérique, articles scientiﬁques. KEYWORDS: TALN Archives, digital archive, scientiﬁc articles.  
Similarities induced by a comparability mapping : meaning and utility in the context of the clustering of comparable texts. In the presence of bilingual comparable corpora it is natural to embed the data in two distinct linguistic representation spaces in which a "computational" notion of similarity is potentially deﬁned. As far as these bilingual data are comparable in the sense of a measure of comparability also computable (Li et Gaussier, 2010), we can establish a connection between these two areas of linguistic representation by exploiting a weighted mapping that can be represented in the form of a weighted bidirectional graph of comparability. We study in this paper the conceptual and practical consequences of such a similarity-comparability connection, while developing an algorithm (Hit-ComSim) based on the concept of similarities induced by the topology of the graph of comparability. We try to evaluate the beneﬁt of this algorithm considering some preliminary categorization or clustering tasks of bilingual (English/French) documents collected from RSS feeds. MOTS-CLÉS : Graphe de comparabilité, Similarités induites, Documents comparables, Clustering. KEYWORDS: Comparability graph, Induced similarities, Comparable documents, Clustering.  515  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Driven Decoding for machine translation Recently, the concept of driven decoding (DD), has been sucessfully applied to the automatic speech recognition (speech-to-text) task : an auxiliary transcription guide the decoding process. There is a strong interest in applying this concept to statistical machine translation (SMT). This paper presents our approach on this topic. Our ﬁrst attempt in driven decoding consists in adding several feature functions corresponding to the distance between the current hypothesis decoded and the auxiliary translations available. Experimental results done for a french-to-english machine translation task, in the framework of the WMT 2011 evaluation, show the potential of the DD approach proposed. MOTS-CLÉS : Décodage guidé, traduction automatique, combinaison de systèmes. KEYWORDS: Driven Decoding, machine translation, system combination.  
Interactive editing of utterances in French sign language dedicated to signing avatars Signing avatars dedicated to French Sign Language (LSF) are more and more used as a communication interface for the deaf community. One of the acceptation criteria of these avatars is the natural and realistic aspect of the constructed gestures. Consequently, gestures synthesis methods have been designed thanks to some corpus of captured and annotated motions, performed by a real signer. However, the enlarging of such a corpus, without requiring of some additional capture sessions, is a major issue. Furthermore, the automatic application of motion transformations (e.g. concatenation, blending, etc.) does not guarantee the semantic consistency of the resulting gesture. Another option is to insert the human operator in the utterance building loop. In this context, this paper provides a ﬁrst interactive editing system of FSL gestures, based on captured motions and dedicated to signing avatars. MOTS-CLÉS : Langue des Signes Française, édition, geste, base de données sémantiques, signeur virtuel, interaction. KEYWORDS: French sign language, editing, gesture, semantic data base, virtual signer, interaction.  547  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
%YHP?Y'ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ' ' #*;' 3);,-6*' ()70*4;*' 63' )736,03;,.4' 18!"#$%&' [/,' -.40;,;/*' (3)' 0.4' *49*):/)*' NTWK'UUU' +.;0O' 6*' ()*+,*)' -.)(/0' 2)34-.(>.4*' 344.;7' *4' 343(>.)*0' *;' -.)727)*4-*0' (*)+*;;34;' 6*' 179*6.((*+*4;' 183(().->*0' -*4;)7*0' 0/)' 6*0' 1.447*0' (./)' 63' )70.6/;,.4' 1*0' 343(>.)*0' *;' 3/;)*0';)3,;*+*4;0'1*'63'-.)727)*4-*\'C8344.;3;,.4'3'7;7')736,07*'0/)';).,0'-.)(/0'1*'(3).6*' -.49*)03;,.44*66*' N!--/*,6ZPVH&' $R]' *;' GHC$O' [/,' 6*' 1*0;,4*4;' (6/0' (3);,-/6,S)*+*4;' 3/' ;)3,;*+*4;'1/'634:3:*'(3)67\'G4'683<0*4-*'187[/,936*4;'(./)'6*'634:3:*'7-),;&',6'*0;';./;*2.,0' 0/0-*(;,<6*' 18,4;7)*00*)' 68*40*+<6*' 1*' 63' -.++/43/;7' R!C\' ^3)' 3,66*/)0&' 6*' 0->7+3' 18344.;3;,.4' )*;*4/' *0;' 0/22,03++*4;' ),->*' (./)' (*)+*;;)*' 1*0' 7;/1*0' *4' 6,4:/,0;,[/*' 1*' -.)(/0\' C*' -.)(/0' 0*)3' 1,22/07' 6,<)*+*4;' _' 63' +,EDUAK' 0./0' 6,-*4-*' #)*3;,9*' #.++.40' VFE "#EH!\' #*;' 3);,-6*' 0*' -.4-*4;)*' 0/)' 03' +,0*' *4' `/9)*' *;' 17-),;' <),S9*+*4;' [/*6[/*0' )70/6;3;0'.<;*4/0'0/)'63'(3);,*'17a_'344.;7*'1*'63')*00./)-*\' !VHR%!#R' ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ'' !"#$%&' ;>*' 2,)0;' 63):*' Q)*4->' 0(*3M,4:' -.)(/0' .2' -.49*)03;,.436' 0(**->' 344.;3;*1',4'-.)*2*)*4-*'';.'<*'2)**6b'393,63<6*\' R>,0' (3(*)' ()*0*4;0' ;>*' 2,)0;' Q)*4->' 0(.M*4' -.)(/0' 344.;3;*1' ,4' -.)*2*)*4-*' c>.0*' 0,@*' NTWK&UUU' c.)10O' ,0' 0/22,-,*4;' ;.' ,49*0;,:3;*' ;>*' 3->,*9*+*4;' .2' 13;3' .),*4;*1' 0b0;*+0' .2' -.)*2*)*4-*' )*0.6/;,.4\' R>*' 344.;3;,.4' c30' -.41/-;*1' .4' ;>)**' 1,22*)*4;' -.)(.)3' .2' -.49*)03;,.436' 0(**->' N!--/*,6ZPVH&' $R]&' GHC$O' </;' ;>,0' )*0./)-*' -34' 360.' <*' ,4;*)*0;,4:' 2.)' "C^' )*0*3)->*)0' c.)M,4:' .4' c),;;*4' 634:/3:*&' -.40,1*),4:' ;>*' 63-M' .2' 3' 63):*' c),;;*4' Q)*4->' -.)(/0' 344.;3;*1' ,4' -.)*2*)*4-*\' d*' 2.66.c*1' 3' ),->' 344.;3;,.4' 0->*+*' c>,->' *43<6*0' 360.' )*0*3)->' +.;,93;*1' <b' 6,4:/,0;,-' -.40,1*)3;,.40\' R>,0' -.)(/0' c,66' <*' 2)**6b' 393,63<6*' N#)*3;,9*' #.++.40' VFE"#EH!O' 3)./41' +,1EDUAK\' R>*' (3(*)' 1*;3,60' ;>*' 3->,*9*+*4;' .2' ;>*' )*0./)-*' 30' c*66' 30' ()*6,+,43)b' *e(*),+*4;0' -.41/-;*1' .4' ;>*' (3);' .2' ;>*'-.)(/0'36)*31b'344.;3;*1\' ' ?$RHE#CYH'f'#.)(/0&'344.;3;,.4&'-.)727)*4-*&'343(>.)*&'(3).6*'-.49*)03;,.44*66*' gGFd$%IH'f'#.)(/0&'344.;3;,.4&'-.)*2*)*4-*&'343(>.)3&'-.49*)03;,.436'0(**->'  555  ￿c ATALA 
Multilingual Compound Splitting Compounding is a common phenomenon for many languages, especially those with a rich morphology. Dealing with compounds is a challenge for natural language processing systems since all compounds can not be included in lexicons. In this paper, we present a compound splitting method combining language independent features (similarity measure, corpus data) and language dependent features (component transformation rules). We report on our experiments in splitting of German and Russian compound terms giving accuracy up to 95% for German and up to 91% for Russian language. We observe that the usage of a corpus of the same domain as compounds improves splitting quality. Mots-clés : segmentation des mots composés, outil multilingue, mesure de similarité, règles de transformation des composants, corpus spécialisés. Keywords: compound splitting, multilingual tool, similarity measure, component trans- formation rules, specialized corpora.  564  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
ABSTRACT _____________________________________________________________________ Complex terminologies management – the case of acronyms Terminology management is still problematic, especially for complex constructions such as acronyms. In this paper, we propose a solution to connect several different terms with a single referent through using the concepts of pivot and prolexeme. These concepts allow for example to link several terms for the same referent: Nations Unies, ONU, Organisation des Nations Unies and onusien. Jibiki is a generic platform for lexical database management, allowing the representation of any type of structure (macro and microstructure). We have implemented a new macrostructure ProAxie in the Jibiki platform to achieve acronym management. MOTS-CLÉS : base lexicale multilingue, macrostructure, Jibiki, Common Dictionary Markup, Proaxie, Prolèxeme KEYWORDS : multilingual lexical database, macrostructure, Jibiki, Common Dictionary Markup, Proaxie, Prolexeme  
This article presents supervised computational methods for the identiﬁcation of Spanish varieties. The features used for this task were the classical character and word n-gram language models as well as POS and morphological information. The use of these features is to our knowledge new and we aim to explore the extent to which it is possible to identify language varieties solely based on grammatical differences. Four journalistic corpora from different countries were used in these experiments : Spain, Argentina, Mexico and Peru. MOTS-CLÉS : classiﬁcation automatique, ngrammes, espagnol, variétés nationales. KEYWORDS: automatic classiﬁcation, n-grams, Spanish, language varieties.  
Improving Minor Opinion Polarity Classiﬁcation with Named Entity Analysis The main part of the work on opinion mining and sentiment analysis concerns polarity classiﬁcation of majority opinions. Supervised machine learning with n-gram features is a common approach to polarity classiﬁcation, which is often biased towards the majority of opinions about a given opinion target, when using this kind of approach with traditional settings. The presence of a speciﬁc term, strongly associated to the opinion target in a document, is often enough to tip the classiﬁer decision toward the majority opinion class. This is actually a good thing for overall accuracy. Howeverm documents about the opinion taget, but expressing a polarity different from the majority one, get misclassiﬁed. It is a problem if we want to detect weak signals (rumor detection) or for anticipating opinion reversal trends. We propose in this paper to improve minor reviews polarity classiﬁcation by taking into account Named Entity information in the computation of speciﬁc weighting scheme used for correcting the bias toward majority opinions. MOTS-CLÉS : Fouille d’opinions, Opinion minoritaires, Entités Nommées, Apprentissage, N-grammes, Pondération. KEYWORDS: Opinion Mining, Minor Opinion, Named Entities, Machine Learning, N-grams, Weighting Scheme.  588  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Static Analysis of Interactions between Elementary Structures of a Grammar We are interested in the semi-automatic construction of computational grammars and in their use for parsing. We consider lexicalized grammars with elementary structures which are trees, underspeciﬁed or not. We present an algorithm that aims at foreseeing all elementary trees attached at words which can come between two given words of a sentence, whose associated elementary trees are companions, that is, they will necessarily interact in the syntactic composition of the sentence. MOTS-CLÉS : grammaire lexicalisée, grammaire d’interaction, construction de grammaires. KEYWORDS: Lexicalized Grammar, Interaction Grammar, Grammar Construction. 
Formalizing an annotation guide : some experiments towards assisted agile annotation This article presents a methodology, inspired from the agile development paradigm, that helps preparing an annotation campaign. The idea behind the methodology is to formalize as much as possible the instructions given in the guidelines, in order to automatically check the consistency of the corpus being annotated with the guidelines, as they are being written. To formalize the guidelines, we use a graph rewriting tool, that allows us to use a rich language to describe the instructions. This formalization allows us to spot the rightfully annotated constructions and, by contrast, those that are not consistent with the guidelines. In case of inconsistency, an expert can either correct the annotation or update the guidelines and rerun the process. MOTS-CLÉS : annotation, guide d’annotation, annotation agile, réécriture de graphes. KEYWORDS: annotation, annotation guide, agile annotation, graph rewriting.  
Second order similarity for exploring multilingual textual databases This paper describes the use of second order similarities for identifying similar texts inside a corpus of aviation incident reports written in both French and English. We use a second bilingual corpus to construct pairs of reference documents and map each target document to a vector so each coordinate represents a similarity score between this document and the part of the reference corpus written in the same language. We evaluate the system using a large corpus of translated incident reports. The results are promising and validate the approach. MOTS-CLÉS : similarité de second ordre, multilingue, ESA. KEYWORDS: second order similarity, multilingual, ESA.  
Converting dependencies for syntactic analysis of French into PASSAGE functional relations We present here a ﬁrst attempt at building a bidrictionnal converter between, on the one hand the dependency based syntaxtic formalism which has been deﬁned to map the French Treebank annotation onto surface dependency trees used by the Bonsai parser, on the other hand the PASSAGE formalism developped intially for French parsing quantitative black-box objective open evaluation campaigns. MOTS-CLÉS : Analyse Syntaxique - Corpus arboré - Dependances -DepFTB - ConLL - PASSAGE. KEYWORDS: Parsing - Treebank - Dependencies - DepFTB -ConLL - PASSAGE.  
Anaphora Resolution for Machine Translation Pronoun translation is one of the current problems within Machine Translation. Since pronouns do not convey enough semantic content by themselves, pronoun processing requires anaphora resolution. Research in anaphora resolution is interested in establishing the link between entities (NPs and pronouns) and their antecedents in the text. In this article, we implement a prototype of a linguistic anaphora resolution method inspired from the Chomskyan Binding Theory in order to improve the translation of personal pronouns between Spanish and French. MOTS-CLÉS : Résolution d’anaphores, traduction automatique à base de règles, sujets nuls. KEYWORDS: Anaphora Resolution, Rule-based Machine Translation, nul subjects.  
SegCV : Eﬁcient parsing of résumés with analysis and correction of errors Over the last years, the online market of jobs and candidatures offers has reached an exponential growth. This has implied great amounts of information (mainly in a text free style) which cannot be processed manually. The résumés are in several formats : .pdf, .doc, .dvi, .ps, etc., that can provoque errors or noise during the conversion to plain text. We propose SegCV, a system that has as goal the automatic parsing of candidates’ résumés. In this article we present the algoritms, which are based over a surface analysis, to segment the résumés in an accurate way. We evaluated the automatic segmentation using a reference corpus that we have created. The preliminary experiments, done over a large collection of résumés in French with noise correction, show good results in precision, recall and F-score. MOTS-CLÉS : RI, Ressources humaines, traitement de CV, Modèle à base de règles. KEYWORDS: Information Retrieval, Human Resources, CV Parsing, Rules Model.  
A corpus of post-edited translations More and more datasets of post-edited translations are being collected. These corpora have many applications, such as failure analysis of SMT systems and the development of quality estimation systems for SMT. This work presents a large corpus of post-edited translations that has been gathered during the ANR/TRACE project. Applications to the detection of frequent errors and to the analysis of the inter-rater agreement of hTER are also reported. MOTS-CLÉS : Traduction automatique,Analyse d’erreur,Post-édiition. KEYWORDS: Machine Translation, Failure Analysis,Post-edition.  
 E'*3"#$"%&'&()%*&+,$'&(-."*2)%3(3&"*U*"44"2&."1*.%*0'F'#"*5N.%*5)2.$"%&*<&"M&"*29'33(-.">* '.5()* ).* F(5,)@* "%* 3"#$"%&3* &+,$'&(-."$"%&* +)$)#W%"3V* G9.3(".13* 01)#1'$$"3* 5"* 1"2+"12+"* 3"* 3)%&* '&&'2+,3* U* &1'(&"1* 9'* 3"#$"%&'&()%* &+,$'&(-."* 5"* [).1%'.M* &,9,F(3,3* <fX@* $'(3* 9"* 01)89W$"* 5"$".1"* 5N'2&.'9(&,* "&* 5)(&* Y&1"* 2)%3(5,1,* 'F"2* 5"* %).F"99"3* 0"130"2&(F"3*  739  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne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`% Ba*)$,(5% UbbcE% '.'(')0*>*.(% -".d#*% !"#$% ,*1>*.(*$% +#% (*D(*% ,6*,(% )&2$2*% !*#% !*$H"$>).(*% ,#$% +*,% -".(*.#,% )#+'"&',#*0,% BV0)&*)#% *(% K*H?&$*5% STUUE% B`#'.)#+*)#% *(% )035% STUTE5% ."#,% )&".,% .2).>"'.,% -N"','% +6)+"!(*$% -*% ,-N2>)% +*% H)d".% /% *.% *D!0"$*$% +*#D% +'>*.,'".,3% K)% !$*>'?$*% *,(% 0)% >2(N"+*% +*% ,20*-('".% +*,% H$".('?$*,% /% !)$('$% +*% 0)% -"#$=*% +*% ,'>'0)$'(2% *(% 0)% ,*-".+*% *,(% 0*% >2-).',>*% +*% !".+2$)('".% +*,% >"(,% #('0',2% !"#$% -)0-#0*$% #.*% &)0*#$% !*$('.*.(*% +*% -"N2,'".% 0*D'-)0*3% V*% -N"'D% .6*,(% .2).>"'.,% !),% $*,($'-('H% *(% 0*,% !$"!",'('".,% +2&*0"!!2*,%+).,%-*(%)$('-0*%!*#&*.(%,6)!!0'A#*$%/%%+*,%%)01"$'(N>*,%!0#,%,"!N',('A#2,3% K6)$('-0*% *,(% ,($#-(#$2% +*% 0)% H)d".% ,#'&).(*%P% 0)% ,*-('".% S% !$2,*.(*% ."($*% )01"$'(N>*% +*% ,*1>*.()('".% (N2>)('A#*5% 0)% ,*-('".% G% !$2,*.(*% #.*% 2&"0#('".% &*$,% #.*% )!!$"-N*% '.(21$2*% '(2$)('&*% A#'% !*$>*(% +*% $)HH'.*$% 0)% !".+2$)('".% 8Fe_@F% +*,% >"(,3% K*,% *D!2$'*.-*,% ,".(% !$2,*.(2*,%+).,%0)%,*-('".%f3%  S C01"$'(N>*%%+*%,*1>*.()('".%(N2>)('A#*%  V">>*% !"#$% 06)01"$'(N>*% 84W88_K_X`5% 0)% ,'>'0)$'(2% *,(% -)0-#02*% *.($*% -N)A#*% !)'$*% +*% =0"-,%  )+:)-*.(,3% K*,% ,*1>*.(,% #.'()'$*,% -".,'+2$2,% ,".(% +*,% 1$"#!*,% +*% ,"#HH0*% B`^E5% -6*,(e/e+'$*%  +*,% ,2A#*.-*,% +*% >"(,% *.($*% +*#D% !)#,*,% +).,% #.% ("#$% +*% !)$"0*3% K*,% !)#,*,% *(% 0*,%  -N).1*>*.(,% +*% 0"-#(*#$% ,".(% +2(*-(2,% )#(">)('A#*>*.(% !)$% 0*% ,Y,(?>*% +*% ($).,-$'!('".%  740  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne !"#$%!#&'"()*+!*,&%&-!.&#/*(,#*0$12*2!-2"-/(*#$"#*!"*-$13*0(*-4/%&,,&$1*5*-4!&0(*04"1(*6(17#.(* 3-&,,!1#(* 0(* #!&--(* 89* (1#.(* 0(,* :-$2,* !0;!2(1#,* 0(* 8* <=* 0(* >!.#* (#* 04!"#.(* 0(* -!* 6.$1#&?.(* >$#(1#&(--()*@-*(1*./,"-#(*"1(*2$".:(*0(*2$A/,&$1*-(B&2!-(*5*>!.#&.*0(*-!'"(--(*,$1#*(B#.!&#(,*-(,* AC>$#A?,(,* 0(* 6.$1#&?.(,)* D!1,* -(,* 0("B* >.(%&?.(,* ,$",E,(2#&$1,9* 1$",* 0/2.&F$1,* 2$%%(1#* ,$1#* ./!-&,/,* -!* >$10/.!#&$1* 0(,* #(.%(,* (#* -(* 2!-2"-* 0(* ,&%&-!.&#/* -(B&2!-()* G$",* >.$>$,$1,* (1,"&#(*"1*!-3$.&#A%(*0(*,/-(2#&$1*0(,*6.$1#&?.(,*5*>!.#&.*0(*-!*2$".:(*0(*2$A/,&$1*$:#(1"()*  H)I +!*>$10/.!#&$1*JKE@DK*&1#.!E0$2"%(1#*  +!* >$10/.!#&$1* JKE@DK* (,#* -!.3(%(1#* "#&-&,/(* (1* .(2A(.2A(* 04&16$.%!#&$1* LM@N* >$".* /F!-"(.*  -(* >$"F$&.* 0&,2.&%&1!1#* 04"1* #(.%(* #* 0!1,* "1* 0$2"%(1#*O*LF&!* JK* P* 6./'"(12(* -$2!-(* 0"*  #(.%(N9* .(-!#&F(%(1#* 5* "1(* 2$--(2#&$1* 0(* 0$2"%(1#,* LF&!* @DK* P* 6./'"(12(* 3-$:!-(* &1F(.,(* 0"*  #(.%(N)* D!1,* -(* 2!0.(* 0(* -!* ,(3%(1#!#&$1* #A/%!#&'"(9* -!* >$10/.!#&$1* 0(,* %$#,* >(.%(#*  04!"3%(1#(.*-!*>(.#&1(12(* 0(,* %(,".(,*0(*,&%&-!.&#/*-(B&2!-(9*(1*.(16$.Q!1#*-!*2$1#.&:"#&$1*  0(*2(.#!&1,*%$#,*0!1,*-4(,#&%!#&$1*0(*2(,* %(,".(,)*D!1,*-(*0$%!&1(*0(*-!*,(3%(1#!#&$1*0(*  2$1#(1",* 0"* #C>(* &16$.%!#&$1* L;$".1!"B* #/-/F&,/,9* ;$".1!"B* .!0&$>A$1&'"(,9* /%&,,&$1,* 0(*  .(>$.#!3(,N9* -(,* >$&0,* ,$1#* 3/1/.!-(%(1#* (,#&%/,* >!.* "1* -!.3(* 2$.>",)* R!.* (B(%>-(9*  L<"&1!"0(!"*(#*S&.,2A:(.39*HTIIN*"#&-&,(1#*-4$"#&-**UVWV*L+(2$.F/*(#*!-)9*HTTXN*'"&*>.$0"&#*0(,*  >$&0,*(,#&%/,*5*>!.#&.*04"1(*2$--(2#&$1*0(*XTTTTT*!.#&2-(,*0"*;$".1!-*+(*Y$10()*Z6&1*0(*1$",*  !66.!12A&.* 0(* -!* 2$1#.!&1#(* 0(* 0&,>$,(.* 04"1(* :!,(* 04!>>.(1#&,,!3(9* 1$",* 1$",* >.$>$,$1,* 0(*  ,"&F.(* -4!>>.$2A(* 0$11/(* 0!1,* LY!-&$"#$F* (#* !-)9* T[N* $\* -(,* !"#(".,* &1#.$0"&,(1#* "1(*  >$10/.!#&$1*&1#.!E0$2"%(1#*>$".*-(*0$%!&1(*0(*-!*,(3%(1#!#&$1*#A/%!#&'"(*0(*2$16/.(12(,)*  =!1,* !"2"1(* &16$.%!#&$1* (B#(.1(9* -(,* >$&0,* JKE@DK* ,$1#* (,#&%/,* "1&'"(%(1#* 5* >!.#&.* 0"*  2$1#(1"*(1*'"(,#&$1)*+(*>.&12&>(* (,#*0(* 0/2$">(.*"1&6$.%/%(1#*-4/%&,,&$1*(1*G*%$.2(!"B*  L$"* 2A"1]N)* ^A!'"(* 2A"1]* (,#* "1(* ,"22(,,&$1* 0(* 3.$">(,* 0(* ,$"66-(,* (#* 2$..(,>$10* 5*  -4/'"&F!-(1#*04"1*0$2"%(1#*(1*M@)**+(*#(.%(*_*0!1,*-(*3.$">(*0(*,$"66-(*`*(,#*!,,$2&/*!"*>$&0,*  WLaL`N9 _N*'"&*0/>(10*0"*2A"1]*aL`N*0!1,*-('"(-*,(*#.$"F(*`)*  **********WLaL`N9 _N  b  cdeLfN9g  *h  ijdg 9*$\*ijdg  b  -$3 k l o***********************LIN* mn  $\***cdeLfN9g ** (,#* -!* 6./'"(12(* 0"* #(.%(*_*0!1,* -(* %$.2(!"*aL`N*(#*pg *(,#* -(* 1$%:.(* 0(* 2A"1],*  0!1,*-('"(-*-(*#(.%(*_*!>>!.!q#)**  ^(##(* !>>.$2A(* >(.%(#* 0(* 6!&.(* .(,,$.#&.* -(,* %$#,* 0&,2.&%&1!1#,* 0!1,* "1* >!,,!3(* 0(* -4/%&,,&$1*.(-!#&F(%(1#*!"B*!"#.(,*>!,,!3(,)*D(,*(B>/.&(12(,*"#&-&,!1#*04!"#.(,*>$10/.!#&$1,* 2$%%(*r]!>&*14$1#*>!,*>(.%&,*04!%/-&$.(.*-(,*>(.6$.%!12(,*0(*-!*,(3%(1#!#&$1)*  H)H ^!-2"-*0(*,&%&-!.&#/*  +!* %(,".(* 2$,&1",* >(.%(#* 0(* %(,".(.* -!* >.$B&%&#/* (1#.(* -!* .(>./,(1#!#&$1* F(2#$.&(--(* 0(* 0("B* :-$2,* !0;!2(1#,* :;* (#* :;sI)* +(* 2$(66&2&(1#* !,,$2&/* !"* #(.%(* #* 0!1,* -!* .(>./,(1#!#&$1* F(2#$.&(--(* 04"1* :-$2* :* (,#* "1(* F!-(".* >$10/./(**tLu9 _N)* D!1,* 1$#.(* !>>.$2A(9* &-* 14C* !* >!,* "1&2&#/* 0(* -!* >$10/.!#&$1* JKE@DK* 0!1,* "1* :-$2* 0$11/* 2!.* -(,* 3.$">(,* 0(* ,$"66-(,* 0"* :-$2* >("F(1#*1(*>!,*!>>!.#(1&.*#$",**!"*%7%(*2A"1])*Z&1,&*-(*2$(66&2&(1#*!,,$2&/*5*#*0!1,*-(*:-$2*:* (,#*$:#(1"*(1*,$%%!1#*-(,*6./'"(12(,*>$10/./(,*0"*#(.%(*#*0!1,*2A!'"(*<=*0"*:-$2*P*  tLu9 _N b v kwf9g h WLaL`N9 _No**********************************************LHN* fxy $\*wf9g *(,#*-!*6./'"(12(*0"*#(.%(*#*0!1,*-(*<=*B)* R$".*"1(*6.$1#&?.(*>$#(1#&(--(*z*(1#.(*0("B*:-$2,*u{ *(#*u{|} 9*-!*,&%&-!.&#/*(,#*0$11/(*>!.*  741  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne  !"#$%&"'()* +  ,--./01234!!"!!#$%&'()*+, !2!!!!!!!!!!!!!!!!!!!!!!!!!!345!  
Search and Visualization of Semantically Related Words We present PatternSim, a new semantic similarity measure that relies on morpho-syntactic patterns applied to very large corpora and on a re-ranking formula that reorder extracted candidates. The system, originally developed for English, was adapted to French. We explain this adaptation, propose a ﬁrst evaluation of it and we describe how this new model was used to build the Serelex online search platform. MOTS-CLÉS : Mesure de similarité sémantique, relations sémantiques. KEYWORDS: Semantic similarity measure, semantic relations.  
Building and exploiting a French corpus for sentiment analysis This work introduces a French corpus for sentiment analysis. We describe the construction and organization of the corpus. We then apply machine learning techniques to automatically predict whether a text is positive or negative (the opinion classiﬁcation task). Two techniques are used : logistic regression and classiﬁcation based on Support Vector Machines (SVM). Finally, we brieﬂy evaluate the merits of applying feature selection algorithms to our models (via elastic net regularization). MOTS-CLÉS : Analyse de sentiments, Corpus, Classiﬁcation, Apprentissage automatique, Sélec- tion de variable. KEYWORDS: Sentiment Analysis, Corpus, Opinion Mining, Classiﬁcation, Machine Learning, Variable Selection.  
Help enrich a terminological repository : proposals and experiments Based on an experience of terminological enrichment, this paper shows how to support the work of terminological acquisition and overcome practical difﬁculties it presents, i.e. the mass of candidate terms to consider and the subjectivity of terminological judgments which depends on the type of terminology to produce. We propose simple strategies to ﬁlter a priori part of the noise from the results of term extractors so as to make the validation practicable for terminologists. We demonstrate their effectiveness on a sample of candidate terms proposed for the validation of two experts. We also show that by applying to term validation campaigns the methodological principles that have been proposed for corpus annotation campaigns, we can control the quality of validation judgments and of the resulting terminologies. MOTS-CLÉS : Acquisition terminologique, validation de candidats-termes, ﬁltrage de termes, distance terminologique, vote, accord inter-juges. KEYWORDS: Terminology acquisition, term candidate validation, term ﬁltering, terminological distance, vote, inter-judge agreement.  779  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Symbolic and statistical learning for chunking : comparison and combinations We describe in this paper how to use grammatical inference algorithms for chunking, then compare and combine them to CRFs (Conditional Random Fields) which are known efﬁcient for this task. Our corpus is extracted from the FrenchTreebank. We propose and evaluate two ways of combining a symbolic model and a statistical model learnt by a CRF, and show that in both cases they beneﬁt from one another. MOTS-CLÉS : apprentissage automatique, chunking, CRF, inférence grammaticale, k-RI, French TreeBank. KEYWORDS: machine learning, chunking, CRF, grammatical inference, k-RI, French TreeBank.  
Unsupervised selection of semantic relations for improving a distributional thesaurus Work about distributional thesauri has shown that the relations in these thesauri are mainly reliable for high frequency words. In this article, we propose a method for improving such a thesaurus through its re-balancing in favor of low frequency words. This method is based on a bootstrapping mechanism : a set of positive and negative examples of semantically similar words are selected in an unsupervised way and used for training a supervised classiﬁer. This classiﬁer is then applied for reranking the semantic neighbors of the thesaurus used for example selection. We show how the relations between the mono-terms of similar nominal compounds can be used for performing this selection and how to associate this criterion with an already tested criterion based on the symmetry of semantic relations. We evaluate the interest of the global procedure for a large set of English nouns with various frequencies. MOTS-CLÉS : Sémantique lexicale, similarité sémantique, thésaurus distributionnels. KEYWORDS: Lexical semantics, semantic similarity, distributional thesauri.  
Grouping of terms based on linguistic and semantic regularities in a cross-lingual context We propose to exploit the Natural Language Processing methods dedicated to terminology structuring independently in two languages (English and French) and then to merge the results obtained in each language. The terms are grouped into clusters thanks to the generated relations. The evaluation of the relations is done via the comparison of the clusters with the reference data and the baseline, while the complementarity of the relations is analyzed through their involvement in the clusters of terms. Our results indicate that : each language contributes almost equally to the generated results ; the number of common hierarchical relations is greater than the number of common synonym relations. On the whole, the obtained results point out that in a cross-language context, each language brings additional linguistic and semantic regularities. The union of the results obtained in each language improves the overall quality of the clusters. MOTS-CLÉS : Relations sémantiques, termes, domaine de spécialité, médecine, contexte cross- langue. KEYWORDS: Semantic relations, terms, specialized areas, medicine, cross-lingual context.  62  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
WoNeF, an improved, extended and evaluated automatic French translation of WordNet Identifying the various possible meanings of each word of the vocabulary is a difﬁcult problem that requires a lot of manual work. It has been tackled by the WordNet lexical semantics database in English, but there are still few resources available for other languages. Automatic translations of WordNet have been tried to many target languages such as French. JAWS is such an automatic translation of WordNet nouns to French using bilingual dictionaries and a syntactic langage model. We improve the existing translation precision and coverage, complete it with translations of verbs and adjectives and enhance its evaluation method, demonstrating the validity of the approach. In addition to the main result called WoNeF, we produce two additional versions : a high-precision version with 93% precision (up to 97% on nouns) and a high-coverage version with 109,447 (literal, synset) pairs. MOTS-CLÉS : WordNet, désambiguïsation lexicale, traduction, ressource. KEYWORDS: WordNet, Word Sense Disambiguation, translation, resource.  76  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Discriminative statistical approaches for multilingual speech understanding Statistical approaches are now widespread in the various applications of natural language processing and the elicitation of an approach usually depends on the targeted task. This paper presents a comparison between the methods used for machine translation and speech understanding. This comparison allows to propose a uniﬁed approach to perform a joint decoding which translates a sentence and assign semantic tags to the translation at the same time. This decoding is achieved through a ﬁnite-state transducer approach which allows to compose a translation graph with an understanding graph. This representation can be generalized to allow the rich transmission of information between the components of a human-machine vocal interface. MOTS-CLÉS : compréhension multilingue, système de dialogue, CRF, graphes d’hypothèses. KEYWORDS: multilingual understanding, dialogue system, CRF, hypothesis graphs.  
Automatically identifying implicit discourse relations using annotated data and raw corpora This paper presents a system for identifying « implicit » discourse relations (that is, relations that are not marked by a discourse connective). Given the little amount of available annotated data for this task, our system also resorts to additional automatically labeled data wherein unambiguous connectives have been suppressed and used as relation labels, a method introduced by (Marcu et Echihabi, 2002). As shown by (Sporleder et Lascarides, 2008) for English, this approach doesn’t generalize well to implicit relations as annotated by humans. We show that the same conclusion applies to French due to important distribution differences between the two types of data. In consequence, we propose various simple methods, all inspired from work on domain adaptation, with the aim of better combining annotated data and artiﬁcial data. We evaluate these methods through various experiments carried out on the ANNODIS corpus : our best system reaches a labeling accuracy of 45.6%, corresponding to a 5.9% signiﬁcant gain over a system solely trained on manually labeled data. MOTS-CLÉS : analyse du discours, relations implicites, apprentissage automatique. KEYWORDS: discourse analysis, implicit relations, machine learning.  104  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
A Supervised learning for the identiﬁcation of semantic relations in parallel enumerative structures This work falls within the framework of ontology engineering and learning from encyclopedic or scientiﬁc texts. Our original contribution lies within the extraction of semantic relations expressed beyond the text linearity. To this end, we relied on the semantics behind the typo-dispositional characters whose function is to supplement the strictly linguistic formulations that could be more difﬁcult to exploit. The work reported here is dealing with the semantic relations carried by the parallel enumerative structures. Although they display discontinuities between their various components, these enumerative structures form a whole at the semantic level. They are textual structures that are prone to hierarchic relations. After deﬁning a typology of the relationships carried by this type of structure, we are proposing a learning approach aimed at their identiﬁcation. Based on features including lexico-syntactic and typo-dispositional informations, the ﬁrst results led an accuracy of 61.1%. MOTS-CLÉS : extraction de relations, structures énumératives parallèles, mise en forme maté- rielle, apprentissage supervisé, construction d’ontologies. KEYWORDS: relationship extraction, parallel enumerative structures, material shaping, supervi- sed learning, ontology learning.  132  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
K5$. * A"A%# * A#%.%;2. * "S"A2"2$?;. * ?T * 25% * -(%#[ * ?A2$?;. * $;2%U#"2%S * $;2? * 25%* ?;)$;% * ,?#A(. * N,$%;2%V2 * .? * ". * 2? * @%22%# * .%#<% * " * ;%3 * "(S$%;,%Y * 6#%;,5* .,$%;2$.2. * 3#$2$;U * $; * F;U)$.5= * g% * "SS%S * A#%&,?S%S * -(%#$%. * 25"2 * S$.A)"[ * 25%* ,?;2%V2. * $; * 35$,5 * "(25?#. * ?T * .,$%;2$T$, * "#2$,)%. * $; * F;U)$.5 * .2"2% * 25%$# * #%.%"#,5 * ?@^%,2$<%=*6(#25%#`?#%B*;%3*T(;,2$?;")*?A2$?;.*%;#$,5*25%*N,$%;a(%.2*$;2%#T",% * "))?3$;U * #%.()2. * 2? * @% * T$)2%#%S * T?# * ;?$.% * ";S * 25%; * ."<%S * T?# * ,?;.()2"2$?; * @[ * " * )"#U%#*A(@)$,= '#%<$?(. * .2(S$%. * ?; * 25% * .,$%;2$T$, * S$.,?(#.% * ";S * #5%2?#$, * ?T * .,$%;2$T$, * "#2$,)%. * 5"<% * 5$U5)$U52%S * 25% * $`A?#2";,% * ?T * %.2"@)$.5$;U * ";S * ?,,(A[$;U * " * #%.%"#,5* ;$,5%= * 1%#%B * T#";,?A5?;% * #%.%"#,5%#. * ";S * S?,2?#") * .2(S%;2. * 3$25?(2 * A#$?#* S$.,(#.$<% * h;?3)%SU%B * ,"; * ",,%..* "(25%;2$, * ";S * `()2$A)% * 3"[. * ?T * T?#`()"2$;U * " * #%.%"#,5 * ?@^%,2$<%= * i(# * %<")("2$?; * ?T * " * 2%.2 * ,?#A(. * .5?3%S * "; * ?<%#"))* ",,(#",[*?T*Hc=d*e=* !iKN&\0MN*Y*";U)"$.B*A"2#?;.*)%V$,?&.[;2"V$-(%.B*N,$%;a(%.2B*N,$%;2%V2= jFkgiLEN*Y*FN'B*)%V$,?&.[;2",2$,*A"22%#;.B*N,$%;a(%.2B*N,$%;2%V2=  146  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne ! "#$%&'()$*&#  
Construction of a Free Large Part-of-Speech Annotated Corpus in French This paper studies the possibility of creating a new part-of-speech annotated corpus in French from an existing one. The objectives are to propose an exit from the restrictive licence of the source corpus and to obtain a perpetual modernisation of texts. Results show that it is possible to train a state-of-the-art POS-tagger from an automatically tagged corpus if this one is large enough. MOTS-CLÉS : corpus arboré, construction de corpus, étiquetage morpho-syntaxique. KEYWORDS: French treebank, Building a corpus, Part-of-Speech Tagging.  
Towards a treebank of spoken French We present the ﬁrst results of an attempt to build a spoken treebank for French. It has been conducted as part of the ANR project Etape (resp. G. Gravier). Contrary to other languages such as English (see the Switchboard treebank (Meteer, 1995)), there is no sizable spoken corpus for French annotated for syntactic constituents and grammatical functions. Our project is to build such a resource which will be a natural extension of the Paris 7 treebank (FTB : (Abeillé et al., 2003))) for written French, in order to be able to compare with similar annotations written and spoken French. We have reused and adapted the parser (Petrov et al., 2006) which has been trained on the written treebank, with manual correction and validation. The ﬁrst results are promising. MOTS-CLÉS : Corpus arboré, français parlé, corpus oral, analyse syntaxique automatique. KEYWORDS: Treebank, spoken French, spoken corpus, parsing.  
A probabilistic segment model combining lexical cohesion and disruption for topic segmentation Identifying topical structure in any text-like data is a challenging task. Most existing techniques rely either on maximizing a measure of the lexical cohesion or on detecting lexical disruptions. A novel method combining the two criteria so as to obtain the best trade-off between cohesion and disruption is proposed in this paper. A new statistical model is deﬁned, based on the work of Isahara and Utiyama (2001), maintaining the properties of domain independence and limited a priori of the latter. Evaluations are performed both on written texts and on automatic transcripts of TV shows, the latter not respecting the norms of written texts, thus increasing the difﬁculty of the task. Experimental results demonstrate the relevance of combining lexical cohesion and disrupture. MOTS-CLÉS : segmentation thématique, cohésion lexicale, rupture de cohésion, journaux télévisés. KEYWORDS: topic segmentation, lexical cohesion, lexical disrupture, TV broadcast news.  202  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Treating ellipsis : two abstract categorial grammar perspectives The treatment of ellipsis in models of the syntax-semantics interface is troublesome as the linguistic material removed in the phonologic interpretation is still necessary in the semantics. Examples are particular cases of coordination, especially the ones involving verbal phrase ellipsis or subject elision. We show a way to use abstract categorial grammars so as to implement a pattern we call extraction/instantiation in order to deal with some of these phenomena ; we exhibit two different constructions of this principle into ACGs. MOTS-CLÉS : ellipse, coordination, interface syntaxe-sémantique, grammaires catégorielles abstraites, grammaires d’arbres adjoints, grammaires IO d’arbres. KEYWORDS: ellipsis, coordination, syntax-semantics interface, abstract categorial grammars, tree-adjoining grammars, IO tree-grammars.  ∗. Ce travail a été ﬁnancé par la DFG, dans le cadre du projet SFB 991 “Die Struktur von Repräsentationen in Sprache, Kognition und Wissenschaft”.  215  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Chunks and the notion of activation : a facilitation model for sentence processing We propose in this paper to integrate the notion of chunk within a global architecture for sentence processing. Chunks play an important role in cognitive theories such as ACT-R cite Anderson04 : they constitute global processing units which can be accessed directly via short or long term memory buffers. Chunks are built on the basis of an activation function evaluating their relationship to the context. We propose an interpretation of this theory applied to parsing. A construction mechanism is proposed, based on an adapted version of the activation function which takes advantage of the representation of linguistic information in terms of constraints. This feature allows to show how chunks are easy to build and how they can facilitate treatment. Several examples are given, illustrating this hypothesis of facilitation. MOTS-CLÉS : Chunks, ACT-R, activation, mémoire, parsing, traitement de la phrase, expérimen- tation. KEYWORDS: Chunks, ACT-R, activation, memory, parsing, sentence processing, experimentation.  229  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Bilingual Lexicon Extraction from Comparable Corpora by Combining Contextual Representations Word’s context characterisation constitute the heart of most methods of bilingual lexicon extraction from comparable corpora. In this article, we ﬁrst revisit the two main strategies of context representation, that is : the window-based and the syntactic based context representation. Secondly, we propose two new methods that exploit jointly these different representations . Our experiments show a signiﬁcant improvement of the results obtained on two different domain speciﬁc comparable corpora. MOTS-CLÉS : Multilingualisme, corpus comparables, lexique bilingue, vecteurs de contexte, dépendances syntaxiques. KEYWORDS: Multilingualism, comparable corpora, bilingual lexicon, context vectors, syntactic dependencies.  
Unsupervised CRF for knowledge discovery Knowledge discovery aims at bringing out coherent groups of entities. They are usually based on clustering ; the challenge is then to deﬁne a notion of similarity between the relevant entities. In this paper, we propose to divert Conditional Random Fields (CRF), which have shown their interest in supervised labeling tasks, in order tocalculate indirectly the similarities among text sequences. Our approach consists in generate artiﬁcial labeling problems on the data to be processed to reveal regularities in the labeling of the entities. We describe how this framework can be implemented and experiment it on two information retrieval tasks. The results demonstrate the usefulness of this unsupervised approach, which opens many avenues for deﬁning similarities for complex representations of sequential data. MOTS-CLÉS : Découverte de connaissances, CRF, clustering, apprentissage non-supervisé, ex- traction d’informations. KEYWORDS: Knowledge discovery, CRF, clustering, unsupervised machine learning, information extraction.  257  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Automatic tagging of a learner corpus of English with a modified version of the Penn Treebank tagset This article covers the issue of automatic annotation of a learner corpus of English. The objective is to show that it is possible to PoS-tag the corpus with a tagger to prepare the ground for learner error analysis. However, in order to have a fine-grain analysis, some functional tags for the study of specific linguistic points are inserted within the tagger's tagset. This tagger is trained on a native-English corpus with an extended tagset and the tagging is done on the learner corpus. This experiment focuses on the incorrect use of this and that by learners. We show how the insertion of a functional layer by way of new tags for the forms allows us to discriminate varying uses among natives and non-natives. This opens the path to the identification of incorrect patterns of use. The functional tags cast a light on the way the discourse functions. MOTS-CLÉS : Apprentissage L2, corpus d'apprenants, analyse linguistique d'erreurs, étiquetage automatique, this, that KEYWORDS : Second Language Acquisition, learner corpus, linguistic error analysis, automated tagging, this, that  271  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
GLÀFF, a Large Versatile French Lexicon This paper introduces GLÀFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLÀFF contains, for each entry, a morphosyntactic description and a phonetic transcription. It distinguishes itself from the other available lexicons mainly by its size, its potential for constant updating and its copylefted license that makes it available for use, modiﬁcation and redistribution. We explain how we have built GLÀFF and compare it to other known resources. We show that its size and quality are strong assets that could allow GLÀFF to become a reference lexicon for NLP, linguistics and psycholinguistics. MOTS-CLÉS : Lexique morpho-phonologique, ressources lexicales libres, Wiktionnaire. KEYWORDS: Morpho-phonological lexicon, free lexical resources, French Wiktionary.  
Identification, Alignment, and Tranlsation of Relational Adjectives from Comparable Corpora In this paper, we extract French relational adjectives and automatically align them with the nouns they are derived from by using a monolingual corpus. The obtained adjective-noun alignments are then used in the compositional translation of compound nouns of the form [N ADJR] with a French-English comparable corpora. A new term [N N￿] (eg, cancer du poumon) is obtained by replacing the relational adjective Ad jR (eg, pulmonaire) in [N AdjR] (eg, cancer pulmonaire) by its corresponding N￿ (eg, poumon). If no translation(s) are obtained for [N AdjR], we consider the one(s) obtained for its paraphrase [N N￿]. We experiment with a comparable corpora in the field of breast cancer, and we get adjective-noun alignments that help in translating French compound nouns of the form [N AdjR] to English with a precision of 86%. MOTS-CLÉS : Adjectifs relationnels, Corpus comparables, Méthode compositionnelle, Termes complexes. KEYWORDS: Relational adjectives, Comparable corpora, Compositional method, Complex terms.  
 This paper presents a new method that aims to improve the results of the standard approach used for bilingual lexicon extraction from specialized comparable corpora. We attempt to solve the problem of context vector word polysemy. Instead of using all the entries of the dictionary to translate a context vector, we only use the words of the lexicon that are more likely to give the best characterization of context vectors in the target language. On two specialised French-English comparable corpora, empirical experimental results show that our method improves the results obtained by the standard approach especially when many words are ambiguous. MOTS-CLÉS : lexique bilingue, corpus comparable spécialisé, désambiguïsation sémantique, WordNet. KEYWORDS: bilingual lexicon, specialized comparable corpora, semantic disambiguation, Word- Net. 
Inductive and deductive inferences in a Crowdsourced Lexical-Semantic Network In Computational Linguistics, validated lexical-semantic networks are crucial resources. Regardless the construction strategies used, automatically inferring new relations from already existing ones may improve coverage and global quality of the resource. In this context, an inference engine aims at producing new conclusions (i.e. potential relations) from premises (pre-existing relations). The approach we propose is based on a triangulation method involving the semantic transitivity with a blocking mechanism to avoid proposing dubious relations. Inferred relations are then proposed to contributors to be validated or rejected. In cas of invalidation, a reconciliation strategy is implemented to identify the cause of the erroneous inference : an exception, an error in the premises, or a confusion caused by polysemy. MOTS-CLÉS : inférence de relations, réconciliation, enrichissement, réseau lexical, peuplo- nomie. KEYWORDS: relation inferences, reconcialiation, enrichment, lexical network, crowdsour- cing.  
Semantic relation clustering for unsupervised information extraction Most studies in unsupervised information extraction concentrate on the relation extraction and few work has been proposed on the organization of the extracted relations. We present in this paper a two-step clustering procedure to group semantically equivalent relations : a ﬁrst step clusters relations with similar expressions while a second step groups these ﬁrst clusters into larger semantic clusters, using different semantic similarities. Our experiments show the stability of distributional similarities over WordNet-based similarities for semantic clustering. We also demonstrate that the use of a multi-level clustering not only reduces the calculations from all relation pairs to basic clusters pairs, but it also improves the clustering results. MOTS-CLÉS : Extraction d’Information Non Supervisée, Similarité Sémantique, Clustering. KEYWORDS: Unsupervised Information Extraction, Semantic Similarity, Relation Clustering.  
On the semantics of determiners in a rich type-theoretical framework The variation of word meaning according to the context led us to enrich the type system of our syntactical and semantic analyser of French based on categorial grammars and Montague semantics (or lambda-DRT). The main advantage of a deep semantic analyse is too represent meaning by logical formulae that can be easily used e.g. for inferences. Determiners and quantiﬁers play a fundamental role in the construction of those formulae and we needed to provide them with semantic terms adapted to this new framework. We propose a solution inspired by the tau and epsilon operators of Hilbert, generic elements that resemble choice functions. This approach uniﬁes the treatment of the different determiners and quantiﬁers and allows a dynamic binding of pronouns. Above all, this fully computational view of determiners ﬁts in well within the wide coverage parser Grail, both from a theoretical and a practical viewpoint. MOTS-CLÉS : Analyse sémantique automatique, Sémantique formelle, Compositionnalité. KEYWORDS: Automated semantic analysis, Formal Semantics, Compositional Semantics.  367  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Parallel areas detection in multi-documents for multilingual alignment This article broaches a central issue of the automatic alignment : diagnosing the parallelism of documents. Previous research was concentrated on the analysis of documents which are parallel by nature such as corpus of regulations, technical documents or simple sentences. Inversions and deletions/additions phenomena that may exist between different versions of a document has often been overlooked. To the contrary, we propose a method to diagnose in context the parallel areas allowing the detection of deletions or inversions between documents to align. This original method is based on the freeing from word and sentence as well as the consideration of the text formatting. The implementation is based on the detection of repeated character strings and the identiﬁcation of parallel segments by image processing. MOTS-CLÉS : détection et alignement de zones, appariement de N-grammes de caractères, corpus de multidocuments. KEYWORDS: area detection and alignment, character N-grams matching, multidocuments corpora.  381  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Translating verbs between MSA and arabic dialects through deep morphological analysis The developpment of NLP tools for dialects faces the severe problem of lack of resources for such dialects. In the case of diglossia, as in arabic, a variant of arabic, Modern Standard Arabic, exists, for which many resources have been developped which can be used to build NLP tools. Taking advantage of the closeness of MSA and dialects, one way to solve the problem consist in performing a surfacic translation of the dialect into MSA in order to use the tools developped for MSA. We describe in this paper an achitecture for such a translation and we evaluate it on arabic verbs. MOTS-CLÉS : dialectes, langues peu dotées, analyse morphologique, traitement automatique de l’arabe. KEYWORDS: dialects, Arabic NLP, morphological analysis.  
Dynamic extension of a French morphological lexicon based a text stream Lexical incompleteness is a recurring problem when dealing with natural language and its variability. It seems indeed necessary today to regularly validate and extend lexica used by tools processing large amounts of textual data. This is even more true when processing real-time text ﬂows. In this context, our paper introduces techniques aimed at addressing words unknown to a lexicon. We ﬁrst study neology (from a theoretic and corpus-based point of view) and describe the modules we have developed for detecting them and inferring information about them (lemma, category, inﬂectional class). We show that we are able, using among others modules for analyzing derived and compound neologisms, to generate lexical entries candidates in real-time and with a good precision. MOTS-CLÉS : Néologismes, analyse morphologique, lexiques dynamiques. KEYWORDS: Neologisms, Morphological Analysis, Dynamic Lexica.  
Mining Partial Annotation Rules for Named Entity Recognition During the last decades, the unremitting increase of numeric data available has led to a more and more urgent need for efﬁcient solution of information retrieval (IR). This paper concerns a problematic of ﬁrst importance for the IR on linguistic data : the recognition of named entities (NE) on speech transcripts issued from radio or TV broadcasts. We present an original approach for named entity recognition which is based on data mining techniques. More precisely, we propose to adapt hierarchical sequence mining techniques to extract automatically from annotated corpora intelligible rules of NE detection. This research was carried out in the framework of the Etape NER evaluation campaign, where mXS, our text-mining based system has shown good performances challenging the best symbolic or data-driven systems MOTS-CLÉS : Entités nommées, Fouille de données, Règles d’annotation. KEYWORDS: Named Entities, Data Mining, Annotation Rules.  421  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
A fully discriminative training framework for Statistical Machine Translation A major pitfall of existing statistical machine translation systems is their lack of a proper training procedure. In fact, the phrase extraction and scoring processes that underlie the construction of the translation model typically rely on a chain of crude heuristics, a situation deemed problematic by many. In this paper, we recast machine translation in the familiar terms of a probabilistic structure learning problem, using a standard log-linear parameterization. The tractability of this enterprise is achieved through an efﬁcient implementation that can take into account all the aspects of the underlying translation process through latent variables. We also address the reference reachability issue by using oracle decoding techniques. This approach is experimentally contrasted with a state-of-the-art system on the French-English BTEC translation task. MOTS-CLÉS : Traduction Automatique, Apprentissage Discriminant. KEYWORDS: Machine Translation, Discriminative Learning.  
Semantic Annotation in Speciﬁc Domains with rich Ontologies  Technical documentations are generally difﬁcult to explore and maintain. Powerful tools can help, but they require that the documents have been semantically annotated. The annotations must be sufﬁciently specialized, rich and consistent. They must rely on some explicit semantic model – usually an ontology – that represents the semantics of the target domain. We observed that traditional approaches have limited success on this task and we propose a novel approach, phrase-based statistical semantic annotation, for predicting semantic annotations from a limited training data set. Such a modeling makes the challenging problem, domain speciﬁc semantic annotation regarding arbitrarily rich semantic models, easily handled. Our approach achieved a good performance, with several evaluation metrics and on two different business regulatory texts. In particular, it obtained 91.9 % and 97.65 % F-measure in the label and position predictions with different settings. This suggests that human annotators can be highly supported in domain speciﬁc semantic annotation tasks. MOTS-CLÉS : Annotation sémantique, Ontologie de domaine, Annotation automatique, Analyse sémantique des textes, Méthodes statistiques.  KEYWORDS: Semantic Annotation, Domain Ontology, Automatic annotation, Semantic Text Analysis, Statistical methods.  464  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
 Web pages segmentation for document selection in Question Answering In this paper, we study two different kinds of web pages segmentation for document selection in question answering. The segmentation is applied prior to indexation in addition to the traditionnal passage retrieval step in question answering. In both cases, the segmentation is textual and processed once the web pages textual content has been extracted using our own extraction system. In the ﬁrst case, a document is tilled homogeneously in text blocs of ﬁxed size while in the second case the segmentation is based on the TextTiling algorithm (Hearst, 1997). Evaluation on 309 factoid questions and a collection of 500K French web pages, coming from the Quaero project (Quintard et al., 2010), showed that such approaches tend to support properly the RITEL–QR system (Rosset et al., 2008) in this task.  MOTS-CLÉS : pages web, TextTiling, sélection de documents, questions-réponses, Quaero, Ritel, segmentation textuelle, segmentation thématique. KEYWORDS: web pages, TextTiling, document selection, question answering, Quaero, Ritel, textual segmentation, topic segmentation.  479  ￿c ATALA  TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 
Studying frequency-based approaches to process lexical simpliﬁcation Lexical simpliﬁcation aims at replacing words or phrases by simpler equivalents. In this paper, we present three models for lexical simpliﬁcation, focusing on the criteria that make one word simpler to read and understand than another. We tested different contexts of the considered word : no context, with a model based on word frequencies in a simpliﬁed English corpus ; a few words context, with n-grams probabilites on Web data, and an extended context, with a model based on co-occurrence frequencies. MOTS-CLÉS : simpliﬁcation lexicale, fréquence lexicale, modèle de langue. KEYWORDS: lexical simpliﬁcation, lexical frequency, language model.  
Since many applications such as timeline summaries and temporal IR involving temporal analysis rely on document timestamps, the task of automatic dating of documents has been increasingly important. Instead of using feature-based methods as conventional models, our method attempts to date documents in a year level by exploiting relative temporal relations between documents and events, which are very effective for dating documents. Based on this intuition, we proposed an eventbased time label propagation model called conﬁdence boosting in which time label information can be propagated between documents and events on a bipartite graph. The experiments show that our event-based propagation model can predict document timestamps in high accuracy and the model combined with a MaxEnt classiﬁer outperforms the state-ofthe-art method for this task especially when the size of the training set is small. 
In this paper we classify the temporal relations between pairs of events on an article-wide basis. This is in contrast to much of the existing literature which focuses on just event pairs which are found within the same or adjacent sentences. To achieve this, we leverage on discourse analysis as we believe that it provides more useful semantic information than typical lexico-syntactic features. We propose the use of several discourse analysis frameworks, including 1) Rhetorical Structure Theory (RST), 2) PDTB-styled discourse relations, and 3) topical text segmentation. We explain how features derived from these frameworks can be effectively used with support vector machines (SVM) paired with convolution kernels. Experiments show that our proposal is effective in improving on the state-of-the-art signiﬁcantly by as much as 16% in terms of F1, even if we only adopt less-than-perfect automatic discourse analyzers and parsers. Making use of more accurate discourse analysis can further boost gains to 35%. 
Distant supervision is a scheme to generate noisy training data for relation extraction by aligning entities of a knowledge base with text. In this work we combine the output of a discriminative at-least-one learner with that of a generative hierarchical topic model to reduce the noise in distant supervision data. The combination signiﬁcantly increases the ranking quality of extracted facts and achieves state-of-the-art extraction performance in an end-to-end setting. A simple linear interpolation of the model scores performs better than a parameter-free scheme based on nondominated sorting. 
Children learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process. Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically during learning. Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks. We test on child-directed utterances in English and Spanish and compare to single-task baselines. In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization. These results provide further evidence that joint learning is useful, but also suggest that the beneﬁts may be different for typologically different languages. 
We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We deﬁne the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed ﬁrst, our model is qualitatively more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence. 
Animacy detection is a problem whose solution has been shown to be beneﬁcial for a number of syntactic and semantic tasks. We present a state-of-the-art system for this task which uses a number of simple classiﬁers with heterogeneous data sources in a voting scheme. We show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error.  only to reweight their votes. We show that such an approach can indeed result in a high performing system, with animacy detection accuracies in the mid 90% range, which compares well with other reported rates. Ensemble methods are well known (see for example, Dietterich (2000)) but our focus here is on using them for interpretability while still maintaining accuracy. 2 Previous Work  
We present a uniﬁed unsupervised statistical model for text normalization. The relationship between standard and non-standard tokens is characterized by a log-linear model, permitting arbitrary features. The weights of these features are trained in a maximumlikelihood framework, employing a novel sequential Monte Carlo training algorithm to overcome the large label space, which would be impractical for traditional dynamic programming solutions. This model is implemented in a normalization system called UNLOL, which achieves the best known results on two normalization datasets, outperforming more complex systems. We use the output of UNLOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation. 
Compared to the edited genres that have played a central role in NLP research, microblog texts use a more informal register with nonstandard lexical items, abbreviations, and free orthographic variation. When confronted with such input, conventional text analysis tools often perform poorly. Normalization — replacing orthographically or lexically idiosyncratic forms with more standard variants — can improve performance. We propose a method for learning normalization rules from machine translations of a parallel corpus of microblog messages. To validate the utility of our approach, we evaluate extrinsically, showing that normalizing English tweets and then translating improves translation quality (compared to translating unnormalized text) using three standard web translation services as well as a phrase-based translation system trained on parallel microblog data. 
In this paper, we address the problem of estimating question difﬁculty in community question answering services. We propose a competition-based model for estimating question difﬁculty by leveraging pairwise comparisons between questions and users. Our experimental results show that our model signiﬁcantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reﬂects the question difﬁculty. This implies the possibility of predicting question difﬁculty from the text of question descriptions. 
We seek to measure political candidates’ ideological positioning from their speeches. To accomplish this, we infer ideological cues from a corpus of political writings annotated with known ideologies. We then represent the speeches of U.S. Presidential candidates as sequences of cues and lags (ﬁller distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 
We present a novel model, Freestyle, that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics, by combining both bottomup token based rule induction and top-down rule segmentation strategies to learn a stochastic transduction grammar that simultaneously learns both phrasing and rhyming associations. In this attack on the woefully under-explored natural language genre of music lyrics, we exploit a strictly unsupervised transduction grammar induction approach. Our task is particularly ambitious in that no use of any a priori linguistic or phonetic information is allowed, even though the domain of hip hop lyrics is particularly noisy and unstructured. We evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction, and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses, measured on fluency and rhyming criteria as judged by human evaluators. To highlight some of the inherent challenges in adapting other algorithms to this novel task, we also compare the quality of the responses generated by our model to those generated by an out-ofthe-box phrase based SMT system. We tackle the challenge of selecting appropriate training data for our task via a dedicated rhyme scheme detection module, which is also acquired via unsupervised learning and report improved quality of the generated responses. Finally, we report results with Maghrebi French hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages.  
When reviewing scientiﬁc literature, it would be useful to have automatic tools that identify the most inﬂuential scientiﬁc articles as well as how ideas propagate between articles. In this context, this paper introduces topical inﬂuence, a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it. Given the text of the articles and their citation graph, we show how to learn a probabilistic model to recover both the degree of topical inﬂuence of each article and the inﬂuence relationships between articles. Experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach. 
 mean”).1  We introduce a novel method to jointly parse and detect disﬂuencies in spoken utterances. Our model can use arbitrary features for parsing sentences and adapt itself with out-ofdomain data. We show that our method, based on transition-based parsing, performs at a high level of accuracy for both the parsing and disﬂuency detection tasks. Additionally, our method is the fastest for the joint task, running in linear time. 
We present a novel vector space model for semantic co-compositionality. Inspired by Generative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others’ meaning representations while generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011). 
In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Speciﬁcally, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is ﬂexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We ﬁnd that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modiﬁcation of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics. 
Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain. In this paper, we develop a novel representation learning approach to address domain adaptation for text classiﬁcation with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task. Speciﬁcally, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation. We train this latent graphical model using a simple expectation-maximization (EM) algorithm. We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classiﬁcation tasks on Amazon product review dataset. The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods. 
Two recent measures incorporate the notion of statistical signiﬁcance in basic PMI formulation. In some tasks, we ﬁnd that the new measures perform worse than the PMI. Our analysis shows that while the basic ideas in incorporating statistical signiﬁcance in PMI are reasonable, they have been applied slightly inappropriately. By ﬁxing this, we get new measures that improve performance over not just PMI but on other popular co-occurrence measures as well. In fact, the revised measures perform reasonably well compared with more resource intensive non co-occurrence based methods also. 
In this paper we present a minimallysupervised approach to the multi-domain acquisition of wide-coverage glossaries. We start from a small number of hypernymy relation seeds and bootstrap glossaries from the Web for dozens of domains using Probabilistic Topic Models. Our experiments show that we are able to extract high-precision glossaries comprising thousands of terms and definitions. 
The creation of a pronunciation lexicon remains the most inefﬁcient process in developing an Automatic Speech Recognizer (ASR). In this paper, we propose an unsupervised alternative – requiring no language-speciﬁc knowledge – to the conventional manual approach for creating pronunciation dictionaries. We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data. When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately. Furthermore, the recognizers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using conventional supervised procedures. 
We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 
This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora. 
Beam search is a fast and empirically effective method for translation decoding, but it lacks formal guarantees about search error. We develop a new decoding algorithm that combines the speed of beam search with the optimal certiﬁcate property of Lagrangian relaxation, and apply it to phrase- and syntax-based translation decoding. The new method is efﬁcient, utilizes standard MT algorithms, and returns an exact solution on the majority of translation examples in our test data. The algorithm is 3.5 times faster than an optimized incremental constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation. 
N gram language models tend to increase in size with inﬂating the corpus size, and consume considerable resources. In this paper, we propose an efﬁcient method for implementing ngram models based on doublearray structures. First, we propose a method for representing backwards sufﬁx trees using double-array structures and demonstrate its efﬁciency. Next, we propose two optimization methods for improving the efﬁciency of data representation in the double-array structures. Embedding probabilities into unused spaces in double-array structures reduces the model size. Moreover, tuning the word IDs in the language model makes the model smaller and faster. We also show that our method can be used for building large language models using the division method. Lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used. 
Language models can be formalized as loglinear regression models where the input features represent previously observed contexts up to a certain length m. The complexity of existing algorithms to learn the parameters by maximum likelihood scale linearly in nd, where n is the length of the training corpus and d is the number of observed features. We present a model that grows logarithmically in d, making it possible to efﬁciently leverage longer contexts. We account for the sequential structure of natural language using treestructured penalized objectives to avoid overﬁtting and achieve better generalization. 
Current automatic machine translation systems are not able to generate error-free translations and human intervention is often required to correct their output. Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works. Here, we describe a new interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a uniﬁed statistical framework. In our experiments, our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system. 
Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are deﬁned on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method signiﬁcantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method. 
Coreference resolution metrics quantify errors but do not analyze them. Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types. Using this tool, we ﬁrst compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task. 
Many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge. Errors in named-entity linking (NEL), on the other hand, are often caused by superﬁcial modeling of entity context. This paper demonstrates that these two tasks are complementary. We introduce NECO, a new model for named entity linking and coreference resolution, which solves both problems jointly, reducing the errors made on each. NECO extends the Stanford deterministic coreference system by automatically linking mentions to Wikipedia and introducing new NEL-informed mention-merging sieves. Linking improves mention-detection and enables new semantic attributes to be incorporated from Freebase, while coreference provides better context modeling by propagating named-entity links within mention clusters. Experiments show consistent improvements across a number of datasets and experimental conditions, including over 11% reduction in MUC coreference error and nearly 21% reduction in F1 NEL error on ACE 2004 newswire data. 
Interpreting anaphoric shell nouns (ASNs) such as this issue and this fact is essential to understanding virtually any substantial natural language text. One obstacle in developing methods for automatically interpreting ASNs is the lack of annotated data. We tackle this challenge by exploiting cataphoric shell nouns (CSNs) whose construction makes them particularly easy to interpret (e.g., the fact that X). We propose an approach that uses automatically extracted antecedents of CSNs as training data to interpret ASNs. We achieve precisions in the range of 0.35 (baseline = 0.21) to 0.72 (baseline = 0.44), depending upon the shell noun. 
Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation. However, the ability of these models is restricted by the availability of annotated data and the design of features. We propose a scalable semi-supervised feature engineering approach. In contrast to previous works using pre-deﬁned taskspeciﬁc features with ﬁxed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus. We update the representation values with a semi-supervised approach. Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961. The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task. 
Training higher-order conditional random ﬁelds is prohibitive for huge tag sets. We present an approximated conditional random ﬁeld using coarse-to-ﬁne decoding and early updating. We show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higher-order models give signiﬁcant improvements over 1st-order models. 
Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms proﬁts from taking the syntactic context of words into account and results in state-ofthe-art models. 
This paper contributes an approach for expressing non-concatenative morphological phenomena, such as stem derivation in Semitic languages, in terms of a mildly context-sensitive grammar formalism. This offers a convenient level of modelling abstraction while remaining computationally tractable. The nonparametric Bayesian framework of adaptor grammars is extended to this richer grammar formalism to propose a probabilistic model that can learn word segmentation and morpheme lexicons, including ones with discontiguous strings as elements, from unannotated data. Our experiments on Hebrew and three variants of Arabic data ﬁnd that the additional expressiveness to capture roots and templates as atomic units improves the quality of concatenative segmentation and stem identiﬁcation. We obtain 74% accuracy in identifying triliteral Hebrew roots, while performing morphological segmentation with an F1-score of 78.1. 
This paper describes a method that predicts which trades players execute during a winlose game. Our method uses data collected from chat negotiations of the game The Settlers of Catan and exploits the conversation to construct dynamically a partial model of each player’s preferences. This in turn yields equilibrium trading moves via principles from game theory. We compare our method against four baselines and show that tracking how preferences evolve through the dialogue and reasoning about equilibrium moves are both crucial to success. 
Human engagement in narrative is partially driven by reasoning about discourse relations between narrative events, and the expectations about what is likely to happen next that results from such reasoning. Researchers in NLP have tackled modeling such expectations from a range of perspectives, including treating it as the inference of the CONTINGENT discourse relation, or as a type of common-sense causal reasoning. Our approach is to model likelihood between events by drawing on several of these lines of previous work. We implement and evaluate different unsupervised methods for learning event pairs that are likely to be CONTINGENT on one another. We reﬁne event pairs that we learn from a corpus of ﬁlm scene descriptions utilizing web search counts, and evaluate our results by collecting human judgments of contingency. Our results indicate that the use of web search counts increases the average accuracy of our best method to 85.64% over a baseline of 50%, as compared to an average accuracy of 75.15% without web search. 
This paper addresses the task of predicting the correct French translations of third-person subject pronouns in English discourse, a problem that is relevant as a prerequisite for machine translation and that requires anaphora resolution. We present an approach based on neural networks that models anaphoric links as latent variables and show that its performance is competitive with that of a system with separate anaphora resolution while not requiring any coreference-annotated training data. This demonstrates that the information contained in parallel bitexts can successfully be used to acquire knowledge about pronominal anaphora in an unsupervised way. 
In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment. Their representations of the shared world are misaligned. Thus referring expression generation (REG) will need to take this discrepancy into consideration. To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment. Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%. However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%). This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue. 
This paper introduces a method for extracting ﬁne-grained class labels (“countries with double taxation agreements with india”) from Web search queries. The class labels are more numerous and more diverse than those produced by current extraction methods. Also extracted are representative sets of instances (singapore, united kingdom) for the class labels. 
In this paper we present an unsupervised approach to relational information extraction. Our model partitions tuples representing an observed syntactic relationship between two named entities (e.g., “X was born in Y” and “X is from Y”) into clusters corresponding to underlying semantic relation types (e.g., BornIn, Located). Our approach incorporates general domain knowledge which we encode as First Order Logic rules and automatically combine with a topic model developed speciﬁcally for the relation extraction task. Evaluation results on the ACE 2007 English Relation Detection and Categorization (RDC) task show that our model outperforms competitive unsupervised approaches by a wide margin and is able to produce clusters shaped by both the data and the rules. 
Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base. Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes. We propose a fast collective disambiguation approach based on stacking. First, we train a local predictor g0 with learning to rank as base learner, to generate initial ranking list of candidates. Second, top k candidates of related instances are searched for constructing expressive global coherence features. A global predictor g1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem. The proposed method is fast and easy to implement. Experiments show its effectiveness over various algorithms on several public datasets. By learning a rich semantic relatedness measure between entity categories and context document, performance is further improved. 
Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase. Entity annotators need to be trained on sample mention snippets. Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation. Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these are a highly biased sample of well-connected, frequently mentioned “head” entities. To bring hope to “tail” entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia. The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities. We present TMI, a bipartite graphical model for joint type-mention inference. TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy. In experiments involving 780,000 people in Wikipedia, 2.3 million people in Freebase, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for “tail” and emerging entities. We also compare with Google’s recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain. 
A large number of Open Relation Extraction approaches have been proposed recently, covering a wide range of NLP machinery, from “shallow” (e.g., part-of-speech tagging) to “deep” (e.g., semantic role labeling–SRL). A natural question then is what is the tradeoff between NLP depth (and associated computational cost) versus effectiveness. This paper presents a fair and objective experimental comparison of 8 state-of-the-art approaches over 5 different datasets, and sheds some light on the issue. The paper also describes a novel method, EXEMPLAR, which adapts ideas from SRL to less costly NLP machinery, resulting in substantial gains both in efﬁciency and effectiveness, over binary and n-ary relation extraction tasks. 
This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classiﬁers, e.g., question classiﬁer and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction. The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction, while using no additional resources and no manual feature engineering. 
Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of speciﬁc domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all conﬁgurations tested. 
Joint compression and summarization has been used recently to generate high quality summaries. However, such word-based joint optimization is computationally expensive. In this paper we adopt the ‘sentence compression + sentence selection’ pipeline approach for compressive summarization, but propose to perform summary guided compression, rather than generic sentence-based compression. To create an annotated corpus, the human annotators were asked to compress sentences while explicitly given the important summary words in the sentences. Using this corpus, we train a supervised sentence compression model using a set of word-, syntax-, and documentlevel features. During summarization, we use multiple compressed sentences in the integer linear programming framework to select salient summary sentences. Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sentence compression model, our summarization system can yield signiﬁcant performance gain as compared to the state-of-the-art. 
Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the conﬁne of translation units. We present the “Anchor Graph” (AG) model where we use a graph structure to model global contextual information that is crucial for reordering. The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors. As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent. We integrate our proposed model into a state-of-the-art translation system and demonstrate the efﬁcacy of our proposal in a largescale Chinese-to-English translation task. 
This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a "bridge" to generate source-target translation. However, one of the weaknesses is that some useful sourcetarget translations cannot be generated if the corresponding source phrase and target phrase connect to different pivot phrases. To alleviate the problem, we utilize Markov random walks to connect possible translation phrases between source and target language. Experimental results on European Parliament data, spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system. 
This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques. In this area, most of techniques usually utilize confusion networks (CN) as their central data structure to compact an exponential number of an potential hypotheses, and because better hypothesis alignment may beneﬁt constructing better quality confusion networks, it is natural to add more useful information to improve alignment results. However, these information may be heterogeneous, so the widely-used Viterbi algorithm for searching the best alignment may not apply here. In the multi-objective optimization framework, each information source is viewed as an independent objective, and a new goal of improving all objectives can be searched by mature algorithms. The solutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks. Experiments on two Chinese-to-English translation datasets show signiﬁcant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems. 
Machine translation beneﬁts from system combination. We propose ﬂexible interaction of hypergraphs as a novel technique combining diﬀerent translation models within one decoder. We introduce features controlling the interactions between the two systems and explore three interaction schemes of hiero and forest-to-string models—speciﬁcation, generalization, and interchange. The experiments are carried out on large training data with strong baselines utilizing rich sets of dense and sparse features. All three schemes significantly improve results of any single system on four testsets. We ﬁnd that speciﬁcation—a more constrained scheme that almost entirely uses forest-to-string rules, but optionally uses hiero rules for shorter spans—comes out as the strongest, yielding improvement up to 0.9 (Ter-Bleu)/2 points. We also provide a detailed experimental and qualitative analysis of the results. 
This paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system. In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language. The features produced by both models are used as soft constraints to guide the translation process. Experiments on Chinese-English translation show that the proposed approach signiﬁcantly improves a strong string-to-dependency translation system on multiple evaluation sets. 
While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge. Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively. The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling’s perspective. Experiments on the NIST 2008 dataset show that our system signiﬁcantly improves over the MaxEnt classiﬁer by 1.07 BLEU points. 
In this paper, we analyze a novel set of features for the task of automatic edit category classiﬁcation. Edit category classiﬁcation assigns categories such as spelling error correction, paraphrase or vandalism to edits in a document. Our features are based on differences between two versions of a document including meta data, textual and language properties and markup. In a supervised machine learning experiment, we achieve a micro-averaged F1 score of .62 on a corpus of edits from the English Wikipedia. In this corpus, each edit has been multi-labeled according to a 21-category taxonomy. A model trained on the same data achieves state-of-the-art performance on the related task of ﬂuency edit classiﬁcation. We apply pattern mining to automatically labeled edits in the revision histories of different Wikipedia articles. Our results suggest that high-quality articles show a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles. 
We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets (RTE and paraphrase), while doing signiﬁcantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential beneﬁt of our alignment model to RTE, paraphrase identiﬁcation and question answering, where even a naive application of our model’s alignment score approaches the state of the art. 
Coreference resolution is a well known clustering task in Natural Language Processing. In this paper, we describe the Latent Left Linking model (L3M), a novel, principled, and linguistically motivated latent structured prediction approach to coreference resolution. We show that L3M admits efﬁcient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning. Experiments on ACE and Ontonotes data show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature. 
The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects. In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an eﬀective way to reduce hubs. We show analytically why hubs emerge and why they are suppressed by centering, under a simple probabilistic model of data. To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering. Our experimental results show that (weighted) centering is eﬀective for natural language data; it improves the performance of the k-nearest neighbor classiﬁers considerably in word sense disambiguation and document classiﬁcation tasks. 
We derive a spectral method for unsupervised learning of Weighted Context Free Grammars. We frame WCFG induction as ﬁnding a Hankel matrix that has low rank and is linearly constrained to represent a function computed by inside-outside recursions. The proposed algorithm picks the grammar that agrees with a sample and is the simplest with respect to the nuclear norm of the Hankel matrix. 
We address the problem of identifying multiword expressions in a language, focusing on English phrasal verbs. Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages. Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set of English phrasal verbs, achieving performance comparable to a human-curated set. 
This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning. We try to avoid task-speciﬁc feature engineering, and use deep layers of neural networks to discover relevant features to the tasks. We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models. Our networks achieved close to state-of-theart performance with minimal computational cost. We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speed up the training process and make the learning algorithm easier to be implemented. 
Chinese word segmentation and part-ofspeech tagging (S&T) are fundamental steps for more advanced Chinese language processing tasks. Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T. In this paper, we propose a uniﬁed model for Chinese S&T with heterogeneous annotation corpora. We ﬁrst automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD). Then we regard the Chinese S&T with heterogeneous corpora as two “related” tasks and train our model on two heterogeneous corpora simultaneously. Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves signiﬁcant improvements over the state-of-the-art methods. 
Studies of the graph of dictionary deﬁnitions (DD) (Picard et al., 2009; Levary et al., 2012) have revealed strong semantic coherence of local topological structures. The techniques used in these papers are simple and the main results are found by understanding the structure of cycles in the directed graph (where words point to deﬁnitions). Based on our earlier work (Levary et al., 2012), we study a different class of word deﬁnitions, namely those of the Free Association (FA) dataset (Nelson et al., 2004). These are responses by subjects to a cue word, which are then summarized by a directed, free association graph. We ﬁnd that the structure of this network is quite different from both the Wordnet and the dictionary networks. This difference can be explained by the very nature of free association as compared to the more “logical” construction of dictionaries. It thus sheds some (quantitative) light on the psychology of free association. In NLP, semantic groups or clusters are interesting for various applications such as word sense disambiguation. The FA graph is tighter than the DD graph, because of the large number of triangles. This also makes drift of meaning quite measurable so that FA graphs provide a quantitative measure of the semantic coherence of small groups of words. 
Creating a language-independent meaning representation would beneﬁt many crosslingual NLP tasks. We introduce the ﬁrst unsupervised approach to this problem, learning clusters of semantically equivalent English and French relations between referring expressions, based on their named-entity arguments in large monolingual corpora. The clusters can be used as language-independent semantic relations, by mapping clustered expressions in different languages onto the same relation. Our approach needs no parallel text for training, but outperforms a baseline that uses machine translation on a cross-lingual question answering task. We also show how to use the semantics to improve the accuracy of machine translation, by using it in a simple reranker. 
In this paper we propose a two-stage method to acquire contradiction relations between typed lexico-syntactic patterns such as Xdrug prevents Ydisease and Ydisease caused by Xdrug. In the ﬁrst stage, we train an SVM classiﬁer to detect contradiction pattern pairs in a large web archive by exploiting the excitation polarity (Hashimoto et al., 2012) of the patterns. In the second stage, we enlarge the ﬁrst stage classiﬁer’s training data with new contradiction pairs obtained by combining the output of the ﬁrst stage’s classiﬁer and that of an entailment classiﬁer. We acquired this way 750,000 typed Japanese contradiction pattern pairs with an estimated precision of 80%. We plan to release this resource to the NLP community. 
A common form of sarcasm on Twitter consists of a positive sentiment contrasted with a negative situation. For example, many sarcastic tweets include a positive sentiment, such as “love” or “enjoy”, followed by an expression that describes an undesirable activity or state (e.g., “taking exams” or “being ignored”). We have developed a sarcasm recognizer to identify this type of sarcasm in tweets. We present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets. We show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition. 
Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach.* 
Recently, much research focuses on event storyline generation, which aims to produce a concise, global and temporal event summary from a collection of articles. Generally, each event contains multiple sub-events and the storyline should be composed by the component summaries of all the sub-events. However, different sub-events have different part-whole relationship with the major event, which is important to correspond to users’ interests but seldom considered in previous work. To distinguish different types of sub-events, we propose a mixture-event-aspect model which models different sub-events into local and global aspects. Combining these local/global aspects with summarization requirements together, we utilize an optimization method to generate the component summaries along the timeline. We develop experimental systems on 6 distinctively different datasets. Evaluation and comparison results indicate the effectiveness of our proposed method. 
Document summarization is an important task in the area of natural language processing, which aims to extract the most important information from a single document or a cluster of documents. In various summarization tasks, the summary length is manually deﬁned. However, how to ﬁnd the proper summary length is quite a problem; and keeping all summaries restricted to the same length is not always a good choice. It is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information. In this paper, we propose a Bayesian nonparametric model for multidocument summarization in order to automatically determine the proper lengths of summaries. Assuming that an original document can be reconstructed from its summary, we describe the ”reconstruction” by a Bayesian framework which selects sentences to form a good summary. Experimental results on DUC2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination. 
We present a method which exploits automatically generated scientiﬁc discourse annotations to create a content model for the summarisation of scientiﬁc articles. Full papers are ﬁrst automatically annotated using the CoreSC scheme, which captures 11 contentbased concepts such as Hypothesis, Result, Conclusion etc at the sentence level. A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories. Summary creation is also guided by the distribution of CoreSC categories found in the full articles, in order to adequately represent the article content. Finally, we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task. Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66% of complex content-related questions designed on the basis of paper abstracts. The questions were answered with a precision of 75%, where the upper bound for human summaries (abstracts) was 95%. 
We present the ﬁrst provably optimal polynomial time dynamic programming (DP) algorithm for best-ﬁrst shift-reduce parsing, which applies the DP idea of Huang and Sagae (2010) to the best-ﬁrst parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial. We prove the correctness of our algorithm rigorously. Experiments conﬁrm that DP leads to a signiﬁcant speedup on a probablistic best-ﬁrst shift-reduce parser, and makes exact search under such a model tractable for the ﬁrst time. 
The problem of learning language models from large text corpora has been widely studied within the computational linguistic community. However, little is known about the performance of these language models when applied to the computer vision domain. In this work, we compare representative models: a window-based model, a topic model, a distributional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction. We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images. We determine the usefulness of different language models in aiding the two visual recognition tasks. The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset. 
This paper presents DefMiner, a supervised sequence labeling system that identiﬁes scientiﬁc terms and their accompanying deﬁnitions. DefMiner achieves 85% F1 on a Wikipedia benchmark corpus, signiﬁcantly improving the previous state-of-the-art by 8%. We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC) – a large, real-world digital library of scientiﬁc articles in computational linguistics. The resulting automatically-acquired glossary represents the terminology deﬁned over several thousand individual research articles. We highlight several interesting observations: more deﬁnitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms. Obtaining a list of popular deﬁned terms in a corpus of computational linguistics papers, we ﬁnd that concepts can often be categorized into one of three categories: resources, methodologies and evaluation metrics. 
Nilsson and Nivre (2009) introduced a treebased model of persons’ eye movements in reading. The individual variation between readers reportedly made application across readers impossible. While a tree-based model seems plausible for eye movements, we show that competitive results can be obtained with a linear CRF model. Increasing the inductive bias also makes learning across readers possible. In fact we observe next-to-no performance drop when evaluating models trained on gaze records of multiple readers on new readers. 
This paper explores to what extent lemmatisation, lexical resources, distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management. The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode. 
Recognizing bridging anaphora is difﬁcult due to the wide variation within the phenomenon, the resulting lack of easily identiﬁable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection features and integrate these into a cascaded minority preference algorithm that models bridging recognition as a subtask of learning ﬁnegrained information status (IS). We substantially improve bridging recognition without impairing performance on other IS classes. 
We present an approach to time normalization (e.g. the day before yesterday⇒2013-04-12) based on a synchronous context free grammar. Synchronous rules map the source language to formally deﬁned operators for manipulating times (FINDENCLOSED, STARTATENDOF, etc.). Time expressions are then parsed using an extended CYK+ algorithm, and converted to a normalized form by applying the operators recursively. For evaluation, a small set of synchronous rules for English time expressions were developed. Our model outperforms HeidelTime, the best time normalization system in TempEval 2013, on four different time normalization corpora. 
The rise of “Big Data” analytics over unstructured text has led to renewed interest in information extraction (IE). We surveyed the landscape of IE technologies and identiﬁed a major disconnect between industry and academia: while rule-based IE dominates the commercial world, it is widely regarded as dead-end technology by the academia. We believe the disconnect stems from the way in which the two communities measure the beneﬁts and costs of IE, as well as academia’s perception that rulebased IE is devoid of research challenges. We make a case for the importance of rule-based IE to industry practitioners. We then lay out a research agenda in advancing the state-of-theart in rule-based IE systems which we believe has the potential to bridge the gap between academic research and industry practice. 
 Automatically constructed Knowledge Bases (KBs) are often incomplete and there is a genuine need to improve their coverage. Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph. For the ﬁrst time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can signiﬁcantly outperform previous PRAbased approaches on the KB inference task. We present extensive experimental results validating this ﬁnding. The resources presented in this paper are publicly available. 
Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 
Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations. However, CSLMs have not been used in the ﬁrst pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking. 
MIRA based tuning methods have been widely used in statistical machine translation (SMT) system with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually deﬁne a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 
Multilingual speakers switch between languages in online and spoken communication. Analyses of large scale multilingual data require automatic language identiﬁcation at the word level. For our experiments with multilingual online discussions, we ﬁrst tag the language of individual words using language models and dictionaries. Secondly, we incorporate context to improve the performance. We achieve an accuracy of 98%. Besides word level accuracy, we use two new metrics to evaluate this task. 
Linking name mentions in microblog posts to a knowledge base, namely microblog entity linking, is useful for text mining tasks on microblog. Entity linking in long text has been well studied in previous works. However few work has focused on short text such as microblog post. Microblog posts are short and noisy. Previous method can extract few features from the post context. In this paper we propose to use extra posts for the microblog entity linking task. Experimental results show that our proposed method signiﬁcantly improves the linking accuracy over traditional methods by 8.3% and 7.5% respectively. 
Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata attributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods. 
This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We ﬁnd that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difﬁcult homophonic ciphers, we ﬁnd that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efﬁciently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.  in accuracy even as we scale the number of random restarts up into the hundred thousands. In both cases the difference between few and many random restarts is the difference between almost complete failure and successful decipherment. We also ﬁnd that millions of random restarts can be helpful for performing exploratory analysis. We look at some empirical properties of decipherment problems, visualizing the distribution of local optima encountered by EM both in a successful decipherment of a homophonic cipher and in an unsuccessful attempt to decipher Zodiac 340. Finally, we attack a series of ciphers generated to match properties of Zodiac 340 and use the results to argue that Zodiac 340 is likely not a homophonic cipher under the commonly assumed linearization order. 2 Decipherment Model  
We explore a model of stress prediction in Russian using a combination of local contextual features and linguisticallymotivated features associated with the word’s stem and suﬃx. We frame this as a ranking problem, where the objective is to rank the pronunciation with the correct stress above those with incorrect stress. We train our models using a simple Maximum Entropy ranking framework allowing for eﬃcient prediction. An empirical evaluation shows that a model combining the local contextual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. 
We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora. 
Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identiﬁcation. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classiﬁcation. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classiﬁcation algorithm substantially improves accuracy. Finally, we combine latent features with ﬁne-grained n-gram overlap features, yielding performance that is 3% more accurate than the prior state-of-the-art. 
Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word’s sentiment polarity. However, no work is done for comparing different corpora in the polarity classiﬁcation task. Nowadays, Twitter has aggregated huge amount of data that are full of people’s sentiments. In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classiﬁcation. Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods. 
Implicit feature detection, also known as implicit feature identiﬁcation, is an essential aspect of feature-speciﬁc opinion mining but previous works have often ignored it. We think, based on the explicit sentences, several Support Vector Machine (SVM) classiﬁers can be established to do this task. Nevertheless, we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods. Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better. 
Online learning algorithms like the perceptron are widely used for structured prediction tasks. For sequential search problems, like left-to-right tagging and parsing, beam search has been successfully combined with perceptron variants that accommodate search errors (Collins and Roark, 2004; Huang et al., 2012). However, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs. In this paper, we generalize the violation-ﬁxing perceptron of Huang et al. (2012) to hypergraphs and apply it to the cube-pruning parser of Zhang and McDonald (2012). This results in the highest reported scores on WSJ evaluation set (UAS 93.50% and LAS 92.41% respectively) without the aid of additional resources.  dependency parsers by Zhang and McDonald (2012) and shown to outperform left-to-right beam search. Both these examples, bottom-up approximate dependency and constituency parsing, can be viewed as speciﬁc instances of inexact hypergraph search. Typically, the approximation is accomplished by cube-pruning throughout the hypergraph (Chiang, 2007). Unfortunately, as the scope of features at each node increases, the inexactness of search and its negative impact on learning can potentially be exacerbated. Unlike sequential search, the impact on learning of approximate hypergraph search – as well as methods to mitigate any ill effects – has not been studied. Motivated by this, we develop online learning algorithms for inexact hypergraph search by generalizing the violation-ﬁxing percepron of Huang et al. (2012). We empirically validate the beneﬁt of this approach within the cube-pruning dependency parser of Zhang and McDonald (2012).  
We present a classiﬁcation model that predicts the presence or omission of a lexical connective between two clauses, based upon linguistic features of the clauses and the type of discourse relation holding between them. The model is trained on a set of high frequency relations extracted from the Penn Discourse Treebank and achieves an accuracy of 86.6%. Analysis of the results reveals that the most informative features relate to the discourse dependencies between sequences of coherence relations in the text. We also present results of an experiment that provides insight into the nature and difﬁculty of the task. 
In Japanese, zero references often occur and many of them are categorized into zero exophora, in which a referent is not mentioned in the document. However, previous studies have focused on only zero endophora, in which a referent explicitly appears. We present a zero reference resolution model considering zero exophora and author/reader of a document. To deal with zero exophora, our model adds pseudo entities corresponding to zero exophora to candidate referents of zero pronouns. In addition, we automatically detect mentions that refer to the author and reader of a document by using lexico-syntactic patterns. We represent their particular behavior in a discourse as a feature vector of a machine learning model. The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora. 
Natural language conversation is widely regarded as a highly difﬁcult problem, which is usually attacked with either rule-based or learning-based models. In this paper we propose a retrieval-based automatic response model for short-text conversation, to exploit the vast amount of short conversation instances available on social media. For this purpose we introduce a dataset of short-text conversation based on the real-world instances from Sina Weibo (a popular Chinese microblog service), which will be soon released to public. This dataset provides rich collection of instances for the research on ﬁnding natural and relevant short responses to a given short text, and useful for both training and testing of conversation models. This dataset consists of both naturally formed conversations, manually labeled data, and a large repository of candidate responses. Our preliminary experiments demonstrate that the simple retrieval-based conversation model performs reasonably well when combined with the rich instances in our dataset. 
Explanatory sentences are employed to clarify reasons, details, facts, and so on. High quality online product reviews usually include not only positive or negative opinions, but also a variety of explanations of why these opinions were given. These explanations can help readers get easily comprehensible information of the discussed products and aspects. Moreover, explanatory relations can also beneﬁt sentiment analysis applications. In this work, we focus on the task of identifying subjective text segments and extracting their corresponding explanations from product reviews in discourse level. We propose a novel joint extraction method using ﬁrstorder logic to model rich linguistic features and long distance constraints. Experimental results demonstrate the effectiveness of the proposed method. 
We present an approach for building multidocument event threads from a large corpus of newswire articles. An event thread is basically a succession of events belonging to the same story. It helps the reader to contextualize the information contained in a single article, by navigating backward or forward in the thread from this article. A speciﬁc effort is also made on the detection of reactions to a particular event. In order to build these event threads, we use a cascade of classiﬁers and other modules, taking advantage of the redundancy of information in the newswire corpus. We also share interesting comments concerning our manual annotation procedure for building a training and testing set1. 
Scope detection is a key task in information extraction. This paper proposes a new approach for tree kernel-based scope detection by using the structured syntactic parse information. In addition, we have explored the way of selecting compatible features for different part-of-speech cues. Experiments on the BioScope corpus show that both constituent and dependency structured syntactic parse features have the advantage in capturing the potential relationships between cues and their scopes. Compared with the state of the art scope detection systems, our system achieves substantial improvement.* 
Temporal variations of text are usually ignored in NLP applications. However, text use changes with time, which can affect many applications. In this paper we model periodic distributions of words over time. Focusing on hashtag frequency in Twitter, we ﬁrst automatically identify the periodic patterns. We use this for regression in order to forecast the volume of a hashtag based on past data. We use Gaussian Processes, a state-ofthe-art bayesian non-parametric model, with a novel periodic kernel. We demonstrate this in a text classiﬁcation setting, assigning the tweet hashtag based on the rest of its text. This method shows signiﬁcant improvements over competitive baselines. 
Direct quotations are used for opinion mining and information extraction as they have an easy to extract span and they can be attributed to a speaker with high accuracy. However, simply focusing on direct quotations ignores around half of all reported speech, which is in the form of indirect or mixed speech. This work presents the ﬁrst large-scale experiments in indirect and mixed quotation extraction and attribution. We propose two methods of extracting all quote types from news articles and evaluate them on two large annotated corpora, one of which is a contribution of this work. We further show that direct quotation attribution methods can be successfully applied to indirect and mixed quotation attribution. 
Web search users frequently modify their queries in hope of receiving better results. This process is referred to as “Query Reformulation”. Previous research has mainly focused on proposing query reformulations in the form of suggested queries for users. Some research has studied the problem of predicting whether the current query is a reformulation of the previous query or not. However, this work has been limited to bag-of-words models where the main signals being used are word overlap, character level edit distance and word level edit distance. In this work, we show that relying solely on surface level text similarity results in many false positives where queries with different intents yet similar topics are mistakenly predicted as query reformulations. We propose a new representation for Web search queries based on identifying the concepts in queries and show that we can signiﬁcantly improve query reformulation performance using features of query concepts. 
Passage retrieval is a crucial ﬁrst step of automatic Question Answering (QA). While existing passage retrieval algorithms are effective at selecting document passages most similar to the question, or those that contain the expected answer types, they do not take into account which parts of the document the searchers actually found useful. We propose, to the best of our knowledge, the ﬁrst successful attempt to incorporate searcher examination data into passage retrieval for question answering. Speciﬁcally, we exploit detailed examination data, such as mouse cursor movements and scrolling, to infer the parts of the document the searcher found interesting, and then incorporate this signal into passage retrieval for QA. Our extensive experiments and analysis demonstrate that our method significantly improves passage retrieval, compared to using textual features alone. As an additional contribution, we make available to the research community the code and the search behavior data used in this study, with the hope of encouraging further research in this area. 
This paper presents the Kazakh Language Corpus (KLC), which is one of the ﬁrst attempts made within a local research community to assemble a Kazakh corpus. KLC is designed to be a large scale corpus containing over 135 million words and conveying ﬁve stylistic genres: literary, publicistic, ofﬁcial, scientiﬁc and informal. Along with its primary part KLC comprises such parts as: (i) annotated sub-corpus, containing segmented documents encoded in the eXtensible Markup Language (XML) that marks complete morphological, syntactic, and structural characteristics of texts; (ii) as well as a sub-corpus with the annotated speech data. KLC has a web-based corpus management system that helps to navigate the data and retrieve necessary information. KLC is also open for contributors, who are willing to make suggestions, donate texts and help with annotation of existing materials. 
We present a method for automatically learning inﬂectional classes and associated lemmas from morphologically annotated corpora. The method consists of a core languageindependent algorithm, which can be optimized for speciﬁc languages. The method is demonstrated on Egyptian Arabic and German, two morphologically rich languages. Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction. 
We present a joint language and translation model based on a recurrent neural network which predicts target words based on an unbounded history of both source and target words. The weaker independence assumptions of this model result in a vastly larger search space compared to related feedforward-based language or translation models. We tackle this issue with a new lattice rescoring algorithm and demonstrate its effectiveness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets. 
Domain adaptation for SMT usually adapts models to an individual speciﬁc domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each speciﬁc domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach signiﬁcantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each speciﬁc domain. 
We present a novel translation model, which simultaneously exploits the constituency and dependency trees on the source side, to combine the advantages of two types of trees. We take head-dependents relations of dependency trees as backbone and incorporate phrasal nodes of constituency trees as the source side of our translation rules, and the target side as strings. Our rules hold the property of long distance reorderings and the compatibility with phrases. Large-scale experimental results show that our model achieves signiﬁcantly improvements over the constituency-to-string (+2.45 BLEU on average) and dependencyto-string (+0.91 BLEU on average) models, which only employ single type of trees, and signiﬁcantly outperforms the state-of-theart hierarchical phrase-based model (+1.12 BLEU on average), on three Chinese-English NIST test sets. 
When using a machine translation (MT) model trained on OLD-domain parallel data to translate NEW-domain text, one major challenge is the large number of out-of-vocabulary (OOV) and new-translation-sense words. We present a method to identify new translations of both known and unknown source language words that uses NEW-domain comparable document pairs. Starting with a joint distribution of source-target word pairs derived from the OLD-domain parallel corpus, our method recovers a new joint distribution that matches the marginal distributions of the NEW-domain comparable document pairs, while minimizing the divergence from the OLD-domain distribution. Adding learned translations to our French-English MT model results in gains of about 2 BLEU points over strong baselines. 
Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n2b) for input of n words and beam size b, compared to O(n3) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efﬁcient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) obtained phrase-based and CKY Hiero with the same translation model. 
This paper addresses the problem of producing a diverse set of plausible translations. We present a simple procedure that can be used with any statistical machine translation (MT) system. We explore three ways of using diverse translations: (1) system combination, (2) discriminative reranking with rich features, and (3) a novel post-editing scenario in which multiple translations are presented to users. We ﬁnd that diversity can improve performance on these tasks, especially for sentences that are difﬁcult for MT. 
While large-scale discriminative training has triumphed in many NLP problems, its deﬁnite success on machine translation has been largely elusive. Most recent efforts along this line are not scalable (training on the small dev set with features from top ∼100 most frequent words) and overly complicated. We instead present a very simple yet theoretically motivated approach by extending the recent framework of “violation-ﬁxing perceptron”, using forced decoding to compute the target derivations. Extensive phrase-based translation experiments on both Chinese-to-English and Spanish-to-English tasks show substantial gains in BLEU by up to +2.3/+2.0 on dev/test over MERT, thanks to 20M+ sparse features. This is the ﬁrst successful effort of large-scale online discriminative training for MT. 
This paper studies the problem of identifying users who use multiple userids to post in social media. Since multiple userids may belong to the same author, it is hard to directly apply supervised learning to solve the problem. This paper proposes a new method, which still uses supervised learning but does not require training documents from the involved userids. Instead, it uses documents from other userids for classifier building. The classifier can be applied to documents of the involved userids. This is possible because we transform the document space to a similarity space and learning is performed in this new space. Our evaluation is done in the online review domain. The experimental results using a large number of userids and their reviews show that the proposed method is highly effective. 
While much work has considered the problem of latent attribute inference for users of social media such as Twitter, little has been done on non-English-based content and users. Here, we conduct the ﬁrst assessment of latent attribute inference in languages beyond English, focusing on gender inference. We ﬁnd that the gender inference problem in quite diverse languages can be addressed using existing machinery. Further, accuracy gains can be made by taking language-speciﬁc features into account. We identify languages with complex orthography, such as Japanese, as difﬁcult for existing methods, suggesting a valuable direction for future research. 
Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal models to support three or more modalities. We ﬁnd that the three-, four-, and ﬁve-dimensional models signiﬁcantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure. 
It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way. We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 – this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline. Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to other grammar extraction strategies. 
NLP models have many and sparse features, and regularization is key for balancing model overﬁtting versus underﬁtting. A recently repopularized form of regularization is to generate fake training data by repeatedly adding noise to real data. We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data. We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs. We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efﬁciently. The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transductive extension. Applied to text classiﬁcation and NER, our method provides a >1% absolute performance gain over use of standard L2 regularization. 
One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation. We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all ngrams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic. Our blocked sampler can efﬁciently search for higher probability space even with higher order n-grams. In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document. 
In this paper we report an empirical study on semi-supervised Chinese word segmentation using co-training. We utilize two segmenters: 1) a word-based segmenter leveraging a word-level language model, and 2) a character-based segmenter using characterlevel features within a CRF-based sequence labeler. These two segmenters are initially trained with a small amount of segmented data, and then iteratively improve each other using the large amount of unlabelled data. Our experimental results show that co-training captures 20% and 31% of the performance improvement achieved by supervised training with an order of magnitude more data for the SIGHAN Bakeoff 2005 PKU and CU corpora respectively. 
A precise syntacto-semantic analysis of English requires a large detailed lexicon with the possibility of treating multiple tokens as a single meaning-bearing unit, a word-with-spaces. However parsing with such a lexicon, as included in the English Resource Grammar, can be very slow. We show that we can apply supertagging techniques over an ambiguous token lattice without resorting to previously used heuristics, a process we call ubertagging. Our model achieves an ubertagging accuracy that can lead to a four to eight fold speed up while improving parser accuracy. 
We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 
Hypernym discovery aims to extract such noun pairs that one noun is a hypernym of the other. Most previous methods are based on lexical patterns but perform badly on opendomain data. Other work extracts hypernym relations from encyclopedias but has limited coverage. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, we try to discover its hypernyms by leveraging knowledge from multiple sources, i.e., search engine results, encyclopedias, and morphology of the entity name. First, we extract candidate hypernyms from the above sources. Then, we apply a statistical ranking model to select correct hypernyms. A set of novel features is proposed for the ranking model. We also present a heuristic strategy to build a large-scale noisy training data for the model without human annotation. Experimental results demonstrate that our approach outperforms the state-of-the-art methods on a manually labeled test dataset. 
This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneﬁcial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 
Why do certain combinations of words such as “disadvantageous peace” or “metal to the petal” appeal to our minds as interesting expressions with a sense of creativity, while other phrases such as “quiet teenager”, or “geometrical base” not as much? We present statistical explorations to understand the characteristics of lexical compositions that give rise to the perception of being original, interesting, and at times even artistic. We ﬁrst examine various correlates of perceived creativity based on information theoretic measures and the connotation of words, then present experiments based on supervised learning that give us further insights on how different aspects of lexical composition collectively contribute to the perceived creativity. 
Assigning a positive or negative score to a word out of context (i.e. a word’s prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classiﬁcation models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words’ prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered. 
Building search engines that can respond to spoken queries with spoken content requires that the system not just be able to ﬁnd useful responses, but also that it know when it has heard enough about what the user wants to be able to do so. This paper describes a simulation study with queries spoken by non-native speakers that suggests that indicates that ﬁnding relevant content is often possible within a half minute, and that combining features based on automatically recognized words with features designed for automated prediction of query difﬁculty can serve as a useful basis for predicting when that useful content has been found. 
The rapid development of Web2.0 leads to signiﬁcant information redundancy. Especially for a complex news event, it is difﬁcult to understand its general idea within a single coherent picture. A complex event often contains branches, intertwining narratives and side news which are all called storylines. In this paper, we propose a novel solution to tackle the challenging problem of storylines extraction and reconstruction. Speciﬁcally, we ﬁrst investigate two requisite properties of an ideal storyline. Then a uniﬁed algorithm is devised to extract all effective storylines by optimizing these properties at the same time. Finally, we reconstruct all extracted lines and generate the high-quality story map. Experiments on real-world datasets show that our method is quite efﬁcient and highly competitive, which can bring about quicker, clearer and deeper comprehension to readers. 
Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difﬁcult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we ﬁnd that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements. 
In current dependency parsing models, conventional features (i.e. base features) deﬁned over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminative power on unseen data. In this paper, we propose a novel semi-supervised approach to addressing the problem by transforming the base features into high-level features (i.e. meta features) with the help of a large amount of automatically parsed data. The meta features are used together with base features in our ﬁnal parser. Our studies indicate that our proposed approach is very effective in processing unseen data and features. Experiments on Chinese and English data sets show that the ﬁnal parser achieves the best-reported accuracy on the Chinese data and comparable accuracy with the best known parsers on the English data. 
Topic segmentation classically relies on one of two criteria, either ﬁnding areas with coherent vocabulary use or detecting discontinuities. In this paper, we propose a segmentation criterion combining both lexical cohesion and disruption, enabling a trade-off between the two. We provide the mathematical formulation of the criterion and an efﬁcient graph based decoding algorithm for topic segmentation. Experimental results on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows demonstrate the beneﬁt of such a combination. Gains were observed in all conditions, with segments of either regular or varying length and abrupt or smooth topic shifts. Long segments beneﬁt more than short segments. However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences. 
We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identiﬁcation from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classiﬁcation by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques signiﬁcantly outperforms various standard and stateof-the-art text classiﬁcation algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the speciﬁc brands. 
Hot trends are likely to bring new business opportunities. For example, “Air Pollution” might lead to a signiﬁcant increase of the sales of related products, e.g., mouth mask. For ecommerce companies, it is very important to make rapid and correct response to these hot trends in order to improve product sales. In this paper, we take the initiative to study the task of how to identify trend related products. The major novelty of our work is that we automatically learn commercial intents revealed from microblogs. We carefully construct a data collection for this task and present quite a few insightful ﬁndings. In order to solve this problem, we further propose a graph based method, which jointly models relevance and associativity. We perform extensive experiments and the results showed that our methods are very effective. 
We investigate the value-add of topic modeling in text analysis for depression, and for neuroticism as a strongly associated personality measure. Using Pennebaker’s Linguistic Inquiry and Word Count (LIWC) lexicon to provide baseline features, we show that straightforward topic modeling using Latent Dirichlet Allocation (LDA) yields interpretable, psychologically relevant “themes” that add value in prediction of clinical assessments. 
We present a statistical model for predicting how the user of an interactive, situated NLP system resolved a referring expression. The model makes an initial prediction based on the meaning of the utterance, and revises it continuously based on the user’s behavior. The combined model outperforms its components in predicting reference resolution and when to give feedback. 
We extend Zhao and Ng's (2007) Chinese anaphoric zero pronoun resolver by (1) using a richer set of features and (2) exploiting the coreference links between zero pronouns during resolution. Results on OntoNotes show that our approach significantly outperforms two state-of-the-art anaphoric zero pronoun resolvers. To our knowledge, this is the first work to report results obtained by an end-toend Chinese zero pronoun resolver. 
This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efﬁciently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone. 
In this paper, we present a recursive neural network (RNN) model that works on a syntactic tree. Our model differs from previous RNN models in that the model allows for an explicit weighting of important phrases for the target task. We also propose to average parameters in training. Our experimental results on semantic relation classiﬁcation show that both phrase categories and task-speciﬁc weighting signiﬁcantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models. 
Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efﬁcacy on a small German→English and a larger French→German translation task with both standard phrase-based and hierarchical phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French→German task and 0.3% BLEU and 1.1% TER on the German→English task. 
This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a signiﬁcant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system. 
We explore the application of neural language models to machine translation. We develop a new model that combines the neural probabilistic language model of Bengio et al., rectiﬁed linear units, and noise-contrastive estimation, and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder. Our large-scale, large-vocabulary experiments across four language pairs show that our neural language model improves translation quality by up to 1.1 Bleu. 
We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings signiﬁcantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task. 
In this paper we present a novel approach to automatic creation of anchor texts for hyperlinks in a document pointing to similar documents. Methods used in this approach rank parts of a document based on the similarity to a presumably related document. Ranks are then used to automatically construct the best anchor text for a link inside original document to the compared document. A number of different methods from information retrieval and natural language processing are adapted for this task. Automatically constructed anchor texts are manually evaluated in terms of relatedness to linked documents and compared to baseline consisting of originally inserted anchor texts. Additionally we use crowdsourcing for evaluation of original anchors and automatically constructed anchors. Results show that our best adapted methods rival the precision of the baseline method. 
Sentence completion is a challenging semantic modeling task in which models must choose the most appropriate word from a given set to complete a sentence. Although a variety of language models have been applied to this task in previous work, none of the existing approaches incorporate syntactic information. In this paper we propose to tackle this task using a pair of simple language models in which the probability of a sentence is estimated as the probability of the lexicalisation of a given syntactic dependency tree. We apply our approach to the Microsoft Research Sentence Completion Challenge and show that it improves on n-gram language models by 8.7 percentage points, achieving the highest accuracy reported to date apart from neural language models that are more complex and expensive to train. 
In this paper, we propose a walk-based graph kernel that generalizes the notion of treekernels to continuous spaces. Our proposed approach subsumes a general framework for word-similarity, and in particular, provides a ﬂexible way to incorporate distributed representations. Using vector representations, such an approach captures both distributional semantic similarities among words as well as the structural relations between them (encoded as the structure of the parse tree). We show an efﬁcient formulation to compute this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results. 
Online resources, such as Wiktionary, provide an accurate but incomplete source of idiomatic phrases. In this paper, we study the problem of automatically identifying idiomatic dictionary entries with such resources. We train an idiom classiﬁer on a newly gathered corpus of over 60,000 Wiktionary multi-word deﬁnitions, incorporating features that model whether phrase meanings are constructed compositionally. Experiments demonstrate that the learned classiﬁer can provide high quality idiom labels, more than doubling the number of idiomatic entries from 7,764 to 18,155 at precision levels of over 65%. These gains also translate to idiom detection in sentences, by simply using known word sense disambiguation algorithms to match phrases to their deﬁnitions. In a set of Wiktionary definition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points.  not. Using these incomplete annotations as supervision, we train a binary Perceptron classiﬁer for identifying idiomatic dictionary entries. We introduce new lexical and graph-based features that use WordNet and Wiktionary to compute semantic relatedness. This allows us to learn, for example, that the words in the phrase diamond in the rough are more closely related to the words in its literal deﬁnition than the idiomatic one. Experiments demonstrate that the classiﬁer achieves precision of over 65% at recall over 52% and that, when used to ﬁll in missing Wiktionary idiom labels, it more than doubles the number of idioms from 7,764 to 18,155. These gains also translate to idiom detection in sentences, by simply using the Lesk word sense disambiguation (WSD) algorithm (1986) to match phrases to their deﬁnitions. This approach allows for scalable detection with no restrictions on the syntactic structure or context of the target phrase. In a set of Wiktionary deﬁnition example sentences, the more complete set of idioms boosts detection recall by over 28 percentage points.  
Tokenization is widely regarded as a solved problem due to the high accuracy that rulebased tokenizers achieve. But rule-based tokenizers are hard to maintain and their rules language speciﬁc. We show that highaccuracy word and sentence segmentation can be achieved by using supervised sequence labeling on the character level combined with unsupervised feature learning. We evaluated our method on three languages and obtained error rates of 0.27 ‰ (English), 0.35 ‰ (Dutch) and 0.76 ‰ (Italian) for our best models. 
We present a novel unsupervised approach to detecting the compositionality of multi-word expressions. We compute the compositionality of a phrase through substituting the constituent words with their “neighbours” in a semantic vector space and averaging over the distance between the original phrase and the substituted neighbour phrases. Several methods of obtaining neighbours are presented. The results are compared to existing supervised results and achieve state-of-the-art performance on a verb-object dataset of human compositionality ratings. 
This research describes efforts to use crowdsourcing to improve the validity of the semantic predicates in VerbNet, a lexicon of about 6300 English verbs. The current semantic predicates can be thought of semantic primitives, into which the concepts denoted by a verb can be decomposed. For example, the verb spray (of the Spray class), involves the predicates MOTION, NOT, and LOCATION, where the event can be decomposed into an AGENT causing a THEME that was originally not in a particular location to now be in that location. Although VerbNet’s predicates are theoretically well-motivated, systematic empirical data is scarce. This paper describes a recently-launched attempt to address this issue with a series of human judgment tasks, posed to subjects in the form of games. 
This paper offers an approach for governments to harness the information contained in social media in order to make public inspections and disclosure more efﬁcient. As a case study, we turn to restaurant hygiene inspections – which are done for restaurants throughout the United States and in most of the world and are a frequently cited example of public inspections and disclosure. We present the ﬁrst empirical study that shows the viability of statistical models that learn the mapping between textual signals in restaurant reviews and the hygiene inspection records from the Department of Public Health. The learned model achieves over 82% accuracy in discriminating severe offenders from places with no violation, and provides insights into salient cues in reviews that are indicative of the restaurant’s sanitary conditions. Our study suggests that public disclosure policy can be improved by mining public opinions from social media to target inspections and to provide alternative forms of disclosure to customers. 
The identification of pseudepigraphic texts – texts not written by the authors to which they are attributed – has important historical, forensic and commercial applications. We introduce an unsupervised technique for identifying pseudepigrapha. The idea is to identify textual outliers in a corpus based on the pairwise similarities of all documents in the corpus. The crucial point is that document similarity not be measured in any of the standard ways but rather be based on the output of a recently introduced algorithm for authorship verification. The proposed method strongly outperforms existing techniques in systematic experiments on a blog corpus. 
Feature computation and exhaustive search have signiﬁcantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates. 
Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classiﬁcation based on learning cross-lingual discriminative distributed representations of words. Speciﬁcally, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classiﬁcation tasks of Amazon product reviews. Our experimental results demonstrate the efﬁcacy of the proposed cross-lingual adaptation approach. 
Often the bottleneck in document classiﬁcation is ﬁnding good representations that zoom in on the most important aspects of the documents. Most research uses n-gram representations, but relevant features often occur discontinuously, e.g., not. . . good in sentiment analysis. In this paper we present experiments getting experts to provide regular expressions, as well as crowdsourced annotation tasks from which regular expressions can be derived. Somewhat surprisingly, it turns out that these crowdsourced feature combinations outperform automatic feature combination methods, as well as expert features, by a very large margin and reduce error by 24-41% over n-gram representations. 
A major challenge in supervised sentence compression is making use of rich feature representations because of very scarce parallel data. We address this problem and present a method to automatically build a compression corpus with hundreds of thousands of instances on which deletion-based algorithms can be trained. In our corpus, the syntactic trees of the compressions are subtrees of their uncompressed counterparts, and hence supervised systems which require a structural alignment between the input and output can be successfully trained. We also extend an existing unsupervised compression method with a learning module. The new system uses structured prediction to learn from lexical, syntactic and other features. An evaluation with human raters shows that the presented data harvesting method indeed produces a parallel corpus of high quality. Also, the supervised system trained on this corpus gets high scores both from human raters and in an automatic evaluation setting, signiﬁcantly outperforming a strong baseline. 
Extractive summarization typically uses sentences as summarization units. In contrast, joint compression and summarization can use smaller units such as words and phrases, resulting in summaries containing more information. The goal of compressive summarization is to ﬁnd a subset of words that maximize the total score of concepts and cutting dependency arcs under the grammar constraints and summary length constraint. We propose an efﬁcient decoding algorithm for fast compressive summarization using graph cuts. Our approach ﬁrst relaxes the length constraint using Lagrangian relaxation. Then we propose to bound the relaxed objective function by the supermodular binary quadratic programming problem, which can be solved efﬁciently using graph max-ﬂow/min-cut. Since ﬁnding the tightest lower bound suffers from local optimality, we use convex relaxation for initialization. Experimental results on TAC2008 dataset demonstrate our method achieves competitive ROUGE score and has good readability, while is much faster than the integer linear programming (ILP) method. 
In a language generation system, a content planner selects which elements must be included in the output text and the ordering between them. Recent empirical approaches perform content selection without any ordering and have thus no means to ensure that the output is coherent. In this paper we focus on the problem of generating text from a database and present a trainable end-to-end generation system that includes both content selection and ordering. Content plans are represented intuitively by a set of grammar rules that operate on the document level and are acquired automatically from training data. We develop two approaches: the ﬁrst one is inspired from Rhetorical Structure Theory and represents the document as a tree of discourse relations between database records; the second one requires little linguistic sophistication and uses tree structures to represent global patterns of database record sequences within a document. Experimental evaluation on two domains yields considerable improvements over the state of the art for both approaches. 
Recent studies on extractive text summarization formulate it as a combinatorial optimization problem such as a Knapsack Problem, a Maximum Coverage Problem or a Budgeted Median Problem. These methods successfully improved summarization quality, but they did not consider the rhetorical relations between the textual units of a source document. Thus, summaries generated by these methods may lack logical coherence. This paper proposes a single document summarization method based on the trimming of a discourse tree. This is a two-fold process. First, we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree, which allows us to take a treetrimming approach to summarization. Second, we formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem, then solve it with integer linear programming (ILP). Evaluation results showed that our method improved ROUGE scores. 
Social media like forums and microblogs have accumulated a huge amount of user generated content (UGC) containing human knowledge. Currently, most of UGC is listed as a whole or in pre-deﬁned categories. This “list-based” approach is simple, but hinders users from browsing and learning knowledge of certain topics effectively. To address this problem, we propose a hierarchical entity-based approach for structuralizing UGC in social media. By using a large-scale entity repository, we design a three-step framework to organize UGC in a novel hierarchical structure called “cluster entity tree (CET)”. With Yahoo! Answers as a test case, we conduct experiments and the results show the effectiveness of our framework in constructing CET. We further evaluate the performance of CET on UGC organization in both user and system aspects. From a user aspect, our user study demonstrates that, with CET-based structure, users perform signiﬁcantly better in knowledge learning than using traditional list-based approach. From a system aspect, CET substantially boosts the performance of two information retrieval models (i.e., vector space model and query likelihood language model). 
 Occidental College, Columbia University  In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.  Execute on Database  Type.University Education.BarackObama  Type.University  alignment  bridging Education  Which  college  BarackObama  alignment  did  Obama  go to ?  Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates.  
We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difﬁcult to represent in a ﬁxed target ontology. For example, even simple phrases such as ‘daughter’ and ‘number of people living in’ cannot be directly represented in Freebase, whose ontology instead encodes facts about gender, parenthood, and population. In this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus. 
The goal of our research is to distinguish veterinary message board posts that describe a case involving a speciﬁc patient from posts that ask a general question. We create a text classiﬁer that incorporates automatically generated attribute lists for veterinary patients to tackle this problem. Using a small amount of annotated data, we train an information extraction (IE) system to identify veterinary patient attributes. We then apply the IE system to a large collection of unannotated texts to produce a lexicon of veterinary patient attribute terms. Our experimental results show that using the learned attribute lists to encode patient information in the text classiﬁer yields improved performance on this task. 
Lexical chains provide a representation of the lexical cohesion structure of a text. In this paper, we propose two lexical chain based cohesion models to incorporate lexical cohesion into document-level statistical machine translation: 1) a count cohesion model that rewards a hypothesis whenever a chain word occurs in the hypothesis, 2) and a probability cohesion model that further takes chain word translation probabilities into account. We compute lexical chains for each source document to be translated and generate target lexical chains based on the computed source chains via maximum entropy classiﬁers. We then use the generated target chains to provide constraints for word selection in document-level machine translation through the two proposed lexical chain based cohesion models. We verify the effectiveness of the two models using a hierarchical phrase-based translation system. Experiments on large-scale training data show that they can substantially improve translation quality in terms of BLEU and that the probability cohesion model outperforms previous models based on lexical cohesion devices. 
The IBM translation models have been hugely inﬂuential in statistical machine translation; they are the basis of the alignment models used in modern translation systems. Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are non-convex, and hence have multiple local optima. In this paper we introduce a convex relaxation of IBM Model 2, and describe an optimization algorithm for the relaxation based on a subgradient method combined with exponentiated-gradient updates. Our approach gives the same level of alignment accuracy as IBM Model 2. 
Pronunciation dictionaries provide a readily available parallel corpus for learning to transduce between character strings and phoneme strings or vice versa. Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings. We examine ﬁnitestate and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics – ﬁnding alternative spellings is harder than alternative pronunciations and beneﬁts from round-trip algorithms when the other does not. We also show that we can increase accuracy by modeling syllable stress. 
Recent work has shown that compositionaldistributional models using element-wise operations on contextual word vectors beneﬁt from the introduction of a prior disambiguation step. The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors). We propose disambiguation algorithms for a number of tensor-based models, which we then test on a variety of tasks. The results show that disambiguation can provide better compositional representation even for the case of tensor-based models. Furthermore, we conﬁrm previous ﬁndings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches. 
We present Multi-Relational Latent Semantic Analysis (MRLSA) which generalizes Latent Semantic Analysis (LSA). MRLSA provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor. Similar to LSA, a lowrank approximation of the tensor is derived using a tensor decomposition. Each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix. The degree of two words having a speciﬁc relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 
We present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion. The paper systematically introduces and describes all key elements of the bootstrapping procedure: (1) starting point or seed lexicon, (2) the conﬁdence estimation and selection of new dimensions of the space, and (3) convergence. We test the quality of the induced bilingual vector spaces, and analyze the inﬂuence of the different components of the bootstrapping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets. 
Continuous space word representations extracted from neural network language models have been used effectively for natural language processing, but until recently it was not clear whether the spatial relationships of such representations were interpretable. Mikolov et al. (2013) show that these representations do capture syntactic and semantic regularities. Here, we push the interpretation of continuous space word representations further by demonstrating that vector offsets can be used to derive adjectival scales (e.g., okay < good < excellent). We evaluate the scales on the indirect answers to yes/no questions corpus (de Marneffe et al., 2010). We obtain 72.8% accuracy, which outperforms previous results (∼60%) on this corpus and highlights the quality of the scales extracted, providing further support that the continuous space word representations are meaningful. 
Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes ﬁne grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classiﬁcation from 80% up to 85.4%. The accuracy of predicting ﬁne-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. 
We propose a novel approach to sentiment analysis for a low resource setting. The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment, may be viewed as a span of sentiment expressed across the entity. This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them. We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource. By leveraging linguisticallyinformed features within conditional random ﬁelds (CRFs) trained to minimize empirical risk, our best models in Spanish signiﬁcantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction. Our models in English, trained on a much smaller dataset, are not yet statistically signiﬁcant against their baselines. 
Aspect extraction is one of the key tasks in sentiment analysis. In recent years, statistical models have been used for the task. However, such models without any domain knowledge often produce aspects that are not interpretable in applications. To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects. However, existing knowledge-based topic models have several major shortcomings, e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge. This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized Pólya urn (E-GPU) model (which is also proposed in this paper). Experiments on real-life product reviews from a variety of domains show that MCLDA outperforms the existing state-of-the-art models markedly. 
We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500%. We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data. In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets.  
Translation into morphologically rich languages is an important but recalcitrant problem in MT. We present a simple and effective approach that deals with the problem in two phases. First, a discriminative model is learned to predict inﬂections of target words from rich source-side annotations. Then, this model is used to create additional sentencespeciﬁc word- and phrase-level translations that are added to a standard translation model as “synthetic” phrases. Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer. We report signiﬁcant improvements in translation quality when translating from English to Russian, Hebrew and Swahili. 
We present an approach to learning bilingual n-gram correspondences from relevance rankings of English documents for Japanese queries. We show that directly optimizing cross-lingual rankings rivals and complements machine translation-based cross-language information retrieval (CLIR). We propose an efﬁcient boosting algorithm that deals with very large cross-product spaces of word correspondences. We show in an experimental evaluation on patent prior art search that our approach, and in particular a consensus-based combination of boosting and translation-based approaches, yields substantial improvements in CLIR performance. Our training and test data are made publicly available. 
We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show ﬁrst that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations. 
Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – speciﬁcally “How?” and “Why?” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We represent processes by graphs whose edges describe a set of temporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e.g., the graphs are connected). Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint inference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show signiﬁcant improvement comparing to baselines that disregard process structure. 
Chambers and Jurafsky (2009) demonstrated that event schemas can be automatically induced from text corpora. However, our analysis of their schemas identiﬁes several weaknesses, e.g., some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor. It is due in part to their pair-wise representation that treats subjectverb independently from verb-object. This often leads to subject-verb-object triples that are not meaningful in the real-world. We present a novel approach to inducing open-domain event schemas that overcomes these limitations. Our approach uses cooccurrence statistics of semantically typed relational triples, which we call Rel-grams (relational n-grams). In a human evaluation, our schemas outperform Chambers’s schemas by wide margins on several evaluation criteria. Both Rel-grams and event schemas are freely available to the research community. 
Cross-lingual topic modelling has applications in machine translation, word sense disambiguation and terminology alignment. Multilingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages. In this paper, we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly deﬁned topics, but then computes latent relations between these. Thus, the method combines the beneﬁts of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language. 
Previous approaches for automated essay scoring (AES) learn a rating model by minimizing either the classiﬁcation, regression, or pairwise classiﬁcation loss, depending on the learning algorithm used. In this paper, we argue that the current AES systems can be further improved by taking into account the agreement between human and machine raters. To this end, we propose a rankbased approach that utilizes listwise learning to rank algorithms for learning a rating model, where the agreement between the human and machine raters is directly incorporated into the loss function. Various linguistic and statistical features are utilized to facilitate the learning algorithms. Experiments on the publicly available English essay dataset, Automated Student Assessment Prize (ASAP), show that our proposed approach outperforms the state-of-the-art algorithms, and achieves performance comparable to professional human raters, which suggests the effectiveness of our proposed method for automated essay scoring. 
Predicting the success of literary works is a curious question among publishers and aspiring writers alike. We examine the quantitative connection, if any, between writing style and successful literature. Based on novels over several different genres, we probe the predictive power of statistical stylometry in discriminating successful literary works, and identify characteristic stylistic elements that are more prominent in successful writings. Our study reports for the ﬁrst time that statistical stylometry can be surprisingly effective in discriminating highly successful literature from less successful counterpart, achieving accuracy up to 84%. Closer analyses lead to several new insights into characteristics of the writing style in successful literature, including ﬁndings that are contrary to the conventional wisdom with respect to good writing style and readability. 
We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model. 
The distributional hypothesis, which states that words that occur in similar contexts tend to have similar meanings, has inspired several Web mining algorithms for paraphrasing semantically equivalent phrases. Unfortunately, these methods have several drawbacks, such as confusing synonyms with antonyms and causes with effects. This paper introduces three Temporal Correspondence Heuristics, that characterize regularities in parallel news streams, and shows how they may be used to generate high precision paraphrases for event relations. We encode the heuristics in a probabilistic graphical model to create the NEWSSPIKE algorithm for mining news streams. We present experiments demonstrating that NEWSSPIKE signiﬁcantly outperforms several competitive baselines. In order to spur further research, we provide a large annotated corpus of timestamped news articles as well as the paraphrases produced by NEWSSPIKE. 
Wikiﬁcation, commonly referred to as Disambiguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most speciﬁc corresponding Wikipedia pages. Previous approaches to D2W focused on the use of local and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these methods fail (often, embarrassingly), when some level of text understanding is needed to support Wikiﬁcation. In this paper we introduce a novel approach to Wikiﬁcation by incorporating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efﬁcient and modular Integer Linear Programming (ILP) formulation of Wikiﬁcation that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candidate generation and ranking Wikipedia titles considerably. Our results show signiﬁcant improvements in both Wikiﬁcation and the TAC Entity Linking task. 
Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the ﬁrst generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20%). 
This paper introduces IQPs (Integer Quadratic Programs) as a way to model joint inference for the task of concept recognition in clinical domain. IQPs make it possible to easily incorporate soft constraints in the optimization framework and still support exact global inference. We show that soft constraints give statistically signiﬁcant performance improvements when compared to hard constraints. 
Different demographics, e.g., gender or age, can demonstrate substantial variation in their language use, particularly in informal contexts such as social media. In this paper we focus on learning gender differences in the use of subjective language in English, Spanish, and Russian Twitter data, and explore cross-cultural differences in emoticon and hashtag use for male and female users. We show that gender differences in subjective language can effectively be used to improve sentiment analysis, and in particular, polarity classiﬁcation for Spanish and Russian. Our results show statistically signiﬁcant relative F-measure improvement over the gender-independent baseline 1.5% and 1% for Russian, 2% and 0.5% for Spanish, and 2.5% and 5% for English for polarity and subjectivity classiﬁcation. 
A very valuable piece of information in newspaper articles is the tonality of extracted statements. For the analysis of tonality of newspaper articles either a big human effort is needed, when it is carried out by media analysts, or an automated approach which has to be as accurate as possible for a Media Response Analysis (MRA). To this end, we will compare several state-of-the-art approaches for Opinion Mining in newspaper articles in this paper. Furthermore, we will introduce a new technique to extract entropy-based word connections which identiﬁes the word combinations which create a tonality. In the evaluation, we use two different corpora consisting of news articles, by which we show that the new approach achieves better results than the four state-of-the-art methods. 
Microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style. In this paper, we study the problem of extracting opinion targets of Chinese microblog messages. Such fine-grained word-level task has not been well investigated in microblogs yet. We propose an unsupervised label propagation algorithm to address the problem. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. Topics in microblogs are identified by hashtags or using clustering algorithms. Experimental results on Chinese microblogs show the effectiveness of our framework and algorithms. 
This paper presents an approach for detecting promotional content in Wikipedia. By incorporating stylometric features, including features based on n-gram and PCFG language models, we demonstrate improved accuracy at identifying promotional articles, compared to using only lexical information and metafeatures. 
We explore Debatepedia, a communityauthored encyclopedia of sociopolitical debates, as evidence for inferring a lowdimensional, human-interpretable representation in the domain of issues and positions. We introduce a generative model positing latent topics and cross-cutting positions that gives special treatment to person mentions and opinion words. We evaluate the resulting representation’s usefulness in attaching opinionated documents to arguments and its consistency with human judgments about positions. 
With the rapid growth of social media, Twitter has become one of the most widely adopted platforms for people to post short and instant message. On the one hand, people tweets about their daily lives, and on the other hand, when major events happen, people also follow and tweet about them. Moreover, people’s posting behaviors on events are often closely tied to their personal interests. In this paper, we try to model topics, events and users on Twitter in a uniﬁed way. We propose a model which combines an LDA-like topic model and the Recurrent Chinese Restaurant Process to capture topics and events. We further propose a duration-based regularization component to ﬁnd bursty events. We also propose to use event-topic afﬁnity vectors to model the association between events and topics. Our experiments shows that our model can accurately identify meaningful events and the event-topic afﬁnity vectors are effective for event recommendation and grouping events by topics. 
Work on authorship attribution has traditionally focused on long texts. In this work, we tackle the question of whether the author of a very short text can be successfully identiﬁed. We use Twitter as an experimental testbed. We introduce the concept of an author’s unique “signature”, and show that such signatures are typical of many authors when writing very short texts. We also present a new authorship attribution feature (“ﬂexible patterns”) and demonstrate a signiﬁcant improvement over our baselines. Our results show that the author of a single tweet can be identiﬁed with good accuracy in an array of ﬂavors of the authorship attribution task. 
This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. An (opinionated) comparison consists of a comparative “predicate” and up to three “arguments”: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. In user-generated product reviews, the “predicate” and “arguments” are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. We address the interesting question how well training an outof-the-box SRL model works for English data. We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identiﬁcation, argument identiﬁcation and argument classiﬁcation) and in three different datasets. 
Constituency parsing with rich grammars remains a computational challenge. Graphics Processing Units (GPUs) have previously been used to accelerate CKY chart evaluation, but gains over CPU parsers were modest. In this paper, we describe a collection of new techniques that enable chart evaluation at close to the GPU’s practical maximum speed (a Teraﬂop), or around a half-trillion rule evaluations per second. Net parser performance on a 4-GPU system is over 1 thousand length30 sentences/second (1 trillion rules/sec), and 400 general sentences/second for the Berkeley Parser Grammar. The techniques we introduce include grammar compilation, recursive symbol blocking, and cache-sharing.  section 22 Marcus et al. (1993), which is comparable with the best CPU parsers Petrov and Klein (2007). Our parser achieves 120 sentences/second per GPU for this sentence set, and over 250 sentences/sec on length ≤ 30 sentences. These results use a Berkeley Grammar approximately twice as big as Yi et al. (2011), an apparent 50x improvement. On a 4-GPU system, we achieve 1000 sentences/sec for length ≤ 30 sentences. This is 2 orders of magnitude faster than CPU implementations that rely heavily on pruning, and 5 orders of magnitude faster than full CKY evaluation on a CPU. Key to these results is a collection of new techniques that enable GPU parsing at close to the GPU’s practical maximum speed (a Teraﬂop for recent GPUs), or around a half-trillion rule evaluations per second. The techniques are:  
In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live ﬁsh) transporter rather than live (ﬁsh transporter)). We show that our plausibility cues outperform a strong baseline and signiﬁcantly improve performance when used in combination with state-of-the-art features. 
We present a new approach to referring expression generation, casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world. Despite an extremely large space of possible expressions, we demonstrate effective learning of a globally normalized log-linear distribution. This learning is enabled by a new, multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms. We train and evaluate the approach on a new corpus of references to sets of visual objects. Experiments show the approach is able to learn accurate models, which generate over 87% of the expressions people used. Additionally, on the previously studied special case of single object reference, we show a 35% relative error reduction over previous state of the art. 
The problem to replace a word with a synonym that ﬁts well in its sentential context is known as the lexical substitution task. In this paper, we tackle this task as a supervised ranking problem. Given a dataset of target words, their sentential contexts and the potential substitutions for the target words, the goal is to train a model that accurately ranks the candidate substitutions based on their contextual ﬁtness. As a key contribution, we customize and evaluate several learning-to-rank models to the lexical substitution task, including classiﬁcation-based and regression-based approaches. On two datasets widely used for lexical substitution, our best models signiﬁcantly advance the state-of-the-art. 
Recent work has developed supervised methods for detecting deceptive opinion spam— fake reviews written to sound authentic and deliberately mislead readers. And whereas past work has focused on identifying individual fake reviews, this paper aims to identify offerings (e.g., hotels) that contain fake reviews. We introduce a semi-supervised manifold ranking algorithm for this task, which relies on a small set of labeled individual reviews for training. Then, in the absence of gold standard labels (at an offering level), we introduce a novel evaluation procedure that ranks artiﬁcial instances of real offerings, where each artiﬁcial offering contains a known number of injected deceptive reviews. Experiments on a novel dataset of hotel reviews show that the proposed method outperforms state-of-art learning baselines. 
Recommendation systems (RS) take advantage of products and users information in order to propose items to consumers. Collaborative, content-based and a few hybrid RS have been developed in the past. In contrast, we propose a new domain-independent semantic RS. By providing textually well-argued recommendations, we aim to give more responsibility to the end user in his decision. The system includes a new similarity measure keeping up both the accuracy of rating predictions and coverage. We propose an innovative way to apply a fast adaptation scheme at a semantic level, providing recommendations and arguments in phase with the very recent past. We have performed several experiments on ﬁlms data, providing textually well-argued recommendations. 
Minimum Error Rate Training (MERT) remains one of the preferred methods for tuning linear parameters in machine translation systems, yet it faces signiﬁcant issues. First, MERT is an unregularized learner and is therefore prone to overﬁtting. Second, it is commonly used on a noisy, non-convex loss function that becomes more difﬁcult to optimize as the number of parameters increases. To address these issues, we study the addition of a regularization term to the MERT objective function. Since standard regularizers such as 2 are inapplicable to MERT due to the scale invariance of its objective function, we turn to two regularizers— 0 and a modiﬁcation of 2— and present methods for efﬁciently integrating them during search. To improve search in large parameter spaces, we also present a new direction ﬁnding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 
Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a signiﬁcant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained outcomes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artiﬁcial semantic models in terms of their cognitive plausibility. 
Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers may be inefﬁcient. We propose to arrange individual local optimizers into organized networks. Our building blocks are operators of two types: (i) transform, which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to ﬁnd better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average over all 19 languages in the 2006/7 CoNLL data) — more than 5% higher than the previous state-of-the-art. 
We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random ﬁeld model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation. 
This paper presents a case study carried out during the Computer Assisted Translation MA course at the Faculty of Translation and Interpreting of the University of Geneva. The main objectives of the CAT course are to provide the following: a general vision of the area of CAT tools (history and evolution of CAT tools, architecture, and current trends); basic technological skills and competences in the use of two well-known commercial CAT tools (SDL Trados Studio 2011 and MultiTrans Prism); and a solid evaluation method suitable for assessing the utility of a specific CAT tool in a defined context of use. The focus of the present paper is to describe the last section of our course, which concerns how to critically evaluate the appropriateness of a CAT tool in a given scenario. The chosen evaluation method was the EAGLES 7-step recipe (1999), which was one of the deliverables of the Evaluation of Natural Language Processing Systems project (EAGLES I and II). We describe in detail how we implemented an evaluation activity driven by new market needs, and present the result of our experience as well as the feedback obtained from our students. 1. Introduction Translation technologies training has been extensively covered in the last years (Jaatinen & Jääskeläienen 2006, Biau Gil 2006, Pym 2006, Muegge 2013, Doherty & Moorkens 2013). In this field, the evaluation of tools has not been generally identified as one of the necessary skills or competences that students need to acquire during a typical translation technology course (ibid). However, the “ability to evaluate the suitability of a tool in relation to technical needs and price” was identified by Pym (2012) as one of the necessary skills that translation students should acquire; later, the same author (2013) emphasises the idea of training the students to develop their own learning and assessing techniques rather than training them to use specific industry tools which could easily be rendered obsolete by changing circumstances. A similar approach was pointed out by Quirion (2002) when talking about localisation training, where he recommended training students on the principles of CAT tools functionalities and their possibilities rather than teaching them a range of specific tools. Our teaching approach combines both: training on the basic CAT tool functionalities and their underlying concepts using two popular commercial tools and, later on in the course, providing students with the necessary knowledge and skills to assess any CAT tool from the standpoint of their specific needs. In particularly, we spend two-thirds of our course teaching the principles of CAT tools, mixing theory lectures with lab sessions in the computer room. In the labs, students are exposed to two industry CAT tools (SDL Trados 2011 and Multitrans Prism 5.5), together with their related alignment and terminology tools. This period helps them to acquire the theoretical  and practical knowledge related to the basic functionalities of a CAT tool: alignment, translation memory management, terminology management and project creation and management. Two assignments complete this section of the course. Each of the assignments presents a simulated translation project, where students receive a translation kit with instructions, related resources and translatable files, and have to hand back the resulting translation project, containing the project package with the translated file, terminology database, translation memory and a corresponding task invoice. The third section of the CAT tool course is dedicated to the evaluation of tools described in the present paper. There are good reasons for putting the evaluation activity at the end of the course; it requires the previous training process, where students acquire the basic knowledge of CAT tools needed to do a proper assessment. CAT tool evaluation is paradoxically very often implemented but less often described in such a way that a clear method can be readily identified. Only a few general frameworks exist, and often they are not even used because of lack of time to construct a thorough evaluation. Most of these evaluations precede a purchase decision and have to be done in haste. The most commonly published evaluation examples are to be found in the form of comparative studies of different CAT tools published in technology magazines or on the Web (Zerfass 2002, 2010; Keller 2011). These cases mainly consist of elaborating a prioritized list of context specific requirements and checking if the required features are present in the systems under comparison. This is the essential foundation for any evaluation attempt (Gow 2003); however, when translators need to choose between several tools, they often do not know how to proceed. For several years now, we have decided to provide our students with a methodology that they will be able to apply during their professional careers. The evaluation method we chose was the EAGLES 7-step recipe (1999), developed in Geneva, which was one of the deliverables of the Evaluation of Natural Language Processing Systems project (EAGLES I and II). The aim of EAGLES was to adapt the relevant ISO standards (ISO/IEC 9126-1 1991 and ISO 14598 1998) to the translation environment and to create a flexible and modifiable evaluation framework using a hierarchical classification of features and attributes (Quah 2006: 142). Since its publication, other measures and methods to evaluate software have been developed; however, to the best of our knowledge, this has not been done in a joint multinational project in the CAT tools area. In the related field of Machine Translation, the European project ISLE1 continued the work started in the original EAGLES project on systematising evaluation procedures and produced the FEMTI framework. The ISO standards ISO/IEC 9126-1 (2001) and ISO/IEC 25010 (2011) are widely used to evaluate software in general but the advantage of the EAGLES recipe is its format, which provides a clear step by step method for performing a user-oriented evaluation. The second reason justifying our choice is the clearly context-based orientation of the method. Our fundamental goal is to convey to our students the fact that: “There is no such thing as a best system, but a best system for a particular situation”, to use Celia Rico’s words (2001). In the literature on CAT tool evaluation and TM system evaluation, three studies are particularly widely referred to: Höge (2002), Gow (2003) and Rico’s previously mentioned article (2000). It is apparent from these studies that several ways of interpreting and implementing EAGLES and the ISO standards exist. Gow (2003) in a perspective of designing and developing a new evaluation 
These recent developments mean traditional translation workflows are becoming supplemented with new ones. Instead of providing a limited choice of service levels, translation providers need to move to a more flexible approach, offering clients a guaranteed quality of output, which need not be 'perfect' human quality. Instead of a single method for evaluating quality, the concept of a “fit-for-purpose” translation will become more widespread.  The required quality depends on factors such as the lifespan and volume of content, its purpose and target audience, and its urgency. For example, while style and fluency are vital for a press release, they’re less important in a technical manual (although accuracy is essential). In some situations, such as real-time conversations, speed is the main priority.  Given these imminent paradigm shifts in the translation process, it is no surprise that Language Service Providers (LSPs) and Language Tools providers are focusing on researching and developing new technologies which support these new use-cases. In this paper we describe Lingo24's Coach translation platform. Coach has been designed with customisable quality levels in mind. As described in Section 4, the tool allows enforcing or disabling of automatic QA checks on a project-by-project basis, which both aids linguists in their task and ensures that the quality level required for each type of content is achieved. The tool also allows for a range of different workflows, such as traditional translation and editing, but also post-editting of customised MT output, as well as raw MT, where appropriate.  The remainder of this paper is organised as follows. Section 2 argues for customisable levels of quality. In Section 3 we detail the many variables that can affect translation quality, explaining how different use-cases can be accommodated by fine-tuning each of them. Section 4 presents our translation platform Coach, which instantiates most of these use-cases by allowing different quality  levels to be ensured on a per-project level. We finish in Section 5 with our conclusions. 2. The Case for Customisable Levels of Quality In Way, (2013), our companion paper, we describe a number of traditional and emerging use-cases where both raw MT and post-edited MT output lead to perfectly acceptable quality for the task at hand. Assuming that ever more use-cases will emerge in the near future, it cannot be the case that where quality is concerned, a 'one-size-fits-all' definition suffices; there must be at least two levels of quality given the existence of both light and full post-editing services. However, there are some who disagree with this basic tenet: One recent trend is the offering of various ‘quality levels’, something professional translators cannot and will not do. For us there is only one quality level: professional, publication-ready quality. (Rose Newell)1 As argued by Way (2013) and Bota et al. (2013:313), (some) translators are quite disparaging towards the work carried out by others in their profession; Bellos (2011:328) – a very well-regarded translator in his own right – refers specifically to criticisms such as “bad translators, 'servile', 'mechanical', second-rate translators”. We're not sure who Newell is referring to by 'us' in the above quote, but it is clear that not all translators would agree with this statement; Bellos observes (op cit., p.335) that “not all [translators] are great at their job”, so the whole notion that there is “one quality level” is inherently flawed. Indeed, rather than trying to be critical of “just about every bulk translation agency”,2 Newell is instead being dismissive of the PEMT work that many thousands of her fellow translators perform – clearly these professional translators are more than capable of offering different levels of quality – especially when one considers the PEMT 'light' service, where the output is less than “professional, publication-ready quality”, but is nonetheless fit for purpose.3 Indeed, one of the main themes emerging from LocWorld 2013 in London was “that the old quality models may no longer be the answer when applied to post-edited output used for new content delivery models”,4 thus rendering Newell's comments even further out-of-date. Sharon O'Brien – another trained translator, note – argues that we are (or at least should be) moving toward a dynamic quality evaluation model for translation.5 She notes that with respect to translation quality evaluation, TAUS members were “dissatisfied with the current ‘one-size-fits-all’ approach and with the fact that little consideration was given to variables such as content type, communicative function, end user requirements, context, perishability, or mode of translation creation (i.e. whether the translation is created by a qualified human translator, unqualified volunteer, MT or TM system or a combination of these).” Much like we argue here, O'Brien states that new ways of measuring quality are required not only because “numerous challenges exist for quality evaluation, including subjectivity, time, inappropriate use of linguistic resources, learning curve and technology”, but also due to recent developments such as budgetary constraints, new paradigms (“the notion of ‘text’ itself is changing, with tweets, blog postings, multi-media and user-generated content all playing a bigger role in the translation production cycle”), new technology, and a new focus from companies on the end-user. Jost Zetzsche agrees with O'Brien (see URL in footnote 2), but as we've alluded to above, he observes that “translation quality will remain a contentious topic of discussion, maybe more so than as a matter of implementation.” He gives specific examples of MT-ed and human-translated Help files on Microsoft's knowledge-base, and notes that “a translator who compares the translation quality of the two articles will immediately have a visceral response: one has 'good quality' and the other seems to scream out its 'poor, machine-translated quality.' But the users? They find them both (virtually) equally helpful” [our emphasis]. He concludes by saying that “the 
In addition, users should be able to easily and quickly make term proposals, to request changes, to give feedback, to discuss terms and exchange ideas with colleagues in a chat. Every user should be invited and be able to contribute to terminology work by providing useful information or expert knowledge. At the same time, quite contrary to a Wiki-type approach, we do believe that terminology should be managed very carefully. It is a valuable asset, containing a huge potential for knowledge management, training, information about the core competencies, processes and products of a company or organisation. Increasingly, we see the worlds of knowledge management, semantic web and terminology merge and approach one other. These worlds, too, are highly standardized, and terminology should therefore also be. What is required, thus, is a collaborative and “motivating” workflow, which, nevertheless, follows strict rules, hierarchies and decision-making processes so that all aspects of a concept will ultimately be accepted by different branches of an organisation. This involves the main staff such as terminologists and approvers, but also less frequent users such as external translators or incountry reviewers. Particularly for these, the collaboration should be multi-channel and intuitive. They simply should read the concepts and click on Approve or Reject. According to best practices, the terminologist should then have the last word and “sign off” on the entry. In our corporate example, the workflow starts with the “source languages” German and English. These are gained through term requests, prepared by the terminologist with the help of “subject matter experts” via the web-based terminology management software quickTerm. Once the entry is prepared, the terminologist submits it to approval within the terminology group, which is made up of people from many different areas of the corporation. If agreed upon, the entry is marked as “final” in German and English. After that, the foreign-language workflow starts which, in this case, is outsourced to the translation service provider, but tightly integrated with an approval process through the in-country reviewers. This again is managed web-based through quickTerm. From Monologue to “Multilogue” As mentioned in the introductory statement, the key to a successful terminology work lies in the motivation of all users. We have witnessed a baffling success of all kinds of Social Media with their possibilities for interactivity and motivation. There are enormously successful crowd sourcing projects in all realms, powered by a common cause, enthusiasm for a product or the feeling of belonging to a community. This gave us the impetus to think how we can incorporate some of these useful participatory elements into a terminology software solution. Another critical success factor of all terminology projects we have seen is the constant “selfmarketing” of the terminology group. It is necessary to permanently remind people of both the  termbase´s existence and benefit for know-how and information. Thus, we have developed some ideas on how we can spice termbases up with some “marketing instruments” and fun little edutainment tools. One example is the “Term of the Week”. For instance, as employees sign in to the intranet, a specially prepared editorial "Term of the Week" welcomes and thus familiarizes them with terminology in an easily accessible way. In this manner, the language department (or any department in charge of terminology work) can provide the colleagues with important concepts and terms, tailored to their work environment. The Term of the Week is updated weekly and can contain quite different information: Dictionary, false friends, new terms, or key words from the media. Useful phrases for everyday business are also offered. And from the Term of the Week, the user can dive into the termbase itself with just one mouse click. All these initiatives increase both the use and the length of use as the extremely positive feedback from companies and organisations as the Oesterreichische Nationalbank (Austria’s central bank) demonstrates impressively. The “Term of the Week” is only one example for “luring” people into the termbase. Another one is to offer regular “term quizzes”. These are terminologically relevant questions which have to do with the termbase content or frequently misused terms, for instance. Again, by giving wrong answers, the user is motivated to check the correct term in the termbase, and once again you have the users right where you want them: in your termbase. With these additional elements, our university example was able to maintain an interest in the termbase and obtain input from many different subject-matter experts. This in turn improves the quality of the termbase, thus enhancing the satisfaction of the user, the acceptance of what it has to offer, and the enthusiasm to collaborate in its improvement and extension. Conclusion The goal of this paper was to show:  It is possible to motivate people to actively participate in the terminology process by offering an intuitive system and additional “fun” elements which “lure” them into the termbase and into participating in the project. This in turn boosts both the quantity and the quality of the input.  It is vital to establish a controlled yet collaborative workflow to define terminology and to enhance acceptance.  This not only requires a solid software foundation, but also constant self-marketing and a well-defined process. Author: Klaus Fleischmann Managing Director klaus@kaleidoscope.at Kaleidoscope GmbH Stojanstr. 26a, A-2344 Maria Enzersdorf, Austria Grew up with languages and computers in Austria and the US. Studied conference interpreting in Vienna & Monterey, California, and technical communication at Danube University Krems. Long work  
Variation between post-editors of machine translation is a well-known issue. This variation shows itself in post-editing speed, amount of editing and differing ﬁnal translations. However, relatively few studies exploring the differences have been reported. This paper describes a postediting task involving controlled language tourist phrases translated from English into Finnish. Post-editors select the best out of three machine translated suggestions, which they can accept without editing or post-edit as necessary. Agreement between editors is analyzed and reported in terms of selecting the best suggestion, deciding its acceptability, and producing a ﬁnal post-edited version. Editors are compared in terms of post-editing time, edit distance and ﬁnal translations created. With a qualitative analysis, we examine differences between the selected and rejected suggestions as well as differences between the post-edited versions created by different editors. Examples of editor preferences are also discussed. 
Post-editing is an increasingly common form of human-machine cooperation for translation. One possible support for the post-editing task is offering several machine outputs to a human translator from which then can choose the most suitable one. This paper investigates the selection process for such method to get a better insight into it so that it can be optimally automatised in future work. Experiments show that only about 70% of the selected sentences are the best ranked ones, and that selection mechanism is tightly related to edit distance. Furthermore, ﬁve types of performed edit operations are analysed: correcting word form, reordering, adding missing words, deleting extra words and correcting lexical choice.  edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by translation memory), they do not examine ranking of these outputs, they have not tested their automatic method by professional translators, and they do not analyse edit distances and the performed edit operations. Our main questions are: • Is the translation output which is best for postediting also the best ranked one? • Is the edit distance of the chosen output lower than edit distances of the other outputs?  
At present, the task of post-editing Machine Translation (MT) is commonly carried out via Translation Memory (TM) tools that, while well-suited for editing of TM matches, do not fully support MT post-editing. This paper describes the results of a survey of professional translators and post-editors, in which they chose features and functions that they would like to see in translation and post-editing User Interfaces (UIs). 181 participants provided details of their translation and post-editing experience, along with their current working methods. The survey results suggest that some of the desired features pertain to supporting the translation task in general, even before post-editing is considered. Simplicity and customizablity were emphasized as important features. There was cautious support for the idea of a UI that made use of post-edits to improve the MT output. This research is intended as a first step towards creating specifications for a UI that better supports the task of post-editing. 
Over the past decade machine translation has reached a high level of maturity and is now routinely utilized by a wide variety of organizations, including multinational corporations, language service providers, and governmental/non-proﬁt organizations. However, there are many communities that could beneﬁt greatly from machine translation but do not actively use it, either because of a lack of awareness of its current capabilities, or because of real or perceived barriers to adopting machine translation technology. In this paper we present a case study of introducing machine translation in combination with post-editing to one such community, namely employees at local and regional public health departments in the U.S. We describe a methodology for determining their translation needs, and describe the development of an integrated post-editing and translation management system speciﬁcally targeted to their typical workﬂow. We report results from user testing and participatory design studies and conclude with a set of recommendations and best practices for introducing machine translation plus post-editing to lay user communities. 
We carried out a machine-translation postediting pilot study with users of an IT support forum community. For both language pairs (English to German, English to French), 4 native speakers for each language were recruited. They performed monolingual and bilingual postediting tasks on machine-translated forum content. The post-edited content was evaluated using human evaluation (ﬂuency, comprehensibility, ﬁdelity). We found that monolingual post-editing can lead to improved ﬂuency and comprehensibility scores similar to those achieved through bilingual post-editing, while we found that ﬁdelity improved considerably more for the bilingual set-up. Furthermore, the performance across post-editors varied greatly and it was found that some post-editors are able to produce better quality in a monolingual set-up than others. 
The poor quality of user-generated content (UGC) found in forums hinders both readability and machine-translatability. To improve these two aspects, we have developed human- and machine-oriented pre-editing rules, which correct or reformulate this content. In this paper we present the results of a study which investigates whether pre-editing rules that improve the quality of statistical machine translation (SMT) output also have a positive impact on post-editing productivity. For this study, pre-editing rules were applied to a set of French sentences extracted from a technical forum. After SMT, the post-editing temporal effort and final quality are compared for translations of the raw source and its pre-edited version. Results obtained suggest that pre-editing speeds up post-editing and that the combination of the two processes is worthy of further investigation. 
Existing translation quality assessment (TQA) metrics have a few major drawbacks: they are often subjective, their scope is limited to the sentence level, and they do not take the translation situation into account. Though suitable for a general assessment, they lack the granularity needed to compare different methods of translation and their respective translation problems. In an attempt to solve these issues, a two-step TQA-approach is presented, based on the dichotomy between adequacy and acceptability. The proposed categorization allows for easy customization and user-deﬁned error weights, which makes it suitable for different types of translation assessment, analysis and comparison. In the ﬁrst part of the paper, the approach is explained. In the second part of the paper, the approach is tested in a pilot study designed to compare human translation with post-editing for the translation of general texts (newspaper articles). Inter-annotator results are presented for the translation quality assessment task as well as general ﬁndings on the productivity and quality differences between postediting and human translation of student translators. 
Translation agencies are introducing statistical machine translation (SMT) into the work flow of human translators. Typically, SMT produces a first-draft translation, which is then post-edited by a person. SMT has met much resistance from translators, partly because of professional conservatism, but partly because the SMT community has often neglected some practical aspects of translation. Our paper discusses one of these: transferring formatting tags such as bold or italic from the source to the target document with a low error rate, thus freeing the post-editor from having to reformat SMT-generated text. In our “two-stream” approach, tags are stripped from the input to the decoder, then reinserted into the resulting target-language text. Tag transfer has been tackled by other SMT teams, but only a few have published descriptions of their work. This paper contributes to understanding tag transfer by explaining our approach in detail. 
This paper presents a comparative evaluation of four tools that can be used to collect user activity data (UAD) in machine translation post-editing (PE) research: Tobii Studio, Translog-II, TransCenter, and PET. These tools are analysed here based on empirical data as a way of providing a picture of what the current state of research has to offer in terms of technology and investigation methods. After an analysis of the features offered by the tools, a summary is drawn and potential room for improvement in the field is identified. 
An interactive Multilingual Access Gateway (iMAG) dedicated to a website S (iMAG-S) is a good tool to make S accessible in many languages immediately and without editorial responsibility. Visitors of S as well as paid or unpaid posteditors and moderators contribute to the continuous and incremental improvement of the most important textual segments, and eventually of all. In this approach, pre-translations are produced by one or more free machine translation (MT) systems. Continuous use since 2008 on many websites and for several access languages shows that a quality comparable to that of a first draft by junior professional translators is obtained in about 40% of the (human) time, sometimes less. There are two interesting side effects obtainable without any added cost: iMAGs can be used to produce highquality parallel corpora, and to set up a permanent task-based evaluation of one or more MT systems. 
This work investigates a crucial aspect for the integration of MT technology into a CAT environment, that is the ability of MT systems to adapt from the user feedback. In particular, we consider the scenario of an MT system tuned for a speciﬁc translation project that after each day of work adapts from the post-edited translations created by the user. We apply and compare different state-of-the-art adaptation methods on post-edited translations generated by two professionals during two days of work with a CAT tool embedding MT suggestions. Both translators worked at the same legal document from English into Italian and German, respectively. Although exactly the same amount of translations was available each day for each language, the application of the same adaptation methods resulted in quite different outcomes. This suggests that adaptation strategies should not be applied blindly, but rather taking into account language speciﬁc issues, such as data sparsity. 
This paper presents an online environment aimed at community post-editing, which can record and store any post-editing action performed on machine translated content. This environment can then be used to generate reports using the standard XLIFF format with a view to provide stakeholders such as machine translation providers, content developers and online community managers with detailed information on post-editing actions. This paper presents in detail the functionality available within the environment as well as the design choices that were made when creating this environment. Preliminary usability feedback received to date suggests that the feature set is sufﬁcient to perform community postediting. 
We present a methodology integrating hybrid and rule-based components for speeding up the development of a patent MT system. The methodology is suitable for highly inflecting languages and described on the example of translating patent claims from Russian into English. Based on different combinations of hybrid and rule-based components the system performs shallow or/and deep parsing and provides for several complementary levels of output, - (i) translation of terminology, that only involves shallow MT procedures, and (ii) full translation that is based on both shallow and deep parsing integrated either automatically, or in an interactive environment. Full translation of the patent claim is output in two formats, - a legal one sentence format and a better readable set of simple sentences. To control the quality of claim translation by better understanding the input, the system also outputs a SL claim decomposed into simple sentences. 
In the previous methods of generating bilingual lexicon from parallel patent sentences extracted from patent families, the portion from which parallel patent sentences are extracted is about 30% out of the whole “Background” and “Embodiment” parts and about 70% are not used. Considering this situation, this paper proposes to generate bilingual lexicon for technical terms not only from the 30% but also from the remaining 70% out of the whole “Background” and “Embodiment” parts. The proposed method employs the compositional translation estimation technique utilizing the remaining 70% as a comparable corpus for validating translation candidates. As the bilingual constituent lexicons in compositional translation, we use an existing bilingual lexicon as well as the phrase translation table trained with the parallel patent sentences extracted from the 30%. Finally, we show that about 3,600 technical term translation pairs can be acquired from 1,000 patent families. 
One of the characteristics of patent sentences is long, complicated modifications. A modification is identified by the presence of a head word in the modifier. We extracted head words with a high occurrence frequency from about 1 million patent sentences. Based on the results, we constructed a modifier correcting system using these head words. About 60% of the errors could be modified with our system. 
The purpose of patent translation is to correctly translate patent documents from one language to another language semantically and syntactically. In this paper, we view patent translation as technical document translation given their domain similarity in terms of their terminologies and writing styles. From this viewpoint, we simply perform patent translation using a technical domain MT system without any further domain adaptation. Experimental results in a Chinese-to-Korean MT system shows that the improved translation performance in technical domain leads to a further improvement in patent translation. 
 order to improve the quality of Japanese-English machine translation of patent documents we pro-  This paper describes the development of a Japanese to English translation system using multiple resources and NTCIR-10 Patent translation collection. The MT system is based on different training data, the Wiktionary as a bilingual dictionary and Moses decoder. Due to the lack of parallel data on the patent domain, additional training data of the general domain was extracted from Wikipedia. Experiments using NTCIR-10 Patent translation data collection showed an improvement of the BLEU score when using a 5-grams language model and when adding the data extracted from Wikipedia but no improvement when adding the Wiktionary.  pose to increase the size of the parallel patent corpus by adding parallel text extracted from Wikipedia and a dictionary of bilingual terminology extracted from the Wiktionary. This paper describes the SMT system developed for the translation of Japanese to English patent documents, using NTCIR-10 data collection1. We try to improve the quality of translation of the Japanese to English patent documents by exploiting multilingual resources such as Wikipedia and Wiktionary. This paper is organized as follows: In Section 2, we describe the patent documents and in Section 3 we describe some recent works for this domain. The approach used to develop the SMT system is described in Section 4. Section 5 and 6 give an overview of the proposed approach to  
Collocation identiﬁcation and anaphora resolution are widely recognized as major issues for natural language processing, and particularly for machine translation. This paper focuses on their intersection domain, that is verb-object collocations in which the object has been pronominalized. To handle such cases, an anaphora resolution procedure must link the direct object pronoun to its antecedent. The identiﬁcation of a collocation can then be made on the basis of the verb and its object or its antecedent. Preliminary results obtained from the translation of a large corpus will be discussed. 
This paper outlines a methodology and a system for collocation retrieval and translation from parallel and comparable corpora. The methodology was developed with translators and language learners in mind. It is based on a phraseology framework, applies statistical techniques, and employs source tools and online resources. The collocation retrieval and translation has proved successful for English and Spanish and can be easily adapted to other languages. The evaluation results are promising and future goals are proposed. Furthermore, conclusions are drawn on the nature of comparable corpora and how they can be better exploited to suit particular needs of target users. 
This paper addresses the impact of multiword translation errors in machine translation (MT). We have analysed translations of multiwords in the OpenLogos rule-based system (RBMT) and in the Google Translate statistical system (SMT) for the English-French, English-Italian, and English-Portuguese language pairs. Our study shows that, for distinct reasons, multiwords remain a problematic area for MT independently of the approach, and require adequate linguistic quality evaluation metrics founded on a systematic categorization of errors by MT expert linguists. We propose an empirically-driven taxonomy for multiwords, and highlight the need for the development of speciﬁc corpora for multiword evaluation. Finally, the paper presents the Logos approach to multiword processing, illustrating how semantico-syntactic rules contribute to multiword translation quality. 
Due to the formal variability and the irregular behaviour of MWEs on different levels of linguistic description, they are a potential source of errors for many NLP applications, e.g. Machine Translation. While most of the known approaches to MWE identiﬁcation focus on one dimension of irregular behaviour, we present an approach that combines morpho-syntactic features (extracted from dependency parsed text) with semantic opacity features (approximated using word alignments). We trained supervised classiﬁers with different feature sub-sets and show that the combination of morphosyntactic and semantic opacity features yields best overall results.  We present a method to identify German MWEs of the type preposition+noun+verb based on a rich feature set comprising morpho-syntactic features extracted from monolingual data and cross-lingual features obtained from word-aligned parallel data (DE-EN, DE-SE). The used features aim at modelling characteristic properties of MWEs, namely ﬁxedness in terms of their disposition for variation with respect to e.g. number or type of determiner (morpho-syntactic features) and irregular translational behaviour, e.g. a broad variation in translational equivalents (cross-lingual features). Our experiments show that combining these different types of features leads to an improved classiﬁcation accuracy. Our approach consists of three main steps: 1. extraction of syntactically related multiword constructions  
This paper proposes a methodological approach to CLIR applications for the development of a system which improves multi-word processing when specific domain translation is required. The system is based on a multilingual ontology, which can improve both translation and retrieval accuracy and effectiveness. The proposed framework allows mapping data and metadata among language-specific ontologies in the Cultural Heritage (CH) domain. The accessibility of Cultural Heritage resources, as foreseen by recent important initiatives like the European Library and Europeana, is closely related to the development of environments which enable the management of multilingual complexity. Interoperability between multilingual systems can be achieved only by means of an accurate multi-word processing, which leads to a more effective information extraction and semantic search and an improved translation quality. 
The translation of English phrasal verbs (PVs) into French is a challenge, specially when the verb occurs apart from the particle. Our goal is to quantify how well current SMT paradigms can translate split PVs into French. We compare two inhouse SMT systems, phrase-based and hierarchical, in translating a test set of PVs. Our analysis is based on a carefully designed evaluation protocol for assessing translation quality of a speciﬁc linguistic phenomenon. We ﬁnd out that (a) current SMT technology can only translate 27% of PVs correctly, (b) in spite of their simplistic model, phrase-based systems outperform hierarchical systems and (c) when both systems translate the PV similarly, translation quality improves. 
This work describes an experimental evaluation of the signiﬁcance of phrasal verb treatment for obtaining better quality statistical machine translation (SMT) results. Phrasal verbs are multiword expressions used frequently in English, independent of the domain and degree of formality of language. They are challenging for natural language processing due to their idiosyncratic semantic and syntactic properties. The meaning of phrasal verbs is often not directly derivable from the semantics of their constituent tokens. In addition, they are hard to identify in text because of their ﬂexible structure and due to ambiguous prepositional phrase attachments. The importance of the detection and special treatment of phrasal verbs is measured in the context of SMT, where the wordfor-word translation of these units often produces incoherent results. Two ways of integrating phrasal verb information in a phrase-based SMT system are presented. Automatic and manual evaluations of the results reveal improvements in the translation quality in both experiments. 
In this paper we describe COACH, a new Computer-Assisted Translation (CAT) tool that we have developed in close conjunction with two focus groups of experienced translators. We explain why we thought it was essential for end-users to be involved from the start, and the process by which their feedback was gathered and acted upon for the ultimate benefit of Lingo24, the translators and our clients. We list a number of key changes that were made as a direct consequence of involving end-users from the get-go. As a result, we believe that COACH, which is already deployed in a live environment, has the potential to be a game-changer in our industry. 
This study presents several experiments to show the power of domain-speciﬁc adaptation by means of hybrid terminology extraction mechanisms and the subsequent terminology integration into a rule based machine translation (RBMT) system, thus avoiding cumbersome human lexicon and grammar customization. Detailed evaluation reveals the great potential of this approach: Translation quality can be improved substantially in two domains. 
This paper presents part of the work carried out in EDI-TA, in the context of the project MultilingualWeb-LT1. The aim is to implement the Internationalization Tag Set 2.0 (ITS 2.0) in an MT context for post-editing purposes. After a brief review of MultilingualWeb-LT’s main objectives and a presentation of ITS 2.0 major features, our paper will concentrate on the description of an Online MT showcase. Here ITS 2.0 information, so called “data categories”, are tested in a postediting scenario. 
Return on Investment (ROI) analysis plays a primary role in business strategic planning; however it is not such a straightforward process in itself. From the perspective of a Language Service Provider’s (LSP) case study, an economic impact analysis of machine translation (MT) technology usage is required. Potential initial development and customization costs are investigated, as well as estimations of financial consequences resulting from the use of MT in professional translation. Different courses of action are weighed up, and supporting arguments are presented for the strategy of choice. 
 TAPTA described in Pouliquen et al 2011), based on Moses (Koehn et al. 2007), and to train  Described is a large-scale implementa-  it with UN documents. This first experimental  tion of a Moses-based machine transla-  system, described in Pouliquen et al. 2012, was  tion system in the United Nations aiming  trained with a corpus of 11 years of English-  at accelerating the work of translators.  Spanish UN documents, translated and revised  The system (called TAPTA4UN) has  by a large team of highly-skilled translators. The  been trained on extensive parallel corpora  results of this system, which were reflected in  in 6 languages. Both automatic and hu-  automatic and human evaluations, elicited a very  man evaluations are provided. The sys-  positive reaction from Spanish translators, in par-  tem is now used in production by profes-  ticular revisers (senior translators), who do not  sional translators. The technical chal-  adopt new technologies easily, but who found  lenges of scalability and the final evalua-  that MT could be useful, as it eliminated the need  tion by users are also described.  for typing and it presented highly consistent ter-  minology (hence the name: translation accelera-  
This paper reports on the results of a user satisfaction survey carried out among 16 translators using a new computer-assisted translation workbench. Participants were asked to provide feedback after performing different post-editing tasks on different conﬁgurations of the workbench, using different features and tools. Resulting from the feedback provided, we report on the utility of each of the features, identifying new ways of implementing them according to the users’ suggestions. 
While the awareness for the importance of effective terminology management processes has grown significantly in the recent past, the starting point for terminology projects or terminology work is often less than ideal: In many cases, usable terminology assets do not exist at all, are of unknown origin or out of sync with the relevant content. Efficiently establishing terminology requirements that incorporate existing assets and creating smart terminology management workflows are almost impossible without linguistic support. Content optimization tools or more generic linguistically based text analytics tools are not always available or do not find their way into the corporate budget because of their often prohibitive cost. However, if an inherent need for MT exists—a global corporate strategy can be reason enough to set up a company-wide MT portal for internal gist translation—two birds can potentially be eliminated with one stone: While an RBMT system can be easily and elegantly integrated into the corporate terminology management workflow, the results that can be achieved by tapping into the inherent linguistic intelligence of an RBMT system beyond its perceived conventional purpose may provide additional justification for purchasing and maintaining such a system. Our presentation will outline a practical approach towards this goal.  
Combining MT engines with a strong service wrapper delivered by experts, we have expanded the well-known triangle Cost/Quality/time, enabling new service levels and broadening the choice for the customer.  Although conceptually simple, this strategy had to address and resolve multiple practical problems, going from quality and specifications of final output, down to enhanced linguistic resources and data lifecycle management controlling and preventing data contamination. We will explore those along this presentation.  
 workflow tool was offered by one of our vendors which links the RMT engine together with the IBM tools and processes landscape. An efficient workflow support is therefore abs lutely key, and the same is true for terminology handling. We consider the smart terminology capabilities of the RMT system a core advantage that may help us to reach the break even point. Our internal analysis shows that an overwhelming number of terms used for translation are more or less complex compounds. An MT system that does not know these terms is likely to fail, and vice versa: If we build up terminology the right way, RMT could potentially pay off. In addition to the RMT engine and the workflow tool which we both bought from vendors, we implemented a so called ‘Terminology Verification Package’ (TVP) ourselves. This is a VBA based spreadsheet solution which compacts the list of unknown terms as listed by the RMT engine and provides an efficient link to the terminologist. Approved terms get converted and fed into the MT lexicon. This gives us highly jobspecific terminology exactly when we need it. This presentation is about IBM's past experience with the RMT solution we have in use, describing the roles of key contributors like terminologists, post-editors and SMEs in due consideration of the results we have achieved so far. 
While SMT systems can learn to translate multiword expressions (MWEs) from parallel text, they typically have no notion of non-compositionality, and thus overgeneralise translations that are only used in certain contexts. This paper describes a novel approach to measure the ﬂexibility of a phrase pair, i.e. its tendency to occur in many contexts, in contrast to phrase pairs that are only valid in one or a few ﬁxed expressions. The measure learns from the parallel training text, is simple to implement and language independent. We argue that ﬂexible phrase pairs should be preferred over inﬂexible ones, and present experiments with phrase-based and hierarchical translation models in which we observe performance gains of up to 0.9 BLEU points. 
The conventional machine translation evaluation metrics tend to perform well on certain language pairs but weak on other language pairs. Furthermore, some evaluation metrics could only work on certain language pairs not language-independent. Finally, no considering of linguistic information usually leads the metrics result in low correlation with human judgments while too many linguistic features or external resources make the metrics complicated and difﬁcult in replicability. To address these problems, a novel language-independent evaluation metric is proposed in this work with enhanced factors and optional linguistic information (part-of-speech, n-grammar) but not very much. To make the metric perform well on different language pairs, extensive factors are designed to reﬂect the translation quality and the assigned parameter weights are tunable according to the special characteristics of focused language pairs. Experiments show that this novel evaluation metric yields better performances compared with several classic evaluation metrics (including BLEU, TER and METEOR) and two state-of-the-art ones including ROSE and MPF. 
We describe a metric for estimating the quality of Statistical Machine Translation (SMT) output based on syntactic features extracted using Combinatory Categorial Grammar (CCG). CCG has been demonstrated to be better suited to deal with SMT texts than context free phrase structure grammar formalisms. We use CCG features to estimate the grammaticality of the translations by dividing them into maximal grammatical chunks extracted from their CCG parse chart. We compare the performance of our CCG features with strong baseline and linguistic feature sets on French–English and Arabic–English data sets annotated with various quality scores. The results show that our CCG features outperform the baseline and linguistic features in most of the experiments. Furthermore, we demonstrate that our CCG features complement other types of features: combining CCG features with the baseline and other linguistic features furthers their performance. 
Human translators are the key to evaluating machine translation (MT) quality and also to addressing the so far unanswered question when and how to use MT in professional translation workﬂows. Usually, human judgments come in the form of ranking outputs of different translation systems and recently, post-edits of MT output have come into focus. This paper describes the results of a detailed large scale human evaluation consisting of three tightly connected tasks: ranking, error classiﬁcation and post-editing. Translation outputs from three domains and six translation directions generated by ﬁve distinct translation systems have been analysed with the goal of getting relevant insights for further improvement of MT quality and applicability. 
We present a study on the Active Learning (AL) paradigm applied to a multitask labeling scenario for Statistical Machine Translation (SMT). The main goal of this research is to show that the learning of a phrase-based model can be improved despite having a small corpus annotated with word-level alignments. We propose a simple scheme for supervised training of a SMT model with a Multi-task AL (MTAL) approach to get a bilingual corpus with word-level alignments from the scarce data. The main advantage of the AL paradigms is the intelligent sampling using informativeness functions throughout the entire labeling process. We experimented with the MTAL approach and this approach is compared with the Single-Task AL (STAL) approach. The STAL process was used to semi-supervised training of SMT models. We found out that the MTAL approach improved the efﬁciency obtained by STAL process. In order to compare the performance of both the STAL and MTAL approaches, we experimented with two types of passive learners named random sampling. An assessment of the entire labeling process was done in order to obtain an overall performance compared to passive learners. In the analysis of the experiments it was found that the MTAL outperformed the passive learners and the STAL approach.  
In this paper we present the results from a pilot study undertaken with translation students to compare community forum content post-editing performance based on suggestions from different translation systems. Output from both Translation Memory (TM) and Statistical Machine Translation (SMT) was presented to participants in the ACCEPT online postediting environment, where they needed to perform a translation task with and without translation proposals. Observed data showed that post-edited MT output obtained higher results on each of the variables measured: the amount of time needed to complete the task, the participants’ keystroke movements and the quality of the resulting translations. 
Some source texts are more difﬁcult to translate than others. One way to handle such texts is to modify them prior to translation. Yet, a prominent factor that is often overlooked is the source translatability with respect to the speciﬁc translation system and the speciﬁc model that are being used. We present an approach, and an interactive tool implementing it, where source sentences are rewritten in order to maximize conﬁdence estimates with respect to the translation model. The automatically-generated rewritings are then proposed for the user’s approval. Such an approach can reduce post-editing effort, replacing it by costeffective pre-editing that can be done by monolinguals. 
We describe a project on introducing an in-house statistical machine translation system for marketing texts from the automobile industry with the ﬁnal aim of replacing manual translation with post-editing, based on the translation system. The focus of the paper is the suitability of such texts for SMT; we present experiments in domain adaptation and decompounding that improve the baseline translation systems, the results of which are evaluated using automatic metrics as well as manual evaluation. 
In this system demonstration paper we present a cloud-based platform providing online terminology services for human and machine users. We focus on the use case for the application of online terminology services in statistical machine translation and describe the applied methods for monolingual and bilingual terminology integration into statistical machine translation during training and translation phases. Keywords: online services, terminology service, statistical machine translation, terminology extraction, terminology translation 
In this paper we tackle the problem of character conversion from simplified Chinese to traditional Chinese. Of those simplified characters that need conversion, about 9.5% of them have more than 2 counterparts in the traditional scripts. We improve upon the previous log-linear approach first used in (Chen et al 2011) by utilizing more data sets and better translation models. We also show that automatic classification and noise reduction of corpus can achieve better performance. As a proof of the validity of our approach, we scored No. 1 in a recent evaluation of simplified to traditional character conversion systems organized by the Chinese Information Processing Society of China. 
This paper addresses the problem of predicting how adequate a machine translation is for gisting purposes. It focuses on the contribution of lexicalised features based on different types of topic models, as we believe these features are more robust than those used in previous work, which depend on linguistic processors that are often unreliable on automatic translations. Experiments with a number of datasets show promising results: the use of topic models outperforms the state-of-the-art approaches by a large margin in all datasets annotated for adequacy. 
We present a novel machine translation model that combines the beneﬁts of phrasebased and n-gram-based statistical machine translation (SMT) and remedies their drawbacks. The model is based on a joint source channel probability model that represents the translation process as a linear sequence of operations. The sequence includes not only translation operations but also reordering operations. As in ngram-based SMT, the model is: (i) based on minimal translation units, (ii) takes both source and target information into account, (iii) does not make a phrasal independence assumption and (iv) avoids the spurious phrasal segmentation problem. As in phrase-based SMT, the model (i) has the ability to memorize lexical reordering triggers, (ii) builds the search graph dynamically and (iii) decodes with large translation units during search. The model has two important properties. First, it strongly couples reordering and translation; this provides a better reordering mechanism for restricting the position to which a word or phrase can be moved and provides a single framework that handles both short and long distance reorderings effectively. Second, no hard reordering constraint needs to be imposed on the model; phrase-based models must impose such constraints because they model reordering poorly. Using BLEU as a metric of translation accuracy, we found that our system performs signiﬁcantly better than state-of-the-art phrasebased systems (Moses and Phrasal) and n-gram-based systems (Ncode) on standard translation tasks. 
Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 303. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. 
This paper proposes a way of augmenting bilingual terminologies by using a “generate and validate” method. Using existing bilingual terminologies, the method generates “potential” bilingual multi-word term pairs and validates their status by searching web documents to check whether such terms actually exist in each language. Unlike most existing bilingual term extraction methods, which use parallel or comparable corpora, the proposed method can take advantage of a wider variety of textual corpora. Experiments using Japanese-English terminologies of ﬁve domains show that the method is highly promising.  optimization,” “linear function,” “convex programming” and “convex function,” we can reasonably assume that the term “convex optimization,” which is not listed in the terminology, may, or will come to, exist (Figure 1). By generating “potential” term candidates and validating their existence by using web data, it should be possible to identify a range of new terms which are not covered in existing terminologies. Assuming bilingual correspondence at the level of constituent units of terms, it is possible to extend this idea to obtain new bilingual term pairs. Based on this idea, we developed a fully operating system for detecting new bilingual term pairs in order to augment bilingual terminologies.  
In an online learning protocol, immediate feedback about each example is used to reﬁne the next prediction. We apply this protocol to statistical machine translation for computer-assisted translation and compare generative and discriminative approaches for online adaptation. We develop our methods on reference translations and test on feedback gathered from professional translators. Experimental results show that improvements of straightforward adaptations of translation and language model are greater than those achieved by discriminative re-ranking. However, the improvements add up to 4 BLEU points over a baseline static model. 
We propose a method for inducing romanization systems directly from a bilingual alignment at the grapheme level. First, transliteration word pairs are aligned using a non-parametric Bayesian approach, and then for each grapheme sequence to be romanized, a particular romanization is selected according to a user-specified criterium. We apply our approach to the task of transliteration mining, and used Levenshtein distance as the selection criterium. We performed experiments on three languages with differing characteristics: Japanese, Russian and Chinese. Our experiments show that the mining system built from the induced romanization system is able to outperform existing baseline romanization systems. By extending our approach to induce romanization systems based on other criteria we expect our technique may find more general application in the future. 
The effective integration of MT technology into CAT tools is a challenging topic both for academic research and the translation industry. Particularly, professional translators feel crucial the ability of MT systems to adapt to their feedback. In this paper, we propose an adaptation scheme to tune a statistical MT system to a translation project using small amounts of postedited texts. By running ﬁeld tests on two domains with 8 professional translators working with a CAT tool, productivity gains up to over 20% were measured after applying MT project adaptation. 
The integration of machine translation in the human translation work ﬂow rises intriguing and challenging research issues. One of them, addressed in this work, is how to dynamically adapt phrase-based statistical MT from user post-editing. By casting the problem in the online machine learning paradigm, we propose a cache-based adaptation technique method that dynamically stores target n-gram and phrase-pair features used by the translator. For the sake of adaptation, during decoding not only recency of the features stored in the cache is rewarded but also their occurrence in similar already translated sentences in the document. Our experimental results show the effectiveness of the devised method both on standard benchmarks and on documents post-edited by professional translators through the real use of the MateCat tool. 
 wood can refer to a piece of a tree or forested area),  Lexical ambiguity can cause critical failure in conversational spoken language translation (CSLT) systems due to the wrong sense being presented in the target language. In this paper, we present a framework for improving translation of ambigu-  or completely unrelated as in general homographs (e.g. fair can mean pale complexion, equitable, or carnival). Statistical machine translation (SMT) of spontaneous conversational speech is particularly susceptible to word sense errors arising from these ambiguous words due to two primary factors.  ous source words that (a) constrains statistical machine translation (SMT) decoding with phrase pair clusters to select a desired sense for translation; (b) automatically predicts the intended sense of an ambiguous source word given its context; and (c) combines the above to deﬁne a set of interactive strategies to conﬁrm the intended sense of an ambiguous word and  First, phrase-based SMT constructs hypotheses based on phrase pairs with limited context. Thus, it is liable to mis-translate less frequent senses of an ambiguous source word if the latter is not disambiguated by the surrounding words. Further, translation errors can occur even if the local context fully disambiguates the word, if that context has not been observed in SMT training data.  guide the system to the correct translation. The novel use of this framework in a realworld CSLT system distinguishes our approach from the existing work focusing on word sense disambiguation (WSD) for non-interactive, batch-mode SMT. In addition to reporting metrics that evaluate this approach in an interactive spoken language translation system, we also present ofﬂine assessments of the component technologies, viz. constrained SMT decoding with  Second, spontaneous conversational speech often depends on dialog context to fully understand and interpret. For instance, the word fair in the utterance “the village head is a fair man” could refer to pale complexion or equitable, depending on the dialog context. In such cases, phrase-based SMT will usually translate the ambiguous source word in the sense that occurs most frequently in the training data, even if a different sense was intended.  sense-speciﬁc phrase pair clusters, and automated word sense prediction.  Table 1 illustrates this problem in a few conversational utterances for English-to-Iraqi Arabic  
The listwise approach to learning to rank has been applied successfully to information retrieval. However, it has not drawn much attention in research on the automatic evaluation of machine translation. In this paper, we present the listwise approach to learning to rank for the automatic evaluation of machine translation. Unlike previous automatic metrics that give absolute scores to translation outputs, our approach directly ranks the translation outputs relative to each other using features extracted from the translation outputs. Two representative listwise approaches, ListNet and ListMLE, are applied to automatic evaluation of machine translation. When evaluated using the dataset of the WMT 2012 Metrics task, the proposed approach achieves higher segment-level correlation with human judgments than the pairwise approach, RankNet, and with all the other metrics that were evaluated during the workshop, and it achieves honorably a comparable system-level correlation with the performance of most competitors. 
Multiword Expression (MWE) contributes to major lexical ambiguity problems for any language and poses a big challenge in statistical machine translation. This paper presents the role of MWEs in improving the performance of phrase based Statistical machine Translation (PB-SMT) system. We preprocess the parallel corpus by single tokenizing the MWEs on both sides which leads to significant improvement over baseline PBSMT system. Automatically aligned MWEs have been incorporated into PBSMT in two ways: indirectly, i.e., added as additional parallel training examples, and directly integrated into the word alignment model. Both the indirect and direct approaches bring some improvements in terms of system performance and the improvements are at par. For MWE alignment, we used baseline PBSMT systems trained on the same parallel corpus in both directions. String level edit distance is used for alignment validation. We bootstrap the whole procedure to get more MWE alignments. Integration of MWE alignment into PB-SMT achieves significant improvements (7.0 BLEU points absolute, 64.1% relative improvement) over the baseline, while bootstrapping with single iteration provides further improvement (9.24 BLEU points absolute, 84.7% relative improvement) in an English—Bengali translation task.  
Research on translation quality annotation and estimation usually makes use of standard language, sometimes related to a speciﬁc language genre or domain. However, real-life machine translation (MT) performed, for instance, by on-line translation services, has to cope with extra difﬁculties related to the usage of open, nonstandard and noisy language. In this paper we study the learning of quality estimation (QE) models able to rank translations from real-life input according to their goodness without the need of translation references. For that, we work with a corpus collected from the 24/7 Reverso.net MT service, translated by 5 different systems, and manually annotated with quality scores. We deﬁne several families of features and train QE predictors in the form of regressors or direct rankers. The predictors show a remarkable correlation with gold standard rankings and prove to be useful in a system combination scenario, obtaining better results than any individual translation system. 
Although good sentence aligners are freely available, our laboratory regularly receives requests from researchers and industries for aligning parallel data. This motivated us to release yet another open-source sentence aligner we wrote nearly 20 years ago. This aligner is simple but it performs surprisingly well and often better than more elaborated ones, and do so very fast, allowing to align very large corpora. We analyze the robustness of our aligner across different text genres and level of noise. We also revisit the alignment procedure with which the Europarl corpus has been prepared and show that better SMT performance can be obtained by simply using our aligner. 
We developed a simultaneous interpretation system for face-to-face services at shops, front desks, and so forth. The system supports interpretation between Japanese and English, or Japanese and Chinese speakers. It incrementally processes user’s continuous and spontaneous speech, and then incrementally produces interpretation results. We conducted a ﬁeld test of the system to evaluate the “solved task ratio” for tasks including buying souvenirs, asking a bus route. As a result, we achieved the solved task ratio of 81%. 
Even though the informal language of spoken text and web forum genres presents great difficulties for automatic semantic role labeling, we show that surprisingly, tuning statistical machine translation against the SRL-based objective function, MEANT, nevertheless leads more robustly to adequate translations of these informal genres than tuning against BLEU or TER. The accuracy of automatic semantic parsing has been shown to degrade significantly on informal genres such as speech or tweets, compared to formal genres like newswire. In spite of this, human evaluators preferred translations from MEANTtuned systems over the BLEU- or TERtuned ones by a significant margin. Error analysis indicates that one of the major sources of errors in automatic shallow semantic parsing of informal genres is failure to identify the semantic frame for copula or existential senses of “be”. We show that MEANT’s correlation with human adequacy judgment on informal text is improved by reconstructing the missing semantic frames for “be”. Our tuning approach is independent of the translation model architecture, so any SMT model can potentially benefit from the semantic knowledge incorporated through our approach. 
Supplementary data selection is a strongly motivated approach in domain adaptation of statistical machine translation systems. In this paper we report a novel approach of data selection guided by automatic quality estimation. In contrast to the conventional approach of using the entire target-domain data as reference for data selection, we restrict the reference set only to sentences poorly translated by the baseline model. Automatic quality estimation is used to identify such poorly translated sentences in the target domain. Our experiments reveal that this approach provides statistically signiﬁcant improvements over the unadapted baseline and achieves comparable scores to that of conventional data selection approaches with signiﬁcantly smaller amounts of selected data. 
We cast the problem of hip hop lyric generation as a translation problem, automatically learn a machine translation system that accepts hip hop lyric challenges and improvises rhyming responses, and show that improving the training data by learning an unsupervised rhyme detection scheme further improves performance. Our approach using unsupervised induction of stochastic transduction grammars is the first to apply the learning algorithms of SMT to the woefully under-explored genre of lyrics in music. A novel feature of our model is that it is completely unsupervised and does not make use of any a priori linguistic or phonetic information. Unlike the handful of previous approaches to modeling lyrics, we choose the domain of hip hop lyrics which is particularly noisy and unstructured. In order to cope with the noisy nature of the data in this domain, we compare the effect of two data selection schemes on the quality of the responses generated, and show the superiority of selection via a dedicated rhyme scheme detector that is also acquired through unsupervised learning. We also propose two strategies to mitigate the effect of disfluencies in the data which are common in the domain of hip hop lyrics, on the performance of our model. Despite the particularly noisy and unstructured nature of the domain, our model produces fluent and rhyming responses compared to a standard phrase based SMT baseline in human evaluations.  
Machine Translation (MT) is now often used to produce approximate translations that are then corrected by trained professional post-editors. As a result, more and more datasets of post-edited translations are being collected. These datasets are very useful for training, adapting or testing existing MT systems. In this work, we present the design and content of one such corpus of post-edited translations, and consider less studied possible uses of these data, notably the development of an automatic Quality Estimation (QE) system and the detection of frequent errors in automatic translations. Both applications require a careful assessment of the variability in post-editions, that we study here. 
The development of natural language processing tools for dialects faces the severe problem of lack of resources. In cases of diglossia, as in Arabic, one variant, Modern Standard Arabic (MSA), has many resources that can be used to build natural language processing tools. Whereas other variants, Arabic dialects, are resource poor. Taking advantage of the closeness of MSA and its dialects, one way to solve the problem of limited resources, consists in performing a translation of the dialect into MSA in order to use the tools developed for MSA. We describe in this paper an architecture for such a translation and we evaluate it on Tunisian Arabic verbs. Our approach relies on modeling the translation process over the deep morphological representations of roots and patterns, commonly used to model Semitic morphology. We compare different techniques for how to perform the cross-lingual mapping. Our evaluation demonstrates that the use of a decent coverage root+pattern lexicon of Tunisian and MSA with a backoff that assumes independence of mapping roots and patterns is optimal in reducing overall ambiguity and increasing recall. 
Diagnostic evaluation of machine translation (MT) is an approach to evaluation that provides ﬁner-grained information compared to state-of-the-art automatic metrics. This paper evaluates DELiC4MT, a diagnostic metric that assesses the performance of MT systems on user-deﬁned linguistic phenomena. We present the results obtained using this diagnostic metric when evaluating three MT systems that translate from English to French, with a comparison against both human judgements and a set of representative automatic evaluation metrics. In addition, as the diagnostic metric relies on word alignments, the paper compares the margin of error in diagnostic evaluation when using automatic word alignments as opposed to gold standard manual alignments. We observed that this diagnostic metric is capable of accurately reﬂecting translation quality, can be used reliably with automatic word alignments and, in general, correlates well with automatic metrics and, more importantly, with human judgements. 
This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the problem associated to polysemous words found in the seed bilingual lexicon when translating source context vectors. To improve the adequacy of context vectors, the use of a WordNetbased Word Sense Disambiguation process is tested. Experimental results on four specialized French-English comparable corpora show that our method outperforms two state-of-the-art approaches. 
One of the major components of Statistical Machine Translation (SMT) are generative translation models. As in other ﬁelds, where the transition from generative to discriminative training resulted in higher performance, it seems likely that translation models should be trained in a discriminative way. But due to the nature of SMT with large vocabularies, hidden alignments, reordering, and large training corpora, the application of discriminative methods is only feasible when using effective speed up techniques. We will show that translation models trained with Conditional Random Fields (CRFs) using classes are useful in translation, even in addition to a strong baseline. Results with an independent CRF translation system and n-best list rescoring will be presented. To design the tandem of CRF translation model and a phrase based baseline we will evaluate two different ways of n-best list integrations. 
In this work, we study the impact of the word order decoding direction for statistical machine translation (SMT). Both phrase-based and hierarchical phrasebased SMT systems are investigated by reversing the word order of the source and/or target language and comparing the translation results with the normal direction. Analysis are done on several components such as alignment model, language model and phrase table to see which of them accounts for the differences generated by various translation directions. Furthermore, we propose to use system combination, alignment combinations and phrase table combinations to take beneﬁt from systems trained with different translation directions. Experimental results show improvements of up to 1.7 points in BLEU and 3.1 points in TER compared to the normal direction systems for the NTCIR9 Japanese-English and Chinese-English tasks. 
We describe a systematic analysis on the effectiveness of features commonly exploited for the problem of predicting machine translation quality. Using a feature selection technique based on Gaussian Processes, we identify small subsets of features that perform well across many datasets for different language pairs, text domains, machine translation systems and quality labels. In addition, we show the potential of the reduced feature sets resulting from our feature selection technique to lead to signiﬁcantly better performance in most datasets, as compared to the complete feature sets. 
This paper presents a bidirectional machine translation system between Kazakh and Tatar, two Turkic languages. Background on the differences between the languages is presented, followed by how the system was designed to handle some of these differences. We provide an evaluation of the system’s performance and directions for future work. 
Linear mixture models are a simple and effective technique for performing domain adaptation of translation models in statistical MT. In this paper, we identify and correct two weaknesses of this method. First, we show that standard maximumlikelihood weights are biased toward large corpora, and that a straightforward preprocessing step that down-samples phrase tables can be used to counter this bias. Second, we show that features inspired by prototypical linear mixtures can be used to loosely simulate discriminative training for mixture models, with results that are almost certainly superior to true discriminative training. Taken together, these enhancements yield BLEU gains of approximately 1.5 over existing linear mixture techniques for translation model adaptation. 
Translators who work by post-editing machine translation output often ﬁnd themselves repeatedly correcting the same errors. We propose a method for Post-edit Propagation (PEPr), which learns posteditor corrections and applies them on-theﬂy to further MT output. Our proposal is based on a phrase-based SMT system, used in an automatic post-editing (APE) setting with online learning. Simulated experiments on a variety of data sets show that for documents with high levels of internal repetition, the proposed mechanism could substantially reduce the post-editing effort. 
The Translation & Localisation theme will address quality improvement in translation technologies coupled with new approaches to localisation that maximise the value of human linguistic judgements across the widest range of domains. 
All of the project objectives were met within the project schedule. Highlights are: 1) Develop ment of a web-based computing infrastructure for deploying university and commercial research systems at the http://labs.reverso.net website. All user interactions with the systems were logged and collected into a feedback database that was distributed within the project for modeling user interaction with statistical translation; 2) Detailed analyses of the types of feedback provided and the effectiveness of user interface designs to encourage users to interact with the system and to contribute useful feedback. A community of users of the project tools was established and nurtured by Softissimo; 3) Tools for robust syntactic annotation of the types of data likely to be submitted for translation were developed for use within the project and for public dissemination; 4) The Asiya open toolkit for automatic evaluation of MT systems (http://asiya.lsi.upc.edu), which provides an online graphical interface, an online search facility for translation and translation feedback, and a RESTful web service; 5) The development of novel approaches to incorporate deep and shallow natural language generation into SMT. Improvements in translation are found through better morphology and generation in the target language; and 6) Softissimo integrates technology developed in FAUST into its Reverso Localise product, and SDL is planning to use the feedback exploitation mechanisms developed in the context of FAUST primarily in professional post-editing environments, where data is produced in massive quantities and can be trusted.  
Internationalization Tag Set (ITS) 2.0 http://www.w3.org/TR/its20/ is a standard that has been developed within the World Wide Web Consortium (W3C). It provides metadata items (“data categories”) that ease the integration of natural language processing into core Web technologies. ITS 2.0 focuses on HTML, XML-based formats in general, and can leverage processing based on the XML Localization Interchange File Format (XLIFF), as well as the Natural Language Processing Interchange Format (NIF). 
Since 2012, two new projects have been enlarging the META community in specific areas. QTLaunchPad (http://www.qt21.eu/launchpad) is an EC-funded initiative dedicated to overcoming quality barriers in machine and human translation and in language technologies. It is preparing a large-scale translation quality initiative for Europe. One of the central outcomes of QTLaunchPad is a new, modular system for translation quality assessment called Multidimensional Quality Metrics (MQM). 
MOLTO was based on the idea of controlled natural languages (CNL), where an underlying semantimodel works as an interlingua and makes it possible to translate with high precision. The challenge of MOLTO was to scale up the CNL idea on several dimensions: 
The Monnet project is concerned mainly with ontology localisation, i.e., the translation of the lexico-terminological level of ontologies. The project outcomes can be described as a set of software components as follows, all of which can be used in combination as well as stand-alone: Ontology Lexicalisation and Localization, Crosslingual Ontology-based Information Extraction, Cross-lingual Knowledge Access & Presentation Ontology Lexicalisation The core objective of Monnet is the provision of advanced services for the translation of the lexico-terminological level of ontologies, which will be instantiated by the ”localization service”. However, as ontologies often have only a very limited representation of lexicoterminological information, a ﬁrst step will be to analyse a given ontology and enrich it with appropriate information on i) the terminological structure of ontology labels, ii) linguistic information on terminology items, and iii) analysis of implicit semantics where needed. Together we refer to these analysis and enrichment steps as ”ontology lexicalisation”, which will be instantiated by the ”lexicalisation service” that takes as input an ontology and outputs an ”ontology-lexicon” for at least one default language.1 A ”corpus service” enables access to external domain corpus evidence for modelling and analysing language use in the ontology labels. The ontology-lexicon is represented on the basis of the ”lemon” format,2 a lexicon model for ontologies that has been deﬁned by the Monnet project for the appropriate integration of lexical, linguistic and terminological information in ontologies. 
 European Union FP7 ICT-2011-7 Coordination and Support Action 288487 http://www.mosescore.eu 
The PANACEA project has focused on the development of a factory of LRs that automates the stages involved in the acquisition, production, updating and maintenance of LRs required by MT systems, and by other based on Language Technologies (LT) applications. This automation is meant to cut down costs significantly, in terms of time and human effort. Such reductions are the only way to guarantee a continuous supply of LRs that MT and other Language Technologies demand in a multilingual Europe. In order to address this objective, PANACEA has worked in (i) the development of a platform, designed as a dedicated factory for the composition of a number of LR production lines based on combinations of different web services and (ii) the integration of advanced components for the acquisition and normalization of corpora, monolingual and parallel corpora, their alignment; the derivation of bilingual dictionaries out of aligned corpora; and the production of monolingual rich information lexica using corpus based automatic methods. The PANACEA factory has been thoroughly evaluated within R&D and industrial settings. The platform and the LRs production lines based on advanced technological components have proved the feasibility of the concept. PANACEA’s contribution and potential impact has been demonstrated in an industrial evaluation carried out with the adaptation of Machine Translation products to a specific/specialized domain. In terms of effort, to produce a domain-adapted bilingual glossary of 1000 entries with PANACEA reduces costs from 30 person/hours to 0.5 person/hours. In terms of quality, there were no significant negative effects in the translation quality of the systems using automatically produced resources. A human evaluation showed that PANACEA domain-tuned SMT gained in quality up to a 6% with respect to the not tuned baseline, and that quality was not significantly worse than the achieved by other state-of-the-art systems as Google Translate. 
In order for PRESEMT to be easily amenable to new language pairs, only relatively inexpens ive, readily available language resources, in the form of monolingual corpora as well as bilin gual lexica, are used. Since for the majority of language pairs the amount of available parallel corpora is very limited, PRESEMT extracts modelling information from monolingual resources. Only a very small bilingual corpus is used to provide information for structural modifications from SL to TL. Translation context is modelled on syntactic phrases, as they have been proven to improve the translation quality. Phrases are produced via a semi-automatic and language-independent process of morphological and syntactic analysis, removing the need for compatible, in terms of output, NLP tools per language pair. So a flexible MT system has been developed, which is enhanced with pattern recognition techniques supporting the development of a language-independent analysis. To compare PRESEMT to other MT methodologies, both objective and subjective evaluations have been performed. All evaluation tasks have shown that PRESEMT has a lower yet comparable translation quality to that achieved by established MT systems (Google Translate, Bing Translator). This is expected as the proposed methodology by design avoids incorporating a priori grammatical knowledge and uses only inexpensive resources from which to extract knowledge as well as publicly available tools (such as parsers and taggers). The evalu ation outcome also reflects the much shorter development time available for PRESEMT. However, PRESEMT consistently generates translations which convey the intended meaning of input text. As such, PRESEMT can be considered successful on the basis of its design brief, namely generating translations suitable for gisting. Currently, work is continuing on a number of aspects so as to improve further the PRESEMT accuracy, with latest results indicating a dy namic for advancing the translation quality. 
Sign languages are the primary means of communication for most deaf and many hard-ofhearing persons. As only few hearing persons know how to sign, it is a serious challenge for the deaf community to integrate into educational, work and social environments. An automatic system translating from a sign language into a spoken language would ease some of these communication problems. The SignSpeak project developed a prototype of such a system. In the project, new annotated single-view video corpora for automatic sign language recognition and translation were created, taking linguistic knowledge (Radboud University Nijmegen, The Netherlands) as well as the requirements of computer vision (Technical University Innsbruck, Austria and CRIC, Barcelona, Spain), automatic sign recognition and translation (RWTH Aachen University, Germany) into account. Based on these corpora, new approaches for object tracking and feature extraction for sign languages were explored, and both the recognition and the translation framework were specifically tailored to the small-scale multimodal corpora at hand. The translation results of the whole SignSpeak pipeline were evaluated by deaf experts, guided by the European Union of the Deaf (Brussels, Belgium). Possible business products were assessed (Telefonica Research and Development, Spain), leading to a prototype video mail application. Altogether, the SignSpeak consortium succeeded in developing a prototype system to translate sign language videos into the text of a spoken language. 
 Processing of documents uploaded by users in different formats: DOC, DOCX, PDF, XMLbased formats like XLIFF, and others;  Automated extraction and retrieval of monolingual term candidates (from documents uploaded by users) using state-of-the-art linguistically and statistically motivated terminology extraction techniques; 
Xerox Research Centre Europe, France Jozef Stefan Institute and Knowledge for All Foundation, Slovenia and UK 
The TTC project leveraged machine translation (MT) systems, computer-assisted translation (CAT) tools and multilingual content (corpora and terminology) management tools by developing methods and tools that allow users to generate bilingual terminologies automatically from comparable (non-parallel) corpora in seven languages: five European languages (English, French, German, Spanish, Latvian) as well as Chinese and Russian, and twelve translation directions. The TTC project has developed generic methods and tools for the automatic extraction and alignment of terminologies, in order to break the lexical acquisition bottleneck in both statistical and rule-based MT. It has also developed and adapted tools for gathering and managing comparable corpora, collected from the web, and managing terminologies. In particular, a topical web crawler and the MyEuroTermBank open terminology platform have been developed. The key output of the project is the TTC web platform. It allows to create thematic corpora given some clues (such as terms or documents on a specific domain), to expand a given corpus, to create a comparable corpora from seeds in two languages, to choose the tools to apply for terminology extraction, to extract monolingual terminology from such corpora, to translate bilingual terminologies, and to export monolingual or bilingual terminologies in order to use them easily in automatic and semi-automatic translation tools. For generating bilingual terminologies automatically from comparable corpora innovative approaches have been researched, implemented and evaluated that constituted the specificities of the TTC approaches: (1) topical web crawling which will gather comparable corpora from domain-specific Web portals or using querybased crawling technologies with several types of conditional analysis; (2) for monolingual term extraction, different techniques, a knowledge-rich and a knowledge-poor approaches were followed; a massive use of morphological knowledge to handle morphologically complex lexical items; (3) for bilingual term extraction, an unified treatment for single word term and multi-word term was designed as well as an hybrid method that used both the internal structure and the context information of the term. Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 449. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. 
Karlsruher Institut für Technologie, Karlsruhe, Germany Universitat politecnica de Catalunya, Barcelona, Spain 
Sign language-to-text translation systems are similar to spoken language translation systems in that they consist of a recognition phase and a translation phase. First, the video of a person signing is transformed into a transcription of the signs, which is then translated into the text of a spoken language. One distinctive feature of sign languages is their multi-modal nature, as they can express meaning simultaneously via hand movements, body posture and facial expressions. In some sign languages, certain signs are accompanied by mouthings, i.e. the person silently pronounces the word while signing. In this work, we closely integrate a recognition and translation framework by adding a viseme recognizer ({``}lip reading system{''}) based on an active appearance model and by optimizing the recognition system to improve the translation output. The system outperforms the standard approach of separate recognition and translation.
In this paper, we introduce a new parallel corpus of subtitles of educational videos: the AMARA corpus for online educational content. We crawl a multilingual collection community generated subtitles, and present the results of processing the Arabic{--}English portion of the data, which yields a parallel corpus of about 2.6M Arabic and 3.9M English words. We explore different approaches to align the segments, and extrinsically evaluate the resulting parallel corpus on the standard TED-talks tst-2010. We observe that the data can be successfully used for this task, and also observe an absolute improvement of 1.6 BLEU when it is used in combination with TED data. Finally, we analyze some of the specific challenges when translating the educational content.
There has been a fair amount of work on automatic speech translation systems that translate in real-time, serving as a computerized version of a simultaneous interpreter. It has been noticed in the field of translation studies that simultaneous interpreters perform a number of tricks to make the content easier to understand in real-time, including dividing their translations into small chunks, or summarizing less important content. However, the majority of previous work has not specifically considered this fact, simply using translation data (made by translators) for learning of the machine translation system. In this paper, we examine the possibilities of additionally incorporating simultaneous interpretation data (made by simultaneous interpreters) in the learning process. First we collect simultaneous interpretation data from professional simultaneous interpreters of three levels, and perform an analysis of the data. Next, we incorporate the simultaneous interpretation data in the learning of the machine translation system. As a result, the translation style of the system becomes more similar to that of a highly experienced simultaneous interpreter. We also find that according to automatic evaluation metrics, our system achieves performance similar to that of a simultaneous interpreter that has 1 year of experience.
We investigate the problem of combining the outputs of different translation systems into a minimum Bayes{'} risk consensus translation. We explore different risk formulations based on the BLEU score, and provide a dynamic programming decoding algorithm for each of them. In our experiments, these algorithms generated consensus translations with better risk, and more efficiently, than previous proposals.
We present a method to estimate the quality of automatic translations when reference translations are not available. Quality estimation is addressed as a two-step regression problem where multiple features are combined to predict a quality score. Given a set of features, we aim at automatically extracting the variables that better explain translation quality, and use them to predict the quality score. The soundness of our approach is assessed by the encouraging results obtained in an exhaustive experimentation with several feature sets. Moreover, the studied approach is highly-scalable allowing us to employ hundreds of features to predict translation quality.
This paper describes our Speech-to-Text (STT) system for French, which was developed as part of our efforts in the Quaero program for the 2013 evaluation. Our STT system consists of six subsystems which were created by combining multiple complementary sources of pronunciation modeling including graphemes with various feature front-ends based on deep neural networks and tonal features. Both speaker-independent and speaker adaptively trained versions of the systems were built. The resulting systems were then combined via confusion network combination and crossadaptation. Through progressive advances and system combination we reach a word error rate (WER) of 16.5{\%} on the 2012 Quaero evaluation data.
In this article, we present a sampling-based approach to improve bilingual sub-sentential alignment in parallel corpora. This approach can be used to align parallel sentences on an as needed basis, and is able to accurately align newly available sentences. We evaluate the resulting alignments on several Machine Translation tasks. Results show that for the tasks considered here, our approach performs on par with the state-of-the-art statistical alignment pipeline giza++/Moses, and obtains superior results in a number of configurations, notably when aligning additional parallel sentence pairs carefully selected to match the test input.
In this paper we describe our work on unsupervised adaptation of the acoustic model of our simultaneous lecture translation system. We trained a speaker independent acoustic model, with which we produce automatic transcriptions of new lectures in order to improve the system for a specific lecturer. We compare our results against a model that was trained in a supervised way on an exact manual transcription. We examine four different ways of processing the decoder outputs of the automatic transcription with respect to the treatment of pronunciation variants and noise words. We will show that, instead of fixating the latter informations in the transcriptions, it is of advantage to let the Viterbi algorithm during training decide which pronunciations to use and where to insert which noise words. Further, we utilize word level posterior probabilities obtained during decoding by weighting and thresholding the words of a transcription.
Current ASR and MT systems do not operate on conversational Finnish, because training data for colloquial Finnish has not been available. Although speech recognition performance on literary Finnish is already quite good, those systems have very poor baseline performance in conversational speech. Text data for relevant vocabulary and language models can be collected from the Internet, but web data is very noisy and most of it is not helpful for learning good models. Finnish language is highly agglutinative, and written phonetically. Even phonetic reductions and sandhi are often written down in informal discussions. This increases vocabulary size dramatically and causes word-based selection methods to fail. Our selection method explicitly optimizes the perplexity of a subword language model on the development data, and requires only very limited amount of speech transcripts as development data. The language models have been evaluated for speech recognition using a new data set consisting of generic colloquial Finnish.
The ability to quickly incorporate incoming training data into a running translation system is critical in a number of applications. Mechanisms based on incremental model update and the online EM algorithm hold the promise of achieving this objective in a principled way. Still, efficient tools for incremental training are yet to be available. In this paper we experiment with simple alternative solutions for interim model updates, within the popular Moses system. Short of updating the model in real time, such updates can execute in short timeframes even when operating on large models, and achieve a performance level close to, and in some cases exceeding, that of batch retraining.
We analyze the performance of source sentence reordering, a common reordering approach, using oracle experiments on German-English and English-German translation. First, we show that the potential of this approach is very promising. Compared to a monotone translation, the optimally reordered source sentence leads to improvements of up to 4.6 and 6.2 BLEU points, depending on the language. Furthermore, we perform a detailed evaluation of the different aspects of the approach. We analyze the impact of the restriction of the search space by reordering lattices and we can show that using more complex rule types for reordering results in better approximation of the optimally reordered source. However, a gap of about 3 to 3.8 BLEU points remains, presenting a promising perspective for research on extending the search space through better reordering rules. When evaluating the ranking of different reordering variants, the results reveal that the search for the best path in the lattice performs very well for German-English translation. For English-German translation there is potential for an improvement of up to 1.4 BLEU points through a better ranking of the different reordering possibilities in the reordering lattice.
Disfluencies in speech pose severe difficulties in machine translation of spontaneous speech. This paper presents our conditional random field (CRF)-based speech disfluency detection system developed on German to improve spoken language translation performance. In order to detect speech disfluencies considering syntactics and semantics of speech utterances, we carried out a CRF-based approach using information learned from the word representation and the phrase table used for machine translation. The word representation is gained using recurrent neural networks and projected words are clustered using the k-means algorithm. Using the output from the model trained with the word representations and phrase table information, we achieve an improvement of 1.96 BLEU points on the lecture test set. By keeping or removing humanannotated disfluencies, we show an upper bound and lower bound of translation quality. In an oracle experiment we gain 3.16 BLEU points of improvement on the lecture test set, compared to the same set with all disfluencies.
Russian is a challenging language for automatic speech recognition systems due to its rich morphology. This rich morphology stems from Russian{'}s highly inflectional nature and the frequent use of preand suffixes. Also, Russian has a very free word order, changes in which are used to reflect connotations of the sentences. Dealing with these phenomena is rather difficult for traditional n-gram models. We therefore investigate in this paper the use of a maximum entropy language model for Russian whose features are specifically designed to deal with the inflections in Russian, as well as the loose word order. We combine this with a subword based language model in order to alleviate the problem of large vocabulary sizes necessary for dealing with highly inflecting languages. Applying the maximum entropy language model during re-scoring improves the word error rate of our recognition system by 1.2{\%} absolute, while the use of the sub-word based language model reduces the vocabulary size from 120k to 40k and the OOV rate from 4.8{\%} to 2.1{\%}.
Research into the translation of the output of automatic speech recognition (ASR) systems is hindered by the dearth of datasets developed for that explicit purpose. For SpanishEnglish translation, in particular, most parallel data available exists only in vastly different domains and registers. In order to support research on cross-lingual speech applications, we introduce the Fisher and Callhome Spanish-English Speech Translation Corpus, supplementing existing LDC audio and transcripts with (a) ASR 1-best, lattice, and oracle output produced by the Kaldi recognition system and (b) English translations obtained on Amazon{'}s Mechanical Turk. The result is a four-way parallel dataset of Spanish audio, transcriptions, ASR lattices, and English translations of approximately 38 hours of speech, with defined training, development, and held-out test sets. We conduct baseline machine translation experiments using models trained on the provided training data, and validate the dataset by corroborating a number of known results in the field, including the utility of in-domain (information, conversational) training data, increased performance translating lattices (instead of recognizer 1-best output), and the relationship between word error rate and BLEU score.
We present the first known experiments incorporating unsupervised bilingual nonterminal category learning within end-to-end fully unsupervised transduction grammar induction using matched training and testing models. Despite steady recent progress, such induction experiments until now have not allowed for learning differentiated nonterminal categories. We divide the learning into two stages: (1) a bootstrap stage that generates a large set of categorized short transduction rule hypotheses, and (2) a minimum conditional description length stage that simultaneously prunes away less useful short rule hypotheses, while also iteratively segmenting full sentence pairs into useful longer categorized transduction rules. We show that the second stage works better when the rule hypotheses have categories than when they do not, and that the proposed conditional description length approach combines the rules hypothesized by the two stages better than a mixture model does. We also show that the compact model learned during the second stage can be further improved by combining the result of different iterations in a mixture model. In total, we see a jump in BLEU score, from 17.53 for a standalone minimum description length baseline with no category learning, to 20.93 when incorporating category induction on a Chinese{--}English translation task.
This paper describes a study of translation hypotheses that can be obtained by iterative, greedy oracle improvement from the best hypothesis of a state-of-the-art phrase-based Statistical Machine Translation system. The factors that we consider include the influence of the rewriting operations, target languages, and training data sizes. Analysis of our results provide new insights into some previously unanswered questions, which include the reachability of previously unreachable hypotheses via indirect translation (thanks to the introduction of a rewrite operation on the source text), and the potential translation performance of systems relying on pruned phrase tables.
Spoken language translation (SLT) systems typically follow a pipeline architecture, in which the best automatic speech recognition (ASR) hypothesis of an input utterance is fed into a statistical machine translation (SMT) system. Conversational speech often generates unrecoverable ASR errors owing to its rich vocabulary (e.g. out-of-vocabulary (OOV) named entities). In this paper, we study the possibility of alleviating the impact of unrecoverable ASR errors on translation performance by minimizing the contextual effects of incorrect source words in target hypotheses. Our approach is driven by locally-derived penalties applied to bilingual phrase pairs as well as target language model (LM) likelihoods in the vicinity of source errors. With oracle word error labels on an OOV word-rich English-to-Iraqi Arabic translation task, we show statistically significant relative improvements of 3.2{\%} BLEU and 2.0{\%} METEOR over an error-agnostic baseline SMT system. We then investigate the impact of imperfect source error labels on error-aware translation performance. Simulation experiments reveal that modest translation improvements are to be gained with this approach even when the source error labels are noisy.
This paper focuses on the user experience (UX) of a simultaneous interpretation system for face-to-face conversation between two users. To assess the UX of the system, we first made a transcript of the speech of users recorded during a task-based evaluation experiment and then analyzed user speech from the viewpoint of UX. In a task-based evaluation experiment, 44 tasks out of 45 tasks were solved. The solved task ratio was 97.8{\%}. This indicates that the system can effectively provide interpretation to enable users to solve tasks. However, we found that users repeated speech due to errors in automatic speech recognition (ASR) or machine translation (MT). Users repeated clauses 1.8 times on average. Users seemed to repeat themselves until they received a response from their partner users. In addition, we found that after approximately 3.6 repetitions, users would change their words to avoid errors in ASR or MT and to evoke a response from their partner users.
In this paper, we apply a set of approaches to, efficiently, rescore the output of the automatic speech recognition over weather-domain data. Since the in-domain data is usually insufficient for training an accurate language model (LM) we utilize an automatic selection method to extract domain-related sentences from a general text resource. Then, an N-gram language model is trained on this set. We exploit this LM, along with a pre-trained acoustic model for recognition of the development and test instances. The recognizer generates a confusion network (CN) for each instance. Afterwards, we make use of the recurrent neural network language model (RNNLM), trained on the in-domain data, in order to iteratively rescore the CNs. Rescoring the CNs, in this way, requires estimating the weights of the RNNLM, N-gramLM and acoustic model scores. Weights optimization is the critical part of this work, whereby, we propose using the minimum error rate training (MERT) algorithm along with a novel N-best list extraction method. The experiments are done over weather forecast domain data that has been provided in the framework of EUBRIDGE project.
The paper overviews the tenth evaluation campaign organized by the IWSLT workshop. The 2013 evaluation offered multiple tracks on lecture transcription and translation based on the TED Talks corpus. In particular, this year IWSLT included two automatic speech recognition tracks, on English and German, three speech translation tracks, from English to French, English to German, and German to English, and three text translation track, also from English to French, English to German, and German to English. In addition to the official tracks, speech and text translation optional tracks were offered involving 12 other languages: Arabic, Spanish, Portuguese (B), Italian, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, Russian. Overall, 18 teams participated in the evaluation for a total of 217 primary runs submitted. All runs were evaluated with objective metrics on a current test set and two progress test sets, in order to compare the progresses against systems of the previous years. In addition, submissions of one of the official machine translation tracks were also evaluated with human post-editing.
We present the results of large-scale human semantic MT evaluation with HMEANT on the IWSLT 2013 German-English MT and SLT tracks and show that HMEANT evaluates the performance of the MT systems differently compared to BLEU and TER. Together with the references, all the translations are annotated by annotators who are native English speakers in both semantic role labeling stage and role filler alignment stage of HMEANT. We obtain high inter-annotator agreement and low annotation time costs which indicate that it is feasible to run a large-scale human semantic MT evaluation campaign using HMEANT. Our results also show that HMEANT is a robust and reliable semantic MT evaluation metric for running large-scale evaluation campaigns as it is inexpensive and simple while maintaining the semantic representational transparency to provide a perspective which is different from BLEU and TER in order to understand the performance of the state-of-the-art MT systems.
This paper gives a description of the University of Edinburgh{'}s (UEDIN) systems for IWSLT 2013. We participated in all the MT tracks and the German-to-English and Englishto-French SLT tracks. Our SLT submissions experimented with including ASR uncertainty into the decoding process via confusion networks, and looked at different ways of punctuating ASR output. Our MT submissions are mainly based on a system used in the recent evaluation campaign at the Workshop on Statistical Machine Translation [1]. We additionally explored the use of generalized representations (Brown clusters, POS and morphological tags) translating out of English into European languages.
This paper describes the systems used for the MSR+FBK submission for the SLT track of IWSLT 2013. Starting from a baseline system we made a series of iterative and additive improvements, including a novel method for processing bilingual data used to train MT systems for use on ASR output. Our primary submission is a system combination of five individual systems, combining the output of multiple ASR engines with multiple MT techniques. There are two contrastive submissions to help place the combined system in context. We describe the systems used and present results on the test sets.
We present the first ever results showing that Chinese MT output is significantly improved by tuning a MT system against a semantic frame based objective function, MEANT, rather than an n-gram based objective function, BLEU, as measured across commonly used metrics and different test sets. Recent work showed that by preserving the meaning of the translations as captured by semantic frames in the training process, MT systems for translating into English on both formal and informal genres are constrained to produce more adequate translations by making more accurate choices on lexical output and reordering rules. In this paper we describe our experiments in IWSLT 2013 TED talk MT tasks on tuning MT systems against MEANT for translating into Chinese and English respectively. We show that the Chinese translation output benefits more from tuning a MT system against MEANT than the English translation output due to the ambiguous nature of word boundaries in Chinese. Our encouraging results show that using MEANT is a promising alternative to BLEU in both evaluating and tuning MT systems to drive the progress of MT research across different languages.
This study presents the NICT automatic speech recognition (ASR) system submitted for the IWSLT 2013 ASR evaluation. We apply two types of acoustic features and three types of acoustic models to the NICT ASR system. Our system is comprised of six subsystems with different acoustic features and models. This study reports the individual results and fusion of systems and highlights the improvements made by our proposed methods that include the automatic segmentation of audio data, language model adaptation, speaker adaptive training of deep neural network models, and the NICT SprinTra decoder. Our experimental results indicated that our proposed methods offer good performance improvements on lecture speech recognition tasks. Our results denoted a 13.5{\%} word error rate on the IWSLT 2013 ASR English test data set.
This paper reports on the participation of FBK at the IWSLT2013 evaluation campaign on automatic speech recognition (ASR): precisely on both English and German ASR track. Only primary submissions have been sent for evaluation. For English, the ASR system features acoustic models trained on a portion of the TED talk recordings that was automatically selected according to the fidelity of the provided transcriptions. Two decoding steps are performed interleaved by acoustic feature normalization and acoustic model adaptation. A final step combines the outputs obtained after having rescored the word graphs generated in the second decoding step with 4 different language models. The latter are trained on: out-of-domain text data, in-domain data and several sets of automatically selected data. For German, acoustic models have been trained on automatically selected portions of a broadcast news corpus, called {''}Euronews{''}. Differently from English, in this case only two decoding steps are carried out without making use of any rescoring procedure.
We describe the Arabic-English and English-Arabic statistical machine translation systems developed by the Qatar Computing Research Institute for the IWSLT{'}2013 evaluation campaign on spoken language translation. We used one phrase-based and two hierarchical decoders, exploring various settings thereof. We further experimented with three domain adaptation methods, and with various Arabic word segmentation schemes. Combining the output of several systems yielded a gain of up to 3.4 BLEU points over the baseline. Here we also describe a specialized normalization scheme for evaluating Arabic output, which was adopted for the IWSLT{'}2013 evaluation campaign.
We participated in the IWSLT 2013 Evaluation Campaign for the MT track for two official directions: German↔English. Our system consisted of a reordering module and a statistical machine translation (SMT) module under a pre-ordering SMT framework. We trained the reordering module using three scalable methods in order to utilize training instances as many as possible. The translation quality of our primary submissions were comparable to that of a hierarchical phrasebased SMT, which usually requires a longer time to decode.
This work describes the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign International Workshop on Spoken Language Translation (IWSLT) 2013. We participated in the English→French, English↔German, Arabic→English, Chinese→English and Slovenian↔English MT tracks and the English→French and English→German SLT tracks. We apply phrase-based and hierarchical SMT decoders, which are augmented by state-of-the-art extensions. The novel techniques we experimentally evaluate include discriminative phrase training, a continuous space language model, a hierarchical reordering model, a word class language model, domain adaptation via data selection and system combination of standard and reverse order models. By application of these methods we can show considerable improvements over the respective baseline systems.
In this paper we describe the ASR system for German built at the University of Edinburgh (UEDIN) for the 2013 IWSLT evaluation campaign. For ASR, the major challenge to overcome, was to find suitable acoustic training data. Due to the lack of expertly transcribed acoustic speech data for German, acoustic model training had to be performed on publicly available data crawled from the internet. For evaluation, lack of a manual segmentation into utterances was handled in two different ways: by generating an automatic segmentation, and by treating entire input files as a single segment. Demonstrating the latter method is superior in the current task, we obtained a WER of 28.16{\%} on the dev set and 36.21{\%} on the test set.
This paper presents NTT-NAIST SMT systems for English-German and German-English MT tasks of the IWSLT 2013 evaluation campaign. The systems are based on generalized minimum Bayes risk system combination of three SMT systems: forest-to-string, hierarchical phrase-based, phrasebased with pre-ordering. Individual SMT systems include data selection for domain adaptation, rescoring using recurrent neural net language models, interpolated language models, and compound word splitting (only for German-English).
This paper describes our English Speech-to-Text (STT) systems for the 2013 IWSLT TED ASR track. The systems consist of multiple subsystems that are combinations of different front-ends, e.g. MVDR-MFCC based and lMel based ones, GMM and NN acoustic models and different phone sets. The outputs of the subsystems are combined via confusion network combination. Decoding is done in two stages, where the systems of the second stage are adapted in an unsupervised manner on the combination of the first stage outputs using VTLN, MLLR, and cMLLR.
This research explores the effects of various training settings from Polish to English Statistical Machine Translation system for spoken language. Various elements of the TED parallel text corpora for the IWSLT 2013 evaluation campaign were used as the basis for training of language models, and for development, tuning and testing of the translation system. The BLEU, NIST, METEOR and TER metrics were used to evaluate the effects of data preparations on translation results. Our experiments included systems, which use stems and morphological information on Polish words. We also conducted a deep analysis of provided Polish data as preparatory work for the automatic data correction and cleaning phase.
In this paper, German and English large vocabulary continuous speech recognition (LVCSR) systems developed by the RWTH Aachen University for the IWSLT-2013 evaluation campaign are presented. Good improvements are obtained with state-of-the-art monolingual and multilingual bottleneck features. In addition, an open vocabulary approach using morphemic sub-lexical units is investigated along with the language model adaptation for the German LVCSR. For both the languages, competitive WERs are achieved using system combination.
EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes one of the collaborative efforts within EUBRIDGE to further advance the state of the art in machine translation between two European language pairs, English→French and German→English. Four research institutions involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the machine translation track of the evaluation campaign at the 2013 International Workshop on Spoken Language Translation (IWSLT). We present the methods and techniques to achieve high translation quality for text translation of talks which are applied at RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show how we have been able to considerably boost translation performance (as measured in terms of the metrics BLEU and TER) by means of system combination. The joint setups yield empirical gains of up to 1.4 points in BLEU and 2.8 points in TER on the IWSLT test sets compared to the best single systems.
This paper describes the MIT-LL/AFRL statistical MT system and the improvements that were developed during the IWSLT 2013 evaluation campaign [1]. As part of these efforts, we experimented with a number of extensions to the standard phrase-based model that improve performance on the Russian to English, Chinese to English, Arabic to English, and English to French TED-talk translation task. We also applied our existing ASR system to the TED-talk lecture ASR task. We discuss the architecture of the MIT-LL/AFRL MT system, improvements over our 2012 system, and experiments we ran during the IWSLT-2013 evaluation. Specifically, we focus on 1) cross-entropy filtering of MT training data, and 2) improved optimization techniques, 3) language modeling, and 4) approximation of out-of-vocabulary words.
This paper describes the Automatic Speech Recognition (ASR) and Machine Translation (MT) systems developed by IOIT for the evaluation campaign of IWSLT2013. For the ASR task, using Kaldi toolkit, we developed the system based on weighted finite state transducer. The system is constructed by applying several techniques, notably, subspace Gaussian mixture models, speaker adaptation, discriminative training, system combination and SOUL, a neural network language model. The techniques used for automatic segmentation are also clarified. Besides, we compared different types of SOUL models in order to study the impact of words of previous sentences in predicting words in language modeling. For the MT task, the baseline system was built based on the open source toolkit N-code, then being augmented by using SOUL on top, i.e., in N-best rescoring phase.
This paper describes the TU ̈ B ̇ITAK Turkish-English submissions in both directions for the IWSLT{'}13 Evaluation Campaign TED Machine Translation (MT) track. We develop both phrase-based and hierarchical phrase-based statistical machine translation (SMT) systems based on Turkish wordand morpheme-level representations. We augment training data with content words extracted from itself and experiment with reverse word order for source languages. For the Turkish-to-English direction, we use Gigaword corpus as an additional language model with the training data. For the English-to-Turkish direction, we implemented a wide coverage Turkish word generator to generate words from the stem and morpheme sequences. Finally, we perform system combination of the different systems produced with different word alignments.
This paper describes the systems submitted by FBK for the MT track of IWSLT 2013. We participated in the English-French as well as the bidirectional Persian-English translation tasks. We report substantial improvements in our English-French systems over last year{'}s baselines, largely due to improved techniques of combining translation and language models. For our Persian-English and English-Persian systems, we observe substantive improvements over baselines submitted by the workshop organizers, due to enhanced language-specific text normalization and the creation of a large monolingual news corpus in Persian.
We present our systems for the machine translation evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2013. We submitted systems for three language directions: German-to-English, Russian-to-English and English-to-Russian. The focus of our approaches lies on effective usage of the in-domain parallel training data. Therefore, we use the training data to tune parameter weights for millions of sparse lexicalized features using efficient parallelized stochastic learning techniques. For German-to-English we incorporate syntax features. We combine all of our systems with large language models. For the systems involving Russian we also incorporate more data into building of the translation models.
This paper describes the University of Edinburgh (UEDIN) English ASR system for the IWSLT 2013 Evaluation. Notable features of the system include deep neural network acoustic models in both tandem and hybrid configuration, cross-domain adaptation with multi-level adaptive networks, and the use of a recurrent neural network language model. Improvements to our system since the 2012 evaluation {--} which include the use of a significantly improved n-gram language model {--} result in a 19{\%} relative WER reduction on the tst2012 set.
This paper describes the NAIST English speech recognition system for the IWSLT 2013 Evaluation Campaign. In particular, we participated in the ASR track of the IWSLT TED task. Last year, we participated in collaboration with Karlsruhe Institute of Technology (KIT). This year is our first time to build a full-fledged ASR system for IWSLT solely developed by NAIST. Our final system utilizes weighted finitestate transducers with four-gram language models. The hypothesis selection is based on the principle of system combination. On the IWSLT official test set our system introduced in this work achieves a WER of 9.1{\%} for tst2011, 10.0{\%} for tst2012, and 16.2{\%} for the new tst2013.
In this paper, we present the KIT systems participating in all three official directions, namely English→German, German→English, and English→French, in translation tasks of the IWSLT 2013 machine translation evaluation. Additionally, we present the results for our submissions to the optional directions English→Chinese and English→Arabic. We used phrase-based translation systems to generate the translations. This year, we focused on adapting the systems towards ASR input. Furthermore, we investigated different reordering models as well as an extended discriminative word lexicon. Finally, we added a data selection approach for domain adaptation.
In this paper, we describe the CASIA statistical machine translation (SMT) system for the IWSLT2013 Evaluation Campaign. We participated in the Chinese-English and English-Chinese translation tasks. For both of these tasks, we used a hierarchical phrase-based (HPB) decoder and made it as our baseline translation system. A number of techniques were proposed to deal with these translation tasks, including parallel sentence extraction, pre-processing, translation model (TM) optimization, language model (LM) interpolation, turning, and post-processing. With these techniques, the translation results were significantly improved compared with that of the baseline system.
