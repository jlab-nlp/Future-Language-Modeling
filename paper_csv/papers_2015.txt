Dialog systems are generally categorized into two types: task oriented and non task oriented systems. Recently, the study of non task oriented dialog systems or chat systems becomes more important since robotic pets or nursing care robots are paid much attention in our daily life. In this paper, as a fundamental technique in a chat system, we propose a method to identify if a speaker displays sympathy in his/her utterance. Our method is based on supervised machine learning. New features are proposed to train a classiﬁer for identifying the sympathy in user’s utterance. Results of our experiments show that the proposed features improve the F-measure by 3-4% over a baseline. 
The information explosion in modern days across various media calls for effective opinion mining for timely digestion of public views and appropriate follow-up actions. Current studies on sentiment analysis have primarily focused on uncovering aspects like subjectivity, sentiment and credibility from written data, while spoken data are less addressed. This paper reports on our pilot work on constructing a corpus of Cantonese verbal comments and making use of multidimensional analysis to characterise different opinion types therein. Preliminary findings on the dimensions identified and their association with various communicative functions are presented, with an outlook on their potential application in subjectivity analysis and opinion classification. 
 Recent researches on EL(Entity Linking) have attempted to disambiguate entities by using a knowledge base to handle the semantic relatedness and up-to-date information. However, EL for tweets using a knowledge base, leads to poor disambiguation performance, because the data tend to address short and noisy contexts and current issues that are updated in real time. In this paper, we propose an approach to building an EL system that links ambiguous entities to the corresponding entries in a given knowledge base through the news articles and the user history. Using news articles, the system can overcome the problem of Wikipedia coverage, which does not handle issues in real time. In addition, because we assume that users post tweets related to their particular interests, our system can also be effectively applied to short tweet data through the user history. The experimental results show that our system achieves a precision of 67.7% and outperforms the EL methods that only use knowledge base for tweets. 
With the development of social media and online forums, users have grown accustomed to expressing their agreement and disagreement via short texts. Elements that reveal the user’s stance or subjectivity thus becomes an important resource in identifying the user’s position on a given topic. In the current study, we observe comments of an online bulletin board in Taiwan for how people express their stance when responding to other people’s post in Chinese. A lexicon is built based on linguistic analysis and annotation of the data. We performed binary classification task using these linguistic features and was able to reach an average of 71 percent accuracy. A linguistic analysis on the confusion caused in the classification task is done for future work on better accuracy for such task. 
We introduce a new method for extracting representative sentential patterns from a corpus for the purpose of assisting ESL learners in academic writing. In our approach, sentences are transformed into patterns for statistical analysis and ﬁltering, and then are annotated with relevant rhetoric moves. The method involves annotating every sentence in a given corpus with part of speech and base phrase information, converting the sentence into formulaic patterns, and ﬁltering salient patterns for key content words (verbs and nouns). We display the patterns in the interactive writing environment, WriteAhead, to prompt the user as they type away. 
Contrary to popular beliefs, idioms show a high degree of formal flexibility, ranging from word-like idioms to those which are like almost regular phrases. However, we argue that their meanings are not transparent, i.e. they are non-compositional, regardless of their syntactic flexibility. In this paper, firstly, we will introduce a framework to represent their syntactic flexibility, which is developed in Chae (2014), and will observe some consequences of the framework on the lexicon and the set of rules. Secondly, there seem to be some phenomena which can only be handled under the assumption that the component parts of idioms have their own separate meanings. However, we will show that all the phenomena, focusing on the behavior of idiom-internal adjectives, can be accounted for effectively without assuming separate meanings of parts, which confirms the non-transparency of idioms. 
This paper is an investigation of LF-copies created by scrambling in the context of FNQ-constructions. It demonstrates that movement leaves a copy at LF only when it targets a position within the next search space; it does not leave an LF copy if movement takes place too close within a single domain of search space. By characterizing this in terms of “Distinctness of Copies,” this paper provides a principled account to all structural variations that have posed substantial problems in previous approaches. 
We extract the book reviews on picture books written on the Web site specialized in picture books, and found that those reviews reﬂect infants’ behavioral expressions as well as their parents’ reading activities in detail. Analysis of the reviews reveals that infants’ reactions written on the reviews are coincident with the ﬁndings of developmental psychology concerning infants’ behaviors. In order to examine how the stimuli of picture books induces varieties of infants’ reactions, this paper proposes to detect an infant’s developmental reactions in reviews on picture books and shows effectiveness of the proposed method through experimental evaluation. 
We are developing a multilingual machine translation system to provide foreign tourists with a multilingual speech translation service in the Winter Olympic Games that will be held in Korea in 2018. For a knowledge learning to make the multilingual expansibility possible, we needed large bilingual corpus. In Korea there were a lot of Korean-English bilingual corpus, but Korean-French bilingual corpus and Korean-Spanish bilingual corpus lacked absolutely. Korean-English-French and KoreanEnglish-Spanish triangle corpus were constructed by crowdsourcing translation using the existing large Korean-English corpus. But we found a lot of translation errors from the triangle corpora. This paper aims at filtering of translation errors in large triangle corpus constructed by crowdsourcing translation to reduce the translation loss of triangle corpus with English as a pivot language. Experiment shows that our method improves +0.34 BLEU points over the baseline system. 
Syntax-based machine translation (MT) is an attractive approach for introducing additional linguistic knowledge in corpus-based MT. Previous studies have shown that treeto-string and string-to-tree translation models perform better than tree-to-tree translation models since tree-to-tree models require two high quality parsers on the source as well as the target language side. In practice, high quality parsers for both languages are difﬁcult to obtain and thus limit the translation quality. In this paper, we explore a method to transfer parse trees from the language side which has a high quality parser to the side which has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately. 
 The conventional Mongolian-Chinese sta-  tistical machine translation (SMT) model  uses Mongolian words and Chinese words  to practice the system. However, data  sparsity, complex Mongolian morphology  and Chinese word segmentation (CWS) er-  rors  lead  to  alignment  errors and ambiguities. Some other works  use finer-grained Mongolian stems and  Chinese characters, which suffer from in-  formation loss when inducting translation  rules. To tackle this, we proposed a meth-  od of using finer-grained Mongolian stems  and Chinese characters for word alignment,  but coarser-grained Mongolian words and  Chinese words for translation rule induc-  tion (TRI) and decoding. We presented a  heuristic technique to transform Chinese  character-based alignment to word-based  alignment. Experimentally, our method  outperformed the baselines: fully finer-  grained and fully coarser-grained, in terms  of alignment quality and translation per-  formance.  
Gospels are one type of translated historical document. There are many versions of the same Gospel that have been translated from the original, or from another Gospel that has already been translated into a different language. Nowadays, it is difﬁcult to determine the language of the original Gospel from where these Gospels were translated. In this paper we use a supervised machine learning technique to determine the origin of a version of the Georgian Gospel. 
In this paper, we study the impact of using a domain-specific bilingual lexicon on the performance of an Example-Based Machine Translation system. We conducted experiments for the EnglishFrench language pair on in-domain texts from Europarl (European Parliament Proceedings) and out-of-domain texts from Emea (European Medicines Agency Documents), and we compared the results of the Example-Based Machine Translation system against those of the Statistical Machine Translation system Moses. The obtained results revealed that adding a domain-specific bilingual lexicon (extracted from a parallel domain-specific corpus) to the general-purpose bilingual lexicon of the Example-Based Machine Translation system improves translation quality for both in-domain as well as outof-domain texts, and the Example-Based Machine Translation system outperforms Moses when texts to translate are related to the specific domain. 
This paper investigated Particle Movement in Korean EFL learners’ writings. Gries (1999, 2001, 2003) adopted a multifactorial analysis to examine Particle Movement of native speakers. Several linguistics factors were proposed in the studies, and it was demonstrated that these factors influenced the choice of the constructions. This paper also employed a multifactorial analysis to examine Particle Movement in Korean EFL learners’ writings. The analysis results illustrated that the Korean EFL learners were slightly different from native speakers in that only some factors were used for the selection of constructions. 
In this paper, we present an efﬁcient semiautomatic method for annotating English phrasal verbs on the OntoNotes corpus. Our method ﬁrst constructs a phrasal verb dictionary based on Wiktionary, then annotates each candidate example on the corpus as an either a phrasal verb usage or a literal one. For efﬁcient annotation, we use the dependency structure of a sentence to ﬁlter out highly plausible positive and negative cases, resulting in a drastic reduction of annotation cost. We also show that a naive binary classiﬁcation achieves better MWE identiﬁcation performance than rule-based and sequence-labeling methods. 
In recent years many people have begun to express their thoughts and opinions on Twitter. Naturally, Twitter has become an effective source to investigate people’s emotions for numerous applications. Classifying only positive and negative tweets has been exploited in depth, whereas analyzing finer emotions is still a difficult task. More elaborate emotion lexicons should be developed to deal with this problem, but existing lexicon sets are mostly in English. Moreover, building such lexicons is known to be extremely laborintensive or resource-intensive. Finer-grained features need to be taken into account when determining finer-emotions, but many existing works still utilize coarse features that have been widely used in analyzing only the polarity of emotion. In this paper, we present a method to automatically build fine-grained emotion lexicon sets and suggest features that improve the performance of machine learning based emotion classification in Korean Twitter texts. 
This paper proposes a new method of Chinese word segmentation based on proportional analogy and majority voting. First, we introduce an analogy-based method for solving the word segmentation problem. Second, we show how to use majority voting to make the decision on where to segment. The preliminary results show that this approach compares well with other segmenters reported in previous studies. As an important and original feature, our method does not need any pretraining or lexical knowledge. 
The rise of Natural Language Processing (NLP) opened new possibilities for various applications that were not applicable before. A morphological-rich language such as Arabic introduces a set of features, such as roots, that would assist the progress of NLP. Many tools were developed to capture the process of root extraction (stemming). Stemmers have improved many NLP tasks without explicit knowledge about its stemming accuracy. In this paper, a study is conducted to evaluate various Arabic stemmers. The study is done as a series of comparisons using a manually annotated dataset, which shows the eﬃciency of Arabic stemmers, and points out potential improvements to existing stemmers. The paper also presents enhanced root extractors by using light stemmers as a preprocessing phase. 
It has been long observed that Latinate verbs in English cannot appear in verb-particle constructions, resultative constructions, and double object constructions. Recent research has revealed that, despite persistent counterexamples, the hypothesis invoking the morphological complexity of verbs is the most promising in dealing with the Latinate/native asymmetry (Coppock, 2009; Harley, 2008; Punske 2012, 2013). This paper aims to show two more cases of the asymmetry in favor of the morphological complexity hypothesis. Moreover, in an attempt to refine the hypothesis, an analysis will be provided within Distributed Morphology (Halle and Marantz, 1993). Specifically, it argues that the asymmetry can be reduced to the selectional properties of the acategorial roots involved. Some roots are obligatorily specified for a particular morpheme and combine with it to form a complex root, while others are not necessarily specified as such and they can either stand alone as a simple root or form a complex root. 1. Introduction It has been long observed that Latinate verbs in English are typically bad with verb-particle constructions (e.g., Whorf, 1956; Di Sciullo and Williams, 1987; Harley, 2008), resultative constructions (Harley, 2008), and double object constructions (Pinker, 1989; Pesetsky, 1995; Harley, 2008), which are commonly found in Germanic languages and considered to be a family  of constructions (Snyder, 1995; Stromswold and Snyder, 1995; Snyder and Stromswold, 1997). Various hypotheses have been proposed to derive the asymmetry observed between Latinate and native verbs in those constructions. Notable among them are the prosodic weight hypothesis, which takes the prosodic weight of verbs as a crucial factor (Grimshaw, 2005; Anttila, 2007), the two-lexicon hypothesis, which makes recourse to two different lexical classes, Latinate and native (Grimshaw, 2005), 1 and the morphological complexity hypothesis, which invokes the morphological complexity of verbs (Pinker, 1989; Harley, 2008). 2 While there are persistent counterexamples, the morphological complexity hypothesis, as it stands, is the most promising hypothesis in dealing with the Latinate/native asymmetry, as convincingly demonstrated in a series of psycholinguistic experiments on ditransitivity by Coppock (2009). In this paper, assuming that the morphological complexity hypothesis is on the right track, I will attempt to further increase the plausibility of the hypothesis. Specifically, I will show that the Latinate/native asymmetry can be observed in two more empirical domains, along with the constructions mentioned above: exocentric V-N compounds and non-compositional verb phrase idioms. Moreover, an analysis will be presented within the framework of Distributed Morphology (henceforth, DM; Halle and Marantz, 1993). Specifically, I will argue that the difference between Latinate and native verbs can be reduced 
This paper analyzes what linguistic features differentiate true and false stories written in Hebrew. To do so, we have defined four feature sets containing 145 features: POStags, quantitative, repetition, and special expressions. The examined corpus contains stories that were composed by 48 native Hebrew speakers who were asked to tell both false and true stories. Classification experiments on all possible combinations of these four feature sets using five supervised machine learning methods have been applied. The Part of Speech (POS) set was superior to all others and has been found as a key component. The best accuracy result (89.6%) has been achieved by a combination of sixteen POS-tags and one quantitative feature. 
Out-of-vocabulary (OOV) bilingual lexicon entries is still a problem for many applications, including translation. We propose a method for machine learning of bilingual stem and sufﬁx translations that are then used in deciding segmentations for new translations. Various state-of-the-art measures used to segment words into their sub-constituents are adopted in this work as features to be used by an SVM based linear classiﬁer for deciding appropriate segmentations of bilingual pairs, speciﬁcally, in learning bilingual sufﬁxation. 
This paper explores the possibility of presenting additional contextual information as a method of answer presentation Question Answering. In particular the paper discusses the result of employing Bag of Words (BoW) and Bag of Concepts (BoC) models to retrieve contextual information from a Linked Data resource, DBpedia. DBpedia provides structured information on wide variety of entities in the form of triples. We utilize the QALD question sets consisting of a 100 instances in the training set and another 100 in the testing set. The questions are categorized into single entity and multiple entity questions based on the number of entities mentioned in the question. The results show that both BoW (syntactic models) and BoC (semantic models) are not capable enough to select contextual information for answer presentation. The results further reveals that pragmatic aspects, in particular, pragmatic intent and pragmatic inference play a crucial role in contextual information selection in the answer presentation. 
Recent trends in Question Answering (QA) have led to numerous studies focusing on presenting answers in a form which closely resembles a human generated answer. These studies have used a range of techniques which use the structure of knowledge, generic linguistic structures and template based approaches to construct answers as close as possible to a human generate answer, referred to as human competitive answers. This paper reports the results of an empirical study which uses the linguistic structure of the source question as the basis for a human competitive answer. We propose a typed dependency based approach to generate an answer sentence where linguistic structure of the question is transformed and realized into a sentence containing the answer. We employ the factoid questions from QALD-2 training question set to extract typed dependency patterns based on the root of the parse tree. Using identiﬁed patterns we generate a rule set which is used to generate a natural language sentence containing the answer extracted from a knowledge source, realized into a linguistically correct sentence. The evaluation of the approach is performed using QALD-2 testing factoid questions sets with a 78.84% accuracy. The top-10 patterns extracted from training dataset were able to cover 69.19% of test questions. 
We show that domain adaptation for word sense disambiguation (WSD) satisﬁes the assumption of covariate shift, and then solve it by learning under covariate shift. Learning under covariate shift has two key points: (1) calculation of the weight of an instance and (2) weighted learning. For the ﬁrst point, we employ unconstrained least squares importance ﬁtting (uLSIF), which models the probability density ratio of the source domain against a target domain directly. Additionally, we propose weight only to the particular instance and using a linear kernel rather than a Gaussian kernel in uLSIF. For the second point, we employ a support vector machine (SVM) rather than the maximum entropy method (ME) that is commonly employed in weighted learning. Three corpora in the Balanced Corpus of Contemporary Written Japanese (BCCWJ) and 16 target words were used in our experiment. The experimental results show that the proposed method demonstrates the highest average precision.  pus A. This is the problem of domain adaptation1. In this paper, we deal with domain adaptation for word sense disambiguation (WSD). WSD identiﬁes the sense c ∈ C of an ambiguous word w in a sentence x. This problem can be solved by the following equation: arg max P (c|x). c∈C The above equation can be solved using supervised learning. However, the domain adaptation problem occurs in a real task. In domain adaptation, Ps(c|x) can be derived from source domain S; therefore, we must estimate Pt(c|x) in the target domain T using Ps(c|x) and other data. Note that the sense c of the word w in sentence x is not changed if sentence x appears in any domain corpus, i.e., P (c|x) does not depend on a domain. As a result, Ps(c|x) = Pt(c|x). Therefore, it seems that we do not need to estimate Pt(c|x) because we have Ps(c|x). However, this is wrong because Ps(x) = Pt(x). The following assumption is referred to as the covariate shift:  
In this paper, we propose an unsupervised domain adaptation for Word Sense Disambiguation (WSD) using Stacked Denoising Autoencoder (SdA). SdA is an unsupervised learning method of obtaining the abstract feature set of input data using Neural Network. The abstract feature set absorbs the difference of domains, and thus SdA can solve a problem of domain adaptation. However, SdA does not always cope with any problems of domain adaptation. Especially, difﬁculty of domain adaptation for WSD depends on the combination of a source domain, a target domain and a target word. As a result, any method of domain adaptation for WSD has adverse effect for a part of the problem, Therefore, we deﬁned the similarity between two domains, and judge whether we use SdA or not through this similarity. This approach avoids an adverse effect of SdA. In the experiments, we have used three domains from the Balanced Corpus of Contemporary Written Japanese and 16 target words. In comparison with baseline, our method has got higher average accuracies for all combinations of two domains. Furthermore, we have obtained better results against conventional domain adaptation methods. 
This paper extracts collocationbasing on semantic dependency parsing, and then constructs a collocation bankwith two levels according to frequency: the instance-level and semantic level. Compared with conventional extracting ways, the collocationsextracted in this paper have closer relationship and higher quality both on the lexical structure and semantic structure. 
 such lexical items.1 Please refer to the following examples.  Functioning as adverbials, yídìng and shìbì in Mandarin Chinese can either express intensification or (strong) epistemic necessity. In addition, context influences their semantics. Hence, dynamic semantics are proposed for them. An information state  is a pair <A, s>, where s is a proposition and A is an affirmative ordering. Yídìng() performs update on an information state: A is updated with  and s is specified to be a subset of or equal of , as long as  is true in one of the absolutely affirmative worlds. Otherwise, uttering yídìng() leads to an absurd state. This is how a strong epistemic necessity reading is derived. To yield an intensification reading, yídìng() performs a test on the information state. Yídìng() gives back the original information state as long as  is true in all of the absolutely affirmative worlds. Otherwise, an absurd state is produced. As for shìbì, its semantics is identical to that of yídìng, except for that the s in an information state  for shìbì is underspecified and needs resolving before a proposition gets an appropriate interpretation. The information needed to resolve the underspecified s for shìbìmust be inferred from the context. 
✁ This study points out that Yòu ( ) and Hái ( ) have their own prominent semantic features and syntactic patterns compared with each other. The differences reflect in the combination with ✁ verbs1. Hái ( ) has absolute superiority in ✂ collocation with V+Bu ( )+V, which tends to express [durative]. Yòu ( ) has advantages in ✄ collocations with V+Le ( )+V and derogatory ✄ verbs. Yòu ( )+V+Le ( )+V tends to express [repetition], and Yòu ( )+derogatory verbs tends to express [repetition, derogatory]. We also find that the two words represent different semantic features when they match with ✄ ☎ grammatical aspect markers Le ( ), Zhe ( ) ✆ and Guo ( ). Different distributions have a close relation with their semantic features. This study is based on the investigation of the large-scale corpus and data statistics, applying methods of corpus linguistics, computational linguistics and semantic background model, etc. We also described and explained the language facts. 
In this article, how word embeddings can be used as features in Chinese sentiment classiﬁcation is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classiﬁcation, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the ﬁnal F1 score is 0.920. 
While substantial studies have been achieved on sentiment analysis to date, it is still challenging to explore enough contextual information or specific cues for polarity classification of short text like online product reviews. In this work we explore review clustering and opinion paraphrasing to build multiple cluster-based classifiers for polarity classification of Chinese product reviews under the framework of support vector machines. We apply our approach to two corpora of product reviews in car and mobilephone domains. Our experimental results demonstrate that opinion clustering and paraphrasing are of great value to polarity classification.  classification using a single general classifier. Furthermore, lacking large annotated corpora is still a fundamental issue for statistical sentiment analysis. To address the above problems, in this work we explore review clustering and opinion paraphrasing to build multiple cluster-based classifiers for polarity classification of Chinese product reviews. To this end, we first explore a two-stage hierarchical clustering with multilevel similarity to cluster the training data into a set of opinion clustering and then building a polarity classifier for each review cluster via supported vector machines (SVMs). In addition, we also exploit paraphrase generation to expand product reviews in each cluster to achieve reliable training for the corresponding polarity classifier. 9  
Many of the language identification (LID) systems are based on language models using machine learning (ML) techniques that take into account the fluctuation of speech over time, such as Hidden Markov Models (HMM). Considering the fluctuation of speech results LID systems use relatively long recording intervals to obtain reasonable accuracy. This research tries to extract enough features from short recording intervals in order to enable successful classification of the tested spoken languages. The classification process is based on frames of 20 milliseconds (ms) where most of the previous LID systems were based on much longer time frames (from 3 seconds to 2 minutes). We defined and implemented 173 low level features divided into three feature sets: cepstrum, relative spectral (RASTA), and spectrum. The examined corpus, containing speech files in seven languages, is a subset of the Oregon Graduate Institute (OGI) telephone speech corpus. Six machine learning (ML) methods have been applied and compared and the best optimized results have been achieved by Random Forest (RF): 89%, 82%, and 80% for 2, 5, and 7 languages, respectively. 
To understand how daily usages can shape the gradual changes of both hěn, a prototypical intensifier in Mandarin Chinese, and the construction hěn X, the study aims to investigate the syntactic and semantic behaviors of hěn X constructions in spoken corpora. The conversational data from the NCCU Corpus of Spoken Chinese and a Taiwan Public Television show Bring Up Parents are extracted and analyzed, focusing in particular on the syntactic categories of X, the grammaticalization of hěn, and the lexicalization of hěn X. Several findings are found. First, the syntactic and semantic distributions of the data from both corpora are quite consistent. While adjectives and stative verbs still claim the majority of X, new categories are discovered, showing host expansion of X. In addition to words, phrases and clauses can play the role of X. The increase of the flexibility and complexity of X demonstrates the gradual grammaticalization of hěn. Moreover, some instances of hěn X can be used as a unit to modify other grammatical constituents, showing a certain degree of lexicalization. When hěn X is fused as a unit, hěn is obligatory, not only indicating a degree but also highlighting the characteristics of X. The analysis shows that the nature of spoken materials enhances the subjectivity of hěn X. The findings of hěn X in spoken corpora can be applied to linguistic studies and Mandarin teaching.  
Phonemic content is one of many important criteria in a development of any kind of speech testing materials. In this paper, we explain a procedure and tool we created in the process of constructing phonetically-balanced (PB) sentence-length materials for Thai, as an assessment for speech reception thresholds. Our procedure includes establishing criteria, preselecting sentences, creating pool of replacement words, determining phonemic distribution, and constructing sentences. Importantly, a tool is created to determine whether set of words or sentences are phonetically balanced. Once the phoneme distribution and the set of words with transcription are specified, the tool efficiently computes phoneme occurrences among words or sentences (within a set) and can be used to manipulate words to achieve goal in  phonetically balanced (PB). To show how this is accomplished, two sentence sets are constructed and evaluated by native speakers. The procedure and tool have characteristics that make them potentially useful in other applications and can be applied to other languages. Keywords: Thai, sentence-length material construction, phonetically balanced speech materials 
Graph theory has recently been used to explore the mathematical structure of the mental lexicon. In this study we tested the influence of graph measures on Mandarin speech production. Thirty-six native Mandarin-speaking adults took part in a shadowing task containing 194 monosyllabic words, 94 of which consisted of 3 phonemes and were the items under analysis. Linear mixed effect modeling revealed that clustering coefficient (C) predicted spoken production of Mandarin monosyllabic words, while network degree, in this case its phonological neighborhood density (PND) failed to account for lexical processing. High C resulted in shorter reaction times, contrary to evidence in English. While these findings suggest that lexical processing is affected by the network structure of the mental lexicon, they also suggest that language specific traits lead to differing behavioral outcomes. While PND can be understood as the underlying lattice for which a similarity network is created, lexical selection is not affected by only a target word’s neighbors but instead the level of interconnectivity of words (C) within the network.  
The performance of many content analysis methods heavily dependent on the features they are applied. A fundamental problem that makes the content analysis difficult is the curse of dimensionality. In this study, we propose a novel feature reduction method which adopts ensemble approach to measure the divergence between the training set and test set and use the divergence to supervise the feature reduction procedure. The proposed method uses pairwise measure to get the diversity between classifiers and selects the complementary classifiers to get the pseudo labels on test set. The pseudo labels are used to measure the divergence between training set and test set. The feature reduction algorithm merges the adjacent feature space according to the divergence, such reduce the feature number. We evaluated the proposed method on several standard datasets. Experiment results shown the efficiency of the proposed feature reduction method. 
We investigated the notion of “popularity” for machine-generated sentences. We deﬁned a popular sentence as one that contains words that are frequently used, appear in many documents, and contain frequent dependencies. We measured the popularity of sentences based on three components: content morpheme count, document frequency, and dependency relationships. To consider the characteristics of agglutinative language, we used content morpheme frequency instead of term frequency. The key component in our method is that we use the product of content morpheme count and document frequency to measure word popularity, and apply language models based on dependency relationships to consider popularity from the context of words. We verify that our method accurately reﬂects popularity by using Pearson correlations. Human evaluation shows that our method has a high correlation with human judgments. 
We propose three methods for obtaining distributed representations for verb-object pairs in predicated argument structures by using word2vec. Word2vec is a method for acquiring distributed representations for a word by retrieving a weight matrix in neural networks. First, we analyze a large amount of text with an HPSG parser; then, we obtain distributed representations for the verb-object pairs by learning neural networks from the analyzed text. We evaluated our methods by measuring the MRR score for verb-object pairs and the Spearman’s rank correlation coefﬁcient for verb-object pairs in experiments. 
This paper explores the problem of parsing Chinese long sentences. Inspired by human sentence processing, a second-stage parsing method, referred as main structure parsing in this paper, are proposed to improve the parsing performance as well as maintaining its high accuracy and efficiency on Chinese long sentences. Three different methods have attempted in this paper and the result shows that the best performance comes from the method using Chinese comma as the boundary of the sub - sentence. According to our experiment about testing on the Chinese dependency Treebank 1.0 data, it improves long dependency accuracy by around 6.0% than the baseline parser and 3.2% than the previous best model. 
This paper introduces a machine learning approach to distinguish machine translation texts from human texts in the sentence level automatically. In stead of traditional methods, we extract some linguistic features only from the target language side to train the prediction model and these features are independent of the source language. Our prediction model presents an indicator to measure how much a sentence generated by a machine translation system looks like a real human translation. Furthermore, the indicator can directly and effectively enhance statistical machine translation systems, which can be proved as BLEU score improvements. 
The study of word classes has a history of over 4000 years, and the word class problem in over 1000 analytic languages like Modern Chinese can be seen as the Goldbach Conjecture in linguistics. This paper first outlines the existing problems in the POS tagging of Modern Chinese corpora with a case study of 自信. Then it introduces the Two-level Word Class Categorization Model in analytic languages, which is based on the perspectives of language as a complex adaptive system and the nature of major parts of speech as propositional speech act functions. Finally, the implications of Two-level Word Class Categorization Model for POS tagging in Modern Chinese corpora are explored. 
This paper is a brief review of the research on language variation using corpus data and statistical modeling methods. The variation phenomena covered in this review include phonetic variation (in spontaneous speech) and syntactic variation, with a focus on studies of English and Chinese. The goal of this paper is to demonstrate the use of corpus-driven statistical models in the study of language variation, and discuss the contribution and future directions of this line of research. 
Detecting language divergences and predicting possible sub-translations is one of the most essential issues in machine translation. Since the existence of translation divergences, it is impractical to straightforward translate from source sentence into target sentence while keeping the high degree of accuracy and without additional information. In this paper, we investigate the problem from an emerging and special point of view: bigrams and the corresponding translations. We ﬁrst proﬁle corpora and explore the constituents of bigrams in the source language. Then we translate unseen bigrams based on proportional analogy and ﬁlter the outputs using an Support Vector Machine (SVM) classiﬁer. The experiment results also show that even a small set of features from analogous can provide meaningful information in translating by analogy. 
This paper proposes a surrounding word sense model (SWSM) that uses the distribution of word senses that appear near ambiguous words for unsupervised all-words word sense disambiguation in Japanese. Although it was inspired by the topic model, ambiguous Japanese words tend to have similar topics since coarse semantic polysemy is less likely to occur than that in Western languages as Japanese uses Chinese characters, which are ideograms. We thus propose a model that uses the distribution of word senses that appear near ambiguous words: SWSM. We embedded the concept dictionary of an Electronic Dictionary Research (EDR) electronic dictionary in the system and used the Japanese Corpus of EDR for the experiments, which demonstrated that SWSM outperformed a system with a random baseline and a system that used a topic model called Dirichlet Allocation with WORDNET (LDAWN), especially when there were high levels of entropy for the word sense distribution of ambiguous words. 
In this paper, we conducted semantic transparency rating experiments using both the traditional laboratory-based method and the crowdsourcing-based method. Then we compared the rating data obtained from these two experiments. We observed very strong correlation coefficients for both overall semantic transparency rating data and constituent semantic transparency data (rho > 0.9) which means the two experiments may yield comparable data and crowdsourcing-based experiment is a feasible alternative to the laboratorybased experiment in linguistic studies. We also observed a scale shrinkage phenomenon in both experiments: the actual scale of the rating results cannot cover the ideal scale [0, 1], both ends of the actual scale shrink towards the center. However, the scale shrinkage of the crowdsourcing-based experiment is stronger than that of the laboratory-based experiment, this makes the rating results obtained in these two experiments not directly comparable. In order to make the results directly comparable, we explored two data transformation algorithms, z-score transformation and adjusted normalization to unify the scales. We also investigated the uncertainty of semantic transparency judgment among raters, we found that it had a regular relation with semantic transparency magnitude and this may further reveal a general cognitive mechanism of human judgment. 
We propose a method for implicit discourse relation recognition using a recursive neural network (RNN). Many previous studies have used the word-pair feature to compare the meaning of two sentences for implicit discourse relation recognition. Our proposed method differs in that we use various-sized sentence expression units and compare the meaning of the expressions between two sentences by converting the expressions into vectors using the RNN. Experiments showed that our method signiﬁcantly improves the accuracy of identifying implicit discourse relations compared with the word-pair method. 
Entity linking is an indispensable operation of populating knowledge repositories for information extraction. It studies on aligning a textual entity mention to its corresponding disambiguated entry in a knowledge repository. In this paper, we propose a new paradigm named distantly supervised entity linking (DSEL), in the sense that the disambiguated entities that belong to a huge knowledge repository (Freebase) are automatically aligned to the corresponding descriptive webpages (Wiki pages). In this way, a large scale of weakly labeled data can be generated without manual annotation and fed to a classiﬁer for linking more newly discovered entities. Compared with traditional paradigms based on solo knowledge base, DSEL beneﬁts more via jointly leveraging the respective advantages of Freebase and Wikipedia. Speciﬁcally, the proposed paradigm facilitates bridging the disambiguated labels (Freebase) of entities and their textual descriptions (Wikipedia) for Web-scale entities. Experiments conducted on a dataset of 140,000 items and 60,000 features achieve a baseline F1measure of 0.517. Furthermore, we analyze the feature performance and improve the F1-measure to 0.545. 
 †§Department of Computer Science, National Chengchi University, Taiwan ‡!Institute for Quantitative Social Science, Harvard University, USA †Graduate Institute of Linguistics, National Chengchi University, Taiwan {†chaolin,§102753029}@nccu.edu.tw, {‡hongsuwang, !pkbol}@fas.harvard.edu  Abstract1 Difangzhi (地方志) is a large collection of local gazetteers complied by local governments of China, and the documents provide invaluable information about the host locality. This paper reports the current status of using natural language processing and text mining methods to identify biographical information of government officers so that we can add the information into the China Biographical Database (CBDB), which is hosted by Harvard University. Information offered by CBDB is instrumental for human historians, and serves as a core foundation for automatic tagging systems, like MARKUS of the Leiden University. Mining texts in Difangzhi is not easy partially because there is litter knowledge about the grammars of literary Chinese so far. We employed techniques of language modeling and conditional random fields to find person and location names and their relationships. The methods were evaluated with realistic Difangzhi data of more than 2 million Chinese characters written in literary Chinese. Experimental results indicate that useful information was discovered from the current dataset. 
A common approach to unsupervised relation extraction builds clusters of patterns expressing the same relation. In order to obtain clusters of relational patterns of good quality, we have two major challenges: the semantic representation of relational patterns and the scalability to large data. In this paper, we explore various methods for modeling the meaning of a pattern and for computing the similarity of patterns mined from huge data. In order to achieve this goal, we apply algorithms for approximate frequency counting and efﬁcient dimension reduction to unsupervised relation extraction. The experimental results show that approximate frequency counting and dimension reduction not only speeds up similarity computation but also improves the quality of pattern vectors. 
The high-dimensionality of lexical features in parsing can be memory consuming and cause over-ﬁtting problems. We propose a general framework to replace all lexical feature templates by low-dimensional features induced from word embeddings. Applied to a near state-of-the-art dependency parser (Huang et al., 2012), our method improves the baseline, performs better than using cluster bit string features, and outperforms a recent neural network based parser. A further analysis shows that our framework has the effect hypothesized by Andreas and Klein (2014), namely (i) connecting unseen words to known ones, and (ii) encouraging common behaviors among invocabulary words. 
In this work, we present a novel way of using neural network for graph-based dependency parsing, which ﬁts the neural network into a simple probabilistic model and can be furthermore generalized to high-order parsing. Instead of the sparse features used in traditional methods, we utilize distributed dense feature representations for neural network, which give better feature representations. The proposed parsers are evaluated on English and Chinese Penn Treebanks. Compared to existing work, our parsers give competitive performance with much more efﬁcient inference. 
Japanese is prescriptively said to be verb-final, but it exhibits postposing in colloquial register, where an element is placed after a verb. Based on narrative data, we show that the syntactic type of postposed element is quite diverse and that, contrary to the prevalent, opposing view, Japanese postposing is not restricted to a matrix clause. These issues are addressed in Dynamic Syntax, with the outcome of developing some formal aspects of the framework.  
In this paper, we propose a novel approach to induce automatically a Part-Of-Speech (POS) tagger for resource-poor languages (languages that have no labeled training data). This approach is based on cross-language projection of linguistic annotations from parallel corpora without the use of word alignment information. Our approach does not assume any knowledge about foreign languages, making it applicable to a wide range of resource-poor languages. We use Recurrent Neural Networks (RNNs) as multilingual analysis tool. Our approach combined with a basic crosslingual projection method (using word alignment information) achieves comparable results to the state-of-the-art. We also use our approach in a weakly supervised context, and it shows an excellent potential for very lowresource settings (less than 1k training utterances). 
Identification of prepositional phrases (PP) has been an issue in the field of Natural Language Processing (NLP). In this paper, towards Chinese patent texts, we present a rule-based method and a CRF-based method to identify the PPs. In the rule-based method, according to the special features and expressions of PPs, we manually write targeted formal identification rules; in the CRF approach, after labelling the sentences with features, a typical CRF toolkit is exploited to train the model for identifying PPs. We then conduct some experiments to test the performance of the two methods, and final precision rates are over 90%, indicating the proposed methods are effective and feasible. 
Traditional sentiment classiﬁcation methods often require polarity dictionaries or crafted features to utilize machine learning. However, those approaches incur high costs in the making of dictionaries and/or features, which hinder generalization of tasks. Examples of these approaches include an approach that uses a polarity dictionary that cannot handle unknown or newly invented words and another approach that uses a complex model with 13 types of feature templates. We propose a novel high performance sentiment classiﬁcation method with stacked denoising auto-encoders that uses distributed word representation instead of building dictionaries or utilizing engineering features. The results of experiments conducted indicate that our model achieves state-of-the-art performance in Japanese sentiment classiﬁcation tasks. 
Wikipedia is supposed to be supporting the “Neutral Point of View”. Instead of accepting this statement as a fact, the current paper analyses its veracity by speciﬁcally analysing a typically controversial (negative) topic, such as war, and answering questions such as “Are there sentiment differences in how Wikipedia articles in different languages describe the same war?”. This paper tackles this challenge by proposing an automatic methodology based on article level and concept level sentiment analysis on multilingual Wikipedia articles. The results obtained so far show that reasons such as people’s feelings of involvement and empathy can lead to sentiment expression differences across multilingual Wikipedia on war-related topics; the more people contribute to an article on a war-related topic, the more extreme sentiment the article will express; different cultures also focus on different concepts about the same war and present different sentiments towards them. Moreover, our research provides a framework for performing different levels of sentiment analysis on multilingual texts. 
High dimension of bag-of-words vectors poses a serious challenge from sparse data, overfitting, irrelevant features to document classification. Filter feature selection is one of effective methods for dimensionality reduction by removing irrelevant features from feature set. This paper focuses on two main problems of filter feature selection which are the feature score computation and the imbalance in the feature selection performance between categories. We propose a novel filter feature selection method, named ExFCFS, to comprehensively resolve these problems. We experiment on related filter feature selection methods with two benchmark datasets - Reuters-21578 dataset and Ohsumed dataset. The experimental results show the effectiveness of our solutions in terms of both Micro-F1 measure and Macro-F1 measure. Keywords— bag-of-words vector, filter feature selection, document classification 
Thai stock brokers issue daily stock news for their customers. One broker labels these news with plus, minus and zero sign to indicate the type of recommendation. This paper proposed to classify Thai stock news by extracting important texts from the news. The extracted text is in a form of a ‘wordpair’. Three wordpair sets, manual wordpairs extraction (ME), manual wordpairs addition (MA), and automate wordpairs combination (AC), are constructed and compared for their precision, recall and f-measure. Using this broker’s news as a training set and unseen stock news from other brokers as a testing set, the experiment shows that all three sets have similar results for the training set but the second and the third set have better classification results in classifying stock news from unseen brokers. Keywords: Thai stock news, sentiment classification, text classification, wordpair features. 
Document sentiment classification is often processed by applying machine learning techniques, in particular supervised learning which consists basically of two major steps: feature extraction and training the learning model. In the literature, most existing researches rely on n-grams as selected features, and on a simple basic classifier as learning model. In the context of our work, we try to improve document classification findings in Arabic sentiment analysis by combining different types of features such as opinion and discourse features; and by proposing an ensemble-based classifier to investigate its contribution in Arabic sentiment classification. Obtained results attained 85.06% in terms of macro-averaged Fmeasure, and showed that discourse features have moderately improved Fmeasure by approximately 3% or 4%. 
This paper argues that the Invertible Construction (IC) in Chinese is a kind of distributive construction. What appears to be an inversion of word order is best understood as the division of the theme NP to be acted upon by a number of agents for an embedded event. This analysis best captures a number of otherwise intractable properties of the IC including the necessarily quantitative interpretation of the NPs and the incompatibility with adverbs of volition. 
The notion of logophoricity has long played a crucial role in understanding the co-referential relations between certain anaphoric expressions cross-linguistically, especially for longdistance anaphors violating a locality constraint and syntactic prominence conditions within the framework of pure syntactic accounts. However, Pan (2001) has shown that the long-distance binding of Chinese ziji should not be treated with the logophoric accounts in some aspects. This paper revisits Pan’s (2001) puzzle, which arises from the ability of ziji to serve as a logophor, in order to call attention to what the alternative to this view might be, and proposes a solution to it through the notion of empathy, in Kuno and Kaburaki’s (1977) sense of the term, so that long-distance anaphors, which are not fully covered in terms of logophoricity, can be reconciled with other East Asian languages, such as Japanese zibun and Korean caki, in terms of a unified treatment. 
A number of researchers claim that the derivation of the Right Dislocation Construction (RDC) involves movement (e.g., Chung, 2012, for Korean; Ott & de Vries, 2012, 2015, for Dutch and German; Tanaka, 2001 and Abe, 2004, for Japanese; Whitman, 2000, for English, Japanese, and Korean). However, the RDC in English does not obey movement constraints such as the Coordinate Structure Constraint and the Left Branch Condition; that is, there are acceptable sentences that seem to violate these movement constraints. This suggests that the derivation of the English RDC should not involve movement. The present paper demonstrates that some syntactic properties of the English RDC can be explained instead through the interaction of independently motivated parsing strategies with a licensing condition for adjoined elements. 
This paper explores the conditions where Mandarin RVCs can be preserved in their Cantonese counterparts. Six types of Mandarin RVCs – ergatives, unergatives, accusatives, causatives, pseudo-passives and objectfronting – have been examined. Modifications have been made for certain kinds of RVCs that are usually misclassified. The analysis has been done at both the lexical and syntactic levels. At the lexical level, the concept of ‘strong resultative’ and ‘weak resultative’ has been adduced to support the idea that indirect causation cannot be expressed by RVC in Cantonese. At the syntactic level, the presentations of the same RVCs falling into different sentence types are illustrated. Since the structure of Mandarin RVCs are often restricted in Cantonese, three substitutive constructions have been introduced for presenting the same resultatives in Cantonese. 
This paper took an experimental approach and examined island constraints in Korean. Among many island constraints, this study took a Complex NP island constraint, and the experiment was designed with 3 related factors: presence vs. absence of island, matrix clause vs. embedded clause, and scrambling. The analysis results illustrated that the presence/absence of complex NP island did not play a role by itself in Korean but that it made distinctions through the interactions with other factor such as matrix vs. embedded clause. 
Although Multiple Subject Constructions in Korean have received significant attention in theoretical literature, few experimental investigations of various syntactic and semantic properties of these constructions have been conducted. In this study, we administered a Magnitude Estimation (ME) experiment in order to compare the acceptability of Multiple Subject and related Single Subject Constructions (MSCs vs. SSCs) and that of two types of MSCs (Possessor-type vs. Adjunct-type MSCs). The results showed that MSCs received lower acceptability than SSCs. In addition, the Adjunct-type MSCs received higher acceptability scores than the Possessor-type MSCs. Possible reasons for these results are discussed. 
This paper contributes the first published evaluation of the quality of automatic translation between Khmer (the official language of Cambodia) and twenty other languages, in both directions. The experiments were carried out using three different statistical machine translation approaches: phrase-based, hierarchical phrase-based, and the operation sequence model (OSM). In addition two different segmentation schemes for Khmer were studied, these were syllable segmentation and supervised word segmentation. The results show that the highest quality machine translation was attained with word segmentation in all of the experiments. Furthermore, with the exception of very distant language pairs the OSM approach gave the highest quality translations when measured in terms of both the BLEU and RIBES scores. For distant languages, our results showed a hierarchical phrase-based approach to be the most effective. An analysis of the experimental results indicated that Kendall’s tau may be directly used as a means of selecting an appropriate machine translation approach for a given language pair. 
This paper proposes a well-formed dependency to string translation model with BTG grammar. By enabling the usage of wellformed sub-structures and allowing ﬂexible reordering of them, our approach is effective to relieve the problems of parsing error and ﬂatness in dependency structure. To utilize the well-formed dependency rules during decoding, we adapt the tree traversal decoding algorithm into a bottom-up CKY algorithm. And a lexicalized reordering model is used to encourage the proper combination of two neighbouring blocks. Experiment results demonstrate that our approach can effectively improve the performance by more than 2 BLEU score over the baseline. 
We present our ongoing work on large-scale Japanese-Chinese bilingual dictionary construction via pivot-based statistical machine translation. We utilize statistical signiﬁcance pruning to control noisy translation pairs that are induced by pivoting. We construct a large dictionary which we manually verify to be of a high quality. We then use this dictionary and a parallel corpus to learn bilingual neural network language models to obtain features for reranking the n-best list, which leads to an absolute improvement of 5% in accuracy when compared to a setting that does not use significance pruning and reranking. 
Feedback utterances are among the most frequent in dialogue. Feedback is also a crucial aspect of all linguistic theories that take social interaction involving language into account. However, determining communicative functions is a notoriously difﬁcult task both for human interpreters and systems. It involves an interpretative process that integrates various sources of information. Existing work on communicative function classiﬁcation comes from either dialogue act tagging where it is generally coarse grained concerning the feedback phenomena or it is token-based and does not address the variety of forms that feedback utterances can take. This paper introduces an annotation framework, the dataset and the related annotation campaign (involving 7 raters to annotate nearly 6000 utterances). We present its evaluation not merely in terms of inter-rater agreement but also in terms of usability of the resulting reference dataset both from a linguistic research perspective and from a more applicative viewpoint. 
Building characters for dialogue agents is important in making the agents more friendly and human-like. To build such characters, utterances suitable for the designated characters are usually manually prepared. However, it is expensive to do this for a large number of utterances for various types of characters. We propose a method for automatically converting system utterances into those that are characteristic of designated personal attributes, such as gender, age and area of residence, to characterize agents. In particular, we focus on converting sentence-end expressions, which are considered to greatly affect personal attributes in Japanese. Conversion is done by (i) automatically collecting conversion candidates from various utterances on the Web (e.g., Twitter postings), and (ii) using syntactic and semantic ﬁlters to suppress the generation of ill-formed utterances. Experimental results show that our method can convert approximately 95% of utterances into those that are grammatically and semantically acceptable and approximately 90% of utterances into those that are perceived to be acceptable for designated personal attributes. 
This paper explores the nature of linguistic synaesthesia in the auditory domain through a corpus-based lexical semantic study of near synonyms. It has been established that the near synonyms 聲 sheng “sound” and 音 yin “sound” in Mandarin Chinese have different semantic functions in representing auditory production and auditory perception respectively. Thus, our study is devoted to testing whether linguistic synaesthesia is sensitive to this semantic dichotomy of cognition in particular, and to examining the relationship between linguistic synaesthesia and cognitive modelling in general. Based on the corpus, we ﬁnd that the near synonyms exhibit both similarities and differences on synaesthesia. The similarities lie in that both 聲 and 音 are productive recipients of synaesthetic transfers, and vision acts as the source domain most frequently. Besides, the differences exist in selective constraints for 聲 and 音 with synaesthetic modiﬁers as well as syntactic functions of the whole combinations. We propose that the similarities can be explained by the cognitive characteristics of the sound, while the differences are determined by the inﬂuence of the semantic dichotomy of production/perception on synaesthesia. Therefore, linguistic synaesthesia is not a random association, but can be motivated and predicted by cognition. 
A novel graph-based utterance generation method for open-domain dialogue systems is proposed in this paper. After an association graph of words and utterance patterns from a dialogue corpus is constructed, a label propagation algorithm is used for generating system utterances from the words and utterance patterns in the association graph that are found to strongly correlate with the words and utterance patterns that appeared in previous user utterances. We also propose a crowdsourcing framework for collecting annotated chat data so that we can implement our method in a cost effective manner. Crowdsourcing is also used for conducting subjective evaluations and the results will show that the proposed method can not only provide interesting and informative responses but it also can appropriately expand the topics by comparing them to a well-known chat system in Japanese. 
This study investigates the habitual expressions of metaphors in language and gesture and the collaboration of these two modalities in conveying metaphors. This study examined 247 metaphoric expressions in Mandarin conversations. The data includes 110 (44.5%) metaphors being conveyed concurrently by speech and gesture as well as 137 (55.5%) metaphors being conveyed in gesture exclusively. Results show that Entity metaphor is the most frequent one expressed in daily conversations. The cooperation of language and gesture enables us to evaluate the various hypotheses of speech-gesture production. Results from this study tend to support the Interface Hypothesis, which suggests that gestures are generated from an interface representation between speaking and spatiomotoric thought. 
This paper describes two experiments that explore the potential role of Chinese character writing on their visual recognition. Taken together, the results suggest that drawing Chinese characters privileges them in memory in a way that facilitates their subsequent visual recognition. This is true even when the congruency of the recognition response and other potential confounds are controlled for. 
A learner corpus is a useful resource for developing automatic assessment techniques for implementation in a computer-assisted language learning system. However, presently, learner corpora are only helpful in terms of evaluating the accuracy of learner output (speaking and writing). Therefore, the present study proposes a learner corpus annotated with evaluation results regarding the accuracy and fluency of performance in speaking (output) and listening (input). 
Language development in infants is a dynamic process that involves the emergence and increase of consciousness, with which built-in learning mechanisms make infants’ imitation and interaction with their surroundings become socially meaningful. Taking Gao & Holland’s (2008, 2013) statements of levels of consciousness for language development as the theoretical guideline, this study proposes a rule-based, signal-processing agent-based model to explore the dynamics of language development in early infants. In this model, we assume that an infant’s rule-based learning behaviors can be featured by different levels of consciousness and that its adaptation processes can be explained in relation to levels of consciousness. In this paper we will discuss properties of consciousness at different levels and identify the influencing factors for reaching them. Our ultimate goal in building up the model is to understand the processes of language development with an approach that can better reflect reality. 
This paper proposes a range of solutions to the challenges of extracting large and highquality bilingual lexicons for low-resource language pairs. In such scenarios there is often no parallel or even comparable data available. We design three effective pivotbased approaches inspired by the state-ofthe-art technique of bilingual topic modelling, extending previous work to take advantage of trilingual data. The proposed models are shown to outperform traditional methods significantly and can be adapted based upon the nature of available training data. We demonstrate the accuracy of these pivot-based approaches in a realistic scenario generating an IcelandicKorean lexicon from Wikipedia. 
 This paper describes a corpus-based contrastive  study of collocation in English and Chinese. In  light of the corpus-based approach to identify  functionally equivalent units, the present paper  attempts to identify the collocational  translation equivalents of zunshou by using a  parallel corpus and two comparable corpora.  This study shows that more often than not, we  can find in English more than one translation  equivalents. By taking collocates into  consideration, we are able to establish bilingual  equivalence with more accuracy. The present  study indicates that semantic preference and  semantic prosody play a vital role in  establishing  equivalence  between  corresponding lexical sequences in English and  Chinese. The studies of collocation across  languages have potentially useful implications  for foreign language teaching and learning,  contrastive linguistic and translation studies, as  well as bilingual lexicography.  
The main challenge of this paper is the syntactico-semantic enrichment of LMF normalized dictionaries. To meet this challenge, we propose an approach based on the content of these dictionaries, namely the “Context” fields and the syntactic and semantic knowledge. The proposed approach is composed of three phases. The first one deals with the data set concerning the syntactic arguments of the “Context” fields. The second consists in connect semantic arguments to the syntactic ones. The last phase links syntactic and semantic arguments. In order to evaluate the proposed approach, we have applied it to an available Arabic normalized dictionary. The results are encouraging with respect to the measurement evaluation. 
This research aims to integrate embodiment with generative lexicon. By analyzing the metaphorically used human body part terms in Sinica Corpus, the first balanced modern Chinese corpus, we reveal how these two theories complement each other. Embodiment strengthens generative lexicon by spelling out the cognitive reasons which underlies the production of meaning, and generative lexicon, specifically the qualia structure, complements embodiment by accounting for the reason underlying the selection of a particular body part for metaphorization. Discussing how the four body part terms— 血 xie “blood”, 肉 rou “flesh”, 骨 gu “bone”, 脈 mai “meridian”— behave metaphorically, this research argues that the visibility and the telic role of the qualia structure are the major reasons motivating the choice of a body part to represent a comparatively abstract notion. The finding accounts for what constrains the selection of body parts for metaphorical uses. It also facilitates the prediction of the behavior of the four body part terms in these uses, which can function as the starting point to examine whether the two factors—visibility and telicity—also motivate the metaphorization of the rest human body parts. 
Clausal izyooni ‘than’-comparatives in Japanese allow izyooni ‘than’-clauses with their degree positons filled. I consider them a degree version of Internally Headed Relative Clauses (IHRCs). In this preliminary study, I adopt Gross and Landman’s (2012) Choose Role analysis of IHRCs in Japanese and propose a similar functional category Choose Degree, which “re-opens” a degree variable position for “closed” izyooni-clauses. This makes it possible for once closed izyooni-clauses to denote a set of degrees. 
This paper develops a Case/case-theoretic acco unt for what Merchant (2008) calls voice mism atch in ellipsis constructions of English. Merch ant (ibid.) reports that VP ellipsis as an elision o f smaller size VP allows voice mismatch, but Ps eudogapping and Sluicing as an elision of bigge r size vP/TP do not. However, Tanaka (2011) ar gues against Merchant's dichotomy in voice mis match between VP ellipsis and Pseudogapping, reporting that voice mismatch in both types of e llipsis is permissible or not while interacting wi th what Kehler (2000) calls discourse coherence relations between ellipsis and antecedent clause s. Departing from Kehler's (2000) insight, we s uggest that vP undergoes ellipsis in a resemblan ce discourse relation, but VP does so in a cause/ effect discourse relation. Given the asymmetry i n the size of ellipsis in tandem with discourse re lations, we argue that since Accusative as well as Nominative Case is checked outside VP, the VP to be elided can meet the identity condition on ellipsis with its antecedent VP as the object element in the former and the subject one in the latter or vice versus have not been Case-checke d yet, thus being identical in terms of Case-feat ure at the point of derivation building a VP. 
The cause of island effects has evoked considerable debate within syntax and other fields of linguistics. The two competing approaches stand out: the grammatical analysis; and the working-memory (WM)-based processing analysis. In this paper we report three experiments designed to test one of the premises of the WM-based processing analysis: that the strength of island effects should vary as a function of individual differences in WM capacity. The results show that island effects present even for L2 learners are more likely attributed to grammatical constraints than to limited processing resources. 
This paper probes into the issue of deverbalization in Chinese by starting from two potential and innovative uses of deverbalization in Mainland Mandarin and Taiwan Mandarin, respectively. Then, we move to the exploration of various nominal categories in Chinese, with regard to their grammatical behaviors as well as their ontological differences. Crucially, we find that nominal categories in Chinese diverge upon individualization, which can be realized along either spatial or temporal dimension, as evidenced by the application of different types of classifiers. Specifically, event nouns and deverbal nouns allow temporal individualization only, while xingwei-marked nouns are exclusively compatible with spatial individualization. By contrast, entity nouns and dongzuomarked nouns allow both spatial and temporal individualization. Hence, individualization is the key to our understanding of nominal categories in Chinese. 
 Munpyo Hong Dept. of German Linguistics & Literature, Sungkyunkwan University / 25-2, Sungkyunkwan-Ro, Jongno-Gu, Seoul, Korea skkhmp@skku.edu  Abstract Korean is one of the well-known „pro-drop‟ languages. When translating Korean zero object into languages in which objects have to be overtly expressed, the resolution of zero object is crucial. This paper proposes a machine learning method to resolve Korean zero object. We proposed 8 linguistically motivated features for ML (Machine Learning). Our approach has been implemented with WEKA 3.6.10 and evaluated by using 10-fold cross validation method. The accuracy of the proposed method reached 73.37%. 
For relieving data sparsity problem, Hierarchical Word Sequence (abbreviated as HWS) language model, which uses word frequency information to convert raw sentences into special n-gram sequences, can be viewed as an effective alternative to normal n-gram method. In this paper, we use directional information to make HWS models more syntactically appropriate so that higher performance can be achieved. For evaluation, we perform intrinsic and extrinsic experiments, both verify the effectiveness of our improved model. 
Neural network language models (NNLMs) have been shown to outperform traditional ngram language model. However, too high computational cost of NNLMs becomes the main obstacle of directly integrating it into pinyin IME that normally requires a real-time response. In this paper, an efﬁcient solution is proposed by converting NNLMs into back-off n-gram language models, and we integrate the converted NNLM into pinyin IME. Our experimental results show that the proposed method gives better decoding predictive performance for pinyin IME with satisﬁed efﬁciency. 
Due to the increasing popularity of microblogging platforms (e.g., Twitter), detecting realtime news from microblogs (e.g., tweets) has recently drawn a lot of attention. Most of the previous work on this subject detect news by analyzing propagation patterns of microblogs. This approach has two limitations: (i) many non-news microblogs (e.g. marketing activities) have propagation patterns similar to news microblogs and therefore they can be falsely reported as news; (ii) using propagation patterns to identify news involves a time delay until the pattern is formed, therefore news are not detected in real time. We propose an alternative approach, which, motivated by the necessity of real-time detection of news, does not rely on propagation of posts. Moreover, we propose a real-time sorting strategy that orders the detected news microblogs using a translational approach. An experimental evaluation on a large-scale microblogging dataset demonstrates the effectiveness of our approach. 
In this paper, we propose a method for extracting trouble information from Twitter. One useful approach is based on machine learning techniques such as SVMs. However, trouble information is a fraction of a percent of all tweets on Twitter. In general, imbalanced distribution is not suitable for machine learning techniques to generate a classiﬁer. Another approach is to extract trouble information by using handwritten rules. However, constructing high coverage rules by handwork is costly. First, we verify these problems in a preliminary experiment. Then, to solve these problems, we apply a bootstrapping method to our trouble information extraction task. We introduce three characteristics and a scoring method to the bootstrapping. As a result, the iteration process on the bootstrapping increased the number of tweets and patterns for trouble information dramatically. 
Our purpose is to use Twitter data to infer personal values in marketing for Japanese consumers. In this paper, we reintroduce our personal value system and apply the model for inferring personal values with tweets. To adapt the model to the rapid change of wording in tweets, we propose a dynamic model based on time-weighted frequency in this research. We evaluated the prediction results from our previous approach, newly proposed approach (the dynamic model), and other methods with 10-fold cross-validation. Our experiment results show that personal values can be inferred from Twitter data, and our approach based on Bayesian network performs well with skewed training data. 
 Event-speciﬁc twitter streams often reveal sudden spikes triggered by users’ upsurge of emotions to crucial moments in the real world. Although upsurge of emotion is usually identiﬁed by a sudden rise in the number of tweets, the detection for diverse event streams is not a trivial task. In this paper, we propose a new method to extract spiking tweets with upsurge of emotions based on characteristic expressions used in tweets. The core part of our method is to use a distant-supervised language model (Spike LM) built from tweets in spikes to capture such expressions. We investigate the performance of detecting emotional spiking tweets using language models including Spike LM. Our experimental results show that the natural language expressions used in emotional upsurge ﬁt speciﬁcally well to Spike LM. 
In regard to document classiﬁcation, semisupervised learning using the Naive Bayes method and EM algorithm was a great success, and we refer to this method as NBEM in this paper. Although NBEM is also effective for domain adaption of document classiﬁcation, there is still room for improvement because NBEM does not employ valuable information for this task, that is the difference between source domain and target domain. Here, according to the similarity between the label distribution of the feature on source domain and the estimated label distribution of the feature on target domain, we set the weight on the features to reconstruct the training data. We use this reconstructed training data to perform document classiﬁcation by NBEM. As a result of experiment by using a part of 20 Newsgroups, the effect of this method was conﬁrmed. 
Paraphrase detection has numerous important applications in natural language processing (such as clustering, summarizing, and detecting plagiarism). One approach to detecting paraphrases is to use predicate argument tuples. Although this approach achieves high paraphrase recall, its accuracy is generally low. Other approaches focus on matching similar words, but word meaning is often contextual (e.g., ‘get along with,’ ‘look forward to’). An effective approach to detecting plagiarism would take into account the fact that plagiarists frequently cut and paste whole phrases and/or replace several words with similar words. This generally results in the paraphrased text containing identical phrases and similar words. Moreover, plagiarists usually insert and/or remove various minor words (prepositions, conjunctions, etc.) to both improve the naturalness and disguise the paraphrasing. We have developed a similarity matching (SimM at) metric for detecting paraphrases that is based on matching identical phrases and similar words and quantifying the minor words. The metric achieved the highest paraphrase detection accuracy (77.6%) when it was combined with eight standard machine translation metrics. This accuracy is better than the 77.4% rate achieved with the state-of-the-art approach for paraphrase detection. 
In this paper we propose a method for a rating prediction task. Each review consists of several ratings for a product, namely aspects. To predict the ratings of the aspects, we utilize not only aspect words, but also aspect sentences. First, our method detects aspect sentences by using a machine learning technique. Then, it incorporates words extracted from aspect sentences with aspect word features. For estimating aspect likelihood of each word, we utilize the variance of words among aspects. Finally, it generates classiﬁers for each aspect by using the extracted features based on the aspect likelihood. Experimental result shows the effectiveness of features from aspect sentences. 
Online user reviews describing various products and services are now abundant on the web. While the information conveyed through review texts and ratings is easily comprehensible, there is a wealth of hidden information in them that is not immediately obvious. In this study, we unlock this hidden value behind user reviews to understand the various dimensions along which users rate products. We learn a set of users that represent each of these dimensions and use their ratings to predict product ratings. Speciﬁcally, we work with restaurant reviews to identify users whose ratings are inﬂuenced by dimensions like ‘Service’, ‘Atmosphere’ etc. in order to predict restaurant ratings and understand the variation in rating behaviour across different cuisines. While previous approaches to obtaining product ratings require either a large number of user ratings or a few review texts, we show that it is possible to predict ratings with few user ratings and no review text. Our experiments show that our approach outperforms other conventional methods by 16-27% in terms of RMSE. 
In this paper, a cross-lingual pseudo relevance feedback (PRF) model based on weak relevant topic alignment (WRTA) is proposed for cross language query expansion on unparallel web pages. Topics in different languages are aligned on the basis of translation. Useful expansion terms are extracted from weak relevant topics according to the bilingual term similarity. Experiment results on web-derived unparalell data show the contribution of the WRTA-based PRF model to cross language information retrieval. 
Recognizing spatial information associated with events expressed in natural language text is essential for the proper interpretation of such events. However, the associations between events and spatial information found throughout the text have been much less studied than other types of spatial association as looked into in SpatialML and ISO-Space. In this paper, we present an annotation framework for the linguistic analysis of the associations between event mentions and spatial expressions in broadcast news articles. Based on the corpus annotation and analysis, we discuss which information should be included in the guidelines and what makes it difficult to achieve a high inter-annotator agreement. We also discuss possible improvements on the current corpus and annotation framework for insights into developing an automated system. 
After the Great East Japan Earthquake in 2011, an abundance of false rumors were disseminated on Twitter that actually hindered rescue activities. This work presents a method for recognizing the negation of predicates on Twitter to ﬁnd Japanese tweets that refute false rumors. We assume that the predicate “occur” is negated in the sentence “The guy who tweeted that a nuclear explosion occurred has watched too many SF movies.” The challenge is in the treatment of such complex negation. We have to recognize a wide range of complex negation expressions such as “it is theoretically impossible that...” and “The guy who... watched too many SF movies.” We tackle this problem using a combination of a supervised classiﬁer and clusters of n-grams derived from large un-annotated corpora. The n-gram clusters give us a gain of about 22% in F-score for complex negations. 
Suicide is one of major public health problems worldwide. Traditionally, suicidal ideation is assessed by surveys or interviews, which lacks of a real-time assessment of personal mental state. Online social networks, with large amount of user-generated data, offer opportunities to gain insights of suicide assessment and prevention. In this paper, we explore potentiality to identify and monitor suicide expressed in microblog on social networks. First, we identify users who have committed suicide and collect millions of microblogs from social networks. Second, we build suicide psychological lexicon by psychological standards and word embedding technique. Third, by leveraging both language styles and online behaviors, we employ Topic Model and other machine learning algorithms to identify suicidal ideation. Our approach achieves the best results on topic-500, yielding F1 − measure of 80.0%, Precision of 87.1%, Recall of 73.9%, and Accuracy of 93.2%. Furthermore, a prototype system for monitoring suicidal ideation on several social networks is deployed. 
 This paper develops a technique that unfolds public mood on social issues from real-time social media for sector index prediction. We first propose a low-dimensional support vector machine (SVM) classifier using surrounding information for twitter sentiment classification. Then, we generate public mood time series by aggregating message-level weighted daily mood (WDM) based on the sentiment classification results. Lastly, we evaluate our method against the real stock index in two kinds of time periods (fluctuating and monotonous) separately using static cross-correlation coefficient (CCF) and dynamic vector auto-regression (VAR). The experiments on “food safety” issue show that the proposed WDM method outperforms the wordlevel baseline method in predicting stock movement, especially during fluctuating period. 
 Task  Errors Tokens Error Rate  We address the problem of class imbalance in supervised grammatical error detection (GED) for non-native speaker text, which  Article Noun Number Preposition∗  6658 234,695 3379 245,026 1955 123,419  2.84% 1.38% 1.58%  is the result of the low proportion of erroneous examples compared to a large number of error-free examples. Most learn-  Table 1: Error rates for GED tasks (NUCLE) ∗ statistics for 10 most frequent prepositions  ing algorithms maximize accuracy which  is not a suitable objective for such imbalanced data. For GED, most systems ad-  the head noun of a noun phrase has the correct or incorrect grammatical number. As opposed  dress this issue by tuning hyperparame-  to rule-based methods, classiﬁcation methods can  ters to maximize metrics like Fβ. Instead, we show that learning classiﬁers that di-  easily incorporate complex and arbitrary evidence as features. Some of the best performing sys-  rectly learn model parameters by optimiz-  tems for the most frequent grammatical errors, viz.  ing evaluation metrics like F1 and F2 score deliver better performance on these met-  noun number, article and preposition errors, are classiﬁcation systems (Ng et al., 2013).  rics as compared to traditional sampling and cost-sensitive learning solutions for addressing class imbalance. Optimizing these metrics is useful in recall-oriented grammar error detection scenarios. We also show that there are inherent difﬁculties in optimizing precision-oriented evaluation metrics like F0.5. We establish this through a systematic evaluation on multiple datasets and different GED tasks.  However, a major problem in learning classiﬁers from an annotated learner corpus is the very low error rate (number of errors per token) in the corpus (Chodorow et al., 2012). For instance, the error rates are less than 3% for noun-number, article and preposition errors in the NUCLE annotated learner corpus (Dahlmeier et al., 2013) as shown in Table 1. Such low error rates (less than 5%) have been observed across various learner corpora (see Table 2) viz. the NUCLE corpus (Dahlmeier  
Online adspaces require the seller/buyer to select a category to post their advertisements. This practice not only causes hindrance to legitimate users while posting their advertisements, but, also deter the user experience as they will see a lot of non-categorized ads due to human error or spam. Classifying advertisements posted on an online adspace can help in spam detection, better information propagation which in turn enhances user experience.  Craigslist Ebay  users 500M+ 300M+  Countries posts/month  70+  80M+  40  50M+  Table 1: Craigslist vs Ebay  are then further subdivided into categories. We chose Craigslist as they have made some of their data publicly available is one of the largest online adspaces currently operating as shown by the statistics in the table below. However, this method can be applied to any online adpsace.  Craigslist is a prevalent platform for local classiﬁed advertisements. In this paper, we present a classiﬁcation system for an online advertisement space such as Craigslist. We show the performance of our algorithm with three standard classiﬁers, viz., Support Vector Machine, Random Forest, and Multinomial Naive Bayes. An accuracy of 84% was achieved with the SVM.  As the table indicates, Craigslist is a more data rich source of advertisements. One reason for this is that Craigslist doesn’t limit itself to goods only whereas other e-commerce websites do. We use supervised learning techniques to classify a Craigslist post into different categories. We use text mining techniques like text normalization(Sproat et al., 2001) and term frequency inverse document frequency for preparing the data for classiﬁcation. We tested several classiﬁcation  
 Sarkar, 2005), (McDonald et al., 2005) report re-  sults of more than 94% F-measure.  Information Extraction from Indian lan-  However, the scenario is different for many In-  guages requires effective shallow parsing, especially identiﬁcation of “meaningful” noun phrases. Particularly, for an agglu-  dian languages. Their sufﬁx agglutinative and free word order nature makes it challenging to identify the correct phrases. In this paper, we focus  tinative and free word order language like Marathi, this problem is quite challenging. We model this task of extracting noun  on the problem of identifying noun phrases from Marathi, a highly agglutinative Indian language. Marathi is spoken by more than 70 million people  phrases as a sequence labelling problem. A Distant Supervision framework is used to automatically create a large labelled  worldwide 1 and it exhibits a large web presence in terms of e-newspapers, Marathi Wikipedia, blogs, social network banter and much more. There are  data for training the sequence labelling model. The framework exploits a set of heuristic rules based on corpus statis-  various motivations for the problem and the most important one being the lack of domain speciﬁc information extraction systems in Indian Languages.  tics for the automatic labelling. Our approach puts together the beneﬁts of heuristic rules, a large unlabelled corpus as well  The problem also poses challenges in terms of NLP resource-poor nature of Indian languages and a complex sufﬁx agglutination scheme in Marathi.  as supervised learning to model complex underlying characteristics of noun phrase occurrences. In comparison to a simple English-like chunking baseline and a publicly available Marathi Shallow Parser, our method demonstrates a better performance.  Keeping these challenges in sight, we propose the use of distantly supervised approach for the task. The task of identifying meaningful noun phrases in Marathi becomes challenging due to another fact that two different noun phrases can be written adjacently in a sentence. Such noun phrases are individually meaningful but their con-  
 and multi-label classiﬁcation tasks. In the for-  A Self-Organizing Map was used to classify the Reuters Corpus, by assigning a label to each of the documents that cluster to a speciﬁc node in the Self-Organizing Map. The predicted label is based on the most frequent label among the training documents attributed to that particular node. Experiments were carried out on different grid sizes (node numbers) to determine their inﬂuence on classiﬁcation results. Informative visualizations of the resulting Self-Organizing Maps are demonstrated. We argue that the Self-Organizing Map is well suited to classify a document collection in which many documents simultaneously belong to several categories. 
The problem of Word Sense Disambiguation (WSD) can be defined as the task of assigning the most appropriate sense to the polysemous word within a given context. Many supervised, unsupervised and semi-supervised approaches have been devised to deal with this problem, particularly, for the English language. However, this is not the case for Hindi language, where not much work has been done. In this paper, a new approach has been developed to perform disambiguation in Hindi language. For training the system, the text in Hindi language is converted into Hyperspace Analogue to Language (HAL) vectors, thereby, mapping each word into a high-dimensional space. We also deal with the fuzziness involved in disambiguation of words. We apply Fuzzy C-Means Clustering algorithm to form clusters denoting the various contexts in which the polysemous word may occur. The test data is then mapped into the high dimensional space created during the training phase. We test our approach on the corpus created using Hindi news articles and Wikipedia. We compare our approach with other significant approaches available in the literature and the experimental results indicate that our approach outperforms all the previous works done for Hindi Language. 1. Introduction: Words in a language may carry more than one sense. Human beings can easily decipher the  context in which the word is being used in a sentence. However, the same cannot be said for the machines. Various applications like speech processing, text processing, search engines, etc, in order to function properly, need to figure out the sense of the word. Thus there is a need for word sense disambiguation for correctly interpreting the meaning of a sentence written in natural language. Given a word and its possible senses, as defined in a knowledge base, the problem of Word Sense Disambiguation (WSD) can be defined as the process of identifying which sense of a word (i.e. meaning) is used in a sentence, when the word has multiple meanings. It was first formulated as a distinct computational task during the early days of machine translation in the 1940s, making it one of the oldest problems in computational linguistics. Warren Weaver (1949), in his famous 1949 memorandum on translation, first introduced the problem in a computational context. Early researchers understood the significance and difficulty of WSD well. Since then there have been various approaches for handling the problem of Word Sense Disambiguation. Majorly, WSD can be done using Knowledge Based Approaches (Navigli, 2009), Machine Based Approaches (Navigli, 2009) and Hybrid Approach (Navigli, 2009). Knowledge-based methods rely primarily on dictionaries, thesauri, and lexical knowledge bases, without using any corpus evidence. Lesk Algorithm (1986) is a classical approach based on Knowledge bases.  49 D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 49–58, Trivandrum, India. December 2015. c 2015 NLP Association of India (NLPAI)  Semi-supervised or minimally supervised methods make use of a secondary source of knowledge such as a small annotated corpus as seed data in a bootstrapping process, or a wordaligned bilingual corpus. Yarowsky Algorithm (1995) is based on this approach. Supervised methods make use of sense-annotated corpora to train from while unsupervised methods (Schütze, 1998) are based on the assumption that similar senses occur in similar contexts, and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context. Although much work is available for WSD in English language, but for Hindi language very few research works have been contributed. Sinha, Reddy and Bhattacharya (2012) did a statistical approach towards Word Sense Disambiguation in Hindi. In their work, a set of context words are selected using the surrounding window and for each sense w of a word, a semantic bag is created by referring to the Hindi WordNet® (hypernymy, hyponymy and meronymy). They claim that sense of the word that has the maximum overlap between the context bag and the semantic bag is the correct sense. But, their system does not detect the underlying similarity in presence of morphological variations. Kumari and Singh (2013) used genetic algorithm to perform word sense disambiguation on Hindi nouns. Genetic algorithm is a heuristic search algorithm used to find approximate solutions to optimization and search problems using techniques inspired by evolutionary biology. But in their work, the recall values associated with the algorithm depend upon the genetic parameters chosen for evaluation and therefore is not universally applicable. Yadav and Vishwakarma (2013) use association rules to first mine the itemsets depending upon the context of the ambiguous word and then mine the association rule corresponding to the most frequent itemset. 50  Tomar et al. (2013) use the technique of PLSA for making ‘k’ clusters representing the senses or the different contexts of the word. The clusters are then further enriched using the Hindi WordNet®. The cluster with which the maximum similarity score (cosine distance) is obtained gives the correct sense of the ambiguous word. The performance of their work varies linearly with the amount of training data used. In (2013), Jain, Yadav and Tayal used graph based approach for Word Sense Disambiguation in Hindi Text. Here, to construct a graph for the sentence each sense of the ambiguous word is taken as a source node and all the paths which connect the sense to other words present in the sentence are added. The importance of nodes in the constructed graph is identified using node neighbor based measures and graph clustering based measures. This method disambiguates all open class words and disambiguates all the words present in the sentence simultaneously. In (2005) , Hao Chen, Tingting He, Donghong Ji and Changqin Quan used an unsupervised approach, for disambiguating words in Chinese Language, where contexts that include ambiguous words are converted into vectors by means of a second-order context method, and these context vectors are then clustered by the kmeans clustering algorithm and lastly, the ambiguous words can be disambiguated after a similarity calculation process is completed. But, the K-means clustering limits the word to belong to only one cluster and hence also limits the accuracy of Word Sense Disambiguation. After going through the literature survey, we found that all the methodologies for WSD used till date do not take into account the fuzziness involved in disambiguation of the words. In this paper, we develop an approach whereby we first train our system taking Hindi newspaper and Wikipedia articles as input and use Hyperspace Analogue to Language model to convert Hindi words into vectors, representing points in the high dimensional Hyperspace.  Fuzzy C-Means Clustering is then applied to get the clusters where each word may belong to more than one cluster with an associated membership value. The words belonging to one context are grouped together in a cluster. The polysemous words belong to more than one cluster, each cluster corresponding to the possible sense of that word. Once the training of the system is complete, the test data containing the polysemous word is processed using Hyperspace Analogue to Language (HAL) model to map the polysemous word of the test data as a point in the hyperspace created in the training phase. Then, Euclidean Distance is calculated between this point and all the cluster centers and the nearest cluster corresponds to the sense of the polysemous word used in the test data. And similar computation can be easily performed for each polysemous word of the test data in order to determine the correct sense of that word. We tested our approach using Netbeans IDE to develop a Hyperspace Analogue to Language (HAL) Model and MATLAB for construction of fuzzy clusters and finding the nearest cluster. The results obtained were compared with all of the previous approaches and our approach shows the best results. The remainder of the paper is organized as follows: Section 2 provides a review of Hyperspace Analogue to Language, Fuzzy Logic and Fuzzy C-Means Clustering Algorithm. Section 3 describes the specifics of the proposed algorithm to carry out word sense disambiguation. Section 4 illustrates the application of the proposed approach on a small data set. Section 5 describes the experimental results obtained for our approach and the compares it with already existing techniques for Word Sense Disambiguation for Hindi language. Finally, the last section concludes the paper, and makes some suggestions for future work. 2 Preliminaries: 2.1 Hyperspace Analogue to Language (HAL): 51  Hyperspace Analogue to Language is a numeric method developed by Kevin Lund and Curt Burgess (1996) for analyzing text. It does so by running a sliding window of fixed length across a text, calculating a matrix of word co-occurrence values along the way. The basic premise that the work relies on is that words with similar meanings repeatedly occur closely (also known as co-occurrence). Given a N word vocabulary, the HAL space is a N × N matrix constructed by moving a window of length l over the corpus by one word increments. Given two words W1 and W2, whose distance within the window is d, the weight of association between them is computed by l − d + 1. After traversing the corpus, an accumulated cooccurrence matrix for all the words in a target vocabulary is produced. HAL is direction sensitive: the co-occurrence information for words preceding each word and co-occurrence information for words following each word are recorded separately by row and column vectors. 2.1.1 Illustration: Consider the following piece of Hindi Text: “भारत की राजधानी दिल्ली है। दिल्ली मे अनेक वर्ग के लोर् है।”  HAL Matrix constructed is as follows: भारत राजधानी दिल्ली वर्ग लोर्  भारत  -  -  -  - -  राजधानी 4 -  -  - -  दिल्ली 4 8  -  - -  वर्ग  -  -  4  - -  लोर्  -  -  
 semi-supervised and unsupervised. Supervised  WSD approaches (Lee et al., 2004; Ng and Lee,  Unsupervised Word Sense Disambiguation (WSD) is one of the challenging problems in natural language processing. Recently, an unsupervised bilingual WSD ap-  1996) always perform better because of the availability of the sense-annotated data. However, the cost of creation of the sense-annotated data limits their applicability to only a few resource rich lan-  proach has been proposed. This approach  guages. On the other hand, semi-supervised ap-  uses context aware EM formulation for estimating the sense distribution by using the co-occurrence counts of cross-linked  proaches (Yarowsky, 1995; Khapra et al., 2010) provide a ﬁne balance in terms of resource requirements and accuracy, but they still rely on  words in comparable corpora. WordNet-  some amount of sense-annotated data. There-  based similarity measures are used for approximating the co-occurrence counts. In this paper, we explore the feasibility of the use of Word Embeddings for approx-  fore, despite of the less accuracy, much focus is given for unsupervised WSD algorithms (Diab and Resnik, 2002; Kaji and Morimoto, 2002; Mihalcea et al., 2004; Jean, 2004; Khapra et al.,  imating these counts, which is an exten-  2011). These algorithms do not need any sense-  sion to the existing approach. We evaluated our approach for Hindi-Marathi language pair, on Health domain. On us-  annotated data for the disambiguation. Moreover, they make use of lexical knowledge resources or comparable/parallel corpora for training the al-  ing the combination of Word Embeddings  gorithm (Kaji and Morimoto, 2002; Diab and  and WordNet-based similarity measures, we observed 8.5% and 2.5% improvement in the F-score of verbs and adjectives respectively for Marathi and 7% improvement in the F-score of adjectives for Hindi. The experiments show that the combination of Word Embeddings and WordNetbased similarity measures is a reasonable approximation for the bilingual WSD.  Resnik, 2002; Specia et al., 2005; Lefever and Hoste, 2010; Khapra et al., 2011). Khapra et al. (2011) have shown that how two resource deprived languages can help each other in WSD without using any sense-annotated data in either of the languages. Here, the intuition is that, the sense distribution remains same across languages when the comparable corpora is provided. They used the Expectation Maximization  
We conduct a cross-modal priming experiment to determine the mental representation and access strategies for compound verbs (CV) in Bangla. Analysis of reaction time indicates that compositionality among CVs triggers priming effects for both the constituent verbs. On the other hand non-compositional CVs exhibit priming only for the polar verb. Thus, compositional CVs are decomposed into their constituent verbs during processing. On the other hand, non-compositional verb phrases are represented and accessed as a whole in the minds of a Bangla speaker. The reaction time data thus collected are used to evaluate our vector space model for compositionality judgment. 
India is a country with diverse culture, language and varied heritage. Due to this, it is very rich in languages and their dialects. Being a multilingual society, a dictionary in multiple languages becomes its need and one of the major resources to support a language. There are dictionaries for many Indian languages, but very few are available in multiple languages. WordNet is one of the most prominent lexical resources in the field of Natural Language Processing. IndoWordNet is an integrated multilingual WordNet for Indian languages. These WordNet resources are used by researchers to experiment and resolve the issues in multilinguality through computation. However, there are few cases where WordNet is used by the non-researchers or general public. This paper focuses on providing an online interface – IndoWordNet Dictionary to nonresearchers as well as researchers. It is developed to render multilingual WordNet information of 19 Indian languages in a dictionary format. The WordNet information is rendered in multiple views such as: sense  based, thesaurus based, word usage based and language based. English WordNet information is also rendered using this interface. The IndoWordNet dictionary will help users to know meanings of a word in multiple Indian languages. 
Cross Lingual Word Semantic (CLWS) similarity is deﬁned as a task to ﬁnd the semantic similarity between two words across languages. Semantic similarity has been very popular in computing the similarity between two words in same language. CLWS similarity will prove to be very effective in the area of Cross Lingual Information Retrieval, Machine Translation, Cross Lingual Word Sense Disambiguation, etc.  languages i.e. Language Ls and Lt, where Ls is treated as resourceful language and Lt is treated as resource scarce language. Given two words in language Ls and Lt the CLWS Similarity engine will analyze which two concepts of the word from Ls and Lt are similar. Let m & n be the number of concepts for the word in Ls & Lt respectively then, the output will generate a sorted list of all the possible combination of concepts (i.e. m ∗ n sorted list) ordered by their similarity score and the topmost combination of the concepts from Ls and Lt are similar to each other. The system is developed and tested for the English and Hindi lan-  In this paper, we discuss a system that  guage where English is Ls and Hindi is Lt.  is developed to compute CLWS similarity  of words between two languages, where  one language is treated as resourceful and other is resource scarce. The system is de-  1.1 Semantic Similarity  veloped using WordNet. The intuition be-  hind this system is that, two words are semantically similar if their senses are similar to each other. The system is tested for English and Hindi with the accuracy 60.5% precision@1 and 72.91% precision@3.  Lot of research effort has been devoted to design semantic similarity measures having monolingual as parameter. WordNet has been widely adopted in semantic similarity measures for English due its large hierarchical organization of synsets. Monolingual semantic similarity can be  
 tic features have been explored (Adafre and de Ri-  A bottleneck for medical domain Temporal Expression Recognition (TER) is the availability of data. An open-domain TER system may not be able to capture domain-speciﬁc expressions, while domain-speciﬁc TER may be cumbersome to implement. We present a novel neural network based medical TER system that uses corpora from news and medical domains. Thus, it serves as a middle ground between an open-domain and a domain-speciﬁc TER. We show that our system outperforms state-of-art open-domain baselines, and gets close to domain-speciﬁc skylines. Thus, our system proves to be a promising alternative for domain speciﬁc TER for domains where data may be limited.  jke, 2005; Bethard, 2013). Joint inference-based classiﬁers like Markov Logic have also been reported (UzZaman and Allen, 2010). Medical domain TER (Sun et al., 2013; Bethard et al., 2015) has resulted in alternate methods and systems for detecting and normalizing temporal expressions. Our system uses a neural network based architecture which has hitherto not been used for TER. In addition, we also deal with a speciﬁc situation: Indomain data being difﬁcult to obtain. Research in TER mostly deals with news domain text, arguably because of availability of large corpora and abundance of temporal expressions in news documents. In recent times, TER has also been applied to other domains like medical. Approaches for medical domain TER in the past have been either rule-based (Sohn et al., 2013; Jindal and Roth, 2013), statistical (Xu et al., 2013; Roberts et al., 2013) or hybrid (Lin et al., 2013).  
The inventory of senses for a given word changes over time – tweet has gained the ‘Twitter post’ sense only relatively recently and this paper addresses the problem of the computational detection of such change. We propose a generative model which conditions context words on a target expression’s sense and conditions the sense choice on the time of writing. We develop an EM algorithm to estimate the parameters from raw time-stamped ngram data with no sense annotation. We are able to demonstrate the inference of parameters which plausibly reﬂect the objective dynamics of sense use frequencies, in particular the emergence of a new sense. 
 ing the sparsity problem is more challenging task.  SMT approaches face the problem of data sparsity while translating into a morphologically rich language. It is very unlikely for a parallel corpus to contain all morphological forms of words. We propose a solution to generate these unseen morphological forms and inject them into original training corpora. We observe that morphology injection improves the quality of translation in terms of both adequacy and ﬂuency. We verify this with the experiments on two morphologically rich languages: Hindi and Marathi, while translating from English.  In this paper, we propose a simple and effective solution of enriching the input corpora with various morphological forms of words. We perform experiments with factored models (Koehn and Hoang, 2007) as well as unfactored models, i.e., phrase-based models (Koehn, Och and Marcu, 2003) while translating from English to Hindi and English to Marathi. Results show that morphology injection performs very well in order to solve the sparsity problem. The rest of the paper is organized as follows: We present related work in Section 2. Then, we study the basics of factored translation models in Section 3. We also describe a general factored model for handling morphology. Then, we discuss  
 in Bengali written by three eminent Bengali  We describe Authorship Attribution of Bengali literary text. Our contributions include a new corpus of 3,000 passages written by three Bengali authors, an end-to-end  authors (Section 3). • Authorship Attribution System: a classiﬁcation system based on character bigrams that achieves 98% accuracy on held-out data (Section 4).  system for authorship classiﬁcation based on character n-grams, feature selection for authorship attribution, feature ranking and analysis, and learning curve to assess the relationship between amount of training data  • Feature Selection: six types of lexical n-gram features, and selection of the best-performing combination on an independent development set (Section 5).  and test accuracy. We achieve state-of-theart results on held-out dataset, thus indicating  • Learning Curve: how the performance on heldout data changes as the number of training  that lexical n-gram features are unarguably the  instances varies (Section 6).  best discriminators for authorship attribution of Bengali literary text.  • Feature Ranking: most discriminative lexical features by Information Gain (Section 7).  
 ing professor on modern agricultural tools  We present TransChat1, an open source, cross platform, Indian language Instant Messaging (IM) application that facilitates cross lingual textual communication over English and multiple Indian Languages. The application is a client-server IM architecture based chat system with multiple Statistical Machine Translation (SMT) engines working towards efficient translation and transmission of messages. TransChat allows users to select their preferred language and internally, selects appropriate translation engine based on the input configuration. For translation quality enhancement, necessary pre- and post-processing steps are applied on the input and output chattexts. We demonstrate the efficacy of TransChat through a series of qualitative evaluations that test- (a) The usability of the system (b) The quality of the translation output. In a multilingual country like India, such applications can help overcome language barrier in domains like tourism, agriculture and health.  and techniques, communication is often hindered by language barrier. This problem has been recognized and well-studied by computational linguists as a result of which a large number of automatic translation systems have been proposed and modified in the last 30 years. Some of the notable Indian Language translation systems include Anglabharati (Sinha et al., 1995), Anusaraka (Padmanathrao, 2009), Sampark (Anthes, 2010) and Sata-Anuvaadak2 (Kunchukuttan et al., 2014). Popular organizations like Google and Microsoft also provide translation solutions for Indian languages through Google- and BingTranslation systems. Stymne (2011) demonstrate techniques for replacement of unknown words and data cleaning for Haitian Creole SMS translation, which can be utilized in a chat scenario. But even after so many years of MT research, one can still claim that these systems have not been able to attract a lot of users. This can be attributed to factors like(a) poor user experience in terms of UI design, (b) systems being highly computationalresource intensive and slow in terms of response time, and (c) bad quality translation output. Moreover, the current MT interfaces do not provide a natural environment to at-  
 First cry of an infant gives signiﬁcant informa-  Infant cry is a mode of communication, for interacting and drawing attention. The infants cry due to physiological, emotional or some ailment reasons. Cry involves high pitch changes in the signal. In this paper we describe an ‘Infant Cry Sounds Database’ (ICSD), collected especially for the study of likely cause of an infant’s cry. The database consists of infant cry sounds due to six causes: pain, discomfort, emotional need, ailment, environmental factors and hunger/thirst. The ground truth cause of cry is established with the help of two medical experts and parents of the infants. Preliminary analysis is carried out using the sound production features, the instantaneous fundamental frequency and frame energy derived from the cry acoustic signal, using auto correlation and linear prediction (LP) analysis. Spectrograms give the base reference. The infant cry sounds due to pain and discomfort are distinguished. The database should be helpful towards automated diagnosis of the causes of infant cry. 
 Naive Bayes classiﬁers estimate posterior probabilities poorly (Zhang, 2004).  P (C|F ) =  i P (fi|C) × P (C) P (F )  (4)  In this paper, we propose a modiﬁcation to the Naive Bayes classiﬁcation algorithm which improves the classiﬁer’s posterior probability estimates without affecting its performance. Since the modiﬁcation involves the use of the reciprocal of the perplexity of the class-conditional feature probabilities, we call the resulting classiﬁer the Perplexed Bayes classiﬁer.  The posterior probability estimates obtained using Equation 4 tend to be extreme as observed in Eyheramendy et al (2003). Improving the posterior probability estimates of Naive Bayes classiﬁers might make them more useful for NLP (Nguyen and O’Connor, 1999). In this paper, we present the Perplexed Bayes classiﬁcation algorithm that produces better calibrated posterior probabilities than the Naive Bayes algorithm and operates with the same accuracy.  We demonstrate that the modiﬁcation re-  2 Related Work  sults in better calibrated posterior probabilities on a gender categorization task.  The Naive Bayes classiﬁcation algorithm is still commonly used as a baseline algorithm for many  
 networks based on the TER algorithm (Rosti et  al., 2008) yields significant improvement in  The present work reports system combination  BLEU score on the GALE test sets. Multi-  task for the Chinese-English statistical machine translation systems. We focus on the strategy to build the candidate systems to enhance the gain of BLEU score by introducing diversity at the early stage of the system combination. One of the most effective strategies is to carry out system combination of the various systems with different word alignment algorithm. Our approach differs  objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques has proven to be useful approach in Chinese-English data sets (Xia et al., 2013). We explore two approaches of system combination (a) using the word alignment and its different symmetrization method for both phrase  from previous work in one important aspect  based and hierarchical SMT paradigms to build  that we report on the diversity of the align-  candidate systems if they are complementary to  ment refinement heuristics of word alignment techniques that are complementary to each other for the system combination. This approach could harness several word alignment possibilities and proved to be beneficial in generating consensus translation where the acting backbone which determines the word  each other for the system combination by sampling the outputs (b) using jointly trained HMM based SMT candidate systems together with MGIZA++ based best candidate systems for system combination for both phrase based and hierarchical SMT paradigms.  order is permitted to switch after each word. We carried out experiments on candidate sys-  2 Related Work  tems of phrasal and hierarchical paradigms and system combination of both the paradigms as well. To our surprise, the combo systems using the various word alignments with various symmetrization techniques of  A multiple string alignment algorithm is used to compute a single confusion network to generate a consensus hypothesis through majority voting by (Bangalore et al., 2001). There are other sys-  both the MT paradigms show gain of 0.8 to  tem combination techniques that uses TER  2.07 absolute BLEU score against the best  (Snover et al., 2006) or ITG( Karakos et al.,  candidates of the respective test sets.  2008) to align system outputs. The confusion  
 tion is concerned only about recognizing the sen-  There are words that change its polarity from domain to domain. For example, the word deadly is of positive polarity in the cricket domain as in “Shane Warne is a ‘deadly’ leg spinner”. However, ‘I witnessed a deadly accident’ carries negative polarity and going by the sentiment in cricket domain will be misleading. In addition to this, there exist domainspeciﬁc words, which have the same polarity across domains, but are used very frequently in a particular domain. For example, blockbuster, is speciﬁc to the movie domain. We combine such words as Do-  timent information received for automobiles only. Therefore, a list of Domain Dedicated Polar Words (DDPW) can be proved as the best lexical resource for domain oriented sentiment analysis. Most sentiment analysis applications rely on the Universal Sentiment Lexicons (USL) as a key feature along with additional features (Riloff and Wiebe, 2003). There are many USL resources like senti-word-net1, subjectivity lexicon2 by Wiebe and a list of positive and negative opinion words3 by Liu. These lexicons contain only those words that are usual and have the same polarity across all the domains. These universal sentiment lexicons have the following problems:  main Dedicated Polar Words (DDPW).  • The words that have ﬂuctuating polarity  A concise feature set made up of principal polarity clues makes the classiﬁer less expensive in terms of time complexity and enhances the accuracy of classiﬁcation. In this paper, we show that DDPW  across domains, but have ﬁxed polarity in a domain are strong candidate for the sentiment analysis in that domain. We call such words chameleon words. Consider the following example of ﬂuctuating polarity phenomenon.  make such a concise feature set for senti-  1. The cars steering was unpredictable  ment analysis in a domain. Use of domain-  while driving. (-ve sentiment)  dedicated polar words as features beats the state of art accuracies achieved independently with unigrams, adjectives or Universal Sentiment Lexicon (USL).  2. The story line of Palmetto was unpredictable. (+ve sentiment) The word unpredictable bears negative polarity in the automobile domain, but it is positive  
 for training classifiers. The trained classifier can  then be used for predicting the validity of a first  First name of a person can tell important  name that is differentiating between a valid first  demographic and cultural information about that person. This paper proposes statistical models for extracting vital information that is gender, religion and name validity from Indian first names. Statistical models combine some classical features like ngrams and Levenshtein distance along with some self observed features like vowel score and religion belief. Rigorous  name and an invalid first name. Even absolute gender and religion pre- dictions are impossible as there is no restriction on the naming process with regards to gender and religion. A person of any gender and any faith can identify himself/herself with any name of his/her choice. For example, a Hindu girl can name herself John without breaking any law. For  evaluation of models has been performed  practical purposes, such cases have been left out  through several machine learning  for construction of the statistical models pro-  algorithms to compare the accuracy, FMeasure, Kappa Static and RMS error. Experimental results give promising and favorable results which indicate that these models proposed can be directly used in other information extraction systems.  posed. The models proposed along with the ma- chine learning algorithms can find direct use in information extraction systems and real time applications such as: i. Automatic field suggestions for form  
 According to Ya¯ska (c.6th-5th centuries BCE);  Nirukta (Sarup, 1920) 1.1:  The goal is to build a verb ontology based on Indian grammatical tradition. We propose here an ontological structure to represent verbs in a language, which can be  The principal meaning signiﬁed by the above utterance is the act of cooking - ‘pacati’ and not the subject - ‘devadattaH’ (who is predicted to be cooking).  adapted across languages. This is an ongoing work and presently the method has been applied to develop ontologically informed etymon in English.  In Pa¯n. ini’s derivational system, by means of which utterances (va¯kya) and their components are accounted for, items assigned the name dha¯tu ‘verb, root’ rank as core elements of utterances that  
 grammatical knowledge is required for preventing such errors.  This paper introduces three types of Sta-  2 Negative in Meaning but Not in Form  tistical Machine Translation (SMT) output errors that would require grammatical knowledge for prevention. The first type is due to words that are negative in meaning but not in form. Problems arise when the negative forms are obligatory in target languages. The second type of errors is derived from the rigidity of pattern phrases or correlatives which do not allow for intervening elements. The third type is caused by ellipses in input sentences which must be reinstated for out-  There are several English determiners and adverbs that are negative in meaning but not in form, which are termed negation-implying words in this paper. Negation-implying determiners are little and few, whereas negation-implying adverbs include little, seldom, rarely, scarcely, hardly and barely. This discrepancy between meaning and form causes problems with translation into such languages as Japanese, the grammars of which require the explicit forms of negation.  put sentences when so required by rules of omission in target languages or the difference in Head-Complement order between source and target languages.  2.1 Negation-implying determiners The word few used as a determiner1 as in Few men in (1a) below means “not many”, emphasizing how small a number of people is.:  
 nologies and research areas like Signal Process-  ing, Natural Language Processing, Statistics and  Automatic Speech Recognition (ASR) has  Cognitive Science. Different factors like gender,  received greater level of acceptance as it  emotional state, accent and pronunciation make  creates speech recognition by the human  speech recognition a complex task. The mode  machine interface. This paper focuses on  of articulation, nasality, pitch, volume, and speed  developing a syllable based speech recog-  variability in speech also make speech recogni-  nition system for Malayalam language.  tion a difﬁcult task. Even when speech is recog-  The proposed system consists of three dif-  nized, the accuracy rate can be less due to vari-  ferent phases such as preprocessing, seg-  ous behaviors of speech, usage of words, higher  mentation and classiﬁcation. The prepro-  variability, background noise and the linguistic  cessing is performed for noise reduction,  features of language and speech. This can seri-  DC component removal, pre-emphasis and  ously affect the system performance. There exists  framing. The segmentation process imple-  works which attempt for speaker dependent and  mented using Syllable Segmentation Al-  speaker independent recognition. Our attempt is  gorithm segments the word utterances into  to achieve Speaker independence, which is difﬁ-  syllables, that are inturn fed into the sys-  cult to achieve because in order to recognize the  tem for feature extraction. In the fea-  speech patterns, these models should be trained  ture extraction step, we have proposed a  with speech data of a large group of people.  novel approach by adding energy and zero  Developing an efﬁcient speech recognizer that can  crossing, along with MFCC features. The  exhibit natural capabilities of every human pos-  classiﬁcation is done using Artiﬁcial Neu-  sesses, along with language capability is much  ral Network and is also compared with  harder. As far as Malayalam language is consid-  HMM classiﬁer. Experiments are carried  ered, which has a rich set of vocabulary and the  out with real-time utterances of 100 words,  modern Malayalam alphabet has 15 vowel letters,  and obtained 96.4 % accuracy in ANN,  41 consonant letters, and a few other symbols.  which outperformed HMM.  Malayalam is one of the richest languages in terms  
 depth and then the outlinks are processed for sub-  sequent depths. This process continues for multi-  In this paper, we propose a domain speciﬁc crawler that decides the domain relevance of a URL without downloading the page. In contrast, a focused crawler re-  ple depths to crawl more documents. Every document crawled needs to be processed before retrieval. Processing a document involves the following sequence of steps:  lies on the content of the page to make the  same decision. To achieve this, we use a  Fetching The process of downloading the docu-  classiﬁer model which harnesses features  ment from the web. This is a bandwidth con-  such as the page’s URL and its parents’  suming task.  information to score a page. The classiﬁer model is incrementally trained at each depth in order to learn the facets of the domain. Our approach modiﬁes the focused crawler by circumventing the need for extra resource usage in terms of bandwidth. We test the performance of our approach on Wikipedia data. Our Conservative Focused Crawler (CFC) shows a per-  Parsing The process of extracting clean content from a web document. Parsing would also involve extracting outlinks, language, domain information and other meta information from the document. This is a CPU intensive task. Indexing The process of storing the document in a searchable format. This is a memory intensive task.  formance equivalent to that of a focused  In the context of crawling under resource con-  crawler (skyline system) with an average  straints, it is important to carefully choose the ap-  resource usage reduction of ≈30% across  propriate outlinks to be crawled at each depth.  two domains viz., tourism and sports.  Choosing the outlinks before actually fetching  
 TTS systems attempts to synthesize speech  Text to speech synthesis system intended for any language, converts the given text in that language to corresponding speech. The major challenge in TTS system is to generate artiﬁcial speech which appears to be natural and intelligible. This is essential for visually impaired people to properly understand and comprehend the generated speech. This paper discuss about text normalization and unit selection for a memory based non-uniform unit selection concatenative speech synthesizer for Malayalam language.  which has the qualities of natural speech generated by humans. The best way to improve the quality of synthesized speech is to mimic the way humans generate speech. The process of mimicking human functionality requires a model of how humans store the language within their brain and how they retrieve appropriate units for speech production. A memory based model for Malayalam TTS can be developed based on Memory Prediction Framework, which is a theory of brain function. This work attempts to implement the front end portions for a memory based Malayalam TTS. The TTS system deals with real world data, hence text preprocessing is an important challenge. The sys-  
Morphological Analyzer is a tool which performs syntactic analysis of a word and finds root form of input inflected word form. Morph analyzer serves as a pre-processing tool for many NLP applications. Significant amount of work has been done in this area for many Indian languages but not much work has been reported for Gujarati language. We present Morph analyzer for Gujarati language. The Morph analyzer is developed using a hybrid approach that combines statistical, knowledge based and paradigm based approach. We present detailed study of different approaches. We demonstrate a significant improvement in overall accuracy and achieve 92.34% and 82.84% accuracy with knowledge based hybrid method and statistical hybrid method respectively. 
 Radhika Mamidi IIIT Hyderabad radhika.mamidi @iiit.ac.in different languages. This is the first attempt at anaphora resolution in Telugu dialogues.  The challenge of anaphora resolution has  2 Related work  been taken up from long time. However, most of the work did not include for dialogues. In this paper we discuss the types of pronouns  Research has been going extensively in this area from past few decades. Many rule based, ma-  and anaphora in Telugu language and make  chine learning based and knowledge based sys-  an attempt to build a rule based pronominal  tems have come up. One of the earliest work has  anaphora resolution algorithm for human to human conversations. The model mainly consists of two parts, creating a knowledge base with a set of pronouns along with its morphological information and designing an algorithm which uses this knowledge base to give an output. In this process we have worked on normal pronominal anaphora and suggested a set of rules applicable for Telugu dialogues.  been done by Hobbs (1976) on pronominal coreference. Hirst (1981) has compared 5 different approaches and presented their strengths, weaknesses in resolving the anaphora. Lappin and Leass (1994) came up with salience feature based approach. Mitkov (1994a) came up with integrated anaphora resolution approach which relies on set of preferences and constraints. Ro-  However, since there was no corpus for the  bust knowledge poor pronoun resolution was  Telugu language, we built a corpus and tested  developed by Mitkov (1998) which makes use of  the algorithm on it. Results show that the suggested algorithm produced an output with an accuracy of 61.1%.  part-of-speech tagger and simple noun phrase rules. Strube and Eckert (1998) made an attempt to resolve discourse diectic anaphora in dia-  
 others are speciﬁc with respect to the language  pair (Lavanya et al., 2005; Saboor and Khan,  Machine Translation has made signiﬁcant achievements for the past decades. However, in many languages, the complexity with its rich inﬂection and agglutina-  2010). Hence, the divergence in the translation need to be studied both perspectives that is across the languages and language speciﬁc pair (Sinha, 2005).The most problematic area in translation is  tion poses many challenges, that forced  the lexicon and the role it plays in the act of creat-  for manual translation to make the corpus available. The divergence in lexical, syntactic and semantic in any pair of languages makes machine translation more difﬁcult. And many systems still depend on rules heavily, that deteriates system performance. In this paper, a study on divergence in Malayalam-Tamil languages is attempted at source language analysis to make translation process easy. In Malayalam-Tamil pair, the divergence is more reported in lexical and structural level, that is been resolved by using bilingual dictionary and transfer grammar. The accuracy is increased to 65 percentage, which is promising. Keywords- Translational divergence; semantic; syntactic; lexical;  ing deviations in sense and reference based on the context of its occurrence in texts (Dash, 2013). Indian languages come under Indo-Aryan or Dravidian scripts. Though there are similarities in scripts, there are many issues and challenges in translation between languages such as lexical divergences, ambiguities, lexical mismatches, reordering, syntactic and semantic issues, structural changes etc. Human translators try to choose the correct wording by using knowledge from various sources, and the factors like phonology, orthography, morphology as well as knowledge of the person, and cultural differences inﬂuences the translation. Therefore, it is hard to get a translation of one person as same as other translator. MT is a complex and challenging research area because language translation itself is very difﬁcult. While human processes language understanding  
 ent formats into one common format of Grammat-  ical Framework. Grammatical framework (GF)  Grammatical framework (GF) is an open source software which supports semantic abstraction and linguistic generalization in terms of abstract syntax in a multi-  is an open source software which supports semantic abstraction and linguistic generalization in terms of abstract syntax in a multi-lingual environment (Ranta Aarne, 2009). Abstract syntax  lingual environment. This makes the soft-  can be viewed as an interlingua in the context of  ware very suitable for automatic multilingual translation using abstract syntax which can be treated as a interlingua. As  multi-lingual machine translation. Apart from abstract syntax there exists another module called concrete syntax in which the morphological spec-  a ﬁrst step towards building multi-Indian  iﬁcation and syntactic behavior of a language can  language translation system using GF platform, we aim to develop an automatic converter which will convert morphological processors available in various formats  be captured. The abstract syntax and concrete syntaxes of all languages that one wants to handle together can produce very good quality, especially for domain based MT systems. This is the mo-  for Indian languages into GF format. In  tivation for converting morphological processors,  this paper we develop a deterministic automatic converter that converts LTtoolbox and ILMT morphological processors into  that at present exist in different format for different Indian languages, into GF format so that good quality meaning driven multi-lingual MT can be  GF format. Currently we have converted  accomplished.  Hindi, Oriya and Tamil processors using our converter with 100% information preserved in the output. We will also report in this paper our effort of converting Sanskrit and Marathi LTtoolbox morphological processor into GF format.  Our morphological converter presently converts morph resources designed in Lttoolbox and ILMT morph framework into GF format. We have experimented with Hindi, Oriya, Tamil, Marathi and Sanskrit morphological processors. We have achieved 100% information preservation for the  
 linguistics thesis in Konkani and elaborate dis-  cussions with linguists. Lemmas1 in the lan-  Automatic selection of morphological  guage are then mapped to appropriate Mor-  paradigm for a noun lemma is neces-  phological Paradigms to create a Morpholog-  sary to automate the task of building  ical Lexicon. Mapping of lemmas to Mor-  morphological analyzer for nouns with  phological Paradigms is time consuming when  minimal human interventions. Mor-  done manually.  phological paradigms can be of two types namely surface level morphological paradigms and lexical level morphological paradigms. In this paper we present a method to automatically select lexical level morphological paradigms for Konkani nouns. Using the proposed concept of paradigm diﬀerentiating measure to generate a training data set we found that logistic regression can be used to automatically select lexical level morphological paradigms with an F-Score of 0.957.  Automating creation of Morphological Lexicon requires automatic mapping of lemmas to morphological paradigms. Morphological Paradigms can be deﬁned at two level surface level and lexical level. At surface level two diﬀerent morphological paradigms will generate a diﬀerent Inﬂection Set for a given lemma whereas at lexical level two diﬀerent morphological paradigms could generate same Inﬂection Set for a given lemma. Thus automatically choosing a correct morphological paradigm at the lexical level cannot be based on Suﬃx Evidence Value as in previous used  
 Existing restaurant recommendation systems  Restaurant recommendation systems are capable of recommending restaurants based on various aspects such as location, facilities and price range. There exists some research that implements restaurant recommendation systems, as well as some famous online recommendation systems such as Yelp. However, automatically rating individual food items of a restaurant based on online customer reviews is an area that has not received much attention. This paper presents Ruchi, a system capable of rating individual food items in restaurants. Ruchi makes use of Named Entity Recognition (NER) techniques to identify food names in restaurant reviews. Typed dependency technique is used to identify opinions associated with different food names in a single sentence, thus it was possible to carry out entity-level sentiment analysis to rate individual food items instead of sentence-level sentiment analysis as done by previous research. 
Question classification is an important part in Question Answering. It refers to classifying a given question into a category. This paper presents a learning based question classifier. The previous works in this field have used UIUC questions dataset for the classification purpose. In contrast to this, we use the WebQuestions dataset to build the classifier. The dataset consists of questions with the links to the Freebase pages on which the answers will be found. To extract the exact answer of a question from a Freebase page, it is very essential to know the domain of the answer as it narrows down the number of possible answer candidates. Proposed classifier will be very helpful in extracting answers from the Freebase. Classifier uses the questions’ features to classify a question into the domain of the answer, given the link to the freebase page on which the answer can be found. 
Entity linking is the task of disambiguating entities in unstructured text by linking them to an entity in a catalog. Several collective entity linking approaches exist that attempt to collectively disambiguate all mentions in the text by leveraging both local mention-entity context and global entity-entity relatedness. However, the complexity of these models makes it unfeasible to employ exact inference techniques and jointly train the local and global feature weights. In this work we present a collective disambiguation model, that, under suitable assumptions makes efﬁcient implementation of exact MAP inference possible. We also present an efﬁcient approach to train the local and global features of this model and implement it in an interactive entity linking system. The system receives human feedback on a document collection and progressively trains the underlying disambiguation model.  selected mentions, based on mention-entity coherence, as well as entity-entity similarity. Some of the recent work (Zhou et al., 2010; Lin et al., 2012) shows that several mentions may have no associated sense in the catalog. This is referred to as the no-attachment (NA) problem (or NIL in the TAC-KBP challenge (McNamee, 2009)). The other, relatively lesser addressed challenge is that of multiple attachments (Kulkarni et al., 2014), where a mention might link to more than one entities from the catalog. This might often be a result of insufﬁcient context and has been acknowledged by some of the recent entity disambiguation challenges1. We present an approach to collective disambiguation of several mentions by combining various mention-entity compatibility and entity-entity relatedness features. Also, unlike most of the prior work, we jointly learn the local and global feature weights. Our Markov network-based model, along with suitable assumptions, makes efﬁcient learning possible. The model links mentions to zero or more entities, thus offering a natural solution to the problem of NAs and multiple attachments.  
 of speech corpora for different speech recognition tasks in  Malayalam language.  Pronunciation dictionary and  Transcription file which are the other two essential resources for  building a speech recognizer are also being created. Speech  recognition performance of different speech recognition tasks  are being presented. Speech corpus of about 18 hours have  been collected for different speech recognition tasks.  Keywords— Speech Recognition, corpus development, Malayalam  I. INTRODUCTION One of the main challenges faced by speech scientist is the unavailability of the three important resources. The prime and most importantly, speech corpus (Speech Database), pronunciation dictionary and transcription file. Very fewer efforts have been made in Indian languages to make these resources available to public compared to English. Creation of these resources is time consuming, boredom and needs so much man power. Creating a well defined pronunciation dictionary needs through knowledge from phonetics, phonological rules, syntactic and semantic structure of the language. It is necessary to have databases which comprises of appropriate sentences spoken by the typical users in realistic acoustic environment. Speech databases can be divided into two groups: (i) a database of speech normally spoken in a specific task domain. In this case, small amount of speech is sufficient to achieve acceptable recognition accuracy. (ii) a general purpose speech database that is not tuned to a particular task domain but consists of general text and hence can be used for recognition of any sentence in that language. The problem with most speech recognition systems is insufficient training data containing speech variations (spontaneous speech) caused by speaker variances (cover large number of speakers). To overcome these problems, a large vocabulary speech database is required to build a robust recognizer. The purpose of selection of phonetically-rich sentences is to provide a good coverage of pairs of phones in the sentence. The current work also aims at the development of databases for Malayalam speech recognition that will facilitate for acoustic phonetic studies, training and testing of automatic speech recognition systems. It is anticipated that the availability of this speech corpus would also stimulate the  basic research in Malayalam acoustic-phonetics and phonology.In this paper three sets of databases have been created ( task specific databases , a general purpose database and a specially designed database for unique phoneme analysis ) . The task specific database includes three domain based databases i.e isolated digit speech database, connected digit speech database and continuous speech database. The general purpose database includes a set of phoneme class wise speech database. Database designed for unique phoneme analysis includes, specially designed 32 minimal pair of words as well as a set of words which include unique phonemes in any word positions. Section 2 discusses phonetic chart of Malayalam language and in section 3 the text corpus that has been prepared is detailed. In section 4 the method of speech data collection is being elaborated. The phoneme list prepared for the work is explained in section 5 followed by the creation of pronunciation dictionary in section 6. In section 7 the format and model of the transcription files being prepared is explained. Section 8 gives the results of various speech recognition tasks. 2. PHONETIC CHART Malayalam has 52 consonant phonemes, encompassing 7 places of articulation and 6 manners of articulation, as shown in Table 1 below. In terms of manner of articulation, plosives are the most complicated, for they demonstrate a six-way distinction in labials, dentals, alveolar, retroflex, palatals, velars and glottal [1]. A labial plosive, for example, is either voiceless or voiced. Within voiceless labial plosives, a further distinction is made between aspirated and un-aspirated ones whereas for voiced labial plosives the distinction is between modal-voiced and breathy-voiced ones. In terms of place of articulation, retroflex are the most complex because they involve all manners of articulation except for semi vowels [2]. Phonetic chart as presented by Kumari, 1972 [3] for Malayalam language is given in table 1 and the same has been referred for this paper. For all speech sounds, the basic source of power is the respiratory system pushing air out of the lungs. Sounds produced when the vocal cords are vibrating are said to be voiced, where the sound produced when the vocal cords are apart are said to be voiceless [4]. The shape and size of the vocal tract is a very important factor in the production of speech. The parts of the vocal tract such as the tongue and the  229 D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 229–236, Trivandrum, India. December 2015. c 2015 NLP Association of India (NLPAI) 1|P a g e  lips that can be used to form sounds are called articulators (fig roof of the mouth (palate) and the pharynx are part of the  .1). The movements of the tongue and lips interacting with the articulatory  process  [5]  Table 1 : Phonetic chart of Malayalam Labial Dental Alveolar Retroflex Palatal  Velar Glottal  Stop / plosive voiced unvoiced voiced unvoiced Voiced Unvoiced voiced unvoiced voiced unvoiced voiced unvoiced  un aspirated aspirated Nasals Fricative Lateral rhotic semi vowels  പp ബ b  ഫph  ഭ bh  മ m  ഫ f  വ v  ത ദd t  ഥ th  ധdh  ന n  സ s  ററ t  ടṭ ഡḍ ചc ജj കk ഗg  ന n II.  ഠ ഢḍh ഛch ഝjh ṭh  ണ ṇ EASEഷOṣF USE  ഞ ñ ശ ś  ഖkh ഘgh ങ ṅ  ഹ h  ല l  ള ḷ  ഴ l  ര r  റ r  യ y  account. It is assumed that speakers will be comfortable in reading 7-digit numbers. Accordingly, a sets of 7-digit numbers were generated, each set containing 20 numbers capturing all distinct word pairs. These sets were generated using different methods of generating word pairs. One such set of 20 numbers is shown in Table 2.  Figure 1: Place of articulation 3. Text Corpus The first step followed in creating the speech database for automatic speech recognizers is the generation of optimal set of textual sentences to be recorded from the native speakers of the Malayalam language. The following are the different set of text corpus collected for different tasks. 3.1 Isolated Digit Recognition Task Digits from 0-9 3.2 Connected Digit Recognition Task Text corpus consists of specially designed, twenty, 7 digit numbers so that maximum combinations of numbers as well as co-articulation of the numbers have been taken into  Table 2: An example of a list of twenty 7-digit numbers read  by speakers  0098765  4159306  4725836  3567801  5432101  6927918  6828162  1345566  1975312  8908634  2612371  6778899  2964203  4074851  1460450  9011217  3952494  1733844  0570223  7913579  3.3 Continuous Speech Recognition task For speaker independent speech recognition, speech data needs to be collected from a large number of speakers. It is not practical to ask speakers to speak/read a lot of sentences that contain all phonemes (in various phonetic contexts) of the language. Hence, it is desirable to construct sets of sentences that are phonetically rich. This construction is a laborious task. Phonetically rich sentences can be selected from a large set of text. Traditional sources of text data are books, magazines and periodicals. However a textual data is needed in electronic form so that it can be processed by a computer. Hence there are two choices: (a) Manually type in the printed data from articles, periodical, magazines etc., and store it in electronic form. (b) Use available online sources of text data. Example of such sources is articles on web and online news papers. There  230  2|P a g e  are several online newspapers that provide content in Malayalam language. Each online content uses its own grapheme encoding scheme to display Malayalam text. To collect the text data online Malayalam news papers being used. In order to compute the phonetic richness of sentences and select sentences, the Malayalam text (and the corresponding phoneme sequence) has to be represented using Roman symbols. However, such a grapheme to phoneme conversion programme for the different fonts was not available. Also, information about the coding scheme was not readily available. Hence a grapheme-to-phoneme (G2P) program was written to take care of most conventions of the font. A tools named Corpuscrt [6] from CMU (Carnie Melon University) is used to select the maximum phonetically rich sentences. Hence the selected text includes about 202 sentences, comprising of about 1600 words.  included such that it should contain all phonemes in all word positions (start, middle and end). Nasal class category contain 74 words which include labial, dental, alveolar and retroflex nasals. Category wise number of words is shown in table 4.  Table 4: List of words with Nasal class phonemes  Category  Total numbers  Labial nasals  20  Dental nasals  20  Alveolar nasals  14  Retroflex nasals  20  3.4 Analysis of Unique Phoneme Features for speech recognition For the analysis of unique phoneme features of Malayalam language, the phonemes Alvelor plosive റ്റ (t't'a) , Alvelor rhotic റ (ra) , Retroflex lateral ള (l'a) ,Palatel lateral ഴ (zha) and Dental Nasal ന (n1a) have been selected. For this analysis, two set of words have been compiled.  i. Lateral class words include a set of 54 words carefully designed which includes palatal, retroflex and alveolar lateral as detailed in table 5.  Table 5: List of words with lateral class phonemes  Category  Total numbers  Palatel lateral  15  i. The first set contains a word-list constituting each of these sounds in all permissible word-positions. In this category, a total of 340 words have been compiled. The following are (Table 3) the total number of section wise, ii. categorization of words with these phonemes that have been collected for the study. Table 3: List of Words with different categories of phonemes  Retroflex lateral  15  Alveolar lateral  24  ii. Fricative class words include a set of 70 words which  includes dental, retroflex and palatal fricatives. Table 6 list the  number of words of each type.  Table 6 : List of words with fricative class phonemes  Category  Total numbers  Category Alvelor plosive Alvelor rhotic Retroflex lateral  Total numbers 27 48 37  Dental fricative  23  Retroflex fricative  15  Palatel fricative  17  Palatel lateral  42  Dental Nasal  40  Glotal fricaitive  15  retroflex rhotic  49  alveolar lateral  44  alveolar nasal  53  ii. Since /la/ and /ɭ'a/, /ra/ and /r'a/ are two pairs of contrastive phonemes and /ʐha/ contrasts in some instances with one or both of the laterals or in other instances with one or both of the rhotics, the data sets are designed to include minimal pairs. Accordingly 32 minimal pairs have been compiled (total of 64 words). In short a total of 404 words were designed for unique phoneme study. 3.5 Phoneme class wise speech recognition task  iii. Plosive/stop class words includes a total of 205 words in different categories such as labial voiced stop, labial unvoiced stop, dental voiced stop, dental unvoiced stop, velar voiced stop, velar unvoiced stop, retroflex unvoiced stop, palatal voice stop and palatal unvoiced stop. Table 7 lists the number of words in each category.  Table 7 : List of words with plosive/stop class phonemes  Category  Total numbers  Labial voiced stop  26  Labial unvoiced stop  20  Dental voiced stop  20  Five phoneme classes of words have been chosen for Dental unvoiced stop  20  this task. The different phoneme classes are stop, lateral,  fricative, nasals and rhotic. Maximum words have been Retroflex voiced stop  21  231  3|P a g e  Retroflex unvoiced stop  15  Palatel voiced stop  20  Palatel unvoiced stop  22  Velar voiced stop  23  Velar unvoiced stop  19  iv. Rhotic class words includes 43 words which include both rhotics (alveolar rhotic and retroflex rhotics ) as detailed in table 8.  Table 8: List of words with rhotic class phonemes  Category Alvelor rhotic Retroflex rhotic  Total numbers 23 20  5. Creation of Phoneme List A phoneme is the basic unit of recognition. Therefore the preparation of a phoneme list is a vital step in creating a pronunciation dictionary. Each phone to be denoted by a set of phonetic notation rather than a single notation, which will be decided by the acoustic property of the phones. All unique phonemes of Malayalam language have been identified and phonetic notation has been assigned according to its phonetic properties. Table 10 lists all the phonemes used in this work along with its phonetic notation. Table 10: Phoneme list  4. Speech data collection Speech data for all the recognition tasks is collected from the age group of 20 to 45 years keeping almost equal male and female ratio. Speakers were requested to read the word /sentences in a normal reading manner. Speech data is collected in normal office environment using a microphone with 1600 frequency. Mistakes made while recording have been corrected by re-recording or by making the corresponding changes in the transcription file. For all the tasks data have been collected from 25 speakers (13 female and 12 male speakers). The task wise list of speech corpus is detailed below.  Isolated Digit recognition task - Digits zero to nine is uttered separately by speakers. Hence Size of the speech corpus for this task is 250 words.  Connected digit recognition task - Text data as detailed in section 3.2 is read by the 25 speakers in normal reading manner. Size of the corpus is 500 words.  Continuous speech recognition task- The 202 continuous sentences which are selected as described above were read by the speakers. Hence the size of the continuous speech corpus is 40000 words.  Unique, phoneme study - The specially designed 404 words were read 25 speakers thereby enhancing the speech corpus by 10100 words.  Phoneme class wise recognition task - For this category of speech corpus the 446 words as mentioned in section 3.5 were read by 25 speakers. Hence the size of speech corpus in this category is 11150 words. These details are given in table 9 below.  Table 9: Speech corpus collection  RecTotaagsbnklieti9o:nSizeswToitzofeoerxtsadptinlseechNcuormpbusercoolflescpteeadkfeorrsdiffcessTporiezreoinenpettacouthlfsasksScionprehpercushs words  Isolated digit  10  male : 12 female :13 250  0.16  connected digit continuous speech recogntion task  20  male : 12 female :13 500  0.63  232  1600 male : 12 female :13 40000 11.67  Analysis of  6. Pronunciation Dictionary (PD) In pronunciation dictionary all words in the training data to be mapped onto the acoustic units which are defined in the phone list. Theoretically, creation of phonetic dictionary is just a mapping of grapheme to phoneme. But this alone would not work especially for a language like Malayalam as many phonemes pronounced differently in different contexts. For example, ഫ (ph’a) pronounced differently in ഫലം (/ph'alam/-fruit) and ഫാൻ (/ph'aan'/ - fan) and ന (n1a and 4|P a g e  na - Nasal dental and Nasal alveolar) is pronounced differently even though the grapheme notation is same (eg. നനക്കുക(/n1anaykkuka/- watering). Hence for creating pronunciation dictionary, initially mapping have been completed for all grapheme into the corresponding phoneme units. Then some phonological rules have been applied manually and edited the dictionary. Multiple pronunciations are also incorporated in the dictionary. The format of pronunciation dictionary for isolated digit recognition is in table 11 and that of continuous speech recognition task is under table 12. The left segment represent orthographic transcription and right segment shows its actual pronunciation is taken from the training speech corpus. The dictionary creation to be done with utmost care and precision as the duplication of phone for different sounds will confuse the trainer and the model so created will give false information to the recognizer. The dictionary must have all alternate pronunciations marked with parenthesized serial numbers starting from (2) for the second pronunciation. The marker (1) is omitted for the first pronunciation. Thus pronunciation dictionary of 2480 words of Malayalam language have been prepared for different tasks. The following are the size of pronunciation dictionary for different tasks/analysis i. Digit recognition task - 10 words ii. Connected word recognition tasks - 20 words iii. Continuous speech recognition task - 1600 words iv. Unique phoneme analysis - 404 words v. Phoneme class wise recognition task - 446 words Table 11: P.D for isolated digit recognition task  Transcription file The transcription file contains the sequence of words transcribed orthographically, and non-speech sounds, written exactly as they occurred in the training speech, followed by a tag which can be used to associate this sequence with the corresponding training speech data. The acoustic speech file is transcribed into its corresponding orthographic representation. The transcription file is to be prepared for every training speech data after closely examining/hearing the wave file. Hence the transcription process is done manually considering even silence, noise or a breath. This is a herculean task and has to be done very carefully, since a minute error in the transcription file will mislead the recognizer and will lead into misclassification of training data. Hence transcription files prepared for a total of 62000 speaker utterances (wave file). Each task has a separate transcription file which consisting of transcriptions for each speaker utterance. Table 13 is a sample of transcription file created for continuous speech recognition task. Table 13 : A sample of transcription file for continuous speech recognition task  Table 12: Format of PD for continuous speech recognition task 8. Automatic Speech Recognition Automatic Speech Recognition is the process of converting speech into text. Speech recognition systems perform two fundamental operations: Signal modeling and pattern matching [7]. Signal modeling represents process of converting speech signal into a set of 233 5|P a g e  parameters. Pattern matching is the task of finding parameter sets from memory which closely matches the parameter set obtained from the input speech signal [ 8]. Hence the two important methodologies used in this works are MFCC Cepstral Coefficients [9] for signal modeling and Hidden Markov Model [10 ] for pattern matching. Mathematically stating computing the probability of a word given a pattern model is computed as the product of two components – acoustic model and language model.[ 11 ]  Ŵ =argmaxW P (Y | W) P(W)) / P(Y)  (1)  The right hand side of equation (1) has two components: i) the probability of the utterance of the word sequence given the acoustic model of the word sequence and ii), and the probability of sequence of words. The first component P(Y/W), known as the observation likelihood, which is computed by the acoustic model. The Second component P(Y) is estimated using the language model..  are also challenging. For instance, the word " talasthaanam' (തലസ്ഥാനം) can be misconstrued as "tala sthaanam' (തലസ്ഥാനം). The use of language model resolves these issues by considering phrases and words that are more likely to be uttered. The trigram based language model with back-off is used for recognition in this work. The language model is created using the CMU statistical LM toolkit [17]. Training and testing is done by n –fold validation techniques. Word Error Rate (WER) is the standard evaluation metric used here for speech recognition. It is computed by SCLITE [18], a scoring and evaluating tool from National Institute of Standards and Technology (NIST) 8.3 Results of Different Speech Recognition Tasks  8.1 Acoustic model The Carnegie Mellon University Sphinx-4[12] system is a framebased, HMM-based, speech recognition system capable of handling large vocabularies. The word modeling is performed based on sub word units (phone set), in terms of which all the words in the dictionary are transcribed. Each phonetic unit considered in its immediate context (which we will refer to as tri-phone) is modeled by 3-state left-to-right HMM model [13] . To reduce the parameter estimation problem, data is shared across states of different triphones. These groups of HMM states sharing distributions between its member states are called senones [25]. The acoustic modeling component of the system has four important stages [14 ] . The first stage is to train the context independent model and then training context dependent models. Decision trees are built on the third stage and finally context independent tied models are created. Here, continuous Hidden Markov models are chosen to represent context dependent phones (tri-phones). The phone likelihood is computed using HMM. The likelihood of the word is computed from the combined likelihood of all the phonemes. The acoustic model thus built is a 3 state continuous HMM, with states clustered using decision tree [15 ] . The acoustic features used for recognition consist of 39-dimensional acoustic vectors derived every 10 ms spanning an analysis window of 20 ms. The feature vector consists of the first 13 cepstral coefficients (including, the frame energy) and two blocks of 13-dimensional coefficients, one composed of the “delta” cepstral features (velocity) and the other composed of “delta-delta” cepstral features (acceleration). Cepstral mean normalization is always applied at the utterance level to remove statistical biases of the mean which might have been introduced by linear channel distortion. 8.2 CREATION OF LANGUAGE MODEL The language model employed in our recognition experiments is a tri gram-based language model developed using a training text corpus. These language models are smoothed using the Good-Turing discounting procedure [16]. Importance of a language model in a speech recognition system is vital as acoustic model alone cannot handle the problem of word ambiguity. Word ambiguity may occur in several forms such as similar sounding sounds and word boundaries. With similar sounding sounds, words are indistinguishable to the ear, but are different in spelling and meaning. The words "paat'am' (പാടം) and " paat'ham' (പാഠ)ം“are such examples. In continuous speech, word boundaries  8.3.1 Isolated Digit Recognition Task Table 14: Result of Digit Recognition Task  In isolated digit recognition task 92.9% accuracy being obatained as shown in table 14. 8.3.2 Connected Digit Recognition Tasks Table 15: Result of Connected Digit Recognition Task  101  99.5  99  97  94.44  95  93  90.59  91  89 87.76  87  85  234  Accuracy % 12 MFCC 12 MFCC+E 12MFCC+E+12∆MFCC+∆E 12MFCC+E+12∆MFCC+∆E+ 12∆∆MFCC+∆∆E  6|P a g e  In connected digit recognition task, as per table 15, a very good accuracy is obtained i.e , 99.5 % (with 39 feature vectors) .Table 16 is the snapshot of the live decoder developed for connected digit recogntion task. Table 16: Live decoder for Connected Digit Recognition Task  8.3.3 Continuous Speech Recognition Task Table 16 Training and Testing results of Continuous Speech recognition - CDHMM vs. SCHMM Models  Sl.No 
 explored to a little extent for CM text (Solorio  We discuss Part-of-Speech(POS) tagging of Hindi-English Code-Mixed(CM) text from social media content. We propose extensions to the existing approaches, we also present a new feature set which addresses the transliteration problem inherent in social media. We achieve an 84% accuracy with the new feature set. We show that the context and joint modeling of language detection and POS tag layers do not help in POS tagging. 
 symbolism. Processing creative writing such  as poetry by computers is challenging, as op-  Computational analysis of poetry is  posed to ordinary everyday text, for comput-  a challenging and interesting task in  ers are efficient in carrying out tasks of a more  NLP. Human expertise on stylistics  logical nature, as compared to those involv-  and aesthetics of poetry is generally  ing creativity. The volume of research in au-  expensive and scarce. In this work,  tomated analysis of poetry has generally been  we delve into the data to automati-  low, and no work has been reported on Bangla  cally extract stylistic and linguistic in-  poetry. Bangla is the seventh most spoken  formation which are useful for anal-  language in the world and has a rich literary  ysis and comparison of poems. We  tradition. While work on stylometry for prose  make use of semantic (word) features to  (Chakraborty and Bandyopadhyay, 2011) and  perform subject-based classification of  author identification (Das and Mitra, 2011)  Bangla poems, and various stylistic as  has been reported, our work is the first of its  well as semantic features for poet iden-  kind to analyse Bangla poetry.  tification. We have used a Multiclass SVM classifier to classify Tagore’s collection of poetry into four categories: devotional, love, nature and nationalism. We identified the most useful word features for each category of poems. The overall accuracy of the classifier was 56.8%, and the analysis led us to conclude that for poetry classification, word features alone do not suffice, due to allusions often being used as a poetic device. We, next, used these features along with stylistic features (syntactic, orthographic and phonemic), for poet identification on a dataset of poems from four poets and achieved a performance of 92.3% using a Multiclass SVM classifier. While contentbased and stylometric analysis of prose in Bangla has been done in the past, this is a first such attempt for poetry.  The computational analysis of poetry is important, for not only can it lead to a better understanding of what makes rich literature, but it also has applications such as making recommendations to readers based on their literary tastes, as also in the psychological effects of poetry (Stirman and Pennebaker, 2001). Identifying the poet is also important for plagiarism detection. We explore various kinds of features from Bangla poems to carry out specific analyses. Firstly, we perform a subject-based classification of poems into pre-determined categories from Tagore’s poems using semantic features, the categories being pooja (devotional), prem (love), prokriti (nature), and swadesh (nationalism). With our experiments, we establish the fact that the words can help only so far, due to frequent use of poetic devices such as allusion and symbolism, which often leave poems open to multiple interpretations. Second,  
 challenges to language processing and new difﬁ-  The paper presents a study on automatic sentence boundary detection in social media texts such as Facebook messages and Twitter micro-blogs (tweets). We explore the limitations of using existing rule-based sentence boundary detection systems on social media text, and as an alternative investigate applying three machine learning algorithms (Conditional Random Fields, Naïve Bayes, and Sequential Minimal Optimization) to the task.  culties for SBD, with state-of-the-art systems failing to perform well on social media, due to the coarse nature of the texts. In spite of its important role for language processing, sentence boundary detection has so far not received enough attention. Previous research in the area has been conﬁned to formal texts only, and either has not addressed the process of SBD directly (Brill, 1994; Collins, 1996), or not the performance related issues of sentence boundary detection (Cutting et al., 1992). In particular, no SBD research to date has addressed the problem in  The systems were tested on three corpora  informal texts such as Twitter and Facebook posts.  annotated with sentence boundaries, one  The growth of social media is a global phe-  containing more formal English text, one  nomenon where people are communicating both  consisting of tweets and Facebook posts  using single languages and using mixes of several  in English, and one with tweets in code-  languages. The social media texts are informal in  mixed English-Hindi. The results show  nature, and posts on Twitter and Facebook tend to  that Naïve Bayes and Sequential Minimal  be full of misspelled words, show extensive use of  Optimization were clearly more successful  home-made acronyms and abbreviations, and con-  than the other approaches.  tain plenty of punctuation applied in creative and  
 moods are prevalent in literature (Hu and Down-  ie, 2010b). Indian music considered as one of the  Digitization of music has led to easier ac-  oldest musical traditions in the world. Indian mu-  cess to different forms music across the  sic can be divided into two broad categories,  globe. Increasing work pressure denies  “classical” and “popular” (Ujlambkar and Attar,  the necessary time to listen and evaluate  2012). Further, classical music tradition of India  music for a creation of a personal music  has two main variants; namely Hindustani and  library. One solution might be develop-  Carnatic. The prevalence of Hindustani classical  ing a music search engine or recommen-  music is found largely in north and central parts  dation system based on different moods.  of India whereas Carnatic classical music domi-  In fact mood label is considered as an  nates largely in the southern parts of India.  emerging metadata in the digital music  Indian popular music, also known as Hindi  libraries and online music repositories. In  Bollywood music or Hindi music, is mostly pre-  this paper, we proposed mood taxonomy  sent in Hindi cinemas or Bollywood movies.  for Hindi songs and prepared a mood an-  Hindi is one of the official languages of India  notated lyrics corpus based on this taxonomy. We also annotated lyrics with  and is the fourth most widely spoken language in the World1. Hindi or Bollywood songs make up  positive and negative polarity. Instead of  72% of the total music sales in India (Ujlambkar  adopting a traditional approach to music  and Attar, 2012). Unfortunately, not much com-  mood classification based solely on audio  putational and analytical work has been done in  features, the present study describes a  this area.  mood classification system from lyrics as  Therefore, mood taxonomy especially for  well by combining a wide range of se-  Hindi songs has been introduced here in order to  mantic and stylistic features extracted  closely investigate the role played by lyrics in  from textual lyrics. We also developed a  music mood classification. The lyrics corpus is  supervised system to identify the senti-  annotated in two steps. In the first step, mood is  ment of the Hindi song lyrics based on  annotated based on the listener’s perspective. In  the above features. We achieved the max-  the second step, the same corpus is annotated  imum average F-measure of 68.30% and  with polarity based on the reader’s perspective.  38.49% for classifying the polarities and  Further, we developed a mood classification sys-  moods of the Hindi lyrics, respectively.  tem by incorporating different semantic and tex-  tual stylistic features extracted from the lyrics. In  
 Millions of people around the globe use Twit-  In this paper, we consider the problem of sentiment classiﬁcation of English Twitter messages using machine learning techniques. We systematically evaluate the use of different feature types on the per-  ter on daily basis to publicly express their opinions regarding various aspects of their everyday lives. Thus, Twitter provides a massive data source which public and private organizations can harness to monitor the opinions of the crowd towards almost anything. Mining sentiments and opinions  formance of two text classiﬁcation methods: Naive Bayes (NB) and Support Vector Machines (SVM). Our goal is threefold: (1) to investigate whether or not partof-speech (POS) features are useful for this task, (2) to study the effectiveness of sparse phrasal features (bigrams and skipgrams) to capture sentiment information, and (3) to investigate the impact of combining unigrams with phrasal features on the classiﬁcation’s performance. For this purpose we conducted a series of classiﬁcation experiments. Our results show that POS features are useful for this task while phrasal features could improve the performance of the classiﬁcation only when combined with unigrams.  expressed in Twitter can provide indispensable information for various applications such as brand monitoring, customer management, and political forecasting. Nevertheless, Twitter messages possess unique linguistic characteristics that make them distinguished from conventional user-generated content on the web (e.g., movie reviews). Tweets are short (maximum length is 140-character), cover a wide spectrum of topics, and written in an informal conversational style. The majority of tweets are poorly written and contain many non-standard lexical units such as emoticons (e.g., * *), emojis (e.g., <3), neologisms (e.g., gr8), hashtags (e.g., #thingsilike), and acronyms (e.g., lol). In addition, most subscribers write their tweets using mobile devices, which increases the frequency of unin-  
 NS as a sequence of nouns in which the noun  For a resource poor language like Hindi, it becomes very difﬁcult to bracket a noun sequence using approaches which are only based on corpus or lexical database. For semantic knowledge, power of both type of resources is needed to be combined. Therefore, afﬁnity in between two nouns is preferred to be measured using backoff association which is the combination of lexical and conceptual association. Also, syntax is important for this task. But syn-  constituents may or may not be separated by the genitives - “kA”, “kI” or “ke”.1 When nouns occur without any intervening postpositions, such special cases are called compound nouns (CN). Batra et al. (2014) have noticed that compound nouns have a tendency to be grouped ﬁrst within a complex NS. They have called this concept, local grouping. This grouping takes place due to indeclinable nature of all the nouns except the last one in a CN. This can be formalized using following context-free grammar:2  tactic rules do not work for the compound nouns which is a special case of noun sequences and it may also occur as the sub-sequence. Using hybrid approach,  G = {V, , R, N S} V = {N S, CN, genitive, noun} = set of common nouns ∪ set of genitives  accuracy of 86.33% has been obtained.  And rules R for this grammar are given by:  We have explored different variations like smoothing, frequency of synonyms and similar words for lexical association. And for conceptual association, different possible noun classes have been used for  NS → NS genitive NS | CN | noun CN → CN CN | noun CN | CN noun | noun noun genitive → kA | kI | ke noun → daravAja | kursI | kamarA | ... “door” “chair” “room”  experiments. Authors have their own way of writing. Sometimes, two nouns can be  Further, genitives and the head noun of the  written together as a single word or dash can be inserted in between the two. This helps in knowing that the two nouns have the tendency to be grouped together and hence this feature has been incorporated for the methods based on conceptual association.  1These allomorphic forms vary with gender, number and case values. Gender can have value - male(m) or female(f), number can have value - singular(s) or plural(p) and case can have - direct(d) or oblique(o) value. Morphological properties of genitives are: kA - msd kI - fsd or fso or fpd or fpo ke - mso or mpd or mpo 2Context-free grammar has set of recursive rules which are used for generating strings from the non-terminals or the terminals (alphabets). It is represented using four tuples  
 hoc, by and large, New York, kick the  Detection of Multiword Expressions (MWEs) is a challenging problem faced by several natural language processing applications. The difficulty emanates from the task of detecting MWEs with respect to a given context. In this paper, we propose approaches that use Word Embeddings and WordNet-based features for the detection of MWEs for Hindi language. These approaches are restricted to two types of MWEs viz., noun compounds and noun+verb compounds. The results obtained indicate that using linguistic information from a rich lexical resource such as WordNet, help in improving the accuracy of MWEs detection. It also demonstrates that the linguistic information which word embeddings capture from a corpus can be comparable to that provided by WordNet. Thus, we can say that, for the detection of above mentioned MWEs, word embeddings can be a reasonable alternative to WordNet, especially for those languages whose WordNets does not have a better coverage.  bucket, etc. Typically, a multiword is a noun, a verb, an adjective or an adverb followed by a light verb (LV) or a noun that behaves as a single unit (Sinha, 2009). Proper detection and sense disambiguation of MWEs is necessary for many Natural Language Processing (NLP) tasks like machine translation, natural language generation, named entity recognition, sentiment analysis, etc. MWEs are abundantly used in Hindi and other languages of Indo Aryan family. Common part-of-speech (POS) templates of MWEs in Hindi language include the following: noun+noun, noun+LV, adjective+LV, adjective+noun, etc. Some examples of Hindi multiwords are पुण्य ित थ (puNya tithi, death anniversary), वादा करना (vaadaa karanaa, to promise), आग लगाना (aaga lagaanaa, to burn), धन दौलत (dhana daulata, wealth), etc. WordNet (Miller, 1995) has emerged as crucial resource for NLP. It is a lexical structure composed of synsets, semantic and lexical relations. One can look up WordNet for information such as synonym, antonym, hypernym, etc. of a word. WordNet was initially built for the English language, which is then followed by almost all widely used languages all over the world. WordNets are developed for differ-  
 high agglutinative nature of Malayalam. Malay-  alam is a morphologically rich, agglutinative lan-  This paper is an attempt to bridge two well known performance degraders in SMT, viz., (i) difference in morphological char-  guage in which complex words are formed by concatenating morphemes together. For example, “अगर बादल नहीं बरसे तो भी” (if cloud  acteristics of the two languages, and (ii)  not rain_verb then also) in Hindi (5 words)  scarcity of parallel corpora. We address these two problems using “word segmentation” and through “pivots” on the mor-  would translate to “മഴാ െപ ുതിെല ിലും” (rain_noun rain_verb+not+even_if+then_also) in Malayalam (2 words).  phologically complex language. Our case study is Malayalam to Hindi SMT. Malayalam belongs to the Dravidian family of languages and is heavily agglutinative.  In this paper, we present a case of translation from Malayalam to Hindi. Our approach is based on combined use of pivot strategies for Statistical Machine Translation (SMT) and word segmenta-  Hindi is a representative of the Indo-Aryan  tion techniques. We show that word segmentation  language family and is morphologically simpler. We use triangulation as pivoting strategy in combination with morphological pre-processing. We observe that (i) sig-  of source language as well as pivot language helps to improve the translation quality. Section 2 contains details about relevant work done in the field. Section 3 explains the design of our system in de-  nificant improvement in translation quality over direct SMT occurs when a pivot is used in combination with direct SMT, (ii) the more the number of pivots, the better  tail. Section 4 describes the experimental setup. Results of the experiments are discussed in Section 5. Section 6 includes concluding remarks on the mal-hin translation task.  the performance and (iii)word segmenta-  tion is a must. We achieved an improve-  2 Related Work  ment of 9.4 BLEU points which is over 58% compared to the baseline direct system. Our work paves way for SMT of languages that face resource scarcity and have widely divergent morphological characteristics.  There is substantial amount on pivot-based SMT. De Gispert and Marino (2006) discuss translation tasks between Catalan and English with the use of Spanish as a pivot language. Pivoting is done using two techniques: pipelining of source-pivot and pivot-target SMT systems and direct translation  
 India where more than 22 ofﬁcial languages are  Parallel corpora are often injected with bilingual dictionaries for improved Indian language machine translation (MT). In absence of such dictionaries, a coarse dictionary may be required. This paper demonstrates the use of a multilingual topic model for creating coarse dictionaries for English-Hindi MT. We compare our approaches with: (a) a baseline with no additional dictionary injection, and (b) a corpus with a good quality dictionary. Our results show that the existing Cartesian product approach which is used to create the pseudo-parallel data results in a degradation on tourism and health datasets, for English-Hindi MT. Our paper points to the fact that existing Cartesian approach using multilingual topics (devised for European languages) may be detrimental for Indian language MT. On the other hand, we present an alter-  spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufﬁcient amount of parallel corpus, it is very difﬁcult to learn the correct correspondences between words that infrequently occur in the training data. Hence, a need for specialized techniques that improve alignment  nate ‘sentential’ approach that leads to a  quality has been felt (Sanchis and Snchez, 2008;  slight improvement. However, our sentential approach (using a parallel corpus injected with a coarse dictionary) outperforms a system trained using parallel corpus and a good quality dictionary.  Lee et al., 2006; Koehn et al., 2007). Mimno et al. (2009) present a multilingual topic model called PolyLDA, and apply it for Machine Translation for European and other languages such as Danish, German, Greek,  
 behind using a pivot is, using one or more pivot  languages to improve the quality of translation  Triangulation in Pivot-Based Statistical Machine Translation(SMT) is a very effective method for building Machine Translation(MT) systems in case of scarcity of  by making use of additional information that the pivot induces. This additional information is mainly in the form of a new set of phrase pairs that are extracted with the help of the pivot language.  the parallel corpus. Phrase Table Triangu-  This method of extracting new phrase pairs in  lation helps in such a resource constrained setting by inducing new phrase pairs with the help of a pivot. However, it does not explore the possibility of extracting reordering information through the use of pivot. This paper presents a novel method for triangulation of reordering tables in Pivot Based SMT. We show that the use of a pivot can help in extracting better reordering information and can assist in improving the quality of the translation. With a detailed example, we show that triangulation of reordering tables also improves the lexical choices a system makes during translation. We observe a BLEU score improvement of 1.06 for Marathi to English MT system with Hindi as a pivot, and also signiﬁcant improvements in 8 other translation systems by using this method.  Pivot-Based SMT is known as the Triangulation method. Current approaches and methods that discuss triangulation generally focus on the triangulation of the phrase tables from source language(source) to pivot language(pivot) and pivot language to target language(target). This improves the translation quality because of the newly added phrase pairs, but the reordering information for these newly added phrase pairs is not present in the reordering table. The focus of our work is to explore the possibility of extracting the reordering information for newly added phrase pairs by making use of the pivot language. To the best of our knowledge this is the ﬁrst work that discusses the approaches for triangulation of reordering tables and shows signiﬁcant improvements. To begin with, Section 2 discusses the work related to Pivot-Based SMT and the Triangulation  
 is the case of Hindi, Bengali and Marathi,  Access to textbooks in one’s own language, in parallel with the original version in the instructional language, is known to be quite helpful for foreign students studying abroad. Cooperative post-editing (PE) of specialized textbook pretranslations by the foreign students themselves is a good way to produce the ”target” versions, if the students ﬁnd it rewarding, and not too time-consuming, that is,  and probably of all Indic languages. Previous experiments seem to have missed that important point, because they were performed on too short texts (often, only a few paragraphs), and on ”easy” pairs like English-French. A consequence is that, for terminologically distant language pairs, one should begin by separately collecting, or if necessary coining, the terms in the target languages.  no longer than about 15-20 minutes per standard page (of 1400 characters or 250  
We address some theoretical and practical issues relating to generation, processing, and management of Translation Corpus (TC) in Indian languages, which is developed in a consortium-mode project (ILCI-II)1 under the DeitY, Govt. of India. Issues are discussed here for the first time keeping in mind the ready application of TC in various domains of computational and applied linguistics. We first define what is a TC; describe the process of its construction; identify its features; exemplify the processes of text alignment in TC; discuss methods of text analysis; propose for restructuring of translational units; define the process of extraction of translational equivalents; propose for generating bilingual lexical database and TermBank from a structured TC; and finally identify areas where a TC and information extracted from it may be utilized. Since construction of TC in Indian languages is full of hurdles, we try to construct a roadmap with a focus on techniques and methodologies that may be applied for achieving the task. The issues are brought under focus to justify the work that generated TC for some Indian languages for future reference and application. 1. What is a Translation Corpus ? Theoretically, a Translation Corpus (TC) suggests that it contains texts and their translation. It is entitled to include bilingual (and multilingual) texts as well as texts that may fit under translation. A TC, by virtue of its character and composition, is made of two parts: a text from a source language (SL) and its translation from a target language (TL) [15] [24], [39]. Although, a TC is normally bilingual and bidirectional [28], it can be multilingual and multidirectional as well [37], as it actually happens in case of the ILCI-I and ILCI-II projects for the Indian languages. In these two projects a new strategy is adopted where Hindi is treated as the only SL and several other Indian languages are treated as the TL (Fig. 1). The issue of multi-directionality can be understood if all the target languages can establish linguistic links with each other as they are linked up with SL. Since the ILCI-I TC has not tried to venture into this direction, it makes sense to keep the present discussion confined within a scheme of bilingualism and bi-directionality, with, for example, Hindi 
Past research has shown that various types of computer assistance can reduce translation effort and improve translation quality over manual translation. This paper directly compares two common assistance types – selection from lists of translation options, and postediting of machine translation (MT) output produced by Google Translate – across two significantly different subject domains for Chinese-to-English translation. In terms of translation effort, we found that the use of options can require less technical effort than MT post-editing for a domain that yields lowerquality MT output. In terms of translation quality, our analysis suggests that individual preferences play a more decisive role than assistance type: a translator tends to consistently work better either with options or with MT post-editing, regardless of domain and of their translation ability. 
 these cases, source-side reordering is used as a pre-  processing step to convert the source sentence to  Post-ordering of Statistical Machine  target language word order (Collins et al., 2005;  Translation (SMT) output to correct  Ramanathan et al., 2008). The best performing ap-  word order errors could be a promising  proaches generally rely on parse information on  area of research to overcome structural  the source side to generate the correct word or-  divergence between language pairs. This  der. However, it has proved to be a very difficult  is especially true when it is difficult to  problem which is far from being solved, especially  incorporate rich linguistic features into  when parse information is not forthcoming. The  the baseline decoder. In this paper, we  computational complexity of searching through a  propose an algorithm for generating oracle  large space of potential reorderings and the need  reorderings of MT output. We use the  for incorporating higher level linguistic informa-  oracle reorderings to empirically quan-  tion are the primary challenges in tackling the re-  tify an upper bound on improvement in  ordering problem.  translation quality through post-ordering  While there is active research in preordering and  techniques. In our study encompassing  in-decoder approaches, there has been little work  multiple language pairs, we show that  on the problem of post-ordering of SMT output.  significant improvement in translation  We define the word-order post-ordering as fol-  quality can be obtained by applying  lows:  reordering transformations on the output  Given the output of an MT system, permute the  of the SMT system. This presents a strong  words of the output to generate a better word or-  case for investing effort in exploring the  der.  post-ordering problem.  The following example shows how simply reorder-  
 adjectival modiﬁers or determiners precede  This paper presents a rule-based reorder-  their head  ing approach for English-Hindi machine  For such reasons and many more described in  translation. We have used the concept of  Section 4, one needs to reorder the sentences to  pada, from Pa¯n. inian Grammar to frame the reordering rules. A pada is a word  arrive at a natural translation. Statistically trained systems give good results  form which is ready to participate in a  in less amount of time and manual effort. But  sentence. The rules are generic enough  any statistically trained system requires huge par-  to apply on any English-Indian language  allel corpus. Indian languages (ILs) lack a reason-  pair. We tested the rules on English-Hindi  able parallel corpus size for English and IL pairs.  language pair and obtained better compre-  Hence, we explore a rule based approach for re-  hensibility score as compared to Google Translate on the same test set. In assess-  ordering taking insights from Pa¯n. inian Grammar (PG).  ing the effectiveness of the rules on padas  The available reordering approaches are dis-  which are analogous to minimal phrases in  cussed in Section 2. Our reordering approach is  English, we achieved upto 93% accuracy  described in Section 3. Section 4 talks about major  on the test data.  divergences between English and Hindi. Reorder-  
areas, this value is well below 40%1. In contrast,  
 The crucial use of a dialog system is to con-  vert simple yet complicated tasks from manual  In a task oriented domain, recognizing  to automated. The process of understanding  the intention of a speaker is important  and generating the dialogs is known as dialog  so that the conversation can proceed  modeling. In dialog modeling, to understand  in the correct direction. This is possi-  the dialogs, speaker’s intent must be recog-  ble only if there is a way of labeling  nized. The recognition of the speaker’s intent  the utterance with its proper intent.  is done with the help of Dialog Acts. Dia-  One such labeling techniques is Dia-  log Acts is a tagset that classiﬁes utterances  log Act (DA) tagging. This work fo-  based on pragmatic, semantic and syntactic  cuses on discussing various n-gram DA  features. Dialog Acts are similar to Austin’s  tagging techniques. In this paper, a  speech acts. According to Austin (1975), a  new method is proposed for DA tagging  speech act represents the meaning of an ut-  in Telugu using n-gram karakas with  terance at the level of illocutionary force. DA  back-oﬀ as n-gram language modeling  tagging is assigning a Dialog Act to an utter-  technique at n-gram level and Memory  ance from the given DA tagset.  Based Learning at utterance level. The results show that the proposed method is on par with manual DA tagging.  Earlier, research in DA tagging was limited to linguistic domain, but now with the help of statistics, machine learning and pattern  Keywords: Dialog Acts, Intention Recognition, Dialog System, n-grams, Karaka Dependencies, Back-oﬀ, Memory Based Learning.  matching, automated DA tagging with various DA recognition approaches (Král and Cerisara, 2012) have come into existence. Some of the DA tagging methods include word based DA tagging (Garner et al., 1996), which shows  
A metric has been proposed to automatically generate well-formed-ness rank for machine generated questions. The grammatical correctness of a question, challenge due to the negation in the source text and number of transformations required to convert an illformed (not appealing to humans) question to a well-formed (meaningful and human appealing) question seem the prominent features. We used 135 questions generated by Heilman and Smith’s system (Heilman and Smith, 2011), developed a regression model using our feature set and tested it. The Rsquared value is 93.5% which is quiet acceptable. Relevance of the model has been corroborated by means of the residual plot. (Heilman and Smith, 2011) Metric is of 187 features and that of (Liu, 2012) is of 11. We suggested 16 features for a general automatic question quality enhancer to look into. However, the present model has been built by considering only 10 out of them. (Heilman and Smith , 2011)generated questions do not possess any infirmities indicated by the remaining ones. 75 questions were used in training and 60 were tested. Recognition accuracy is 94.4%. 1. Introduction Automatic question generation systems (AQG) have been attempted since the mid-1970s (Wolfe J.H., 1976). A significant amount of work has been reported during the last decade, see details in Sec-  tion 2 below. To a certain extent AQGs have assisted human question-paper setters, but the quality of the automatically generated questions is often found to be inadequate. The strength and weaknesses of AQGs and a few remedies to enhance the quality of the questions have been discussed by us in (Salgaonkar and Divate, 2013). It is necessary to formally compute the worth of an AQG by employing a quality metric. Developing such a metric for a given set of questions and therefore for an AQG is a research problem. The quality metric would serve the purpose of evaluating any automatic system that claims to provide quality enhancement of a set of questions. Call such a system an “automatic question quality enhancer” (AQQE). In the present paper we propose a new quality metric for questions generated by an AQG, suggest a framework for an AQQE, and demonstrate the improvement in quality due to this framework. It goes without saying that the attributes of a question are the arguments of the quality function, and that, to improve the quality of a question, the attribute values have to be changed appropriately. Therefore the task of the AQQE is to rearticulate a given question in such a way that the changed attribute values yield a higher quality according to the metric. To improve the productivity of an AQG, our aim has to be optimization of the “objective function” of the metric, since we aspire to make a system to generate more and more questions that are acceptable to humans. 2. Previous Research  384 D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 384–393, Trivandrum, India. December 2015. c 2015 NLP Association of India (NLPAI)  AQGs that attempt to measure factual as well as deep learning, including reading comprehension, writing ability and vocabulary, have been proposed in (Kunichika et al., 2004; Brown et al., 2005; Liu et al., 2005; Chen et al., 2006; Liu et al., 2012). Systems generating MCQ type, fill-inthe-blank type or wh-type questions are another thrust area (Aist, 2001; Hashino and Hiroshi, 2005; Aldabe et al., 2006; Mitkov, 2006; Prasad et al.,2008; W. Chen et al., 2009; Chen et al., 2009; Heilman and Smith, 2009; Ali et al., 2010; Liske, 2011; Agarwal and Mannem, 2011a; Agarwal and Mannem, 2011b). Incorrect articulation of linguistic structures and an undue focus on relatively less important points are two commonly observed flaws in automatically generated questions. The statistics of the other types of errors observed in our experiments is given in the Table 1 below  Error Type No error Wrong WH Vague Formatting error Ungrammatical Does not make sense Co reference not resolve  % 43.10 30.17 11.21 6.90 5.17 4.31 3.45  Table 1: Flaws observed in system generated questions  According to (Heilman and Smith, 2009), only 27.3% of the questions among the ones generated through their system are acceptable [Henceforth, the two authors are abbreviated as H&S]. The rank of a question has been computed by employing the linear regression model with 187 features (H&S, 2010). Among them, 53 features are as follows: length-based (3), language model (6), grammatical aspects (23), transformations (8), vagueness (3), negation (1), and wh-words (9). The remaining 134 are values computed from binary histograms of length and count features that serve as thresholds for controlling non-linearity among the numerical parameters used for describing syntax, length and linguistic features. Empirical testing of the automatically generated questions revealed that out of the top ranked 20% (86 out of 428 questions), 52.3% were ac-  ceptable, where top rank is defined to be rank 3.5 and above on a 5-point scale, as selected by a team of human raters. (Liu, 2012) is another notable work in the area of AQGs. This system helps in review writing and extracting citations from a given text. Further, the system categorizes the text and identifies the keywords that are used to fill in suitable question templates from the authors’ question template library. Among the approaches used in (Liu, 2012) to rank a question, two give the best results: point-wise logistic regression and pair wise support vector machines (the later technique is named RankSVM). With 11 features, when considering the top 25% ranked questions the acceptability of RankSVM was found to be 75.8%, while that of logistic regression was 74.2%, whereas 71.3 % of the top 50% are found acceptable. This figure is comparable to the result of (H&S, 2009). Also, it confirms the later result (H&S, 2011) that the increase in the percentage of acceptable questions is statistically significant when considering only the top ranked ones. The 187 features of (H&S, 2010 and 2011) is arguably a large number, because a dozen features of (Liu, 2012) has led to a comparable result. Therefore the hypothesis arises whether it is possible to independently develop yet another reliable quality metric for questions using a model with a small number of features. The findings about our 10 features regression model are reported in this paper. Section 3 is on nomenclature that is to set up a background for a novice reader. Details of our experiment are listed in Section 4. Observations and interpretations are presented in Section 5. Findings are summarized in the end of the paper. 3. Nomenclature A question is ill-formed, i.e., unacceptable, if one or more flaws of the following types are observed in its articulation: incorrect grammar, semantic inadequacy, vagueness with respect to the answer, inadequate data, a wrong choice of WH form while presenting a question, or some editing is needed (H&S, 2009; Mannem et. al, 2010). A question is well-formed, hence acceptable, if it is not ill-formed, i.e., it does not have any of the 6 flaws mentioned above. A Base sentence is the source text for generating the questions.  385  An answer phrase is a machine-selected portion of the base sentence that is expected to be an answer to one or many potential questions that would be generated by an AQG. Example (base sentence): As is the case in a Parliamentary system, the government is formed by the party, alliance or group of assembly members who command the majority. (Answer phrase) by the party, alliance or group of assembly members who command the majority IFQ (generated by HSAQG): What is the government formed by? WFQ (manually generated): Who forms the government in a Parliamentary system? Flaw: Wrong choice of wh-form. 4. Methodology Base text database consisted of five randomly selected paragraphs, of length 1 to 6 sentences, from wiki articles on a variety of topics: University of Mumbai, Lokmanya Tilak as a journalist, Government of Maharashtra, renewable energy, solar technology [Appendix 1]. 135 questions were generated with HSAQG, out of which we randomly selected 75 as a training set. 13 questions (17.33% or a little over 1/6) were found to be redundant. Of the remaining 62, we found that 26 questions (42%) were acceptable to humans or WFQ, and 36 questions (58%) needed reframing in order to make them WFQs. It appears that there is enough scope for further improvement in systems like HSAQG, hence this is a substantive research problem. Inspection of the data reveals that the changes required to improve the acceptability of the AQG-generated questions include removal of formatting errors, extra precision of questions to enhance clarity, more complete selection of answer phrases. In the first step we analyzed the system generated IFQs with respect to the corresponding WFQs that were provided by a human expert. This exercise laid the foundation for generating the 16 di-  mensional vector representation of a question which is the core of our quality metric. In the Table 2 below we list a few sample IFQs from our experiment, the corresponding WFQs and the flaws observed in each case. Sentence simplification is another challenge in AQG systems. AQG system frames all possible questions from the modifier phrase, subordinate clause, appositives phrases, and from leading prepositional phrases. Those questions are unacceptable because they are vague or grammatically incorrect, or do not make sense. Our analysis reveals that AQGs fail to produce WFQs from the negative sentences. In the next step we had a team of 5 human raters rank each of the questions on a 5-point scale. The protocol for generating the rank of a question was that the majority carries the vote. In case of much divergence in perceived rank, we took cognizance of others’ views before choosing a rank. The sentiments of the raters were shared, which gave us an idea about what is not appealing to them. We formalized this feedback using the grammatical framework of English sentence structure. This exercise led to 16 rules for computing the rank of a question. These rules have been listed as R1 to R16. Only 10 rules (*-marked) were applicable for the H&S AQG that we used. Examples for remaining 6 rules (which are not *- marked) are taken from various internet sources. Illarticulations are underlined and corrections are in bold. R1(verbTenseInQuestion): Verb exists AND Verb Tense is correct => 1 else 0 Base sentence: The Martians landed near the aqueduct. IFQ: Where did the Martians landed? WFQ: Where did the Martians land? * R2(AuxVerb): AuxiliaryVerb is present in the base sentence AND (incorrect or no use of AuxiliaryVerb in the question) => 0 else 1 The University of Mumbai was established 1857 by Dr John Wilson (after whom Wilson College in Mumbai is named), according to  386  Sr Base Sentence No  Answer IFQ phrase  WFQ  Flaw  Flaw type  
 order of the sentences matter. If the problem was  This paper describes our system which  solves simple arithmetic word problems.  The system takes a word problem  described in natural language, extracts  information required for representation,  orders the facts presented, applies  procedures and derives the answer. It  then displays this answer in natural  language.  The emphasis of this  paper is on the natural language  processing(NLP) techniques used to  retrieve the relevant information from the  English word problem. The system shows  improvements over existing systems.  “John gave 1 apple to Mary. He has 3 apples now. How many apples did John have?”, then the answer is completely different. To elaborate, the problem solving process begins by processing the question that is expressed in natural language. After processing the question, the information required by the knowledge representation is extracted. This is a knowledge-based natural language processing system. Hence, the domain knowledge provided by the underlying representation can also help clear ambiguities faced by the natural language processor. For example, one of the heuristics used is described as follows. If the problem is, “There  
Pause plays important roles for the intelligibility, naturalness and fluency of speech. This paper reported the effect of native (L1) Bengali speakers’ fluency of English on occurrence probability and duration of sentencemedial pauses with respect to three factors: phrase type, phrase length (l), distance (d). In this analysis, 40 nonnative (L2) English (L1 Bengali) speakers’ data was divided into five different groups (poor, average, good, very good and excellent) based on their English fluency level. From result of this comparative study, it is seen that occurrence probability and duration of sentence-medial pauses for each phrase type, each l value and each d value increase as L2 English speakers’ fluency decreases. Moreover like L1 English speakers, occurrence probability and duration of sentence-medial pauses are almost linearly dependent on l and d respectively for L2 English speakers regardless of their fluency. Furthermore effect of three factors on sentencemedial pauses of fluent L2 English speakers is more close to that of L1 English speakers compared to less fluent L2 English speakers. 
 speakers is one of the few languages which have both  The goal of this paper is to conduct an acoustic phonetic investigation of both primary and secondary cues that aid in the distinction between voiced and voiceless geminates in Bangla. Results of the statistical analyses examining both duration and nondurational correlates show that besides closure duration secondary cues such as the amplitude of the stop release burst and the fundamental frequencies of the vowels immediately preceding and following the obstruent are acoustically signiﬁcant in the distinction between voiced and voiceless geminates and singletons. Voiced stops have signiﬁcantly greater burst amplitudes than voiceless ones, and geminates are ﬂanked by vowels with signiﬁcantly higher F0’s than those ﬂanking corresponding singletons.  voiced and voiceless geminates in their inventory. Work on gemination in Bangla has been sparse. The only existing study on Bangla geminates focuses on voiceless stops and does not take voiced geminates into account (Lahiri and Hankamer, 1988; Hankamer et al., 1989). This paper aims to ﬁll that gap by taking voiced geminates and the effects they might have on acoustic cues into consideration. It also takes a look at how vowel to vowel coarticulation might be affected because of gemination, especially in the case of dental stops. The articulatory motivation is that there should be some lingual fronting in the case of dental geminates, in order to maintain air pressure for a longer duration, and this fronting should then pose higher resistance to V-to-V coarticulation. The rest of the paper is organised as follows: In Section 2, we cover a brief overview of work done on geminates and outline the aims and motivations of the current study. Section 3 presents the materials and  We also brieﬂy explore the effects of gemi-  methods used for the study, while Section 4 discusses  nation on V-to-V coarticulation. The assump-  the parameters measured in the study. Section 5 elu-  tion is that longer consonantal duration will  cidates the statistical analysis employed for drawing  act as a more effective barrier to V-to-V coar-  inferences from the data. Section 6 deals with the F2  ticulation in the case of geminates as opposed  locus equations ﬁtted for investigating the variation  to singletons. Particularly, in the case of  of the degree of coarticulatory resistance. Section 7  dental geminates, we hypothesize that longer  collates the results, and Section 8 concludes the work  consonantal duration contributes to lingual  with a discussion on the inferences drawn from anal-  fronting, which manifests itself as greater re-  ysis and explores issues that need further work.  sistance to V-to-V coarticulation.  
We present the ﬁrst application of the adjunct/argument distinction to Hierarchical Phrase-Based SMT. We use rule labelling to characterize synchronous recursion with adjuncts and arguments. Our labels are bilingual obtained from dependency annotations and extended to cover nonsyntactic phrases. The label set we derive in this manner is extremely small, as it contains only thirty-six labels, and yet we ﬁnd it useful to cluster these labels even further. We present a clustering method that uses label similarity based on left-hand-side/right-hand-side joint trained-model estimates. The results of initial experiments show that our model performs similarly to Hiero on in-domain French-English data. 
The idea to improve MT quality by using deep linguistic and knowledge-driven information has frequently been expressed. If the goal is to use deep information for building an MT system, there are two extreme options: (1) to start from a purely knowledge-driven approach (RBMT) and try to arrive at the same recall found in current SMT systems; (2) to start from an SMT system and try to arrive at higher precision by modifying it so that more knowledge drives the translation process. The system architecture we will describe in this paper starts in the middle of these extreme options. It is a hybrid architecture that we take as a starting point for future experiments and extensions to increase MT quality by more knowledge-driven processing. 
Compounding is a highly productive word-formation process in some languages that is often problematic for natural language processing applications. In this paper, we investigate whether distributional semantics in the form of word embeddings can enable a deeper, i.e., more knowledge-rich, processing of compounds than the standard string-based methods. We present an unsupervised approach that exploits regularities in the semantic vector space (based on analogies such as “bookshop is to shop as bookshelf is to shelf”) to produce compound analyses of high quality. A subsequent compound splitting algorithm based on these analyses is highly effective, particularly for ambiguous compounds. German to English machine translation experiments show that this semantic analogy-based compound splitter leads to better translations than a commonly used frequency-based method. 
Source-side reordering has recently seen a surge in popularity in machine translation research, often providing enormous reductions in translation time and showing good empirical results in translation quality. For many language pairs, however—especially for translation into morphologically rich languages—the assumptions of these models may be too crude. But while such language pairs call for more complex models, these could increase the search space to an extent that would diminish their beneﬁts. In this paper, we examine the question whether purely syntaxoriented adaptation models (i.e., models only considering word order) can be used as a means to delimit the search space for more complex morphosyntactic models. We propose a model based on a popular preordering algorithm (Lerner and Petrov, 2013). This novel preordering model is able to produce both n-best word order predictions as well as distributions over possible word order choices in the form of a lattice and is therefore a good ﬁt for use by richer models taking into account aspects of both syntax and morphology. We show that the integration of non-local language model features can be beneﬁcial for the model’s preordering quality and evaluate the space of potential word order choices the model produces. 
In this document we report on a user scenario based evaluation aiming at assessing the performance of a machine translation (MT) system in a real context of use. This extrinsic evaluation exempliﬁes a framework that makes it possible to estimate MT performance and to verify if improvements of MT technology lead to better performance in a real usage scenario. We report on the evaluation of Moses baselines for several languages in a cross-lingual IT helpdesk scenario. 
The existence of translation divergence precludes straightforward mapping in machine translation (MT) system. An increase in the number of divergences also increases the complexity, especially in linguistically motivated transfer-based MT systems. In other words, divergence is directly proportional to the complexity of MT. Here we propose a divergence index (DI) to quantify the number of parametric variations between languages, which helps in improving the success rate of MT. This paper deals with how to build divergence index for a given language pair by giving examples between Telugu and Tamil, the major Dravidian languages spoken in South India. It also proposes handling strategies to overcome these divergences. The presentation of the paper also includes a live demo of Telugu-Tamil MT. 
Deep-syntax approaches to machine translation have emerged as an alternative to phrase-based statistical systems, which seem to lack the capacity to address essential linguistic phenomena for translation. As an alternative, TectoMT is an open source framework for transfer-based MT which works at the deep tectogrammatical level and combines linguistic knowledge and statistical techniques. This work describes the development of machine translation systems for EnglishSpanish in both directions, leveraging on the modules for the English-Czech TectoMT system. We show that it is feasible to develop basic systems with relatively low effort in 9 months. Our evaluation shows that despite not yet being able to beat a phrase-based statistical system, the TectoMT architecture offers ﬂexible customization options, which considerably increase the BLEU scores. 
Despite the common assumption that word sense disambiguation (WSD) should help to improve lexical choice and improve the quality of the output of machine translation systems, how to successfully integrate word senses into such systems remains an unanswered question. While signiﬁcant improvements have been reported using reformulated approaches to the disambiguation task itself – most notably in predicting translations of full phrases as opposed to the senses of single words – little improvement or encouragement has been gleaned from the incorporation of traditional WSD into machine translation. In this paper, we present preliminary results that suggest that incorporating output from WSD as contextual features in a maxent-based translation model yields a slight improvement in the quality of machine translation and is potentially a step in the right direction, in contrast to other approaches to introducing word senses into a machine translation system which signiﬁcantly impede its performance. 
In this work lexical choice in generation for Machine Translation is explored using lexical semantics. We address this problem by replacing lemmas with synonyms in the abstract representations that are used as input for generation, given a WordNet synset. In order to ﬁnd the correct lemma for each node we propose to map dependency trees to Hidden Markov Trees that describe the probability of a node given its parent node. A tree-modiﬁed Viterbi algorithm is then utilized to ﬁnd the most probable hidden tree containing the correct lemmas given their context. The model is implemented in a Machine Translation system for English to Dutch. The output sentences, generated from the modiﬁed dependency structures, contained a lot of erroneous substituted words. This is mainly due to the fact that a large amount of synsets, used as input for the model, are incorrect. The input to the model now contains the synset that is most frequent given the lemma in general, not the optimal synset given the domain of the sentences. We therefore propose to implement a domain speciﬁc WSD-system in our pipeline in future work. 
This study explores methods for developing a large scale Quality Estimation framework for Machine Translation. We expand existing resources for Quality Estimation across related languages by using different transfer learning methods. The transfer learning methods are: Transductive SVM, Label Propagation and Self-taught Learning. We use transfer learning methods on the available labelled datasets, e.g. en-es, to produce a range of Quality Estimation models for Romance languages, while also adapting for subtitling as a new domain. The Self-taught Learning method shows the most promising results among the used techniques. 
We present an implementation of domain adaptation by translation model interpolation in the TectoMT translation system with deep transfer. We evaluate the method on six language pairs with a 1000-sentence in-domain parallel corpus, and obtain improvements of up to 3 BLEU points. The interpolation weights are set uniformly, without employing any tuning. 
In this paper, we present some preliminary results on Statistical Machine Translation from Bulgarian-to-English and English-to-Bulgarian. Linguistic knowledge has been added gradually as factors in the MOSES system. The tests were performed on the QTLeap corpus data in IT domain for Pilot 1. The training was done on news parallel data as well as on IT domain data. The BLEU scores show that the addition of linguistic knowledge improves the Machine Translation. 
In this paper, we address the problem of machine translation (MT) of domain-speciﬁc texts for which large amounts of parallel data for training are not available. We focus on the IT domain and on English to Portuguese machine translation, and compare different strategies for improving system performance over two baselines, the ﬁrst using only large dataset of out-of-domain data, and the second using only a small dataset of in-domain data. Our results indicate that adding a domain-speciﬁc bilingual lexicon to the training dataset signiﬁcantly improves the performance of both a hybrid MT system and a PBSMT system, while adding out-of-domain sentence pairs to the training dataset only improves the performance of a hybrid MT system. Furthermore, we perform a human evaluation of the sentences generated by the hybrid MT system and the standard PBSMT system built using the same training datasets. The results indicate some signiﬁcant differences between those two MT approaches in this speciﬁc task. 
Campus do Pici, Bloco 942-A – CEP 60455-760 – Fortaleza – CE – Brazil jclext@great.ufc.br Abstract. JCLexT is a compiler of finite-state transducers from full-form lexicons, this tool seems to be the first Java implementation of such functionality. A comparison between JCLexT and Foma was performed based on extensive data from Portuguese. The main disadvantage of JCLexT is the slower compilation time, in comparison to Foma. However, this is negated by the fact that a large transducer compiled with JCLexT was shown to be 8.6% smaller than the Foma created counterpart. 1. Introduction Finite-state transducers (FSTs) have been the preferred devices used to implement morphological parsers (Trommer, 2004). They store information in a compact manner and allow for quick lookup times, being superior to concurring alternatives. Computing is increasingly moving towards mobile platforms. Due to this, the need for natural language processing tools which are designed for the specifics of this setting has emerged. Alencar et al. (2014), for example, addresses this issue with the proposal of JMorpher, a finite-state morphological parser in Java able to natively run on Android devices. However, the JMorpher tool is limited in its functionality, as it was designed to apply an existing finite-state transducer to an input text. JCLexT aims to resolve this limitation, it is a Java tool that can compile a fullform lexicon into a finite-state transducer. JCLext emulates the read spaced-text command found in XFST (Beesley and Karttunen, 2003), Foma (Hulden, 2009) and HFST (Lindén, Silfverberg and Pirinen, 2009). It is the first Java tool of this sort that we are aware of, furthermore it was not based on any existing implementation. JCLexT was inspired by the minimization algorithm for acyclic deterministic automata (DFSA) proposed by Revuz (1992). As a pure Java implementation, JCLexT inherits the advantages of this programming language, this includes better portability, such as being able to run on Android, desktop, servers or as a web service with minor changes to the existing implementation. Furthermore being written in Java keeps JCLexT platform compatible with the existing JMorpher software. The main purpose of this work was to emulate and improve on the existing functionality of Foma, with the goal of trying to achieve better results with very large full-form lexicons. * R. M. C. Andrade is a CNPq Research Fellow, DT Level 2. For invaluable contributions, we are grateful to Priscila Sales, Kleber Bernado, and Pedro Belmino. 15  JCLext: A Java Tool for Compiling Finite-State Transducers from Full-Form Lexicons It is worth noting as a result of this work we created LEXPT01, a lexical transducer of Portuguese which contains approximately 4 million paths. As far as we know, this is the largest lexical transducer of Portuguese currently in existence. 2. Algorithmic issues In this paper, we skip the details of the main algorithm that underlies JCLexT. This algorithm is responsible for compiling a full-form lexicon into an FST, this has a reduced size in comparison to a baseline resulting from the simple union of the transducers encoding each individual word-parse pair, as was formulated for finite-state automata by Jurafsky and Martin (2009). Although the minimization algorithm proposed by Revuz influenced the design of JCLexT's transducer size-reduction algorithm, differences do exist. Firstly, as Revuz formulated his algorithm in a relatively high level pseudo-code, a direct port to Java is difficult due to programming constraints. Secondly, there is a fundamental difference between acyclic DFSAs and FSTs, since the latter are not always determinizable and minimizable, see e.g. Jurafsky and Martin (2009), Beesley and Karttunen (2003), and Allauzen and Mohri (2003). Only p-subsequentializable transducers can be determinized and minimized. By contrast, simple automata, i.e. acceptors, are always determinizable and minimizable. One important aspect of full-form lexicons is that they typically contain ambiguities and word-parse pairs of unequal length. As a consequence, FSTs compiled from such lexicons with Foma and XFST have arcs labeled with epsilons on the input side and are non-sequential. They are of the type which are classified as restricted by Alencar et al. (2014), being processed much faster than unrestricted FSTs. Three goals were pursued in designing the compiling algorithm. Firstly, the generated FSTs should be restricted. Secondly, significant compression of the source should be achieved and finally, the FSTs generated should be superior to the FSTs produced by Foma. 3. Test Data We tested JCLexT with LEXPT01, our own Portuguese lexical transducer with approximately 4 million paths. It was created primarily from FST03, an existing resource containing about 1.3 Million paths (Alencar et al., 2014), by extending its coverage to handle different orthographies and productive word-formation processes. Foma's print lower-words command was applied to LEXPT01, extracting its lower language. Then Foma's flookup utility was used to parse this language with LEXPT01. This output was the full-form lexicon DIC. In order to assess if FSTs generated by JCLexT behaved in an identical manner to analogous FSTs compiled by Foma with respect to unknown words, two corpora were used. The first one, MACM, is a slightly modified version of the Mac-Morpho corpus distributed with NLTK (Bird, Klein and Loper, 2009). The second being a word list from Projecto Natura referred to in this work as NAT.1 Both corpora contain about one million items. 
{daniela.amaral, maiki.buffet}@acad.pucrs.br, renata.vieira@pucrs.br Abstract. Conditional Random Fields (CRF) is a probabilistic Machine Learning (ML) method based on structured prediction. It has been applied in several areas, such as Natural Language Processing (NLP), image processing, computer vision, and bioinformatics. In this paper we analyse two different notations for identifying the words that compose a Named Entity (NE): BILOU and IO. We found out that IO notation presents better results in F-measure than BILOU notation in all categories of HAREM corpus. 
2Departamento de Letras Modernas (DLM) Universidade Estadual Paulista (UNESP) - Araraquara, SP – Brazil {deboradom@gmail.com, bento.silva@gmail.com} Abstract. This article presents the idea of the PrepNet.Br, a semantic network consisting of synsets of prepositions to be built for Brazilian Portuguese along the lines of the French PrepNet [Saint-Dizier 2005]. This enterprise is of relevance to the linguistic description of the class of prepositions as well as to Natural Language Processing resource-building. The construction of this network also brings a linguistic analysis alternative to the traditional one, investigating the polysemic nature of prepositions within a cognitive view of language. 1. Introduction In the last decades, many studies have been made about nouns, verbs and adjectives, both in Linguistics and Computational Linguistics. The study of prepositions, however, has been more modest due to its high degree of polysemy, which makes it difficult to predict their semantics and their realizations across different languages [Saint-Dizier 2006]. For example, while in Portuguese the same preposition can occur in three distinct contexts - Você mora no (em+o) campo; Você conhece pessoas na (em+a) festa; and Você entra em férias -, in English, we have three different prepositions (in, at and on) in the very same contexts - You live in the country; You meet people at the party; and You go on holiday” [Taylor 1995], respectively. An accurate model of preposition usage is crucial to avoid repeatedly making errors in terms of parsing and generation. This paper outlines the idea of a semantic network of prepositions to be built for Brazilian Portuguese – the PrepNet.Br – that aims to do a better characterization of this part of speech. 2. Prepositions and different approaches In previous studies about prepositions, the biggest challenge has always been to define its semantic value, since there is a tradition that excludes or limits the inherent significance of these items to the syntax. For example, traditional grammars usually describe prepositions as grammatical items that receive some kind of meaning only in context. This perspective assumes that the native speaker must master the prepositions one by one, because their use is idiomatic and therefore "must be memorized". This results in an unsystematic characterization of prepositions, in which there is no agreement regarding its semantics. 75  PrepNet.Br: a Semantic Network for Prepositions. However, the facility that the native speakers learn how to use prepositions in every new context indicates that its usage is highly structured by human mind and the alleged flexibility in the use of prepositions would be guided by some (mental) logic, not just memorization. As a consequence of this perception, there are new studies that look to the plurality of meanings that each preposition takes in different contexts from a semantic point of view. To the cognitive framework, for example, prepositions do have a certain kind of meaning – their meaning is probably more complex than the meanings of other lexical categories – and the many uses of a preposition are considered “extensions of its core meaning”. Thus, prepositions have been discussed as polysemous items. [Castilho 2010]. The cognitive framework supports the idea of a lexicon-grammar continuum, where "all the prepositions, regardless of their degree of grammaticalization, can introduce adjuncts, expressing various relations and senses"1 [Ilari et.al. 2008]. For example, the prepositions “de” (Portuguese) and “on” (English), instantiate either grammatical items (Eu dependo de você / I depend on you) or lexical items, which bear semantics (Maria é de São Paulo / Maria is from São Paulo (Source), and O jornal está sobre o tapete / The newspaper is on the mat (Spatial Location)). Once presented the view that prepositions are not only grammatical items, we introduce the idea of the PrepNet.Br: a semantic network of prepositions whose structure aims to represent their meanings and usage in terms of a particular computational linguistic representation. 3. A network of prepositions From a technological point of view, prepositions have great importance to enrich and assist Natural Language Processing (NLP) applications, since they encode essential meanings for understanding the propositions (the logical and conceptual meanings of sentences). For example, prepositions conceptualize location (put the book on the shelf), instrumentality (cut the meat with a knife), origin-goal (traveled from São Paulo to Rio de Janeiro), recipient (the wine given to his friend), time interval (arrive between noon and 1 p.m.) and space location (it was between the table and the wall). These concepts are vital to precise language understanding. A PrepNet network is a computational linguistic resource for NLP stemming from Saint-Dizier (2005, 2006): a repository of prepositions from different languages with a formal specification of their syntactic and semantic behaviors. This idea has three motivations: (i) to construct a system similar to wordnets [Miller; Fellbaum 1991] and with the possibility to complement them; (ii) to assign thematic roles [Bakker; Siewierska 2002; Jackendoff 1991], and, above all, (iii) to reach a more complete and robust conceptual description of prepositions. Saint-Dizier (2005) believes that a PrepNet should be the starting point for a better characterization of prepositions, necessary before analyzing their interaction with verbs, for example. 
Caixa Postal: 668 – CEP: 13566-970 – São Carlos/SP paulastm@gmail.com; taspardo@icmc.usp.br Abstract. Automatic multi-document summarization aims at selecting the essential content of related documents and presenting it in a summary. In this paper, we propose some methods for automatic summarization based on Rhetorical Structure Theory and Cross-document Structure Theory. They are chosen in order to properly address the relevance of information, multidocument phenomena and subtopical distribution in the source texts. The results show that using semantic discourse knowledge in strategies for content selection produces summaries that are more informative. Resumo. Sumarização automática multidocumento visa à seleção das informações mais importantes de um conjunto de documentos para produzir um sumário. Neste artigo, propõem-se métodos para sumarização automática baseando-se em conhecimento semântico-discursivo das teorias Rhetorical Structure Theory e Cross-document Structure Theory. Tais teorias foram escolhidas para tratar adequadamente a relevância das informações, os fenômenos multidocumento e a distribuição de subtópicos dos documentos. Os resultados mostram que o uso de conhecimento semântico-discursivo para selecionar conteúdo produz sumários mais informativos. 1. Introduction Automatic Multi-Document Summarization (MDS) aims at selecting the relevant information from multiple documents on the same topic to produce a summary (Mani, 2001). It has seen increasing attention because it can be useful in a variety of areas, mainly due to help coping with information overload. Two main approaches are generally considered in MDS. The superficial approach uses statistical or some limited linguistic information to build a summary, usually has low cost and is more robust (Haghighi and Vanderwende, 2009; Ribaldo, 2013; Castro Jorge, 2015). The deep approach uses linguistically motivated assumptions and demands high-cost resources, but it produces summaries of higher quality in terms of information, coherence and cohesion (Marcu, 1997; Afantenos et al., 2007; Uzêda et al., 2010; Castro Jorge and Pardo, 2010). However, studies based on superficial or deep knowledge do not deal jointly with relevance of different sentences in a source text, multi-document phenomena and subtopics. 81  Joint semantic discourse models for automatic multi-document summarization In a source text, some sentences are more important than others because of their position in the text or in a rhetorical structure, thus, they cannot be treated uniformly (Wan, 2008). In the case of news texts, it is known that the first or leading paragraph usually expresses the main fact reported in the news. Therefore, selecting sentences from the beginning of the text could be a good summary (Saggion and Poibeau, 2013). More sophisticated techniques use analysis of the discourse structure of texts for determining the most important sentences (Marcu, 1997; O’Donnell, 1997; Uzêda et al., 2010). In order to deal with multi-document phenomena such as redundant, contradictory and complementary information, that occur in a collection of texts, approaches that achieve good results use multi-document semantic discourse models (Radev, 2000; Zhang et al., 2002; Castro Jorge and Pardo, 2010; Kumar et al., 2014). However, those works are not concerned about the relevance of sentences in each text together with multi-document phenomena as a human does when writing a summary. Another feature is that each text of a collection develops the main topic, exposing different subtopics as well. A topic is a particular subject that we write about or discuss, and subtopics are represented in pieces of text that cover different aspects of the main topic (Hearst, 1997; Salton et al., 1997; Hennig, 2009). For example, a set of news texts related to an earthquake typically contains information about the magnitude of the earthquake, its location, casualties and rescue efforts (Bollegala et al., 2010). There are some proposals that combine the subtopical structure and multi-document relationship (Salton et al., 1997; Wan, 2008; Harabagiu and Lacatusu, 2010) to find important information, but without treating the salience of a sentence in its text. We may say that current strategies for MDS have separately used each of the three criteria of relevance of information, multi-document phenomena and subtopical distribution, resulting in summaries that are not representative of the subtopics and less informative than they could be. However, human summarization behaviour looks at (i) the subtopics and rhetorical structure of texts to select content (Jaidka et al., 2010) and considers that (ii) the redundant information (that is repeated across texts) tends to be important (Mani, 2001). Therefore, we need effective summarization methods to analyze the information from different texts and produce informative summaries. As an example, Figure 1 shows an automatic multi-document summary produced from two texts organized in four subtopics related to the health of Maradona, the famous Argentine soccer player: the history of Maradona’s disease, current state of health, messages of support and Maradona’s relapse. The summary has repeated content (highlighted in bold) and sentences are only from two subtopics: current state of health (S1 and S3) and Maradona’s relapse (S2). The summary would be better if the three criteria for summary production had been used. In this paper, we propose to model the process of MDS using semantic discourse theories, in order to properly address the three cited criteria. To do that, we choose the theories RST (Rhetorical Structure Theory) (Mann e Thompson, 1987) and CST (Crossdocument Structure Theory) (Radev, 2000) due to their importance for automatic summarization described in many works (O’Donnell, 1997; Marcu, 1997; Zhang et al., 2002; Castro Jorge and Pardo, 2010; Castro Jorge, 2015). The RST model details major aspects of the organization of a text and indicates relevant discourse units. The CST 82  Joint semantic discourse models for automatic multi-document summarization model, in turn, describes semantically related textual units from topically related texts. We present some methods for content selection, aiming at producing more informative and representative summaries from the source texts. For this purpose, we use a multidocument corpus manually annotated with RST and CST. The methods produce satisfactory results, improve the state of the art and indicate that the use of semantic discourse knowledge positively affects the production of informative extracts. To the best of our knowledge, this is the first time RST and CST are combined in methods for MDS. Both theories' relations are domain-independent. [S1] “Maradona had a relapse in acute hepatitis. Now he is stable. Despite he had got better on Sunday, he should continue hospitalized”, said Cahe to the news La Nación. [S2] Hospitalized in Buenos Aires, he had a relapse and felt pain again due to acute hepatitis, according to his personal doctor, Alfredo Cahe. [S3] Cahe said that Maradona had not started to drink alcoholic beverages again, and that the causes of the relapse are being investigated. Figure 1: Example of multi-document summary (Castro Jorge and Pardo, 2010) The remainder of this paper is organized as follows: Section 2 gives a brief background about the semantic discourse models RST and CST; Section 3 presents some related work; Section 4 shows the developed methods for MDS; the corpus is described in Section 5; Section 6 presents some results; Section 7 presents some final remarks. 2. Discourse knowledge RST (Mann and Thompson, 1987) is a descriptive theory of major aspects of the organization of a text. It represents relations among propositions in a text and discriminates nuclear (i.e., important propositions) and satellite (i.e., additional information). Each sentence may be formed by one or more propositions. Relations composed of one nucleus and one satellite are named mononuclear relations. On the other hand, in multinuclear relations, two or more units participate and are equally important. The relationships are traditionally structured in a tree-like form (where larger units – composed of more than one proposition – are also related in the higher levels of the tree). RST is probably the most used discourse model in computational linguistics and has influenced works in all language processing fields. Particularly for automatic summarization, it takes advantage of the fact that text segments are classified according to their importance: nuclei are more informative than satellites. Inspired by RST and other researches, CST appears as a theory for relating text passages from different texts on the same topic (Radev, 2000). It is composed by a set of relations that detect similarities and differences among related texts. Differently from RST, CST was devised mainly for dealing with multi-document organization. The relations are commonly identified between pairs of sentences, coming from different sources, which are related by a lexical similarity significantly higher than random. The result of annotating a group of texts is a graph, which is probably disconnected, since not all segments present relations with other segments. CST was applied in MDS studies for English (Zhang et al., 2002; Kumar et al., 2014) and Portuguese texts (Castro Jorge and Pardo, 2010). These researchers take advantage of the fact that CST relationships indicate relevant information between sources and facilitate the processing of multidocument phenomena. 83  Joint semantic discourse models for automatic multi-document summarization 3. Related work There are several works based on semantic discourse knowledge for MDS. Zhang et al. (2003) replace low-salience sentences with sentences that maximize the total number of CST relations in the summary. Afantenos et al. (2007) propose a summarization method based on pre-defined templates and ontologies. Kumar et al. (2014) take into account the generic components of a news story within a specific domain, such as who, what and when, to provide contextual information coverage and use CST to identify the most important sentences. Castro Jorge (2015) incorporates features given by RST to generative modelling approaches. For news texts in Brazilian Portuguese, the state of the art consists in two different summarization approaches of Castro Jorge and Pardo (2010) and Ribaldo (2013). Based on deep knowledge, Castro Jorge and Pardo developed the CSTSumm system that employs CST relations to produce preference-based summaries. Sentences are ranked according to the number of CST relationship they hold. Ribaldo, in turn, took advantage of superficial knowledge and developed a multi-document system, called RSumm, which segments texts into subtopics using TextTiling (an adapted version for Portuguese, described in Cardoso et al., 2013) and group the subtopics using measures of similarity. After clustering, a relationship map is created and the relevant content is selected by the segmented bushy path (Salton et al., 1997). In the segmented bushy path, at least one sentence of each subtopic is selected to compose the summary. As we can see, those works do not combine semantic discourse knowledge such as RST and CST for content selection. In this study, we argue that the semantic discourse knowledge improves the process of MDS. 4. The CSTNews corpus Our main resource is the CSTNews1 corpus (Cardoso et al., 2011), composed of 50 clusters of news articles written in Brazilian Portuguese, collected from several sections of mainstream news agencies: Politics, Sports, World, Daily News, Money, and Science. The corpus contains 140 texts altogether, amounting to 2,088 sentences and 47,240 words. On average, the corpus conveys in each cluster 2.8 texts, 41.76 sentences and 944.8 words. Besides the original texts, each cluster conveys single-document manual summaries and multi-document manual and automatic summaries. The size of each summary corresponds to 30% of the size of the biggest text in the cluster (considering that the size is given in terms of the number of words). All the texts in the corpus were manually annotated with RST and CST structures in a systematic way, with satisfactory annotation agreement values. 5. Methods for MDS In this section, we describe how RST, CST and subtopics may be used together in some strategies for content selection. This investigation was organized in three groups: (1) methods based solely on RST, (2) methods that combine RST and CST, and (3) 
{lucelene.lopes,renata.vieira}@pucrs.br Abstract. This paper proposes a technique to build entity proﬁles starting from a set of deﬁning corpora, i.e., a corpus considered as the deﬁnition of each entity. The proposed technique is applied in a classiﬁcation task in order to determine how much a text, or corpus, is related to each of the proﬁled entities. This technique is general enough to be applied to any kind of entity, however, this paper experiments are conduct over entities describing a set of professors of a computer science graduate school through their advised M.Sc. thesis and Ph.D. dissertations. The proﬁles of each entity are applied to categorize other texts into one of the builded proﬁles. The analysis of the obtained results illustrates the power of the proposed technique. 1. Introduction The amount of available written material is larger than ever, and it clearly tends to keep growing as not only new material is made available, but also previously produced material is being digitalized and made accessible through the Internet. Often the search for information tends to ﬁnd as obstacle not the unavailability of texts, but the impossibility to read all available material. In such abundant data environment, the challenge is to automatically gather information from text sources [Balog et al. 2013]. The focus of this paper is to gather information in order to proﬁle entities considering the existence of written material characterizing these entities [Zhou and Chang 2013]. Once these entities are dully proﬁled, many applications of the proﬁles may be envisaged [Liu and Fang 2012]. Therefore, this paper objective is to proposed a technique to proﬁle entities according to deﬁning corpora, i.e., a corpus capable to characterize each entity. Additionally, we exemplify the application of such entities proﬁles to categorize texts according to their great or small similarity to each entity. Speciﬁcally, we chose as entities a group of professors acting on a graduate Computer Science program and we consider as the deﬁning texts of each professor the M.Sc. and Ph.D. dissertations produced under his/her advisory. Therefore, each professor is proﬁled according to the produced texts under his/her supervision, and these proﬁles are applied to compute the similarity of other texts to each professor’s production, thus allowing to categorize other texts with respect to each professor. It is important to call the reader attention that the proposed proﬁling procedure can be applied to any set of entities giving that deﬁning corpora characterizing each entity are available. Also, the exempliﬁed application to categorize texts by the similarity to each entity could be replaced by other applications without any loss of generality. 91  Building and Applying Proﬁles Through Term Extraction  This paper is organized as follows: the next section brieﬂy presents related work; Section 3 describes the proposed technique to build proﬁles; Section 4 exempliﬁes the application of builded proﬁles to categorize texts; Section 5 presents practical experiments of the proposed technique to a practical case. Finally, the conclusion summarizes this paper contribution and suggests future works.  2. Related Work  Automatic proﬁling entities is, at the same time, an interesting research topic [Wei 2003, Liu and Fang 2012], and a complex task with important economic potential [Kummamuru and Krishnapuram 2007].  For instance, Liu and Fang [2012] propose two methods to build entities proﬁles  for research papers published in a speciﬁc track of a speciﬁc conference. In their work,  Liu and Fang made Approaches (KBA)  traanckexopferthime e2n1tstprToeﬁxltinRgeptraiepvearl  published in Conference,  the Knowledge-Based TREC 2012. For this  experiment, the authors consider 29 entities (topics) manually chosen from the English  collection of Wikipedia that were representative of topics usually covered by KBA track  papers along the previous editions.  Basically, Liu and Fang’s methods perform the computation of a numerical score based on the number of occurrences of the entity names found in each paper. The methods differences rely on the use of weighting schemas to estimate the relevance of each occurrence according to the presence of co-occurrence of other entities. The conclusions of Liu and Fang indicate that these methods were effective to select relevant documents among the papers appearing in TREC 2012 proceedings.  Another related work worth mentioning is the paper authored by Xue and Zhou [2009] that proposes a method to perform text categorization using distributional features. This work does not explicitly mention the construction of entity proﬁles, but Xue and Zhou’s method do create a descriptor of each possible category to be considered in the form of features. In such way, the category descriptors can be easily viewed as the category proﬁle, and the categorization itself can be viewed as the computation of similarities between each category proﬁle and each text features.  Putting our current work in perspective with these related works, our proposed technique carries on a proﬁle building task that is similar to Xue and Zhou’s category descriptors. The main difference of our approach, however, resides on the descriptors contents. While Xue and Zhou’s techniques are generic features (number of words, etc.) found in the texts, our descriptors are remarkable terms (most relevant concept bearing terms) found in the texts. In this sense our work can be seen as an evolution of [De Souza et al. 2007].  Our proposed text categorization is similar to Liu and Fang’s score computation, since we also compute a similarity index to estimate how related a text is to each entity. The main difference between Liu and Fang’s and our approach resides in the speciﬁc score formulation. While Liu and Fang’s observe co-occurrences of entities names, our approach weights more relevant concepts bearing terms found at the entities describing corpora and at the texts to categorize. In this sense, we revisit an old approach [Cavnar and Trenkle 1994], but we use a more effective term extraction.  92  Building and Applying Proﬁles Through Term Extraction  3. Building Proﬁles Through Term Extraction from Corpora The proposed technique starts creating entities descriptors, i.e., a set of data associated to each entity that summarizes the relevant information for each entity. In our approach these descriptors are basically a set of relevant concept bearing terms found in the entity’s deﬁning corpus. To obtain these terms we perform a sophisticated term extraction procedure [Lopes and Vieira 2012] followed by a relevance index computation [Lopes et al. 2012]. Speciﬁcally, we submit the deﬁning corpora of all entities to an extraction procedure that is actually performed in two steps: The texts are syntactically annotated by the parser PALAVRAS [Bick 2000]; The annotated texts are submitted to ExATOlp [Lopes et al. 2009] that performs the extraction procedure and relevance index computation. It is important to mention that our proposed technique can be applied with other tools to text annotation or term extraction with, at the authors best knowledge, no loss of generality.  Term extraction performed by ExATOlp delivers only concept bearing terms, since it only considers terms that are Noun Phrases (NP) and free of determiners (articles, pronouns, etc.). In fact, the extraction procedure performed by ExATOlp considers a set of linguistic based heuristics that delivers the state of the art concept extraction for Portuguese language texts [Lopes and Vieira 2012].  Term frequency, disjoint corpora frequency (tf-dcf ) is also computed by ExATOlp. tf-dcf is an index that estimates the relevance of a term directly proportional to its frequency in the target corpus, and inversely proportional to its frequency in a set of contrasting corpora. Consequently, the computation of the relevance index requires not only the deﬁning corpora, but also a set of contrasting corpora [Lopes et al. 2012].  Once the terms of the deﬁning corpus for each entity are extracted and associated to their respective relevance indices, the proposed construction of each entity descriptor is composed by two lists of terms with their relevance indices: • top terms - The ﬁrst list is composed by the n top relevant terms1, i.e., the n terms with higher tf-dcf values; • drop terms - The second list is composed by the n more frequent, but common, terms, i.e., the terms with the higher frequency and lower tf-dcf values. To rank the terms for the top terms list it sufﬁces to rank the terms according to the tf-dcf index, which is numerically deﬁned for term t in the target corpus c considering a set of contrasting corpora G as:  tf-dcf(tc) =  tf(tc) 1 + log 1 + tf(tg)  (1)  ∀g∈G  where tf(tc) is the term frequency of term t in corpus c.  To rank terms for the drop terms lists, it is possible to consider a relevance drop  index numerically deﬁned as the difference between the term frequency and the tf-dcf  index, i.e.:  drop(tc) = tf(tc) − tf-dcf(tc)  (2)  1The number of terms in each list is an arbitrary choice that is not fully analyzed yet. However, preliminary experiments indicate that lists of n = 50 terms seem effective.  93  Building and Applying Proﬁles Through Term Extraction  An important point of the entity descriptors building process is to take into account the fact that sometimes distinct entities can have quite unbalanced corpora. This can be the result of entities with corpora with very different sizes, but it may also happen due to intrinsic characteristics of each deﬁning corpus. In fact, even corpora with similar sizes can have very distinct occurrence distributions. Therefore, in order to equalize the eventual differences between values of distinct corpora we decided to adopt as numerical values of tf-dcf and drop indices not their raw value expressed by Eqs. 1 and 2, but the logarithm of those values. Such decision follows the basic idea formulated by the Zipf Law [Zipf 1935] that states that the distribution of term occurrences follows and exponential distribution. Consequently, adopting the logarithm values of tf-dcf and drop is likely to brings those index to a linear distribution2. Formally, the descriptor of each entity e, with e ∈ {1, 2, . . . , E}, is denoted by the lists Te and De composed by the information: • term(tie) the i-th term of Te • idx(tie) the logarithmic value of the tf-dcf of the i-th term of Te • term(die) the i-th term of De • idx(die) the logarithmic value of the drop index of the i-th term of De Figure 1 describes this descriptor building process. In this ﬁgure, each entity is described by a deﬁning corpus and from such corpus a term extraction and relevance index computation is made in order to generate a pair of lists to describe each entity.  Deﬁning Corpus for Entity 1  Term Extraction tf-dcf Computation top and drop lists Computation  Contrasting Corpora 1  Descriptor 1  
Abstract. This article describes a corpus of news texts in Brazilian Portuguese. News were collected from four big newswire outlets, segmented in paragraphs, and marked up by a group of four annotators, who had to classify each paragraph according to two dimensions: target entity (that is the person which is the main subject of the news contained in the paragraph), and the paragraph’s polarity with respect to the target entity. The corpus comprises 131 news, segmented in 1,447 paragraphs, with 65,675 words in total. Along with the corpus, we have also built a gold standard, where paragraphs are classiﬁed according to the opinion of the majority of annotators. This gold standard and annotated corpus are available to the community under a Creative Commons licence. 1. Introduction In recent years, sentiment analysis has drawn researchers’ attention due to the vast amount of information available through the internet, along with the development of machine learning techniques applied to natural language processing [Pang and Lee 2008]. With this kind of analysis, it is possible to gather information of great commercial interest, such as what costumers are saying about some product, ﬁlm or person, for example. In this sense, one of the ﬁrst domains to serve as a testing ﬁeld for sentiment analysis was that of customer reviews (e.g. [Turney 2001, Pang et al. 2002]), where products are classiﬁed as recommended or not by customers (e.g. [Turney 2001]). Alternatively, a number of “stars” may be attributed to some product or information which, in turn, are used to classify the reviews according to their valence (i.e. positive, neutral or negative, e.g. [Pang et al. 2002]). Differently from customer reviews, however, the newswire domain usually comes with no such hint on customers’ (i.e. readers’) opinion about the product (i.e. the news itself), or even on the content of the news. As such, researchers have no inbuilt hint that can help them ﬁgure out the valence of the sentiment associated with that news, be it the sentiment expressed along with the news, or the sentiment it elicits in customers. In order to allow for sentiment analysis techniques to be used and evaluated, it is necessary then to manually annotate a set of news. As a matter of fact, such annotated corpora can already be found in some languages, such as Arabian [Abdul-Mageed and Diab 2012], Portuguese [Rocha and Santos 2000, Aleixo and Pardo 2008] and English [Curran and Koprinska 2013], for example. These, however, are designed for general use, not focusing on a speciﬁc subject, such as political 101  An Annotated Corpus for Sentiment Analysis in Political News news, for instance. On this account, only the German language seems to have a corpus dedicated to this kind of news (cf. [Li et al. 2008]). The focus on politics, in turn, is justiﬁable given its usually polarised nature, whereby one always have a situation and an opposition. Such a polarisation can be a fertile ground for research on bias (in its different forms), economical situation forecasting, or even political action prediction, which could be inferred from some tendency detected in this kind of news. To help reduce this lack of resources, speciﬁcally in Brazilian Portuguese, in this article we present a corpus of political news texts, annotated with sentiment information according to two dimensions: the entity referred to by the news, and the valence of that reference. From resulting annotations, we have also built a gold standard, which can be used both to evaluate different sentiment analysis techniques, thereby providing a common ground for future comparisons, and to allow for machine learning techniques to be applied. Both corpus and gold standard are publicly available under a Creative Commons licence at http://www.each.usp.br/norton/ viesnoticias/index_ing.html. The rest of this article is organised as follows. Section 2 provides an overview of current related research on news corpora annotation. Section 3, in turn, describes the process of data gathering, along with the methodology followed to annotate these data. Section 4 presents the annotation results, in terms of inter-annotator agreement, along with the steps taken to build our gold standard and label distribution within it. These results are then discussed further in this Section. Finally, Section 5 presents our conclusions and directions for future research. 2. Related Work When annotating newswire texts, it is usual to have a group of annotators classify the news according to some feature, such as polarity (e.g. [Li et al. 2008, Kaya et al. 2012]) for example. When adopting this approach, however, researchers need to deal with interannotator agreement issues, such as those faced by [Balahur et al. 2010], who report an interannotator agreement lower than 50%, for a binary classiﬁcation of citations by three annotators. After asking annotators to classify the citations according to their target (i.e. the cited entities), without accounting for their polarity, agreement raised to only 60%. Scores as high as 81% were obtained only when asking annotators not to use any previous knowledge they could have when assessing the citations. Apart from this problem, there is also the issue regarding the number of annotators necessary to carry out the task, since it has been noticed that with a high number of annotators comes a reduction in the agreement amongst them [Das and Bandyopadhyay 2010], even though the use of more than two annotators is advisable [Artstein and Poesio 2005]. Alternatively to the use of human annotators, another approach found is the use of external sources of information to classify the news. This is the approach taken by [Siering 2012], who used stock market ﬂuctuations to determine the polarity of news related to some speciﬁc stock. As such, if the stock price raised after the news, then that news is regarded as positive, otherwise, it is negative. However solving the problem of low interannotator agreement scores, this kind of approach raises issues of its own. In this speciﬁc case, one can never be too sure about the time that it takes for the news to produce 102  An Annotated Corpus for Sentiment Analysis in Political News any measurable impact on the stock market, there being a potential confounding between the news content and other external variables that might have inﬂuenced the sock prices, but which are unrelated to the news itself. Besides deﬁning the methodology underlying the classiﬁcation of news, another related question is at what level the news are to be annotated. Since news can refer to multiple facts and, consequently, have multiple polarities, splitting them in smaller units of annotation might help capture each of these individual facts. This, however, is still an unsettled issue, with current approaches ranging from segmenting news in sentences (e.g. [Balahur et al. 2010, Abdul-Mageed and Diab 2012]) to separating out text spans, such as third party citations (e.g. [Balahur et al. 2009, Drury and Almeida 2012, Curran and Koprinska 2013]), for example. In this research, we adopted the ﬁrst approach, and relied on a group of annotators to classify the polarity of news, along with the entity to which it refer. To do so, texts were segmented in paragraphs, instead of sentences, so as to offer annotators a wider context in which to work. Given that our intention was to cover political news in Brazilian Portuguese from a greater variety of news producers (so as to allow for a reasonable comparison amongst them), we had to collect a corpus of our own, since existing initiatives, such as CSTNews [Cardoso et al. 2011], CHAVE [Rocha and Santos 2000] and TeMa´rio [Pardo and Rino 2003], for example, however important, do not ﬁt perfectly our purposes, either because the amount of political news is still small, or because the corpus focus in just a couple of newspapers. 3. Materials and Methods From 06/09/2014 to 12/09/2014, news on politics were extracted from a set of public twitter proﬁles1. During this period, every day at 20:00, a crawler retrieved the last 20 tweets from each of the selected proﬁles2. After ﬁltering out retweets (i.e. the re-publishing of an already published tweet) and tweets without a link to the text of the news, the links in the remaining tweets were followed, so we could retrieve the original news texts as published at the producers’ website. Retrieved news were then classiﬁed by one of the authors according to their relevance to the corpus. News were considered relevant whenever they referred either to one of the three main candidates running for president of Brazil (i.e. Dilma Rousseff, Ae´cio Neves and Marina Silva), or to one of the three main candidates running for governor of the State of Sa˜o Paulo (i.e., Geraldo Alckmin, Paulo Skaf and Alexandre Padilha). At the end of this process, 131 news3 were selected to form the corpus, comprising 1,447 paragraphs with 65,675 words in total. Table 1 summarises the results for each analysed proﬁle, in terms of number of retrieved and selected tweets, along with the amount of retweets, while Algorithm 1 describes the data collection process. The choice for twitter proﬁles was mainly guided by the subjective importance of the newswire outlet, as perceived by its popularity. As such, we selected a set of ﬁve news producers: Folha de Sa˜o Paulo, Estado de Sa˜o Paulo, G1, Veja and Carta Capital. Folha 1http://twitter.com 2News from 09/09/2014 could not be extracted due to a technical problem in the extraction system that day. 3That is 131 texts as published at the producers’ websites. 103  An Annotated Corpus for Sentiment Analysis in Political News  Proﬁle @EstadaoPolitica @g1politica @folha poder @cartacapital @VEJA  Table 1. Selected Twitter Proﬁles  Name  Selected Tweets Retrieved Tweets  Pol´ıtica Estada˜o  7  17  G1 - Pol´ıtica  25  118  Folha Poder  64  120  Carta Capital  14  114  VEJA  21  118  Retweets 1 2 0 42 8  Algorithm 1 Data collection initialDate ← 06/09/2014 endDate ← 12/09/2014 for ref erenceDate ← initialDate to endDate do dailyN ews ← extractN ewsF romT witter(ref erenceDate) for all news in dailyN ews do if eligible(news) then addT oCorpus(news) end if end for end for  de Sa˜o Paulo and Estado de Sa˜o Paulo were chosen due to the fact that they are the biggest newspapers in the State of Sa˜o Paulo, also being amongst the biggest ones in Brazil. G1, in turn, was chosen because it is one of the biggest online news portal in Brazil. Finally, Veja and Carta Capital were chosen for being popular weekly new magazines, which are usually taken as presenting opposite editorial proﬁles. Selected news were then segmented in paragraphs and presented to a set of four annotators (see Table 2 for details on annotators’ age, sex, knowledge area and educational attainment). The annotation format corresponds to the inline addition of XML tags, along the lines presented in [Roman 2013] (even though the non-annotated plain corpus is also made available). We chose to use paragraphs as our basic unit of annotation in order to present annotators with a wider context, when compared to other units such as sentences, for example, while still trying to avoid topic changes.  Table 2. Annotators’ details  ID Age Sex Knowledge Area Educational Attainment  
Caixa Postal 676 – 13565-905 – São Carlos – SP – Brazil 2Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo Caixa Postal 668 – 13566-970 – São Carlos – SP – Brazil renatatironi@hotmail.com, arianidf@gmail.com, taspardo@icmc.usp.br Abstract. In this paper, using a corpus with manual alignments of humanwritten summaries and their source news, we show that such summaries consist of information that has specific linguistic features, revealing human content selection strategies, and that these strategies produce indicative results that are competitive with a state of the art system for Portuguese. Resumo. Neste artigo, a partir de um corpus com alinhamentos manuais entre sumários e suas respectivas notícias-fonte, evidencia-se que tais sumários são compostos por informações que possuem características linguísticas específicas, revelando estratégias humanas de sumarização, e que essas estratégias produzem resultados iniciais que são competitivos com um sistema do estado da arte para o português. 1. Introduction The increasing of new technologies has had an impact on the amount of available textual information on the web. Consequently, Multi-document Summarization (MDS) appears to be a useful Natural Language Processing (NLP) application to promote quick access to large quantities of information, since it produces a unique summary from a collection or cluster of texts on the same topic or related topics [Mani 2001]. Within a generic perspective, the multi-document summary should ideally contain the most relevant information of the topic that is being discussed in the source texts. Moreover, MDS should not only focus on the extraction of relevant information, but also deal with the multi-document challenges, such as redundant, complementary and contradictory information, different writing styles and varied referential expressions. There are two ways of approaching MDS [Mani 2001]. The superficial/shallow approach uses little linguistic information (or statistics) to build summaries. The deep approach is characterized by the usage of deep linguistic knowledge, i.e., syntactic, semantic or discourse information. The superficial approach usually requires low-cost processing, but produces summaries that tend to have lower linguistic quality. The deep approach is said to produce summaries of higher quality in terms of information, coherence and cohesion, but it demands various high-cost resources. The deep and superficial MDS applications commonly produce extracts (i.e., summaries generated by concatenating sentences taken exactly as they appear in the source texts), but deep approach can also generate abstracts (i.e., with rewriting operations). 141  On Strategies of Human Multi-Document Summarization To select the sentences to compose the summaries, MDS may take into account human strategies from single-document summarization, codified in features such as sentence position and word frequency [Kumar and Salim 2012]. Regarding human multi-document summarization (HMDS), only redundancy has been widely applied as criterion for content selection, which is based on the empirical observation that the most repeated information covers the main topic of the cluster [Mani 2001; Nenkova 2006]. In this context, this work is focused on the investigation of HMDS content selection strategies. Particularly, for a corpus of news texts, we study some superficial and deep sentence features that may be useful for summarization. Since the source sentences in this corpus are aligned to the sentences of the correspondent reference (human) summary, we show that a machine learning technique could identify that a few features characterize well the aligned sentences (i.e., the sentences whose content was selected to the summary), achieving 70.8% of accuracy. We also show that additional experiments with the best learned HMDS strategy indicated that it may produce competitive results with a state of the art system for Portuguese, outperforming it for a small test corpus. Consequently, this work contributes to the understanding of the HMDS task and to the improvement of the automatic process by providing linguistic insights. To describe this work, we organized the paper in 5 sections. In Section 2, we describe the main human content selection strategies and the correspondent features of the literature. In Section 3, the used methodology is reported. In Section 4, results are discussed, and, in Section 5, some final remarks are made. 2. Human Content Selection in Text Summarization In one of the most comprehensive study of human summarization, Endres-Niggemeyer (1998) established that humans perform single-document summarization in three stages: (i) document exploration, (ii) relevance assessment, and (iii) summary production. This means that humans first interpret the source-text, then select important information from it, and finally present a new text in the form of a summary. Regarding the relevance assessment stage, where, according to Hasler (2007), humans perform the core summarization task (i.e., the selection of the relevant information), Endres-Niggemeyer pointed out the use of some strategies. Some wellknown shallow features are [Kumar and Salim 2012]: (i) sentence length or size, according to which very short or long sentences may not be suitable to compose the summary; (ii) sentence position, according to which sentences in the initial positions of a text should compose a summary; (iii) word frequency, according to which the summary is produced by retrieving and putting together sentences with the highest frequent content words in the cluster; (iv) title/subtitle word, according to which the relevance of a sentence is the sum of all the content words appearing in the title and (sub-)headings of their text, and; (v) cue word/expression: according to which the relevance of a sentence is computed by the presence or absence of certain cue words or expressions. Although multi-document summarization can be conceived as an extension of the single one, humans seem to use specific strategies for relevance assessment in the scenario of multiple source texts, which have been empirically observed and reported in MDS 142  On Strategies of Human Multi-Document Summarization literature. The main one is the selection of the most redundant information in a collection to produce the corresponding summary, as we have already mentioned before [Mani 2001; Nenkova 2006]. The other is that humans choose one text of their preference as a basis to select the main information and then they seek the other texts of the cluster to complement the multi-document summary information [Mani 2001; Camargo 2013]. For the choice of the basis source text, many linguistic or extralinguistic factors may influence, such as: (i) date of publication (i.e., humans can first consider the latest or the oldest text, depending on the interest), (ii) prestige of the journalistic vehicle, etc. In feature-based methods of MDS, word frequency may indicate redundancy. In other shallow methods, such as those based on clustering, highly similar sentences of a collection are grouped into one cluster, which generates a number of clusters. A very populous cluster represents redundant information or topic. Hence, for each of the most populous clusters, the methods select only one sentence to compose the summary, which is based on the closeness of the sentence to the centroids (i.e., frequent occurring words) of the cluster. In graph-based methods, the source documents are represented in a graph where each sentence becomes a node and the weighted connections between nodes codify the similarity between the corresponding sentences. A redundant sentence is the one that is strongly connected to other sentences. In deep approaches, semantic-based MDS methods commonly map nouns of the input sentences onto concepts of a hierarchy or ontology, and then select the sentences with the most frequent concepts of the collection to produce the summary (e.g. Lin et al. (2010)). Discourse-based methods take into account discourse relations such as those of the Cross-document Structure Theory (CST) [Radev 2000]. These works represent the input texts in a graph, where each node codifies one sentence and the connections represent the CST relations established among those sentences. For content selection, one method consists in extracting sentences that have more CST connections with other sentences, assuming that they are redundant and, then, more relevant. In this paper, we test features from the above approaches to look for a good summarization strategy. We describe the method used in this work in the next section. 3. Corpus-based Investigation of HMDS strategies The experiments in this work were conducted over CSTNews corpus [Cardoso et al. 2011], a multi-document corpus that is composed of 50 clusters of news texts in Brazilian Portuguese. Each cluster contains 2 or 3 news texts on the same topic, automatic and human multi-document summaries (with a 70% compression rate1), and many annotation layers. In this corpus, each sentence of the input texts is aligned to one or more sentences of the correspondent human multi-document summary, which indicates the origin of the summary content. The manual alignment was performed in the summary-to-text direction according to content overlap rules [Camargo et al. 2013; Agostini et al. 2014]. To illustrate, the summary sentence (1), “17 people died after a plane crash in the Democratic Republic of Congo”, is aligned to the text sentence (2), “A crash in the town of Bukavu in the eastern Democratic Republic of Congo (DRC), killed 17 people on Thursday afternoon, said on Friday a spokesman of the United 
Av. Trabalhador São-carlense, 400 - Centro CEP: 13566-590 - São Carlos/SP, Brazil. {marciosd,taspardo}@icmc.usp.br Abstract. This paper describes how discursive knowledge, given by the discursive theories RST (Rhetorical Structure Theory) and CST (Crossdocument Structure Theory), may improve the automatic evaluation of local coherence in multi-document summaries. Two of the main coherence models from literature were incremented with discursive information and obtained 91.3% of accuracy, with a gain of 53% in relation to the original results. 1. Introduction Coherence is an important aspect that affects the quality of texts produced by textual generators such as summarizers, question/answering systems, etc. A coherent multidocument summary makes reading and understanding easier than one summary with contradictions and repetitive information. According to Koch and Travaglia (2002), coherence means the possibility of establishing a meaning for the text. Coherence supposes that there are relationships among the elements of the text for it to make sense. It also involves aspects that are out of the text, for example, the shared knowledge between the producer (writer) and the receiver (reader) of the text, inferences, intertextuality, intentionality and acceptability, among others [Kock and Travagila 2002]. Textual coherence occurs in local and global levels [Dijk and Kintsch 1983]. Local level coherence is presented by the local relationships among the parts of a text, for instance, adjacent sentences and shorter sequences. On the other hand, a text presents global coherence when this text links all its elements as a whole. Local coherence is essential in order to achieve global coherence [Mckoon and Ratcliff 1992]. Thus, many researches in computational linguistics have been developing models for dealing with local coherence ([Barzilay and Lapata 2005], [Barzilay and Lapata 2008], [Burstein et al. 2010], [Castro Jorge 2014], [Dias et al. 2014b], [Eisner and Charniak 2011], [Elsner et al. 2007], [Feng et al. 2014], [Filippova and Strube 2007], [Foltz et al. 1998], [Freitas 2013], [Guinaudeau and Strube 2013], and [Lin et al 2011]). To illustrate the problem we have in hands, Figure 1 shows two summaries, a coherent (Summary A) and a less coherent one (Summary B). Summary B presents redundant information among the sentences: S1 with S3, and S2 with S4. These redundancies damage the quality and the informativity of the text and, consequently, its coherence. 151  Enriching entity grids and graphs with discourse relations: the impact in local coherence evaluation  Summary A (coherent summary)  Summary B (incoherent summary)  (S1) In the last five years, astronomers have identified a few dozen objects that are even smaller than brown dwarfs that are not bound to any star system, nicknamed the planetary mass objects, or planemos located around star-forming regions. (S2) By using telescopes at the European Southern Observatory (ESO), astronomers have discovered a planet that is seven times the size of Jupiter, the heaviest that revolves around the sun, and the other that is twice its size. (S3) The mass of these two worlds is similar to other already cataloged exoplanets but they do not revolve around a star, they revolve around each other. (S4) Ray Jayawardhana, from the University of Toronto, and Valentin Ivanov, from the European Southern Observatory, have published the findings in "Science Express", the "Science" magazine website.  (S1) By using telescopes at the European Southern Observatory (ESO), astronomers have discovered a planet that is seven times the size of Jupiter, the heaviest that revolves around the sun, and the other that is twice its size. (S2) The mass of these two worlds is similar to other already cataloged exoplanets but they do not revolve around a star, they revolve around each other. (S3) The biggest celestial body, whose size is seven times greater than Jupiter, was detected about 400 light years from our solar system. (S4) The extraordinary fact is that it does not revolve around a star, but around another cold body that is twice its size.  Figure 1. Examples of coherent (A) and incoherent (B) summaries  The discursive information used in this work is related to intra or inter text organization, i.e., the Rhetorical Structure Theory (RST) [Mann and Thompson 1987] and the Crossdocument Structure Theory (CST) [Radev 2000], respectively. RST considers that each text presents an underlying rhetorical structure that allows the recovery of the writer’s communicative intention. RST relations are structured in the form of a tree, where Elementary Discourse Units (EDUs) are located in the leaves of this tree, whereas CST organizes multiple texts on the same topic and establishes relations among different textual segments, forming a graph. Considering that all well-formed and coherent texts have a well-defined discursive organization, this paper shows how discursive information (RST and CST) may improve the accuracy of local coherence models in order to automatically differentiate coherent from incoherent (less coherent) summaries. Thus, local coherence models from the literature have been enriched with discursive information. In addition, the original approaches have been re-implemented to have their performances analyzed with the corpus of multi-document summaries used in this work. In particular, this work is based on the following assumptions: (i) there are regularities on the distribution of discursive relations in coherent summaries; (ii) coherent summaries show distinct organization of intra- and inter-discursive relations. We show that such assumptions hold and that we improve the original results in the area. Section 2 presents an overview of the most relevant researches related to local coherence. In Section 3, the coherence models proposed in this work are described. Section 4 shows the experimental setup and the obtained results. Finally, Section 5 presents some final remarks.  2. Related Work One of the most used local coherence models is the one of Barzilay and Lapata (2008), which proposed an Entity Grid Model to evaluate local coherence, i.e., to classify coherent or incoherent texts. This model is based on Centering Theory [Grosz et al. 1995]; the authors’ hypothesis is that locally coherent texts present certain regularities concerning entity distribution. These regularities are calculated over a matrix (entity grid) in which the rows represent the sentences of the text, and the columns the text entities. Barzilay and Lapata's approach used (+) or not (-) syntactical, coreference and salience information. The syntactical information uses the grammatical function of the entities. For example, in the “Department” column in the entity grid in Figure 2b, it is  152  Enriching entity grids and graphs with discourse relations: the impact in local coherence evaluation  shown that the “Department” entity happens in the first sentence in the subject (S) position. The hyphen (‘-’) indicates that the entity did not happen in the corresponding sentence, (O) object position and (X) nor subject or object. Coreference occurs when words refer to the same entity and, therefore, these words may be represented by a single column in the grid. For example, when the text in Figure 2a mentions “Microsoft Corp.”, “Microsoft”, and “the company”, such references are mapped to a single column (“Microsoft”) in its entity grid in Figure 2b. Salience is related to the frequency of entities in texts, allowing to build grids with the least and/or the most frequent entities in the text.  
We describe a system for event extraction across documents and languages. We developed a framework for the interoperable semantic interpretation of mentions of events, participants, locations and time, as well as the relations between them. Furthermore, we use a common RDF model to represent instances of events and normalised entities and dates. We convert multiple mentions of the same event in English and Spanish to a single representation. We thus resolve cross-document event and entity coreference within a language but also across languages. We tested our system on a Wikinews corpus of 120 English articles that have been manually translated to Spanish. We report on the cross-lingual crossdocument event and entity extraction comparing the Spanish output with respect to English. 
The concept of Linked Data has attracted increased interest in recent times due to its free and open availability and the sheer of volume. We present a framework to generate patterns which can be used to lexicalize Linked Data. We use DBpedia as the Linked Data resource which is one of the most comprehensive and fastest growing Linked Data resource available for free. The framework incorporates a text preparation module which collects and prepares the text after which Open Information Extraction is employed to extract relations which are then aligned with triples to identify patterns. The framework also uses lexical semantic resources to mine patterns utilizing VerbNet and WordNet. The framework achieved 70.36% accuracy and a Mean reciprocal Rank value of 0.72 for ﬁve DBpedia ontology classes generating 101 lexicalizations. 
Linked open data (LOD) presents an ideal platform for connecting the multilingual lexical resources used in natural language processing (NLP) tasks, but the use of machine translation to ﬁll in gaps in lexical coverage for resource-poor languages means that large amounts of data are potentially unveriﬁed. For graph-based word sense disambiguation (WSD), one approach has been to ﬁrst translate terms into English in order to disambiguate using richer, fuller lexical knowledge bases (LKBs) such as WordNet. In this paper, we show that this approach actually creates more ambiguity and is far less accurate than using languagespeciﬁc resources, which, regardless of their smaller size, can provide results comparable in accuracy to the state-of-theart reported for graph-based WSD in English. For LOD, this demonstrates the importance of continuing to grow and extend language-speciﬁc resources in order to continually verify and reintegrate them as accurate resources. 
A pilot study is reported on developing the basic Linguistic Linked Open Data (LLOD) infrastructure for hashtags from social media posts. Our goal is the encoding of linguistically and semantically enriched hashtags in a formally compact way using the machinereadable OntoLex model. Initial hashtag processing consists of data-driven decomposition of multi-element hashtags, the linking of spelling variants, and part-of-speech analysis of the elements. Then we explain how the OntoLex model is used both to encode and to enrich the hashtags and their elements by linking them to existing semantic and lexical LOD resources: DBpedia and Wiktionary. 
In the LOD era, the conceptual interoperability of language resources is established by using modular architectures like the Ontologies of Linguistic Annotations (Chiarcos, 2008a, OLiA). Available as a part of the Linguistic Linked Open Data (LLOD) cloud,1 OLiA provides ontological representations of annotation schemes for over 70 languages, as well as their linking to a reference model. We successfully train an ontology-based POS tagger on corpora with different tag sets of divergent granularity and partially compatible annotations. Making use of OLiA, we achieve interoperability of annotation schemes, and, despite sparse training data, we do not only outperform state-of-the-art POS taggers in concept coverage, but also show how traing on heterogeneously annotated data produces richer morphosyntactic annotation with no or only marginal loss of precision. 
In the paper we present the construction of the FactForge service. FactForge represents a reason-able view over several Linked Open Data (LOD) datasets including DBPedia, Freebase and Geonames. It enables users to easily identify resources in the LOD cloud by providing a general unified method for querying a group of datasets. FactForge is designed also as a use case for large-scale reasoning and data integration. We describe the datasets, ontologies, inference rules, and manipulations done over the data. The datasets are unified via a common ontology – PROTON, whose concepts are mapped to the concepts of the involved LOD datasets. Each of the mapping rules relates a PROTON class or a PROTON property to the corresponding class or property of the other ontologies. This mechanism of constructing a reason-able view over selected LOD datasets ensures that the redundant instance representations are cleaned as much as possible. The instances are grouped in equivalent classes of instances. 
Linguistic resources are essential for Language Learning applications. However, available resources are usually created in isolation, thus, they are scattered and need to be linked before they can be used for a speciﬁc task such as learning of a foreign language. To address these problems we present a new resource that link linguistic resources of multiple languages using the framework of Linguistic Linked Open Data (LLOD).  tries at the level of sense. Corpus data is particularly important for language learning as it provides massive amounts of real language use. Section 2 describes related work on resources and technologies of sense linking. Section 3 presents the resources integrated in the GuanXi Network. Section 4 presents the different methods used to build the GuanXi network for each language pair, depending on available resources and section 5 presents the data model using the LLOD framework. Section 6 provides both automatic and manual evaluations of sense linking strategies and section 7 concludes on future work.  
In this paper a methodology for learning the complex agglutinative morphology of some Indian languages using Adaptor Grammars and morphology rules is presented. Adaptor grammars are a compositional Bayesian framework for grammatical inference, where we define a morphological grammar for agglutinative languages and morphological boundaries are inferred from a plain text corpus. Once morphological segmentations are produce, regular expressions for sandhi rules and orthography are applied to achieve the final segmentation. We test our algorithm in the case of two complex languages from the Dravidian family. The same morphological model and results are evaluated comparing to other state-of-the art unsupervised morphology learning systems. 
The accuracy of an annotated corpus can be increased through evaluation and revision of the annotation scheme, and through adjudication of the disagreements found. In this paper, we describe a novel process that has been applied to improve a part-of-speech (POS) tagged corpus for the African language Igbo. An inter-annotation agreement (IAA) exercise was undertaken to iteratively revise the tagset used in the creation of the initial tagged corpus, with the aim of reﬁning the tagset and maximizing annotator performance. The tagset revisions and other corrections were efﬁciently propagated to the overall corpus in a semi-automated manner using transformation-based learning (TBL) to identify candidates for correction and to propose possible tag corrections. The affected word-tag pairs in the corpus were inspected to ensure a high quality end-product with an accuracy that would not be achieved through a purely automated process. The results show that the tagging accuracy increases from 88% to 94%. The tagged corpus is potentially re-usable for other dialects of the language. 
For a single semantic meaning, various linguistic expressions exist the Mainland China, Hong Kong and Taiwan variety of Mandarin Chinese, a.k.a., the Greater China Region (GCR). Differing from the current bilingual word alignment corpus, in this paper, we have constructed two monolingual GCR corpora. One is a 11,623-triple GCR word dictionary corpora which is automatically extracted and manually annotated from 30 million sentence pairs from Wikipedia. The other one is a manually annotated 12,000 sentence pairs GCR word alignment corpus from Wikipedia and news website. In addition, we present a rulebased word alignment model which systematically explores the different word alignment case, e.g. 1-1, 1-n and m-n mapping, from Mainland China to Hong Kong or Taiwan. Evaluation results on our two different GCR word alignment corpora verify the effectiveness of our model, which significantly outperforms the current Hidden Markov Model (HMM) based method, GIZA++ and their enhanced versions. 
We introduce a new dependency treebank for Croatian within the Universal Dependencies framework. We construct it on top of the SETIMES.HR corpus, augmenting the resource by additional part-of-speech and dependency-syntactic annotation layers adherent to the framework guidelines. In this contribution, we outline the treebank design choices, and we use the resource to benchmark dependency parsing of Croatian and Serbian. We also experiment with cross-lingual transfer parsing into the two languages, and we make all resources freely available. 
We present an account of analytic verb forms in a treebank of Czech texts. According to the Czech linguistic tradition, description of periphrastic constructions is a task for morphology. On the other hand, their components cannot be analyzed separately from syntax. We show how the paradigmatic and syntagmatic views can be represented within a single framework. 
Being able to identify that different mentions refer to the same entity is beneﬁcial for applications such as question answering and text summarization. In this paper, we propose the ﬁrst model for entity coreference resolution for Croatian. We enforce transitivity constraints with integer linear programming on top of pairwise decisions produced by the supervised mention-pair model. Experimental results show that the proposed model signiﬁcantly outperforms two different rule-based baselines, reaching performance of 74.4% MUC score and 77.6% B3 score. 
In this paper we discuss the performance of existing tools for coreference resolution for Polish from the perspective of information extraction tasks. We take into consideration the source of mentions, i.e., gold standard vs mentions recognized automatically. We evaluate three existing tools, i.e., IKAR, Ruler and Bartek on the KPWr corpus. We show that the widely used metrics for coreference evaluation (B3, MUC, CEAF, BLANC) do not reﬂect the real performance when dealing with the task of semantic relations recognition between named entities. Thus, we propose a supplementary metric called PARENT, which measures the correctness of linking between referential mentions and named entities. 
This paper presents preliminary experiments on Open Relation Extraction for Polish. In particular, a variant of a priorart algorithm for open relation extraction for English has been adapted and tested on a set of articles from Polish on-line news. The paper provides initial evaluation results, which constitute the point of departure for in-depth research in this area. 
Nikola Ljubesˇic´  Maja Milicˇevic´  Corpus Lab  Dept. of Information  Dept. of General Linguistics  URPP Language and Space and Communications Sciences Faculty of Philology  University of Zurich  Faculty of Humanities  University of Belgrade  tanja.samardzic@uzh.ch and Social Sciences m.milicevic@fil.bg.ac.rs  University of Zagreb  nljubesi@ffzg.hr  
 In the CoCoCo project we develop methods to extract multi-word expressions of various kinds—idioms, multi-word lexemes, collocations, and colligations—and to evaluate their linguistic stability in a common, uniform fashion. In this paper we introduce a Web interface, which provides the user with access to these measures, to query Russian-language corpora. Potential users of these tools include language learners, teachers, and linguists. 
E-law module is the web application which works mainly as the set of information retrieval and extraction tools dedicated for the lawyers. E-law module consists of following tools: (1) document search engine; (2) context oriented search engine plugin; (3) legal phrase oriented machine translation; (4) document metatagger; (5) verdict ﬁnder. Machine translation, document meta-tagger and verdict ﬁnder tools are available for the general public. Other tools are restricted and are accessible after logging into the module. 
Supervised word sense disambiguation (WSD) has been shown to achieve state-ofthe-art results but at high annotation costs. Active learning can ameliorate that problem by allowing the model to dynamically choose the most informative word contexts for manual labeling. In this paper we investigate the use of active learning for Croatian WSD. We adopt a lexical sample approach and compile a corresponding senseannotated dataset on which we evaluate our models. We carry out a detailed investigation of the different active learning setups, and show that labeling as few as 100 instances sufﬁces to reach near-optimal performance. 
This paper presents work in progress on a machine learning method for classiﬁcation of morphosemantic relations between verb and noun synsets. The training data comprises 5,584 verb–noun synset pairs from the Bulgarian WordNet, where the morphosemantic relations were automatically transferred from the Princeton WordNet morphosemantic database. The machine learning is based on 4 features (verb and noun endings and their respective semantic primes). We apply a supervised machine learning method based on a decision tree algorithm implemented in Python and NLTK. The overall performance of the method reached F1-score of 0.936. Our future work focuses on automatic identiﬁcation of morphosemantically related synsets and on improving the classiﬁcation. 
The paper presents an application of Multidimensional (MD) analysis initially developed for the analysis of register variation in English (Biber, 1988) to the investigation of a genre diverse corpus, which was built from modern texts of the Russian Web. The analysis is based on the idea that each linguistic feature has different frequencies in different registers, and statistically stable co-occurrence of linguistic features across texts can be used for automatic identification of texts with similar communicative functions. By using a software tool which counts a set of linguistic features in texts in Russian and by performing factor analysis in R, we identified six dimensions of variation. These dimensions show significant similarities with Biber's original dimensions of variation. We studied the distribution of texts in the space of the dimensions of our factors and investigated their link to 17 externally defined Functional Text Dimensions (Forsyth and Sharoff, 2014), which were assigned to each text of the corpus by a group of annotators. The results show that dimensions of linguistic feature variation can be used for better understanding of the genre structure of the Russian Web. 
Elliptical constructions can help to avoid repetition of identical constituents during natural-language generation. From grammar books, it is not easy to extract executable rules for ellipsis—in our case in Russian. Therefore we follow a different strategy. We test the accuracy of a rule set that has been evaluated for the two Germanic languages, Dutch and German, and the two Finno-Ugric languages, Estonian and Hungarian. For a Russian test corpus of about 100 syntactically annotated coordinated sentences that systematically vary the conditions of rule application, our Java program can automatically produce all elliptical variants. Over- and undergeneration in the resulting lists have been tested in two experiments with native speakers. Basically, the rules work very well for Russian. Within the four target languages, Russian works best with the Estonian amendments. Here we report two slight deviations partially known from the linguistic literature. 
The paper presents the strategies and conversion principles of BulTreeBank into Universal Dependencies annotation scheme. The mappings are discussed from linguistic and technical point of view. The mapping from the original resource to the new one has been done on morphological and syntactic level. The ﬁrst release of the treebank was issued in May 2015. It contains 125 000 tokens, which cover roughly half of the corpus data. 
The paper studies the diversity of ways to express entity aspects in users’ reviews. Besides explicit aspect terms, it is possible to distinguish implicit aspect terms and sentiment facts. These subtypes of aspect terms were annotated during SentiRuEval evaluation of Russian sentiment analysis systems organized in 2014–2015. The created annotation gives the possibility to analyze the contribution of non-explicit aspects to the overall sentiment of a review, their main patterns, and possible use. 
In this work we are solving authorship attribution and author proﬁling tasks (by focusing on the age and gender dimensions) for the Lithuanian language. This paper reports the ﬁrst results on literary texts, which we compared to the results, previously obtained with different functional styles and language types (i.e., parliamentary transcripts and forum posts). Using the Naïve Bayes Multinomial and Support Vector Machine methods we investigated an impact of various stylistic, character, lexical, morpho-syntactic features, and their combinations; the different author set sizes of 3, 5, 10, 20, 50, and 100 candidate authors; and the dataset sizes of 100, 300, 500, 1,000, 2,000, and 5,000 instances in each class. The highest 89.2% accuracy in the authorship attribution task using a maximum number of candidate authors was achieved with the Naïve Bayes Multinomial method and document-level character tri-grams. The highest 78.3% accuracy in the author proﬁling task focusing on the age dimension was achieved with the Support Vector Machine method and token lemmas. An accuracy reached 100% in the author proﬁling task focusing on the gender dimension with the Naïve Bayes Multinomial method and rather small datasets, where various lexical, morpho-syntactic, and character feature types demonstrated a very similar performance. 
Statistical analysis of parliamentary roll call votes is an important topic in political science because it reveals ideological positions of members of parliament (MP) and factions. However, it depends on the issues debated and voted upon. Therefore, analysis of carefully selected sets of roll call votes provides a deeper knowledge about MPs. However, in order to classify roll call votes according to their topic automatic text classiﬁers have to be employed, as these votes are counted in thousands. It can be formulated as a problem of classiﬁcation of short legal texts in Lithuanian (classiﬁcation is performed using only headings of roll call vote). We present results of an ongoing research on thematic classiﬁcation of roll call votes of the Lithuanian Parliament. The problem differs signiﬁcantly from the classiﬁcation of long texts, because feature spaces are small and sparse, due to the short and formulaic texts. In this paper we investigate performance of 3 feature representation techniques (bag-of-words, n-gram and tf-idf ) in combination with Support Vector Machines (with different kernels) and Multinomial Logistic Regression. The best results were achieved using tf-idf with SVM with linear and polynomial kernels. 
This paper aims at bridging the gap between academic research on Translation Memories (TMs) and the actual needs and wishes of translators. Starting from an internal survey in a translation company, we analyse what translators wished translation memories offered them. We are currently implementing one of their suggestions to assess its feasibility and whether or not it retrieves more TM matches as originally expected. We report on how the suggestion is being implemented, and the results obtained in a pilot study. 
The problem of spotting false translations in the bi-segments of translation memories can be thought of as a classiﬁcation task. We test the accuracy of various machine learning algorithms to ﬁnd segments that are not true translations. We show that the Church-Gale scores in two large bisegment sets extracted from MyMemory can be used for ﬁnding positive and negative training examples for the machine learning algorithms. The performance of the winning classiﬁcation algorithms, though high, is not yet sufﬁcient for automatic cleaning of translations memories. 
We propose the integration of clause splitting as a pre-processing step for match retrieval in Translation Memory (TM) systems to increase the number of relevant sub-segment matches. Through a series of experiments, we investigate the impact of clause splitting in instances where the input does not match an entire segment in the TM, but only a clause from a segment. Our results show that there is a statistically significant increase in the number of retrieved matches when both the input segments and the segments in the TM are first processed with a clause splitter. 
Computer-assisted translation (CAT) tools have become the major language technology to support and facilitate the translation process. Those kind of programs store previously translated source texts and their equivalent target texts in a database and retrieve related segments during the translation of new texts. However, most of them are based on string or word edit distance, not allowing retrieving of matches that are similar. In this paper we present an innovative approach to match sentences having different words but the same meaning. We use NooJ to create paraphrases of Support Verb Constructions (SVC) of all source translation units to expand the fuzzy matching capabilities when searching in the translation memory (TM). Our first results for the EN-IT language pair show consistent and significant improvements in matching over state-of-the-art CAT systems, across different text domains. 
Translation memories (TMs) used in computer-aided translation (CAT) systems are the highest-quality source of parallel texts since they consist of segment translation pairs approved by professional human translators. The obvious problem is their size and coverage of new document segments when compared with other parallel data. In this paper, we describe several methods for expanding translation memories using linguistically motivated segment combining approaches concentrated on preserving the high translational quality. The evaluation of the methods was done on a medium-size real-world translation memory and documents provided by a Czech translation company as well as on a large publicly available DGT translation memory published by European Commission. The asset of the TM expansion methods were evaluated by the pre-translation analysis of widely used MemoQ CAT system and the METEOR metric was used for measuring the quality of fully expanded new translation segments. 
This paper explores a new TM-based CAT tool entitled CATaLog. New features have been integrated into the tool which aim to improve post-editing both in terms of performance and productivity. One of the new features of CATaLog is a color coding scheme that is based on the similarity between a particular input sentence and the segments retrieved from the TM. This color coding scheme will help translators to identify which part of the sentence is most likely to require post-editing thus demanding minimal eﬀort and increasing productivity. We demonstrate the tool’s functionalities using an English Bengali dataset. 
We are developing a real-time Japanese sign language recognition system that employs abstract hand motions based on three elements familiar to sign language: hand motion, position, and pose. This study considers the method of hand pose recognition using depth images obtained from the Kinect v2 sensor. We apply the contour-based method proposed by Keogh to hand pose recognition. This method recognizes a contour by means of discriminators generated from contours. We conducted experiments on recognizing 23 hand poses from 400 Japanese sign language words. Index Terms: hand pose, contour, sign language recognition, real-time, Kinect 1. Introduction In Japan, Japanese sign language is usually used among hearing impaired people to communicate. In addition, these people often communicate with others through a third person who understands both oral and sign language. The alternative is to use a computer that acts as an interpreter. However, no practical sign language recognition system exists, even one that recognizes isolated words. The difﬁculties lie in the nature of visual language and its complex structure. Compared with speech recognition, sign language recognition incorporates various visual components, such as hand motions, hand poses and facial expressions. In addition, no established study exists on representing the structure of Japanese sign language in a similar manner to that of spoken language. Therefore, few attempts recognize sign language by units such as hand motions and hand poses [1, 2]. Our study develops with real-time recognition of sign language words. In Japanese sign language, a sentence consists of several words and non-manual signals such as facial expressions. To recognize words is a ﬁrst step and essential to recognize sentences. The number of Japanese sign language words is said to be 3,000 or more. Recognition by discriminators that are independent of every word has proven ineffective. To produce a practical system, analysis and reconstruction of sign language words are critical. We want to emphasize that database of sign language words is required when we analyze such words. However, no established database currently exists for sign language recognition. Therefore, we employ a database from a computerized sign language word dictionary instead. Our system is based on three elements of sign language: hand motion, position, and pose. This study considers the method of hand pose recognition for our system. Speeding up hand pose recognition is difﬁcult, because of the number and variety of hand poses caused by rotations, altering the angle from the sensor, and diversities in bone structures. This study considers a hand pose recognition using depth images obtained from a single depth sensor. We apply the contour-based method pro-  posed by Keogh [3] to hand pose recognition. This method recognizes a contour by means of discriminators learned from contours. We conducted experiments to recognize 23 hand poses from 400 Japanese sign language words. 2. System overview Figure 1 shows the ﬂowchart of the entire system. We use Kinect v2 sensor [4] to obtain data from sign motions produced by an actual person. First, data obtained from the sensor is segmented into sign language words. Second, the three aforementioned elements are recognized individually. Finally, the recognition result is determined by the weighted sum of each score. The recognition process of the hand pose and other two components employs depth data of the hand region and coordinates of joints, respectively. This study partially considers the method of hand pose recognition and does not discuss other processes on the ﬂowchart. To utilize the structure in sign language recognition requires an expert knowledge of sign language. We apply a database from the computerized sign language word dictionary produced by Kimura [5] to sign language recognition. Our hand pose recognition is based on the classiﬁcation of hand types employed in this dictionary. Table 1 shows a portion of the database in the dictionary. This database includes approximately 2,600 Japanese sign language words. Each word is represented by speciﬁc sign language types in Table 2 and other elements are indicated in Figure 2. For example, the word “red” which belongs to the type 1 in Table 2 is expressed by the dominant hand and the other hand is not used. 3. Method of hand pose recognition Some methods of hand pose estimation classify depth pixels into parts to obtain joint coordinates [6, 7]. However, these methods present difﬁculties when the palm does not face the  Kinect v2 sensor  process by coordinates of joints  segment sign motion  cut out hand region  recognize motion  recognize position unify 3 results  recognize hand shape process by depth image  Figure 1: Flowchart of the entire system.  17 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 17–21, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  Table 1: Portion of the database in the dictionary.  Word  SL Hand Palm Type type direction Position  Motion  love  3  B  down  NS  circle  between  4  B  side  NS  down  blue  
Speech production assessment in disordered speech relies on tests such as intelligibility and/or comprehensibility tests. These tests are subjective and time-consuming for both the patients and the practitioners. In this paper, we report on the use of automatically-derived pronunciation scores to predict comprehensibility ratings, on a pilot development corpus comprised of 120 utterances recorded by 12 speakers with distinct pathologies. We found high correlation values (0.81) between Goodness Of Pronunciation (GOP) scores and comprehensibility ratings. We compare the use of a baseline implementation of the GOP algorithm with a variant called forced-GOP, which showed better results. A linear regression model allowed to predict comprehensibility scores with a 20.9% relative error, compared to the reference scores given by two expert judges. A correlation value of 0.74 was obtained between both the manual and the predicted scores. Most of the prediction errors concern the speakers who have the most extreme ratings (the lowest or the largest values), showing that the predicted score range was globally more limited than the one of the manual scores due to the simplicity of the model. Index Terms: pronunciation assessment, Goodness of Pronunciation, disordered speech, comprehensibility 1. Introduction The assessment of speech production abilities in motor speech disorders relies almost exclusively on subjective tests such as intelligibility tests. These tests have two main disadvantages. They are very time-consuming and often imply subjective judgments: speakers read lists of words or sentences while one or several judge(s) evaluate their production. Within this framework automatic methods for speakers evaluation appear as practical alternatives. Recent advances in Automatic Speech Recognition (ASR) – especially in the ﬁeld of Computer-Assisted Language Learning (CALL) – have contributed to develop techniques that may be of great interest for this purpose. ASR techniques developed for the assessment of foreign language learners’ pronunciation skills focused both on the segmental and the suprasegmental levels, giving birth to two research ﬁelds respectively called individual error detection and overall pronunciation assessment [1]. For individual error detection (i.e., automatic detection of mispronounced phones), two kinds of methods are used: • methods based on the comparison of target phone models and learners’ phone models (e.g. nonnativeness [2] or  scores derived from classiﬁcation methods such as linear discriminant analysis and alike [3]); • methods independent of the learner’s native language, such as raw recognition scores [4], or Goodness of Pronunciation scores (GOP [5, 6]). Since the latter methods do not rely on any assumption concerning the errors possibly made by the speakers, their relevance may not be limited to the ﬁeld of CALL. For example, GOP scores can be calculated to get an idea on how conﬁdent the ASR system is about each phone identity. In a previous research work [7], GOP scores were compared to perceptual analysis results in order to detect mispronounced phonemes in individuals with unilateral facial palsy (UFP). The algorithm was found to be effective: it detected 49.6% of mispronunciations (CR rate) and 84.6% of correct pronunciations. In [8] a preliminary test was conducted in order to study the relationship between mean GOP scores at sentence-level and subjective comprehensibility. Results were encouraging as highly signiﬁcant correlations were observed, with absolute Pearson’s coefﬁcients ranging from .68 to .79. However, several questions remain concerning this last study. First, only the baseline implementation of the GOP algorithm was used. Recent algorithm reﬁnements for CALL applications suggest that the accuracy of GOP results can be greatly improved, as in Forced-aligned GOP measurements (FGOP [9]). Moreover, the ability of GOP scores to predict comprehensibility judgments or measures was not assessed since the number of speakers was too limited. As a consequence the aim of the present work is twofold: 1) comparing the efﬁciency of GOP vs. F-GOP scores when dealing with disordered speech and 2) extending the number of speakers so as to test the ability of GOP measures to actually predict comprehensibility. 2. GOP algorithms The purpose of the GOP algorithm is to automatically provide pronunciation scores at segmental level, that is one score per phone realization. The larger the score, the larger the difference between a phone realization and the corresponding phone model. In other words, large scores indicate potential mispronunciations. In this work, we used two different implementations: the original “baseline” one [5, 6], and a variant called Forced-aligned GOP (F-GOP) [9]. The baseline algorithm can be decomposed into three steps: 1) forced phone alignment phase, 2) free phone recognition phase and 3) score computation as the difference between the  42 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 42–46, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  Table 1: Mean GOP values, reaction time and comprehensibility scores for 6 speakers. AP: Patients suffering from structural (anatomic) disorders, NP: Patients suffering from neurological disorders  Speaker Mean GOP value Mean F-GOP value Mean Reaction Time Mean comprehensibility  to oral commands (s)  score  AP1  1.60 (0.56)  NP1  2.32 (0.66)  NP2  2.54 (0.48)  AP2  2.86 (0.71)  AP3  3.67 (0.46)  AP4  4.15 (0.67)  0.81 (0.36) 1.11 (0.38) 1.42 (0.77) 1.99 (0.58) 2.50 (0.68) 4.01 (1.18)  4.11 (0.77) 4.63 (1.08) 5.54 (1.17) 5.50 (1.20) 7.51 (1.15) 9.64 (2.56)  5.65 (0.45) 5.30 (0.40) 4.70 (0.40) 4.05 (0.45) 4.25 (0.35) 1.65 (0.25)  log-likelihoods of the two preceding phases for each forcedaligned phone. The forced alignment phase is intended to provide the ASR system with the orthographic transcription of the input sentence along with a pronunciation lexicon. It consists of forcing the system to align the speech signal with an expected phone sequence. On the contrary, free phone recognition determines the most likely phone sequence matching the audio input without constraint (free phone loop recognition). GOP scores typically range from zero (perfect match) to values up to 10. Higher values often indicate that the aligning step failed for some reason and scores are meaningless in this case. In order to decide whether a phone was mispronounced (“rejected”) or not (“accepted”), phone-dependent thresholds can be determined on a development set. In this work, our goal was not to detect individual mispronunciations but rather to compute average GOP scores per utterance in order to correlate them with comprehensibility scores given by human judges at utterancelevel. The forced-aligned GOP version is exactly the same as the baseline one with the only difference that the phone boundaries found during forced alignment constrain the free phone recognition phase. For each aligned phone, a single phone is recognized. In [9], better correlations between GOP and manual scores were found with F-GOP than with baseline GOP in the context of a CALL experiment. Indeed, F-GOP removes the issues of comparing a single aligned phone with potentially several phones recognized within the same time interval. 3. Main objective and methodology This study aims at verifying the ability of GOP measures to predict disordered speech comprehensibility. To this end, 12 pathological speakers were recorded. In a ﬁrst experiment, these recordings were split in two subsets, each consisting of the sentences (imperative commands) recorded by 6 speakers: a development corpus and a test corpus (section 4). Reference comprehensibility scores, presented in section 5, were obtained a) by asking 24 listeners to react to the sentences using software created for this purpose and b) by asking two trained speech pathologists to evaluate each sentence comprehensibility on a 7-points rating scale. Automatic measures found in GOP experiments (section 6) are compared so as to establish a predictive model of speakers’ comprehensibility. This model is ﬁnally used to predict speech pathologists’ comprehensibility judgments in 6 other patients (section 7). Since data from 6 speakers constitute a very small dataset with 60 utterances only, we also report prediction results in a cross-validation setup.  Comprehensibility / F−GOP  7 Judge 1 Judge 2 7 − F−GOP measure 6  5  4  3  2  
Recent dysarthric speech recognition studies using mixed data from a collection of neurological diseases suggested articulatory data can help to improve the speech recognition performance. This project was speciﬁcally designed for the speakerindependent recognition of dysarthric speech due to amyotrophic lateral sclerosis (ALS) using articulatory data. In this paper, we investigated three across-speaker normalization approaches in acoustic, articulatory, and both spaces: Procrustes matching (a physiological approach in articulatory space), vocal tract length normalization (a data-driven approach in acoustic space), and feature space maximum likelihood linear regression (a model-based approach for both spaces), to address the issue of high degree of variation of articulation across different speakers. A preliminary ALS data set was collected and used to evaluate the approaches. Two recognizers, Gaussian mixture model (GMM) - hidden Markov model (HMM) and deep neural network (DNN) - HMM, were used. Experimental results showed adding articulatory data signiﬁcantly reduced the phoneme error rates (PERs) using any or combined normalization approaches. DNN-HMM outperformed GMM-HMM in all conﬁgurations. The best performance (30.7% PER) was obtained by triphone DNN-HMM + acoustic and articulatory data + all three normalization approaches, a 15.3% absolute PER reduction from the baseline using triphone GMM-HMM + acoustic data. Index Terms: Dysarthric speech recognition, Procrustes matching, vocal track length normalization, fMLLR, hidden Markov models, deep neural network 1. Introduction Although automatic speech recognition (ASR) technologies have been commercially available for healthy talkers, these technologies did not perform satisfactorily well when directly used for talkers with dysarthria, a motor speech disorder due to neurological or other injury [1]. Dysarthric speech is always with degraded speech intelligibility due to impaired voice and articulation functions [1–3]. For example, Parkinson’s disease and amyotrophic lateral sclerosis (ALS) impact the patient’s motor functions and therefore impair their speech. Only a few studies have been focused on dysarthric speech recognition [4–6]. Recent studies using mixed data from a variety of neurological diseases indicated articulatory data can improve the speech recognition performance [7, 8]. However, dysarthric speech recognition particularly for ALS has rarely been studied. ALS, also known as Lou Gehrig’s disease, is the most common motor neuron disease that causes the death of both up-  per and lower motor neurons [9]. The cause of the disease is unknown for most of the patients and only a small portion (5-10%) of patients is inherited [10]. As the disease progresses, the patient’s speech intelligibility declines [11, 12]. Eventually all patients have degraded speech and need an assistive device for communication [13]. Normal speech recognition technology (typically trained on healthy talkers’ data) does not work satisfactorily well for the patients. Therefore, ALS patients’ ability to use modern speech technology (e.g., smart home environment control driven by speech recognition) is limited. This project, to our best knowledge, is the ﬁrst one speciﬁcally designed to improve speech recognition performance for ALS using articulatory data. Based on the recent literature on speech recognition with articulatory data (e.g., [7, 14–20]), we hypothesized the followings for dysarthric speech recognition for ALS: 1) adding articulatory data (collected from ALS patients) would improve the speech recognition performance, 2) feature normalization in articulatory, acoustic, and both spaces is critical and necessary for speaker-independent dysarthric speech recognition with articulatory data, and 3) recent state-of-the-art approach, deep neural network (DNN)-hidden Markov model (HMM) would outperform the long-standing approach, Gaussian mixture model (GMM)-HMM. The high degree of variation in articulatory patterns across speakers has been a barrier for speaker-independent speech recognition with articulatory data. Multiple sources contributed to the inter-talker variation including gender, dialect, individual vocal tract anatomy, and different co-articulation patterns [21]. However, speaker-independent approaches are important for reducing the amount of training data required from each user. Only limited articulatory data samples are often available from individuals with ALS (even with healthy talkers) due to the logistic difﬁculty of articulatory data collection [22]. For example, in data collection using electromagnetic articulograph (EMA), small sensors have to be attached on the tongue using dental glue [23]. The procedure requires the patient to hold his/her tongue to a position for a while so that the glue can take effect. To reduce speaker-speciﬁc difference, researchers have tried different approaches to normalize the articulatory movements including data-driven approaches (e.g., principal component analysis [7]) or physiological approaches including aligning the tongue position when producing vowels [24–26], consonants [27, 28], and pseudo-words [29] to a reference (e.g., palate [24, 25], or a general tongue shape [27]).  47 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 47–54, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  (a) Wave System  (b) Sensor Locations. Labels are described in text. Figure 1: Data collection setup.  Procrustes matching, a bidimensional shape analysis technique [30], has been used to minimize the translational, scaling, and rotational effects of articulatory data across speakers [28, 29, 31]. Recent studies indicated Procrustes matching was effective for speaker-independent silent speech recognition (i.e., recognizing speech from articulatory data only) [18, 19]. Procrustes matching, however, has rarely been used in dysarthric speech recognition with articulatory data. In addition, we adopted two other representative approaches for across-speaker data normalization. Vocal tract length normalization (VTLN) which has been widely used in acoustic speech recognition [32–36], a data-driven approach in acoustic space, was used to extract normalized acoustic features. The third approach, feature space maximum likelihood linear regression (fMLLR), a model-based adaptation, was used for both acoustic and articulatory data. In this paper, we investigated the use of 1) articulatory data as additional information source for speech, 2) Procrustes matching, VTLN, and fMLLR as feature normalization approaches individually or combined, 3) two machine learning classiﬁers, GMM-HMM and DNN-HMM. The effectiveness of these speaker-independent dysarthric speech recognition approaches were evaluated with a preliminary data collected from multiple early diagnosed ALS patients. 2. Data Collection The dysarthric speech and articulatory data used in this experiment were part of an ongoing project that targets to assess the motor speech decline due to ALS [12, 37]. 2.1. Participants and stimuli Five patients with ALS (3 females and 2 males), American English talkers, participated in the data collection (Table 1). They are all early diagnosed (within half to one year). Severity of these participants with ALS was mild with average speech intelligibility of 94.54% (SD=3.40), with SPK2 not measured. The average age of the patients was 59.80 (SD=7.73). During each session, each subject produced up to 2 or 4 repetitions of 20 unique sentences at their normal speaking rate and loudness. These sentences are used in daily conversations (e.g., How are you?) or related to patients (e.g., This is an emergency, I need to see a doctor.). Some of the sentences were selected from [18, 38].  2.2. Tongue motion tracking device - Wave The Wave system (NDI Inc., Waterloo, Canada) was used to register the 3-dimensional (x, y, and z; lateral, vertical, and anterior-posterior axes) movements of the tongue and lips during speech production (Figure 1a). Our previous studies [39–41] found four articulators, tongue tip, tongue body back, upper lip, and lower lip, are optimal for this application. Therefore, we used the optimal four sensors for data collection. One sensor was attached on the subject’s head and the data were used to calculate the movements of other articulators independent of the head [42]. Wave records tongue movements by establishing a calibrated electromagnetic ﬁeld that induces electric current into tiny sensor coils that are attached to the surface of the articulators. A similar data collection procedure has been used in [22, 23, 38]. The spatial precision of motion tracking using Wave is approximately 0.5 mm [43]. The sampling rate for recording was 100 Hz. 2.3. Procedure Participants were seated with their head within a calibrated magnetic ﬁeld (right next to the textbook-sized magnetic ﬁeld generator). Five sensors were attached to the surface of each articulator using dental glue (PeriAcryl 90, GluStitch) or tape, including one on the head, two on the tongue and two on the lips. A three-minute training session helped the participants to adapt to the wired sensors before the formal data collection. Figure 1b shows the positions of the ﬁve sensors attached to a participant’s head, tongue, and lips. HC (Head Center) was on the bridge of the glasses. The movements of HC were used to calculate the head-independent movements of other articulators. TT (Tongue Tip) and TB (Tongue Body Back) were attached at the mid-line of the tongue [22]. TT was about approximately 10 mm from the tongue apex. TB was as far back as possible and about 30 to 40 mm from TT [22]. Lip sensors were attached to the vermilion borders of the upper (UL) and lower (LL) lips at mid-line. Data collected from TT, TB, UL, and LL were used  Table 1: ALS participants and data size information.  Gender Age # Phrases # Frames  SPK1 Female 53 SPK2 Female 71 SPK3 Male 61 SPK4 Female 52 SPK5 Male 62  39  5776  39  5219  79  9463  80 13625  79  9520  Total  316 43603  48  y(mm) y(mm)  -55 UL -60  -65  -70  -75  LL  -80  -85  -90  TT TB  15  10  TB  5  UL  0  TT  -5  -10 LL -15  -95 -50 -40 -30 -20 -10 0 z(mm) (a) Original Data  10 20  -20  -40  -20  0  20  40  60  z(mm)  (b) After Procrustes Matching  Figure 2: Example of a shape (motion path of four articulators; TT, TB, UL, and LL of SPK5) for producing “Call me back when you can”. In this coordinate system, y is vertical and z is anterior-posterior.  for analysis.  2.4. Data processing Data processing was applied on the raw sensor position data prior to analysis. First, the head translations and rotations were subtracted from the tongue and lip data to obtain headindependent tongue and lip movement data. The orientation of the derived 3D Cartesian coordinates system is displayed in Figure 1b, in which x is left-right, y is vertical, and z is front-back. Second, a low pass ﬁlter (i.e., 20 Hz) was applied for removing noise [22, 23]. In total, 316 sentence samples (for unique twenty phrases) were obtained from the ﬁve participants and were used for analysis. It could be expected ALS patients have different lateral movement patterns with healthy subjects (x in Figure 1b) [22], however for this study only y and z coordinates of the tongue and lip sensors were used for analysis.  3. Method 3.1. Procrustes matching: A physiological approach for articulatory data Procrustes matching (or Procrustes analysis [30]) is a robust statistical bidimensional shape analysis technique, where a shape is represented by a set of ordered landmarks on the surface of an object. Procrustes matching aligns two objects by removing the locational, rotational, and scaling effects [22, 29, 31]. In this project, Procrustes matching was used to match the physiological inter-talker difference (tongue and lip orientation). The downsampled time-series multi-sensor and multidimensional articulatory data form articulatory shapes. An example is shown in Figure 2 [18]. This shape contains trajectories of the continuous motion paths of four sensors attached on tongue and lips, TT, TB, UL, and LL. A step-by-step procedure of Procrustes matching between two shapes includes (1) aligning the centroids of the two shapes, (2) scaling the shapes to a unit size, and (3) rotating one shape to match the other [19, 22, 31]. Let S be a set of landmarks as shown below.  S = {(yi, zi)}, i = 1, . . . , n  (1)  where (yi, zi) represents the i-th data point (spatial coordinates) of a sensor, and n is the total number of data points, where y is vertical and z is front-back. The transformation in Procrustes  matching is described using parameters {(cy, cz), (βy, βz), θ}:  y¯i z¯i  =  cos θ − sin θ sin θ cos θ  βy βz  yi − cy zi − cz  (2)  where (cy, cz) are the translation factors (centroids of the two shapes); Scaling factor β is the square root of the sum of the squares of all data points along the dimension; θ is the angle to rotate [30]. Each participant’s articulatory shape was transformed into an “normalized shape”, which had a centroid at the origin (0, 0) and aligned to the vertical line formed by the average positions (centroids) of the upper and lower lips. Scaling was not used in this experiment, because preliminary tests indicated scaling will cause slightly worse performance in speaker-independent dysarthric speech recognition. The normalization procedure was done in two steps. First, all articulatory data (e.g., a shape in Figure 2) of each speaker were translated to the centroid (average position of all data points in the shape). This step removed the locational effects between speakers. Second, all shapes of speakers were rotated to make sure the sagittal plane was oriented such that the centroid of lower and upper lip movements deﬁned the vertical axis. This step reduces the variation of rotational effects due to the difference in facial anatomy between speakers. Thus in Eq. 2, (cy, cz) are the centroid of shape S; Scaling factor (βy, βz) is set to [ 1 1 ]′; θ is the angle of the S to the reference shape in which upper and lower lips form a vertical line. Figure 2 shows an example, original data (Figure 2a) and the shape after Procrustes matching (Figure 2b).  3.2. Vocal tract length normalization: A data-driven approach for acoustic data Vocal tract length normalization is a representative approach to normalize speaker-dependent characteristics for speech recognition systems [32–36]. This approach is to normalize vocal tract length indirectly from acoustic data, because vocal tract length is highly relevant with pitch and formants [34]. Warping factor α is applied in linear frequency space by Bilinear rule,  Fˆ = F + 2 tan−1  (1 − α) sin(F ) 1 − (1 − α) cos(F )  (3)  where F is normalized frequency (i.e., divided by sampling frequency, Fs) and α is the warping factor and F = w/(2πFs). Warped Mel-frequency is calculated by applying warping factor  49  (Mel) Frequency (Hz) (Mel) Frequency (Hz)  8000 7000 6000  linear Bi-linear warped (, = 0.85) Mel Mel-warped (, = 0.85)  8000 7000 6000  linear Bi-linear warped (, = 1.25) Mel Mel-warped (, = 1.25)  5000  5000  4000  4000  3000  3000  2000  2000  1000  1000  0 0 1000 2000 3000 4000 5000 6000 7000 8000 Frequency (Hz) (a) Warping factor = 0.85 (minumum)  0 0 1000 2000 3000 4000 5000 6000 7000 8000 Frequency (Hz) (b) Warping factor = 1.25 (maximum)  Figure 3: Example of (Mel) warped frequency scale (sampling rate: 16 kHz).  α in Mel-frequency space,  Mα(w) = 2595 log10  
This paper presents an enhancement system for early stage Spanish Esophageal Speech (ES) vowels. The system decomposes the input ES into neoglottal waveform and vocal tract ﬁlter components using Iterative Adaptive Inverse Filtering (IAIF). The neoglottal waveform is further decomposed into fundamental frequency F0, Harmonic to Noise Ratio (HNR), and neoglottal source spectrum. The enhanced neoglottal source signal is constructed using a natural glottal ﬂow pulse computed from real speech. The F0 and HNR are replaced with natural speech F0 and HNR. The vocal tract formant frequencies (spectral peaks) and bandwidths are smoothed, the formants are shifted downward using second order frequency warping polynomial and the bandwidth is increased to make it close to the natural speech. The system is evaluated using subjective listening tests on the Spanish ES vowels /a/, /e/, /i/, /o/, /u/. The Mean Opinion Score (MOS) shows signiﬁcant improvement in the overall quality (naturalness and intelligibility) of the vowels. Index Terms: speech enhancement, glottal ﬂow, analysis synthesis vocal tract, spectral sharpening, warping 1. Introduction The removal of the larynx after a Total Laryngectomy (TL), changes the speech production mechanism. The trachea which connects the larynx and lungs for air source is now connected to a stoma (hole on neck) for breathing. The vocal folds which resided in larynx are no more available. After TL, there is no voicing and air source for speech production. Therefore alternative voicing and air source are needed for speech restoration. Three methods are available for this purpose, i) Esophageal Speech (ES), ii) Tracheo-Esophageal Speech (TES), and iii) Electrolarynx (EL). ES and TES both use a common voicing source, the Phyarngo-Esophageal (PE) segment, but with a different air source, while EL uses external devices for voicing source with no air source. The ES is preferred over other methods, because it does not require surgery (TES) or external devices (EL). ES involves, however, a low pressure air source, and an irregular PE segment vibration which results in low quality and low intelligible speech. Compared to the production of normal speech according to the source-ﬁlter model [1], the voicing source in ES is severely altered and does not have any fundamental frequency or harmonic components. The vocal tract ﬁlter is also shortened in ES. The ES can be enhanced by transforming the source and ﬁlter components to those of normal speech using signal processing algorithms. In previous studies ES is typically decomposed into its source and ﬁlter components using Linear Predication (LP)  based analysis-synthesis techniques. Based on this assumption the authors in [2, 3] replaced the voicing source with the Liljencrants- Fant (LF) voicing source, and reported signiﬁcant enhancements. Fundamental frequency smoothing and correction with the synthetic LF source model were used for quality enhancement also in [4]. ES enhancement based on formant synthesis has also shown signiﬁcant improvement in intelligibility [5, 6]. In [7] the source and ﬁlter components were modiﬁed by replacing the source with the LF model and increasing the bandwidth of ﬁlter formants for better quality speech. Statistical conversion from ES to normal speech has also improved intelligibility, but requires more ES data [8]. Some other not so common approaches are based on Kalman ﬁltering [9, 10, 11, 12], and modulation ﬁltering enhancement [13, 14]. Almost all methods available in the literature assume that the fundamental frequency of ES can be estimated accurately. The voicing source signal is then modiﬁed with the synthetic LF model voicing source. The vocal tract formants are typically considered to be the same as in normal speech signals. In reality, however, the fundamental frequency of ES is highly irregular and the voicing source resembles whispered speech. Moreover, formants center frequencies are affected by the shortening of vocal tract length due to surgery. In order to deal with these deﬁciencies, this paper proposes an ES enhancement method based on the GlottHMM single pulse synthesis [15, 16, 17]. The system decomposes ES into neoglottal waveform and vocal tract ﬁlter components using Iterative Adaptive Inverse Filtering (IAIF) [18]. Natural glottal pulse extracted from real speech is used to construct the glottal waveform by borrowing F0 curve and HNR from normal speech. The vocal tract ﬁlter is also modiﬁed by smoothing the spectral peaks and their bandwidths. The spectral peaks of the vocal tract ﬁlter are also moved to lower frequencies in order to compensate the rising of formant in ES. The formant bandwidths are also increased for better quality speech. The system is validated with Spanish Esophageal Vowels subjectively using the Mean Opinion Score (MOS). The paper in next section describes the system in detail. The subsequent sections contain results, discussion and ﬁnally conclusions. 2. System Description The proposed system, shown in Figure 1, is divided into three main components, i) analysis, ii) transformation, and iii) synthesis. The analysis part decomposes the voiced speech frame into its source and ﬁlter components. The transformation provides the modiﬁed source and ﬁlter components. Finally the modiﬁed components are combined in the synthesis part to generate enhanced ES.  55 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 55–59, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  HNR (dB)  0 5 10 15 20 25 30 35 
Dysarthria is a speech disorder caused by difﬁculties in controlling muscles, such as the tongue and lips, that are needed to produce speech. These differences in motor skills cause speech to be slurred, mumbled, and spoken relatively slowly, and can also increase the likelihood of dysﬂuency. This includes nonspeech sounds, and ‘stuttering’, deﬁned here as a disruption in the ﬂuency of speech manifested by prolongations, stop-gaps, and repetitions. This paper investigates different types of input features used by deep neural networks (DNNs) to automatically detect repetition stuttering and non-speech dysﬂuencies within dysarthric speech. The experiments test the effects of dimensionality within Mel-frequency cepstral coefﬁcients (MFCCs) and linear predictive cepstral coefﬁcients (LPCCs), and explore the detection capabilities in dyarthric versus non-dysarthric speech. The results obtained using MFCC and LPCC features produced similar recognition accuracies; repetition stuttering in dysarthric speech was identiﬁed correctly at approximately 86% and 84% for non-dysarthric speech. Non-speech sounds were recognized with approximately 75% accuracy in dysarthric speakers. Index Terms: Dysarthria, stuttering, non-speech dysﬂuency, DNN, MFCC, LPCC 1. Introduction Many studies have researched ways to improve the intelligibility of dysarthric speech, including methods that targeted particular aspects of speech to modify. Kain et al. [1] implemented a system of transformations that focused strictly on mapping vowels from individuals with dysarthria to vowels more characteristic of non-dysarthric speech. Those experiments showed an intelligibility increase of 6%. In 2013, Rudzicz [2] proposed a method that added the correction of other pronunciation errors and adjusted tempo. Among a cohort of listeners unfamiliar with the speech of people with cerebral palsy, word recognition rates increased by 19.6%. Crucially, the Levenshtein-based detection of phoneme repetitions and non-speech dysﬂuencies in that work depended on full phoneme segmentation, which may itself be quite challenging for dysarthric speech. Chee et al. [3] provided an overview of automatic stuttering detection, emphasizing its difﬁculty across a number of classiﬁcation methods. Czyzewski et al. [4], e.g., implemented artiﬁcial neural networks (ANNs) and ‘rough sets’ to detect three types of ‘stuttering’: stop-gaps, vowel prolongations, and syllable repetitions, obtaining accuracies up to 73.25% with ANNs and 91% with rough sets. Wis´niewski et al. [5, 6] performed two studies that used hidden Markov models with Melfrequency cepstral coefﬁcients (MFCCs) to detect stuttering.  The ﬁrst focused on both prolongation of fricative phonemes and blockades with repetition of stop phonemes that produced an accuracy of 70% [5]; the second strictly focused on prolongation of fricative phonemes and found an improvement in accuracy to approximately 80% [6]. Rath investigated modiﬁcations to MFCC feature vectors in speaker adaptation using deep neural networks (DNNs) [7], obtaining 3% improvements over Gaussian mixture models (GMMs) baselines. Across various types of speech features, deep learning has shown considerable improvements across several areas of speech recognition [8], compared with traditional techniques such as hidden Markov models. Here, we compare MFCCs (which are the most commonly used feature set in this domain [3]) and linear predictive cepstral coefﬁcients (LPCCs), which are another popular but less utilized feature set. An exception was Chee et al. [9], who applied LPCCs with k-nearest-neighbors and linear discriminant analysis classiﬁers to automatically detect prolongations and repetition stutters, with recognition accuracy up to 89.77%. In the related ﬁeld of automatic speech recognition ()ASR), MFCCs have consistently generated better results than LPCCs [10, 11]; to see if this trend extends to the domain of dysﬂuency detection, we compare these feature types with DNNs. 2. Methodology Figure 1: Overview of automatic stuttering detection method. 2.1. Data The TORGO database [12] was created by a collaboration between the departments of Computer Science and Speech-  60 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 60–64, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  Language Pathology at the University of Toronto, and the Holland-Bloorview Kids Rehab hospital. The corpus consists of recordings from seven participants, three females and four males ranging in age from 16 to 50, diagnosed with cerebral palsy or amyotrophic lateral sclerosis. Additionally, there are recordings from seven control speakers matched for age and gender. A combination of non-words, short words, restricted sentences, and unrestricted sentences were recorded by all participants with a 16 kHz sampling frequency using two microphones. The database also includes articulatory measurements using electromagnetic articulography, which is not used here. 2.2. Segmentation 
Dysarthria is a neurological speech disorder, which exhibits multi-fold disturbances in the speech production system of an individual and can have a detrimental effect on the speech output. In addition to the data sparseness problems, dysarthric speech is characterised by inconsistencies in the acoustic space making it extremely challenging to model. This paper investigates a variety of baseline speaker independent (SI) systems and its suitability for adaptation. The study also explores the usefulness of speaker adaptive training (SAT) for implicitly annihilating inter-speaker variations in a dysarthric corpus. The paper implements a hybrid MLLR-MAP based approach to adapt the SI and SAT systems. ALL the results reported uses UASPEECH dysarthric data. Our best adapted systems gave a signiﬁcant absolute gain of 11.05% (20.42% relative) over the last published best result in the literature. A statistical analysis performed across various systems and its speciﬁc implementation in modelling different dysarthric severity sub-groups, showed that, SAT-adapted systems were more applicable to handle disﬂuencies of more severe speech and SI systems prepared from typical speech were more apt for modelling speech with low level of severity. Index Terms: speech recognition, dysarthric speech, speaker adaptation, speaker adaptive training 1. Introduction Dysarthria is the collective name for a group of motor speech disorders, which result from single or multiple lesions in the brain. It usually results in the loss of motor speech control due to muscular atrophy and incoordination [1, 2]. Across various aetiologies, dysarthric speech is usually characterised by imprecise consonant production, reduced stress, slow speech rate, hypernasality, harsh and strained voice, muscular rigidity, spasticity, monopitch and limited range of speech movements [1, 2]. Dysarthria can either be congenital, occurring with conditions such as in cerebral palsy, or acquired, where it develops due conditions such as a stroke or Parkinson’s disease. The effect on speech production of dysarthria is not limited to the musculoskeletal structures, but it can also affect parts of subglottal, laryngeal and supraglottal systems [3]. It usually leads to reduced intelligibility of speech, which can be inversely related to the severity of the underlying condition. On a broad operational scale, severity can be indexed as mild, moderate, severe or any approximation within, such as mild-moderate. For people with severe dysarthria, their speech can be largely unintelligible to unfamiliar listeners.  It is estimated that around 1% of UK population is diagnosed with a neurological disorder each year, although, not all the conditions lead to dysarthria. In UK alone; stroke (416 per 100,000), cerebral palsy (200-300 per 100,000) and Parkinson’s disease (200 per 100,000) are amongst the most prevelant causes of motor speech disorders [4, 5]. 1.1. Speech interface and dysarthria Speech has provided an attractive interface for people with dysarthria by enhancing human-human & human-computer interaction. It can enable people with dysarthria to participate in social settings where they can interact with non-familiar communication partners. Moreover, speech as an interface can provide users with a more real-time communication experience to convey messages, in comparison to traditional hardwired switch based interfaces. Earlier studies have shown that systems that deploy automatic speech recognition (ASR) as an interface in a dysarthric setup can have a lower accuracy than hardwired switch-based systems, but, the ﬁnal message transfer is around 2.5 times faster than the later, even with mis-recognitions followed by corrections [6, 7]. According to a report by [8], more than 70% of dysarthric population with Parkinson’s disease or motor neuron disease and around 20% with cerebral palsy or stroke could beneﬁt from some implementation of an augmentative or alternative communication (AAC) device. The beneﬁts of such a setup has proved effective for dysarthric people using speech as an interface for natural communication [9] or enabling them to control physical devices through speech commands [7]. 1.2. Automatic speech recognition for dysarthric speech Dysarthric speech recognition has been investigated for more than two decades [10, 11]. The efﬁcacy of commercial systems has been limited for speakers with mild or mild-moderate dysarthria [12, 13]. In general, decreasing recognition accuracy is linearly related to increasing severity. As a consequence, it has been concluded that the systems are not suited to the higher variability inherent in dysarthric speech. From a research perspective; acoustic modelling, speaker adaptation and signal enhancement techniques have been explored by researchers to deal with variabilities and disﬂuencies in dysarthric speech. The system can be (i) speaker dependent (SD) , which is modelled to recognise only a particular speaker, (ii) speaker independent (SI), which is a generic model map to recognise a range of seen and unseen speakers and (iii) speaker adapted (SA), which attempts to minimise the mismatch between a  65 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 65–71, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics 
Pronunciation variation is a major problem in disordered speech recognition. This paper focus on handling the pronunciation variations in dysarthric speech by forming speaker-speciﬁc lexicons. A novel approach is proposed for identifying mispronunciations made by each dysarthric speaker, using state-speciﬁc vector (SSV) of phone-cluster adaptive training (Phone-CAT) acoustic model. SSV is low-dimensional vector estimated for each tied-state where each element in a vector denotes the weight of a particular monophone. The SSV indicates the pronounced phone using its dominant weight. This property of SSV is exploited in adapting the pronunciation of a particular dysarthric speaker using speaker-speciﬁc lexicons. Experimental validation on Nemours database showed an average relative improvement of 9% across all the speakers compared to the system built with canonical lexicon. Index Terms: Dysarthric speech recognition, phone-CAT, lexical modeling, pronunciations, phone confusion matrix 1. Introduction Clinical applications of speech technology play an important role in aiding communication for people with motor speech disorders. One such motor speech disorder is dysarthria, acquired secondary to stroke, traumatic brain injury, cerebral palsy etc. This affects more than one subsystem of speech production, leading to unintelligible speech. Some of the common characteristics of dysarthria include slurred speech, swallowing difﬁculty, slow speaking rate with increased effort to speak and muscle fatigue while speaking [1, 2]. All these effects affect the speech intelligibility but also the social interaction ability of people with speech disorders. Clinical applications of speech technology provide way to improve their communication in terms of the alternative and augmentative communication (AAC) devices. Automatic speech recognition (ASR) systems play a major role as an AAC device for aiding communication in terms of command/control in their daily lives. Only handful of databases are available for dysarthric speech, due to the fatigue and discomfort faced by the dysarthric speaker in providing data for longer time. With such constraints, acoustic models are usually built-in speaker adaptation framework [3, 4, 5]. The impairment in phonatory subsystem of a person affected with dysarthria leads to pronunciation errors. The slow rate of speech leads to a single syllable word being misrecognized as two syllable words. Frequent occurrences of non-speech sounds like hesitations false starts occur as part of dysarthric speech. These hesitations also lead to mis-  recognition of words as explained in [6, 4]. Imprecise consonant production is another characteristic of dysarthric speech. Since consonant production involves complex articulations compared to vowels, the errors are more frequent [7]. Muscle fatigue and lack of breath support increase the pronunciation errors of a dysarthric speaker [8]. All these effects increase the rate of insertions, substitutions, deletions and distortions in the dysarthric ASR systems. Thus the issue of pronunciation errors makes the design of dysarthric ASR system more challenging. The focus of this paper lies in handling these pronunciation errors especially substitutions by improving the lexical models. The lexicon contains the multiple pronunciations for each word expanded in terms of phones. The alternate pronunciations of a word is either formed manually [9] or obtained from the list of phone confusion pairs [10, 11]. This paper introduces a recently developed phonecluster adaptive training (Phone-CAT) [12] acoustic modeling technique. Phone-CAT method build robust acoustic models using lesser number of parameters and limited amount of data. Thus the method can be used for limited data available domains especially in the case of dysarthric speech recognition. The main contributions of this paper are as follows: • A novel approach to form speaker-speciﬁc phone confusion matrix using the low-dimensional SSV of PhoneCAT • Using the speaker-speciﬁc phone confusion matrix to identify the confusion pairs (substitution phones) to form alternate pronunciations in the speaker-speciﬁc lexicon Our proposed approach helps in forming phone confusion matrix directly from the Phone-CAT acoustic model, compared to the existing methods [10, 11] which align the decoded transcription with canonical transcription to form the phone confusion matrix. Thus we circumvent the usage of expensive decoding process. This preliminary study using Nemours database shows a relative performance improvement of 9% using our proposed approach compared to baseline model built using canonical lexicon. 2. Related work Multiple pronunciations of a word in the lexicon improves the recognition performance. The lexical models are improved either implicitly or explicitly handling the pronunciation errors [13]. In order to improve the lexical models, the phones mispronounced by each dysarthric speaker need to be identiﬁed. Earlier work handled multiple pronunciations using expert knowledge by adapting pronunciations manually [9]. Per-  72 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 72–78, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  sonalized speaker articulation patterns were obtained from the speaker-adapted models along with the confusion matrix. These speaker-adapted models were obtained using universal disordered matrix and the posterior probability from the ASR system in an unsupervised fashion [13]. Another approach for identifying the mispronounced phones is by aligning the decoded text with the true transcription. A phone confusion matrix is formed using the decoded transcription and canonical transcription. This phone confusion matrix is used to identify the mispronunciations [10]. The substitution, insertion and deletion errors, were modeled as discrete hidden Markov model (HMM) called metamodels [11]. Another variant of this system is to train the extended metamodels from an integrated confusion matrix using genetic algorithm [14]. The concept of weighted ﬁnite state transducer (WFST) improves the performance of speech recognition systems. Composing confusion matrix along with the lexicon and language models in the WFST framework provides complementary information to the system. This concept was used in speech recognition [15] and keyword searching [16]. In dysarthric speech recognition framework, different methods were used to form confusion matrices to be used with WFST. One such method is to use KL distance measure between two context-dependent triphones to form confusion matrix [17, 18]. Deep neural networks (DNN) can also be used to improve pronunciation models. The posterior probabilities from pre-trained DNN were used to identify mispronunciations. They were further analyzed to generate pronunciations to form speaker-speciﬁc lexicons [19]. All the above methods, uses confusion matrix obtained by aligning the decoded transcriptions with the canonical transcriptions to improve the lexicons.  MLLR transform matrices  Monophone models /sil/  State Specific Vectors  ContextDependant States /b/-/ae/+/t/  UBM  /ix/  . . . /zh/  . . /k/-/ix/+/n/ . . /g/-/o/+/v/  U – Unimpaired Speakers D – Dysarthric Speakers  For each Dysarthric Speaker  Figure 1: Phone-CAT architecture In this paper, a novel approach is proposed to form the confusion matrix using the low-dimensional vector from the Phone-CAT acoustic model. Each tied-state in Phone-CAT model is modeled using SSV. The dominant weight of the SSV represents the pronounced phone. The mispronounced phone of each dysarthric speaker obtained using SSV is compared with the canonical phone to form the phone confusion matrix. This matrix is used to improve the lexical models by providing alternate pronunciations of words. Since each speaker has a separate pronunciation pattern, speaker-speciﬁc lexicons are  formed. Acoustic models rebuilt using these lexicon, improve the performance of the system.  3. Phone-Cluster Adaptive Training Acoustic Models The acoustic models are usually built using hidden Markov model–Gaussian mixture model (HMM–GMM) framework. The acoustic variations of speech due to age, gender, environmental changes and pronunciation variations are being modeled using GMM. The sequence information involving coarticulation is modeled as HMM. The triphone model represents a phone along with its left and right contexts capturing the co-articulation effects. For example, consider the triphone /ax/ − /b/ + /k/ representing the model for the center phone /b/, capturing the effect of its left context /ax/ and right context /k/. Several triphones with similar acoustic characteristics and same center phone /b/ are clustered to form a single tiedstate. The GMM parameters are then estimated independently to model each tied-state. This estimation requires huge number of parameters and sufﬁcient amount of data. This issue is handled using the recently proposed phone-CAT acoustic model by robustly modeling the available data with lesser number of parameters. Phone-CAT is a HMM-GMM system in which the GMM parameters are represented in a compact form. In other words, the GMM for each tied-state is formed by the linear combination of all the monophone GMMs in that language. For example, the tied-state /ax/ − /b/ + /k/ containing triphones /ax/−/b/+/k/,/ch/−/b/+/k/,/ae/−/b/+/k/ is formed from the linear combination of all the monophone GMMs like /sil/, /ax/, .../k/, ..., /zh/. The weights of each monophone GMMs are represented by vj(1), vj(2) . . . vj(P ), where P is the number of monophones. The vector containing the monophone weights is called SSV and is represented as vj = vj (1) vj (2) . . vj (P ) T with P dimensions. The monophone GMMs are in turn formed by adapting the universal background model (UBM) using maximum likelihood linear regression [20] transformation. The UBM is a GMM built using the available speech data from all the speakers. This UBM is adapted using the transformation matrices W1, W2, ...., WP for each of the P monophones, forming P monophone GMMs. The Phone-CAT architecture is shown in ﬁgure 1. The GMM parameters of the tied-state model are: means µji, covariances Σi and Gaussian priors wji. The mean parameter for each monophone models µi(p) with Gaussian mixture i is combined to form the mean parameter of the tied-state j using the following equations: µi(p) = W P ξi = W P [µi 1]T  P  µji =  µi(p) vj(p)  p=1  Here ξi is the extended mean vector [µi 1]T with µi as the canonical mean of the Gaussian component i of the UBM. Since the mean µji and the Gaussian prior wji are represented in terms of the vector SSV vj as in [12], the parameters are rep- resented in low dimensions. Also the covariances Σi are esti- mated in a shared fashion across the tied-states. This reduction  in the number of parameters helps in reducing the amount of  data needed for estimation. More details of the model training  and estimation of each parameters are explained in [12].  73  4. Importance of state-speciﬁc vectors The SSV is a low-dimensional vector of dimension P representing each tied-state j uniquely. It captures the context information since it represent the weights with which each monophone GMM linearly combine to form a single tied-state. We know that different triphones with the same acoustical characteristics are tied together in order to form tied-state. The SSV plot of the second state of the triphone /ch/ − /ix/ + /ng/ is shown in ﬁgure 2. It is clearly shown that, the dominant weight corresponds to the center phone /ix/. Apart from the center phone, the left and right context phones also get some considerable weight. The negative value represents the direction of the vector, but we are interested only in the absolute value of the elements of the SSV. Figure 2: SSV plot of the second state of the triphone /ch/ − /ix/ + /ng/ Figure 3: This two-dimensional scatter plot is obtained using the t-SNE toolkit by plotting the SSV of all the tied-states in Nemours database A statistics of the dominant weight property of SSV was performed for unimpaired (control) speech data from Nemours speech database. The aim of the task was to check the statistics of the SSV picking the center phone of the tied-state correctly. It was found that out of 204 tied-states, the dominant component of SSV correctly picks the center phone 76% of the times and the top three weight values in the SSV picks up center phone 88% of the time. A similar analysis was also performed for the standard Switchboard database (≈300 hours of data), with 2400 tied-states. It was found that 70% of the time, the center phone was correctly picked up by the dominant component of SSV.  Also ≈92% of the time, the left/center/right phones are picked up as the dominant component of SSV [21]. This shows that the SSV uniquely represents the enunciated phone (center phone of the tied-state) through its dominant weight most likely. The scatter plot of the P dimensional SSV reduced to two dimension is shown in the ﬁgure 3. The SSV related to each cluster represents a particular monophone (each in different color, a total of 39 phones were present in the Nemours database). These clusters are located at articulatory position of the vowel triangle in a well discriminated manner. This shows that SSV has the capacity to capture the phonetic information along with context information. Thus the analysis of SSV in this section leads us to the following conclusions: • The dominant weight in SSV most likely represents the enunciated phone (center phone) of the tied-state • Provides discriminable phonetic class information, since each vector is modeled for a particular tied-state • SSV is hypothesized to capture the pronunciations of each dysarthric speaker when speaker-speciﬁc PhoneCAT models are built This leads us to proceed to the proposed method of building Phone-CAT model speciﬁc to each speaker, thereby capturing the pronunciations of each dysarthric speaker.  Table 1: Extract dysarthric enunciated phone from SSV  Tied-states  Phones  (Canl)  sil aa . . . ey . . . zh Dysp  ∗ − /sil/ + ∗ 1.75 0.21 . . . 0.38 . . . 0.12 /sil/  ∗ − /aa/ + ∗ 0.03 0.09 . . . 0.01 . . . 0.08 /aa/  ...  ...  ...  ...  ...  ...  ...  ...  ∗ − /jh/ + ∗ 0.19 0.11 . . . 0.36 . . . 0.13 /ey/  ch−/ix/+ng 0.48 0.50 . . . 0.01 . . . 0.22 /aa/  n − /ix/ + k 0.10 0.90 . . . 0.25 . . . 0.76 /aa/  ...  ...  ...  ...  ...  ...  ...  ...  ∗ − /zh/ + ∗ 2.01 0.02 . . . 0.10 . . . 3.06 /zh/  Canl - canonical pronunciation; Dysp - dysarthric pronunciations  The numbers inside circle shows the absolute maximum value in each  SSV corresponding to dysarthric pronounced phone  5. Proposed Method for Improving Lexical Models 5.1. Phone-CAT model for each dysarthric speaker The major step of our proposed method is to build speakerspeciﬁc Phone-CAT model. Initially, using the unimpaired speaker’s data in the dysarthric database, a Phone-CAT model is built. The speaker-speciﬁc Phone-CAT model is obtained from the unimpaired speaker model by re-estimating the SSV and providing dysarthric speaker’s data in maximum likelihood framework. The SSVs are initialized as (1/number of monophones), to allow the system to learn the weights of the monophone GMM using the available dysarthric speaker’s data. At the end of this training process, Phone-CAT speaker-speciﬁc models are built. The architecture of speaker-speciﬁc PhoneCAT model is shown in ﬁgure 1. Finally, we obtain a set of tied-states speciﬁc to each dysarthric speaker from the speakerspeciﬁc Phone-CAT model.  74  
Articulatory data have gained increasing interest in speech recognition with or without acoustic data. Electromagnetic articulograph (EMA) is one of the affordable, currently used techniques for tracking the movement of ﬂesh points on articulators (e.g., tongue) during speech. Determining an optimal set of sensors is important for optimizing the clinical applications of EMA data, due to the inconvenience of attaching sensors on tongue and other intraoral articulators, particularly for patients with neurological diseases. A recent study found an optimal set (tongue tip and body back, upper and lower lips) on tongue and lips for isolated phoneme, word, or short phrase classiﬁcation from articulatory movement data. This four-sensor set, however, has not been veriﬁed in continuous silent speech recognition. In this paper, we investigated the use of data from sensor combinations in continuous speech recognition to verify the ﬁnding using a publicly available data set MOCHA-TIMIT. The long-standing speech recognition approach Gaussian mixture model (GMM)-hidden Markov model (HMM) and a recently available approach deep neural network (DNN)-HMM were used as the recognizers. Experimental results conﬁrmed that the four-sensor set is optimal out of the full set of sensors on tongue, lips, and jaw. Adding upper incisor and/or velum data further improved the recognition performance slightly. Index Terms: silent speech recognition, deep neural network, hidden Markov model, electromagnetic articulograph, articulation, dysarthria 1. Introduction With the availability of affordable devices for tongue movement data collection, articulatory data have obtained interest not only in speech science [1, 2, 3, 4] but also in speech technology (i.e., automatic speech recognition) [5, 6]. First, articulatory data have been successfully used to improve the speech recognition accuracy [5]. Articulatory data are particularly useful when speech signals are noisy or low quality [7] for recognizing dysarthric speech [8, 9]. Second, when acoustic data is not available, a silent speech interface (SSI) based on articulatory data has potential clinical applications [10, 11]. An SSI recognizes speech from articulatory data only (without using audio data) [12, 13] and then drives a text-to-speech synthesizer for sound playback [14, 15]. For example, SSIs can be used to assist the oral communication for patients with severe voice disorders or without the ability to produce speech  sounds (e.g., due to laryngectomy, a surgical removal of larynx due to treatment of laryngeal cancer) [16]. There are currently limited options to assist speech communication for those individuals (e.g., esophageal speech, tracheo-esophageal speech or tracheo-esophageal puncture (TEP) speech, and electrolarynx). These approaches, however, produce an abnormal sounding voice [17, 18], which impacts the quality of life of laryngectomees. Current text-to-speech technologies have been able to produce speech with natural sounding voice for SSIs [19]. One of the current challenges of SSI development is silent speech recognition algorithms (without using audio data) [10, 20] or mapping articulatory information to speech [21, 22, 23]. Electromagnetic motion tracking is one of the affordable, currently used technologies for tracking tongue movement during speech [19, 24, 25]. There are currently two commercially available devices, EMA AG series (by Carstens) and Wave system (by NDI, Inc.) [26]. Tongue tracking using electromagnetic devices is accomplished through attaching small sensors on the surface of tongue and other articulators. In prior work, the number of tongue sensors and their locations have been justiﬁed based on long-standing assumptions about tongue movement patterns in classic phonetics [27], or the speciﬁc purpose of the study. Other techniques that have been used to record non-audio articulatory information include ultrasound [28, 29], and surface electromyography (EMG) [30, 31]. Determining an optimal set of tongue sensors for speech production is signiﬁcant for both science and technology. Scientiﬁcally, determining an optimal set of sensors can improve the understanding of the coordination of articulators for speech production [32]. Technologically, it can be helpful for clinical applications including (1) silent speech interfaces, (2) speech recognition with articulatory information [5, 33], and (3) speech training using real-time visual feedback of tongue movements [34, 35]. In literature, three or four EMA sensors on the tongue have been commonly used (e.g., [1, 3, 4, 5, 36, 37]). The use of more sensors than necessary comes at a cost for both researchers and subjects; the procedure for attaching sensors to the tongue is time intensive and can cause discomfort and therefore may limit the scope of EMA for practical use, particularly for persons with neurological diseases (e.g., Parkinson’s disease [38] and amyotrophic lateral sclerosis [39]). Here, optimal set means a sensor set that contains the least number of sensors that performs no worse than other sets with more sensors. There may be more than one optimal set with the same number of sensors.  79 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 79–85, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  Until recently, a study found two tongue sensors (Tongue Tip and Tongue Body Back) and two lip sensors (Upper Lip and Lower Lip) are optimal for isolated phoneme (vowels and consonants), word, and short phrase classiﬁcation [32, 40]. The classiﬁcation results based on data using the optimal set were not signiﬁcantly different from these based on data from the full set with four tongue sensors (Tongue Tip, Tongue Blade, Tongue Body Front, and Tongue Body Back) plus the two lip sensors [32]. However, this set has not been veriﬁed in continuous silent speech recognition or speech recognition from both acoustic and articulatory data. If the two-tongue-sensor set can be conﬁrmed for continuous speech recognition, it would be beneﬁcial for future collection of a larger articulatory data set. Other studies compared the whole tongue and lips (e.g., [41] using ultrasound and optical data), but not on ﬂesh points. In this paper, we investigated the optimal set of tongue sensors for speaker-dependent continuous silent speech recognition (using articulatory data only) and speech recognition (using combined acoustic and articulatory data). The goals were (1) to conﬁrm if more than two tongue sensors are unnecessary for continuous silent speech recognition and speech recognition using both acoustic and articulatory data when only tongue and lips are used, and (2) to provide a reference for choosing the number of sensors and their locations on the tongue, lips, jaw and other articulators for future studies. However, due to the space limitation, this paper did not verify if the hypothesized optimal four-sensor set is unique. The articulatory and acoustic data in the MOCHA-TIMIT data set [42] were used in this experiment. The MOCHA-TIMIT data set is appropriate for this study because it contains data collected from sensors attached on multiple articulators, including three sensors on the tongue, two on the lips, two on the incisors, and one on the velum. In addition, both MOCHA-TIMIT and the data set in [32] have tongue tip and body back (or dorsum). Thus the ﬁrst goal of this paper became to verify if the tongue blade sensor is unnecessary in addition to the hypothesized optimal set [32, 40]. The traditional speech recognition approach Gaussian mixture model (GMM)-hidden Markov model (HMM) [5] and a recently available and promising approach deep neural network (DNN)-HMM [43, 44] were used. 2. Method 2.1. Data set MOCHA (Multi-CHannel Articulatory)-TIMIT data set consists of simultaneous recordings of speech, articulatory movement, and other forms of data collected from 2 British English speakers (1 male - MSAK0 and 1 female - FSEW0) [42]. There are 920 sentences (extracted from TIMIT database) in total. Individual phonemes and silences within each sentence have been labeled. The articulatory and acoustic data in MOCHA-TIMIT were collected using an Electromagnetic Articulograph (EMA, Carstens Medizinelektronik GmbH, Germany) by attaching sensors to upper lip (UL), lower lip (LL), upper incisor (UI), lower incisor (LI), tongue tip (TT), tongue blade (TB), tongue dorsum (TD), and velum (V) with 500 Hz sampling rate. Each sensor had x (front-back) and y (vertical) trajectories. Therefore, the acoustic data and the 16-dimensional x and y motion data obtained from UI, LI, V, UL, LL, TT, TB, and TD were used. TT was 5-10 mm to the tongue apex; TB was 2-3 cm from TT; TD was 2-3 cm from TB [42]. This roughly matched with  the tongue tip sensor in [32, 40], which was also 5-10 mm to tongue apex, and the tongue body back in [32, 40], which was about 40 mm from tongue tip. Thus, as mentioned earlier, the goal (1) in this paper became to verify if the middle tongue sensor (TB) was unnecessary. 2.2. Recognizers A long-standing approach GMM-HMM and a promising approach DNN-HMM were used as the recognizers in this experiment. 2.2.1. Gaussian Mixture Model-Hidden Markov Model GMM-HMM has been used in speech recognition for decades [45]. The core idea of GMM is compact representation of distribution using means and variances. GMM is a generative model and trained to represent as closely as possible the distribution (e.g., using means and variances) of training data. In many applications, the number of mixtures for GMMs is adjusted to avoid overﬁtting. 2.2.2. Deep Neural Network-Hidden Markov Model DNN-HMM recently attracted the interests of speech recognition researchers because it showed a signiﬁcant performance improvement compared with GMM-HMM when replacing GMM to DNN in (acoustic) speech recognition [44, 46]. We adopted the DNN training approach based on restricted Boltzmann machines (RBMs) [47]. The DNN (stacked RBMs) were subsequently ﬁne-tuned using the backpropagation algorithm. A detailed explanation and discussion of the DNN can be found in [47, 48]. 2.3. Experimental setup Data from individual sensors or combinations of sensors were used in speech recognition experiments (from articulatory data only or from combined acoustic and articulatory data). The recognition performances obtained from individual sensors or their combinations were compared to determine (1) if Tongue Blade was unnecessary in addition to the other two tongue sensors and lips (Tongue Tip, Tongue Dorsum, Upper Lip, and Lower Lip), and (2) if the performance improved when more sensor’s data (e.g., upper incisor and velum) were added. In each experiment, a 5-fold cross validation strategy with a jackknife procedure was performed to set training and test sets in the experiment [42, 49]. In each of the ﬁve executions, a group of 92 sentences were selected for test with the remaining 368 sentences for training. Due to the high degree of variation in the articulation across speakers and there were only two speakers in MOCHA-TIMIT, speaker-dependent recognition was conducted. The average training data length for each cross validation became 21.3 mins (368 sentences) for the female speaker and 20.6 mins (368 sentences) for the male speaker. The average test data length along 5 cross validations was 5.3 mins (92 sentences) for the female speaker and 5.2 mins (92 sentences) for the male speaker, respectively. Articulatory features were extracted from the corpus using EMAtools [50]. The original articulatory features and their ﬁrst and second derivatives were concatenated to build various dimensional feature vectors for each set of sensors. The “breath” segments were merged with “silence” for both training and testing [49]. The input features in DNN were a concatenation of articulatory feature vectors (number of sensors × 2-dimension articulatory movement data + ∆ + ∆∆) with 9  80  Table 1: Experimental setup.  Articulatory Feature Low pass ﬁltering Sampling rate Feature vector Acoustic Feature Sampling rate Feature vector Frame size Common Frame rate Mean normalization  40 Hz cutoff, 5th order Butterworth 100 Hz (downsampled from 500 Hz) articulatory movement vector + ∆ + ∆∆ (e.g., 6 dim. for 1 sensor, 48 dim. for 8 sensors) 16 kHz MFCC vector (13 dim.) + ∆ + ∆∆ (39 dim.) 25 ms 10 ms Applied  GMM-HMM topology Monophone Training method  context-independent 137 states (44 phones × 3 states, 5 states for silence), ≈ 14 mixtures 3-state left to right HMM Maximum likelihood estimation  DNN-HMM topology Monophone Training method  context-independent input layer dimension varies based on the set of sensors (e.g., 54 for 1 sensor, 432 for 8 sensors) 137 output layer dimension (including 5 outputs for silence) 1,024 nodes for each hidden layer 1 to 6-depth hidden layers RBM pre-training, back-propagation  Language model  bi-gram phoneme language model  frames (4 preceding, current, and 4 succeeding frames). As it concatenates multiple feature vectors in the time domain, DNN has time-dependent context information which HMM takes using multiple states [43, 51]. Mel-frequency cepstral coefﬁcients (MFCCs) were extracted from the acoustic data and used as the acoustic features in the recognition experiments. The GMM-HMM system was trained using maximum likelihood estimation (MLE) without using segment information provided in MOCHA-TIMIT corpus (ﬂat initialization). The DNN-HMM system was pre-trained using contrastivedivergence algorithm on RBMs and ﬁne-tuned using backpropagation algorithm. A bi-gram phoneme language model was trained using all 44 phonemes provided in label ﬁles of the corpus. Table 1 lists the details of the experimental setup and major parameters in GMM-HMM and DNN-HMM. The training and decoding were performed using the Kaldi speech recognition toolkit [52]. A phoneme error rate (PER) was used as a performance measure, which is the ratio of the sum of the number of errors over the total number of phonemes. The PER is represented by  PER = (S + D + I)/N  (1)  where S represents the number of substitution errors, D is the number of deletion errors, I stands for the number of insertion errors, and N is the total number of phonemes in the test set. For DNN, we conducted experiments using 1 to 6 hidden layers and the best performance was reported. Finally, the PERs from  each test group in the 5-fold cross validation were averaged as the overall PER. 3. Results and Discussion Experimental results are shown in Figures 1 to 4 and discussed below. Figures 1 and 2 show the silent speech recognition performance on individual or combinations of sensors for both speakers using GMM-HMM or DNN-HMM, respectively. Figures 3 and 4 give the speech recognition from MFCCs plus individual or combinations of sensors’ data using GMM-HMM and DNN-HMM, respectively. 3.1. General observations First, the recognition performances obtained from individual sensor’s data had consistently lower performance (higher PERs) than from the combinations of sensors (Figures 1 to 4). Although it seems intuitive, to our knowledge, this is the ﬁrst time the individual EMA sensor’s performance were examined in continuous silent speech recognition or speech recognition from combined acoustic and articulatory data. Second, when the performances obtained using data from individual sensors were compared, upper incisor (UI) and velum (V) had the worst performance; the three individual tongue sensors had a similar performance and were the best among all sensors; lip sensors were between the tongue sensors (TT, TB, TD) and UI and velum (V). This ﬁnding is highly consistent with the descriptive knowledge in classic phonetics that tongue is the primary articulator [27]. 3.2. {TT, TD, UL, LL} and other combinations Silent speech recognition performance substantially degraded if any of the sensor in previously found optimal four-sensor set (i.e., TT, TD, UL, and LL, marked bold in Figures 1 and 2) was not used [32]. The optimal set of sensors using GMM-HMM and articulatory data yielded a PER of 42.0% and 40.9% for the female and male speakers, respectively. DNN-HMM with this optimal set yielded a PER of 38.2% and 36.5% for the female and male speakers, respectively. As TB, UI, LI (jaw), or all of the three sensors’ data were added on top of the four-sensor set, there was no improvement using GMM-HMM, but a slight improvement using DNNHMM. When using all sensors’ (including V) data together, a substantial improvement was obtained using either GMMHMM or DNN-HMM. These results suggest the four-sensor set ({TT, TD, UL, LL}) was an optimal set for silent speech recognition out of the full set of sensors on the tongue, lips, and jaw. However, adding extra data source (e.g., UI and V) could still improve the performance. Speech recognition from combined acoustic and articulatory data (Figures 3 and 4) also substantially degraded if any of the sensor in {TT, TD, UL, and LL} was missing, for recognizers. However, GMM-HMM and DNN-HMM results showed different patterns when adding more sensors data to {TT, TD, UL, LL}. GMM-HMM showed no improvement to the optimal set (23.0% for female and 22.6% for male) when adding more sensor’s data (22.7% for female and 22.8% for male); while DNN-HMM (19.7% for female and 19.5% for male) showed signiﬁcant error reduction compared to the optimal set (20.4% for female and 20.3% for male). This observation suggests DNN has more potential than GMM to incorporate more data sources to further improve the recognition performance.  81  Phoneme Error Rate (%)  84.0 74.0 64.0 54.0 44.0 34.0 24.0 UI  Female Male  V  LI UL LL TT TB TD UI,LI UL,LL TT,TD TT, UL,LL, LI, UL,LL, LI, UI,LI, LI, UI,LI, MFCC  TB,TD TT,TD UL,LL, TT,TB, UL,LL, UL,LL, UL,LL, UL,LL,  TT,TD TD TT,TB, TT,TB, TT,TB, TT,TB,  TD TD TD, V TD, V  Combination of Sensors  Figure 1: Phoneme Error Rates (PER; %) obtained using GMM-HMM and articulatory features.  Phoneme Error Rate (%)  Phoneme Error Rate (%)  84.0 74.0 64.0 54.0 44.0 34.0 24.0 UI  Female Male  V  LI UL LL TT TB TD UI,LI UL,LL TT,TD TT, UL,LL, LI, UL,LL, LI, UI,LI, LI, UI,LI, MFCC  TB,TD TT,TD UL,LL, TT,TB, UL,LL, UL,LL, UL,LL, UL,LL,  TT,TD TD TT,TB, TT,TB, TT,TB, TT,TB, TD TD TD, V TD, V  Combination of Sensors  Figure 2: Phoneme Error Rates (PER; %) obtained using DNN-HMM and articulatory features.  32.0  30.0  Female  28.0  Male  26.0  24.0  22.0  20.0  18.0  MFCC MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+ MFCC+  UI  V  LI UL LL TT TB TD UI,LI UL,LL TT,TD TT, UL,LL, LI, UL,LL, LI, UI,LI, LI, UI,LI, 
This paper discusses the development of an Arabic Symbol Dictionary for Augmentative and Alternative Communication (AAC) users, their families, carers, therapists and teachers as well as those who may benefit from the use of symbols to enhance literacy skills. With a requirement for a bi-lingual dictionary, a vocabulary list analyzer has been developed to evaluate similarities and differences in word frequencies from a range of word lists in order to collect suitable AAC lexical entries. An online bespoke symbol management has been created to hold the lexical entries alongside specifically designed symbols which are then accepted via a voting system using a series of criteria. Results to date have highlighted how successful these systems can be when encouraging participation along with the need for further research into the development of personalised context sensitive core vocabularies. Index Terms: symbols, Augmentative and Alternative Communication, AAC, core vocabularies 1. Introduction In the last few years it has become clear that many therapists and teachers working with individuals who have speech and language difficulties in the Arabic speaking Gulf area, are depending on westernized symbols and English core vocabularies. Issues around limited Arabic language knowledge and dependency on translations or working in English can cause difficulties for those who need Augmentative and Alternative forms of Communication (AAC) due to disabilities. Huer [1] reports that “observations of communication across cultures reveal that non-symbolic as well as symbolic forms of communication are culturally dependent” and her later work “suggests that consumers, families, and clinicians from some cultural backgrounds may not perceive symbols in the same way as they are perceived within the dominant European-American culture” [2]. With this in mind the Arabic Symbol Dictionary research team were determined to take a participatory approach to their  project, involving AAC users and those supporting them as well as other researchers working in the field of Arabic linguistics and graphic design. 2. Background Much has been written by speech and language therapists about the necessity for core vocabularies that have been adapted to suit symbol users who need to enhance their language skills [3], [4], [5] and [6]. Research has shown that with a few hundred of the most frequently used words 80% of one’s communication needs can be accommodated [7]. More recently concept coding [8] with the idea of mapping different symbol vocabularies along with a focus on psychosocial and environmental factors [9] to improve outcomes have been added to the mix. However, there is very little research that has been undertaken to provide therapists with suitable vocabularies for Arabic AAC users [10]. In English these vocabularies tend to be lists of frequently used words from spoken and written language across all age groups and some from AAC users. Despite considerable searching there are very few of these vocabularies available in Arabic with most coming from language learning or frequently used word lists with no specified ages or Arabic AAC users. In some areas there is also a lack of understanding regarding the complexities of Arabic spoken and written language that disproportionately affect those who may have communication and reading difficulties [11], [12] and [13]. Usziel-Karl et al [13] cite several researchers in the course of their study concerning Arabic and Hebrew linguistic frameworks and discuss the “critical importance of morphology as the main organizing principle both of the lexicon and of numerous grammatical inflections”. The authors go on to point out the diglossia [two variations of a language in different social situations] nature of Arabic which means there is a ‘phonological distance [in grapheme-to-phoneme mapping] that has a negative impact on the acquisition of basic literacy skills in young Arabic children…” Words or word phrases (referents) may also be presented above or below a corresponding symbol, with changing forms depending on  91 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 91–96, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  grammatical status, gender and/or number plus many letters will change their shape depending on their position within a word. The authors of this research and others have also found there are key cultural and family values/orientations that should be considered in order to increase the effectiveness of symbolreferent vocabulary interventions [14] with individuals who use AAC within Arab communities. To this end not only has research concentrated on word frequency lists and collating an AAC user core vocabulary, but also instigating a voting system for symbol acceptance, so that words or multiword/word phrases are represented by symbols that are suitable culturally, linguistically and for the settings in which they will be used. 3. Methodology for Building a Core Vocabulary The building of an Arabic AAC core vocabulary is ongoing, but began with the collection of word lists used by AAC users, their families, carers, speech and language therapists and teachers in Doha (Qatar) (List a). Sixty three of these individuals joined an AAC forum and these participants have continued to work with the team as symbols for the vocabularies have been developed. The initial aim was to collect around 100 localised Arabic most frequently used words and multiwords to compare with those already in use that were in English or translated into Arabic based on English core vocabularies. Participating therapists felt a further 400 words/multiwords would be the maximum the majority of their users would have in their communication books or devices. Most English speaking three year olds use over a thousand words [15] so it was essential that the fringe vocabulary should be enlarged with words specific to the environment and personal needs including Qatari colloquial words and place names as well as to be relevant to all ages. Surveys of core vocabularies in Arabic have revealed that few are freely available [16] and even less make good companions when thinking of basic language and literacy learning for AAC users. In order to expand the list of 500 words a comparison was carried out against five other Arabic word frequency lists. Those for general conversation included the Kelly Project [17], 101languages.net 1000 most common spoken Arabic words and Aljazeera comments often using colloquial language [18]. The Supreme Education Council (SEC) literacy lists Grade 1,2,3 and Lebanese reading lists [19] have been used for literacy skill building in Modern Standard Arabic (MSA). 3.1. Building a vocabulary list analyser An automatic system was developed that took as an input two main pieces of information: List a: The list to be analyzed as a basis for the new core vocabulary list: This list could optionally have frequency of each entry included. If no frequency is available then a default value should be added to all the entries before running the program. Frequency in this case equated to how often a word was used. This frequency does not have to correspond to an actual frequency of occurrence in a text somewhere. Lists b: Lists combining existing vocabularies from a number of sources with the same structure as List a. Multiple vocabularies are used in Lists b in an attempt to weight the  occurrence of individual words. These vocabularies are ideally from different sources and should be large enough so that the frequencies of the entries listed are reliable. The system produced three lists shown in Figure 1: List 1: Initial list containing the words in List a (the in-put list to be analyzed) that did not occur in any of Lists b. This output only contained the words with no frequency scores. List 2: The coverage list: containing the words that occurred in List a and at least once in a source vocabulary in Lists b. This output also contained scores for each word by source vocabulary list (each word was given several scores, one for each list in Lists b). Each score equals the frequency with which each word appeared in the list from Lists b, normalized by dividing the frequencies of each word by the sum of all frequencies in that list. The score was set to 0 if the word did not occur in that list. Figure 1. Input lists (list a and lists b) List 3: Remaining word list: This list contained all the words that were in Lists b but were not contained in List a. This output also contained the scores for each word and is the example of the system in use (Figure 2). This is the list on which the comparison in the section 3.2 is based. Figure 2. Example Output from lists viewed in Excel Figure 2. shows frequencies are normalized to allow source vocabularies to be compared (column one), this process can be problematic if the list is too small as the numbers may become too high and significantly affect results. Even if there is  92  sufficient data, it is still imperative that an expert goes through the different output list to inspect the results, correct errors and choose the set of words to be added or removed from the input list. The scores given only act as a guide to assist the expert in the process. In practical terms words with high scores in List 3 could be deemed suitable for inclusion in the Arabic Symbol Dictionary and added to List a. The system has been run repeatedly as lists have been added so that results become more robust. 3.2. Results of the Core Vocabulary building When comparing the list provided by participants as examples of AAC users’ vocabularies (List a), there were very small overlaps with those words most frequently found where the top words were based on very high frequency scores for those most commonly used (Lists b). To provide an instant comparison between Output 1 and 3 the top 20 words translated from Arabic are listed below. Output from 1 (List a) ordered by those most often used in AAC lists. “I/me (am), go, ball, car, banana, on/to, thing/something, to, chair, clock/watch, want, in, sit, was, eat, bike, flower/rose, play, cup, door” Output from 3 (Lists b) ordered by frequency “the, God, about, oh, to, which (masculine), and not, people, no, which (feminine), in, even, or, on, against, only, however, Arabs, must, order” Further analysis of the Lists b that were about spoken and colloquial language shows that nouns only made up 5% of the total list from the Kelly project, 25 to 30 % of the Aljazeera and Oweini-Hazoury lists, but 50% of the AAC lists. A concrete noun, even if it is considered part of a fringe vocabulary, is a much easier concept to illustrate with a symbol and may be seen as one of the early building blocks to language acquisition. Verbs, however are more complex and have low frequency rates; between 5 to 20 %. The Aljazeera list has the lowest and the AAC lists have the highest. The other parts of speech, equally pertinent in communication, such as adjectives, adverbs, prepositions, pronouns and conjunctions were found to be variably frequent from one list to another. The Aljazeera list has a quarter of its frequencies made up of prepositions, whereas Kelly’s list, SEC and the AAC user list have only 5%. Conjunctions also show low frequencies through the lists in question; between 1% and 15%. It is worth mentioning that pronouns are totally nonexistent in Kelly’s project list, either under their detached form or attached form. It should also be noted that therapists may choose nouns rather than pronouns for the purpose of symbol transparency. The other lists had less than 20% of pronouns all types combined. Arabic pronouns, and also some prepositions combine with nouns or with other parts of speech as single words, this morphological aspect could be the reason why their frequencies are rather undermined. Adverbs are also rarely listed, The Owein-Hazoury list has none; the highest adverb frequency is found in the 1000 most common Arabic words list (4%). In Arabic most adverbs of time and space are prepositional groups; typically a structure made of a preposition followed by a noun. This structural definition of adverbs explains the low number or even the lack of adverbs  in some of the core vocabulary lists. The users would frame appropriate phrases to express adverbs by using existing prepositions combined with nouns. Further confirmation for these differences in the frequency of various parts of speech was sought for the literacy skill vocabularies. The conversational based lists were replaced with reading lists forming Lists b. Arabic lists such as those used SEC and Arabic sight words [19]. It was found that in their top 100 frequently used words 30 and 38 were nouns respectively. 3.3. Discussion about the core vocabulary data collection As can be seen from the top 20 words in List a and Lists b, both show nouns that would not be found in the top twenty frequently used words in an English core vocabulary and in reality would be considered fringe words. However, the lists do illustrate that in Arabic there are elements of the grammar that are equally as important such as conjunctions and prepositions. There are considerable issues with the fact that root words in Arabic clearly appear within other words and this can affect the results as well as the fact that the lists collected from AAC users are based on popular use, rather than large scale frequency levels within a huge corpus. There will always be the need to improve outcomes by collecting more lists from AAC users in the future to improve the balance between words used for symbol communication and those based on frequency of use, although the latter informs vocabulary development By using this system the combined AAC word lists from the Doha schools and clinics making up ‘List a’ once translated into English, could be compared to the Prenke Romich 100 Frequently Used Core Words [20], [21] (as Lists b). It was noted that the Doha Arabic AAC user list (List a) contained 38 nouns in the top 100 words compared to none appearing in the English core vocabulary. It has been said that in English the use of nouns goes from 7% in the top 100 words to 20% in the top 300 [22] whereas in MSA the corresponding frequency levels are 26% and 45% according to one of the largest frequency lists [23]. These results highlight the need for further exploration into this aspect of vocabulary building. In particular there is a need to collect more wide ranging conversations to evaluate the differences in the type of words and multiwords required to successfully build Arabic AAC personalised and context sensitive vocabularies. There is also the need to be aware of the differences in lists used for enhancing reading skills where MSA is used rather than the colloquial dialects of the area. A further distinction may be needed between adult and children’s vocabularies where religious and social language requirements may impact on AAC use. The Speech and Language therapists attending meetings with the team also noted the importance of vocabularies sensitive to user’s characters, interests and social setting commenting on dress and gender issues as well as being aware of the issues of using lists from AAC users of school age due to the lack of available adult AAC users in the region at the time of writing. 4. Methodology for Symbol Management Just as it was found that there was a paucity of core AAC vocabulary lists in Arabic, the same could be said about the symbols provided for AAC devices. Some centres in Doha  93  were providing specifically designed symbols for the Arabic culture, environment, social and personalised linguistic needs but there were no adapted symbol sets that were freely available for sharing. Nor had any symbols been evaluated for transparency or cultural sensitivity by local AAC users, their supporting professionals and families. A bespoke Symbol Management system was developed that allowed the team to store symbols. The system also offered participants the chance to take an active role in the decisions made around the development and evaluation of appropriate symbols as they could see and vote on uploaded symbols representing the core vocabularies previously collected. The online database was based on a Model-View-Controller (MVC) framework using MongodB with JavaScript (NodeJS and an Express JS plugin). The code is open source and available on bitbucket. View templates which generated the html pages were built suing the Jade templating engine. The only other plugins used were for authentication and list filtering. The latter will provide the basis for browse and search features in the final Arabic Symbol Dictionary website. 4.1. Building symbol acceptance system As part of the online management system a simple voting set up was created using the filters developed for batches of symbols. During voting sessions participants have been presented with a series of around 60-65 images of newly designed symbols, the referent in MSA, Qatari (where applicable) and English. The voting criteria are presented with large selection areas on a scale of 1 to 5 where 5 is completely acceptable (see Figure 3) so that different visual displays can be used. The four criteria are listed with a free text box for comments: • Feelings about the symbol as a whole • Represents the word or phrase • Color contrast • Cultural sensitivity Figure 3 Voting system with criteria for acceptance on a scale of 1-5 where 5 is completely acceptable 4.2. Results from voting sessions The initial batch of symbols had 63 voters logging into the Symbol Manager resulting in 2341 votes for 65 symbols. Overwhelmingly the decisions were very favourable with all mean ratings significantly greater than a rating of 3.5. The average was 4.0. (See Table 1) All voting data was  anonymized and comments collated to inform the graphic designer. Two AAC users were also able to vote on the symbols via an adapted system using their own Sensory Software Grid 2 systems with the symbols added plus a 1-5 or 1-3 ‘thumbs up’ to ‘thumbs down’ scoring depending on their ability. This produced equally good results and comments were captured via recordings. More AAC users are being encouraged to join the forum and as further batches of symbols are developed it is hoped that voting sessions will continue to occur both during face to face meetings and remotely.  Table 1. One Sample T test for Difference of Mean Ratings from 3.5  Criteria  Number of voters  
Clinical applications of speech technology face two challenges. The ﬁrst is data sparsity. There is little data available to underpin techniques which are based on machine learning and, because it is difﬁcult to collect disordered speech corpora, the only way to address this problem is by pooling what is produced from systems which are already in use. The second is personalisation. This ﬁeld demands individual solutions, technology which adapts to its user rather than demanding that the user adapt to it. Here we introduce a project, CloudCAST, which addresses these two problems by making remote, adaptive technology available to professionals who work with speech: therapists, educators and clinicians. Index Terms: assistive technology, clinical applications of speech technology 1. Introduction to CloudCAST In this working paper, we introduce CloudCAST, a Leverhulme Trust International Network funded from January 2015 for 3 years. The network partners are The University of Shefﬁeld (United Kingdom), AIAS Onlus Bologna (Italy), The University of the West Indies (Jamaica), and the University of Toronto (Canada). In recent years, there has been signiﬁcant progress in Clinical Applications of Speech Technology (CAST) in diagnosis of speech disorders [1], tools to correct pronunciation and improve reading skills [2], recognition of disordered speech [3] and voice reconstruction by synthesis [4]. The aim of CloudCAST is to make progress in this domain and to provide a freely-available platform for worldwide collaboration. We aim to place CAST tools in the hands of professionals who deal with clients with speech and language difﬁculties, including therapists, pathologists, teachers, and assistive technology experts. We intend to do this by means of a free-of-charge (if possible), remotely-located, internet-based resource ‘in the cloud’ which will provide a set of software tools including personalised speech recognition, diagnosis and interactive spoken language learning. Following a user-centred design methodology, we will provide interfaces which will make these tools easy to use for professionals and their clients, who are not necessarily speech technology experts. There are various models for user-centred design [5], among which the ISO standard 9241-210 [6] is prominent. This standard for human-centred design processes includes six guiding principles (P): P1. understand the user, the task and environ-  mental requirements; P2. encourage early and active involvement of users; P3. be driven and reﬁned by user-centered evaluation; P4. include iteration of design solutions; P5. address the whole user experience; P6. encourage multi-disciplinary design. The CloudCAST resources will also facilitate speech data collection necessary to inform the machine learning techniques which underpin this technology: we will be able to automatically collect data from systems which are already in use, as well as provide a database scheme for collecting and hosting databases related to this domain. Our 3-year aim is to create a self-sustaining CloudCAST community to manage future development beyond our current funding period. While CloudCAST will build on previous work by its partners and others, we believe that it offers several ‘unique selling points’, including: • The resource will be available worldwide, and free of charge. • We will provide interfaces, resources and tools targeted at several kinds of users, including: – Developers, who want to embed CloudCAST technology into their own applications, for instance voice control of domestic robots, – Speech professionals, who want to use CloudCAST technology to work with their clients, for instance, to devise personalised therapy exercise programmes, – End users, for whom applications are developed, e.g., children learning to read, – Speech technologists, who are improving or adding to the CloudCAST technology itself. • The technology will be based on open source toolkits such as Kaldi for automatic speech recognition and OpenHab for smart homes [7, 8]. • Subject to ethical constraints, we will collect speech data and metadata from every CloudCAST interaction. All this material will therefore be available for re-training the technology, and for analysis. In this way, – we will be able to personalise the technology for each End User, – by pooling the data, we will address the problem that for abnormal speech the large datasets needed for speech technology development are not available, – we will be able to underpin and evaluate improvements in analysis and classiﬁcation of speech disorders.  97 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 97–102, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  2. Challenges for CloudCAST CloudCAST’s success requires meeting a number of technical, scientiﬁc and more general challenges: • The technology will run remotely, but in many applications it must deliver results rapidly, within a few seconds. • The technology should improve its performance as it is used, by adaptation to the data it is collecting. • It will not be possible to control the conditions under which the tools are used to the extent that one might like. For example, diverse recording devices and recording conditions may make normalization challenging. • There must be shared functionality of tools over applications. For instance, pronunciation tutors and reading tutors have much in common. • There must be interfaces, and guides to these interfaces, which are suitable for each user-group listed above. • There must be a scheme which protects the security and privacy of CloudCAST users and their data. • There is understandable resistance to technology from some speech professionals, based on bad experiences. • For this reason, and others, the technology must adapt to its user, rather than the other way round. • There must be a strategy for developing a self-sustaining CloudCAST community. Our intention is to commence with three exemplar applications: small vocabulary command-and-control with disordered speech, a literacy tutor and a computer aid for therapists. These are described after the next section, in which we introduce the common speech technology resource that will support them. 3. Speech technology resource Several toolkits exist which provide core speech recognition facilities on which applications can be built, notably Speechmatics [9], Google’s Web Speech API [10] and SoundHound [11]. Speechmatics provides a queue-based speech transcription service supporting multiple languages and audio formats, performs automatic punctuation, capitalisation and diarization (speaker separation) and supplies individual word timings and conﬁdences. It’s authors claim to achieve near real-time turnaround with very high accuracy. Google, through its proposed Web Speech API, provides both speech recognition and synthesis. The speech recognition service outputs the results in the form of multiple hypotheses of word-level transcriptions with associated conﬁdence scores. SoundHound provides a speech-tomeaning service that performs simultaneous speech recognition and natural language understanding. This process outputs its results in the form of structured commands instead of plain text transcription. For CloudCAST, these solutions fall short in terms of the types and details of the results they return, the ﬂexibility of the recognition process, provisions for customisation of the speech models, and modes of interaction. The maximum level of detail provided in all these solutions is an N -best list of wordlevel transcriptions with associated conﬁdences. In the case of Speechmatics, word-level time alignments are also available. However higher-level details such as phone time alignments are not accessible. Furthermore, other types of results such as decoding lattices and word confusion networks (WCN) [12] are not provided. The grammars (or language models) used in  these systems are ﬁxed to general-domain dictation applications (in Google’s Web Speech API, the introduction of a grammar speciﬁcation function was discussed in 2012, but to the best of our knowledge it has not been concretized or implemented in Chrome). While Googles service does provide an interactive mode in which partial results of the decoding process are immediately available, this is not the case with the service provided by Speechmatics. None of the services provide any means of creating custom models using speciﬁc training material. This precludes targeting disordered speech or other niche cases. The requirements of CloudCAST include providing an interactive speech recognition service where the client must be able to modify the grammar, the model, and other relevant parameters. The client should have instant feedback about the recognition process, such as partial decoding as well as access to fully detailed results such as phone-level alignments and posterior probabilities. Crucially, interactions of clients with CloudCAST should provide data resources to improve the recognition process and the training of future models. The main architecture of CloudCAST (Figure 1) can be split into the exemplars, the frontend, and the backend. The exemplars are services using CloudCAST, for instance, webapps that perform literacy tutoring or command-and-control (see next section). The frontend is the visible CloudCAST website, from which users can manage their recordings, developers can obtain API keys, professionals can create models, and so on. Finally, the backend is the server which consumes audio from the exemplars and provides speech recognition results. The backend is also in charge of applying the parameter changes that the exemplars may request to the recognition process. Both the frontend and the backend have access to a common storage space and database where they store models, recordings, and authentication details. The frontend and backend are both backed by worker processes, whose roles are to perform computationally intensive tasks, such as the training of the models and actual speech recognition, which may be run in separate devices. This split ensures the scalability of the system.  Exemplar Literacy Tutor Exemplar Command & Control  Frontend  Frontend Workers  DB  Storage  Backend  Backend Workers  Figure 1: Architecture of the CloudCAST resource. CloudCAST is developed with open source software. We have decided to create the frontend using Flask, a free software microframework for web development. To implement the speech recognition service (the backend) we have decided to build on kaldi-gstreamer-server de-  98  veloped by Tanel Alume [13]. Kaldi-gstreamer-server is a distributed online speech-to-text system that features real-time speech recognition and a full-duplex user experience where the partially transcribed utterance is provided to the client. The system includes a simple client-server communication protocol and scalability to many concurrent sessions. The system is open source and based on free software and therefore serves as a starting point for building CloudCAST, allowing us to deploy recognisers developed at Shefﬁeld within the CloudCAST framework [14]. It uses Kaldi [7] for speech recognition processing. Kaldi is a well-known free software library widely used in the research community partly due to its modular and ﬂexible architecture. To facilitate the creation of services using CloudCAST, we are also developing a speech recognition client in JavaScript based on the existing library dictate.js. The proposed client extends dictate.js with multiple types of interactions with the server, such as swapping grammars, models and other parameters, as well as interpreting the different results provided by the server. 4. Exemplars 4.1. Literacy tutor Among the ﬁrst set of exemplars to be developed will be an automated literacy tutor. In some respects the literacy tutor represents the most complex type of application that can be developed using the tools CloudCAST will make available. In addition to being a good showcase for the tools, the literacy tutor can be useful in bolstering current efforts to combat illiteracy as it will be a freely available, cloud-based resource that can be modiﬁed to meet the needs of individual users. 
The diagnosis and monitoring of Alzheimer’s Disease (AD), which is the most common form of dementia, has been the motivation for the development of several screening tests such as Mini-Mental State Examination (MMSE), AD Assessment Scale (ADAS-Cog), and others. This work aims to develop an automatic web-based tool that may help patients and therapists to perform screening tests. The tool was implemented by adapting an existing platform for aphasia treatment, known as Virtual Therapist for Aphasia Treatment (VITHEA). The tool includes the type of speech-related exercises one can ﬁnd in the most common screening tests, totalling over 180 stimuli, as well as the Animal Naming test. Its great ﬂexibility allows for the creation of different exercises of the same type (repetition, calculation, naming, orientation, evocation, ...). The tool was evaluated with both healthy subjects and others diagnosed with cognitive impairment, using a representative subset of exercises, with satisfactory results. 1. Introduction Alzheimer’s Disease (AD) is a neurodegenerative disease which represents 60 to 70% of the dementia cases in Portugal [1, 2, 3]. However, its ﬁrst signs can go unnoticed [1, 2, 4, 5]. Typically, AD is known to cause alterations of memory and of spacial and temporal orientation [3, 4, 6]. Furthermore, AD increases dramatically with age and it has no cure. Nevertheless, an early diagnosis may slow down its progression by enabling a more effective treatment [5]. For this purpose, several neuropsychological tests exist in the literature, each targeting different cognitive domains and capabilities. The Mini-Mental State Examination (MMSE) [7] and the AD Assessment Scale - Cognitive subscale (ADAS-Cog) [7] are two of the most popular tests used in Portugal for screening cognitive performance and tracking alterations of cognition over time. They involve the assessment of different capabilities, such as orientation to time and place, attention and calculus, language (naming, repetition, and comprehension), or immediate and delayed recall. Another type of test also commonly applied by therapists in the diagnosis of AD is the Verbal Fluency test [7]. In this test, the patient should produce as many words as he can beginning with a particular letter (phonemic ﬂuency test) or belonging to a particular category, e.g. fruits (semantic ﬂuency test), during 60s. This test is used both in assessing the verbal initiative ability and executive function such as the inhibition ability, the difﬁculty in switching among tasks, and the perseverance attitude [7]. Typically, the most commonly used versions for the Portuguese population consider the letter “P” for the phonemic version and the “Animal” category for the semantic category. Other tests not so frequently adopted for this population are the Wechsler Adult  Intelligent Scale - III (WAIS-III) [7], which provides a measure of general intellectual function in older adolescent and adults, and the Stroop test [7], which is a measure of cognitive control, evaluating how easily a person can maintain a goal in mind while suppressing habitual responses. Most of these tests include a verbal component provided in response to a visual or spoken stimulus solicited by a therapist. Thus, due to their nature, and the need to continuously monitor the cognitive decline over time, these tests lend themselves naturally to be automated through speech and languages technologies (SLT). A tool including the digitized version of these tests with the possibility of an immediate evaluation through automatic speech recognition could be of valuable support in health care centres. The therapist will have access to an organized archive of tests which could be administered in the traditional way, or remotely, when the physical dislocation of the subject is hampered by logistic constraints or physical disabilities. Recordings and evaluations will be stored and made available for later consultation. On the other hand, research has shown that cognitive skills, which can fade without stimulation as we age, can be improved by playing games that stimulate brain activity [8]. An automated tool for the monitoring of AD could be easily extended to support exercises and brain games for the daily training of cognitive capabilities such as short-time memory, attention, calculus, reasoning ability and many others. Up to our knowledge, there are few works in the literature that exploit SLT to automate certain types of neuropsychological tests. Some of the most relevant are the kiosk system designed to use at home as a prevention instrument for early detection of AD described in [5], the end-to-end system for automatically scoring the Logic Memory test of the WAIS-III presented in [9], and the system that implements a modiﬁed version of the MMSE based on the IBM ViaVoice recognition engine of [10]. These works show the recent increasing interest on this area, but also the long road ahead to support the large variety of existing neuropsychological tests (e.g., some of them are not fully automated). This work makes a step towards ﬁlling this gap by introducing a set of neuropsychological tests for AD intended for the Portuguese population, which were integrated into an automatic web-based system [11]. The system presented in this work extends an on-line platform named VITHEA [12] used for aphasia treatment that incorporates SLT to provide word naming exercises. For this to be possible, the system resorts to a keyword spotting technique which consists of detecting a certain set of words by using a competing background model with the keywords model [13]. This platform is used daily by patients and speech therapists and has received several awards from both the speech and the health-care communities. The success of this platform and its ﬂexibility, that allows to create different ex-  103 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 103–109, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  ercises, have motivated its use as a foundation for this work. Our ﬁrst step was the automation of the exercises in MMSE and ADAS-COG that involve speech. The second step was the implementation of the semantic ﬂuency test, starting with the Animal category, also known as Animal Naming test. As explained in the next sections, the automation of such tests have raised several technological challenges, both for the automatic speech recognition and text-to-speech synthesis technologies. In the following, Section 2 brieﬂy presents the VITHEA platform that was used as a foundation for this work, while Section 3 describes the extended system resulting from the implementation of the selected neuropsychological tests into the VITHEA platform. Section 4 reports how each type of test was concretely implemented. Then, in Section 5 the focus is on the experiments, both detailing the automatic speech recognition module, the speech corpus used for evaluation and the experimental results. Finally, Section 6 presents the conclusions and future work. 2. The VITHEA platform VITHEA (Virtual Therapist for Aphasia Treatment) is a webbased platform developed with the collaboration of the Spoken Language Processing Lab of INESC-ID (L2F) and the Language Research Laboratory of the Lisbon Faculty of Medicine (LEL). The system aims at acting as a ”virtual therapist”, allowing the remote rehabilitation from a particular language disorder, aphasia. For this to be possible, the platform comprises two speciﬁc modules, dedicated respectively to the patients, for carrying out the therapy sessions, and to the clinicians, for the administration of the functionalities related to them (e.g., manage patient data, manage exercises, and monitor user performance). In this way, speech therapy exercises created by speech therapists through the clinician module, can be later accessed by aphasia patients through the patient module with a webbrowser. During the training sessions, the role of the speech therapist is taken by a ”virtual therapist” that presents the exercises and is able to validate the patients answers. The overall ﬂow of the system can be described as follows: when a therapy session starts, the virtual therapist shows to the patient, one at a time, a series of exercises. These may include either the presentation of images, the reproduction of short videos or audios, and textual information. The patient is then required to respond verbally by naming the contents of the object or action that is represented. The utterance produced is recorded, encoded and sent via network to the server side. Here, a web application server receives the audio ﬁle which is processed by the ASR system, generating a textual representation of the patient’s answer. This result is then compared with a set of predetermined textual answers (for the given question) in order to verify the correctness of the patient’s input. Finally, feedback is sent back to the patient with the correctness of the answer provided. Figure 1 illustrates the use of the VITHEA platform. 3. Extending VITHEA for neuropsychological screening Extending VITHEA for including neuropsychological tests involved important alterations in the original platform, both on the patient and the clinician modules. However, the ﬂexibility of VITHEA allows for the easy addition of new categories of exercises. These can then be combined in multiple ways by the clinician to form new tests, and to create different exercises of  Figure 1: A caption of the VITHEA platform during the presentation of an exercise. the same type. According to the original system, in order to answer the question presented by the virtual therapist, the patient needs to manually interact with the system to start and stop the recording of his answer, and to advance among different stimuli. The usability of this interface has been adapted to meet the needs of an ageing population, with cognitive impairments. In particular, we considered important to implement the following updates: • To simplify the interaction with the tool and make the evaluation process more ﬂuid, we minimized the use of the mouse. The interface now automates part of the recording process and the progression between stimuli. An action from the patient is only required to stop the recording process. • Since cognitive impairments and ageing often results in a limited auditory capability, the speech rate of the therapist has been tuned until ﬁnding the best compromise between a more understandable but still natural voice. Also, the feedback from the neurologists involved in this work provided us important guidelines regarding the presentation of the tests. Following their advices, we introduced the alterations listed below: • To make the interaction with the system more natural the virtual therapist now provides a random feedback to the patient when the evaluation switches among different classes of stimuli. • Optional instructions have been added for the more complex questions. • For some stimuli, the virtual therapist now provides a semantic hint if the patient has not provided an answer after a given amount of time. The platform now allows also to store some additional personal information of the proﬁle of the patient that are needed for the assessment of some sub-tests (i.e. place of birth, age, etc.), and the result of the assessment in terms of test score obtained. During the application of a neuropsychological test, the scores are individually calculated for each question. After the answer has been processed by the automatic speech recognition (ASR) system, the platform computes both the maximum score allowed for the current question, and the actual score obtained by the patient. These results are stored in the database. At the end of the test, both the maximum scores and the obtained scores for each question are summed separately to obtain  104  a global score in the form score/maxScore (e.g., a score of 18/22). This result can then be consulted by the patient. In order to follow the patient’s progress, each time an evaluation test is performed, the platform sends an e-mail with a summary of the patient’s performance to the therapist assigned to him/her. Overall, these alterations contributed to building a simpliﬁed interface, suited for aged people, especially if cognitively impaired. 4. Automated tests Since the selected neuropsychological tests comprise common or similar questions, we may approach its concrete implementation organized by type of question and the underlying technology with which they were implemented, rather than per test. Each type of question has set different challenges, each of which has been addressed individually with ad-hoc solutions. Overall, a total of 185 stimuli belonging to different types of tests have been selected for their implementation in the platform. 4.1. Naming objects and ﬁngers This type of stimuli belongs both to the MMSE and the ADAScog tests and evaluates a person’s naming ability. Similarly to the exercises used in aphasia treatment, it consists of naming a series of objects that are shown in pictures, one at a time. These stimuli were implemented following a keyword spotting approach. A maximum score of 1 is given for each correct answer. The major innovation relative to the VITHEA exercises was the introduction of an optional semantic cue for some of the questions. This was implemented by adding a timer in the component responsible for the answer’s recording and by making the virtual therapist to speak the cue after 20 seconds if no answer is detected. For this to be possible, both the clinician module and the internal structure of the database had to be extended for managing and storing the additional information. In fact, since the recording process is started from the beginning, the semantic cue is also recorded together with the patient’s answer. Consequently, the logic of the patient module had also to be updated in order to remove the semantic cue spoken by the virtual therapist. 4.2. Repetition The repetition question is part of the MMSE test and consists of repeating the following sentence: ”O rato roeu a rolha“ (the mouse gnawed the stopper). This question could be easily implemented with a keyword/key-phrase spotting approach, just like the ones for aphasia treatment. The maximum score is 1, which corresponds to a sentence correctly repeated. 4.3. Attention and calculation This type of question belongs to the MMSE test, the idea is to successively subtract 3 beginning on 30 until 5 answers are given. In our ﬁrst approach, we created a set of 5 different stimuli, each one asking separately for a speciﬁc calculation. These questions were also implemented with a keyword spotting approach. A score of 1 is given for each stimulus that corresponds to a correct answer, for a maximum score of 5. 4.4. Orientation to time, place and person These type of stimuli are part both of the MMSE and the ADAScog, though some questions differ. They comprise stimuli in-  tended to evaluate a person’s orientation ability, asking the patient to report the current year, day, month, his name, the country and the town he lives in, among others. These are dynamic questions in the sense that there is not a universal answer to each question as it changes depending on the time, place and person. The solution was to provide several pre-compiled language models that were carefully structured so that, at any time, the platform knows which is the right model to chose. For the questions of orientation to person, the necessary information is acquired at the time of the creation of the user proﬁle and then it is used to automatically generate the corresponding language models. The majority of these questions were implemented based on a standard keyword spotting approach. However, for the day and hour, it was necessary to create dedicated rule-based grammars. A correct answer is always scored with 1 point, while an incorrect answer scores 0. 4.5. Word recognition The word recognition stimuli belong to the ADAS-Cog test and consist of presenting the patient a list with 12 words to learn, one at a time. Words are written in block letters on white cards. The learning process is made by asking the patient to read each word aloud and try to remember it. Then, a new list with 24 words is shown in the same way. This new list contains the 12 original words of the learning list, plus 12 new distracting words that are carefully chosen in terms of phonetic similarity and semantic meaning. For each word, the patient is then asked to indicate whether it was on the learning list or not. This whole process is repeated in 3 trials. Just like the day and hour questions, rule-based hand crafted recognition grammars for positive, negative or neutral answers were built. For the word recognition task itself, each presented word is individually scored. Specifically, a correct answer corresponds to a maximum score of 1, which yields a total score of 72 for the word recognition sub-test (i.e., 24 for each trial). 4.6. Evocation Generally speaking, an evocation question consists of recalling a series of words, whether they have been previously learned or if they are subject to compliance with certain requirements. In either cases, the spoken answers produced for this kind of stimuli are commonly followed by ﬁlled pauses, i.e. hesitation sounds. For this reason, we adopted a keyword spotting approach that incorporates an ad-hoc model to deal with ﬁlled pauses. In terms of score, the calculation is processed by considering the number of keywords that are correctly produced, without repetitions. For MMSE, the evocation question consists of the immediate and delayed recall of 3 words. This was implemented with an auditory stimuli for the immediate recall task and with a textual stimuli for the delayed recall task. The presentation of the evocation question that belongs to the ADAS-cog is very similar to the word recognition question. Basically, it consists of the immediate recall of a list with 10 words that were previously learned, the whole process is repeated in 3 trials. 4.7. Verbal Fluency The animal naming question, which belongs to the Verbal Fluency test, is the most challenging among the evocation tests. This is explained by the fact that, contrarily to the other cases, which are based on a limited domain vocabulary tasks, this question comprises a more extended domain composed of the name of all known species of animals. Theoretically, the lan-  105  guage model should cover all the known species of animals, however, in practice this is infeasible. Moreover, the size of the language model directly impacts the output of the ASR system. In fact, while a shorter list will cause the missing keywords to never be recognized, a longer list will increase instead the perplexity of the task. The automatic creation of an ad-hoc language model for this type of question is detailed in [14], and is brieﬂy reported here. The starting point consisted of an existing list of animal names [15] that included 6044 animal names, grouped, classiﬁed, and labelled with its semantic category, without inﬂected forms. Since some names are more likely and common than others, the initial list was used to build a probabilistic language model that exploited this information. The likelihood of each term was computed considering the total number of results that is returned by querying a web search engine. The retrieval strategy had to be reﬁned several times in order to ﬁnd the optimal approach, in fact initial queries have led to incorrect counts due to homonyms of some terms. The ﬁnal approach consisted in using the animal name and the semantic category associated. Finally, the likelihood associated with each term also allows to sort the list numerically and thus to reduce its size by ﬁltering out less popular terms. After several experiments, the language model that achieved the best results contained the 802 most popular animal names. 5. Experimental set-up 5.1. ASR/KWS system The monitoring tool integrates the in-house ASR engine named AUDIMUS [16, 17], a hybrid recognizer that follows the connectionist approach [18]. The baseline system combines three MLP-based acoustic models trained with Perceptual Linear Prediction features (PLP, 13 static + ﬁrst derivative), log-RelAtive SpecTrAl features (RASTA, 13 static + ﬁrst derivative) and Modulation SpectroGram features (MSG, 28 static). These model networks were trained with 57 hours of downsampled Broadcast News data and 58 hours of mixed ﬁxed-telephone and mobile-telephone data in European Portuguese [19]. The number of context input frames is 13 for the PLP and RASTA networks and 15 for the MSG network. Neural networks are composed by two hidden layers of 1500 units each one. Monophone units are modelled, which results in MLP networks of 39 soft-max outputs (38 phonemes + 1 silence) [12]. In order to support a keyword spotting approach, the baseline ASR system was modiﬁed to incorporate a competing background speech model that is estimated without the need for acoustic model re-training. In fact, while keyword models are described by their sequence of phonetic units provided by an automatic grapheme-to-phoneme module, the problem of background speech modelling must be speciﬁcally addressed. Here, the posterior probability of a background speech unit is estimated as the mean probability of the top-6 most likely outputs of the phonetic network at each time frame. In this way, there is no need for acoustic network re-training. 5.2. Portuguese cognitive impaired speech corpus To evaluate the feasibility of the monitoring tool, we collected an ad-hoc speech corpus. This includes recordings of 5 people diagnosed with cognitive impairments and 5 healthy control subjects. All the participants are Portuguese native speakers. Recordings took place in different environments with different acoustic conditions. In fact, healthy subjects were  recorded in a quiet, domestic environment, while patients were recorded at CHPL, the Psychiatric Hospital of Lisbon. No particular constraints were imposed over background noise conditions. Each session consisted approximately of a 20 to 30minutes recording. The data was originally captured with the platform at 16 kHz, and later down-sampled to 8 kHz to match the acoustic models sampling frequency. The collection of the patients data, besides being emotionally demanding, it is a valuable resource which implied logistic difﬁculties.  Table 1: Speech corpus data, including gender, age, education and diagnosis. B.E.: Basic Education, S.E.: Secondary Education, MCI: Mild Cognitive Impairment, AD: Alzheimer Disease, PTD: Post-traumatic Dementia  User Gender Age Education Diagnosis  
We describe how a Dutch Text-to-Pictograph translation system, designed to augment written text for people with Intellectual or Developmental Disabilities (IDD), was adapted in order to be usable for English and Spanish. The original system has a language-independent design. As far as the textual part is concerned, it is adaptable to all natural languages for which interlingual WordNet [1] links, lemmatizers and part-of-speech taggers are available. As far as the pictographic part is concerned, it can be modiﬁed for various pictographic languages. The evaluations show that our results are in line with the performance of the original Dutch system. Text-to-Pictograph translation has a wide application potential in the domain of Augmentative and Alternative Communication (AAC). The system will be released as an open source product. Index Terms: Augmentative and Alternative Communication, Pictographic Languages, Text-to-Pictograph Translation 1. Introduction In our daily lives, we are constantly confronted with pictographs. Think of trafﬁc signs, signs in buildings that direct visitors to the elevators, the meeting rooms, the toilets, and the emergency exits, or signs for telling people that dogs need to be kept on a leash (see Figure 1). Figure 1: Pictographs in our daily lives. Similar pictographs are used as a form of Augmentative and Alternative Communication (AAC). AAC assists people with severe communication disabilities to be more socially active in interpersonal interaction, learning, education, community activities, employment, volunteering, and care management. Schools, institutions, and sheltered workshops use speciﬁc pictographs that are related to everyday activities and objects to allow accessible written communication between children or adults with Intellectual or Developmental Disabilities (IDD) and their caregivers, in an ofﬂine setting. It is undeniable that current technological advances inﬂuence our lives in various aspects. Not being able to access or use information technology is a major form of exclusion. In order to reduce social isolation, there is an acute need for digital picture-based communication interfaces that enable contact for people with IDD. Adding pictographs to text can provide help in reading and understanding the text. It is estimated that between  two and ﬁve million people in the European Union could beneﬁt from symbols or symbol-related text as a means of written communication [2]. The Dutch Text-to-Pictograph translation system that is described in Vandeghinste et al. [3] is used in the WAI-NOT1 communication platform. WAI-NOT is a Flemish, non-proﬁt organization that gives people with severe communication disabilities the opportunity to familiarize themselves with computers, the internet, and social media. The website makes use of an email client that automatically augments written text with a series of Beta2 or Sclera3 pictographs. WAI-NOT’s ﬁrst translation system would rely on a simple one-on-one match between the input words and the pictograph ﬁle names, usually leading to erroneous translations and leaving many words untranslated. Vandeghinste et al. [3] improved this engine by introducing linguistic analysis. Their Text-to-Pictograph translation system was made as language-independent as possible. Within the framework of Able to Include,4 which aims to improve the living conditions of people with IDD, we built English and Spanish versions of this system. English and Spanish being a Germanic and a Romance language, respectively, we show that the engine manages to generalize well over different European language families. After a discussion of related work (section 2), we introduce the Beta and Sclera pictograph sets (section 3), followed by an explanation of how existing links between WordNets can be used to automatically connect pictographs to words in source languages other than Dutch (section 4). In the remainder of this paper, we describe the system’s general architecture (section 5). The evaluations (section 6) show that our results are in line with the performance of the Dutch system. Section 7 shows that the Text-to-Pictograph system has a wide application potential in the domain of AAC. Finally, we describe our conclusions and future work (section 8). 2. Related work Pictographic communication has grown from local initiatives, some of which have scaled up to larger communities. Across Europe, many pictograph sets are in place, such as Blissymbolics,5 PCS,6 Pictogram,7, ARASAAC,8 Widgit,9 Beta, and Sclera. 1http://www.wai-not.be/ 2https://www.betasymbols.com/ 3http://www.sclera.be/ 4http://abletoinclude.eu 5http://blissymbolics.org/ 6http://www.mayer-johnson.com/category/symbols-and-photos 7http://www.pictogram.se/ 8http://www.catedu.es/arasaac/ 9https://widgit.com/  110 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 110–117, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  Many of the problems that written languages encounter can be overcome by the use of pictographic languages. For instance, they can be understood across language barriers10 [4] and there is less ambiguity involved. Pictographic communication systems for remote, online communication include Messenger Visual, an instant messaging service [5], Communicator [6], Pictograph Chat Communicator III [7], and VIL, a Visual Inter Lingua [4]. Mihalcea and Leong [8] argue that the understanding of graphical sentences is similar to that of target language texts obtained by means of machine translation. Leemans [4] shows that an appropriately designed iconic language, built according to a set of ﬁxed principles, leads to no difference in the recognition rate of icons for people of western and non-western culture, yielding an average rate of about 79%. None of these abovementioned authors, however, consider users with IDD when designing the system. Other pictograph-based communication systems are specifically designed for people with IDD. Patel et al. [9] introduce Image-Oriented Communication Aid, an interface using the Widgit symbol set, allowing users to build picture-supported messages on a touch screen computer. Motocos [10] are image exchange devices that are designed for children with autism, including audio cues for easier understanding of the image cards. The mobile application PhotoTalk [11] aids people with aphasia by providing a digital photograph management system in support of verbal communication. Nevertheless, all these systems require face-to-face communication in an ofﬂine setting. The use of online information technology systems as a way to enhance the quality of life of people with IDD is a recent development. For accessible, remote communication, Keskinen et al. [2] introduce SymbolChat, a platform for picture-based instant messaging, where the interaction is based on touch screen input and speech output. The Text-to-Pictograph conversion system described in Vandeghinste et al. [3] applies shallow linguistic analysis to Dutch input text and automatically generates sequences of Beta and Sclera pictographs, allowing people with IDD to read messages independently. Only few other publications related to the task of translating texts for pictographsupported communication can be found in the literature, such as Goldberg et al. [12] and Mihalcea and Leong [8], but these systems do not translate the whole sentence or are not focused on IDD. 3. Pictographic languages Mihalcea and Leong [8] note that complex and abstract concepts (such as democracy) are not always easy to depict. Some characteristics of natural languages may not be present in the pictographic languages.11 Usually, no distinction between singular and plural is made. Tense, aspect, and inﬂection information is removed, and so are the auxiliaries and the articles.12 Pictographic languages are simpliﬁed languages, that are often speciﬁcally designed for people with IDD. Although experiments with the Pictogram set [13] have revealed that many pictographs are difﬁcult and wrongly interpreted, a correct interpretation is easily accepted and remembered without any problem. By giving people with speech and language disorders the opportunity to familiarize themselves 10Although cultural differences remain. 11We use the term pictographic language in order to refer to the combination of individual pictographs, that belong to a speciﬁc pictograph set, into a larger meaningful structure. 12There are some exceptions. Beta, for instance, contains the Dutch articles.  with the pictographs, they learn to interpret the symbols more easily. However, a deliberate effort is needed. The Text-to-Pictograph translation system currently gives access to two pictograph sets, Sclera and Beta (see Figure 2). Sclera pictographs13 are mainly black-and-white pictographs, although colour is sometimes used to indicate permission (green) or prohibition (red). Over 13,000 pictographs are available and more are added upon user request. Sclera pictographs often represent complex concepts, such as a verb and its object (such as to feed the dog) or compound words (such as carrot soup). There are hardly any pictographs for adverbs or prepositions. The Beta set14 consists of more than 3,000 coloured pictographs. Easy recognition being one of the main objectives, Beta is characterized by its overall consistency and the use of different types of arrows and dashes (pointing to an object, indicating changes in space or time or depicting actions). Beta hardly contains any complex pictographs. Most of the pictographs represent simplex concepts. Figure 2: Example of a sentence being translated into Sclera and Beta pictographs. Tense information is removed. The Sclera translation contains a complex pictograph, namely carrot soup. 4. Linking pictographs to other WordNets WordNets, lexical-semantic databases, are an essential component of the Text-to-Pictograph translation system. For the original Dutch system, Cornetto [14, 15] was used. Its English and Spanish counterparts are Princeton WordNet 3.0 [1]15 and the Spanish Multilingual Central Repository (MCR) 3.0 [16].16 WordNets contain synsets (groupings of synonyms that have an abstract, usually numeric identiﬁer, see Figure 3) and are designed in such way that each synset is connected to one or more lemmas. Vandeghinste and Schuurman [17] manually linked 5710 Sclera pictographs and 2760 Beta pictographs to Dutch synsets in Cornetto.17 An essential step in building Text-to-Pictograph translation systems for other languages is making sure that the pictographs are connected to (sets of) words in those languages. 13Freely available under Creative Commons License 2.0. 14The coloured pictographs can be obtained at reasonable prices, while their black-and-white equivalents are available for free. 15http://wordnet.princeton.edu/ 16http://adimen.si.ehu.es/web/MCR/ 17As a Cornetto license can no longer be obtained, the authors will transfer these links to the Open Source Dutch WordNet (http://wordpress.let.vupr.nl/odwn/).  111  Figure 3: An example of a lemma, rock, having different meanings and belonging to different synsets. Two synsets are shown here. Manually linking thousands of pictographs all over again would be a very time-consuming procedure. Instead, by transferring the connections automatically (see Figure 4), this process can be sped up drastically. Sevens et al. [18] note that connections between WordNets are an important resource in knowledge-based multilingual language processing. The already mentioned Cornetto database for Dutch, used to build the Dutch Text-to-Pictograph translation system, contains connections to the English Princeton WordNet. We describe how we automatically connected Beta and Sclera pictographs to synsets in Princeton WordNet 3.0 in section 4.1. Many WordNets nowadays contain high-quality links between the source language’s synsets and Princeton WordNet 3.0, which is often viewed as the central WordNet. Princeton WordNet 3.0 now also plays this central role in our Textto-Pictograph translation system. Having obtained the links between Beta and Sclera pictographs and Princeton WordNet 3.0, it becomes possible to automatically assign pictographs to synsets in any WordNet that has decent connections with Princeton WordNet,18 allowing us to quickly build Text-toPictograph translation systems for many other languages. For example, with the English pictograph connections in place, a mapping between the pictographs and Spanish synsets in MCR 3.0 became possible. This process is described in section 4.2. Figure 4: Making Princeton WordNet 3.0 the central WordNet of the Text-to-Pictograph translation system and transferring the links to the MCR 3.0 for Spanish. 4.1. Connecting pictographs to Princeton WordNet 3.0 Cornetto’s equivalence relations establish connections between Dutch and English synsets in Princeton WordNet. These relations have originally been established semi-automatically by 18A full list can be found on http://globalwordnet.org/wordnets-inthe-world/  Vossen et al. [19], ﬁlling the database with more than 80000 links between Dutch and English synsets. Sevens et al. [18] showed that a considerable amount of the original links were highly erroneous, making them not yet very reliable for multilingual processing. By using these equivalence relations, we would risk assigning pictographs to unrelated synsets in Princeton WordNet 3.0. In the case of a Dutch synset being wrongly connected to an English synset, writing a message in English would allow the system to generate pictographs that depict another concept. Therefore, we used the ﬁltered,19 more reliable connections that were established by Sevens et al. [18]. As a result, it became possible to automatically assign a large amount of Sclera and Beta pictographs to English synsets in Princeton WordNet 3.0. However, 154 (5.58%) Beta pictographs and 288 (5.04%) Sclera pictographs still had to be connected manually, either because the original equivalence relation was rejected by the ﬁltering algorithm, or because the Dutch compound word corresponded to multiple words in English and forced us to treat the pictograph as a complex pictograph20 in English (such as the Dutch word vanillesuiker, meaning vanilla sugar in English). In some rare cases, no equivalent English concept existed in the WordNet for an existing Dutch concept (for instance, the ﬁctional character Zwarte Piet or typical kinds of food such as choco, which can roughly be translated as chocolate spread). 4.2. Connecting pictographs to the Spanish MCR 3.0 The MCR 3.0 integrates in the same EuroWordNet framework WordNets from ﬁve different languages, namely English, Catalan, Spanish, Basque, and Galician. Words in one language are connected to words in any of the other languages through InterLingual-Indexes. Sevens et al. [18] showed that the links between English and Spanish synsets were correctly established, making it possible for us to create highly reliable connections between Beta and Sclera pictographs and Spanish synsets. This exact same process can be done for any language’s WordNet that establishes reliable links to Princeton WordNet 3.0. 5. The Text-to-Pictograph translation system for English and Spanish In this section, we describe how a textual message is converted into a sequence of Sclera or Beta pictographs [3] (see Figure 5), with an application to English and Spanish. The source text ﬁrst undergoes shallow linguistic analysis (section 5.1). For further processing, two routes can be taken. The semantic route is only applied to content words (nouns, verbs, adjectives, adverbs) that are present in the WordNets. It consists of linking the source text to synsets in the databases (section 5.2) and retrieving the pictographs that are connected to these synsets (section 5.3). The direct route (section 5.4), which runs in parallel with the semantic route, contains speciﬁc rules for appropriately dealing with pronouns, and it uses a dictionary for parts-of-speech that are not present in the WordNets. The system contains a handful of parameters (section 5.5), which were tuned beforehand (section 5.6). Finally, as explained in section 5.7, an optimal sequence of pictographs is selected. 19Filtering was done by using large bilingual dictionaries. 20A pictograph that is connected to multiple synsets instead of just one synset. For example, the pictograph depicting vanilla sugar is connected to both the synset that contains the lemma vanilla and the synset that contains the lemma sugar.  112  Figure 5: Architecture of the translation engine. 5.1. Shallow linguistic analysis The source text undergoes shallow linguistic processing, consisting of several sub-processes (see Figure 6). This process is analogous to the linguistic processing step in the original Dutch tool.  Tagger [21].27 TreeTagger is available for a large variety of European languages. The Text-to-Pictograph translation system works on the sentence level. Although most messages sent by the users only contain one sentence, sentence detection is applied. Segmentation is based on full stops, which will eventually correspond to line breaks in the resulting pictographic representation. The next step is lemmatization, which requires a languagespeciﬁc treatment. For English, we built a lemmatizer based on a list of English token/part-of-speech combinations and their lemma.28 As mentioned before, for Spanish, part-of-speech tagging and lemmatization are done with TreeTagger. One additional adaptation concerns the treatment of the Spanish pro-drop phenomenon (which occurs in all Romance languages, with the exception of French), meaning that personal pronouns in subject position are usually omitted (unless emphasis is given). Translating such a message into pictographs would leave us with no subject, as the pictographic representations of words are based on the lemma form and do not retain any grammatical information. However, person information can be inferred from the verb in the source sentence. We wrote a set of rules that explicitly adds the personal pronouns in the message before converting it into a series of pictographs.29 When a matching personal pronoun is already found within a window of three words (since adverbs or pronouns can appear between the subject and the verb), these rules are not applied (see Figure 7).  Figure 6: An example of shallow linguistic processing.  First, tokenization is applied to split the punctuation signs from the words, with the exception of the hyphen/dash and the apostrophe, as they often belong to the word. As the targeted users have different levels of illiterateness, basic spelling correction (one deletion, one insertion, one substitution)21 aids in ﬁnding the correct variant of words that do not appear in the lexicon22 and the list of ﬁrst names.23 Next, part-of-speech tagging is applied. For English, we used HunPos [20], an open source tagger, using the English training data (with Penn Treebank tags24) made available on its website.25 For Spanish, part-of-speech tagging (with TreeTagger tags26) and lemmatization are done in one step with Tree- 21We are currently designing a spelling corrector that is speciﬁcally tailored towards Dutch text written by people with IDD. Our approach does not rely on the use of parallel corpora (erroneous text - corrected text). Therefore, it can also prove to be useful for spelling correction in other languages. 22http://www.anc.org/SecondRelease/frequency2.html (for English) and http://corpus.leeds.ac.uk/frqc/internet-es-forms.num (for Spanish) 23http://www.quietafﬁliate.com/free-ﬁrst-name-and-last-namedatabases-csv-and-sql (for English and Spanish) 
This paper presents a speech synthesis method for people with articulation disorders resulting from athetoid cerebral palsy. For people with articulation disorders, there are duration, pitch and spectral problems that cause their speech to be less intelligible and make communication difﬁcult. In order to deal with these problems, this paper describes a Hidden Markov Model (HMM)-based text-to-speech synthesis approach that preserves the voice individuality of those with articulation disorders and aids them in their communication. For the unstable pitch problem, we use the F0 patterns of a physically unimpaired person, with the average F0 being converted to the target F0 in advance. Because the spectrum of people with articulation disorders is often unstable and unclear, we modify generated spectral parameters from the HMM synthesis system by using a physically unimpaired person’s spectral model while preserving the individuality of the person with an articulation disorder. Through experimental evaluations, we have conﬁrmed that the proposed method successfully synthesizes intelligible speech while maintaining the target speaker’s individuality. Index Terms: Articulation disorders, Speech synthesis system, Hidden Markov Model, Assistive Technologies 1. Introduction In this study, we focus on a person with an articulation disorder resulting from the athetoid type of cerebral palsy. About two babies in 1,000 are born with cerebral palsy [1]. Cerebral palsy results from damage to the central nervous system, and the damage causes movement disorders. It is classiﬁed into the following types: 1) spastic, 2) athetoid, 3) ataxic, 4) atonic, 5) rigid, and a mixture of these types [2]. Athetoid symptoms develop in about 10-15% of cerebral palsy sufferers [1]. In the case of persons with articulation disorders resulting from the athetoid type of cerebral palsy, his/her movements are sometimes more unstable than usual. That means their utterances (especially their consonants) are often unstable or unclear due to their athetoid symptoms, and there is a great need for voice systems that can assist them in their communication. An HMM-based speech synthesis system [3] is a text-tospeech (TTS) system that can generate signals from input text data. A TTS system may be useful for those with articulation disorders because they have difﬁculty moving their lips. In an HMM-based speech synthesis system, the spectrum, F0 and duration are modeled simultaneously in a uniﬁed framework. Melcepstral coefﬁcients are used as spectral features, which are modeled by continuous density HMMs. F0 patterns are modeled by a hidden Markov model based on multi-space probabil-  Figure 1: HMM-based sound synthesis system ity distribution (MSD-HMM [4]), and state duration densities are modeled by single Gaussian distributions [5]. In the ﬁeld of assistive technology, Veaux et al. [6] used HMM-based speech synthesis to reconstruct the voice of individuals with degenerative speech disorders resulting from Amyotrophic Lateral Sclerosis (ALS). They have proposed a reconstruction method for degenerative speech disorders using an HMM sound synthesis system. In this method, the subject’s utterances are used to adapt an average voice model pre-trained on many speakers. Creer et al. [7] also adapt the average voice model of multiple speakers to the severe dysarthria data. And Khan et al. [8] uses such adaption method to the laryngectomy patient’s data. Yamagishi et al. [9] proposed a project called “Voice Banking and Reconstruction”. In that project, various types of voices were collected, and they proposed TTS for ALS using that database. Also, Rudzicz [10] proposed a speech adjustment method for people with articulation disorders based on observations from the database. In this paper, we propose an HMM-based speech synthesis method for articulation disorders because there are several problems in the recorded voice of persons with articulation disorders, and this causes the output synthesized signals to be unintelligible. To deal with these problems, it is necessary to develop a speech synthesis system in which the output signals become more intelligible and include the subject’s individuality. To generate an intelligible voice while preserving the speaker’s individuality, we train the speech synthesis system using training data from both a person with an articulation disorder and a physically unimpaired person. Because the utterance rate of persons with articulation disorders differs from that of a  118 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 118–123, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  (a) a physically unimpaired person  Figure 2: Diagram of HMM-based sound synthesis method for articulation disorders  physically unimpaired person, we utilize the duration model of a physically unimpaired person only in our method. In addition to the utterance rate problem, the F0 patterns of persons with articulation disorders are often unstable compared to those of physically unimpaired persons. In our method, the F0 model is trained from a physically unimpaired person’s F0 patterns, and the average F0 is used as the F0 pattern for the person with an articulation disorder. As for the spectral problem associated with persons with articulation disorders, the consonant parts of their speech are often unstable or unclear, which causes their voice to be unintelligible. To resolve this consonant problem, we conduct different operations on the consonant and vowel parts. For the consonants parts, we basically generate the output spectrum from the spectral model of a physically unimpaired person. For the vowel parts, we generate the output spectrum from the spectral model of a person with an articulation disorder in order to preserve the person’s individuality.  2. HMM-based sound synthesis  2.1. Basic approach  Fig. 1 shows the overview of the basic approach to text-tospeech synthesis (TTS) based on HMMs. This ﬁgure shows the training and synthesis parts of the HMM-based TTS system. In the training part, parameters (spectral, F0, and aperiodicity) are extracted as feature vectors. These features are modeled by context-dependent HMMs. Also, by installing the duration model, it is able to model each parameter, as well as the duration in the uniﬁed framework. In the synthesis part, a context-dependent label sequence is obtained from an input text by text analysis. A sentence HMM is constructed by concatenating context-dependent HMMs according to the context-dependent label sequence. Then, HMM state sequences q = [q1, · · · , qT ] are decided from the duration model as follows:  qˆ = arg max P (q|λ)  (1)  q  (b) a person with an articulation disorder Figure 3: Examples of spectrogram uttered for // g e N j i ts u o  where T , qt, and λ represent the number of frames, index of the HMM-state of the t-th input frame, and the parameter sets of HMM, respectively. The explicit constraint between static and dynamic features, and signal parameter sets are generated with maximizing HMM likelihood [11].  c = arg max P (Wc|qˆ, λ)  (2)  c  In Eq. (2), c = [c1T, · · · , cTt , · · · , cTT ]T represents signal parameter sequences, ct = [c(1), · · · , c(D)]T represents a signal parameter vector of the t-th frame, and W represents the matrix constructed from weights which are used for calculating dynamic features [12]. Finally, by using an MLSA (Mel-Log Spectrum Approximation) ﬁlter [13], speech is synthesized from the generated parameters.  2.2. HMM-based sound synthesis for articulation disorders If each feature parameter is trained using the acoustic features obtained from a person with an articulation disorder, the synthesized sound becomes unintelligible. Therefore, we created a more intelligible synthesized sound while preserving the speaker’s individuality by mixing the voices of a person with an articulation disorder and a physically unimpaired person.  119  Fig. 2 shows the overview of our method. In this method, we train the speech synthesis system using training data from both a person with an articulation disorder and a physically unimpaired person. First, we extract three acoustic parameters (F0 contour, spectral envelope, and aperiodicity index (AP)) from these two person’s speaking voices by using STRAIGHT analysis [14]. After extracting the features, the F0 patterns of a physically unimpaired person are modiﬁed as explained in Section 2.3. Because the duration of persons with articulation disorders is slower than that of physically unimpaired people, the duration model is generated using only the context-dependent label sequences of a physically unimpaired person. With the input text and the duration model, context-dependent label sequences are generated. Then, spectral, F0 and AP parameters are generated based on the label sequences and trained HMMs, where F0 parameters are generated from the modiﬁed F0 model and AP parameter sequences are generated from the AP model of a person with an articulation disorder. Each spectral parameter is generated from each person’s spectral model. After parameter generation, the spectral parameters of a person with an articulation disorder are modiﬁed as explained in Section 2.4. Finally, the output signal is synthesized from the features (spectral envelope, F0 contour, and aperiodicity index) by using the synthesis part of the STRAIGHT. In the following section, we explain the details of the operations related to spectral and F0 parameters.  2.3. F0 modiﬁcation  In this method, the F0 patterns of a physically unimpaired person are used for training the F0 model in HMM synthesis because the F0 patterns of a person with an articulation disorder are often unstable. To make the F0 feature’s characteristics close to those of a person with an articulation disorder, the F0 features of a physically unimpaired person are modiﬁed to those of a person with an articulation disorder. The F0 model is trained from the converted F0 sequences, which means that the F0 model includes the individuality of a person with articulation disorder. The F0 features of a physically unimpaired person are modiﬁed by using the following linear transformation:  xˆt  =  σy σx  (xt  − µx) + µy  (3)  where xt represents the log-scaled F0 of the physically unimpaired person at the frame t, µx and σx represent the mean and standard deviation of xt, respectively. µy and σy represents the mean and standard deviation of the log-scaled F0 of a person with an articulation disorder, respectively.  2.4. Spectral modiﬁcation Fig. 3 shows the original spectrograms for the word “genjitsuo” (“real” in English) of a physically unimpaired person and a person with an articulation disorder. As shown in Fig. 3, the high-frequency spectral power of a person with an articulation disorder is weaker compared to that of a physically unimpaired person. This fact implies that the synthesized spectrum of the consonant components for a person with an articulation disorder becomes weak, which makes the person’s speech difﬁcult to understand. For the spectral vowel components, the spectral parameters of a person with an articulation disorder are needed in order to preserve the target individuality. As shown in Fig. 2, after being  Figure 4: Plot of the function fm and fg (green: fm blue: fg )  given the input text, we generate spectral parameter sequences from each person’s spectral model. Then, we create the combined spectral parameter sequences, which include the parameters of a physically unimpaired person at the high-frequency part and the parameters of a person with an articulation disorder at the low-frequency part. This combination of spectral parameters is given by  Sˆ(ij) = fm(j)Sm (ij) + fg(j)Sg(ij)  (4)  where Sm, Sg, Sˆ, i and j represent the spectrum of a physically unimpaired person, the spectrum of a person with an articulation disorder, the modiﬁed spectrum, the index of spectral frames, and the frequency index, respectively. The weight functions are given by  fm(j)  =  
This paper presents a system to recognize distress speech in the home of seniors to provide reassurance and assistance. The system is aiming at being integrated into a larger system for Ambient Assisted Living (AAL) using only one microphone with a ﬁx position in a non-intimate room. The paper presents the details of the automatic speech recognition system which must work under distant speech condition and with expressive speech. Moreover, privacy is ensured by running the decoding on-site and not on a remote server. Furthermore the system was biased to recognize only set of sentences deﬁned after a user study. The system has been evaluated in a smart space reproducing a typical living room where 17 participants played scenarios including falls during which they uttered distress calls. The results showed a promising error rate of 29% while emphasizing the challenges of the task. Index Terms: Smart home, Vocal distress call, Applications of speech technology for Ambient Assisted Living 1. Introduction Life expectancy has increased in all countries of the European Union in the last decade. Therefore the part of the people who are at least 75 years old has strongly increased and solutions are needed to satisfy the wishes of elderly people to live as long as possible in their own homes. Ageing can cause functional limitations that –if not compensated by technical assistance or environmental management– lead to activity restriction [1][2]. Smart homes are a promising way to help elderly people to live independently at their own home, they are housings equipped with sensors and actuators [3][4][1][5]. Another aspect is the increasing risk of distress, among which falling is one of the main fear and lethal risk, but also blocking hip or fainting. The most common solution is the use of kinematic sensors worn by the person [6] but this imposes some constraints in the everyday life and worn sensors are not always a good solution because some persons can forget or refuse to wear it. Nowadays, one of the best suited interfaces is the voice-user interface (VUI), whose technology has reached maturity and is avoiding the use of worn sensors thanks to microphones set up in the home and allowing hands-free and distant interaction [7]. It was demonstrated that VUI is useful for system integrating speech commands [8]. The use of speech technologies in home environment requires to address particular challenges due to this speciﬁc envi-  ronment [9]. There is a rising number of smart home projects considering speech processing in their design. They are related to wheelchair command [10], vocal command for people with dysarthria [11][8], companion robot [12], vocal control of appliances and devices [13]. Due to the experimental constraints, few systems were validated with real users in realistic situation condition like in the SWEET-HOME project [14] during which a dedicated voice based home automation system was able to drive a smart home thanks to vocal commands with typical people [15] and with elderly and visually impaired people [16]. In this paper we present an approach to provide assistance in a smart home for seniors in case of distress situation in which they can’t move but can talk. The challenge is due to expressive speech which is different from standard speech: is it possible to use state of the art ASR techniques to recognize expressive speech? In our approach, we address the problem by using the microphone of a home automation and social system placed in the living room with ASR decoding and voice call matching. In this way, the user must be able to command the environment without having to wear a speciﬁc device for fall detection or for physical interaction (e.g., a remote control too far from the user when needed). Though microphones in a home is a real breach of privacy, by contrast to current smart-phones, we address the problem using an in-home ASR engine rather than a cloud based one (private conversations do not go outside the home). Moreover, the limited vocabulary ensures that only speech relevant to the command of the home is correctly decoded. Finally, another strength of the approach is to have been evaluated in realistic conditions. The paper is organised as follow. Section 2 presents the method for speech acquisition and recognition in the home. Section 3, presents the experimentation and the results which are discussed in Section 5. 2. Method The distress call recognition is to be performed in the context of a smart home which is equipped with e-lio1, a dedicated system for connecting elderly people with their relatives as shown in Figure 1. e-lio is equipped with one microphone for video conferencing. The typical setting and the distress situations were determined after a sociological study conducted by the GRePS laboratory [17] in which a representative set of seniors were included. From this sociological study, it appears that this equipment 1http://www.technosens.fr/  124 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 124–129, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  TV E−LIO 000111000111000000111111000111  Microphone  User  Sofa Carpet 000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111000000000111111111  Cupboard  Figure 1: Microphone position in the smart home  is set on a table in the living room in font of the sofa. In this way, an alert could be given if the person falls due to the carpet or if it can’t stand up from the sofa. This paper presents only the audio part of the study, for more details about the global audio and video system, the reader is referred to [18]. 2.1. Speech analysis system The audio processing was performed by the software CIRDOX[19] whose architecture is shown in Figure 2. The microphone stream is continuously acquired and sound events are detected on the ﬂy by using a wavelet decomposition and an adaptive thresholding strategy [20]. Sound events are then classiﬁed as noise or speech and, in the latter case, sent to an ASR system. The result of the ASR is then sent to the last stage which is in charge of recognizing distress calls. In this paper, we focus on the ASR system and present different strategies to improve the recognition rate of the calls. The remaining of this section presents the methods employed at the acoustic and decoding level. 2.2. Acoustic modeling The Kaldi speech recognition tool-kit [21] was chosen as ASR system. Kaldi is an open-source state-of-the-art ASR system with a high number of tools and a strong support from the community. In the experiments, the acoustic models were contextdependent classical three-state left-right HMMs. Acoustic features were based on Mel-frequency cepstral coefﬁcients, 13 MFCC-features coefﬁcients were ﬁrst extracted and then expanded with delta and double delta features and energy (40 features). Acoustic models were composed of 11,000 contextdependent states and 150,000 Gaussians. The state tying is performed using a decision tree based on a tree-clustering of the phones. In addition, off-line fMLLR linear transformation acoustic adaptation was performed. The acoustic models were trained on 500 hours of transcribed French speech composed of the ESTER 1&2 (broadcast news and conversational speech recorded on the radio) and REPERE (TV news and talk-shows) challenges as well as from 7 hours of transcribed French speech of the SH corpus (SWEETHOME) [22] which consists of records of 60 speakers interacting in the smart home and from 28 minutes of the Voix-détresse corpus [23] which is made of records of speakers eliciting a distress emotion. 2.2.1. Subspace GMM Acoustic Modelling The GMM and Subspace GMM (SGMM) both model emission probability of each HMM state with a Gaussian mixture model, but in the SGMM approach, the Gaussian means and the mixture component weights are generated from the phonetic and speaker subspaces along with a set of weight projections.  The SGMM model [24] is described in the following equations:    Mj  I     p(x|j) =    cjm wjmiN (x; µjmi, Σi),    m=1  i=1  µjmi = Mivjm,        wjmi =  . exp wiT vjm  I i′ =1  exp  wiT′  vjm  where x denotes the feature vector, j ∈ {1..J} is the HMM state, i is the Gaussian index, m is the substate and cjm is the substate weight. Each state j is associated to a vector vjm ∈ RS (S is the phonetic subspace dimension) which derives the means, µjmi and mixture weights, wjmi and it has a shared number of Gaussians, I. The phonetic subspace Mi, weight projections wiT and covariance matrices Σi i.e; the globally shared parameters Φi = {Mi, wiT , Σi} are common across all states. These parameters can be shared and estimated over multiple record conditions. A generic mixture of I gaussians, denoted as Universal Background Model (UBM), models all the speech training data for the initialization of the SGMM. Our experiments aims at obtaining SGMM shared parameters using both SWEET-HOME data (7h), Voix-détresse (28mn) and clean data (ESTER+REPERE 500h). Regarding the GMM part, the three training data set are just merged in a single one. [24] showed that the model is also effective with large amounts of training data. Therefore, three UBMs were trained respectively on SWEET-HOME data, Voix-détresse and clean data. These tree UBMs contained 1K gaussians and were merged into a single one mixed down to 1K gaussian (closest Gaussians pairs were merged [25]). The aim is to bias speciﬁcally the acoustic model with the smart home and expressive speech conditions.  2.3. Recognition of distress calls The recognition of distress calls consists in computing the phonetic distance of an hypothesis to a list of predeﬁned distress calls. Each ASR hypothesis Hi is phonetized, every voice commands Tj is aligned to Hi using Levenshtein distance. The deletion, insertion and substitution costs were computed empirically while the cumulative distance γ(i, j) between Hj and Ti is given by Equation 1.  γ(i, j) = d(Ti, Hj)+ min{γ(i − 1, j − 1), γ(i − 1, j), γ(i, j − 1)} (1)  The decision to select or not a detected sentence is then taken according a detection threshold on the aligned symbol score (phonems) of each identiﬁed call. This approach takes into account some recognition errors like word endings or light variations. Moreover, in a lot of cases, a miss-decoded word is phonetically close to the good one (due to the close pronunciation). From this the CER (Call Error Rate i.e., distress call error rate) is deﬁned as:  CER = Number of missed calls  (2)  Number of calls  This measure was chosen because of the content of the corpus Cirdo-set used in this study. Indeed, this corpus is made of sentences and interjections. All sentences are calls for help, without any other kind of sentences like home automation orders or colloquial sentences, and therefore it is not possible to determine a false alarm rate in this framework. 
When individuals lose the ability to produce their own speech, due to degenerative diseases such as motor neurone disease (MND) or Parkinson’s, they lose not only a functional means of communication but also a display of their individual and group identity. In order to build personalized synthetic voices, attempts have been made to capture the voice before it is lost, using a process known as voice banking. But, for some patients, the speech deterioration frequently coincides or quickly follows diagnosis. Using HMM-based speech synthesis, it is now possible to build personalized synthetic voices with minimal data recordings and even disordered speech. The power of this approach is that it is possible to use the patient’s recordings to adapt existing voice models pre-trained on many speakers. When the speech has begun to deteriorate, the adapted voice model can be further modified in order to compensate for the disordered characteristics found in the patient’s speech, we call this process "voice repair". In this paper we compare two methods of voice repair. The first method follows a trial and error approach and requires the expertise of a speech therapist. The second method is entirely automatic and based on some a priori statistical knowledge. A subjective evaluation shows that the automatic method achieves similar results than the manually controlled method. Index Terms: HTS, Speech Synthesis, Voice Banking, Voice Reconstruction, Voice Output Communication Aids, MND. 1. Introduction Degenerative speech disorders have a variety of causes that include Multiple Sclerosis, Parkinson’s, and Motor Neurone Disease (MND) also known in the USA as Amyotrophic Lateral Sclerosis (ALS). MND primarily affects the motor neurones in the brain and spinal cord. This causes a worsening muscle weakness that leads to a loss of mobility and difficulties with swallowing, breathing and speech production. Initial symptoms may be limited to a reduction in speaking rate, an increase of the voice’s hoarseness, or an imprecise articulation. However, at some point in the disease progression, 80 to 95% of patients are unable to meet their daily communication needs using their speech [1]. As speech becomes difficult to understand, these individuals may use a voice output communication aid (VOCA). These devices consist of a text entry interface such as a keyboard, a touch screen or an eye-tracker, and a text-to-speech synthesizer that generates the corresponding speech. However, when individuals lose the ability to produce their own speech, they lose not only a functional means of communication but also a display of their individual and social identity through their vocal characteristics.  Current VOCAs are not ideal as they are often restricted to a limited set of impersonal voices that are not matched to the age or accent of each individual. Feedback from patients, careers and patient societies has indicated that there is a great unmet need for personalized VOCAs as the provision of personalized voice is associated with greater dignity and improved self-identity for the individual and their family [2]. In order to build personalized VOCAs, several attempts have been made to capture the voice before it is lost, using a process known as voice banking. One example of this approach is ModelTalker [3], a free voice building service that can be used from any home computer in order to build a synthetic voice based on diphone concatenation, a technology developed in the 1980s. The user of this service has to record around 1800 utterances in order to fully cover the set of diphones and the naturalness of the synthetic speech is rather low. Cereproc [4] has provided a voice building service for individuals, at a relatively high cost, which uses unit selection synthesis, and is able to generate synthetic speech of increased naturalness. However, these speech synthesis techniques require a large amount of recorded speech in order to build a good quality voice. Moreover the recorded speech data must be as intelligible as possible, since the data recorded is used directly as the voice output. This requirement makes such techniques more problematic for those patients whose voices have started to deteriorate. Therefore, there is a strong motivation to improve the voice banking and voice building techniques, so that patients can use their own synthetic voices, even if their speech is already disordered at the time of recordings. A first approach is to try to separate out the disorders from the recorded speech. In this way, Rudzicz [5] has proposed a combination of several speech processing techniques. However, some disorders cannot be simply filtered out by signal processing techniques and a modelbased approach seems more appropriate. Kain [6] has proposed a voice conversion framework for the restoration of disordered speech. In its approach, the low-frequency spectrum of the voiced speech segment is modified according to a mapping defined by a Gaussian mixture model (GMM) learned in advance from a parallel dataset of disordered and target speech. The modified voiced segments are then concatenated with the original unvoiced speech segments to reconstruct the speech. This approach can be seen as a first attempt of model-based voice reconstruction although it relies only on a partial modeling of the voice components. A voice building process using the hidden Markov model (HMM)-based speech synthesis technique has been investigated to create personalized VOCAs [7-10]. This approach has been shown to produce high quality output and offers two major advantages over existing methods for voice banking and voice building. First, it is possible to use existing speaker-independent voice models pre-trained over a number of speakers and to adapt them towards a target speaker. This process known as speaker adaptation [11] requires only a very  130 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 130–133, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  small amount of speech data. The second advantage of this approach is that we can control and modify various components of the adapted voice model in order to compensate for the disorders found in the patient’s speech. We call this process “voice repair”. In this paper, we compare different strategies of voice repair using the HMM-based synthesis framework. The first method follows a trial and error approach and requires the expertise of a speech therapist. The second method is entirely automatic and based on some a priori statistical knowledge. 2. HMM-Based Speech Synthesis Our voice building process is based on the state-of-the-art HMM-based speech synthesizer, known as HTS [12]. As opposed to diphone or unit-selection synthesis, the HMM-based speech synthesizer does not use the recorded speech data directly as the voice output. Instead it is based on a vocoder model of the speech and the acoustic parameters required to drive this vocoder are represented by a set of statistical models. The vocoder used in HTS is STRAIGHT and the statistical models are contextdependent hidden semi-Markov models (HSMMs), which are HMMs with explicit state duration distributions. The state output distributions of the HSMMs represent three separate streams of acoustic parameters that correspond respectively to the fundamental frequency (logF0), the band aperiodicities and the mel-cepstrum, including their dynamics. For each stream, additional information is added to further describe the temporal trajectories of the acoustic parameters, such as their global variances over the learning data. Finally, separate decision trees are used to cluster the state durations probabilities and the state output probabilities using symbolic context information at the phoneme, syllable, word, and utterance level. In order to synthesize a sentence, a linguistic analyser is used to convert the sequence of words into a sequence of symbolic contexts and the trained HSMMs are invoked for each context. A parametergeneration algorithm is then used to estimate the most likely trajectory of each acoustic parameter given the sequence of models. Finally the speech is generated by the STRAIGHT vocoder driven by the estimated acoustic parameters. 3. Speaker Adaptation One advantage of the HMM-based speech synthesis for voice building is that the statistical models can be estimated from a very limited amount of speech data thanks to speaker adaptation. This method [9] starts with a speaker-independent model, or “average voice model”, learned over multiple speakers and uses model adaptation techniques drawn from speech recognition such as maximum likelihood linear regression (MLLR), to adapt the speaker independent model to a new speaker. It has been shown that using 100 sentences or approximately 6-7 minutes of speech data is sufficient to generate a speaker-adapted voice that sounds similar to the target speech [7]. In the following of this paper we refer the speaker-adapted voices as “voice clones”. This provides a much more practical way to build a personalized voices for patients. For instance, it is now possible to construct a synthetic voice for a patient prior to a laryngectomy operation, by quickly recording samples of their speech [8]. A similar approach can also be used for patients with neurodegenerative diseases such as MND. However, we do not want to reproduce the symptoms of a vocal problem if the speech has already been disordered at the time of the recording. This is the aim of the voice repair methods introduced in the section 5 of this paper.  4. Database of Voice Donors Ideally, the average voice model used for the speaker adaptation should be close to the vocal identity of the patient. On the other hand, a minimum number of speakers are necessary to train robust average voice models. Therefore, we have created a database of more than 900 healthy voice donors with various accents (Scottish, Irish, Other UK). Each speaker recorded about one hour of speech (400 sentences). This database of healthy voices is first used to create the average voice models used for speaker adaptation. Ideally, the average voice model should be close to the vocal identity of the patient and it has been shown that gender and regional accent are the most influent factors in speaker similarity perception [13]. Therefore, the speakers are clustered according to their gender and their regional accent in order to train specific average voice models. A minimum of 10 speakers is required in order to get robust average voice models. Furthermore, the database is also used to select a reference donor for the voice repair procedures described in section 5. The voice repair is most successful when the reference donor is as close as possible to the patient in terms of vocal identity. 5. Voice Repair Some individuals with neurodegenerative disease may already have speech symptoms at the time of the recording. In that case, the speaker adaptation process will also replicate these symptoms in the speaker-adapted voice. Therefore we need to remove speech disorders from the synthetic voice, so that it sounds more natural and more intelligible. Repairing synthetic voices is conceptually similar to the restoration of disordered speech mentioned in Section 1, but we can now exploit the acoustic models learned during the training and the adaptation processes in order to control and modify various speech features. This is the second major advantage of using HMM-based speech synthesis. In particular, HTS has statistically independent models for duration, log-F0, band aperiodicity and mel-cepstrum. This allows the substitution of some models in the patient's speakeradapted voice by that of a well-matched healthy voice or an average of multiple healthy voices. For example, patients with MND often have a disordered speaking rate, contributing to a loss of the speech intelligibility. The substitution of the state duration models enables the timing disruptions to be regulated at the phoneme, word, and utterance levels. Furthermore, MND speakers often have breathy or hoarse speech, in which excessive breath through the glottis produces unwanted turbulent noise. In such cases, we can substitute the band aperiodicity models to produce a less breathy or hoarse output. In the following part of this section, we present two different methods of model substitution. The first one is manually controlled whereas the second one is automatic. 5.1. Manual voice repair In the manual approach, a speech therapist first selects a reference voice among all the available voices with same accent, gender and age range than the patient. Then the models of this reference voice are used to correct some of the patient’s voice models. This correction is based on mean and variance interpolation between models. A graphical interface allows the speech therapist to control the amount of interpolation between the patient’s voice models and the reference voice models as illustrated in Figure 1.  131  Figure 1: Graphical interface for model interpolation. The following models and information can be interpolated: • Duration • Dynamics coefficients of the log-F0 • Dynamics coefficients of the mel-cepstrum • Low-order coefficients of the mel-cepstrum • High-order coefficients of the mel-cepstrum The voiced/unvoiced weights and aperiodicity models are simply substituted since their impact on voice identity is rather limited and their replacement of will fix the breathiness disorders. The interpolation of the high order static coefficients and the dynamics coefficients of the mel-cepstrum will help to reduce the articulation disorders without altering the timbre. The interpolation of the dynamics coefficients of the log-F0 will help to regulate the prosodic disorders such as monotonic F0. Finally the global variances of all the parameters are also simply substituted. We will refer to this method as the manual repair. 5.2. Automatic voice repair The manual voice repair requires a lot of expertise from the speech therapist, as it is a trial and error approach. Therefore, we aim to replace it by a fully automated voice repair procedure. We measure the Kullback-Leibler distance (KLD) between the models of the patient voice and the models of the reference voice as illustrated in Figure 2. Then the likelihood of each of the measured distance is evaluated given the statistical distribution of KLD distances between healthy voice models of similar accent, gender and age band. The likelihood values are used to control the interpolation between the patient and reference voice models. For instance, if the likelihood of the KLD distance for a given model of the patient voice is very low, the corresponding model of the reference voice is used to replace it in the patient voice. The reference voice model is also selected automatically as the one that maximizes the likelihood of the patient recording data. 6. Experiment The manual and automatic voice repair methods presented in Section 5 were evaluated for the case of a MND patient. This patient was a 45 years old Scottish male that we recorded twice. A first recording of one hour (500 sentences) has been made just after diagnosis when he was at the very onset of the disease.  Figure 2: Graphical interface for model interpolation.  At that time, his voice did not show any disorders and could still be considered as “healthy”. A second recording of 15 minutes (50 sentences) has been made 10 months later. He has then acquired some speech disorders typically associated with MND, such as excessive hoarseness and breathiness, disruption of speech fluency, reduced articulation and monotonic prosody. These two recordings were used separately as adaptation data in order to create two speaker-adapted voices from the same maleScottish average voice model. The synthetic voice created from the first recording of the patient (“healthy” speech) was used as the reference voice for the subjective evaluations. This choice of a synthetic voice as reference instead of the natural recordings was done to avoid any bias due to the loss of quality inherent to the synthesis. Two different reconstructed voices were created from the second recording of the patient (“impaired” speech) using the manual and the automatic voice repair methods respectively. In order to evaluate the voice repair methods, two subjective tests were conducted. The first one assesses the intelligibility of the reconstructed voices whereas the second one measures their similarity with synthetic voice created from “healthy” speech of the patient. We also included the synthetic voices of the donors selected for the manual and the automatic voice repair in the similarity test. All the synthetic voices used in the experiment are summarized in Table 1.  Voice MD AD HC IC IR_v1 IR_v2  Description Voice of donor used in manual voice repair Voice of donor used in automatic voice repair Voice clone of the “healthy” speech (1st recording) Voice clone of the “impaired” speech (2nd recording) Reconstructed voice using manual voice repair Reconstructed voice using automatic voice repair  Table 1: Voices compared in the evaluation tests.  6.1. Listening Intelligibility Test The same 50 semantically unpredictable sentences were synthesized for each of the voices created from the patient’s recordings (see Table 1). The resulting 200 synthesized samples were divided into 4 groups such that each voice is represented by 10 samples in a group. A total of 40 native English participants  132  were asked to transcribe the synthesized samples, with 10 participants for each group. Within each group, the samples were presented in random order for each participant. The participants performed the test with headphones. The transcriptions were evaluated by measuring the word error rate (WER).  HC  IC  IR_v1  IR_v2  0  10  20  30  40  50  60  70  80  90  Figure 3: Word Error Rate (mean and standard deviation)  6.2. Speaker Similarity Test The same test sentence “People look, but no one ever finds it.” was synthesized for each of the voices in Table 1. Participants were asked to listen alternatively to the reference voice (HC) and to the same sentence synthesized with one of the other voices. The presentation order of the voice samples was randomized. The participants have been asked to rate the similarity in terms of speaker identity between the tested voice and the reference (HC) on a 5-point scale (1: Very dissimilar, 2: Dissimilar, 3: Quite Similar, 4: Very similar; and 5: Identical). A total of 40 native English speakers performed the test using headphones.  IR_v2  IR_v1  AD  MD  
We use a set of 477 lexicosyntactic, acoustic, and semantic features extracted from 393 speech samples in DementiaBank to predict clinical MMSE scores, an indicator of the severity of cognitive decline associated with dementia. We use a bivariate dynamic Bayes net to represent the longitudinal progression of observed linguistic features and MMSE scores over time, and obtain a mean absolute error (MAE) of 3.83 in predicting MMSE, comparable to within-subject interrater standard deviation of 3.9 to 4.8 [1]. When focusing on individuals with more longitudinal samples, we improve MAE to 2.91, which suggests at the importance of longitudinal data collection. Index Terms- Alzheimer’s disease, dementia, Mini-Mental State Examination (MMSE), dynamic Bayes network, feature selection 1. Introduction Research into the early assessment, pathogenesis, and progression of dementia is becoming increasingly important, as the proportion of people it affects grows every year. Alzheimer’s disease (AD), the most common type of dementia, affects more than half of the population above 80 years of age and its impact on society is expected to grow as the “baby boomer” generation ages [2, 3, 4]. There is no single laboratory test that can identify dementia with absolute certainty. Typically, probable dementia is diagnosed using the Mini Mental State Examination (MMSE), which provides a score on a scale of 0 (greatest cognitive decline) to 30 (no cognitive decline), based on a series of questions in ﬁve areas: orientation, registration, attention, memory, and language [5]. While MMSE provides a uniﬁed scale for measuring the severity of the disease, it can be time-consuming and relatively costly, often requiring a trained neuropsychologist or physician to administer the test in a clinical setting. Changes in cognitive ability due to neurodegeneration associated with AD lead to a progressive decline in memory and language quality. Patients experience deterioration in sensory, working, declarative, and non-declarative memory, which leads to a decrease in the grammatical complexity and lexical content of their speech [6]. Such changes differ from the pattern of decline expected in older adults [6], which suggests that temporal changes in linguistic features can aid in disambiguation of healthy older adults from those with dementia. Some previous work used machine learning classiﬁers with linguistic features for two-class separation of patients with AD from controls (see section 1.1), but there appears to be no previous research that has used them to infer a clinical score for dementia — an indicator of the degree of cognitive decline. The  present work uses a set of automatically-extracted lexicosyntactic, acoustic, and semantic (LSAS) features for estimating continuous MMSE scores on a scale of 0 to 30, using a dynamic Bayes network for representing relationships between observed linguistic measures and underlying clinical scores. Since dynamic changes in linguistic ability in patients with AD differ from those in typical healthy older adults [6], we hypothesize that considering speech samples over time would aid in estimating underlying cognitive status. Previous studies analyzing dynamic progression of language features in patients with AD did not employ machine learning techniques, and are characterized by a small number of subjects (between 3 and 6) and a limited set of features that do not include acoustics. The present work improves on these analyses by extracting LSAS features from a relatively large collection of longitudinal speech, in order to estimate MMSE scores. 1.1. Related Work Previous work has explored the use of lexicosyntactic features for identifying individuals with AD from controls. Orimaye et al. [7] used DementiaBank1, one of the largest existing datasets of pathological speech [8], to perform binary classiﬁcation of 242 patients with dementia and 242 controls; a support vector machine classiﬁer achieved their best F-measure of 0.74 [7]. Another experiment by Jarrold et al. collected spontaneous speech data from 9 controls, 9 patients with AD, and 30 patients with frontotemporal lobar degeneration (FTLD) [9]. A multi-layer perceptron model obtained classiﬁcation accuracy of 88% on a two-class task (AD:controls, and FTLD:controls), and 80% on a three-class task (AD:FTLD:controls). While these studies have obtained promising results in classifying patients with dementia based on linguistic features, there is limited work modelling the progression of such features over time. Le et al. [10] examined the longitudinal changes in a small set of hand-selected lexicosyntactic measures, such as vocabulary size, repetition, word class deﬁcit, and syntactic complexity, in 57 novels of three British authors written over a period of several decades. They found statistically signiﬁcant lexical deterioration in Agatha Christie’s work evidenced by vocabulary impoverishment and a pronounced increase in word repetitions [10], but the measures for syntactic complexity did not yield conclusive results. A similar analysis performed by Sundermann examined the progression of a small set of lexicosyntactic features, such as length, frequency, and vocabulary measures in 6 patients with AD or mild cognitive impairment (MCI), with a minimum of 3 longitudinal samples in DementiaBank [11]. Analysis of the features over time did not reveal con- 1http://talkbank.org/DementiaBank/  134 SLPAT 2015, 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 134–139, Dresden, Germany, 11 September, 2015. c 2015 The Association for Computational Linguistics  clusive patterns; Sundermann suggested that the limited sample size and feature set selection may be the cause. Neither study involved acoustics or machine learning techniques. 2. Methodology 2.1. Data We use data from DementiaBank, a large dataset of speech produced by people with dementia (including probable AD, possible AD, vascular dementia, and MCI) and healthy older adults, recorded longitudinally at the University of Pittsburgh’s Alzheimer’s Disease Research Center [8]. Annual visits with each subject consist of a recording of speech data, its textual transcription, and an MMSE score. Subjects have a variable number of longitudinal samples (min = 1, max = 5, M = 1.54, SD = 0.79). Each speech sample consists of a verbal description of the Boston Cookie Theft picture, which typical lasts about a minute. We partition subjects between controls (CT) and those with probable AD, possible AD or MCI (or, collectively,“AD” 2). Considering only subjects with associated MMSE scores, the working set consists of 393 speech samples from 255 subjects (165 AD, 90 CT). 2.2. Features Three major types of features are extracted from the speech samples and their transcriptions: (1) lexicosyntactic measures, extracted from syntactic parse trees constructed with the Brown parser and POS-tagged transcriptions of the narratives [12, 13, 14, 15, 16]; (2) acoustic measures, including the standard Melfrequency cepstral coefﬁcients (MFCCs), formant features, and measures of disruptions in vocal fold vibration regularity [17]; and (3) semantic measures, pertaining to the ability to describe concepts and objects in the Cookie Theft picture. The full list of features, along with their major type and subtype, is shown in Table 1. 2.3. Feature Analysis Two feature selection methods are used to identify the most informative features for disambiguating AD from CT. Since the MMSE score is a measure of the progression of cognitive impairment and is used to distinguish AD from CT generally, we hypothesize that highly discriminating features of the two groups would also be good predictors of MMSE. This is quantiﬁed by Spearman’s rank-order correlation between the most informative features and the MMSE score, ρMMSE, shown in Table 2. The ﬁrst feature ranking method is a two-sample t-test (α = 0.001, two-tailed) which quantiﬁes the signiﬁcance of the difference in each feature value between the two classes; the features are ordered by increasing p-value. Table 2 shows the type and p-value of the top 10 features, along with their correlation with MMSE. Control subjects use longer utterances, more gerund + prepositional phrase constructions (VP→ VBG PP, e.g., standing on the chair), more content words such as noun phrases (NP) and verbs, and are more likely to talk about what they see through the window (info_window), which is in the background of the scene (e.g., it seems to be summer out). On the other hand, subjects with AD use more words not found in the dictionary (NID), and more function words such as pronouns (PRP). Honoré’s statistic measures lexical richness, ex- 2Ongoing work distinguishes between AD and MCI.  tending type-token ratio, which is decreased in AD. These ﬁndings are consistent with expectations.  Table 2: The top 10 features selected by a two-sample t-test (α = 0.001, two-tailed) as the most informative discriminators of AD versus CT. ρMMSE is Spearman’s rank-order correlation coefﬁcient between the given feature and the MMSE score. The features in bold are among the top 10 selected by mRMR.  Feature  Feature type  p ρMMSE  avelength VP → VBG PP NID NP → DT NN NP → PRP prp_ratio honoré verbs frequency info_window  lexicosyntactic lexicosyntactic lexicosyntactic lexicosyntactic lexicosyntactic lexicosyntactic lexicosyntactic lexicosyntactic lexicosyntactic semantic  1.24E-13 1.90E-13 3.23E-11 1.12E-10 2.14E-10 1.16E-09 2.53E-09 4.81E-09 9.37E-09 1.27E-08  0.3837 0.3757 -0.3712 0.3438 -0.3186 -0.3089 0.3400 0.2604 -0.3725 0.3420  Since the majority of the extracted acoustic features consist of MFCCs and measures related to aperiodicity of vocal fold vibration, the lack of signiﬁcance of the acoustic features as discriminators between the two classes may be attributed to the fact that AD is not strongly associated with motor impairment of the articulators involved in speech production. The second feature selection method is minimumredundancy-maximum-relevance (mRMR), which minimizes the average mutual information between features and maximizes the mutual information between each feature and the class [18]; the features were ranked from most relevant to least. The results of this technique generally corroborate the selection made by the t-test, with no acoustic features among the top 10 selected. Here, mRMR selects a greater proportion of semantic features (e.g., mentions of the window and sink, and the number of occurrences of curtain and stool), placing more weight on the content of what the speaker is saying as a way of discriminating the two classes. All of the features displayed in Table 2 have moderate statistically signiﬁcant correlation with MMSE (p < 0.001). Since we are interested in the task of predicting clinical MMSE scores, the experiments described in Sec. 3 use correlation itself as a third feature selection method. The features are ranked by their correlation with MMSE, and the ones with the highest correlations are selected. 3. Experiments 3.1. Predicting MMSE score using LSAS features To model the longitudinal progression of MMSE scores and LSAS features, we constructed a dynamic Bayes network (DBN) with continuous nodes, i.e., a Kalman ﬁlter with 2 variables, shown in Figure 1. Each time slice (Qt, Yt) represents one annual visit for a subject. Each conditioning node Qt represents the underlying continuous MMSE score for that visit (R1×1), while each node Yt represents the vector of observed continuous LSAS features (R477×1). A Kolmogorov-Smirnov test for normality was performed on the MMSE scores of all AD subjects, with the null hypothesis that they come from a normal distribution. The test did not reject this null hypothesis at the 5% conﬁdence level, demonstrating that the data come from a  135  Table 1: Summary of all extracted features (477 in total). The number of features in each type and subtype is shown in parentheses.  Type Feature Subtype  Description and examples  Lexicosyntactic (182)  Production rule (121) Phrase type (9) Syntactic complexity (4) Subordination/coordination (3) Word type (25) Word quality (10) Length measures (5) Perseveration (5) MFCCs (170) Pauses and ﬁllers (8) Pitch and Formants (8) Aperiodicity (13) Other speech measures (11) Mention of a concept (21) Word frequency (64)  Number of times a production rule is used, divided by the total number of productions. Phrase type proportion, rate and mean length. Depth of the syntactic parse tree. Proportion of subordinate and coordinate phrases to the total number of phrases, and ratio of subordinate to coordinate phrases. Word type proportion; type-to-token ratio, Honoré’s statistic. Imageability; age of acquisition (AoA); familiarity; transitivity. Average length of utterance, T-unit and clause, and total words per transcript. Cosine distance between pairs of utterances within a transcript. The ﬁrst 42 MFCC parameters, along with their means, kurtosis and skewness, and the kurtosis and skewness of the mean of means. Total and mean duration of pauses; long and short pause counts; pause to word ratio; ﬁllers (um, uh). Mean and variance of F0, F1, F2, F3. Jitter, shimmer, recurrence rate, recurrence period density entropy, determinism, length of diagonal structures, laminarity. Total duration of speech, zero-crossing rate, autocorrelation, linear prediction coefﬁcients, transitivity. Presence of mentions of indicator lemmas, related to key concepts in the Cookie Theft picture. Number of times a given lemmatized word, relating to the Cookie Theft picture, was mentioned  Acoustic (210)  Sem. (85)  normal distribution with M=18.52, SD=5.16. There are three conditional probability densities: the MMSE prior probability P (Q1), the MMSE transition probability P (Qt|Qt−1), and the LSAS feature observation probability P (Yt|Qt).  Q1  Q2  Q3  ···  Y1  Y2  Y3  ···  Figure 1: Temporal Bayes network (TBN) with continuous hidden (Qt) and observed (Yt) nodes. Hidden nodes represent MMSE score, and observed vectors represent LSAS features extracted from speech. The feature set described in Sec. 2.2 is preprocessed to (i) remove features with zero variance across all samples, and (ii) normalize feature values to zero-mean and unit-variance, as is standard practice. Since the number of features (477) is large compared to the number of samples (393), the three feature selection methods described in Sec. 2.3 (i.e., a paired two-tailed t-test, mRMR, and correlation with MMSE score) are used to avoid overﬁtting, by varying the number of features selected by each method in order to determine the optimal feature set size. The parameters of the three probability distributions in our model are trained using maximum likelihood estimation (MLE) since all training data are fully observed. During testing, the observed features for each test case are provided and junction  tree inference on the trained model computes the marginal distribution of the now hidden (MMSE) nodes. Performance is measured as the mean absolute error (MAE) between actual and predicted MMSE scores. Since not all subjects have the same number of longitudinal samples, MAE is evaluated at the ﬁrst and last hidden node, and averaged. Experiments are performed with leave-one-out cross-validation, where data from each subject, in turn, are used for testing and all other data for training, over all 255 subjects. The results, with varying feature set sizes and feature selection methods, are shown in Table 3. The lowest MAE of 3.83 (σ = 0.49) is achieved when correlation is used to select the top 40 features. A two-factor repeated measures ANOVA performed on the mean MAE shows that both main effects are statistically signiﬁcant, i.e., feature set size (F7,24 = 8.67, p < 0.001) and the feature selection method (F2,24 = 4.07, p < 0.05). The interaction effect is not signiﬁcant (F14,24 = 0.16, ns), as expected given that the factors are independent. To illustrate the longitudinal changes in cognitive and linguistic ability, Fig. 2 shows the pattern of decline of MMSE and the top 5 most correlated features for the subset of subjects with AD. This demonstrates the MMSE score declining nonmonotonically over four annual visits (the maximum number of visits for AD subjects in DementiaBank), along with similar patterns across the indicated LSAS features. 3.2. Effect of longitudinal data on predicted MMSE score To test the hypothesis that using longitudinal speech data aids in identifying underlying cognitive status (i.e., improving MMSE estimation), the Kalman ﬁlter experiment described in 3.1 is repeated for subsets of the dataset consisting of different amounts  136  Table 3: MAE in predicting MMSE scores using three feature selection methods and different feature set sizes. The lowest error for each feature selection method is highlighted in bold.  Nfeatures t-test mRMR ρMMSE  
System architecture, experimental settings and evaluation results of EHR group in the en-ja, zh-ja, JPCzh-ja and JPCko-ja tasks are described. Our system concept is combination of a rule based method and a statistical method. System combination of rule-based machine translation (RBMT), RBMT plus statistical post-editing (SPE) and preordering plus statistical machine translation (SMT) is conducted. From the multiple outputs of three systems, candidate selection part selects the best output by language model score. For JPCzh-ja task devtest data translation, SPE improves BLEU score by 17.81, preordering improves BLEU score by 1.89 and system combination improves BLEU score by 0.26.  
This year, the Nara Institute of Science and Technology (NAIST)’s submission to the 2015 Workshop on Asian Translation was based on syntax-based statistical machine translation, with the addition of a reranking component using neural attentional machine translation models. Experiments re-conﬁrmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also conﬁrmed for the ﬁrst time that these results also carry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, ﬁnding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words. 
Translation systems of our NICT team at the 2nd Workshop on Asian Translation (WAT 2015) are described in this paper. We participated in two translation tasks: Japanese-to-English (JE) and Korean-toJapanese (KJ). A baseline phrased-based (PB) statistical machine translation (SMT) system in Moses was used. On JE translation, two pre-reordering approaches were applied: a simple reverse preordering and a dependency-based approach. On KJ translation, the processing was purely conducted on character-level. Evaluation results show that even simple approaches can improve JE and KJ PB SMT signiﬁcantly. These techniques can be easily applied in practice because of the simplicity. 
This paper provides the system description of Toshiba Machine Translation System for the 2nd Workshop on Asian Translation (WAT2015). We participated in all tasks that consist of “scientific papers subtask” and “patents subtask”. We submitted statistically post edited translation (SPE) results based on our rule based translation system and SMT for each language pair. In addition, we submitted system combination results between SPE and SMT with a recurrent neural language model (RNNLM). In experimental results, the system combination achieved higher BLEU scores than single system with reranking. We also obtained improvements in Chinese translation in crowdsourcing evaluations. 
This paper introduces the KyotoEBMT example-based machine translation framework. Since last year’s workshop we have replaced input trees with forests, improved alignment, added new features, and introduced bilingual neural network reranking. The major beneﬁts of our system include online example retrieval and ﬂexible reordering. We also use syntactic dependency analysis for both source and target languages in the hope of learning how to translate non-local structure. The system implementation (this paper refers to version 1.0) is available as open-source. 
In this paper, we evaluate Neural Machine Translation (NMT) models in English-Japanese translation task. Various network architectures with different recurrent units are tested. Additionally, we examine the effect of using pre-reordered data for the training. Our experiments show that even simple NMT models can produce better translations compared with all SMT baselines. For NMT models, recovering unknown words is another key to obtaining good translations. We describe a simple workaround to find missing translations with a back-off system. To our surprise, performing prereordering on the training data hurts the model performance. Finally, we provide a qualitative analysis demonstrates a specific error pattern in NMT translations which omits some information and thus fail to preserve the complete meaning. 
In this paper, we describe NAVER machine translation system for English to Japanese and Korean to Japanese tasks at WAT 2015. We combine the traditional SMT and neural MT in both tasks. 
Automatic evaluation of machine translation (MT) quality is essential in developing high quality MT systems. Despite previous criticisms, BLEU remains the most popular machine translation metric. Previous studies on the schism between BLEU and manual evaluation highlighted the poor correlation between MT systems with low BLEU scores and high manual evaluation scores. Alternatively, the RIBES metric—which is more sensitive to reordering—has shown to have better correlations with human judgements, but in our experiments it also fails to correlate with human judgements. In this paper we demonstrate, via our submission to the Workshop on Asian Translation 2015 (WAT 2015), a patent translation system with very high BLEU and RIBES scores and very poor human judgement scores. 
This paper describes the Beijing Jiaotong University Chinese-Japanese machine translation system which participated in the 2st Workshop on Asian Translation (WAT2015). We exploit the syntactic and semantic knowledge encoded in dependency tree to build a dependency-to-string translation model for Chinese-Japanese statistical machine translation (SMT). Our system achieves a BLEU of 34.87 and a RIBES of 79.25 on the Chinese-Japanese translation task in the official evaluation. 
This paper describes Chinese–Japanese translation systems based on different alignment methods using the JPO corpus and our submission (ID: WASUIPS) to the subtask of the 2015 Workshop on Asian Translation. One of the alignment methods used is bilingual hierarchical sub-sentential alignment combined with sampling-based multilingual alignment. We also accelerated this method and in this paper, we evaluate the translation results and time spent on several machine translation tasks. The training time is much faster than the standard baseline pipeline (GIZA++/Moses) and MGIZA/Moses. 
This paper presents our Chinese-toJapanese patent machine translation system for WAT 2015 (Group ID: ntt) that uses syntactic pre-ordering over Chinese dependency structures. A head word and its modiﬁer words are reordered by hand-written rules or a learning-to-rank model. Our system outperforms baseline phrase-based machine translations and competes with baseline tree-to-string machine translations. 
There are various approaches to statistical machine translation (SMT). In particular, phrase-based SMT (PBSMT) is used as a de facto standard for many language pairs because it works robustly across languages and it is easy to implement. However, the results of PBSMT can include ungrammatical sentences, since it typically does not take syntactic structure into account. To overcome this problem, we propose a linguistically motivated approach based on segmenting a source phrase using a dependency structure and translating each phrase with PBSMT. This paper presents the results of our method on JapaneseEnglish translation and discusses potential improvements. 
This paper presents the work done to port a deep-transfer rule-based machine translation system to translate from a different source language by maximizing the exploitation of existing resources and by limiting the development work. Speciﬁcally, we report the changes and effort required in each of the system’s modules to obtain an English-Basque translator, ENEUS, starting from the Spanish-Basque Matxin system. We run a human pairwise comparison for the new prototype and two statistical systems and see that ENEUS is preferred in over 30% of the test sentences. 
This paper presents a hybrid machine translation framework based on a preprocessor that translates fragments of the input text by using example-based machine translation techniques. The preprocessor resembles a translation memory with named-entity and chunk generalization, and generates a high quality partial translation that is then completed by the main translation engine, which can be either rule-based (RBMT) or statistical (SMT). Results are reported for both RBMT and SMT hybridization as well as the preprocessor on its own, showing the effectiveness of our approach. 
This paper motivates the need for an homogeneous way of measuring and estimating translation effort (quality) in computeraided translation. It then deﬁnes a general framework for the measurement and estimation of translation effort so that translation technologies can be both optimized and combined in a principled manner. In this way, professional translators will beneﬁt from the seamless integration of all the technologies at their disposal when working on a translation job. 
This paper investigates to what extent the use of paraphrasing in translation memory (TM) matching and retrieval is useful for human translators. Current translation memories lack semantic knowledge like paraphrasing in matching and retrieval. Due to this, paraphrased segments are often not retrieved. Lack of semantic knowledge also results in inappropriate ranking of the retrieved segments. Gupta and Ora˘san (2014) proposed an improved matching algorithm which incorporates paraphrasing. Its automatic evaluation suggested that it could be beneﬁcial to translators. In this paper we perform an extensive human evaluation of the use of paraphrasing in the TM matching and retrieval process. We measure post-editing time, keystrokes, two subjective evaluations, and HTER and HMETEOR to assess the impact on human performance. Our results show that paraphrasing improves TM matching and retrieval, resulting in translation performance increases when translators use paraphrase enhanced TMs. 
We propose a novel dependency-based reordering model for hierarchical SMT that predicts the translation order of two types of pairs of constituents of the source tree: head-dependent and dependent-dependent. Our model uses the dependency structure of the source sentence to capture the medium- and long-distance reorderings between these pairs of constituents. We describe our reordering model in detail and then apply it to a language pair in which the languages involved follow different word order patterns, English (SVO) and Farsi (free word order being SOV the most frequent pattern). Our model outperforms a baseline (standard hierarchical SMT) by 0.78 BLEU points absolute, statistically signiﬁcant at p = 0.01. 
The modelling of natural language tasks using data-driven methods is often hindered by the problem of insufﬁcient naturally occurring examples of certain linguistic constructs. The task we address in this paper – quality estimation (QE) of machine translation – suffers from lack of negative examples at training time, i.e., examples of low quality translation. We propose various ways to artiﬁcially generate examples of translations containing errors and evaluate the inﬂuence of these examples on the performance of QE models both at sentence and word levels. 
In this paper we apply distributional semantic information to document-level machine translation. We train monolingual and bilingual word vector models on large corpora and we evaluate them ﬁrst in a cross-lingual lexical substitution task and then on the ﬁnal translation task. For translation, we incorporate the semantic information in a statistical document-level decoder (Docent), by enforcing translation choices that are semantically similar to the context. As expected, the bilingual word vector models are more appropriate for the purpose of translation. The ﬁnal document-level translator incorporating the semantic model outperforms the basic Docent (without semantics) and also performs slightly over a standard sentencelevel SMT system in terms of ULC (the average of a set of standard automatic evaluation metrics for MT). Finally, we also present some manual analysis of the translations of some concrete documents. 
This paper aims at exploring the potential of a lay community as post-editors. It focusses on 15 members of an online technology support forum, native speakers of the target language (TL) and some knowledge of the source language (SL) translating content that was machine translated from English into German speciﬁc to their own domain. It presents the most predominant errors remaining in the post-edited output and the impact of these on the quality of the post-edited output as measured by domain specialists evaluating adequacy and ﬂuency. This paper further explores examples of these errors and possible solutions to reducing the occurrence of these and maximising the community’s potential. The targeted post-editing quality was “‘good enough”, as determined in the postediting guidelines. The PE results demonstrate that there is still room for improvement in terms of quality. 
Sharon O’Brien ADAPT Centre/SALIS/CTTS Dublin City University Ireland sharon.obrien@dcu.ie  Abstract The increasing use of post-editing in localisation workflows has led to a great deal of research and development in the area, much of it requiring user evaluation. This paper compares some results from a post-editing user interface study carried out using novice and expert translator groups. By comparing rates of productivity, edit distance, engagement with the research, and qualitative findings regarding each group‟s attitude to post-editing, we find that there are trade-offs to be considered when selecting participants for evaluation tasks. Novices may generally be more positive and enthusiastic and will engage considerably with the research while professionals will be more efficient, but their routines and attitudes may prevent full engagement with research objectives. 
Statistical machine translation (SMT) suffers from various problems which are exacerbated where training data is in short supply. In this paper we address the data sparsity problem in the Farsi (Persian) language and introduce a new parallel corpus, TEP++. Compared to previous results the new dataset is more efﬁcient for Farsi SMT engines and yields better output. In our experiments using TEP++ as bilingual training data and BLEU as a metric, we achieved improvements of +11.17 (60%) and +7.76 (63.92%) in the Farsi– English and English–Farsi directions, respectively. Furthermore we describe an engine (SF2FF) to translate between formal and informal Farsi which in terms of syntax and terminology can be seen as different languages. The SF2FF engine also works as an intelligent normalizer for Farsi texts. To demonstrate its use, SF2FF was used to clean the IWSLT–2013 dataset to produce normalized data, which gave improvements in translation quality over FBK’s Farsi engine when used as training data. 
In this paper the author presents methods for dynamic terminology integration in statistical machine translation systems using a source text pre-processing workﬂow. The workﬂow consists of exchangeable components for term identiﬁcation, inﬂected form generation for terms, and term translation candidate ranking. Automatic evaluation for three language pairs shows a translation quality improvement from 0.9 to 3.41 BLEU points over the baseline. Manual evaluation for seven language pairs conﬁrms the positive results; the proportion of correctly translated terms increases from 1.6% to 52.6% over the baseline. 
The best way to improve a statistical machine translation system is to identify concrete problems causing translation errors and address them. Many of these problems are related to the characteristics of the involved languages and differences between them. This work explores the main obstacles for statistical machine translation systems involving two morphologically rich and under-resourced languages, namely Serbian and Slovenian. Systems are trained for translations from and into English and German using parallel texts from different domains, including both written and spoken language. It is shown that for all translation directions structural properties concerning multi-noun collocations and exact phrase boundaries are the most difﬁcult for the systems, followed by negation, preposition and local word order differences. For translation into English and German, articles and pronouns are the most problematic, as well as disambiguation of certain frequent functional words. For translation into Serbian and Slovenian, cases and verb inﬂections are most difﬁcult. In addition, local word order involving verbs is often incorrect and verb parts are often missing, especially when translating from German. 
Multiple references in machine translation evaluation are usually under-explored: they are ignored by alignment-based metrics and treated as bags of n-grams in string matching evaluation metrics, none of which take full advantage of the recurring information in these references. By exploring information on the n-gram distribution and on divergences in multiple references, we propose a method of ngram weighting and implement it to generate new versions of the popular BLEU and NIST metrics. Our metrics are tested in two into-English machine translation datasets. They lead to a signiﬁcant increase in Pearson’s correlation with human ﬂuency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation. 
In this paper we analyse the use of popular automatic machine translation evaluation metrics to provide labels for quality estimation at document and paragraph levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 
In this paper we present an approach to reduce data sparsity problems when translating from morphologically rich languages into less inﬂected languages by selectively stemming certain word types. We develop and compare three different integration strategies: replacing words with their stemmed form, combined input using alternative lattice paths for the stemmed and surface forms and a novel hidden combination strategy, where we replace the stems in the stemmed phrase table by the observed surface forms in the test data. This allows us to apply advanced models trained on the surface forms of the words. We evaluate our approach by stemming German adjectives in two German→English translation scenarios: a low-resource condition as well as a large-scale state-of-the-art translation system. We are able to improve between 0.2 and 0.4 BLEU points over our baseline and reduce the number of out-of-vocabulary words by up to 16.5%. 
This paper provides additional observations on the viability of a strategy independently proposed in 2012 and 2013 for evaluation of machine translation (MT) for assimilation purposes. The strategy involves human evaluators, who are asked to restore keywords (to ﬁll gaps) in reference translations. The evaluation method is applied to two language pairs, Basque–Spanish and Tatar–Russian. To reduce the amount of time required to prepare tasks and analyse results, an open-source task management system is introduced. The evaluation results show that the gap-ﬁlling task may be suitable for measuring MT quality for assimilation purposes. 
This article presents a method of training maximum-entropy models to perform lexical selection in a rule-based machine translation system. The training method described is unsupervised; that is, it does not require any annotated corpus. The method uses source-language monolingual corpora, the machine translation (MT) system in which the models are integrated, and a statistical target-language model. Using the MT system, the sentences in the sourcelanguage corpus are translated in all possible ways according to the different translation equivalents in the bilingual dictionary of the system. These translations are then scored on the target-language model and the scores are normalised to provide fractional counts for training source-language maximum-entropy lexical-selection models. We show that these models can perform equally well, or better, than using the target-language model directly for lexical selection, at a substantially reduced computational cost. 
The concept of fuzzy matching in translation memories can take place using linguistically aware or unaware methods, or a combination of both. We designed a ﬂexible and time-efﬁcient framework which applies and combines linguistically unaware or aware metrics in the source and target language. We measure the correlation of fuzzy matching metric scores with the evaluation score of the suggested translation to ﬁnd out how well the usefulness of a suggestion can be predicted, and we measure the difference in recall between fuzzy matching metrics by looking at the improvements in mean TER as the match score decreases. We found that combinations of fuzzy matching metrics outperform single metrics and that the best-scoring combination is a non-linear combination of the different metrics we have tested. 
This paper presents experiments on the human ranking task performed during WMT2013. The goal of these experiments is to re-run the human evaluation task with translation studies students and to compare the results with the human rankings performed by the WMT development teams during WMT2013. More speciﬁcally, we test whether we can reproduce, and if yes to what extent, the WMT2013 ranking task and whether specialised knowledge from translation studies inﬂuences the results in terms of intra- and inter-annotator agreement as well as in terms of system ranking. We present two experiments on the English-German WMT2013 machine translation output. Analysis of the data follows the methods described in the ofﬁcial WMT2013 report. The results indicate a higher inter- and intra-annotator agreement, less ties and slight differences in ranking for the translation studies students as compared to the WMT development teams. 
Translation memories (TM) are widely used in the localization industry to improve consistency and speed of human translation. Several approaches have been presented to integrate the bilingual translation units of TMs into statistical machine translation (SMT). We present an extension of these approaches to the integration of partial matches found in a large, monolingual corpus in the target language, using cross-language information retrieval (CLIR) techniques. We use locality-sensitive hashing (LSH) for efﬁcient coarse-grained retrieval of match candidates, which are then ﬁltered by ﬁnegrained fuzzy matching, and ﬁnally used to re-rank the n-best SMT output. We show consistent and signiﬁcant improvements over a state-of-the-art SMT system, across different domains and language pairs on tens of millions of sentences. 
We present a translation system that models the selection of prepositions in a targetside generation component. This novel approach allows the modeling of all subcategorized elements of a verb as either NPs or PPs according to target-side requirements relying on source and target side features. The BLEU scores are encouraging, but fail to surpass the baseline. We additionally evaluate the preposition accuracy for a carefully selected subset and discuss how typical problems of translating prepositions can be modeled with our method. 
The quality and quantity of articles in each Wikipedia language varies greatly. Translating from another Wikipedia is a natural way to add more content, but the translation process is not properly supported in the software used by Wikipedia. Past computer-assisted translation tools built for Wikipedia are not commonly used. We created a tool that adapts to the speciﬁc needs of an open community and to the kind of content in Wikipedia. Qualitative and quantitative data indicates that the new tool helps users translate articles easier and faster. 
This paper describes the challenges of building a Statistical Machine Translation (SMT) system for non-ﬁctional subtitles. Since our experiments focus on a “difﬁcult“ translation direction (i.e. FrenchGerman), we investigate several methods to improve the translation performance. We also compare our in-house SMT systems (including domain adaptation and pre-reordering techniques) to other SMT services and show that prereordering alone signiﬁcantly improves the baseline systems. 
This paper presents a machine translation tool – based on Moses – developed for the International Maritime Organization (IMO) for the automatic translation of documents from Spanish, French, Russian and Arabic to/from English. The main challenge lies in the insufﬁcient size of inhouse corpora (especially for Russian and Arabic). The United Nations (UN) granted IMO the right to use UN resources and we describe experiments and results we obtained with different translation model combination techniques. While BLEU results remain inconclusive for combinations, we also analyze user preferences for certain models (when choosing betweeen IMO only or combined with UN). The combined models are perceived by translators as being much better for general texts while IMO only models seem better for technical texts. 
tion of a special ‘automotive’ user dictionary, to be used additionally for automotive translations. This procedure is described in detail in (Thurmair & Aleksić 2012). Result of these efforts were four test systems, for German-to-English, and tuned for automotive domain with the same adaptation data: SMT-base: Moses with just baseline data SMT-adapted:Mosesbaselineplusin-domain data RMT-base: PT-baseline out-of-the-box RMT-adapted: PT with an automotive dictionary. 3 Evaluation Data In total about 1500 sentences were taken from the collected strongly comparable automotive corpora for tests, with one reference translation each. The sentences represent ‘real-life’ data; they were not cleaned or corrected.. 4 Evaluation methodology 4.1 General options Several methods can be applied for the evaluation of MT results, cf. Figure 1. 
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed translations and the pushdown assembler. J. Comput. System Sci., 3(1):37–56. 
We present a method to store additional information in a minimal automaton so that it is possible to compute a corresponding tree node number for a state. The number can then be used to retrieve additional information. The method works for minimal (and any other) deterministic acyclic ﬁnite state automata (DFAs). We also show how to compute the inverse mapping. 
We prove a Chomsky-Schützenberger representation theorem for weighted multiple context-free languages. 
We present an approach to obtain language models from a tree corpus using probabilistic regular tree grammars (prtg). Starting with a prtg only generating trees from the corpus, the prtg is generalized step by step by merging nonterminals. We focus on bottom-up deterministic prtg to simplify the calculations. 
The tree languages of tree-adjoining grammars are precisely those of linear monadic context-free tree grammars. Unlike the original proof, we present a direct transformation of a tree-adjoining grammar into an equivalent linear monadic context-free tree grammar.  ture of H is altered along the way to G. In the following, we describe an alternative direct construction of an equivalent linear monadic cftg G from H. We argue that G resembles H very closely. After introducing some preliminary notions in Section 2, we will describe our construction and prove its correctness in Section 3. Section 4 illustrates the construction in an example, and contrasts it to the construction of Kepser and Rogers.  
This paper presents a fully automated lossless hyper-minimization method for ﬁnitestate morphological analyzers in Xerox lexc formalism. The method utilizes ﬂag diacritics to preserve the structure of the original lexc description in the ﬁnite-state analyzer, which results in reduced size of the analyzer. We compare our method against an earlier solution by Drobac et al. (2014) which requires manual selection of ﬂag diacritics and results in slow lookup. We show that our method gives similar size reductions while maintaining fast lookup without requiring any manual input. 
In this paper we show how traditional composition-based ﬁnite-state grammars can be augmented to preserve intermediate results in a chain of compositions. These intermediate strings can be very helpful for various tasks: enriching information while parsing or generating, providing accurate information for debugging purposes as well as offering explicit alignment information between morphemes and tags in morphological grammars. The implementation strategies discussed in the paper hinge on a representation of multi-tape automata as a single-tape automaton. A simple composition algorithm for such multitape automata is provided. 
Durative forces are introduced to Finite State Temporality (the application of Finite State Methods to Temporal Semantics). Punctual and durative forces are shown to have natural representations as ﬂuents which place certain constraints on strings. These forces are related to previous work on stative explanations of aspectual classiﬁcation. Given this extended ontology, it is shown how type coercion can be handled in this framework. 
Building morphological parsers with existing ﬁnite state toolkits can result in something of a mis-match between the programming language of the toolkit and the linguistic concepts familiar to the average linguist. We illustrate this mismatch with a particular linguistic construct, suppletive allomorphy, and discuss ways to encode suppletive allomorphy in the Stuttgart Finite State tools (sfst). The complexity of the general solution motivates our work in providing an alternative formalism for morphology and phonology, one which can be translated automatically into sfst or other morphological parsing engines. 
This article investigates a probabilistic model to describe how signs form words in Ancient Egyptian writing. This applies to both hieroglyphic and hieratic texts. The model uses an intermediate layer of sign functions. Experiments are concerned with ﬁnding the most likely sequence of sign functions that relates a given sequence of signs and a given sequence of phonemes. 
We present a simple and effective approach to the task of grapheme-tophoneme conversion based on a set of manually edited grapheme-phoneme mappings which drives not only the alignment of words and corresponding pronunciations, but also the segmentation of words during model training and application, respectively. The actual conversion is performed with the help of a conditional random ﬁeld model, after which a language model selects the most likely string of grapheme-phoneme segment pairs from the set of hypotheses. We evaluate our approach by comparing it to a state-ofthe-art joint sequence model with respect to two different datasets of contemporary German and one of contemporary English. 
A string encoding for a subclass of bipartite graphs enables graph rewriting used in autosegmental descriptions of tone phonology via existing and highly optimized ﬁnite-state transducer toolkits (YliJyr 2013). The current work oﬀers a rigorous treatment of this code-theoretic approach, generalizing the methodology to all bipartite graphs having no crossing edges and unordered nodes. We present three bijectively related codes each of which exhibit unique characteristics while preserving the freedom to violate or express the OCP constraint. The codes are inﬁnite, ﬁnite-state representable and optimal (eﬃciently computable, invertible, locally iconic, compositional) in the sense of Kornai (1995). They extend the encoding approach with visualization, generality and ﬂexibility and they make encoded graphs a strong candidate when the formal semantics of autosegmental phonology or non-crossing alignment relations are implemented within the conﬁnes of regular grammar. 
 Telugu is a Dravidian language with nearly 85 million first language speakers. In this paper we report a realization engine for Telugu that automates the task of building grammatically well-formed Telugu sentences from an input specification consisting of lexicalized grammatical constituents and associated features. Our realization engine adapts the design approach of SimpleNLG family of surface realizers.  
In this paper we analyse a statistical approach for generating Spanish sentences focused on the surface realisation stage guided by an input seed feature. This seed feature can be anything such as a word, a phoneme, a sentiment, etc. Our approach attempts to maximise the appearance of words with that seed feature along the sentence. It follows three steps: ﬁrst we train a language model over a corpus; then we obtain a bag of words having that concrete seed feature; and ﬁnally a sentence is generated based on both, the language model and the bag of words. Depending on the selected seed feature, this kind of sentences can be useful for a wide range of applications. In particular, we have focused our experiments on generating sentences in order to reinforce the phoneme pronunciation for dyslalia disorder. Automatic generated sentences have been evaluated manually obtaining good results in newly generated meaningful sentences. 
We present a method for automatically generating descriptions of biological events encoded in the KB BIO 101 Knowledge base. We evaluate our approach on a corpus of 336 event descriptions, provide a qualitative and quantitative analysis of the results obtained and discuss possible directions for further work. 
We describe an algorithm for inducing clause-combining rules for use in a traditional natural language generation architecture. An experiment pairing lexicalized text plans from the SPaRKy Restaurant Corpus with logical forms obtained by parsing the corresponding sentences demonstrates that the approach is able to learn clause-combining operations which have essentially the same coverage as those used in the SPaRKy Restaurant Corpus. This paper ﬁlls a gap in the literature, showing that it is possible to learn microplanning rules for both aggregation and discourse connective insertion, an important step towards ameliorating the knowledge acquisition bottleneck for NLG systems that produce texts with rich discourse structures using traditional architectures. 
Typically, human evaluation of NLG output is based on user ratings. We collected ratings and reading time data in a simple, low-cost experimental paradigm for text generation. Participants were presented corpus texts, automatically linearised texts, and texts containing predicted referring expressions and automatic linearisation. We demonstrate that the reading time metrics outperform the ratings in classifying texts according to their quality. Regression analyses showed that self-reported ratings discriminated poorly between the kinds of manipulation, especially between defects in word order and text coherence. In contrast, a combination of objective measures from the low-cost mouse contingent reading paradigm provided very high classiﬁcation accuracy and thus, greater insight into the actual quality of an automatically generated text. 
In the present study, we investigate if speakers refer to moving entities in route directions (RDs) and how listeners evaluate these references. There is a general agreement that landmarks should be perceptually salient and stable objects. Animated movement attracts visual attention, making entities salient. We ask speakers to watch videos of crossroads and give RDs to listeners, who in turn have to choose a street on which to continue (Experiment 1) or choose the best instruction among three RDs (Experiment 2). Our results show that speakers mention moving entities, especially when their movement is informative for the navigation task (Experiment 1). Listeners understand and use moving landmarks (Experiment 1), yet appreciate stable landmarks more (Experiment 2). 
Understanding what has led to a failure is crucial for addressing problems with computer systems. We present a meta-NLG system that can be conﬁgured to generate natural explanations from error trace data originating in an external computational system. Distinguishing features are the generic nature of the system, and the underlying ﬁnite-state technology. Results of a two-pronged evaluation dealing with naturalness and ease of use are described. 
In this paper we present a snapshot of endto-end NLG system evaluations as presented in conference and journal papers1 over the last ten years in order to better understand the nature and type of evaluations that have been undertaken. We ﬁnd that researchers tend to favour speciﬁc evaluation methods, and that their evaluation approaches are also correlated with the publication venue. We further discuss what factors may inﬂuence the types of evaluation used for a given NLG system. 
This paper proposes a method for reordering words in a Japanese sentence based on concurrent execution with dependency parsing so that the sentence becomes more readable. Our contributions are summarized as follows: (1) we extend a probablistic model used in the previous work which concurrently performs word reordering and dependency parsing; (2) we conducted an evaluation experiment using our semi-automatically constructed evaluation data so that sentences in the data are more likely to be spontaneously written by natives than the automatically constructed evaluation data in the previous work. 
We present the sentence ordering part of a natural language generation module, used in the framework of a knowledge base of electronic navigation charts and sailing directions. The particularity of the knowledge base is that it is based on a controlled hybrid language, that is the combination of a controlled natural language and a controlled visual language. The sentence ordering process is able to take into account hybrid (textual and visual) information, involving cartographic data, as well as landscape “read” by the navigator. 
We present a Pictograph-to-Text translation system for people with Intellectual or Developmental Disabilities (IDD). The system translates pictograph messages, consisting of one or more pictographs, into Dutch text using WordNet links and an ngram language model. We also provide several pictograph input methods assisting the users in selecting the appropriate pictographs. 
This paper presents an ongoing project about the symbolic translation from Italian to Italian Signed Language (LIS) in the rail stations domain. We describe some technical issues in the generation side of the translation, i.e. the use of XML templates for microplanning, the implementation of some LIS linguistic features in the grammar. 
This paper presents a parsing paradigm for natural language generation task, which learns a tailored probabilistic context-free grammar for encoding meaning representation (MR) and its corresponding natural language (NL) expression, then decodes and yields natural language sentences at the leaves of the optimal parsing tree for a target meaning representation. The major advantage of our method is that it does not require any prior knowledge of the MR syntax for training. We deployed our method in response generation for a Chinese spoken dialogue system, obtaining results comparable to a strong baseline both in terms of BLEU scores and human evaluation. 
Automatic story generation is the subject of a growing research effort which has mainly focused on ﬁctional stories. In this paper, we present some preliminary work to generate re´cits (stories) from sensors data acquired during a ski sortie. In this approach, the story planning is performed using a task model that represents domain knowledge and sequential constraints between ski activities. To test the validity of the task model, a small-scale user evaluation was performed to compare the human perception of re´cit plans from hand written or automatically generated re´cits. This evaluation showed no difference in story plan identiﬁcation adding credence to the eligibility of the task model for representing story plan in NLG. To go a step further, a basic NLG system to generate narrative from activities extracted from GPS data is also reported. 
Referring to landmarks has been identiﬁed to lead to improved navigation instructions. However, a previous corpus study suggests that human “wizards” also choose to refer to street names and generate user-centric instructions. In this paper, we conduct a task-based evaluation of two systems reﬂecting the wizards’ behaviours and compare them against an improved version of previous landmark-based systems, which resorts to user-centric descriptions if the landmark is estimated to be invisible. We use the GRUVE virtual interactive environment for evaluation. We ﬁnd that the improved system, which takes visibility into account, outperforms the corpus-based wizard strategies, however not signiﬁcantly. We also show a signiﬁcant effect of prior user knowledge, which suggests the usefulness of a user modelling approach. 
Unreliable data is present in datasets, and is either ignored, acknowledged ad hoc, or undetected. This paper discusses data quality issues with a potential framework in mind to deal with them. Such a framework should be applied within data-to-text systems at the generation of text rather than being an afterthought. This paper also shows ways to express uncertainty through language and World Health Organisation (WHO) corpus studies, and an experiment which analyses how subjects approached summarising data with data quality issues. This work is still ongoing. 
We investigate the task of predicting prepositions that can be used to describe the spatial relationships between pairs of objects depicted in images. We explore the extent to which such spatial prepositions can be predicted from (a) language information, (b) visual information, and (c) combinations of the two. In this paper we describe the dataset of object pairs and prepositions we have created, and report ﬁrst results for predicting prepositions for object pairs, using a Naive Bayes framework. The features we use include object class labels and geometrical features computed from object bounding boxes. We evaluate the results in terms of accuracy against human-selected prepositions. 
As dialog systems are getting more and more ubiquitous, there is an increasing number of application domains for natural language generation, and generation objectives are getting more diverse (e.g., generating informationally dense vs. less complex utterances, as a function of target user and usage situation). Flexible generation is difﬁcult and labourintensive with traditional template-based generation systems, while fully data-driven approaches may lead to less grammatical output, particularly if the measures used for generation objectives are correlated with measures of grammaticality. We here explore the combination of a data-driven approach with two very simple automatic grammar induction methods, basing its implementation on OpenCCG. 
JSrealB is an English and French text realizer written in JavaScript to ease its integration in web applications. The realization engine is mainly rule-based. Table driven rules are defined for inflection and algorithmic propagation rules, for agreements. It allows its user to build a variety of French and English expressions and sentences from a single specification to produce dynamic output depending on the content of a web page. Natural language generation can automate a significant part of textual production, only requiring a human to supply some important aspects and thus saving considerable time for producing consistent grammatically correct output. In recent years, tools such as SimpleNLG (Gatt and Reiter, 2009) facilitated text realization by a programmer provided they program their application in Java. This system was then extended with SimpleNLG-EnFr (Vaudry and Lapalme, 2013), a English-French version of SimpleNLG. Another approach to text realization is JSreal (Daoust and Lapalme, 2014), a French Web realizer written in JavaScript. This paper describes an attempt at combining the ideas of SimpleNLGEnFr and JSreal to produce a bilingual realizer for French and English from a single specification. JSrealB generates well-formed expressions and sentences. It can be used standalone for linguistic demonstrations or be integrated into complex text generation projects. But like JSreal, it is aimed at web developers, from taking care of morphology, declension and conjugation to creating well-formed texts. A web programmer who wishes to use JSrealB to produce flexible English and/or French textual or HTML output only needs to add two lines in the page: one for im-  porting program and one for calling JSrealB loader to load the resources (i.e. lexicon and rules). The principles underlying JSrealB are similar to those of SimpleNLG: programming language instructions create data structures corresponding to the constituents of the sentence to be produced. Once the data structure (a tree) is built in memory, it is traversed to produce the list of tokens of the sentence. This data structure is built by function calls whose names are the same as the symbols usually used for classical syntax trees: for example, N to create a noun structure, NP for a Noun Phrase, V for a Verb, D for a determiner, S for a Sentence and so on. Features added to the structures using the dot notation can modify the values according to what is intended. JSrealB syntactic representation is patterned after classical constituent grammar notations. For example, S(NP(D("a"),N("woman")).n("p"), VP("eat").t("ps")) is the JSrealB specification for The women ate. Plural is indicated with feature n("p") where n indicates number and p plural. The verb is conjugated to past tense indicated by the feature tense t and value ps. Agreement between NP and VP is performed automatically. French and English are languages whose structures are similar. Both languages use the same alphabet, they are both fusional languages sharing a similar conjugation system and their word order follows the same basic Subject-VerbObject paradigm (Shoebottom, 1996). But structural differences do exist: e.g. the position of adjectives differs and rules for gender and number agreement differ for nouns and pronoun between these languages. These differences must be taken into account at many levels. First, syntactic differences and agreements (i.e. features propagation), must be handled at the phrase or sentence level by  Proceedings of the 15th European Workshop on Natural Language Generation (ENLG), pages 109–111,  Brighton, September 2015. c 2015 Association for Computational Linguistics  
Decision-making is often dependent on uncertain data, e.g. data associated with conﬁdence scores, such as probabilities. A concrete example of such data is weather data. We will demo a game-based setup for exploring the effectiveness of different approaches (graphics vs NLG) to communicating uncertainty in rainfall and temperature predictions (www.macs.hw.ac.uk/ InteractionLab/weathergame/ ). The game incorporates a natural language extension of the MetOfﬁce Weather game1. The extended version of the game can be used in three ways: (1) to compare the effectiveness of different information presentations of uncertain data; (2) to collect data for the development of effective data-driven approaches; and (3) to serve as a task-based evaluation setup for Natural Language Generation (NLG). 
 2 General Approach and Some Specificities  Despite considerable research invested in the generation of referring expressions (GRE), there still exists no adequate generic procedure for GRE involving relations. In this paper, we present a system for GRE that combines attributes and relations, using best-first search technique. Preliminary evaluations show its effectiveness; the design enables the use of heuristics that meet linguistic preferences. 
In this paper, we present the task of generating image descriptions with gold standard visual detections as input, rather than directly from an image. This allows the Natural Language Generation community to focus on the text generation process, rather than dealing with the noise and complications arising from the visual detection process. We propose a ﬁne-grained evaluation metric speciﬁcally for evaluating the content selection capabilities of image description generation systems. To demonstrate the evaluation metric on the task, several baselines are presented using bounding box information and textual information as priors for content selection. The baselines are evaluated using the proposed metric, showing that the ﬁnegrained metric is useful for evaluating the content selection phase of an image description generation system. 
We describe an initial version of an algorithm for generating named references to locations of geographic scale. We base the algorithm design on evidence from corpora and experiments, which show that named entity usage is extremely frequent, even in less obvious scenes, and that names are normally used as the ﬁrst focus on a global region. The current algorithm normally selects the Frames of Reference that humans also select, but it needs improvement to mix frames via a mereological mechanism. 
This paper describes a method for extracting potential causal relations from temporal data and using them to structure a generated report. The method is applied to the Activity of Daily Living domain. The extracted relations seem to be useful to locally link activities with explicit rhetorical relations. However, further work is needed to better exploit them for improving coherence at the global level. 
We have explored how a conversational agent can introduce a selected topic in an ongoing non-task oriented interaction with a user, where the selected topic has little to do with the current topic. Based on the reasoning process of the agent we have constructed a set of transition strategies to introduce the new topic. We tested the effects of each of these strategies on the perception of the dialogue and the agent. 
Usage based car insurances, which use sensors to track driver behaviour, are enjoying growing popularity. Although the data collected by these insurances could provide detailed feedback about the driving style, this information is usually kept away from the driver and is used only to calculate insurance premiums. In this paper, we explored the possibility of providing drivers with textual feedback based on telemetric data in order to improve individual driving, but also general road safety. We report that textual feedback generated through NLG was preferred to non-textual summaries currently popular in the ﬁeld and speciﬁcally was better at giving users a concrete idea of how to adapt their driving. 
A controlled use of omnipresent data can leverage a potential of services never reached before. In this paper, we propose a user driven approach to take advantage of massive data streams. Our solution, named Stream2Text, relies on a personalized and continuous reﬁnement of data to generate texts (in natural language) that provide a tailored synthesis of relevant data. It enables monitoring by a wide range of users as text streams can be shared on social networks or used individually on mobile devices. 
In this paper, an original framework to model human-machine spoken dialogues is proposed to deal with co-adaptation between users and Spoken Dialogue Systems in non-cooperative tasks. The conversation is modeled as a Stochastic Game: both the user and the system have their own preferences but have to come up with an agreement to solve a non-cooperative task. They are jointly trained so the Dialogue Manager learns the optimal strategy against the best possible user. Results obtained by simulation show that non-trivial strategies are learned and that this framework is suitable for dialogue modeling. 
Model-free reinforcement learning has been shown to be a promising data driven approach for automatic dialogue policy optimization, but a relatively large amount of dialogue interactions is needed before the system reaches reasonable performance. Recently, Gaussian process based reinforcement learning methods have been shown to reduce the number of dialogues needed to reach optimal performance, and pre-training the policy with data gathered from different dialogue systems has further reduced this amount. Following this idea, a dialogue system designed for a single speaker can be initialised with data from other speakers, but if the dynamics of the speakers are very different the model will have a poor performance. When data gathered from different speakers is available, selecting the data from the most similar ones might improve the performance. We propose a method which automatically selects the data to transfer by deﬁning a similarity measure between speakers, and uses this measure to weight the inﬂuence of the data from each speaker in the policy model. The methods are tested by simulating users with different severities of dysarthria interacting with a voice enabled environmental control system. 
We describe an empirical study that crowdsourced human-authored recovery strategies for various problems encountered in physically situated dialogue. The purpose was to investigate the strategies that people use in response to requests that are referentially ambiguous or impossible to execute. Results suggest a general preference for including speciﬁc kinds of visual information when disambiguating referents, and for volunteering alternative plans when the original instruction was not possible to carry out. 
In this paper, we apply reinforcement learning (RL) to a multi-party trading scenario where the dialog system (learner) trades with one, two, or three other agents. We experiment with different RL algorithms and reward functions. The negotiation strategy of the learner is learned through simulated dialog with trader simulators. In our experiments, we evaluate how the performance of the learner varies depending on the RL algorithm used and the number of traders. Our results show that (1) even in simple multi-party trading dialog tasks, learning an effective negotiation policy is a very hard problem; and (2) the use of neural ﬁtted Q iteration combined with an incremental reward function produces negotiation policies as effective or even better than the policies of two strong hand-crafted baselines. 
This paper deals with an incremental turntaking model that provides a novel solution for end-of-turn detection. It includes a ﬂexible framework that enables active system barge-in. In order to accomplish this, a systematic procedure of teaching a dialog system to produce meaningful system barge-in is presented. This procedure improves system robustness and success rate. It includes constructing cost models and learning optimal policy using reinforcement learning. Results show that our model reduces false cut-in rate by 37.1% and response delay by 32.5% compared to the baseline system. Also the learned system barge-in strategy yields a 27.7% increase in average reward from user responses. 
Although restating part of a student’s correct response correlates with learning and various types of restatements have been incorporated into tutorial dialogue systems, this tactic has not been tested in isolation to determine if it causally contributes to learning. When we explored the effect of tutor restatements that support inference on student learning, it did not beneﬁt all students equally. We found that students with lower incoming knowledge tend to beneﬁt more from an increased level of these types of restatement while students with higher incoming knowledge tend to beneﬁt more from a decreased level of such restatements. This ﬁnding has implications for tutorial dialogue system design since an inappropriate use of restatements could dampen learning. 
Multi-document summarization is a very important area of Natural Language Processing (NLP) nowadays because of the huge amount of data in the web. People want more and more information and this information must be coherently organized and summarized. The main focus of this paper is to deal with the coherence of multi-document summaries. Therefore, a model that uses discursive information to automatically evaluate local coherence in multi-document summaries has been developed. This model obtains 92.69% of accuracy in distinguishing coherent from incoherent summaries, outperforming the state of the art in the area. 
We describe a new model for Dialog State Tracking called a Stacked Relational Tree, which naturally models complex relationships between entities across user utterances. It can represent multiple conversational intents and the change of focus between them. Updates to the model are made by a rule-based system in the language of tree regular expressions. We also introduce a probabilistic version that can handle ASR/NLU uncertainty. We show how the parameters can be trained from log data, showing gains on a variety of standard Belief Tracker metrics, and a measurable impact on the success rate of an end-to-end dialog system for TV program discovery. 
This paper introduces Eve, a highperformance agent that plays a fast-paced image matching game in a spoken dialogue with a human partner. The agent can be optimized and operated in three different modes of incremental speech processing that optionally include incremental speech recognition, language understanding, and dialogue policies. We present our framework for training and evaluating the agent’s dialogue policies. In a user study involving 125 human participants, we evaluate three incremental architectures against each other and also compare their performance to human-human gameplay. Our study reveals that the most fully incremental agent achieves game scores that are comparable to those achieved in human-human gameplay, are higher than those achieved by partially and nonincremental versions, and are accompanied by improved user perceptions of efﬁciency, understanding of speech, and naturalness of interaction. 
This paper presents a taxonomy of errors in chat-oriented dialogue systems. Compared to human-human conversations and task-oriented dialogues, little is known about the errors made in chat-oriented dialogue systems. Through a data collection of chat dialogues and analyses of dialogue breakdowns, we classiﬁed errors and created a taxonomy. Although the proposed taxonomy may not be complete, this paper is the ﬁrst to present a taxonomy of errors in chat-oriented dialogue systems. We also highlight the difﬁculty in pinpointing errors in such systems. 
Full discourse parsing in the PDTB framework is a task that has only recently been attempted. We present the Two Taggers approach, which reformulates the discourse parsing task as two simpler tagging tasks: identifying the relation within each sentence, and identifying the relation between each pair of adjacent sentences. We then describe a system that uses two CRFs to achieve an F1 score of 39.33, higher than the only previously existing system, at the full discourse parsing task. Our results show that sequential information is important for discourse relations, especially cross-sentence relations, and that a simple approach to argument span identiﬁcation is enough to achieve state of the art results. We make our easy to use, easy to extend parser publicly available. 
We explore different evaluation methods for 4 different synthetic voices and 1 human voice. We investigate whether intelligibility, naturalness, or likability of a voice is correlated to the voice’s evocative function potential, a measure of the voice’s ability to evoke an intended reaction from the listener. We also investigate the extent to which naturalness and likability ratings vary depending on whether or not exposure to a voice is extended and continuous vs. short-term and sporadic (interleaved with other voices). Finally, we show that an automatic test can replace the standard intelligibility tests for text-to-speech (TTS) systems, which eliminates the need to hire humans to perform transcription tasks saving both time and money. 
We present a dialog act annotation for German Twitter conversations. In this paper, we describe our annotation effort of a corpus of German Twitter conversations using a full schema of 57 dialog acts, with a moderate inter-annotator agreement of multi-π = 0.56 for three untrained annotators. This translates to an agreement of 0.76 for a minimal set of 10 broad dialog acts, comparable to previous work. Based on multiple annotations, we construct a merged gold standard, backing off to broader categories when needed. We draw conclusions wrt. the structure of Twitter conversations and the problems they pose for dialog act characterization. 
Dialogue topic tracking aims at analyzing and maintaining topic transitions in on-going dialogues. This paper proposes to utilize Wikiﬁcation-based features for providing mention-level correspondences to Wikipedia concepts for dialogue topic tracking. The experimental results show that our proposed features can signiﬁcantly improve the performances of the task in mixed-initiative human-human dialogues. 
We developed a natural language dialog listening agent that uses a knowledge base (KB) to generate rich and relevant responses. Our system extracts an important named entity from a user utterance, then scans the KB to extract contents related to this entity. The system can generate diverse and relevant responses by assembling the related KB contents into appropriate sentences. Fifteen students tested our system; they gave it higher approval scores than they gave other systems. These results demonstrate that our system generated various responses and encouraged users to continue talking. 
Dialogue interaction with remote interlocutors is a difﬁcult application area for speech recognition technology because of the limited duration of acoustic context available for adaptation, the narrow-band and compressed signal encoding used in telecommunications, high variability of spontaneous speech and the processing time constraints. It is even more difﬁcult in the case of interacting with non-native speakers because of the broader allophonic variation, less canonical prosodic patterns, a higher rate of false starts and incomplete words, unusual word choice and smaller probability to have a grammatically well formed sentence. We present a comparative study of various approaches to speech recognition in non-native context. Comparing systems in terms of their accuracy and real-time factor we ﬁnd that a Kaldi-based Deep Neural Network Acoustic Model (DNN-AM) system with online speaker adaptation by far outperforms other available methods. 
When implementing a conversational educational teaching agent, user-intent understanding and dialog management in a dialog system are not sufficient to give users educational information. In this paper, we propose a conversational educational teaching agent that gives users some educational information or triggers interests on educational contents. The proposed system not only converses with a user but also answer questions that the user asked or asks some educational questions by integrating a dialog system with a knowledge base. We used the Wikipedia corpus to learn the weights between two entities and embedding of properties to calculate similarities for the selection of system questions and answers. 
In this paper we present some information theoretical and statistical features including function word skip n-grams for detecting plagiarism intrinsically. We train a binary classiﬁer with different feature sets and observe their performances. Basically, we propose a set of 36 features for classifying plagiarized and non-plagiarized texts in suspicious documents. Our experiment ﬁnds that entropy, relative entropy and correlation coefﬁcient of function word skip n-gram frequency proﬁles are very effective features. The proposed feature set achieves F-Score of 85.10%. 
We use reinforcement learning (RL) to learn a multi-issue negotiation dialogue policy. For training and evaluation, we build a hand-crafted agenda-based policy, which serves as the negotiation partner of the RL policy. Both the agendabased and the RL policies are designed to work for a large variety of negotiation settings, and perform well against negotiation partners whose behavior has not been observed before. We evaluate the two models by having them negotiate against each other under various settings. The learned model consistently outperforms the agenda-based model. We also ask human raters to rate negotiation transcripts between the RL policy and the agenda-based policy, regarding the rationality of the two negotiators. The RL policy is perceived as more rational than the agenda-based policy. 
With Language Understanding Intelligent Service (LUIS), developers without machine learning expertise can quickly build and use language understanding models speciﬁc to their task. LUIS is entirely cloud-based: developers log into a website, enter a few example utterances and their labels, and then deploy a model to an HTTP endpoint. Utterances sent to the endpoint are logged and can be efﬁciently labeled using active learning. Visualizations help identify issues, which can be resolved by either adding more labels or by giving hints to the machine learner in the form of features. Altogether, a developer can create and deploy an initial language understanding model in minutes, and easily maintain it as usage of their application grows. 
At SIGDIAL-2013 our talking robot demonstrated Wikipedia-based spoken information access in English. Our new demo shows a robot speaking different languages, getting content from different language Wikipedias, and switching languages to meet the linguistic capabilities of different dialogue partners. 
In this demonstration we show how situated multi-party human-robot interaction can be modelled using the open source framework IrisTK. We will demonstrate the capabilities of IrisTK by showing an application where two users are playing a collaborative card sorting game together with the robot head Furhat, where the cards are shown on a touch table between the players. The application is interesting from a research perspective, as it involves both multi-party interaction, as well as joint attention to the objects under discussion. 
Determining when conversational participants agree or disagree is instrumental for broader conversational analysis; it is necessary, for example, in deciding when a group has reached consensus. In this paper, we describe three main contributions. We show how different aspects of conversational structure can be used to detect agreement and disagreement in discussion forums. In particular, we exploit information about meta-thread structure and accommodation between participants. Second, we demonstrate the impact of the features using 3-way classiﬁcation, including sentences expressing disagreement, agreement or neither. Finally, we show how to use a naturally occurring data set with labels derived from the sides that participants choose in debates on createdebate.com. The resulting new agreement corpus, Agreement by Create Debaters (ABCD) is 25 times larger than any prior corpus. We demonstrate that using this data enables us to outperform the same system trained on prior existing in-domain smaller annotated datasets. 
We propose a generic, memory-based approach for the detection of implicit semantic roles. While state-of-the-art methods for this task combine hand-crafted rules with specialized and costly lexical resources, our models use large corpora with automated annotations for explicit semantic roles only to capture the distribution of predicates and their associated roles. We show that memory-based learning can increase the recognition rate of implicit roles beyond the state-of-the-art. 
There has been a recent explosion in applications for dialogue interaction ranging from direction-giving and tourist information to interactive story systems. Yet the natural language generation (NLG) component for many of these systems remains largely handcrafted. This limitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content. We propose that a solution to this problem lies in new methods for developing language generation resources. We describe the ES-TRANSLATOR, a computational language generator that has previously been applied only to fables, and quantitatively evaluate the domain independence of the EST by applying it to personal narratives from weblogs. We then take advantage of recent work on language generation to create a parameterized sentence planner for story generation that provides aggregation operations, variations in discourse and in point of view. Finally, we present a user evaluation of different personal narrative retellings. 
In this talk, I plan to summarize recent work in these areas, focusing on their connection, as a promise for wide coverage spoken language understanding in conversational systems, while at the same time investigating what is still lacking for natural human-machine interactions and related challenges. 
This paper presents the ﬁrst evaluation of a full automated prototype system for time-offset interaction, that is, conversation between a live person and recordings of someone who is not temporally copresent. Speech recognition reaches word error rates as low as 5% with generalpurpose language models and 19% with domain-speciﬁc models, and language understanding can identify appropriate direct responses to 60–66% of user utterances while keeping errors to 10–16% (the remainder being indirect, or off-topic responses). This is sufﬁcient to enable a natural ﬂow and relatively open-ended conversations, with a collection of under 2000 recorded statements. 
The REAL Challenge took place for the ﬁrst time in 2014, with a long term goal of creating streams of real data that the research community can use, by fostering the creation of systems that are capable of attracting real users. A novel approach is to have high school and undergraduate students devise the types of applications that would attract many real users and that need spoken interaction. The projects are presented to researchers from the spoken dialog research community and the researchers and students work together to reﬁne and develop the ideas. Eleven projects were presented at the ﬁrst workshop. Many of them have found mentors to help in the next stages of the projects. The students have also brought out issues in the use of speech for real applications. Those issues involve privacy and significant personalization of the applications. While long-term impact of the challenge remains to be seen, the challenge has already been a success at its immediate aims of bringing new ideas and new researchers into the community, and serves as a model for related outreach efforts. 
Online forums are now one of the primary venues for public dialogue on current social and political issues. The related corpora are often huge, covering any topic imaginable. Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic. We frame this goal as consisting of two tasks: argument extraction and argument facet similarity. We focus here on the argument extraction task, and show that we can train regressors to predict the quality of extracted arguments with RRSE values as low as .73 for some topics. A secondary goal is to develop regressors that are topic independent: we report results of cross-domain training and domain-adaptation with RRSE values for several topics as low as .72, when trained on topic independent features. 
The problem of extractive text summarization for a collection of documents is deﬁned as the problem of selecting a small subset of sentences so that the contents and meaning of the original document set are preserved in the best possible way. In this paper we describe the linear programming-based global optimization model to rank and extract the most relevant sentences to a summary. We introduce three different objective functions being optimized. These functions deﬁne a relevance of a sentence that is being maximized, in different manners, such as: coverage of meaningful words of a document, coverage of its bigrams, or coverage of frequent sequences of words. We supply here an overview of our system’s participation in the MultiLing contest of SIGDial 2015. 
This paper describes the results of the Call Centre Conversation Summarization task at Multiling’15. The CCCS task consists in generating abstractive synopses from call centre conversations between a caller and an agent. Synopses are summaries of the problem of the caller, and how it is solved by the agent. Generating them is a very challenging task given that deep analysis of the dialogs and text generation are necessary. Three languages were addressed: French, Italian and English translations of conversations from those two languages. The ofﬁcial evaluation metric was ROUGE-2. Two participants submitted a total of four systems which had trouble beating the extractive baselines. The datasets released for the task will allow more research on abstractive dialog summarization. 
In this paper, we evaluate our automatic text summarization system in multilingual context. We participated in both single document and multi-document summarization tasks of MultiLing 2015 workshop. Our method involves clustering the document sentences into topics using a fuzzy clustering algorithm. Then each sentence is scored according to how well it covers the various topics. This is done using statistical features such as TF, sentence length, etc. Finally, the summary is constructed from the highest scoring sentences, while avoiding overlap between the summary sentences. This makes it language-independent, but we have to afford preprocessed data ﬁrst (tokenization, stemming, etc.). 
Online commenting to news articles provides a communication channel between media professionals and readers offering a crucial tool for opinion exchange and freedom of expression. Currently, comments are detached from the news article and thus removed from the context that they were written for. In this work, we propose a method to connect readers’ comments to the news article segments they refer to. We use similarity features to link comments to relevant article segments and evaluate both word-based and term-based vector spaces. Our results are comparable to state-of-theart topic modeling techniques when used for linking tasks. We demonstrate that article segments and comments representation are relevant to linking accuracy since we achieve better performances when similarity features are computed using similarity between terms rather than words. 
In this paper we present the approach and results of our participation in the 2015 MultiLing Single-document Summarization task. Our approach is based on the Principal Component Analysis (PCA) technique enhanced with lexical-semantic knowledge. For testing our approach, different conﬁgurations were set up, thus generating different types of summaries (i.e., generic and topic-focused), as well as testing some language-speciﬁc resources on top of the language-independent basic PCA approach, submitting a total of 6 runs for each selected language (English, German, and Spanish). Our participation in MultiLing has been very positive, ranking at intermediate positions when compared to the other participant systems, showing that PCA is a good technique for generating language-independent summaries, but the addition of lexical-semantic knowledge may heavily depend on the size and quality of the resources available for each language. 
We present our state of the art multilingual text summarizer capable of single as well as multi-document text summarization. The algorithm is based on repeated application of TextRank on a sentence similarity graph, a bag of words model for sentence similarity and a number of linguistic pre- and post-processing steps using standard NLP tools. We submitted this algorithm for two different tasks of the MultiLing 2015 summarization challenge: Multilingual Singledocument Summarization and Multilingual Multi-document Summarization. 
In this paper we present an overview of MultiLing 2015, a special session at SIGdial 2015. MultiLing is a communitydriven initiative that pushes the state-ofthe-art in Automatic Summarization by providing data sets and fostering further research and development of summarization systems. There were in total 23 participants this year submitting their system outputs to one or more of the four tasks of MultiLing: MSS, MMS, OnForumS and CCCS. We provide a brief overview of each task and its participation and evaluation.  @essex.ac.uk tasks to promote research in summarizing human dialog in online fora and customer call centers. This report provides an outline of the four tasks MultiLing supported at SIGdial; speciﬁcally the objective of each task, the data sets used by each task, and the level of participation and success by the research community within the task. The remainder of the paper is organised as follows: section §2 brieﬂy presents the Multilingual Single-document Summarization task, section §3 the Multilingual Multi-document summarization task, section §4 the Online Forum Summarization task, section §5 the Call-center Conversation summarization task, and ﬁnally we draw conclusions on the overall endeavour in section §6.  
The natural language generation (NLG) component of a spoken dialogue system (SDS) usually needs a substantial amount of handcrafting or a well-labeled dataset to be trained on. These limitations add signiﬁcantly to development costs and make cross-domain, multi-lingual dialogue systems intractable. Moreover, human languages are context-aware. The most natural response should be directly learned from data rather than depending on predeﬁned syntaxes or rules. This paper presents a statistical language generator based on a joint recurrent and convolutional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or predeﬁned grammar trees. Objective metrics suggest that this new model outperforms previous methods under the same experimental conditions. Results of an evaluation by human judges indicate that it produces not only high quality but linguistically varied utterances which are preferred compared to n-gram and rule-based systems. 
This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response. 
Recently, constrained Markov Bayesian polynomial (CMBP) has been proposed as a data-driven rule-based model for dialog state tracking (DST). CMBP is an approach to bridge rule-based models and statistical models. Recurrent Polynomial Network (RPN) is a recent statistical framework taking advantages of rulebased models and can achieve state-ofthe-art performance on the data corpora of DSTC-3, outperforming all submitted trackers in DSTC-3 including RNN. It is widely acknowledged that SLU’s reliability inﬂuences tracker’s performance greatly, especially in cases where the training SLU is poorly matched to the testing SLU. In this paper, this effect is analyzed in detail for RPN. Experiments show that RPN’s tracking result is consistently the best compared to rule-based and statistical models investigated on different SLUs including mismatched ones and demonstrate RPN’s is very robust to mismatched semantic parsers. 
In this paper we present a data-driven model for detecting opportunities and obligations for a robot to take turns in multi-party discussions about objects. The data used for the model was collected in a public setting, where the robot head Furhat played a collaborative card sorting game together with two users. The model makes a combined detection of addressee and turn-yielding cues, using multi-modal data from voice activity, syntax, prosody, head pose, movement of cards, and dialogue context. The best result for a binary decision is achieved when several modalities are combined, giving a weighted F1 score of 0.876 on data from a previously unseen interaction, using only automatically extractable features. 
In this paper, reinforcement learning (RL) is used to learn an efﬁcient turn-taking management model in a simulated slotﬁlling task with the objective of minimising the dialogue duration and maximising the completion task ratio. Turn-taking decisions are handled in a separate new module, the Scheduler. Unlike most dialogue systems, a dialogue turn is split into microturns and the Scheduler makes a decision for each one of them. A Fitted Value Iteration algorithm, Fitted-Q, with a linear state representation is used for learning the state to action policy. Comparison between a non-incremental and an incremental handcrafted strategies, taken as baselines, and an incremental RL-based strategy, shows the latter to be signiﬁcantly more efﬁcient, especially in noisy environments. 
Non-Sentential Utterances (NSUs) are short utterances that do not have the form of a full sentence but nevertheless convey a complete sentential meaning in the context of a conversation. NSUs are frequently used to ask follow up questions during interactions with question answer (QA) systems resulting into in-correct answers being presented to their users. Most of the current methods for resolving such NSUs have adopted rule or grammar based approach and have limited applicability. In this paper, we present a data driven statistical method for resolving such NSUs. Our method is based on the observation that humans identify keyword appearing in an NSU and place them in the context of conversation to construct a meaningful sentence. We adapt the keyword to question (K2Q) framework to generate natural language questions using keywords appearing in an NSU and its context. The resulting questions are ranked using different scoring methods in a statistical framework. Our evaluation on a data-set collected using mTurk shows that the proposed method perform signiﬁcantly better than the previous work that has largely been rule based. 
Technical systems evolve from simple dedicated task solvers to cooperative and competent assistants, helping the user with increasingly complex and demanding tasks. For this, they may proactively take over some of the users responsibilities and help to ﬁnd or reach a solution for the user’s task at hand, using e.g., Artiﬁcial Intelligence (AI) Planning techniques. However, this intertwining of user-centered dialog and AI planning systems, often called mixed-initiative planning (MIP), does not only facilitate more intelligent and competent systems, but does also raise new questions related to the alignment of AI and human problem solving. In this paper, we describe our approach on integrating AI Planning techniques into a dialog system, explain reasons and effects of arising problems, and provide at the same time our solutions resulting in a coherent, userfriendly and efﬁcient mixed-initiative system. Finally, we evaluate our MIP system and provide remarks on the use of explanations in MIP-related phenomena. 
In this paper, we present a data-driven approach for detecting instances of miscommunication in dialogue system interactions. A range of generic features that are both automatically extractable and manually annotated were used to train two models for online detection and one for offline analysis. Online detection could be used to raise the error awareness of the system, whereas offline detection could be used by a system designer to identify potential flaws in the dialogue design. In experimental evaluations on system logs from three different dialogue systems that vary in their dialogue strategy, the proposed models performed substantially better than the majority class baseline models. 
Dialogue Management (DM) is a key issue in Spoken Dialogue System. Most of the existing data-driven DM schemes train the dialogue policy for some specific domain (or vertical domain), only using the dialogue corpus in this domain, which might suffer from the scarcity of dialogue corpus in some domains. In this paper, we divide Dialogue Act (DA), as semantic representation of utterance, into DA type and slot parameter, where the former one is domain-independent and the latter one is domain-speciﬁc. Firstly, based on multiple-domain dialogue corpus, the DA type prediction model is trained via Recurrent Neutral Networks (RNN). Moreover, DA type decision problem is modeled as a multi-order POMDP, and transformed to be a one-order MDP with continuous states, which is solved by Natural Actor Critic (NAC) algorithm and applicable for every domain. Furthermore, a slot parameter selection scheme is designed to generate a complete machine DA according to the features of speciﬁc domain, which yields the Multi-domain Corpus based Dialogue Management (MCDM) scheme. Finally, extensive experimental results illustrate the performance improvement of the MCDM scheme, compared with the existing schemes. 
Adapting Spoken Dialogue Systems to the user is supposed to result in more efﬁcient and successful dialogues. In this work, we present an evaluation of a quality-adaptive strategy with a user simulator adapting the dialogue initiative dynamically during the ongoing interaction and show that it outperforms conventional non-adaptive strategies and a random strategy. Furthermore, we indicate a correlation between Interaction Quality and dialogue completion rate, task success rate, and average dialogue length. Finally, we analyze the correlation between task success and interaction quality in more detail identifying the usefulness of interaction quality for modelling the reward of reinforcement learning strategy optimization. 
Understanding contextual information is key to detecting metaphors in discourse. Most current work aims at detecting metaphors given a single sentence, thus focusing mostly on local contextual cues within a short text. In this paper, we present a novel approach that explicitly leverages global context of a discourse to detect metaphors. In addition, we show that syntactic information such as dependency structures can help better describe local contextual information, thus improving detection results when combined. We apply our methods on a newly annotated online discussion forum, and show that our approach outperforms the state-of-the-art baselines in previous literature. 
Ideally, the users of spoken dialogue systems should be able to speak at their own tempo. The systems thus need to correctly interpret utterances from various users, even when these utterances contain disﬂuency. In response to this issue, we propose an approach based on a posteriori restoration for incorrectly segmented utterances. A crucial part of this approach is to classify whether restoration is required or not. We improve the accuracy by adapting the classiﬁer to each user. We focus on the dialogue tempo of each user, which can be obtained during dialogues, and determine the correlation between each user’s tempo and the appropriate thresholds for the classiﬁcation. A linear regression function used to convert the tempos into thresholds is also derived. Experimental results showed that the proposed user adaptation for two classiﬁers, thresholding and decision tree, improved the classiﬁcation accuracies by 3.0% and 7.4%, respectively, in ten-fold cross validation. 
Inspired by studies of human-human conversations, we present methods for incrementally coordinating speech production with listeners’ visual foci of attention. We introduce a model that considers the demands and availability of listeners’ attention at the onset and throughout the production of system utterances, and that incrementally coordinates speech synthesis with the listener’s gaze. We present an implementation and deployment of the model in a physically situated dialog system and discuss lessons learned. 
Gaussian processes reinforcement learning provides an appealing framework for training the dialogue policy as it takes into account correlations of the objective function given different dialogue belief states, which can signiﬁcantly speed up the learning. These correlations are modelled by the kernel function which may depend on hyper-parameters. So far, for real-world dialogue systems the hyperparameters have been hand-tuned, relying on the designer to adjust the correlations, or simple non-parametrised kernel functions have been used instead. Here, we examine different kernel structures and show that it is possible to optimise the hyperparameters from data yielding improved performance of the resulting dialogue policy. We conﬁrm this in a real user trial. 
This paper introduces a novel approach to eliminate the domain dependence of dialogue state and action representations, such that dialogue policies trained based on the proposed representation can be transferred across different domains. The experimental results show that the policy optimised in a restaurant search domain using our domain-independent representations can be deployed to a laptop sale domain, achieving a task success rate very close (96.4% relative) to that of the policy optimised on in-domain dialogues. 
Statistical spoken dialogue systems have the attractive property of being able to be optimised from data via interactions with real users. However in the reinforcement learning paradigm the dialogue manager (agent) often requires signiﬁcant time to explore the state-action space to learn to behave in a desirable manner. This is a critical issue when the system is trained on-line with real users where learning costs are expensive. Reward shaping is one promising technique for addressing these concerns. Here we examine three recurrent neural network (RNN) approaches for providing reward shaping information in addition to the primary (task-orientated) environmental feedback. These RNNs are trained on returns from dialogues generated by a simulated user and attempt to diffuse the overall evaluation of the dialogue back down to the turn level to guide the agent towards good behaviour faster. In both simulated and real user scenarios these RNNs are shown to increase policy learning speed. Importantly, they do not require prior knowledge of the user’s goal. 
In this study, we examine the effects of using a game for encouraging the use of a spoken dialogue system. As a case study, we developed a word-chain game, called Shiritori in Japanese, and released the game as a module in a Japanese Android/iOS app, Onsei-Assist, which is a Siri-like personal assistant based on a spoken dialogue technology. We analyzed the log after the release and conﬁrmed that the game can increase the number of user utterances. Furthermore, we discovered a positive side effect, in which users who have played the game tend to begin using non-game modules. This suggests that just adding a game module to the system can improve user engagement with an assistant agent. 
Using the Internet for the collection of data is quite common these days. This process is called crowdsourcing and enables the collection of large amounts of data at reasonable costs. While being an inexpensive method, this data typically is of lower quality. Filtering data sets is therefore required. The occurring errors can be classiﬁed into different groups. There are technical issues and human errors. For speech recording, technical issues could be a noisy background. Human errors arise when the task is misunderstood. We employ several techniques for recognizing errors and eliminating faulty data sets in user input data for a Spoken Dialog System (SDS). Furthermore, we compare three different kinds of questionnaires (QNRs) for a given set of seven tasks. We analyze the characteristics of the resulting data sets and give a recommendation which type of QNR might be the most suitable one for a given purpose. 
We have previously presented HALEF–an open-source spoken dialog system–that supports telephonic interfaces and has a distributed architecture. In this paper, we extend this infrastructure to be cloud-based, and thus truly distributed and scalable. This cloud-based spoken dialog system can be accessed both via telephone interfaces as well as through web clients with WebRTC/HTML5 integration, allowing in-browser access to potentially multimodal dialog applications. We demonstrate the versatility of the system with two conversation applications in the educational domain. 
We present an end-to-end conversational system for TV program discovery that uniquely combines advanced technologies for NLU, Dialog Management, Knowledge Graph Inference and Personalized Recommendations. It uses a semantically rich relational representation of dialog state and knowedge graph inference for queries. The recommender combines evidence for user preferences from multiple modalities such as dialog, user viewing history and activity logs. It is tightly integrated with the Dialog System, especially for explanations of recommendations. A demo of the system on a iPad will be shown. 
 This paper describes the work-in-progress prototype of a dialog system that simulates a virtual patient (VP) consultation. We report some challenges and difﬁculties that are found during its development, especially in managing the interaction and the vocabulary from the medical domain.  
Despite the prevalence of libraries that provide speech recognition and text-tospeech synthesis “in the cloud”, it remains difﬁcult for developers to create user-friendly, consistent spoken language interfaces to their mobile applications. In this paper, we present the Speechify / Cohort libraries for rapid speech enabling of Android applications. The Speechify library wraps several publicly available speech recognition and synthesis APIs, incorporates state-of-the-art voice activity detection and simple and ﬂexible hybrid speech recognition, and allows developers to experiment with different modes of user interaction. The Cohort library, built on a stripped-down version of OpenDial, facilitates ﬂexible interaction between and within “Speechiﬁed” mobile applications. 
The unsupervised extraction of narrative schemas—sets of events with associated argument chains—has been explored and evaluated from many angles (Chambers and Jurafsky, 2009; Jans et al. 2012; Balasubramanian et al., 2013; Pichotta and Mooney 2014). While the extraction process and evaluation of the products has been well-researched and debated, little insight has been garnered on properties of narrative schemas themselves. We examine how well extracted narrative schemas align with existing document categories using a novel procedure for retrieving candidate category alignments. This was tested against alternative baseline alignment procedures that disregard some of the complex information the schemas contain. We ﬁnd that a classiﬁer built with all available information in a schema is more precise than a classiﬁer built with simpler subcomponents. Coreference information plays an crucial role in schematic knowledge. 
Event Detection (ED) aims to identify instances of speciﬁed types of events in text, which is a crucial component in the overall task of event extraction. The commonly used features consist of lexical, syntactic, and entity information, but the knowledge encoded in the Abstract Meaning Representation (AMR) has not been utilized in this task. AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. In this paper, we demonstrate the effectiveness of AMR to capture and represent the deeper semantic contexts of the trigger words in this task. Experimental results further show that adding AMR features on top of the traditional features can achieve 67.8% (with 2.1% absolute improvement) F-measure (F1), which is comparable to the state-of-the-art approaches. 
A web search engine usually returns a long list of documents and it may be difﬁcult for users to navigate through this collection and ﬁnd the most relevant ones. We present an approach to post-retrieval snippet clustering based on pattern structures construction on augmented syntactic parse trees. Since an algorithm may be too slow for a typical collection of snippets, we propose a reduction method that allows us to construct a reduced pattern structure and make it scalable. Our algorithm takes into account discourse information to make clustering results independent of how information is distributed between sentences. 
This paper introduces MediaMeter, an application that works to detect and track emergent topics in the US online news media. What makes MediaMeter unique is its reliance on a labeling algorithm which we call WikiLabel, whose primary goal is to identify what news stories are about by looking up Wikipedia. We discuss some of the major news events that were successfully detected and how it compares to prior work. 
News360 is the news aggregation system with personalization. Initially created for English, it was recently adapted for German. In this paper, we show that it is possible to adapt such systems automatically, without any manual labour, using only open knowledge bases and Wikipedia dumps. We propose a method for adaptation named entity linking and classiﬁcation to target language. We show that even though the quality of German system is worse than the quality of English one, this method allows to bootstrap a new language for the system very quickly and fully automatically. 
Stories are the most natural ways for people to deal with information about the changing world. They provide an efﬁcient schematic structure to order and relate events according to some explanation. We describe (1) a formal model for representing storylines to handle streams of news and (2) a ﬁrst implementation of a system that automatically extracts the ingredients of a storyline from news articles according to the model. Our model mimics the basic notions from narratology by adding bridging relations to timelines of events in relation to a climax point. We provide a method for deﬁning the climax score of each event and the bridging relations between them. We generate a JSON structure for any set of news articles to represent the different stories they contain and visualize these stories on a timeline with climax and bridging relations. This visualization helps inspecting the validity of the generated structures. 
This paper describes a new method for narrative frame alignment that extends and supplements models reliant on graph theory from the domain of ﬁction to the domain of nonﬁction news articles. Preliminary tests of this method against a corpus of 24 articles related to private security ﬁrms operating in Iraq and the Blackwater shooting of 2007 show that prior methods utilizing a graph similarity approach can work but require a narrower entity set than commonly occurs in non-ﬁction texts. They also show that alignment procedures sensitive to abstracted event sequences can accurately highlight similar narratological moments across documents despite syntactic and lexical differences. Evaluation against LDA for both the event sequence lists and source sentences is provided for performance comparison. Next steps include merging these semantic and graph analytic approaches and expanding the test corpus. 
Grammatical error diagnosis is an essential part in a language-learning tutoring system. Participating in the second Chinese grammar error detection task, we proposed a new system which measures the likelihood of sentences generated by deleting, inserting, or exchanging characters or words. Two sentence likelihood functions were proposed based on frequencies of spaceremoved version of Google n-grams. The best system achieved a precision of 23.4% and a recall of 36.4% in the identification level. 
We present Collocation Assistant, a prototype of a collocational aid designed to promote the collocational competence of learners of Japanese as a second language (JSL). Focusing on noun-verb constructions, the tool automatically ﬂags possible collocation errors and suggests better collocations by using corrections extracted from a large annotated Japanese language learner corpus. Each suggestion includes several usage examples to help learners choose the best candidate. In a preliminary user study with JSL learners, Collocation Assistant received positive feedback, and the results indicate that the system is helpful to assist learners in choosing correct word combinations in Japanese. 
We propose a strategy for the semiautomatic generation of learning material for reading-comprehension tests, guided by semantic relations embedded in expository texts. Our approach combines methods from the areas of information extraction and paraphrasing in order to present a language teacher with a set of candidate multiple-choice questions and answers that can be used for verifying a language learners reading capabilities. We implemented a web-based prototype showing the feasibility of our approach and carried out a pilot user evaluation that resulted in encouraging feedback but also pointed out aspects of the strategy and prototype implementation which need improvements. 
We propose WordNews, a web browser extension that allows readers to learn a second language vocabulary while reading news online. Injected tooltips allow readers to look up selected vocabulary and take simple interactive tests. We discover that two key system components needed improvement, both which stem from the need to model context. These two issues are real-world word sense disambiguation (WSD) to aid translation quality and constructing interactive tests. For the ﬁrst, we start with Microsoft’s Bing translation API but employ additional dictionary-based heuristics that signiﬁcantly improve translation in both coverage and accuracy. For the second, we propose techniques for generating appropriate distractors for multiple-choice word mastery tests. Our preliminary user survey conﬁrms the need and viability of such a language learning platform. 
We introduce a method that extracts keywords in a language with the help of the other. The method involves estimating preferences for topical keywords and fusing language-specific word statistics. At run-time, we transform parallel articles into word graphs, build crosslingual edges for word statistics integration, and exploit PageRank with word keyness information for keyword extraction. We apply our method to keyword analysis and language learning. Evaluation shows that keyword extraction benefits from cross-language information and language learners benefit from our keywords in reading comprehension test. 
This paper presents an annotation project that explores the relationship between textual entailment and short answer scoring (SAS). We annotate entailment relations between learner and target answers in the Corpus of Reading Comprehension Exercises for German (CREG) with a ﬁnegrained label inventory and compare them in various ways to correctness scores assigned by teachers. Our main ﬁnding is that although both tasks are clearly related, not all of our entailment tags can be directly mapped to SAS scores and that especially the area of partial entailment covers instances that are problematic for automatic scoring and need further investigation. 
Multiple Choice Question (MCQ) plays a major role in educational assessment as well as in active learning. In this paper we present a system that generates MCQs automatically using a sports domain text as input. All the sentences in a text are not capable of generating MCQs; the first step of the system is to select the informative sentences. We propose a novel technique to select informative sentences by using topic modeling and parse structure similarity. The parse structure similarity is computed between the parse structure of an input sentence and a set of reference parse structures. In order to compile the reference set we use a number of existing MCQs collected from the web. Keyword selection is done with the help of occurrence of domain specific word and named entity word in the sentence. Distractors are generated using a set of rules and name dictionary. Experimental results demonstrate that the proposed technique is quite accurate. 
Japan’s public broadcasting corporation, NHK, launched “News Web Easy” in April 2012 1. It provides users with ﬁve simpliﬁed news scripts (easy Japanese news) on a daily basis. This web service provides users with ﬁve daily simpliﬁed news scripts of “easy” Japanese news. Since its inception, this service has been favorably received both in Japan and overseas. Users particularly appreciate its value as a Japanese learning and teaching resource. In this paper, we discuss this service and its possible contribution to language education. We focus on difﬁculty levels of sentence-end expressions, compiled from the news, that create ambiguity and problems when rewriting news items. These are analyzed and compared within regular news and News Web Easy, and their difﬁculty is assessed based on Japanese learners’ reading comprehension levels. Our results revealed that current rewriting of sentence-end expressions in News Web Easy is appropriate. We further identiﬁed features of these expressions that contribute to difﬁculty in comprehension. 
Multi-word expressions (MWEs) have been recognized as important linguistic information and much research has been conducted especially on their extraction and interpretation. On the other hand, they have hardly been used in real application areas. While those who are learning English as a second language (ESL) use MWEs in their writings just like native speakers, MWEs haven’t been taken into consideration in grammatical error correction tasks. In this paper, we investigate the grammatical error correction method using MWEs. Our method proposes a straightforward application of MWEs to grammatical error correction, but experimental results show that MWEs have a beneﬁcial effect on grammatical error correction. 
Heritage language learners are learners of the primary language of their parents which they might have been exposed to but have not learned it as a language they can fluently use to communicate with other people. Salinlahi, an Interactive Learning Environment, was developed to teach these young Filipino heritage learners about basic Filipino vocabulary while Salinlahi II included a support for collaborative learning. With the aim of teaching learners with basic knowledge in Filipino we developed Salinlahi III to teach higher level lessons focusing on Filipino grammar and sentence construction. An internal evaluation of the system has shown that the user interface and feedback of the tutor was appropriate. Moreover, in an external evaluation of the system, experimental and controlled field tests were done and results showed that there is a positive learning gain after using the system. 
The interest and demand to foreign language learning are increased tremendously along with the globalization and freedom of movement in the world. Today, the technological developments allow the creation of supportive materials for foreign language learners. However, the language acquisition between languages with high typological differences still poses challenges for this area and the learning task it self. This paper introduces our preliminary study for building an educational application to help foreign language learning between Turkish and English. The paper presents the use of ﬁnite state technology for building a Turkish word synthesis system (which allows to choose word-related features among predeﬁned grammatical afﬁx categories such as tense, modality and polarity etc...) and a wordlevel translation system between the languages in focus. The developed system is observed to outperform the popular online translation systems for word-level translation in terms of grammatically correct outputs. 
The foreign learners are not easy to learn Chinese as a second language. Because there are many special rules different from other languages in Chinese. When the people learn Chinese as a foreign language usually make some grammatical errors, such as missing, redundant, selection and disorder. In this paper, we proposed the conditional random fields (CRFs) to detect the grammatical errors. The features based on statistical word and part-ofspeech (POS) pattern were adopted here. The relationships between words by part-of-speech are helpful for Chinese grammatical error detection. Finally, we according to CRF determined which error types in sentences. According to the observation of experimental results, the performance of the proposed model is acceptable in precision and recall rates. 
This paper describes our system in the Chinese Grammatical Error Diagnosis (CGED) task for learning Chinese as a Foreign Language (CFL). Our work adopts a hybrid model by integrating rulebased method and n-gram statistical method to detect Chinese grammatical errors, identify the error type and point out the position of error in the input sentences. Tri-gram is applied to disorder mistake. And the rest of mistakes are solved by the conservation rules sets. Empirical evaluation results demonstrate the utility of our CGED system. 
Noisy user-generated text poses problems for natural language processing. In this paper, we show that this statement also holds true for the Irish language. Irish is regarded as a low-resourced language, with limited annotated corpora available to NLP researchers and linguists to fully analyse the linguistic patterns in language use in social media. We contribute to recent advances in this area of research by reporting on the development of part-ofspeech annotation scheme and annotated corpus for Irish language tweets. We also report on state-of-the-art tagging results of training and testing three existing POStaggers on our new dataset. 
Dialect features typically do not make it into formal writing, but ﬂourish in social media. This enables largescale variational studies. We focus on three phonological features of African American Vernacular English and their manifestation as spelling variations on Twitter. We discuss to what extent our data can be used to falsify eight sociolinguistic hypotheses. To go beyond the spelling level, we require automatic analysis such as POS tagging, but social media language still challenges language technologies. We show how both newswire- and Twitter-adapted stateof-the-art POS taggers perform significantly worse on AAVE tweets, suggesting that large-scale dialect studies of language variation beyond the surface level are not feasible with out-ofthe-box NLP tools. 
 AiTi Aw* Institute for Infocomm Research (I2R), A*STAR, Singapore aaiti@i2r.a-star.edu.sg ous well-known normalization approaches.  The use of social network services and microblogs, such as Twitter, has created valuable text resources, which contain extremely noisy text. Twitter messages contain so much noise that it is difficult to use them in natural language processing tasks. This paper presents a new approach using the maximum entropy model for normalizing Tweets. The proposed approach addresses words that are unseen in the training phase. Although the maximum entropy needs a training dataset to adjust its parameters, the proposed approach can normalize unseen data in the training set. The principle of maximum entropy emphasizes incorporating the available features into a uniform model. First, we generate a set of normalized candidates for each out-ofvocabulary word based on lexical, phonemic, and morphophonemic similarities. Then, three different probability scores are calculated for each candidate using positional indexing, a dependency-based frequency feature and a language model. After the optimal values of the model parameters are obtained in a training phase, the model can calculate the final probability value for candidates. The approach achieved an 83.12 BLEU score in testing using 2,000 Tweets. Our experimental results show that the maximum entropy approach significantly outperforms previ-  
It is widely accepted that translating usergenerated (UG) text is a difﬁcult task for modern statistical machine translation (SMT) systems. The translation quality metrics typically used in the SMT literature reﬂect the overall quality of the system output but provide little insight into what exactly makes UG text translation difﬁcult. This paper analyzes in detail the behavior of a state-of-the-art SMT system on ﬁve different types of informal text. The results help to demystify the poor SMT performance experienced by researchers who use SMT as an intermediate step of their UG-NLP pipeline, and to identify translation modeling aspects that the SMT community should more urgently address to improve translation of UG data. 
User-generated contents (UGC) represent an important source of information for governments, companies, political candidates and consumers. However, most of the Natural Language Processing tools and techniques are developed from and for texts of standard language, and UGC is a type of text especially full of creativity and idiosyncrasies, which represents noise for NLP purposes. This paper presents UGCNormal, a lexicon-based tool for UGC normalization. It encompasses a tokenizer, a sentence segmentation tool, a phonetic-based speller and some lexicons, which were originated from a deep analysis of a corpus of product reviews in Brazilian Portuguese. The normalizer was evaluated in two different data sets and carried out from 31% to 89% of the appropriate corrections, depending on the type of text noise. The use of UGCNormal was also validated in a task of POS tagging, which improved from 91.35% to 93.15% in accuracy and in a task of opinion classification, which improved the average of F1-score measures (F1-score positive and F1-score negative) from 0.736 to 0.758. 1. Introduction The increasing volume of text posted by users on the web is regarded as an extremely useful opportunity to reveal public opinion on many issues. For a variety of reasons, governments, companies, political candidates, and consumers want to explore such web content. This type of text is referred to in the literature as UGC (usergenerated content) or EWoM (electronic word-ofmouth). However, due to the large amount of data available, it is impossible for humans to analyze  all available UGC for most issues. As a result, processing and analyzing UGC became a task of NLP (Natural Language Processing). The problem is that, until now, almost all NLP tools and techniques were developed from, and for, standard language text, but UGC displays a range of creative and idiosyncratic differences, which represent noise for NLP purposes. In order to reuse the NLP tools to process UGC, the normalization or standardization of this genre of text became an essential preprocessing step, aiming to make UGC as close as possible to standard language. The level of noise in UGC varies depending on the social media in which it is posted. Short messages (SMS and microblogs, such as Twitter) tend to be much noisier than texts posted in blogs and sites of reviews, as users need to be creative to deal with character limitations (140 characters for Twitter and 160 for SMS). The challenge for NLP is to determine the aspects in which UGC deviates from standard language and develop strategies to deal with the normalization of these aspects. Many of UGC’s deviations from standard language are motivated by wordplay (U=you, 4=for), by the need to save space (short messages have a limited length), by the influence of pronunciation, or even by a low level of literacy. Regardless of the causes of UGC deviations from standard language, if they are recurrent, they need to be addressed by normalization processes. Some characteristics of UGC are languageindependent, as the long vowels used to express emphasis (Gooooooooooooood) and the unconventional use of lower and upper cases (proper names in lowercase and common words in uppercase). Other characteristics are languagedependent, such as the apostrophe suppression in English (wont=won’t) and the omission of  38 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 38–47, Beijing, China, July 31, 2015. c 2015 Association for Computational Linguistics  diacritics and cedilla under “c” in Portuguese (eleicao=eleição). UGC differs from the standard language mainly in the lexical level. For this reason, the normalization problem is approached by strategies of word correction (the lexical items of the UGC are treated as “errors”) and strategies for machine translation (the UGC is treated as source language and the standard language as target language). We address herein the normalization process as a set of procedures that deal with different types of deviation. The input consists of consumer reviews on electronic products. The main purpose is to convert such texts, as closely as possible, into the form expected by NLP tools trained on corpora of standard language. This work was preceded by the detection and analysis of out-of-vocabulary1 (OOV) words in a corpus of product reviews (Hartmann et al. 2014). In another preliminary investigation, we have found other different types of deviations and their impact on a tagging task (Duran et al., 2014). Such diagnosis has resulted in the procedures that integrate the normalization system proposed here. The remainder of this paper is organized as follows. Section 2 presents related works. Section 3 describes the characteristics of the product review corpus and the problems they pose to normalization. Section 4 reports the methodology used to construct the normalization tool. Section 5 describes and discusses the evaluation and validation results. Finally, in Section 6, we make some final remarks and outline future work. 2. Related works Text normalization is a term used to convey the idea of converting the format of a text to meet the requirements of a given purpose. There are many text normalization processes reported in the NLP literature and they vary in: i) the genre of the input text; ii) the desired output format; iii) the purpose of the normalization, and iv) the method used to perform the task. It is important to take into account such characteristics to clearly define what “text normalization” means in each context. The input text may or may not be well-written. The task of normalizing text from a newspaper (as  in Schlippe et al., 2012) is quite different from normalizing texts produced by non-professional internet users, i.e. UGC. In addition, the normalization of UGC may depend on the social media used. For example, there are substantial differences between short message texts (SMS and microblogs), on-line chats and users’ reviews. Short messages and chats deviate much more from the standard language than users’ reviews and are commonly regarded as “noisy texts”. The normalization processes of short messages, such as SMS and Twitter messages (Contractor et. al. 2010; Liu et al. 2011; Han et al., 2013; Bali, 2013; Chrupała, 2014) and longer UGC texts, such as reviews and blogs, have much in common, but the differences are sufficiently significant to justify addressing them separately. Different normalization purposes may require the use of substantially different normalization procedures For example, converting text-tospeech requires the expansion of acronyms and abbreviations, as well as the conversion of numeric or mathematical expressions into words (Boros et al., 2012, Schlippe et al. 2012); conversely, normalization for purpose of storing data may perform the reduction of word forms into their stems. Even a “noisy text” of UGC may be normalized for different purposes. For example, while Mosquera et al. (2012) use normalization to improve the accessibility of web content, Aw et al. (2006) and Contractor et al. (2010) see the normalization as a prerequisite for other automatic processing tasks. Approaches to text normalization may be roughly divided into two groups: those that “translate” non-standard language into standard language using contextual information (based on language models), and those that replace OOV words (lexical-based) by suitable forms in the standard language. For the latter, lexical information is essential; for the former, parallel corpora of non-standard and standard language are required. Lexical-based approaches are commonly used to normalize general texts, whereas machine-translation approaches are usually an option to tackle SMS normalization. Aw et al. (2006) first proposed to regard SMS normalization as a machine translation problem. Many other studies have followed this approach  
Our submission to the W-NUT Named Entity Recognition in Twitter task closely follows the approach detailed by Cherry and Guo (2015), who use a discriminative, semi-Markov tagger, augmented with multiple word representations. We enhance this approach with updated gazetteers, and with infused phrase embeddings that have been adapted to better predict the gazetteer membership of each phrase. Our system achieves a typed F1 of 44.7, resulting in a third-place ﬁnish, despite training only on the ofﬁcial training set. A post-competition analysis indicates that also training on the provided development data improves our performance to 54.2 F1. 
In this paper we propose a differential evolution (DE) based named entity recognition (NER) system in twitter data. In the ﬁrst step, we develop various NER systems using different combinations of the features. We implemented these features without using any domain-speciﬁc features and/or resources. As a base classiﬁer we use Conditional Random Field (CRF). In the second step, we propose a DE based feature selection approach to determine the most relevant set of features and its context information. The optimized feature set applied to the training set yields the precision, recall and Fmeasure values of 60.68%, 29.65% and 39.84%, respectively for the ﬁne-grained named entity (NE) types. When we consider only the coarse-grained NE types, it shows the precision, recall and F-measure values of 63.43%, 51.44% and 56.81%, respectively. 
Twitter is a type of social media that contains diverse user-generated texts. Traditional models are not applicable to tweet data because the text style is not as grammaticalized as that of newswire. In this paper, we construct word embeddings via canonical correlation analysis (CCA) on a considerable amount of tweet data and show the efﬁcacy of word representation. Besides word embedding, we use partof-speech (POS) tags, chunks, and brown clusters induced from Wikipedia as features. Here, we describe our system and present the ﬁnal results along with their analysis. Our model achieves an F1 score of 37.21% with entity types and distinguishes 53.01% of the entity boundaries. 
This paper describes the Twitter lexical normalization system submitted by IHS R&D Belarus team for the ACL 2015 workshop on noisy user-generated text. The proposed system consists of two components: a CRFbased approach to identify possible normalization candidates, and a post-processing step in an attempt to normalize words that do not have normalization variants in the lexicon. Evaluation on the test data set showed that our unconstrained system achieved the Fmeasure of 0.8272 (rank 1 out of 5 submissions for the unconstrained mode, rank 2 out of all 11 submissions). 
User generated content often contains non-standard words that hinder effective automatic text processing. In this paper, we present a system we developed to perform lexical normalization for English Twitter text. It first generates candidates based on past knowledge and a novel string similarity measurement and then selects a candidate using features learned from training data. The system has a constrained mode and an unconstrained mode. The constrained mode participated in the W-NUT noisy English text normalization competition (Baldwin et al., 2015) and achieved the best F1 score. 
In this article we describe the microtext normalization system we have used to participate in the Normalization of Noisy Text Task of the ACL W-NUT 2015 Workshop. Our normalization system was originally developed for text mining tasks on Spanish tweets. Our main goals during its development were ﬂexibility, scalability and maintainability, in order to test a wide variety of approximations to the problem at hand with minimum effort. We will pay special attention to the process of adapting the components of our system to deal with English tweets which, as we will show, was achieved without major modiﬁcations of its base structure. 
In this paper we report our work for normalization of noisy text in Twitter data. The method we propose is hybrid in nature that combines machine learning with rules. In the ﬁrst step, supervised approach based on conditional random ﬁeld is developed, and in the second step a set of heuristics rules is applied to the candidate wordforms for the normalization. The classiﬁer is trained with a set of features which were are derived without the use of any domain-speciﬁc feature and/or resource. The overall system yields the precision, recall and F-measure values of 90.26%, 71.91% and 80.05% respectively for the test dataset. 
As a participant in the W-NUT Lexical Normalization for English Tweets challenge, we use deep learning to address the constrained task. Specifically, we use a combination of two augmented feed forward neural networks, a flagger that identifies words to be normalized and a normalizer, to take in a single token at a time and output a corrected version of that token. Despite avoiding off-the-shelf tools trained on external data and being an entirely context-free model, our system still achieved an F1-score of 81.49%, comfortably surpassing the next runner up by 1.5% and trailing the second place model by only 0.26%. 
Brown clusters enable POS taggers to generalize better to words that did not occur in the labeled data, clustering distributionally similar seen and unseen words, thereby making models more robust to sparsity effects and domain shifts. However, Brown clustering is a transductive clustering method, and OOV effects still arise. Words neither in the labeled data nor in the unlabeled data cannot be assigned to a cluster, and hence, are frequently mis-tagged. This paper presents a simple method of learning ﬁnite state automata from Brown clusters that accept and give representations to truly unseen words. We show that using automata rather than Brown clusters lead to signiﬁcant improvements in performance in unsupervised cross-domain POS tagging. 
Part-of-Speech (POS) tagging is a key step in many NLP algorithms. However, tweets are difﬁcult to POS tag because there are many phenomena that frequently appear in Twitter that are not as common, or are entirely absent, in other domains: tweets are short, are not always written maintaining formal grammar and proper spelling, and abbreviations are often used to overcome their restricted lengths. Arabic tweets also show a further range of linguistic phenomena such as usage of different dialects, romanised Arabic and borrowing foreign words. In this paper, we present an evaluation and a detailed error analysis of stateof-the-art POS taggers for Arabic when applied to Arabic tweets. The accuracy of standard Arabic taggers is typically excellent (96-97%) on Modern Standard Arabic (MSA) text ; however,their accuracy declines to 49-65% on Arabic tweets. Further, we present our initial approach to improve the taggers’ performance. By making improvements based on observed errors, we are able to reach 74% tagging accuracy. 
Previous research has demonstrated the benefits of using linguistic resources to analyze a user’s social media profiles in order to learn information about that user. However, numerous linguistic resources exist, raising the question of choosing the appropriate resource. This paper compares Extended WordNet Domains with DBpedia. The comparison takes the form of an investigation of the relationship between users’ descriptions of their knowledge and background on LinkedIn with their description of the same characteristics on Twitter. The analysis applied in this study consists of four parts. First, information a user has shared on each service is mined for keywords. These keywords are then linked with terms in DBpedia/Extended WordNet Domains. These terms are ranked in order to generate separate representations of the user’s interests and knowledge for LinkedIn and Twitter. Finally, the relationship between these separate representations is examined. In a user study with eight participants, the performance of this analysis using DBpedia is compared with the performance of this analysis using Extended WordNet Domains. The best results were obtained when DBpedia was used. 
The number of tools and services for sentiment analysis is increasing rapidly. Unfortunately, the lack of standard formats hinders interoperability. To tackle this problem, previous works propose the use of the NLP Interchange Format (NIF) as both a common semantic format and an API for textual sentiment analysis. However, that approach creates a gap between textual and sentiment analysis that hampers multimodality. This paper presents a multimedia extension of NIF that can be leveraged for multimodal applications. The application of this extended model is illustrated with a service that annotates online videos with their sentiment and the use of SPARQL to retrieve results for different modes. 
This note describes OpenWordnet-PT, an automatically created, manually curated wordnet for Portuguese and introduces the newly developed web interface we are using to speed up its manual curation. OpenWordNet-PT is part of a collection of wordnets for various languages, jointly described and distributed through the Open MultiLingual WordNet and the Global WordNet Association. OpenWordnet-PT has been primarily distributed, from the beginning, as RDF ﬁles along with its model description in OWL, and it is freely available for download. We contend the creation of such large, distributed and linkable lexical resources is on the cusp of revolutionizing multilingual language processing to the next truly semantic level. But to get there, there is a need for user interfaces that allow ordinary users and (not only computational) linguists to help in the checking and cleaning up of the quality of the resource. We present our suggestion of one such web interface and describe its features supporting the collaborative curation of the data. This showcases the use and importance of its linked data features, to keep track of information provenance during the whole life-cycle of the RDF resource. 
We present sar-graphs, a knowledge resource that links semantic relations from factual knowledge graphs to the linguistic patterns with which a language can express instances of these relations. Sar-graphs expand upon existing lexicosemantic resources by modeling syntactic and semantic information at the level of relations, and are hence useful for tasks such as knowledge base population and relation extraction. We present a languageindependent method to automatically construct sar-graph instances that is based on distantly supervised relation extraction. We link sar-graphs at the lexical level to BabelNet, WordNet and UBY, and present our ongoing work on pattern- and relationlevel linking to FrameNet. An initial dataset of English sar-graphs for 25 relations is made publicly available, together with a Java-based API. 
Language resources are a cornerstone of linguistic research and for the development of natural language processing tools, but the discovery of relevant resources remains a challenging task. This is due to the fact that relevant metadata records are spread among different repositories and it is currently impossible to query all these repositories in an integrated fashion, as they use different data models and vocabularies. In this paper we present a ﬁrst attempt to collect and harmonize the metadata of different repositories, thus making them queriable and browsable in an integrated way. We make use of RDF and linked data technologies for this and provide a ﬁrst level of harmonization of the vocabularies used in the different resources by mapping them to standard RDF vocabularies including Dublin Core and DCAT. Further, we present an approach that relies on NLP and in particular word sense disambiguation techniques to harmonize resources by mapping values of attributes – such as the type, license or intended use of a resource – into normalized values. Finally, as there are duplicate entries within the same repository as well as across different repositories, we also report results of detection of these duplicates. 
Language resources are very often valuable assets which are offered to the public under the terms of licenses that determine which uses are allowed and under which circumstances. These licenses have been typically published as natural language texts whose speciﬁc contents cannot be easily processed by a computer. This paper proposes a structured representation for the most commonly used licenses for language resources, reusing existing vocabularies and extending the Open Digital Rights Language core model. Examples and guidelines to use the ‘Rights Information for Language Resources’ vocabulary are given. 
The interest in publishing language resources as linked data is increasing, as clearly corroborated by the recent growth of the Linguistic Linked Data cloud. However, the actual value of data published as linked data is the fact that it is linked across datasets, supporting integration and discovery of data. As the manual creation of links between datasets is costly and therefore does not scale well, automatic linking approaches are of great importance to increase the quality and degree of linking of the Linguistic Linked Data cloud. In this paper we examine an automatic approach to link four different datasets to each other: two terminologies, the European Migration Network (EMN) glossary as well as the Interactive Terminology for Europe (IATE), BabelNet, and the Manually Annotated Subcorpus (MASC) of the American National Corpus. We describe our methodology, present some results on the quality of the links and summarize our experiences with this small linking exercise We will make sure that the resources are added to the linguistic linked data cloud. 
 In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic ﬁeld, etc.) that can be used for either ﬁltering the pairs or performing an in-depth analysis of the results. The tuples were extracted from a combination of ConceptNet 5.0 and WordNet 4.0, and subsequently ﬁltered through automatic methods and crowdsourcing in order to ensure their quality. The dataset is freely downloadable1. An extension in RDF format, including also scripts for data processing, is under development.  
The present study describes recent developments of Chinese Wordnet, which has been reformatted using the lemon model and published as part of the Linguistic Linked Open Data Cloud. While lemon sufﬁces for modeling most of the structures in Chinese Wordnet at the lexical level, the model does not allow for ﬁnergrained distinction of a word sense, or meaning facets, a linguistic feature also attended to in Chinese Wordnet. As for the representation of synsets, we use the WordNet RDF ontology for integration’s sake. Also, we use another ontology proposed by the Global WordNet Association to show how Chinese Wordnet as Linked Data can be integrated into the Global WordNet Grid. 
We present a Portuguese↔English hybrid deep MT system based on an analysistransfer-synthesis architecture, with transfer being done at the level of deep syntax, a level that already includes a great deal of semantic information. The system received a few months of development, but its performance is already similar to that of baseline phrase-based MT, when evaluated using BLEU, and surpasses the baseline under human qualitative assessment. 
This paper describes a hybrid machine translation (HMT) system that employs several online MT system application program interfaces (APIs) forming a MultiSystem Machine Translation (MSMT) approach. The goal is to improve the automated translation of English – Latvian texts over each of the individual MT APIs. The selection of the best hypothesis translation is done by calculating the perplexity for each hypothesis. Experiment results show a slight improvement of BLEU score and WER (word error rate). 
We present a thorough analysis of a combination of a statistical and a transferbased system for English→Czech translation, Moses and TectoMT. We describe several techniques for inspecting such a system combination which are based both on automatic and manual evaluation. While TectoMT often produces bad translations, Moses is still able to select the good parts of them. In many cases, TectoMT provides useful novel translations which are otherwise simply unavailable to the statistical component, despite the very large training data. Our analyses conﬁrm the expected behaviour that TectoMT helps with preserving grammatical agreements and valency requirements, but that it also improves a very diverse set of other phenomena. Interestingly, including the outputs of the transfer-based system in the phrase-based search seems to have a positive effect on the search space. Overall, we ﬁnd that the components of this combination are complementary and the ﬁnal system produces signiﬁcantly better translations than either component by itself. 
The present article reports on efforts to improve the translation accuracy of a corpus– based hybrid MT system developed using the PRESEMT methodology. This methodology operates on a phrasal basis, where phrases are linguistically-motivated but are automatically determined via a dedicated module. Here, emphasis is placed on improving the structure of each translated sentence, by replacing the Example-Based MT approach originally used in PRESEMT with a sub-sentential approach. Results indicate that an improved accuracy can be achieved, as measured by objective metrics. 
 There are two primary approaches to the use bilingual dictionary in statistical machine translation: (i) the passive approach of appending the parallel training data with a bilingual dictionary and (ii) the pervasive approach of enforcing translation as per the dictionary entries when decoding. Previous studies have shown that both approaches provide external lexical knowledge to statistical machine translation thus improving translation quality. We empirically investigate the effects of both approaches on the same dataset and provide further insights on how lexical information can be reinforced in statistical machine translation.  
This discussion paper presents and analyses the main conceptual differences and similarities between the human task of simultaneous interpretation and the statistical approach to machine translation. A psycho-cognitive model of the simultaneous interpretation process is reviewed and compared with the phrase-based statistical machine translation approach. Some interesting differences are identified and their possible implications on machine translation methods are discussed. Finally, the most relevant research problems related to them are identified. 1. Introduction Nowadays, translation has become an important element of daily life. Indeed, the emergence of modern information and communication technologies and the resulting globalization phenomenon are continuously boosting the need for translation services and applications. Within the context of professional translation, three different types of human translation tasks can be identified: Document translation. This task refers to the situation in which the professional translator is required to generate a target-language-version of a given source document. In this kind of situations, full understanding of the source material is required and full generation of the target must be accomplished. In general, free translations are acceptable, no specific time constraints are imposed, and the best translation quality is expected. Consecutive interpretation. This task refers to the situation in which the professional translator is required to mediate the communication between  two persons that speaks different languages. The basic communication protocol in this case is based in a turn-taking strategy, in which interlocutors must speak one at a time when they are given the right to speak. In this kind of situations, full understanding of the source material is required and full generation of the target must be ideally accomplished, while a ‘shared’ time constrains exists. Simultaneous interpretation. This task refers to the situation in which the professional translator is required to translate on-the-fly what other person is saying in a different language. In this case no turntaking is allowed as the translator is expected to produce the translated speech while the main speaker continues speaking. In these situations, full understanding and full generation is not mandatory, as the interpreter must keep the main speaker’s pace because ‘concurrent’ time constraints exist. Current machine translation technologies have been theoretically and empirically designed under assumptions related to the first and second categories defined above. As far as we know, only few attempts have been done to apply machine translation to the specific problem of simultaneous interpretation. Indeed, previous research in this area can be traced back to the Vermobil1 project, as well as to work from Kitano (1991) and Furuse and Iida (1996), who proposed the use of incremental translation. Later on, Mima et al. (1998) developed the idea of example-based incremental transfer. The main objective of this discussion paper is to highlight the differences and similarities between the human task of simultaneous interpretation and statistical machine translation aiming at proposing 
Machine Translation (MT) quality is typically assessed using automatic evaluation metrics such as BLEU and TER. Despite being generally used in the industry for evaluating the usefulness of Translation Memory (TM) matches based on text similarity, fuzzy match values are not as widely used for this purpose in MT evaluation. We designed an experiment to test if this fuzzy score applied to MT output stands up against traditional methods of MT evaluation. The results obtained seem to confirm that this metric performs at least as well as traditional methods for MT evaluation. 
Dictionary extraction using parallel corpora is well established. However, for many language pairs parallel corpora are a scarce resource which is why in the current work we discuss methods for dictionary extraction from comparable corpora. Hereby the aim is to push the boundaries of current approaches, which typically utilize correlations between co-occurrence patterns across languages, in several ways: 1) Eliminating the need for initial lexicons by using a bootstrapping approach which only requires a few seed translations. 2) Implementing a new approach which first establishes alignments between comparable documents across languages, and then computes cross-lingual alignments between words and multiword-units. 3) Improving the quality of computed word translations by applying an interlingua approach, which, by relying on several pivot languages, allows an effective multi-dimensional cross-check. 4) We investigate that, by looking at foreign citations, language translations can even be derived from a single monolingual text corpus. 
Chinese and Spanish have different morphology structures, which poses a big challenge for translating between this pair of languages. In this paper, we analyze several strategies to better generalize from the Chinese non-morphology-based language to the Spanish rich morphologybased language. Strategies use a ﬁrst-step of Spanish morphology-based simpliﬁcations and a second-step of fullform generation. The latter can be done using a translation system or classiﬁcation methods. Finally, both steps are combined either by concatenation in cascade or integration using a factored-based style. Ongoing experiments (based on the United Nations corpus) and their results are described. 
We present an implicit tensor factorization method for learning the embeddings of transitive verb phrases. Unlike the implicit matrix factorization methods recently proposed for learning word embeddings, our method directly models the interaction between predicates and their two arguments, and learns verb phrase embeddings. By representing transitive verbs as matrices, our method captures multiple meanings of transitive verbs and disambiguates them taking their arguments into account. We evaluate our method on a widely-used verb disambiguation task and three phrase similarity tasks. On the disambiguation task, our method outperforms previous state-ofthe-art methods. Our experimental results also show that adjuncts provide useful information in learning the meanings of verb phrases. 
Tree-structured recursive neural networks (TreeRNNs) for sentence meaning have been successful for many applications, but it remains an open question whether the ﬁxed-length representations that they learn can support tasks as demanding as logical deduction. We pursue this question by evaluating whether two such models— plain TreeRNNs and tree-structured neural tensor networks (TreeRNTNs)—can correctly learn to identify logical relationships such as entailment and contradiction using these representations. In our ﬁrst set of experiments, we generate artiﬁcial data from a logical grammar and use it to evaluate the models’ ability to learn to handle basic relational reasoning, recursive structures, and quantiﬁcation. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language. 
We propose to base the development of vector-space models of semantics on concept extensions, which deﬁnes concepts to be sets of entities. We investigate two sources of knowledge about entities that could be relevant: distributional information provided by word or phrase embeddings, and ontological information derived from a knowledge base. We develop a feedforward neural network architecture to learn entity representations that are used to predict their concept memberships, and show that the two sources of information are actually complementary. In an entity ranking experiment, the combination approach that uses both types of information outperforms models that only rely on one of the two. We also perform an analysis of the output using fuzzy logic techniques to demonstrate the potential of learning concept extensions for supporting inference involving classical semantic operations. 
Inferring semantic relevance among entities (e.g., entries of Wikipedia) is important and challenging. According to the information resources, the inference can be categorized into learning with either raw text data, or labeled text data (e.g., wiki page), or graph knowledge (e.g, WordNet). Although graph knowledge tends to be more reliable, text data is much less costly and offers a better coverage. We show in this paper that different resources are complementary and can be combined to improve semantic learning. Particularly, we present a joint learning approach that learns vectors of entities by leveraging resources of both text data and graph knowledge. The experiments conducted on the semantic relatedness task show that text-based learning works well on general domain tasks, however for tasks in speciﬁc domains, joint learning that involves both text data and graph knowledge offers signiﬁcant improvement. 
The paper investigates the use of semantic similarity scores as feature in the phrase based machine translation system. We propose the use of partial least square regression to learn the bilingual word embedding using compositional distributional semantics. The model outperforms the baseline system which is shown by an increase in BLEU score. We also show the effect of varying the vector dimension and context window for two different approaches of learning word vectors. 
It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efﬁcient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve signiﬁcant improvements without over-ﬁtting the small adaptation data. 
In this paper we show the surprising effectiveness of a simple observed features model in comparison to latent feature models on two benchmark knowledge base completion datasets, FB15K and WN18. We also compare latent and observed feature models on a more challenging dataset derived from FB15K, and additionally coupled with textual mentions from a web-scale corpus. We show that the observed features model is most effective at capturing the information present for entity pairs with textual relations, and a combination of the two combines the strengths of both model types. 
Most state-of-the-art named entity recognition (NER) systems rely on handcrafted features and on the output of other NLP tasks such as part-of-speech (POS) tagging and text chunking. In this work we propose a language-independent NER system that uses automatically learned features only. Our approach is based on the CharWNN deep neural network, which uses word-level and character-level representations (embeddings) to perform sequential classiﬁcation. We perform an extensive number of experiments using two annotated corpora in two different languages: HAREM I corpus, which contains texts in Portuguese; and the SPA CoNLL2002 corpus, which contains texts in Spanish. Our experimental results give evidence of the contribution of neural character embeddings for NER. Moreover, we demonstrate that the same neural network which has been successfully applied to POS tagging can also achieve state-of-theart results for language-independet NER, using the same hyperparameters, and without any handcrafted features. For the HAREM I corpus, CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score for the total scenario (ten NE classes). For the SPA CoNLL-2002 corpus, CharWNN outperforms the state-ofthe-art system by 0.8 point in the F1. 
This paper describes an undergoing experiment to compare two tagsets for Named Entities (NE) annotation. We compared Klue 2 tagset, developed by IBM Research, with HAREM tagset, developed for tagging the Portuguese corpora used in Second HAREM competition. From this report, we expected to evaluate our methodology for comparison and to survey the problems that arise from it. 
We present preliminary results for the named entity recognition problem in the Vietnamese language. For this task, we build a system based on conditional random fields and address one of its challenges: how to combine labeled and unlabeled data to create a stronger system. We propose a set of features that is useful for the task and conduct experiments with different settings to show that using bootstrapping with an online learning algorithm called Margin Infused Relaxed Algorithm increases the performance of the models. 
In this paper we examine the effectiveness of neural network sequence-to-sequence transduction in the task of transliteration generation. In this year’s shared evaluation we submitted two systems into all tasks. The primary system was based on the system used for the NEWS 2012 workshop, but was augmented with an additional feature which was the generation probability from a neural network. The secondary system was the neural network model used on its own together with a simple beam search algorithm. Our results show that adding the neural network score as a feature into the phrase-based statistical machine transliteration system was able to increase the performance of the system. In addition, although the neural network alone was not able to match the performance of our primary system (which exploits it), it was able to deliver a respectable performance for most language pairs which is very promising considering the recency of this technique. 
This paper presents our system (BJTU-NLP system) for the NEWS2015 evaluation task of Chinese-to-English and English-to-Chinese named entity transliteration. Our system adopts a hybrid machine transliteration approach, which combines several features. To further improve the result, we adopt external data extracted from wikipeda to expand the training set. In addition, pre-processing and post-processing rules are utilized to further improve the performance. The final performance on the test corpus shows that our system achieves comparable results with other state-of-the-art systems. 
This paper describes our approach to English-Korean and English-Chinese transliteration task of NEWS 2015. We use different grapheme segmentation approaches on source and target languages to train several transliteration models based on the M2M-aligner and DirecTL+, a string transduction model. Then, we use two reranking techniques based on string similarity and web co-occurrence to select the best transliteration among the prediction results from the different models. Our English-Korean standard and non-standard runs achieve 0.4482 and 0.5067 in top-1 accuracy respectively, and our English-Chinese standard runs achieves 0.2925 in top-1 accuracy. 
Complex mechanisms, such as cell-signaling pathways, consist of many highly interconnected components, yet they are often described in disconnected fragmentary ways. The goal of DRUM (Deep Reader for Understanding Mechanisms) is to develop a system that can read papers and combine results of individual studies into a comprehensive explanatory model. A first step is to automatically extract relevant events and event relationships from the literature. This paper describes initial steps in extending an existing general deep language understanding system, TRIPS, to read biomedical papers. In a preliminary evaluation, our system was the best performing system among the participants, achieving results close to human expert performance. These results suggested that our system is viable for complex event extraction and, ultimately, understanding complex systems and mechanisms. 1. Introduction Complex mechanisms consist of many highly interconnected components, yet they are often described in disconnected fragmentary ways. Examples include ecosystems, social dynamics and signaling networks in biology. The study of these complex systems is often focused on a small portion of a mechanism at a time. In addition, the huge volume of scientific literature makes it difficult to track the fast developments in the field to achieve a comprehensive understanding of the often distant and convoluted interactions in the system. The goal of the DRUM (Deep Reader for Understanding Mechanisms) project is to develop a system that can read papers and combine research results of individual studies into a comprehensive explanatory model of a complex mechanism. The system will automatically read scientific papers, extract relevant new model fragments, and compose them into larger models that will expose the interactions and relationships between disparate elements in the mechanism. A first step towards this goal is to automatically extract relevant events and event relation-  ships from the literature. In this paper we will describe initial steps in extending an existing general deep language understanding system, TRIPS (Allen et al, 2008), to the genre of scientific writing, in particular in the biomedical domain. Events in biomedical research papers are described in a highly specialized and technical language, with complex formulations and nested constructions. We will discuss adaptations made and how the design principles of TRIPS facilitate such adaptations. We will report on an experimental evaluation of this extended system on extracting events and relationships centered on the Ras signaling pathways from a number of text passages in scientific papers. Our system was the best performing system among those evaluated, achieving results close to human expert performance. Admittedly this was a small and preliminary evaluation. However, the results suggested our system is viable for complex event extraction. Of note, unlike typical statistical approaches, we did not train on text describing the Ras signaling pathways (or on any other text for that matter). Our results were achieved using a general deep language understanding system, with little domain-specific customization beyond the recognition of named entities and some specialized vocabularies. Most important, our goal does not stop at the surface extraction of events, as is the case for many existing bio-event extraction tasks. With a general deep language understanding system, we are in a good position to develop an understanding of the underlying connections in complex models, and the methods developed to achieve that understanding will be readily transferrable to domains other than biology. 2. The TRIPS Architecture Much recent text processing work has focussed on developing “shallow”, statistically driven techniques. TRIPS takes a different approach, using statistical methods as a preprocessing step to provide guidance to a deep parsing system that uses a detailed, hand-built, grammar of English with a rich set of semantic restrictions. Figure 1 shows an overview of the system architecture. In  
Automatic recognition of relationships between key entities in text is an important problem which has many applications. Supervised machine learning techniques have proved to be the most effective approach to this problem. However, they require labelled training data which may not be available in sufﬁcient quantity (or at all) and is expensive to produce. This paper proposes a technique that can be applied when only limited training data is available. The approach uses a form of distant supervision but does not require an external knowledge base. Instead, it uses information from the training set to acquire new labelled data and combines it with manually labelled data. The approach was tested on an adverse drug data set using a limited amount of manually labelled training data and shown to outperform a supervised approach. 
Kernel-based methods are widely used for relation extraction task and obtain good results by leveraging lexical and syntactic information. However, in biomedical domain these methods are limited by the size of dataset and have difﬁculty in coping with variations in text. To address this problem, we propose Extended Dependency Graph (EDG) by incorporating a few simple linguistic ideas and include information beyond syntax. We believe the use of EDG will enable machine learning methods to generalize more easily. Experiments conﬁrm that EDG provides up to 10% f-value improvement over dependency graph using mainstream kernel methods over ﬁve corpora. We conducted additional experiments to provide a more detailed analysis of the contributions of individual modules in EDG construction. 
Biomedical event extraction systems have the potential to provide a reliable means of enhancing knowledge resources and mining the scientiﬁc literature. However, to achieve this goal, it is necessary that current event extraction models are improved, such that they can be applied conﬁdently to unseen data with a minimal rate of error. Motivated by this requirement, this work targets a particular type of error, namely partial events, where an event is missing one or more arguments. Speciﬁcally, we attempt to improve the performance of a state-of-the-art event extraction tool, EventMine, when applied to a new cancer pathway curation corpus. We propose a post-processing ranking approach based on relaxed constraints, in order to reconsider the candidate arguments for each event trigger, and suggest possible new arguments. The proposed methodology, applicable to the output of any event extraction system, achieves an improvement in argument recall of 2%-4% when applied to EventMine output, and thus constitutes a promising direction for further developments. 
This paper describes an an open-source software system for the automatic conversion of NLP event representations to system biology structured data interchange formats such as SBML and BioPAX. It is part of a larger effort to make results of the NLP community available for system biology pathway modelers. 
A typical NLP system for medical fact coding uses multiple layers of supervision involving factattributes, relations and coding. Training such a system involves expensive and laborious annotation process involving all layers of the pipeline. In this work, we investigate the feasibility of a shallow medical coding model that trains only on fact annotations, while disregarding fact-attributes and relations, potentially saving considerable annotation time and costs. Our results show that the shallow system, despite using less supervision, is only 1.4% F1 points behind the multi-layered system on Disorders, and contrary to expectation, is able to improve over the latter by about 2.4% F1 points on Procedure facts. Further, our experiments also show that training the shallow system using only sentence-level fact labels with no span information has no negative effect on performance, indicating further cost savings through weak supervision. 
The goal of our research is to extract medical concepts from clinical notes containing patient information. Our research explores stacked generalization as a metalearning technique to exploit a diverse set of concept extraction models. First, we create multiple models for concept extraction using a variety of information extraction techniques, including knowledgebased, rule-based, and machine learning models. Next, we train a meta-classiﬁer using stacked generalization with a feature set generated from the outputs of the individual classiﬁers. The meta-classiﬁer learns to predict concepts based on information about the predictions of the component classiﬁers. Our results show that the stacked generalization learner performs better than the individual models and achieves state-of-the-art performance on the 2010 i2b2 data set. 
Disease-symptom relationships are of primary importance for biomedical informatics, but databases that catalog them are incomplete in comparison with the state of the art available in the scientiﬁc literature. We propose in this paper a novel method for automatically extracting disease-symptom relationships from text, called SPARE (standing for Syntactic PAttern for Relationship Extraction). This method is composed of 3 successive steps: ﬁrst, we learn patterns from the dependency graphs; second, we select best patterns based on their respective quality and speciﬁcity (their ability to identify only disease-symptom relationships); ﬁnally, the patterns are used on new texts for extracting disease-symptom relationships. We experimented SPARE on a corpus of 121,796 abstracts of PubMed related to 457 rare diseases. The quality of the extraction has been evaluated depending on the pattern quality and speciﬁcity. The best F-measure obtained is 55.65% (for speci f icity ≥ 0.5 and quality ≥ 0.5). To provide an insight on the novelty of disease-symptom relationship extracted, we compare our results to the content of phenotype databases (OrphaData and OMIM). Our results show the feasibility of automatically extracting disease-symptom relationships, including true relationships that were not already referenced in phenotype databases and may involve complex symptom descriptions. 
In the medical domain, identifying and expanding abbreviations in clinical texts is a vital task for both better human and machine understanding. It is a challenging task because many abbreviations are ambiguous especially for intensive care medicine texts, in which phrase abbreviations are frequently used. Besides the fact that there is no universal dictionary of clinical abbreviations and no universal rules for abbreviation writing, such texts are difﬁcult to acquire, expensive to annotate and even sometimes, confusing to domain experts. This paper proposes a novel and effective approach – exploiting taskoriented resources to learn word embeddings for expanding abbreviations in clinical notes. We achieved 82.27% accuracy, close to expert human performance. 
Complex noun phrases are pervasive in biomedical texts, but are largely underexplored in entity discovery and information extraction. Such expressions often contain a mix of highly speciﬁc names (diseases, drugs, etc.) and common words such as “condition”, “degree”, “process”, etc. These words can have different semantic types depending on their context in noun phrases. In this paper, we address the task of classifying these common words onto ﬁne-grained semantic types: for instance, “condition” can be typed as “symptom and ﬁnding” or “conﬁguration and setting”. For information extraction tasks, it is crucial to consider common nouns only when they really carry biomedical meaning; hence the classiﬁer must also detect the negative case when nouns are merely used in a generic, uninformative sense. Our solution harnesses a small number of labeled seeds and employs label propagation, a semisupervised learning method on graphs. Experiments on 50 frequent nouns show that our method computes semantic labels with a microaveraged accuracy of 91.34%. 
 Clinical depression is a mental disorder  involving genetics and environmental  factors. Although much work studied its  genetic causes and numerous candidate  genes have consequently been looked into  and reported in the biomedical literature,  no gene expression changes or mutations  regarding depression have yet been  adequately collected and analyzed for its  full pathophysiology. In this paper, we  present a depression-specific annotated  corpus for text mining systems that target  at providing a concise review of  depression-gene relations, as well as  capturing complex biological events such  as gene expression changes. We describe  the annotation scheme and the conducted  annotation procedure in detail. We discuss  issues regarding proper recognition of  depression terms and entity interactions  for future approaches to the task. The  corpus  is  available  at  http://www.biopathway.org/CoMAGD.  
Understanding lexical characteristics of clinical documents is the foundation of sublanguage based Medical Language Processing (MLP) approach. However, there are limited studies focused on the lexical characters of Chinese clinical documents. In this study, a lexical characteristics analysis on both syntactic and semantic levels was conducted in a clinical corpus which contains 3,500 clinical documents generated during daily practices. The analysis was based on the automatic tagging results of a lexiconbased part-of-speech (POS) and semantic tagging method. The medical lexicon contains 237,291 entries annotated with both semantic and syntactic classes. The normalized frequency of different terms, syntactic and semantic classes was calculated and visualized. Major contribution of this paper is providing a wide-coverage Chinese medical semantic lexicon and presenting the lexical characteristics of Chinese clinical documents. Both of these will lay a good foundation for sublanguage based MLP studies in China. 
This study examines whether the readability of medical research journal abstracts changed from 1960 to 2010. Abstracts from medical journals were downloaded from PubMed.org in ten-year batches (1960s, 1970s, etc.). Abstracts in each decade underwent processing via a custom Python script to determine their Coleman-Liau Index (CLI) readability score. Analysis using one-way ANOVA found statistically significant differences between the mean CLI readability scores of each decade (F(4, 6689135) = 12936.91,p<0.0001). Posthoc analysis using Tukey’s method also found all pairwise comparisons between decades’ mean CLI readability scores to be statistically significant (p<0.001). Readability scores increased from decade to decade beginning with a mean CLI score of 16.0813 in the 1960s and ending with a mean CLI score of 16.8617 in the 2000s. These results indicate a 0.7804 grade level increase in the difficulty of reading medical research journal abstracts over time and raises questions about the accessibility of medical research for broader audiences.  practitioners can access and thoroughly comprehend research to better ensure new treatments and knowledge reaches patients and that patient care revolves around evidence-based practices (Pravikoff, Tanner, & Pierce, 2005; Woolf, 2008). Beyond seeking to leverage new research among medical practitioners, translational research also focuses on supporting patients in becoming more active and involved in their healthcare (Woolf, 2008). With the advent of the information age, patients and patients’ family members have substantial opportunities to research their own medical conditions and their treatment options. Navigating and understanding medical research requires that it proves accessible in terms of its readability. This study is a diachronic analysis of the readability of medical research. Specifically, this study seeks to answer whether the readability of medical research journal abstracts has changed from the 1960s to the 2000s. Results from this study may have implications for how researchers could communicate their findings to patients and how to address discrepancies between the reading level of medical journals and lay audiences’ reading abilities.  
 thereby empower them to take a more active role in their health. EHRs present a new and  The Centers for Medicare & Medicaid Services Incentive Programs promote meaningful use of electronic health records (EHRs), which, among many benefits, allow patients to receive electronic copies of their EHRs and thereby empower them to take a more active role in their health. In the United States, however, 17% population is Hispanic, of which 50% has limited English language skills. To help this  personalized communication channel that has the potential to increase patient involvement in care and improve communication between physicians and patients and their caregivers. In particular, allowing patients access to their physicians’ notes has the potential to enhance patients’ understanding of their conditions and disease and improve medication adherence and self-managed care.  population take advantage of their EHRs, we are developing English-Spanish machine translation (MT) systems for EHRs. In this study, we first built an English-Spanish parallel corpus and trained NoteAidSpanish, a statistical MT (SMT) system. Google Translator and Microsoft Bing Translator are two baseline MT systems. In addition, we evaluated hybrid MT systems that first replace medical jargon in EHR notes with lay terms and then translate the notes with SMT systems. Evaluation on a small set of EHR notes, our results show that Google Translator outperformed NoteAidSpanish. The hybrid SMT systems first map medical jargon to lay language. This step improved the translation. A fully implemented hybrid MT system is available at http://www.clinicalnotesaid.org. The English-Spanish parallel-aligned MedlinePlus corpus is available upon request.  However, most EHRs are written in English. In the United States, 17% population is Hispanic, of which 50% has limited English language skills. Many general-purpose MT systems are available. For example, Google Translate is a free service that has been used by health professions. Like most general-purpose MT systems, it is based on SMT, looking for patterns in hundreds of millions of WWW documents. In contrast, EHRs contain medical terms, shortened forms, complex disease and medication names, and other domain-specific jargon that do not typically appear in WWW documents, and therefore Google Translate may not perform well for EHRs, as was found in a prior study that evaluated general-purpose MT systems (Zeng-Treitler et al., 2010). Furthermore, the Health Insurance Portability and Accountability Act of 1996 protects the  privacy and security of individually identifiable  
Given a set of abstracts retrieved from a search engine such as Pubmed, we aim to automatically identify the claim zone in each abstract and then select the best sentence(s) from that zone that can serve as an answer to a given query. The system can provide a fast access mechanism to the most informative sentence(s) in abstracts with respect to the given query. 
Identifying smoking status of patients is vital for assessing their risk for a disease. With the rapid adoption of electronic health records (EHRs), patient information is scattered across various systems in the form of structured and unstructured data. In this study, we aimed to develop a hybrid system using rule-based, unsupervised and supervised machine learning techniques to automatically identify the smoking status of patients in unstructured EHRs. In addition to traditional features, we used per-document topic model distribution weights as features in our system. We also discuss the performance of our hybrid system using different feature sets. Our preliminary results demonstrated that combining per-document topic model distribution weights with traditional features improve the overall performance of the system. 
Clinical documents have been an emerging target of natural language applications. Information stored in documents created at clinical settings can be very useful for doctors or medical experts. However, the way these documents are created and stored is often a hindrance to accessing their content. In this paper, an automatic method for restoring the intended structure of Hungarian ophthalmology documents is described. The statements in these documents in their original form appeared under various subheadings. We successfully applied our method for reassigning the correct heading for each line based on its content. The results show that the categorization was correct for 81.99% of the statements in our testset, compared to a human categorization. 
Recently there is a surge in interest in learning vector representations of words using huge corpus in unsupervised manner. Such word vector representations, also known as word embedding, have been shown to improve the performance of machine learning models in several NLP tasks. However efﬁciency of such representation has not been systematically evaluated in biomedical domain. In this work our aim is to compare the performance of two state-of-the-art word embedding methods, namely word2vec and GloVe on a basic task of reﬂecting semantic similarity and relatedness of biomedical concepts. For this, vector representations of all unique words in the corpus of more than 1 million full-length research articles in biomedical domain are obtained from the two methods. These word vectors are evaluated for their ability to reﬂect semantic similarity and semantic relatedness of word-pairs in a benchmark data set of manually curated semantic similar and related words available at http:// rxinformatics.umn.edu. We observe that parameters of these models do affect their ability to capture lexicosemantic properties and word2vec with particular language modeling seems to perform better than others. 
Microblog services such as Twitter are an attractive source of data for public health surveillance, as they avoid the legal and technical obstacles to accessing the more obvious and targeted sources of health information. Only a tiny fraction of tweets may contain useful public health information but in Twitter this is oﬀset by the sheer volume of tweets posted. We present a system which can identify medical named entities in a real-time stream of Twitter posts and determine their geographic locations, as well as preliminary experiments in using this information for health surveillance purposes. 
This study examined the use of neural word embeddings for clinical abbreviation disambiguation, a special case of word sense disambiguation (WSD). We investigated three different methods for deriving word embeddings from a large unlabeled clinical corpus: one existing method called Surrounding based embedding feature (SBE), and two newly developed methods: Left-Right surrounding based embedding feature (LR_SBE) and MAX surrounding based embedding feature (MAX_SBE). We then added these word embeddings as additional features to a Support Vector Machines (SVM) based WSD system. Evaluation using the clinical abbreviation datasets from both the Vanderbilt University and the University of Minnesota showed that neural word embedding features improved the performance of the SVMbased clinical abbreviation disambiguation system. More specifically, the new MAX_SBE method outperformed the other two methods and achieved the state-of-the-art performance on both clinical abbreviation datasets. 
Constructing standard and computable clinical diagnostic criteria is an important and challenging research area in clinical informatics community. In this study, we present our framework and methods for representing clinical diagnostic criteria in Quality Data Model (QDM) using natural language processing (NLP) technologies. We used a clinical NLP tool known as cTAKES for preprocessing of textual diagnostic criteria. We created mappings between cTAKES type system and QDM elements in both datatype and data levels. We evaluated the performance of our NLP-based approach by annotating 218 individual diagnostic criteria in the categories of Symptom and Laboratory Test. In conclusion, our NLP-based approach is a feasible solution in developing diagnostic criteria representation and computerization. 
In this paper, we investigate the feasibility of using the chronology of changes in historical editions of Encyclopaedia Britannica (EB) to track the changes in the landscape of cultural knowledge, and specifically, the rise and fall in reputations of historical ﬁgures. We describe the dataprocessing pipeline we developed in order to identify the matching articles about historical ﬁgures in Wikipedia, the current electronic edition of Encyclopaedia Britannica (edition 15), and several digitized historical editions, namely, editions 3, 9, 11. We evaluate our results on the tasks of article segmentation and cross-edition matching using a manually annotated subset of 1000 articles from each edition. As a case study for the validity of discovered trends, we use the Wikipedia category of 18th century classical composers. We demonstrate that our data-driven method allows us to identify cases where a historical ﬁgure’s reputation experiences a drastic fall or a dramatic recovery which would allow scholars to further investigate previously overlooked instances of such change. 
We present a quantitative study of the Annals of the Joseon Dynasty, the daily written records of the ﬁve hundred years of a monarchy in Korea. We ﬁrst introduce the corpus, which is a series of books describing the historical facts during the Joseon dynasty. We then deﬁne three categories of the monarchial ruling styles based on the written records and compare the twentyﬁve kings in the monarchy. Finally, we investigate how kings show different ruling styles for various topics within the corpus. Through this study, we introduce a very unique corpus of monarchial records that span an entire monarchy of ﬁve hundred years and illustrate how text mining can be applied to answer important historical questions. 
Although sentiment analysis in Chinese social media has attracted a lot of interest in recent years, it has been less explored in traditional Chinese literature (e.g., classical Chinese poetry) due to the lack of sentiment lexicon resources. In this paper, we propose a weakly supervised approach based on Weighted Personalized PageRank (WPPR) to create a sentiment lexicon for classical Chinese poetry. We evaluate our lexicon intrinsically and extrinsically. We show that our graphbased approach outperforms a previous well-known PMI-based approach (Turney and Littman, 2003) on both evaluation settings. On the basis of our sentiment lexicon, we analyze sentiment in the Complete Anthology of Tang Poetry. We extract topics associated with positive (negative) sentiment using a position-aware sentimenttopic model. We further compare sentiment among different poets in Tang Dynasty (AD 618 – 907). 
This paper presents an approach to organizing folktales based on a data structure called a plot graph, which captures the narrative ﬂow of events in a folktale. The similarity between two folktales can be computed as the structural similarity between their corresponding plot graphs. This is performed using the well-known Needleman-Wunsch algorithm. To test the efﬁcacy of this approach, experiments are carried out using a small collection of 24 folktales grouped into 5 categories based on the Aarne-Thompson index. The best result is obtained by combining the proposed structural-based similarity measure with a more conventional bag of words vector space model, where 19 out of the 24 folktales (79.16%) yield higher average similarity with folktales within their respective categories as opposed to across categories. 
We report on ﬁrst annotation experiments on narrative segments. Narrative segments are a pragmatic intermediate layer that allows studying more complex narratological phenomena. Our experiments show that segmenting on limited context information alone is difﬁcult. High interannotator agreement on this task can be achieved by coupling the segmentation with summarization and aligning parts of the summaries to segments of the text. 
In this paper, we present three approaches to automatic ranking of relevant verb phrases extracted from historical text. These approaches are based on conditional probability, log likelihood ratio, and bagof-words classiﬁcation respectively. The aim of the ranking in our study is to present verb phrases that have a high probability of describing work at the top of the results list, but the methods are likely to be applicable to other information needs as well. The results are evaluated by use of three different evaluation metrics: precision at k, R-precision, and average precision. In the best setting, 91 out of the top-100 instances in the list are true positives. 
Public events are often accompanied by a social media commentary that documents the public opinion and topics of importance related to these events. In this work, we describe work in collaboration with the State Library of New South Wales (NSW) to archive the social media commentary for the Australian state election in NSW, in March 2015, as a record for social scientists and historians to study in the years to come. Here, we provide an example of how one might utilise this data set, with an analysis of the data focusing on election issues. Speciﬁcally, we describe a method to produce rankings of election issues, which we ﬁnd to correlate moderately to those of ofﬁcial commentators. Furthermore, using our time-series data, we show how the importance of key issues stabilises approximately a month before the actual election. 
Continuous space representations of words are currently at the core of many state-of-the-art approaches to problems in natural language processing. In spite of several advantages of using such methods, they have seen little usage within digital humanities. In this paper, we show a case study of how such models can be used to ﬁnd interesting relationships within the ﬁeld of late antiquity. We use a word2vec model trained on over one billion words of Latin to investigate the relationships between persons and concepts of interest from works of the 6th-century scholar Cassiodorus. The results show that the method has high potential to aid the humanities scholar, but that caution must be taken as the analysis requires the assessment by the traditional historian. 
In this paper, we will demonstrate a system that shows great promise for creating Part-of-Speech taggers for languages with little to no curated resources available, and which needs no expert involvement. Interlinear Glossed Text (IGT) is a resource which is available for over 1,000 languages as part of the Online Database of INterlinear text (ODIN) (Lewis and Xia, 2010). Using nothing more than IGT from this database and a classiﬁcation-based projection approach tailored for IGT, we will show that it is feasible to train reasonably performing annotators of interlinear text using projected annotations for potentially hundreds of world’s languages. Doing so can facilitate automatic enrichment of interlinear resources to aid the ﬁeld of linguistics. 
Interlinear glossing is a type of annotation of morphosyntactic categories and crosslinguistic lexical correspondences that allows linguists to analyse sentences in languages that they do not necessarily speak. Automatising this annotation is necessary in order to provide glossed corpora big enough to be used for quantitative studies. In this paper, we present experiments on the automatic glossing of Chintang. We decompose the task of glossing into steps suitable for statistical processing. We ﬁrst perform grammatical glossing as standard supervised part-of-speech tagging. We then add lexical glosses from a stand-off dictionary applying context disambiguation in a similar way to word lemmatisation. We obtain the highest accuracy score of 96% for grammatical and 94% for lexical glossing. 
This paper describes an on-going project of transcribing and annotating digitized manuscripts of medieval Spanish with paleographic and lexical information. We link lexical units from the manuscripts with the Multilingual Central Repository (MCR), making terms retrievable by any of the languages that integrate MCR. The goal of the project is twofold: creating a paleographic knowledge base from digitized medieval facsimiles, that will allow paleographers, philologist, historical linguist, and humanities scholars in general, to analyze and retrieve graphemic, lexical and textual information from historical documents; and on the other hand, developing machine readable documents that will link image representations of graphemic and lexical units in a facsimile with Linked Open Data resources. This paper concentrates on the encoding and cross-linking procedures. 
Due to proliferation of digital publishing, e-book catalogs are abundant but noisy and unstructured. Tools for the digital librarian rely on ISBN, metadata embedded into digital ﬁles (without accepted standard) and cryptographic hash functions for the identiﬁcation of coderivative or nearduplicate content. However, unreliability of metadata and sensitivity of hashing to even smallest changes prevents efﬁcient detection of coderivative or similar digital books. Focus of the study are books with many versions that differ in certain amount of OCR errors and have a number of sentence-length variations. Identiﬁcation of similar books is performed using small-sized ﬁngerprints that can be easily shared and compared. We created synthetic datasets to evaluate ﬁngerprinting accuracy while providing standard precision and recall measurements. 
This paper introduces the main features of Traduco, a Web-based, collaborative Computer-Assisted Translation (CAT) tool developed to support the translation of ancient texts. In addition to the standard components offered by traditional CAT tools, Traduco includes a number of features designed to ease the translation of ancient texts, such as the Babylonian Talmud, posing speciﬁc structural, stylistic, linguistic and hermeneutical challenges. 
A diachronic thesaurus is a lexical resource that aims to map between modern terms and their semantically related terms in earlier periods. In this paper, we investigate the task of collecting a list of relevant modern target terms for a domain-speciﬁc diachronic thesaurus. We propose a supervised learning scheme, which integrates features from two closely related ﬁelds: Terminology Extraction and Query Performance Prediction (QPP). Our method further expands modern candidate terms with ancient related terms, before assessing their corpus relevancy with QPP measures. We evaluate the empirical beneﬁt of our method for a thesaurus for a diachronic Jewish corpus. 
This paper describes the Linear A/Minoan digital corpus and the approaches we applied to develop it. We aim to set up a suitable study resource for Linear A and Minoan. Firstly we start by introducing Linear A and Minoan in order to make it clear why we should develop a digital marked up corpus of the existing Linear A transcriptions. Secondly we list and describe some of the existing resources about Linear A: Linear A documents (seals, statuettes, vessels etc.), the traditional encoding systems (standard code numbers referring to distinct symbols), a Linear A font, and the newest (released on June 16th 2014) Unicode Standard Characters set for Linear A. Thirdly we explain our choice concerning the data format: why we decided to digitize the Linear A resources; why we decided to convert all the transcriptions in standard Unicode characters; why we decided to use an XML format; why we decided to implement the TEI-EpiDoc DTD. Lastly we describe: the developing process (from the data collection to the issues we faced and the solving strategies); a new font we developed (synchronized with the Unicode Characters Set) in order to make the data readable even on systems that are not updated. Finally, we discuss the corpus we developed in a Cultural Heritage preservation perspective and suggest some future works. 
We present a survey of tagging accuracies — concerning part-of-speech and full morphological tagging — for several taggers based on a corpus for medieval church Latin (see www.comphistsem.org). The best tagger in our sample, Lapos, has a PoS tagging accuracy of close to 96% and an overall tagging accuracy (including full morphological tagging) of about 85%. When we ‘intersect’ the taggers with our lexicon, the latter score increases to almost 91% for Lapos. A conservative assessment of lemmatization accuracy on our data estimates a score of 93-94% for a lexicon-based lemmatization strategy and a score of 94-95% for lemmatizing via trained lemmatizers. 
This study aims to show that frequency of occurrence over time for technical terms and keyphrases differs from general language terms in the sense that technical terms and keyphrases show a strong tendency to be recent coinage, and that this difference can be exploited for the automatic identiﬁcation and extraction of technical terms and keyphrases. To this end, we propose two features extracted from temporally labelled datasets designed to capture surface level n-gram neology. Our analysis shows that these features, calculated over consecutive bigrams, are highly indicative of technical terms and keyphrases, which suggests that both technical terms and keyphrases are strongly biased to be surface level neologisms. Finally, we evaluate the proposed features on a gold-standard dataset for technical term extraction and show that the proposed features are comparable or superior to a number of features commonly used for technical term extraction. 
 Deutschlehrer  A core assumption of keyphrase extraction is that a concept is more important if it is mentioned more often in a document. Especially in languages like German that form large noun compounds, frequency counts might be misleading as concepts “hidden” in compounds are not counted. We hypothesize that using decompounding before counting term frequencies may lead to better keyphrase extraction. We identiﬁed two effects of decompounding: (i) enhanced frequency counts, and (ii) more keyphrase candidates. We created two German evaluation datasets to test our hypothesis and analyzed the effect of additional decompounding for keyphrase extraction. 
We introduce a global inference model for keyphrase extraction that reduces overgeneration errors by weighting sets of keyphrase candidates according to their component words. Our model can be applied on top of any supervised or unsupervised word weighting function. Experimental results show a substantial improvement over commonly used word-based ranking approaches. 
Social media not only carries information that is up-to-date, but also bears the wisdom of the crowd. In social media, new words are developed everyday, including slangs, combinations of existing terms, entity names, etc. These terms are initially used in small communities, which can later grow popular and become new standards. The ability to early recognize the existence and understand the meanings of these terms can prove to be crucial, especially to emergence detection applications. We present an ongoing research work that investigates the use of topical analysis to extract semantic of terms in social media. In particular, the proposed method extracts semantically related words associated with a target word from a corpus of tweets. We provide preliminary, anecdotal results comprising the semantic extraction of ﬁve different keywords. 
Emotions, a complex state of feeling results in physical and psychological changes that influence human behavior. Thus, in order to extract the emotional key phrases from psychological texts, here, we have presented a phrase level emotion identification and classification system. The system takes pre-defined emotional statements of seven basic emotion classes (anger, disgust, fear, guilt, joy, sadness and shame) as input and extracts seven types of emotional trigrams. The trigrams were represented as Context Vectors. Between a pair of Context Vectors, an Affinity Score was calculated based on the law of gravitation with respect to different distance metrics (e.g., Chebyshev, Euclidean and Hamming). The words, Part-Of-Speech (POS) tags, TF-IDF scores, variance along with Affinity Score and ranked score of the vectors were employed as important features in a supervised classification framework after a rigorous analysis. The comparative results carried out for four different classifiers e.g., NaiveBayes, J48, Decision Tree and BayesNet show satisfactory performances. 
This paper describes taking parsed sentences, going to meaning representations (the stopover), and then back to parsed sentences (the round trip). Keeping to the same language tests the combined success of building meaning representations from parsed input and of generating parsed output. Switching languages when manipulating meaning representations would achieve translation. Transfer shortfall is seen with meaning representations built from parsed parallel corpora data, with English-Japanese as an example. 
Divergence of syntactic structures between languages constitutes a major challenge in using linguistic structure in Machine Translation (MT) systems. Here, we examine the potential of semantic structures. While semantic annotation is appealing as a source of cross-linguistically stable structures, little has been accomplished in demonstrating this stability through a detailed corpus study. In this paper, we experiment with the UCCA conceptual-cognitive annotation scheme in an English-French case study. First, we show that UCCA can be used to annotate French, through a systematic type-level analysis of the major French grammatical phenomena. Second, we annotate a parallel English-French corpus with UCCA, and quantify the similarity of the structures on both sides. Results show a high degree of stability across translations, supporting the usage of semantic annotations over syntactic ones in structure-aware MT systems. 
This paper presents a novel approach to enhance hierarchical phrase-based (HPB) machine translation systems with case frame (CF).we integrate the Japanese shallow CF into both rule extraction and decoding. All of these rules are then employed to decode new sentences in Japanese with source language case frame. The results of experiments carried out on Japanese-Chinese test sets. It shows that our approach maintains the advantages of HPB translation systems while at the same time naturally incorporates CF constraints. The case frame rules can complement Hiero-style rules. Our approach is especially effective for language pairs with large word order differences, such as Japanese-to-Chinese. 
We present a feature-rich discriminative model for machine translation which uses an abstract semantic representation on the source side. We include our model as an additional feature in a phrase-based decoder and we show modest gains in BLEU score in an n-best re-ranking experiment. 
The above comparison in LIVAC is made possible by rigorous improvement to the common and simplistic approach to the cultivation and use of databases. The augmentation eﬀorts included the rigorous cultivation of 3 comparable (sub-) corpora for Beijing, Hong Kong and Taipei through geographical (horizontal ), chronological (vertical ) and domain (topical ) partitioning of what is often assumed to be a common linguistic database. This partitioning required well-reasoned pre-conceived criteria to ensure adequate equivalency in comparability in terms of size, period and depth of analysis. To facilitate comparison we propose a Cognitive-cultural Salience Index (CSI) which draws on comparable corpus data (e.g. LIVAC) to provide comparison of the relative saliency of target words in the relevant corpus and presented as word clouds. The results are viewed in the light of the Sketch Engine output  
Multiple approaches to grab comparable data from the Web have been developed up to date. Nevertheless, coming out with a high-quality comparable corpus of a speciﬁc topic is not straightforward. We present a model for the automatic extraction of comparable texts in multiple languages and on speciﬁc topics from Wikipedia. In order to prove the value of the model, we automatically extract parallel sentences from the comparable collections and use them to train statistical machine translation engines for speciﬁc domains. Our experiments on the English– Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 
Common technologies for automatic coreference resolution require either a language-speciﬁc rule set or large collections of manually annotated data, which is typically limited to newswire texts in major languages. This makes it difﬁcult to develop coreference resolvers for a large number of the so-called low-resourced languages. We apply a direct projection algorithm on a multi-genre and multilingual corpus (English, German, Russian) to automatically produce coreference annotations for two target languages without exploiting any linguistic knowledge of the languages. Our evaluation of the projected annotations shows promising results, and the error analysis reveals structural differences of referring expressions and coreference chains for the three languages, which can now be targeted with more linguistically-informed projection algorithms. 
Each entry (concept) in DBpedia comes along a set of surface strings (property rdfs:label) which are possible realizations of the concept being described. Currently, only a ﬁfth of the English DBpedia entries have a surface string in French, which severely limits the deployment of Semantic Web Annotation for this language. In this paper, we investigate the task of identifying missing translations, contrasting two projective approaches. We show that the problem is actually challenging, and that a carefully engineered baseline is not easy to outperform. 
Alignment from comparable corpora usually involves two languages, one source and one target language. Previous works on bilingual lexicon extraction from parallel corpora demonstrated that more than two languages can be useful to improve the alignments. Our works have investigated to which extent a third language could be interesting to bypass the original alignment. We have deﬁned two original alignment approaches involving pivot languages and we have evaluated over four languages and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 
In order to develop effective computerassisted language teaching systems for learners of English as a foreign language, it is first necessary to identify gaps between learners and native speakers in the four basic linguistic skills (reading, writing, pronunciation, and listening). To identify these gaps, the accuracy and fluency in language use between learners and native speakers should be compared using a learner corpus. However, previous corpora have not included all necessary types of linguistic data. Therefore, in this study, we aimed to design and build a new corpus comprising all types of linguistic data necessary for comparing accuracy and fluency in basic linguistic skills between learners and native speakers. 
In the process of translating patent documents, a bilingual lexicon of technical terms is inevitable knowledge source. It is important to develop techniques of acquiring technical term translation equivalent pairs automatically from parallel patent documents. We take an approach of utilizing the phrase table of a state-of-theart phrase-based statistical machine translation model. First, we collect candidates of synonymous translation equivalent pairs from parallel patent sentences. Then, we apply the Support Vector Machines (SVMs) to the task of identifying bilingual synonymous technical terms. This paper especially focuses on the issue of examining the effectiveness of each feature and identiﬁes the minimum number of features that perform as comparatively well as the optimal set of features. Finally, we achieve the performance of over 90% precision with the condition of more than or equal to 25% recall. 
This paper aims to present a novel method of extracting bilingual lexica from comparable corpora using one of the artificial neural network algorithms, self-organizing maps (SOMs). The proposed method is very useful when a seed dictionary for translating source words into target words is insufficient. Our experiments have shown stunning results when contrasted with one of the other approaches. For future work, we need to fine-tune various parameters to achieve stronger performances. Also we should investigate how to construct good synonym vectors. 
This study explores methods for developing Machine Translation dictionaries on the basis of word frequency lists coming from comparable corpora. We investigate (1) various methods to measure the similarity of cognates between related languages, (2) detection and removal of noisy cognate translations using SVM ranking. We show preliminary results on several Romance and Slavonic languages. 
This paper presents a framework for aligning comparable documents collection. Our feature based model is able to consider different characteristics of documents for evaluating their similarities. The model uses the content of documents while no link, special tag or Metadata are available. And also we apply a filtering mechanism which made our model to be properly applicable for a large collection of data. According to the results, our model is able to recognize related documents in the target language with recall of 45.67% for the 1-best and 62% for the 5-best. 
This paper describes the LINA system for the BUCC 2015 shared track. Following (Enright and Kondrak, 2007), our system identify comparable documents by collecting counts of hapax words. We extend this method by ﬁltering out document pairs sharing target documents using pigeonhole reasoning and cross-lingual information. 
This paper describes a grammar-based translation system built by a company for a paying customer. The system uses a multilingual grammar for English, Finnish, German, Spanish, and Swedish written in GF (Grammatical Framework). The grammar covers a corpus of technical texts in Swedish, describing properties of places and objects related to accessibility by disabled people. This task is more complex than most previous GF tasks, which have addressed controlled languages. The main goals of the paper are: (1) to ﬁnd a grammar architecture and workﬂow for domain-speciﬁc grammars on real data (2) to estimate the quality reachable with a reasonable engineering effort (3) to assess the cost of grammar-based translation and its commercial viability. 
This paper presents the creation and the initial stage development of a broadcoverage Indonesian Resource Grammar (INDRA) within the framework of Head Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Minimal Recursion Semantics (MRS) (Copestake et al., 2005). At the present stage, INDRA focuses on verbal constructions and subcategorization since they are fundamental for argument and event structure. Verbs in INDRA were semi-automatically acquired from the English Resource Grammar (ERG) (Flickinger, 2000) via Wordnet Bahasa (Nurril Hirfana Mohamed Noor et al., 2011; Bond et al., 2014). In the future, INDRA will be used in the development process of machine translation. A preliminary evaluation of INDRA on the MRS test-suite shows promising coverage. 
This paper introduces our attempts to model the Chinese language using HPSG and MRS. Chinese refers to a family of various languages including Mandarin Chinese, Cantonese, Min, etc. These languages share a large amount of structure, though they may differ in orthography, lexicon, and syntax. To model these, we are building a family of grammars: ZHONG [ ]. This grammar contains instantiations of various Chinese languages, sharing descriptions where possible. Currently we have prototype grammars for Cantonese and Mandarin in both simpliﬁed and traditional script, all based on a common core. The grammars also have facilities for robust parsing, sentence generation, and unknown word handling. 
Categorial grammars are attractive because they have a clear account of unbounded dependencies. This accounting is especially important in Mandarin Chinese which makes extensive usage of unbounded dependencies. However, parsers trained on existing categorial grammar annotations (Tse and Curran, 2010) extracted from the Penn Chinese Treebank (Xue et al., 2005) are not as accurate as those trained on the original treebank, possibly because enforcing a small set of inference rules in these grammars leads to large sets of categories, which cause sparse data problems. This work reannotates the Penn Chinese Treebank into a generalized categorial grammar which uses a larger rule set and a substantially smaller category set while retaining the capacity to model unbounded dependencies. Experimental results show a statistically signiﬁcant improvement in parsing accuracy with this categorial grammar. 
Orthography is an integral part of language but in grammar engineering it is often ignored, simpliﬁed or just delegated to external tools. We present new extensions to Grammatical Framework, which allow one and the same formalism to describe both orthography, syntax and morphology. These extensions are also easily generalizable to other formalisms. 
Writing deep linguistic grammars has been considered a highly specialized skill, requiring the use of tools with steep learning curves and complex installation procedures. As the use of statistical methods has increased, new generations of computational linguists are getting less and less prepared for grammar writing tasks. In an aim to provide a better learning experience for grammar writers, we present a grammar engineering tool that resides in the cloud. It has been used in several tutorial courses and self-studies, and it allows absolute beginners to write their ﬁrst grammars and parse examples in 10 minutes. The cloud-based grammar engineering tool is built on top of GF (Grammatical Framework), a grammar formalism that has an explicit tecto/phenogrammar distinction, is based on ideas from type theory and functional programming and comes equipped with a grammar library supporting 30 languages. 
This paper presents a semi-automatic approach to acquire a computational construction grammar from the semi-formal Swedish Constructicon. The implementation is based on the resource grammar library provided by Grammatical Framework and can be seen as an extension to the existing Swedish resource grammar. An important consequence of this work is that it generates feedback, explicit and implicit, on how to improve the annotation consistency and adequacy of the original construction resource. 
Within the context of grammar engineering, modelling honoriﬁcs has been regarded as one of the components for improving machine translation and anaphora resolution. Using the HPSG and MRS framework, this paper provides a computational model of honoriﬁcs. The present study incorporates the honoriﬁc information into the meaning representation system via Individual Constraints with an eye toward semantics-based processing. 
 In this paper, we describe the treatment  of extraction in HaG, an emerging compu-  tational grammar of Hausa, concentrating  on the intricate patterns of interaction be-  tween resumptive and gap strategies. We  shall argue with Tuller (1986) that Hausa  resumption (both overt and covert) opens  up the possibility for relativisation only to  escape well-attested extraction islands in  the language. As suggested by the mutual  compatibility of gaps and resumptives in  ATB extraction, however, we shall conclude  that both strategies must be regarded as un-  bounded dependencies (UDCs) to be mod-  elled via the  feature in HPSG. We  shall discuss how the treatment of UDCs  has been generalised, in HaG, to permit  more than one simultaneous  depen-  dency, and focus in particular on how the  distinction between true gaps and resump-  tive  elements can be exploited to ad-  dress eﬃciency issues.  
Different names may be popular in different countries. Hence, person names may give a clue to a person’s country of origin. Along with other features, mapping names to countries can be helpful in a variety of applications such as country tagging twitter users. This paper describes the collection of Arabic Twitter user names that are either written in Arabic or transliterated into Latin characters along with their stated geographical locations. To classify previously unseen names, we trained naive Bayes and Support Vector Machine (SVM) multi-class classiﬁers using primarily bag-of-words features. We are able to map Arabic user names to speciﬁc Arab countries with 79% accuracy and to speciﬁc regions (Gulf, Egypt, Levant, Maghreb, and others) with 94% accuracy. As for transliterated Arabic names, the accuracy per country and per region was 67% and 83% respectively. The approach is generic and language independent, and can be used to collect and classify names to other countries or regions, and considering language-dependent name features (like the compound names, and person titles) yields to better results. 
 In this paper, deep learning framework is proposed for text sentiment classification in Arabic. Four different architectures are explored. Three are based on Deep Belief Networks and Deep Auto Encoders, where the input data model is based on the ordinary Bag-of-Words, with features based on the recently developed Arabic Sentiment Lexicon in combination with other standard lexicon features. The fourth model, based on the Recursive Auto Encoder, is proposed to tackle the lack of context handling in the first three models. The evaluation is carried out using Linguistic Data Consortium Arabic Tree Bank dataset, with benchmarking against the state of the art systems in sentiment classification with reported results on the same dataset. The results show high improvement of the fourth model over the state of the art, with the advantage of using no lexicon resources that are scarce and costly in terms of their development.  
Most advanced mobile applications require server-based and communication. This often causes additional energy consumption on the already energy-limited mobile devices. In this work, we provide to address these limitations on the mobile for Opinion Mining in Arabic. Instead of relying on compute-intensive NLP processing, the method uses an Arabic lexical resource stored on the device. Text is stemmed, and the words are then matched to our own developed ArSenL. ArSenL is the first publicly available large scale Standard Arabic sentiment lexicon (ArSenL) developed using a combination of English SentiWordnet (ESWN), Arabic WordNet, and the Arabic Morphological Analyzer (AraMorph). The scores from the matched stems are then processed through a classifier for determining the polarity. The method was tested on a published set of Arabic tweets, and an average accuracy of 67% was achieved. The developed mobile application is also made publicly available. The application takes as input a topic of interest and retrieves the latest Arabic tweets related to this topic. It then displays the tweets superimposed with colors representing sentiment labels as positive, negative or neutral. The application also provides visual summaries of searched topics and a history showing how the sentiments for a certain topic have been evolving.  
This paper presents a wide literature review of natural language processing for dialectical Arabic. Four main research areas were identiﬁed and the dialect coverage in research work was outlined. The paper can be used as a quick reference to identify relevant contributions that address a speciﬁc NLP aspect for a speciﬁc dialect. 
This paper presents DIWAN, an annotation interface for Arabic dialectal texts. While the Arabic dialects differ in many respects from each other and from Modern Standard Arabic, they also have much in common. To facilitate annotation and to make it as efficient as possible, it is therefore not advisable to treat each Arabic dialect as a separate language, unrelated to the other variants of Arabic. Instead, we make analyses from other variants available to the annotator, who then can choose to use them or not. 1. Introduction Arabic is a Central Semitic language, closely related to Aramaic, Hebrew, Ugaritic and Phoenician. It is spoken by 420 million speakers (native and non-native) in the Arab World. Arabic also is a liturgical language of 1.6 billion Muslims around the world. Modern Standard Arabic (MSA) is the official Arabic language. It is the educational language and official language used in news and official communication across the Arabic-speaking world. When Arabs communicate spontaneously in informal settings, they use dialectal Arabic (DA). There are divisions of many dialects of the Arabic language that occur between the spoken languages of different regions. Some varieties of Arabic in North Africa, for example, are incom-  prehensible to an Arabic speaker from the Levant or the Arabian Peninsula.1 Within these broad regions, further and considerable geographic distinctions exist – within countries, across country borders, and between cities and villages. Some examples include Gulf Ara- bic, Bahraini Arabic, Najdi Arabic, Hijazi Ara- bic, Yemeni Arabic, Yemeni Hadhrami Arabic, Yemeni Sanaani Arabic, Yemeni Ta'izzi-Adeni Arabic, Dhofari Arabic, Omani Arabic, Shihhi Arabic, and the Peninsular Arabic dialects. Despite this diversity, all Arabic dialects share certain properties: much of their phonology, templatic morphology augmented by affixes and a large set of clitics, large parts of their syntax, and important (though unpredictable) parts of the lexicon. Current natural language processing (NLP) tools work well with MSA because they were designed specifically for the processing of MSA, and because of the abundance of MSA resources. Applying the NLP tools designed for MSA directly to DA yields significantly lower performance (Chiang et al., 2006; Habash and Rambow, 2006; Benajiba et al., 2010; Habash et al., 2012). This makes it imperative to direct research to building resources and tools for DA processing. 
Developing natural language processing tools usually requires a large number of resources (lexica, annotated corpora, etc.), which often do not exist for less-resourced languages. One way to overcome the problem of lack of resources is to devote substantial efforts to build new ones from scratch. Another approach is to exploit existing resources of closely related languages. In this paper, we focus on developing a part-of-speech tagger for the Tunisian Arabic dialect (TUN), a lowresource language, by exploiting its closeness to Modern Standard Arabic (MSA), which has many state-of-the-art resources and tools. Our system achieved an accuracy of 89% (∼20% absolute improvement over an MSA tagger baseline). 
Algerian Arabic is an Arabic dialect spoken in Algeria characterized by the absence of writing resources and standardization, hence it is considered as an under-resourced language. It differs from Modern Standard Arabic on all levels of linguistic representation, from phonology and morphology to lexicon and syntax. In this paper, we present a conventional orthography for Algerian Arabic, following a previous effort on developing a conventional orthography for Dialectal Arabic (or CODA), demonstrated for Egyptian and Tunisian Arabic. We explain the design principles of Algerian CODA and provide a detailed description of its guidelines. 
Arabic script writing is typically underspeciﬁed for short vowels and other mark up, referred to as diacritics. Apart from the lexical ambiguity found in words, similar to that exhibited in other languages, the lack of diacritics in written Arabic script adds another layer of ambiguity which is an artifact of the orthography. Diacritization of written text has a signiﬁcant impact on Arabic NLP applications. In this paper, we present a pilot study on building a diacritized multi-genre corpus in Arabic. We annotate a sample of nondiacritized words extracted from ﬁve text genres. We explore different annotation strategies: Basic where we present only the bare undiacritized forms to the annotators, Intermediate (Basic forms+their POS tags), and Advanced (automatically diacritized words). We present the impact of the annotation strategy on annotation quality. Moreover, we study different diacritization schemes in the process. 
We present a method for annotating targets of opinions in Arabic in a two-stage process using the crowdsourcing tool Amazon Mechanical Turk. The ﬁrst stage consists of identifying candidate targets “entities” in a given text. The second stage consists of identifying the opinion polarity (positive, negative, or neutral) expressed about a speciﬁc entity. We annotate a corpus of Arabic text using this method, selecting our data from online commentaries in different domains. Despite the complexity of the task, we ﬁnd high agreement. We present detailed analysis. 
In this paper, we investigate different approaches in crowdsourcing transcriptions of Dialectal Arabic speech with automatic quality control to ensure good transcription at the source. Since Dialectal Arabic has no standard orthographic representation, it is very challenging to perform quality control. We propose a complete recipe for speech transcription quality control that includes using output of an Automatic Speech Recognition system. We evaluated the quality of the transcribed speech and through this recipe, we achieved a reduction in transcription error of 1.0% compared with 13.2% baseline with no quality control for Egyptian data, and down to 4% compared with 7.8% for the North African dialect. 
Arabic has a very co mp lex morphological system, though a very structured one. Character patterns are often indicative of word class and word segmentation. In this paper, we e xplore a novel approach to Arabic word segmentation and part-of-speech tagging relying on character info rmation. The approach is lexicon-free and does not require any morphological analysis, eliminat ing the factor of dictionary coverage. Using character-based analysis, the developed system yielded stateof-the-art accuracy comparing favourably with other taggers that involve external res o u rces . 
Dialectal Arabic has no standard orthographic representation. This creates a challenge when evaluating an Automatic Speech Recognition (ASR) system for dialect. Since the reference transcription text can vary widely from one user to another, we propose an innovative approach for evaluating dialectal speech recognition using Multi-References. For each recognized speech segments, we ask ﬁve different users to transcribe the speech. We combine the alignment for the multiple references, and use the combined alignment to report a modiﬁed version of Word Error Rate (WER). This approach is in favor of accepting a recognized word if any of the references typed it in the same form. Our method proved to be more effective in capturing many correctly recognized words that have multiple acceptable spellings. The initial WER according to each of the ﬁve references individually ranged between 76.4% to 80.9%. When considering all references combined, the Multi-References MR-WER was found to be 53%. 
We present a new and improved part of speech tagger for Arabic text that incorporates a set of novel features and constraints. This framework is presented within the MADAMIRA software suite, a state-of-the-art toolkit for Arabic language processing. Starting from a linear SVM model with basic lexical features, we add a range of features derived from morphological analysis and clustering methods. We show that using these features signiﬁcantly improves part-of-speech tagging accuracy, especially for unseen words, which results in better generalization across genres. The ﬁnal model, embedded in a sequential tagging framework, achieved 97.15% accuracy on the main test set of newswire data, which is higher than the current MADAMIRA accuracy of 96.91% while being 30% faster. 
Online Arabic content is growing very rapidly, with unmatched growth in Arabic structured resources. Systems that perform standard Natural Language Processing (NLP) tasks such as Named Entity Disambiguation (NED) struggle to deliver decent quality due to the lack of rich Arabic entity repositories. In this paper, we introduce EDRAK, an automatically generated comprehensive Arabic entity-centric resource. EDRAK contains more than two million entities together with their Arabic names and contextual keyphrases. Manual evaluation conﬁrmed the quality of the generated data. We are making EDRAK publicly available as a valuable resource to help advance research in Arabic NLP and IR tasks such as dictionary-based NamedEntity Recognition, entity classiﬁcation, and entity summarization. 
We propose a linguistically driven approach to represent discourse relations in Chinese text as sequences. We observe that certain surface characteristics of Chinese texts, such as the order of clauses, are overt markers of discourse structures, yet existing annotation proposals adapted from formalism constructed for English do not fully incorporate these characteristics. We present an annotated resource consisting of 325 articles in the Chinese Treebank. In addition, using this annotation, we introduce a discourse chunker based on a cascade of classiﬁers and report 70% top-level discourse sense accuracy. 
The manual Chinese word segmentation dataset WordSegCHC 1.0 which was built by eight crowdsourcing tasks conducted on the Crowdflower platform contains the manual word segmentation data of 152 Chinese sentences whose length ranges from 20 to 46 characters without punctuations. All the sentences received 200 segmentation responses in their corresponding crowdsourcing tasks and the numbers of valid response of them range from 123 to 143 (each sentence was segmented by more than 120 subjects). We also proposed an evaluation method called manual segmentation error rate (M SER) to evaluate the dataset; the M SER of the dataset is proved to be very low which indicates reliable data quality. In this work, we applied the crowdsourcing method to Chinese word segmentation task and the results confirmed again that the crowdsourcing method is a promising tool for linguistic data collection; the framework of crowdsourcing linguistic data collection used in this work can be reused in similar tasks; the resultant dataset filled a gap in Chinese language resources to the best of our knowledge, and it has potential applications in the research of word intuition of Chinese speakers and Chinese language processing. 
A central problem in research on automatic proficiency scoring is to differentiate the variability between and within groups of standard and non-standard speakers. Along with the effort to improve the robustness of techniques and models, we can also select test sentences that are more reliable for measuring the between-group variability. This study demonstrated that the performance of an automatic scoring system could be significantly improved by excluding “bad” sentences from the scoring procedure. The experiments on a dataset of Putonghua Shuiping Ceshi (Mandarin proficiency test) showed that, compared to all available sentences, using only best-performed sentences improved the speaker-level correlation between human and automatic scores from r = .640 to r = .824. 
While morphological information has been demonstrated to be useful for various Chinese NLP tasks, there is still a lack of complete theories, category schemes, and toolkits for Chinese morphology. This paper focuses on the morphological structures of Chinese bi-character words, where a corpus were collected based on a welldeﬁned morphological type scheme covering both Chinese derived words and compound words. With the corpus, a morphological analyzer is developed to classify Chinese bi-character words into the deﬁned categories, which outperforms strong baselines and achieves about 66% macro F-measure for compound words, and effectively covers derived words. 
This paper introduces the SIGHAN 2015 Bake-off for Chinese Spelling Check, including task description, data preparation, performance metrics, and evaluation results. The competition reveals current state-of-the-art NLP techniques in dealing with Chinese spelling checking. All data sets with gold standards and evaluation tool used in this bake-off are publicly available for future research. 
Increased interest in China from foreigners has led to a corresponding interest in the study of Chinese. However, the learning of Chinese by non-native speakers will encounter many difﬁculties, Chinese spelling check techniques for Chinese as a Foreign Language(CFL) learners is highly desirable. This paper presents our work on the SIGHAN-2015 Chinese Spelling Check task. The task focuses on spelling checking on Chinese essays written by CFL learners. We propose a uniﬁed framework called HANSpeller++ based on our previous HANSpeller for Chinese spelling correction. The framework consists of candidate generating,candidates re-ranking and ﬁnal global decision making. Experiments show good performance on the test data of the task. 
In order to detect Chinese spelling errors, especially for essays written by foreign learners, a word vector/conditional random field (CRF)based detector is proposed in this paper. The main idea is to project each word in a test sentence into a high dimensional vector space in order to reveal and examine their relationships by using a CRF. The results are then utilized to constrain the time-consuming language model rescoring procedure. Official SIGHAN2015 evaluation results show that our system did achieve reasonable performance with about 0.601/0.564 ac-curacies and 0.457/0.375 F1 scores in the detection/correction levels. 1! Introduction Chinese spelling check could be treated as an abnormal word sequence detection and correction problem. Convention approaches to do this job often heavenly rely on a language models (LM) trained from a large text corpus (for example Chinese Gigaword1) to find potential errors and provide suitable candidate words (Bengio 2003, Wang 2013) to replace them. These approaches usually could be successfully applied to examine essays written by Chinese element or junior school students. However, for essays written by foreign learners, conventional LM methods may not be so helpful. Because, the writing behaviors of foreign learners are usually different with native Chinese writers. They may embedded spelling errors into rarely used word sequences (low LM scores, but are somehow grammar or syntactic corrected). For example: !! @/)-1TSOU <Yee D+<(“+<” should beA”) 
The detection and correction of erroneous Chinese characters is an important problem in many applications. This paper proposed an automatic method for correcting erroneous Chinese characters. The method is divided into two parts, which separately handle two types of erroneous character: the occurrence of an erroneous character in a word length of one, and the occurrence in a word length of two or more. The first primarily makes use of a rulesbased method, while the second integrates parameters of similarity and syntax rationality using a linear regression model to predict erroneous characters. Experimental results shown that the F1 and FPR of the proposed method are 0.34 and 0.18 respectively. 
This paper presents the overview of Topic-based Chinese Message Polarity Classification in SIGHAN 2015 bake-off. Topic-based message polarity classification plays an important role in sentiment analysis, information extraction, event tracking, and other related research areas. This task is designed to evaluate the techniques for Chinese message polarity classification towards a given topic. The task organizers manually constructed 25 topics together with 24,374 corresponding messages which were annotated to construct the training and testing datasets. The evaluation results achieved by the participators provide good suggestion for the future research. 
Topic-based sentiment analysis for Chinese microblog aims to identify the user attitude on speciﬁed topics. In this paper, we propose a joint model by incorporating Support Vector Machines (SVM) and deep neural network to improve the performance of sentiment analysis. Firstly, a SVM Classiﬁer is constructed using N-gram, NPOS and sentiment lexicons features. Meanwhile, a convolutional neural network is applied to learn paragraph representation features as the input of another SVM classiﬁer. The classiﬁcation results outputted by these two classiﬁers are merged as the ﬁnal classiﬁcation results. The evaluations on the SIGHAN-8 Topic-based Chinese microblog sentiment analysis task show that our proposed approach achieves the second rank on micro average F1 and the fourth rank on macro average F1 among a total of 13 submitted systems. 
We describe our participation in the TopicBased Chinese Message Polarity Classiﬁcation Task, based on the restricted and unrestricted resources respectively. In the restricted resource based classiﬁcation, we focus on the selection of parameters in a multi-class classiﬁcation model with highly-biased training data. In the unrestricted resource based classiﬁcation, we explore the distributed representation of Chinese words through unsupervised feature learning and the annotation of salient samples through active learning, with a raw corpus of over 90 million messages extracted from Chinese Weibo Platform. For two classiﬁcation subtasks, our submitted results ranked the 4th and the 2nd respectively. 
This paper describes our system (MSIIP THU) used for Topic-Based Chinese Message Polarity Classiﬁcation Task in SIGHAN-8. In our system, a lexiconbased classiﬁer and a statistical machine learning-based classiﬁer are built up, followed by a linear combination of these two models. The overall performance of the proposed framework ranks in the middle of all terms participating in the task. 
The BI ( 比 )-structure, which highlights a contrasting characteristic between two items, is the key comparative sentence structure in Chinese. In this paper, we explore the methods of extracting the 6 constituents of the BI-structure. Previous studies are often restricted to probabilistic classification methods, where the feature used hardly embodies linguistic knowledge, therefore unintuitive. As an alternative, we propose the use of two linguistic knowledge-driven approaches, namely the POS chunking-based and TBL-based methods. The first model effectively captures grammatical restrictions over POS sequential patterns. The second model set up on new and lesser templates performs better than Brill’s (1995). Experimental results show that the proposed models are simple and effective methods for Chinese comparative element extraction task. 
This paper presents a Conditional Random Field (CRF) method of identifying prepositional phrases (PP) in Chinese patent documents. By using the CRF model, the identification process can be recognized as sequence labelling issue. After analyzing the characteristics of PP chunks in large scale corpus, we design several essential and helpful features and feature templates for recognizing PP chunks, and then use a CRF toolkit to train the model to identify PPs. At last, some experiments are conducted to justify the effects of the model, both the precision and recall rates are over 92%, higher than the baseline, indicating the method is reasonable and effective. 
Previous researches have focused on analyzing emotion through monolingual text, when in fact bilingual or code-switching posts are also common in social media. Despite the important implications of code-switching for emotion analysis, existing automatic emotion extraction methods fail to accommodate for the code-switching content. In this paper, we propose a general framework to construct and analyze the code-switching emotional posts in social media. We first propose an annotation scheme to identify the emotions associated with the languages expressing them in a Chinese-English code-switching corpus. We then make some observations and generate statistics from the corpus to analyze the linguistic phenomena of code-switching texts in social media. Finally, we propose a multiple-classifier-based automatic detection approach to detect emotion in the codeswitching corpus for evaluating the effectiveness of both Chinese and English texts. 
Grammatical Framework (GF) is a grammar formalism based on type theory and functional programming. It is also a platform for multilingual applications such as translation, localization, and information retrieval. To enable non-linguist programmers to build linguistically precise applications, GF provides a Resource Grammar Library (RGL), which deﬁnes the basic syntax, morphology, and lexicon of languages in the form of easily usable software libraries. The RGL is an open-source collaborative project, which currently covers 30 languages with a shared tree structure. Chinese, in addition to basic RGL, has a translation lexicon of over 30,000 lemmas and a mobile translation app. This paper gives an overview of GF, emphasizing applications where Chinese is related to other languages. We also address the theoretical question how Chinese ﬁts into the framework with a shared tree structure. 
We present an automated quick news system called KWB. KWB crawls and collects around the clock news items from over 120 news websites in mainland China, eliminates duplicates, and retrieves a summary of up to 600 characters for each news article using a proprietary summary engine. It then uses a Labeled-LDA classiﬁer to classify the remaining news items into 19 categories, computes popularity ranks called PopuRank of the newly collected news items in each category, and displays the summaries of news items in each category sorted according to PopuRank together with a picture, if there is any, on http://www.kuaiwenbao.com and mobile apps. We will describe in this paper the system architecture of KWB, the data crawler structure, the functionalities of the central database, and the deﬁnition of PopuRank. We will show, through experiments, the running time of obtaining PopuRank. We will also demonstrate the use of KWB. 
This paper presents an application of Chinese syntactic knowledge for semantic role labeling (SRL). Besides basic morphological information, syntactic structures are crucial in SRL. However, it is difﬁcult to learn such information from limited, small-scale, manually annotated training data. Instead of manually increasing the size of annotated data, we use a large amount of automatically extracted syntactic knowledge to improve the performance of SRL. 
This paper presents our system in the Chinese spelling check (CSC) task of SIGHAN-8 Bake-Off. Given a sentence, our systems are designed to detect and correct the spelling error. As we know, CSC is still a hot topic today and it is an open problem yet. N-gram language modeling (LM) is widely used in CSC, since its simplicity and power. We present a model based on joint bi-gram and trigram LM and Chinese word segmentation. Besides, we apply dynamic programming to increase efficiency and employ smoothing technique to address the sparseness of the n-gram in training data. The evaluation results show the utility of our CSC system. 
This paper describes details of NTOU Chinese spelling check system in SIGHAN-8 Bakeoff. Besides the basic architecture of the previous system participating in last two CSC tasks, three new preference rules were proposed to deal with Simplified Chinese characters, variants, sentence-final particles, and DE-particles. A new sentence likelihood function was proposed based on frequencies of space-removed version of Google n-gram datasets. Two formal runs were submitted where the best one was created by the system using Google n-gram frequency information. 
Sentiment analysis in social media has attracted signiﬁcant attention. Although researchers have proposed many methods, a single method is hard to meet requirement in industrial applications. In this paper, based on massive data of Tencent and industrial practice, we present a multilayered analysis system (MAS) on social media. The system is composed of three sub-systems, including topic correlation calculation, topic-related sentence recognition and sentence polarity classiﬁcation. Each sub-system is composed of several simple models. Also, we have set up a closed-loop feature mining and model updating system, which will continuously promote performance of MAS. In addition, this ofﬂine system requires very little intervention. The system, including online and ofﬂine parts, has been applied in several practical projects and obtained the best results in the evaluation of task 2 of SIGHAN-8. 
Weibo messages sentiment polarity classification towards given topics refers to that the machine automatically classifies whether the weibo message is of positive, negative, or neutral sentiment towards the given topic. The algorithm the sentiment analysis system CUCsas adopts to perform this task includes three steps: (1) whether there is an “exp” (short for “expression having evaluation meaning”) in the weibo message; (2) whether there is a semantic orientation relationship between the exp and topic; (3) the sentiment polarity classification of the exp. CUCsas completes step (1) based on the sentiment lexicon and sentiment value assignment rules, completes step (2) based on the topic extraction and sentiment polarity classification rule base, and completes step (3) based on the sentiment computing rules. Taking 20 given topics and a total of 19,469 weibo messages released by SIGHAN-2015 Bake-off as the test data, the overall F value of the rule-based system CUCsas is 0.69 in the unrestricted test. 
This paper describes the topic-based Chinese message polarity classification system submitted by LCYS_TEAM at SIGHAN8-Task2. The system mainly includes two parts: 1) a graph-based ranking model integrating local and global information is adopted to represent the classification ability of words towards different topics. In construction of graph model, a new weighting approach and a PMI-based random jumping probability selection method is proposed. 2) For sentimental features, word embedding is employed for acquiring expanded topical words and syntactic dependency is adopted for getting topic-related sentimental words. Experiment results demonstrate the effectiveness of our system. 
In this study, an automatic classification method based on the sentiment polarity of text is proposed. This method uses two sentiment dictionaries from different sources: the Chinese sentiment dictionary CSWN that integrates Chinese WordNet with SentiWordNet, and the sentiment dictionary obtained from a training corpus labeled with sentiment polarities. In this study, the sentiment polarity of text is analyzed using these two dictionaries, a mixed-rule approach, and a statistics-based prediction model. The proposed method is used to analyze a test corpus provided by the Topic-Based Chinese Message Polarity Classification task of SIGHAN-8, and the F1measure value is tested at 0.62. 
This paper presents our Chinese microblog sentiment classification (CMSC) system in the Topic-Based Chinese Message Polarity Classification task of SIGHAN-8 Bake-Off. Given a message from Chinese Weibo platform and a topic, our system is designed to classify whether the message is of positive, negative, or neutral sentiment towards the given topic. Due to the difficulties like the out-ofvocabulary Internet words and emoticons, polarity classification of Chinese microblogs is still an open problem today. In our system, Maximum Entropy (MaxEnt) is employed, which is a discriminative model that directly models the class posteriors, allowing them to incorporate a rich set of features. Moreover, oversampling approach is used to hand the unbalance problem. Evaluation results demonstrate the utility of our system, showing an accuracy of 66.4% for restricted resource and 66.6% for unrestricted resource. 
In this paper, we focus on topic-based microblog sentiment classiﬁcation task that classify the microblog’s sentiment polarities toward a speciﬁc topic. Most of the existing approaches for sentiment analysis usually adopt the target-independent strategy, which may assign irrelevant sentiments to the given topic. In this paper, we leverage the non-negative matrix factorization to get the relevant topic words and then further incorporate target-dependent features for topic-based microblog sentiment classiﬁcation. According to the experiment results, our system (NDMSCS) has achieved a good performance in the SIGHAN 8 Task 2. 
In this paper, we describe our system for the topic-based Chinese message polarity classification in SIGHAN 8 Task 2. Our system integrates two SVM classifiers which consist of LinearSVC and LibSVM to train the classification model and predict the results of Chinese message polarity in the restricted resource and the unrestricted resource, respectively. In order to assure our feature engineering effort on the task, we use some feature selection methods, such as LDA, word2vec, and sentiment lexicons including DLUT emotion ontology and NTUSD. Our system achieves the overall F1 score of 74.88% in the restricted evaluation and 74.43% in the unrestricted evaluation. 
Translated texts (in any language) are so markedly different from original ones that text classiﬁcation techniques can be used to tease them apart. Previous work has shown that awareness to these differences can signiﬁcantly improve statistical machine translation. These results, however, required meta-information on the ontological status of texts (original or translated) which is typically unavailable. In this work we show that the predictions of translationese classiﬁers are as good as meta-information. First, when a monolingual corpus in the target language is given, to be used for constructing a language model, predicting the translated portions of the corpus, and using only them for the language model, is as good as using the entire corpus. Second, identifying the portions of a parallel corpus that are translated in the direction of the translation task, and using only them for the translation model, is as good as using the entire corpus. We present results from several language pairs and various data sets, indicating that these results are robust and general. 
We present a method that improves data selection by combining a hybrid word/part-of-speech representation for corpora, with the idea of distinguishing between rare and frequent events. We validate our approach using data selection for machine translation, and show that it maintains or improves BLEU and TER translation scores while substantially improving vocabulary coverage and reducing data selection model size. Paradoxically, the coverage improvement is achieved by abstracting away over 97% of the total training corpus vocabulary using simple part-of-speech tags during the data selection process. 
 DFKI participated in the shared translation task of WMT 2015 with the GermanEnglish language pair in each translation direction. The submissions were generated using an experimental hybrid system based on three systems: a statistical Moses system, a commercial rule-based system, and a serial coupling of the two where the output of the rule-based system is further translated by Moses trained on parallel text consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM1 models based on POS 4-grams (contrastive submission). 
We build parallel FDA5 (ParFDA) Moses statistical machine translation (SMT) systems for all language pairs in the workshop on statistical machine translation (Bojar et al., 2015) (WMT15) translation task and obtain results close to the top with an average of 3.176 BLEU points difference using signiﬁcantly less resources for building SMT systems. ParFDA is a parallel implementation of feature decay algorithms (FDA) developed for fast deployment of accurate SMT systems (Bic¸ici, 2013; Bic¸ici et al., 2014; Bic¸ici and Yuret, 2015). ParFDA Moses SMT system we built is able to obtain the top TER performance in French to English translation. We make the data for building ParFDA Moses SMT systems for WMT15 available: https://github. com/bicici/ParFDAWMT15. 
This paper describes our WMT15 system submission for the translation task, a hybrid system for English-to-Czech translation. We repeat the successful setup from the previous two years. 
In this paper, the KIT systems submitted to the Shared Translation Task are presented. We participated in two translation directions: from German to English and from English to German. Both translations are generated using phrase-based translation systems. The performance of the systems was boosted by using language models built based on different tokens such as word, part-of-speech, and automacally generated word clusters. The difference in word order between German and English is addressed by part-of-speech and syntactic tree-based reordering models. In addition to a discriminative word lexicon, we used hypothesis rescoring using the ListNet algorithm after generating the translation with the phrase-based system. We evaluated the rescoring using only the baseline features as well as using additional computational complex features. 
This article describes the Aalto University entry to the English-to-Finnish shared translation task in WMT 2015. The system participates in the constrained condition, but in addition we impose some further constraints, using no language-speciﬁc resources beyond those provided in the task. We use a morphological segmenter, Morfessor FlatCat, but train and tune it in an unsupervised manner. The system could thus be used for another language pair with a morphologically complex target language, without needing modiﬁcation or additional resources. 
This paper describes the AFRL-MITLL statistical MT systems and the improvements that were developed during the WMT15 evaluation campaign. As part of these efforts we experimented with a number of extensions to the standard phrasebased model that improve performance on the Russian to English translation task creating three submission systems with different decoding strategies. Out of vocabulary words were addressed with named entity postprocessing. 
This paper presented the joined submission of KIT and LIMSI to the English to German translation task of WMT 2015. In this year submission, we integrated a neural network-based translation model into a phrase-based translation model by rescoring the n-best lists. Since the computation complexity is one of the main issues for continuous space models, we compared two techniques to reduce the computation cost. We investigated models using a structured output layer as well as models trained with noise contrastive estimation. Furthermore, we evaluated a new method to obtain the best log-linear combination in the rescoring phase. Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points. 
This paper describes the submission of the University of Edinburgh and the Johns Hopkins University for the shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). We set up phrase-based statistical machine translation systems for all ten language pairs of this year’s evaluation campaign, which are English paired with Czech, Finnish, French, German, and Russian in both translation directions. Novel research directions we investigated include: neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features. 
Neural machine translation (NMT) systems have recently achieved results comparable to the state of the art on a few translation tasks, including English→French and English→German. The main purpose of the Montreal Institute for Learning Algorithms (MILA) submission to WMT’15 is to evaluate this new approach on a greater variety of language pairs. Furthermore, the human evaluation campaign may help us and the research community to better understand the behaviour of our systems. We use the RNNsearch architecture, which adds an attention mechanism to the encoderdecoder. We also leverage some of the recent developments in NMT, including the use of large vocabularies, unknown word replacement and, to a limited degree, the inclusion of monolingual language models. 
This paper describes the GF Widecoverage MT system submitted to WMT 2015 for translation from English to Finnish. Our system uses a interlingua based approach, in which the interlingua is a shared formal representation, that abstracts syntactic structures over multiple languages. Our ﬁnal submission is a reranked system in which we combine this baseline MT system with a factored LM model. 
This paper describes LIMSI’s submissions to the shared WMT’15 translation task. We report results for French-English, Russian-English in both directions, as well as for Finnish-into-English. Our submissions use NCODE and MOSES along with continuous space translation models in a post-processing step. The main novelties of this year’s participation are the following: for Russian-English, we investigate a tailored normalization of Russian to translate into English, and a two-step process to translate ﬁrst into simpliﬁed Russian, followed by a conversion into inﬂected Russian. For French-English, the challenge is domain adaptation, for which only monolingual corpora are available. Finally, for the Finnish-to-English task, we explore unsupervised morphological segmentation to reduce the sparsity of data induced by the rich morphology on the Finnish side. 
This paper describes the UdS-Sant English–German Hybrid Machine Translation (MT) system submitted to the Translation Task organized in the Workshop on Statistical Machine Translation (WMT) 2015. Our proposed hybrid system brings improvements over the baseline system by incorporating additional knowledge such as extracted bilingual named entities and bilingual phrase pairs induced from example-based methods. The reported ﬁnal submission is the result of a hybrid system obtained from confusion network based system combination that combines the best performance of each individual system in a multi-engine pipeline. 
This paper describes the statistical machine translation system developed at RWTH Aachen University for the German→English translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). A phrase-based machine translation system was applied and augmented with hierarchical phrase reordering and word class language models. Further, we ran discriminative maximum expected BLEU training for our system. In addition, we utilized multiple feed-forward neural network language and translation models and a recurrent neural network language model for reranking. 
We present an experimental statistical tree-to-tree machine translation system based on the multi-bottom up tree transducer including rule extraction, tuning and decoding. Thanks to input parse forests and a “no pruning” strategy during decoding, the obtained translations are competitive. The drawbacks are a restricted coverage of 70% on test data, in part due to exact input parse tree matching, and a relatively high runtime. Advantages include easy redecoding with a different weight vector, since the full translation forests can be stored after the ﬁrst decoding pass. 
This paper provides an overview of the Shefﬁeld University submission to the WMT15 Translation Task for the FinnishEnglish language pair. The submitted translations were created from a system built using the CDEC decoder. Finnish is a morphologically rich language with elements such as nouns and verbs carrying a large number of inﬂectional types. Consequently, our improvements are based on morphology and include preprocessing steps to handle of morphological inﬂections inherent in the language, and which otherwise result in lexical sparsity and loss of information. 
This paper describes baseline systems for Finnish-English and English-Finnish machine translation using standard phrasebased and factored models including morphological features. We experiment with compound splitting and morphological segmentation and study the effect of adding noisy out-of-domain data to the parallel and the monolingual training data. Our results stress the importance of training data and demonstrate the effectiveness of morphological pre-processing of Finnish. 
This paper presents the machine translation systems submitted by the Abu-MaTran project for the Finnish–English language pair at the WMT 2015 translation task. We tackle the lack of resources and complex morphology of the Finnish language by (i) crawling parallel and monolingual data from the Web and (ii) applying rule-based and unsupervised methods for morphological segmentation. Several statistical machine translation approaches are evaluated and then combined to obtain our ﬁnal submissions, which are the top performing English-to-Finnish unconstrained (all automatic metrics) and constrained (BLEU), and Finnish-to-English constrained (TER) systems. 
 2 Methodology  In this year’s WMT translation task, Finnish-English was introduced as a language pair of competition for the ﬁrst time. We present experiments examining several variations on a morphologically-aware statistical phrase-based machine translation system for translating Finnish into English. Our system variations attempt to mitigate the issue of rich agglutinative morphology when translating from Finnish into English. Our WMT submission for Finnish-English preprocesses Finnish data with omorﬁ (Pirinen, 2015), a Finnish morphological analyzer. We also present results for two other language pairs with morphologically interesting source languages, namely German-English and Czech-English.  We use the current stable release (v3) of Moses, a state-of-the-art statistical phrase-based machine translation system. We trained translation models using the Europarl corpus (Koehn, 2005), using the latest available versions (v7 for German-English and CzechEnglish, and v8 for Finnish-English), as well as the Common Crawl corpus and News Commentary (v10) corpus for German-English and Czech-English, and the Wiki Headlines corpus for Finnish-English. We trained a back-off language model (LM) with modiﬁed Kneser-Ney smoothing (Katz, 1987; Kneser and Ney, 1995; Chen and Goodman, 1998) on the English Gigaword v5 corpus (Parker et al., 2011) using lmplz from KenLM (Heaﬁeld et al., 2013). 3 Finnish-English  
This paper describes the syntax-based systems built at the University of Edinburgh for the WMT 2015 shared translation task. We developed systems for all language pairs except French-English. This year we focused on: translation out of English using tree-to-string models; continuing to improve our English-German system; and source-side morphological segmentation of Finnish using Morfessor.  2 System Overview 2.1 Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We ﬁrst normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013).  
We present a hierarchical statistical machine translation system which supports discontinuous constituents. It is based on synchronous linear context-free rewriting systems (SLCFRS), an extension to synchronous context-free grammars in which synchronized non-terminals span k ≥ 1 continuous blocks on either side of the bitext. This extension beyond contextfreeness is motivated by certain complex alignment conﬁgurations that are beyond the alignment capacity of current translation models and their relatively frequent occurrence in hand-aligned data. Our experiments for translating from German to English demonstrate the feasibility of training and decoding with more expressive translation models such as SLCFRS and show a modest improvement over a context-free baseline. 
For several languages only potentially non-projective dependency parses are readily available. Projectivizing the parses and utilizing them in syntax-based translation systems often yields particularly bad translation results indicating that those translation models cannot properly utilize such information. We demonstrate that our system based on multi bottom-up tree transducers, which can natively handle discontinuities, can avoid the large translation quality deterioration, achieve the best performance of all classical syntax-based translation systems, and close the gap to phrase-based and hierarchical systems that do not utilize syntax. 
The log-linear combination of different features is an important component of SMT systems. It allows for the easy integartion of models into the system and is used during decoding as well as for nbest list rescoring. With the recent success of more complex models like neural network-based translation models, n-best list rescoring attracts again more attention. In this work, we present a new technique to train the log-linear model based on the ListNet algorithm. This technique scales to many features, considers the whole list and not single entries during learning and can also be applied to more complex models than a log-linear combination. Using the new learning approach, we improve the translation quality of a largescale system by 0.8 BLEU points during rescoring and generate translations which are up to 0.3 BLEU points better than other learning techniques such as MERT or MIRA. 
We propose a novel extended translation model (ETM) to counteract some problems in phrase-based translation: The lack of translation context when using singleword phrases and uncaptured dependencies beyond phrase boundaries. The ETM operates on word-level and augments the IBM models by an additional bilingual word pair and a reordering operation. Its implementation in a phrase-based decoder introduces translation and reordering dependencies for single-word phrases and dependencies across phrase boundaries. More, the model incorporates an explicit treatment of multiple and empty alignments. Its integration outperforms competitive systems that include lexical and phrase translation models as well as hierarchical reordering models on 4 language pairs signiﬁcantly by +0.7% BLEU on average. Although simpler and using fewer dependencies, the ETM proves to be on par with 7-gram operation sequence models (Durrani et al., 2013b). 
This work explores the application of recurrent neural network (RNN) language and translation models during phrasebased decoding. Due to their use of unbounded context, the decoder integration of RNNs is more challenging compared to the integration of feedforward neural models. In this paper, we apply approximations and use caching to enable RNN decoder integration, while requiring reasonable memory and time resources. We analyze the effect of caching on translation quality and speed, and use it to integrate RNN language and translation models into a phrase-based decoder. To the best of our knowledge, no previous work has discussed the integration of RNN translation models into phrase-based decoding. We also show that a special RNN can be integrated efﬁciently without the need for approximations. We compare decoding using RNNs to rescoring n-best lists on two tasks: IWSLT 2013 German→English, and BOLT Arabic→English. We demonstrate that the performance of decoding with RNNs is at least as good as using them in rescoring. 
We use referential translation machines (RTMs) for predicting translation performance. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain speciﬁc information or resource. We improve our RTM models with the ParFDA instance selection model (Bic¸ici et al., 2015), with additional features for predicting the translation performance, and with improved learning models. We develop RTM models for each WMT15 QET (QET15) subtask and obtain improvements over QET14 results. RTMs achieve top performance in QET15 ranking 1st in document- and sentence-level prediction tasks and 2nd in word-level prediction task. 
This paper introduces our SAU-KERC system that achieved F1 score of 0.39 in the world-level quality estimation task in WMT2015. The goal is to assign each translated word a “OK” or “BAD” label indicating translation quality. We adopt the sequence labeling model, conditional random fields (CRF), to predict the labels. Since “BAD” labels are rare in the training and development sets, recognition rate of "BAD" is low. To solve this problem, we propose two strategies. One is to replace “OK” label with sub-labels to balance label distribution. The other is to reconstruct the training set to include more "BAD" words. 
Translations generated by current statistical systems often have a large variance, in terms of their quality against human references. To cope with such variation, we propose to evaluate translations using a multi-level framework. The method varies the evaluation criteria based on the clusters to which a translation belongs. Our experiments on the WMT metric task data show that the multi-level framework consistently improves the performance of two benchmarking metrics, resulting in better correlation with human judgment. 
This paper describes VERTa’s submission to the 2015 EMNLP Workshop on Statistical Machine Translation. VERTa is a linguistically-motivated metric that combines linguistic features at different levels. In this paper, VERTa is described briefly, as well as the three versions submitted to the workshop: VERTa70Adeq30Flu, VERTa-EQ and VERTaW. Finally, the experiments conducted with the WMT14 data are reported and some conclusions are drawn. 
An important limitation of automatic evaluation metrics is that, when comparing Machine Translation (MT) to a human reference, they are often unable to discriminate between acceptable variation and the differences that are indicative of MT errors. In this paper we present UPF-Cobalt evaluation system that addresses this issue by penalizing the differences in the syntactic contexts of aligned candidate and reference words. We evaluate our metric using the data from WMT workshops of the recent years and show that it performs competitively both at segment and at system levels. 
This paper presents our metric (UoWLSTM) submitted in the WMT-15 metrics task. Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve the best results. We use a metric based on dense vector spaces and Long Short Term Memory (LSTM) networks, which are types of Recurrent Neural Networks (RNNs). For WMT15 our new metric is the best performing metric overall according to Spearman and Pearson (Pre-TrueSkill) and second best according to Pearson (TrueSkill) system level correlation. 
We propose the use of character n-gram F-score for automatic evaluation of machine translation output. Character ngrams have already been used as a part of more complex metrics, but their individual potential has not been investigated yet. We report system-level correlations with human rankings for 6-gram F1-score (CHRF) on the WMT12, WMT13 and WMT14 data as well as segment-level correlation for 6gram F1 (CHRF) and F3-scores (CHRF3) on WMT14 data for all available target languages. The results are very promising, especially for the CHRF3 score – for translation from English, this variant showed the highest segment-level correlations outperforming even the best metrics on the WMT14 shared evaluation task. 
We describe the submissions of ILLC UvA to the metrics and tuning tasks on WMT15. Both submissions are based on the BEER evaluation metric originally presented on WMT14 (Stanojevic´ and Sima’an, 2014a). The main changes introduced this year are: (i) extending the learning-to-rank trained sentence level metric to the corpus level (but still decomposable to sentence level), (ii) incorporating syntactic ingredients based on dependency trees, and (iii) a technique for ﬁnding parameters of BEER that avoid “gaming of the metric” during tuning. 
Human-designed sub-structures are required by most of the syntax-based machine translation evaluation metrics. In this paper, we propose a novel evaluation metric based on dependency parsing model, which does not need this human involvement. Experimental results show that the new single metric gets better correlation than METEOR on system level and is comparable with it on sentence level. To introduce more information, we combine the new metric with many other metrics. The combined metric obtains state-of-theart performance on both system level evaluation and sentence level evaluation on WMT 2014. 
We deﬁne a new algorithm, named “Drem”, for tuning the weighted linear model in a statistical machine translation system. Drem has two major innovations. First, it uses scaled derivative-free trust-region optimization rather than other methods’ line search or (sub)gradient approximations. Second, it interpolates the decoder output, using information about which decodes produced which translations. 
In this paper, we describe our submission to WMT 2015 Tuning Task. We integrate a dependency-based MT evaluation metric, RED, to Moses and compare it with BLEU and METEOR in conjunction with two tuning methods: MERT and MIRA. Experiments are conducted using hierarchical phrase-based models on Czech–English and English–Czech tasks. Our results show that MIRA performs better than MERT in most cases. Using RED performs similarly to METEOR when tuning is performed using MIRA. We submit our system tuned by MIRA towards RED to WMT 2015. In human evaluations, we achieve the 1st rank in all 7 systems on the English–Czech task and 6/9 on the Czech– English task. 
We show that, consistent with MEANTtuned systems that translate into Chinese, MEANT-tuned MT systems that translate into English also outperforms BLEUtuned systems across commonly used MT evaluation metrics, even in BLEU. The result is achieved by significantly improving MEANT’s sentence-level ranking correlation with human preferences through incorporating a more accurate distributional semantic model for lexical similarity and a novel backoff algorithm for evaluating MT output which automatic semantic parser fails to parse. The surprising result of MEANT-tuned systems having a higher BLEU score than BLEU-tuned systems suggests that MEANT is a more accurate objective function guiding the development of MT systems towards producing more adequate translation. 
Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accuracy. However, it is not clear whether an MT system that is better for human consumption is also better for CLQA. In this paper, we investigate the relationship between manual and automatic translation evaluation metrics and CLQA accuracy by creating a data set using both manual and machine translations and perform CLQA using this created data set.1 As a result, we ﬁnd that QA accuracy is closely related with a metric that considers frequency of words, and as a result of manual analysis, we identify 3 factors of translation results that affect CLQA accuracy. 
In English-to-Japanese translation, BLEU (Papineni et al., 2002), the de facto standard evaluation metric for machine translation (MT), has very weak correlation with human judgments (Goto et al., 2011; Goto et al., 2013). Therefore, RIBES (Isozaki et al., 2010; Hirao et al., 2014) was proposed. RIBES measures similarity of the word order of a machine-translated sentence and that of a corresponding human-translated reference sentence. RIBES has much stronger correlation than BLEU but most Japanese sentences have alternative word orders (scrambling), and one reference sentence is not sufﬁcient for fair evaluation. Isozaki et al. (2014) proposed a solution to this problem. This solution generates semantically equivalent word orders of reference sentences. Automatically generated word orders are sometimes incomprehensible or misleading, and they introduced a heuristic rule that ﬁlters out such bad sentences. However, their rule is too conservative and generated alternative word orders for only 30% of reference sentences. In this paper, we present a rule-free method that uses a dependency parser to check scrambled sentences and generated alternatives for 80% of sentences. The experimental results show that our method improves sentence-level correlation with human judgments. In addition, strong system-level correlation of single reference RIBES is not damaged very much. We expect this method can be applied to other languages such as German, Korean, ∗This work was done while the second author was a graduate student of Okayama Prefectural University.  0.0 NTCIR-7 JE RIBES JE BLEU NTCIR-9 JE RIBES JE BLEU EJ RIBES EJ BLEU NTCIR-10 JE RIBES JE BLEU EJ RIBES EJ BLEU  Spearman’s ρ with adequacy 0.2 0.4 0.6 0.8 1.0  Figure 1: RIBES has better correlation with adequacy than BLEU (system-level correlation)  Turkish, Hindi, etc. 
In this paper, we take a closer look at the MT evaluation process from a glass-box perspective using eye-tracking. We analyze two aspects of the evaluation task – the background of evaluators (monolingual or bilingual) and the sources of information available, and we evaluate them using time and consistency as criteria. Our ﬁndings show that monolinguals are slower but more consistent than bilinguals, especially when only target language information is available. When exposed to various sources of information, evaluators in general take more time and in the case of monolinguals, there is a drop in consistency. Our ﬁndings suggest that to have consistent and cost effective MT evaluations, it is better to use monolinguals with only target language information. 
In this paper, we enhance the traditional confusion network system combination approach with an additional model trained by a neural network. This work is motivated by the fact that the commonly used binary system voting models only assign each input system a global weight which is responsible for the global impact of each input system on all translations. This prevents individual systems with low system weights from having inﬂuence on the system combination output, although in some situations this could be helpful. Further, words which have only been seen by one or few systems rarely have a chance of being present in the combined output. We train a local system voting model by a neural network which is based on the words themselves and the combinatorial occurrences of the different system outputs. This gives system combination the option to prefer other systems at different word positions even for the same sentence. 
We address the problem of performing polarity classiﬁcation on Twitter over different languages, focusing on English and Spanish, comparing three techniques: (1) a monolingual model which knows the language in which the opinion is written, (2) a monolingual model that acts based on the decision provided by a language identiﬁcation tool and (3) a multilingual model trained on a multilingual dataset that does not need any language recognition step. Results show that multilingual models are even able to outperform the monolingual models on some monolingual sets. We introduce the ﬁrst code-switching corpus with sentiment labels, showing the robustness of a multilingual approach. 
We present a pilot study analyzing the connotative language found in a bilingual corpus of French and English headlines. We ﬁnd that (1) manual annotation of connotation at the word-level is more reliable than using segment-level judgments, (2) connotation polarity is often, but not always, preserved in reference translations produced by humans, (3) machine translated text does not preserve the connotative language identiﬁed by an English connotation lexicon. These lessons will helps us build new resources to learn better models of connotation and translation. 
The rise in popularity and ubiquity of Twitter has made sentiment analysis of tweets an important and well-covered area of research. However, the 140 character limit imposed on tweets makes it hard to use standard linguistic methods for sentiment classiﬁcation. On the other hand, what tweets lack in structure they make up with sheer volume and rich metadata. This metadata includes geolocation, temporal and author information. We hypothesize that sentiment is dependent on all these contextual factors. Different locations, times and authors have different emotional valences. In this paper, we explored this hypothesis by utilizing distant supervision to collect millions of labelled tweets from different locations, times and authors. We used this data to analyse the variation of tweet sentiments across different authors, times and locations. Once we explored and understood the relationship between these variables and sentiment, we used a Bayesian approach to combine these variables with more standard linguistic features such as n-grams to create a Twitter sentiment classiﬁer. This combined classiﬁer outperforms the purely linguistic classiﬁer, showing that integrating the rich contextual information available on Twitter into sentiment classiﬁcation is a promising direction of research. 
Sarcasm understanding may require information beyond the text itself, as in the case of ‘I absolutely love this restaurant!’ which may be sarcastic, depending on the contextual situation. We present the ﬁrst quantitative evidence to show that historical tweets by an author can provide additional context for sarcasm detection. Our sarcasm detection approach uses two components: a contrast-based predictor (that identiﬁes if there is a sentiment contrast within a target tweet), and a historical tweet-based predictor (that identiﬁes if the sentiment expressed towards an entity in the target tweet agrees with sentiment expressed by the author towards that entity in the past). 
Agile social media analysis involves building bespoke, one-off classiﬁcation pipelines tailored to the analysis of speciﬁc datasets. In this study we investigate how the DUALIST architecture can be optimised for agile social media analysis. We evaluate several semi-supervised learning algorithms in conjunction with a Na¨ıve Bayes model, and show how these modiﬁcations can improve the performance of bespoke classiﬁers for a variety of tasks on a large range of datasets. 
On an e-commerce site, product blurbs (short promotional statements) and user reviews give us a lot of information about products. While a blurb should be appealing to encourage more users to click on a product link, sometimes sellers may miss or misunderstand which aspects of the product are important to their users. We therefore propose a novel task: suggesting aspects of products for an advertisement improvement. As reviews have a lot of information about aspects from the perspective of users, review analysis enables us to suggest aspects that could attract more users. To achieve this, we break this task into the following two subtasks: aspect grouping and aspect group ranking. Aspect grouping enables us to treat product aspects at the semantic level rather than expression level. Aspect group ranking allows us to show users only aspects important for them. On the basis of experimental results using travel domain hotel data, we show that our proposed solution accomplishes NDCG@3 score of 0.739, which shows our solution is effective in achieving our goal. 
Opinion mining aims at summarizing the content of reviews for a speciﬁc brand, product, or manufacturer. However, the actual desire of a user is often one step further: Produce a ranking corresponding to speciﬁc needs such that a selection process is supported. In this work, we aim towards closing this gap. We present the task to rank products based on sentiment information and discuss necessary steps towards addressing this task. This includes, on the one hand, the identiﬁcation of gold rankings as a fundament for an objective function and evaluation and, on the other hand, methods to rank products based on review information. To demonstrate early results on that task, we employ real world examples of rankings as gold standard that are of interest to potential customers as well as product managers, in our case the sales ranking provided by Amazon.com and the quality ranking by Snapsort.com. As baseline methods, we use the average star ratings and review frequencies. Our best textbased approximation of the sales ranking achieves a Spearman’s correlation coefﬁcient of ρ = 0.23. On the Snapsort data, a ranking based on extracting comparisons leads to ρ = 0.51. In addition, we show that aspect-speciﬁc rankings can be used to measure the impact of speciﬁc aspects on the ranking. 
Opinions in social media play such an important role for customers and companies that there is a growing tendency to post fake reviews in order to change purchase decisions and opinions. In this paper we propose the use of different features for a low dimension representation of opinions. We evaluate our proposal incorporating the features to a Support Vector Machines classiﬁer and we use an available corpus with reviews of hotels in Chicago. We perform comparisons with previous works and we conclude that using our proposed features it is possible to obtain competitive results with a small amount of features for representing the data. Finally, we also investigate if the use of emotions can help to discriminate between truthful and deceptive opinions as previous works show to happen for deception detection in text in general. 
In this contribution, we report on an effort to annotate German data with information relevant to opinion inference. Such information has previously been referred to as effect or couched in terms of eventevaluation functors. We extend the theory and present an extensive scheme that combines both approaches and thus extends the set of inference-relevant predicates. Using these guidelines to annotate 726 German synsets, we achieve good inter-annotator agreement. 
Contemporary sentiment analysis approaches rely heavily on lexicon based methods. This is mainly due to their simplicity, although the best empirical results can be achieved by more complex techniques. We introduce a method to assess suitability of generic sentiment lexicons for a given domain, namely to identify frequent bigrams where a polar word switches polarity. Our bigrams are scored using Lexicographers Mutual Information and leveraging large automatically obtained corpora. Our score matches human perception of polarity and demonstrates improvements in classiﬁcation results using our enhanced contextaware method. Our method enhances the assessment of lexicon based sentiment detection algorithms and can be further used to quantify ambiguous words. 
The form of a thesaurus often restricts its use to word look ups and ﬁnding related words. We present Imagisaurus, an online interactive visualizer for the Roget’s Thesaurus, which not only provides a way for word lookups but also helps users quickly grasp the nature and size of the thesaurus taxonomy. Imagisaurus connects thesaurus entries with a large valence and emotion association lexicon. Easy-touse sliders give the user ﬁne control over depicting only those categories with the desired strength of association with positive or negative sentiment, as well as eight basic emotions. A second interactive visualization is used to explore the emotion lexicon. Both the Roget’s Thesaurus and the emotion lexicon have tens of thousands of entries. Our visualizers help users better understand these lexical resources in terms of their make up as a whole. 
Psychology research suggests that certain personality traits correlate with linguistic behavior. This correlation can be effectively modeled with statistical natural language processing techniques. Prediction accuracy generally improves with larger data samples, which also allows for more lexical features. Most existing work on personality prediction, however, focuses on small samples and closed-vocabulary investigations. Both factors limit the generality and statistical power of the results. In this paper, we explore the use of social media as a resource for large-scale, openvocabulary personality detection. We analyze which features are predictive of which personality traits, and present a novel corpus of 1.2M English tweets annotated with Myers-Briggs personality type and gender. Our experiments show that social media data can provide sufﬁcient linguistic evidence to reliably predict two of four personality dimensions. 
The paper describes the ﬁrst sophisticated negation scope detection system for Twitter sentiment analysis. The system has been evaluated both on existing corpora from other domains and on a corpus of English Twitter data (tweets) annotated for negation. It produces better results than what has been reported in other domains and improves the performance on tweets containing negation when incorporated into a state-of-the-art Twitter sentiment analyser. 
Sentiment lexicons and other linguistic knowledge proved to be beneﬁcial in polarity classiﬁcation. This paper introduces a linguistically informed Convolutional Neural Network (lingCNN), which incorporates this valuable kind of information into the model. We present two intuitive and simple methods: The ﬁrst one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 
This short paper describes a sentiment analysis system for micro-post data that includes analysis of tweets from Twitter and Short Messaging Service (SMS) text messages. We discuss our system that makes use of Word Sense Disambiguation techniques in sentiment analysis at the message level, where the entire tweet or SMS text was analysed to determine its dominant sentiment. Previous work done in the area of Word Sense Disambiguation does not throw light on its inﬂuence on the analysis of social-media text and micropost data, which is what our work aims to achieve. Our experiments show that the use of Word Sense Disambiguation alone has resulted in an improved sentiment analysis system that outperforms systems built without incorporating Word Sense Disambiguation. 
With microblogging platforms such as Twitter generating huge amounts of textual data every day, the possibilities of knowledge discovery through Twitter data becomes increasingly relevant. Similar to the public voting mechanism on websites such as the Internet Movie Database (IMDb) that aggregates movies ratings, Twitter content contains reflections of public opinion about movies. This study aims to explore the use of Twitter content as textual data for predictive text mining. In this study, a corpus of tweets was compiled to predict the rating scores of newly released movies on IMDb. Predictions were done with several different machine learning algorithms, exploring both regression and classification methods. In addition, this study explores the use of several different kinds of textual features in the machine learning tasks. Results show that prediction performance based on textual features derived from our corpus of tweets improved on the baseline for both regression and classification tasks. 
This paper presents the methodology and results of a project for the large-scale analysis of public messages in political discourse on Facebook, the dominant social media site in Hungary. We propose several novel social psychologymotivated dimensions for natural language processing-based text analysis that go beyond the standard sentiment-based analysis approaches. Communion describes the moral and emotional aspects of an individual’s relations to others, while agency describes individuals in terms of the efficiency of their goalorientated behavior. We treat these by custom lexicons that identify positive and negative cues in text. We measure the level of optimism in messages by examining the ratio of events talked about in the past, present and future by looking at verb tenses and temporal expressions. For assessing the level of individualism, we build on research that correlates it to pronoun dropping. We also present results that demonstrate the viability of our measures on 1.9 million downloaded public Facebook comments by examining correlation to party preferences in public opinion poll data.  
We introduce description logics as a means to carry out sentiment inferences triggered by some verbs on their semantic roles. Verbs might impose polar effects on some roles, but also have polar expectations on other roles. For instance, an entity inherits a positive polarity just by being actor of some verb (“she succeeds”). More complicated scenarios arise if we take subclause embeddings, negation and polarity conﬂicts into consideration. Polarity propagation and effect inversion need to be coped with, then. We have implemented a prototype in OWL covering a substantial subset of our verb lexicon covering about 140 German verbs. 
In this paper we introduce a novel computational technique of extraction of personality traits (HEXACO) of employees from Enterprise Social Media posts. We deal with challenges such as not being able to use existing survey instruments for scoring and not being able to directly use existing psychological studies on written text due to lack of overlapping words between the existing dictionary and words used in Enterprise Social Media. Using our approach we are able to infer personality traits (HEXACO) from posts and ﬁnd better coverage and usage of the extended dictionary. 
We offer a critical review of the current state of opinion role extraction involving opinion verbs. We argue that neither the currently available lexical resources nor the manually annotated text corpora are sufﬁcient to appropriately study this task. We introduce a new corpus focusing on opinion roles of opinion verbs from the Subjectivity Lexicon and show potential beneﬁts of this corpus. We also demonstrate that state-of-the-art classiﬁers perform rather poorly on this new dataset compared to the standard dataset for the task showing that there still remains signiﬁcant research to be done. 
Natural language is a common type of input for data processing systems. Therefore, it is often required to have a large testing data set of this type. In this context, the task to automatically generate natural language texts, which maintain the properties of real texts is desirable. However, current synthetic data generators do not capture natural language text data sufﬁciently. In this paper, we present a preliminary study on different generative models for text generation, which maintain speciﬁc properties of natural language text, i.e., the sentiment of a review text. In a series of experiments using different data sets and sentiment analysis methods, we show that generative models can generate texts with a speciﬁc sentiment and that hidden Markov model based text generation achieves less accuracy than Markov chain based text generation, but can generate a higher number of distinct texts. 
A support vector classiﬁer was compared to a lexicon-based approach for the task of detecting the stance categories speculation, contrast and conditional in English consumer reviews. Around 3,000 training instances were required to achieve a stable performance of an F-score of 90 for speculation. This outperformed the lexicon-based approach, for which an Fscore of just above 80 was achieved. The machine learning results for the other two categories showed a lower average (an approximate F-score of 60 for contrast and 70 for conditional), as well as a larger variance, and were only slightly better than lexicon matching. Therefore, while machine learning was successful for detecting speculation, a well-curated lexicon might be a more suitable approach for detecting contrast and conditional. 
This paper seeks to identify sentiment and non-sentiment bearing hashtags by combining existing lexical resources. By using a lexicon-based approach, we achieve 86.3% and 94.5% precision in identifying sentiment and non-sentiment hashtags, respectively. Moreover, results obtained from both of our classiﬁcation models demonstrate that using combined lexical, emotion and word resources is more effective than using a single resource in identifying the two types of hashtags. 
Probabilistic learning models have the ability to be calibrated to improve the performance of tasks such as sentiment classiﬁcation. In this paper, we introduce a framework for sentiment classiﬁcation that enables classiﬁer recalibration given the presence of related, context-bearing documents. We investigate the use of probabilistic thresholding and document similarity based recalibration methods to yield classiﬁer improvements. We demonstrate the performance of our proposed recalibration methods on a dataset of online clinical reviews from the patient feedback domain that have adjoining management responses that yield sentiment bearing information. Experimental results show the proposed recalibration methods outperform uncalibrated supervised machine learning models trained for sentiment analysis, and yield signiﬁcant improvements over a robust baseline. 
We propose visually-veriﬁable textual entailment as a challenge task for the emerging ﬁeld of combining language and vision. This task is a variant of the wellstudied NLP task of recognizing textual entailment (Dagan et al., 2006) where every entailment judgment can be made purely by reasoning with visual knowledge. We believe that this task will spur innovation in the language and vision ﬁeld while simultaneously producing inference algorithms that can be used in NLP. 
Multimodal integration of visual and linguistic data is a longstanding but crucial challenge for modeling human understanding. We propose a framework that uses an unsupervised bitext alignment method to integrate visual and linguistic data. We present an empirical study of the various parameters of the framework. Our results exceed baselines using both exact and delayed temporal correspondence. The resulting alignments can be used for image classiﬁcation and retrieval. 
This poster presents a pilot where audio description is used to enhance automatic content analysis, for a project aiming at creating a tool for easy access to large AV archives. 
The automatic generation of image captions has received considerable attention. The problem of evaluating caption generation systems, though, has not been that much explored. We propose a novel evaluation approach based on comparing the underlying visual semantics of the candidate and ground-truth captions. With this goal in mind we have deﬁned a semantic representation for visually descriptive language and have augmented a subset of the Flickr-8K dataset with semantic annotations. Our evaluation metric (BAST) can be used not only to compare systems but also to do error analysis and get a better understanding of the type of mistakes a system does. To compute BAST we need to predict the semantic representation for the automatically generated captions. We use the Flickr-ST dataset to train classiﬁers that predict STs so that evaluation can be fully automated 1. 
We examine the possibility that recent promising results in automatic caption generation are due primarily to language models. By varying image representation quality produced by a convolutional neural network, we ﬁnd that a state-of-theart neural captioning algorithm is able to produce quality captions even when provided with surprisingly poor image representations. We replicate this result in a new, ﬁne-grained, transfer learned captioning domain, consisting of 66K recipe image/title pairs. We also provide some experiments regarding the appropriateness of datasets for automatic captioning, and ﬁnd that having multiple captions per image is beneﬁcial, but not an absolute requirement. 
A key task to understand an image and its corresponding caption is not only to ﬁnd out what is shown on the picture and described in the text, but also what is the exact relationship between these two elements. The long-term objective of our work is to be able to distinguish different types of relationship, including literal vs. non-literal usages, as well as ﬁnegrained non-literal usages (i.e., symbolic vs. iconic). Here, we approach this challenging problem by answering the question: ‘How can we quantify the degrees of similarity between the literal meanings expressed within images and their captions?’. We formulate this problem as a ranking task, where links between entities and potential regions are created and ranked for relevance. Using a Ranking SVM allows us to leverage from the preference ordering of the links, which help us in the similarity calculation for the cases of visual or textual ambiguity, as well as misclassiﬁed data. Our experiments show that aggregating different features using a supervised ranker achieves better results than a baseline knowledge-base method. However, much work still lies ahead, and we accordingly conclude the paper with a detailed discussion of a short- and longterm outlook on how to push our work on relationship classiﬁcation one step further. 
The problem of lack of training images becomes even more sever when we target recognition problems within a general category, i.e., subordinate categorization, for example building classiﬁers for different bird species or ﬂower types (estimated over 10000 living bird species, similar for ﬂowers). 
Semantic data regarding points of interest in urban areas are hard to visualize. Due to the high number of points and categories they belong, as well as the associated textual information, maps become heavily cluttered and hard to read. Using traditional visualization techniques (e.g. dot distribution maps, typographic maps) partially solve this problem. Although, these techniques address different issues of the problem, their combination is hard and typically results in an efﬁcient visualization. In our approach, we present a method to represent clusters of points of interest as shapes, which is based on vacuum package metaphor. The calculated shapes characterize sets of points and allow their use as containers for textual information. Additionally, we present a strategy for placing text onto polygons. The suggested method can be used in interactive visual exploration of semantic data distributed in space, and for creating maps with similar characteristics of dot distribution maps, but using shapes instead of points. 
We address the problem of interactively learning perceptually grounded word meanings in a multimodal dialogue system. We design a semantic and visual processing system to support this and illustrate how they can be integrated. We then focus on comparing the performance (Precision, Recall, F1, AUC) of three state-of-the-art attribute classiﬁers for the purpose of interactive language grounding (MLKNN, DAP, and SVMs), on the aPascal-aYahoo datasets. In prior work, results were presented for object classiﬁcation using these methods for attribute labelling, whereas we focus on their performance for attribute labelling itself. We ﬁnd that while these methods can perform well for some of the attributes (e.g. head, ears, furry) none of these models has good performance over the whole attribute set, and none supports incremental learning. This leads us to suggest directions for future work. 
Semantically complex queries which include attributes of objects and relations between objects still pose a major challenge to image retrieval systems. Recent work in computer vision has shown that a graph-based semantic representation called a scene graph is an effective representation for very detailed image descriptions and for complex queries for retrieval. In this paper, we show that scene graphs can be effectively created automatically from a natural language scene description. We present a rule-based and a classiﬁerbased scene graph parser whose output can be used for image retrieval. We show that including relations and attributes in the query graph outperforms a model that only considers objects and that using the output of our parsers is almost as effective as using human-constructed scene graphs (Recall@10 of 27.1% vs. 33.4%). Additionally, we demonstrate the general usefulness of parsing to scene graphs by showing that the output can also be used to generate 3D scenes. 
We introduce the task of visualizing distributed semantic representations by generating images from word vectors. Given the corpus-based vector encoding the word broccoli, we convert it to a visual representation by means of a cross-modal mapping function, and then use the mapped representation to generate an image of broccoli as “dreamed” by the distributed model. We propose a baseline dream synthesis method based on averaging pictures whose visual representations are topologically close to the mapped vector. Two experiments show that we generate dreams that generally belong to the the right semantic category, and are sometimes accurate enough for subjects to distinguish the intended concept from a related one. 
Demographic attribute inference of social networking service (SNS) users is a valuable application for marketing and for targeting advertisements. Several studies have examined Twitter-user gender inference in natural language processing, image recognition, and other research domains. Reportedly, a combined approach using text data and image data outperforms an individual data approach. This paper presents a proposal of a novel hybrid approach. A salient beneﬁt of our system is that features provided from a text classiﬁer and from an image classiﬁer are combined appropriately to infer male or female gender using logistic regression. The experimentally obtained results demonstrate that our approach markedly improves an existing combination-based method. 
In this paper we present a free, open source platform, that translates in real time (written) European Portuguese into Portuguese Sign Language, being the signs produced by an avatar. We discuss basic needs of such a system in terms of Natural Language Processing and Animation Synthesis, and propose an architecture for it. Moreover, we have selected a set of existing tools that couple with our free, open-source philosophy, and implemented a prototype with them. Several case studies were conducted. A preliminary evaluation was done and, although the translation possibilities are still scarce and some adjustments still need to be done, our platform was already much welcomed by the deaf community. 
The context for the work we report here is the automatic description of spatial relationships between pairs of objects in images. We investigate the task of selecting prepositions for such spatial relationships. We describe the two datasets of object pairs and prepositions we have created for English and French, and report results for predicting prepositions for object pairs in both of these languages, using two methods: (a) an existing approach which manually ﬁxes the mapping from geometrical features to prepositions, and (b) a Naive Bayes classiﬁer trained on the English and French datasets. For the latter we use features based on object class labels and geometrical measurements of object bounding boxes. We evaluate the automatically generated prepositions on unseen data in terms of accuracy against the human-selected prepositions. 
This paper investigates whether the wider context in which a sentence is located can contribute to a distributional representation of sentence meaning. We compare a vector space for sentences in which the features are words occurring within the sentence, with two new vector spaces that only make use of surrounding context. Experiments on simple subject-verbobject similarity tasks show that all sentence spaces produce results that are comparable with previous work. However, qualitative analysis and user experiments indicate that extra-sentential contexts capture more diverse, yet topically coherent information. 
The main contribution of this paper is a cross-linguistic empirical analysis of two interacting levels of linguistic analysis of written text: situation entity (SE) types, the semantic types of situations evoked by clauses of text, and discourse modes (DMs), a characterization of passages at the sub-document level. We adapt an existing annotation scheme for SEs in English to be used for German data, with a detailed discussion of the most important differences. We create the ﬁrst parallel corpus annotated for SEs, and the ﬁrst DM-annotated corpus. We ﬁnd that: (a) the adapted scheme is supported by evidence from a large-scale experimental study; (b) SEs mainly correspond to each other in parallel text, and a large part of the mismatches are systematic; (c) the DM annotation task can be performed intuitively with reasonable agreement; and (d) the annotated DMs show the predicted differences in the distributions of SE types. 
Discourse relations are a bridge between sentence-level semantics and discourselevel semantics. They can be signalled explicitly with discourse connectives or conveyed implicitly, to be inferred by a comprehender. The same discourse units can be related in more than one way, signalled by multiple connectives. But multiple connectives aren’t necessary: Multiple relations can be conveyed even when only one connective is explicit. This paper describes the initial phase in a larger experimental study aimed at answering two questions: (1) Given an explicit discourse adverbial, what discourse relation(s) do naive subjects take to be operative, and (2) Can this be predicted on the basis of the explicit adverbial alone, or does it depend instead on other factors? 
In this paper we present ongoing work to produce an expressive TTS reader that can be used both in text and dialogue applications. The system has been previously used to read (English) poetry and it has now been extended to apply to short stories. The text is fully analyzed both at phonetic and phonological level, and at syntactic and semantic level. The core of the system is the Prosodic Manager which takes as input discourse structures and relations and uses this information to modify parameters for the TTS accordingly. The text is transformed into a poem-like structures, where each line corresponds to a Breath Group, semantically and syntactically consistent. Stanzas correspond to paragraph boundaries. Analogical parameters are related to ToBI theoretical indices but their number is doubled. 
Modal verbs have different interpretations depending on their context. Previous approaches to modal sense classiﬁcation achieve relatively high performance using shallow lexical and syntactic features. In this work we uncover the difﬁculty of particular modal sense distinctions by eliminating both distributional bias and sparsity of existing small-scale annotated corpora used in prior work. We build a semantically enriched model for modal sense classiﬁcation by novelly applying features that relate to lexical, proposition-level, and discourse-level semantic factors. Besides improved classiﬁcation performance, especially for difﬁcult sense distinctions, closer examination of interpretable feature sets allows us to obtain a better understanding of relevant semantic and contextual factors in modal sense classiﬁcation. 
Lexical cues are linguistic expressions that can signal the presence of a rhetorical relation. However, such cues can be ambiguous as they may signal more than one relation or may not always function as a relation indicator. In this study, we ﬁrst conduct a corpus-based analysis to derive a set of n-grams as potential lexical cues. These cues are then utilized in graph-based probabilistic models to determine the syntactic context in which the cue is signaling the presence of a particular relation. Evaluation results are reported for various cues of the CIRCUMSTANCE relation, conﬁrming the value of syntactic features for the task of cue disambiguation in the context of Rhetorical Structure Theory. Moreover, using a graph to encode syntactic information is shown to be a more generalizable and effective approach compared to the direct usage of syntactic features. 
It is in PropBank’s ARGM annotation of clausal adjuncts that sentential semantics meets discourse relation annotation in the Penn Discourse TreeBank. This paper discusses complementarities between the two annotation systems: How PropBank ARGM annotation can be used to seed annotation of additional discourse relations in the PDTB, and how PDTB annotation can be used to reﬁne or enrich PropBank ARGM annotation. 
This paper targets an understanding of how metadiscourse functions in spoken language. Starting from a metadiscourse taxonomy, a set of TED talks is annotated via crowdsourcing and then a lexical grade level predictor is used to map the distribution of the distinct discourse functions of the taxonomy across levels. The paper concludes showing how speakers use these functions in presentational settings. 
The goal of paraphrase identiﬁcation is to decide whether two given text fragments have the same meaning. Of particular interest in this area is the identiﬁcation of paraphrases among short texts, such as SMS and Twitter. In this paper, we present idiomatic expressions as a new domain for short-text paraphrase identiﬁcation. We propose a technique, utilizing idiom deﬁnitions and continuous space word representations that performs competitively on a dataset of 1.4K annotated idiom paraphrase pairs, which we make publicly available for the research community. 
We present a toy world model for interpreting textual descriptions of the movement record of a historical ﬁgure such as Genghis Khan or Napoleon. We cast the problem of document understanding as the task of ﬁnding episodes that do not violate the soft constraint conditions derived from the document. The model thus allows us to infer his or her locations by ﬁnding multiple solutions of an optimization problem. Our experimental results using Wikipedia text on Alexander the Great demonstrate that such inference can indeed be performed with reasonable accuracy. We also show that the information obtained from such inference is useful in solving a hard coreference resolution problem. 
High agreement is a common objective when annotating data for word senses. However, a number of factors make perfect agreement impossible, e.g. the limitations of sense inventories, the difﬁculty of the examples or the interpretation preferences of the annotators. Estimating potential agreement is thus a relevant task to supplement the evaluation of sense annotations. In this article we propose two methods to predict agreement on wordannotation instances. We experiment with a continuous representation and a threeway discretization of observed agreement. In spite of the difﬁculty of the task, we ﬁnd that different levels of agreement can be identiﬁed—in particular, low-agreement examples are easier to identify. 
In this position paper we argue that an adequate semantic model must account for language in use, taking into account how discourse context affects the meaning of words and larger linguistic units. Distributional semantic models are very attractive models of meaning mainly because they capture conceptual aspects and are automatically induced from natural language data. However, they need to be extended in order to account for language use in a discourse or dialogue context. We discuss phenomena that the new generation of distributional semantic models should capture, and propose concrete tasks on which they could be tested. 
Cancer stages, which summarizes extent of cancer progression, is an important tool for evidence-based medical research. However, they are not always recorded in the electronic medical record. In this paper, we describe work for annotating a medical text corpus with the goal of predicting patient level liver cancer staging in hepatocellular carcinoma (HCC) patients. Our annotation consisted of identifying 11 parameters, used to calculate liver cancer staging, at the text span level as well as at the patient level. Also at the patient level, we annotated stages for three commonly-used liver cancer staging schemes. Our inter-rater agreement showed text annotation consistency 0.73 F1 for partial text match and 0.91 F1 at the patient level. After annotation, we performed several document classiﬁcation experiments for the text span annotations using standard machine learning classiﬁers, including decision trees, maximum entropy, naive Bayes and support vector machines. Thereby, we identiﬁed baseline performances for our task at 0.63 F1 as well as strategies for future improvement. 
Online health forums provide advice and emotional solace to their users from a social network of people who have faced similar conditions. Continued participation of users is thus critical to their success. In this paper, we develop machine learning models for predicting whether or not a user will continue to participate in an online health forum. The prediction models are trained and tested over a large dataset collected from the support group based social networking site dailystrength.org. We ﬁnd that our models can predict continued participation with over 83% accuracy after as little as 1 month observing the user’s activities, and that performance increases rapidly up to 1 year of observation. We also show that features such as the time since a user’s last activity are consistently predictive regardless of the length of the observation period, while other features, such as the number of times a user replies to others, decrease in predictiveness as the observation period grows. 
The use of Electronic Health Records (EHRs) is becoming more prevalent in healthcare institutions world-wide. These digital records contain a wealth of information on patients’ health in the form of Natural Language text. The electronic format of the clinical notes has evident advantages in terms of storage and shareability, but also makes it easy to duplicate information from one document to another through copy-pasting. Previous studies have shown that (copy-paste-induced) redundancy can reach high levels in American EHRs, and that these high levels of redundancy have a negative effect on the performance of Natural Language Processing (NLP) tools that are used to process EHRs automatically. In this paper, we present a preliminary study on the level of redundancy in French EHRs. We study the evolution of redundancy over time, and its occurrence in respect to different document types and sections in a small corpus comprising of three patient records (361 documents). We ﬁnd that average redundancy levels in our subset are lower than those observed in U.S. corpora (respectively 33% vs. up to 78%), which may indicate different cultural practices between these two countries. Moreover, we ﬁnd no evidence of the incremental increase (over time) of redundant text in clinical notes which has been found in American EHRs. These results suggest that redundancy mitigating strategies may not be needed when processing French EHRs.  
De-identiﬁcation aims at preserving patient conﬁdentiality while enabling the use of clinical documents for furthering medical research. Herein, we aim to evaluate whether patient re-identiﬁcation is possible on a corpus of de-identiﬁed clinical documents in French. Personal Health Identiﬁers are automatically marked by a de-identiﬁcation system applied to the corpus, followed by reintroduction of plausible surrogates. The resulting documents are shown to individuals with varying knowledge of the documents and de-identiﬁcation method. The individuals are asked to re-identify the patients. The amount of information recovered increases with familiarity with the documents and/or de-identiﬁcation method. Surrogate re-introduction with localization from the same (vs. different) geographical area as the original documents is found more effective. The amount of information recovered was not sufﬁcient to re-identify any of the patients, except when privileged access to the hospital health information system and several documents about the same patient were available. 
Choosing the right tokenizer is a non-trivial task, especially in the biomedical domain, where it poses additional challenges, which if not resolved means the propagation of errors in successive Natural Language Processing analysis pipeline. This paper aims to identify these problematic cases and analyze the output that, a representative and widely used set of tokenizers, shows on them. This work will aid the decision making process of choosing the right strategy according to the downstream application. In addition, it will help developers to create accurate tokenization tools or improve the existing ones. A total of 14 problematic cases were described, showing biomedical samples for each of them. The outputs of 12 tokenizers were provided and discussed in relation to the level of agreement among tools. 
Communication of follow-up recommendations when abnormalities are identified on imaging studies is prone to error. The absence of an automated system to identify and track radiology recommendations is an important barrier to ensuring timely follow-up of patients especially with non-acute incidental findings on imaging studies. We are in the process of building a natural language processing (NLP) system to identify follow-up recommendations in free-text radiology reports. In this paper, we describe our efforts in creating a multiinstitutional radiology report corpus annotated for follow-up recommendation information. The annotated corpus will be used to train and test the NLP system. 
Health campaigns that aim to raise awareness and subsequently raise funds for research and treatment are commonplace. While many local campaigns exist, very few attract the attention of a global audience. One of those global campaigns is Movember, an annual campaign during the month of November, that is directed at men’s health with special foci on cancer & mental health. Health campaigns routinely use social media portals to capture people’s attention. Recently, researchers began to consider to what extent social media is effective in raising the awareness of health campaigns. In this paper we expand on those works by conducting an investigation across four different countries, while not only restricting ourselves to the impact on awareness but also on fund-raising. To that end, we analyze the 2013 Movember Twitter campaigns in Canada, Australia, the United Kingdom and the United States. 
National cancer registries collect cancer related information from multiple sources and make it available for research. Part of this information originates from pathology reports, and in this pre-study the possibility of a system for automatic extraction of information from Norwegian pathology reports is investigated. A set of 40 pathology reports describing breast cancer tissue samples has been used to develop a rule based system for information extraction. To validate the performance of this system its output has been compared to the data produced by experts doing manual encoding of the same pathology reports. On average, a precision of 80%, a recall of 98% and an F-score of 86% has been achieved, showing that such a system is indeed feasible. 
We present a distributional approach to the problem of inducing parameters for unseen words in probabilistic parsers. Our KNN-based algorithm uses distributional similarity over an unlabelled corpus to match unseen words to the most similar seen words, and can induce parameters for those unseen words without retraining the parser. We apply this to domain adaptation for three different parsers that employ ﬁne-grained syntactic categories, which allows us to focus on modifying the lexicon, while leaving the structure of the parser itself intact. We demonstrate uplifts for dependency recovery of 2%-6% on novel vocabulary in biomedical text. 
Approaches to determining the factuality of diagnoses and ﬁndings in clinical text tend to rely on dictionaries of marker words for uncertainty and negation. Here, a method for semi-automatically expanding a dictionary of marker words using distributional semantics is presented and evaluated. It is shown that ranking candidates for inclusion according to their proximity to cluster centroids of semantically similar seed words is more successful than ranking them according to proximity to each individual seed word. 
Distant supervision is a useful technique for creating relation classiﬁers in the absence of labelled data. The approaches are often evaluated using a held-out portion of the distantly labelled data, thereby avoiding the need for lablelled data entirely. However, held-out evaluation means that systems are tested against noisy data, making it difﬁcult to determine their true accuracy. This paper examines the effectiveness of using held-out data to evaluate relation extraction systems by comparing the results that are produced with those generated using manually labelled versions of the same data. We train classiﬁers to detect two UMLS Metathesaurus relations (may-treat and may-prevent) in Medline abstracts. A new evaluation data set for these relations is made available. We show that evaluation against a distantly labelled gold standard tends to overestimate performance and that no direct connection can be found between improved performance against distantly and manually labelled gold standards. 
Structuring of information helps people to gain a quick overview of complex issues and facilitates the transfer of large amounts of data. In the medical ﬁeld, such data are transferred using deﬁned standards (HL71, DICOM2) or in conjunction with terminology systems (ICD-103, LOINC4, SNOMED CT5). This paper focuses on the structuring of diagnostic reports in the ﬁeld of anatomic pathology. It describes how to make the content of these reports semantically understandable for machines. Finally, it will be shown that structured pathology reports can be checked for completeness of content in a computerized way by using terminological knowledge. For this purpose, an ontology has been designed that describes the subdomain of reporting a radical prostatectomy specimen. 
Crowdsourcing platforms are a popular choice for researchers to gather text annotations quickly at scale. We investigate whether crowdsourced annotations are useful when the labeling task requires medical domain knowledge. Comparing a sentence classiﬁcation model trained with expert-annotated sentences to the same model trained on crowd-labeled sentences, we ﬁnd the crowdsourced training data to be just as effective as the manually produced dataset. We can improve the accuracy of the crowd-fueled model without collecting further labels by ﬁltering out worker labels applied with low conﬁdence. 
Linking electronic health records (EHRs) to relevant education materials can provide patient-centered tailored education which can potentially improve patients’ medical knowledge, self-management and clinical outcome. It is shown that EHR query generation using key concept identiﬁcation improves retrieval of education materials. In this study, we explored domain adaptation approaches to improve key concept identiﬁcation. Our experiments show that a 20.7% improvement in the F1 measure can be achieved by leveraging data from Wikipedia. Queries generated from the best performing approach achieved a 20.6% and 27.8% improvement over the queries generated from the baseline approach. 
A method to ﬁnd adverse drug reactions in electronic health records written in Swedish is presented. A total of 14,751 health records were manually classiﬁed into four groups. The records are normalised by pre-processing using both dictionaries and manually created word lists. Three different supervised machine learning algorithm were used to ﬁnd the best results; decision tree, random forest and LibSVM. The best performance on a test dataset was with LibSVM obtaining a precision of 0.69 and a recall of 0.66, and a F-score of 0.67. Our method found 865 of 981 true positives (88.2%) in a 3-class dataset which is an improvement of 49.5% over previous approaches.  free text. Health records are usually long and written by different authors with different writing styles (Allvin et al., 2011; Wijesekera, 2013). To identify entities in a text, to extract meaning and terms, and to consider their context, advanced methods must carried out. Several strategies and methods have been developed, and some approaches are described in the next section. 2 Related research There are several studies on automatically identifying ADEs from the text of electronic health records using either rule-based or machine learning-based methods. In this section different approaches and their results are summarised. 2.1 Rule based methods  
The paper illustrates the results of a case study aimed at investigating and enhancing the accessibility of Italian health– related documents by relying on advanced NLP techniques, with particular attention to informed consent forms. Results achieved show that the features automatically extracted from the linguistically annotated text and ranging across different levels of linguistic description have a high discriminative power in order to guarantee a reliable readability assessment. 
Biomedical synonyms are important resources for Natural Language Processing in Biomedical domain. Existing synonym resources (e.g., the UMLS) are not complete. Manual efforts for expanding and enriching these resources are prohibitively expensive. We therefore develop and evaluate approaches for automated synonym extraction from Wikipedia. Using the inter-wiki links, we extracted the candidate synonyms (anchor-text e.g., “increased thirst”) in a Wikipedia page and the title (e.g., “polyuria”) of its corresponding linked page. We rank synonym candidates with word embedding and pseudo-relevance feedback (PRF). Our results show that PRF-based reranking outperformed word embedding based approach and a strong baseline using interwiki link frequency. A hybrid method, Rank Score Combination, achieved the best results. Our analysis also suggests that medical synonyms mined from Wikipedia can increase the coverage of existing synonym resources such as UMLS. 
Electronic health records have emerged as a promising source of information for pharmacovigilance. Adverse drug events are, however, known to be heavily underreported, which makes it important to develop capabilities to detect such information automatically in clinical text. While machine learning offers possible solutions, it remains unclear how best to represent clinical notes in a manner conducive to learning high-performing predictive models. Here, 42 representations are explored in an empirical investigation using 27 real, clinical datasets, indicating that combining local and global (distributed) representations of words and named entities yields higher accuracy than using either in isolation. Subsequent analyses highlight the relative importance of various named entity classes for predicting adverse drug events. 
This work focuses on using anaphora for machine translation with deep-syntactic transfer. We compare multiple coreference resolvers for English in terms of how they affect the quality of pronoun translation in English-Czech and EnglishDutch machine translation systems with deep transfer. We examine which pronouns in the target language depend on anaphoric information, and design rules that take advantage of this information. The resolvers’ performance measured by translation quality is contrasted with their intrinsic evaluation results. In addition, a more detailed manual analysis of Englishto-Czech translation was carried out. 
Previous work on pronouns in SMT has focussed on third-person pronouns, treating them all as anaphoric. Little attention has been paid to other uses or other types of pronouns. Believing that further progress requires careful analysis of pronouns as a whole, we have analysed a parallel corpus of annotated English-German texts to highlight some of the problems that hinder progress. We combine this with an assessment of the ability of two state-of-the-art systems to translate different pronoun types. 
Current Statistical Machine Translation (SMT) is signiﬁcantly affected by Machine Translation (MT) evaluation metric. Nowadays the emergence of document-level MT research increases the demand for corresponding evaluation metric. This paper proposes two superior yet low-cost quantitative objective methods to enhance traditional MT metric by modeling document-level phenomena from the perspectives of gist consistency and text cohesion. The experimental results show the proposed metrics can obtain better correlation with human judgments than traditional metrics on evaluating document-level translation quality. 
Translation of discourse connectives varies more in human translations than in machine translations. Building on Murray’s (1997) continuity hypothesis and Sanders’ (2005) causality-by-default hypothesis we investigate whether expectedness influences the degree of implicitation and explicitation of discourse relations. We manually analyze how source text connectives are translated, and where connectives in target texts come from. We establish whether relations are explicitly signaled in the other language as well, or whether they have to be reconstructed by inference. We demonstrate that the amount of implicitation and explicitation of connectives in translation is influenced by the expectedness of the relation a connective signals. In addition, we show that the types of connectives most often added in translation are also the ones most often deleted. 
Most current machine translation systems translate each sentence independently, ignoring the context from previous sentences. This discourse unawareness can lead to incorrect translation of words or phrases that are ambiguous in the sentence. For example, the German term Typen in the phrase diese Typen can be translated either into English types or guys. However, knowing that it co-refers to the compound Ko¨rpertypen (“body types”) in the previous sentence helps to disambiguate the term and translate it into types. We propose a method of automatically detecting document-level trigger words (like Ko¨rpertypen), whose presence helps to disambiguate translations of ambiguous terms. In this preliminary study we analyze the method and its limitations, and outline future work directions. 
Coherence in Machine Translation (MT) has received little attention to date. One of the main issues we face in work in this area is the lack of labelled data. While coherent (human authored) texts are abundant and incoherent texts could be taken from MT output, the latter also contains other errors which are not speciﬁcally related to coherence. This makes it difﬁcult to identify and quantify issues of coherence in those texts. We introduce an initiative to create a corpus consisting of data artiﬁcially manipulated to contain errors of coherence common in MT output. Such a corpus could then be used as a benchmark for coherence models in MT, and potentially as training data for coherence models in supervised settings. 
For some language pairs, pronoun translation is a discourse-driven task which requires information that lies beyond its local context. This motivates the task of predicting the correct pronoun given a source sentence and a target translation, where the translated pronouns have been replaced with placeholders. For crosslingual pronoun prediction, we suggest a neural network-based model using preceding nouns and determiners as features for suggesting antecedent candidates. Our model scores on par with similar models while having a simpler architecture. 
The Idiap NLP Group has participated in both DiscoMT 2015 sub-tasks: pronounfocused translation and pronoun prediction. The system for the ﬁrst sub-task combines two knowledge sources: grammatical constraints from the hypothesized coreference links, and candidate translations from an SMT decoder. The system for the second sub-task avoids hypothesizing a coreference link, and uses instead a large set of source-side and target-side features from the noun phrases surrounding the pronoun to train a pronoun predictor. 
This paper presents baseline models for the cross-lingual pronoun prediction task and the pronoun-focused translation task at DiscoMT 2015. We present simple yet effective classiﬁers for the former and discuss the impact of various contextual features on the prediction performance. In the translation task we rely on the document-level decoder Docent and a cross-sentence target language-model over selected words based on the parts-of-speech of the aligned source language words. 
In this paper, we apply text classiﬁcation techniques to prove how well translated texts obey linguistic conventions of the target language measured in terms of registers, which are characterised by particular distributions of lexico-grammatical features according to a given contextual conﬁguration. The classiﬁers are trained on German original data and tested on comparable English-to-German translations. Our main goal is to see if both human and machine translations comply with the nontranslated target originals. The results of the present analysis provide evidence for our assumption that the usage of parallel corpora in machine translation should be treated with caution, as human translations might be prone to errors. 
Research in domain adaptation for statistical machine translation (SMT) has resulted in various approaches that adapt system components to speciﬁc translation tasks. The concept of a domain, however, is not precisely deﬁned, and most approaches rely on provenance information or manual subcorpus labels, while genre differences have not been addressed explicitly. Motivated by the large translation quality gap that is commonly observed between different genres in a test corpus, we explore the use of document-level genrerevealing text features for the task of translation model adaptation. Results show that automatic indicators of genre can replace manual subcorpus labels, yielding significant improvements across two test sets of up to 0.9 BLEU. In addition, we ﬁnd that our genre-adapted translation models encourage document-level translation consistency. 
Usage of discourse connectives (DCs) differs across languages, thus addition and omission of connectives are common in translation. We investigate how implicit (omitted) DCs in the source text impacts various machine translation (MT) systems, and whether a discourse parser is needed as a preprocessor to explicitate implicit DCs. Based on the manual annotation and alignment of 7266 pairs of discourse relations in a Chinese-English translation corpus, we evaluate whether a preprocessing step that inserts explicit DCs at positions of implicit relations can improve MT. Results show that, without modifying the translation model, explicitating implicit relations in the input source text has limited effect on MT evaluation scores. In addition, translation spotting analysis shows that it is crucial to identify DCs that should be explicitly translated in order to improve implicit-to-explicit DC translation. On the other hand, further analysis reveals that the disambiguation as well as explicitation of implicit relations are subject to a certain level of optionality, suggesting the limitation to learn and evaluate this linguistic phenomenon using standard parallel corpora. 
In this paper, we introduce document level features that capture necessary information to help MT system perform better word sense disambiguation in the translation process. We describe enhancements to a Maximum Entropy based translation model, utilizing long distance contextual features identiﬁed from the span of entire document and from both source and target sides, to improve the likelihood of the correct translation for words with multiple meanings, and to improve the consistency of the translation output in a document setting. The proposed features have been observed to achieve substantial improvement of MT performance on a variety of standard test sets in terms of TER/BLEU score. 
In this paper, we analyse cross-linguistic variation of discourse phenomena, i.e. coreference, discourse relations and modality. We will show that contrasts in the distribution of these phenomena can be observed across languages, genres, and text production types, i.e. translated and non-translated ones. Translations, regardless of the method they were produced with, are different from their source texts and from the comparable originals in the target language, as it was stated in studies on translationese. These differences can be automatically detected and analysed with exploratory and automatic clustering techniques. The extracted frequencybased proﬁles of variables under analysis (languages, genres, text production types) can be used in further studies, e.g. in the development and enhancement of MT systems, or in further NLP applications. 
The translation process in statistical machine translation (SMT) is shaped by technical constraints and engineering considerations. SMT explicitly models translation as search for a target-language equivalent of the input text. This perspective on translation had wide currency in mid-20th century translation studies, but has since been superseded by approaches arguing for a more complex relation between source and target text. In this paper, we show how traditional assumptions of translational equivalence are embodied in SMT through the concepts of word alignment and domain and discuss some limitations arising from the word-level/corpus-level dichotomy inherent in these concepts. 
This paper investigates to what extent grammatical functions of a word can be predicted from gaze features obtained using eye-tracking. A recent study showed that reading behavior can be used to predict coarse-grained part of speech, but we go beyond this, and show that gaze features can also be used to make more ﬁnegrained distinctions between grammatical functions, e.g., subjects and objects. In addition, we show that gaze features can be used to improve a discriminative transition-based dependency parser. 
We show that metrics derived from recording gaze while reading, are better proxies for machine translation quality than automated metrics. With reliable eyetracking technologies becoming available for home computers and mobile devices, such metrics are readily available even in the absence of representative held-out human translations. In other words, readingderived MT metrics offer a way of getting cheap, online feedback for MT system adaptation. 
We examine the ability of several models of computation and storage to explain reading time data. Speciﬁcally, we demonstrate on both the Dundee and the MIT reading time corpora, that fragment grammars, a model that optimizes the tradeoff between computation and storage, is able to better explain people’s reaction times than two baseline models which exclusively favor either storage or computation. Additionally, we make a contribution by extending an existing incremental parser to handle more general grammars and scale well to larger rule and data sets.1 
We aim to demonstrate that agent-based models can be a useful tool for historical linguists, by modeling the historical development of verbal cluster word order in Germanic languages. Our results show that the current order in German may have developed due to increased use of subordinate clauses, while the English order is predicted to be inﬂuenced by the grammaticalization of the verb to have. The methodology we use makes few assumptions, making it broadly applicable to other phenomena of language change. 
We present a prototype model, based on a combination of count-based distributional semantics and prediction-based neural word embeddings, which learns about syntactic categories as a function of (1) writing contextual, phonological, and lexical-stress-related information to memory and (2) predicting upcoming context words based on memorized information. The system is a ﬁrst step towards utilizing recently popular methods from Natural Language Processing for exploring the role of prediction in childrens’ acquisition of syntactic categories.1 
Starting from the distributional bootstrapping hypothesis, we propose an unsupervised model that selects the most useful distributional information according to its salience in the input, incorporating psycholinguistic evidence. With a supervised Parts-of-Speech tagging experiment, we provide preliminary results suggesting that the distributional contexts extracted by our model yield similar performances as compared to current approaches from the literature, with a gain in psychological plausibility. We also introduce a more principled way to evaluate the effectiveness of distributional contexts in helping learners to group words in syntactic categories. 
Experiments on the emergence of a shared language in a population of agents usually rely on the control of the complexity by the experimenter. In this article we show how agents provided with the autotelic principle, a system by which agents can regulate their own development, progressively develop an emerging language evolving from one word to multi-word utterances, increasing its discriminative power. 
In this paper, a particular algorithm for lexical acquisition – taken as a problem of learning the mapping from words to meanings – is evaluated. The algorithm in Siskind (1996) is adapted to handle more complex input data, including data of Brazilian Portuguese. In particular, the input data in the present study covers a broader grammatical knowledge, showing both polysemy and higher inﬂectional and agreement morphology. Results indicate that these properties create difﬁculties to the learner and that more substantial developments to the algorithm are needed in order to increase its cross-linguistic capabilities. 
This study investigates the use of syllables and phone(me)s in computational models of segmentation in early language acquisition. We results of experiments with both syllables and phonemes as the basic unit using a standard state-of-the-art segmentation model. We evaluate the model output based on both word- and morpheme-segmented gold standards on child-directed speech corpora from two typologically different languages. Our results do not indicate a clear advantage for one unit or the other. We argue that the computational advantage for the syllable suggested in earlier research may be an artifact of the particular language and/or segmentation strategy used in these studies. 
Redundancy is an important psycholinguistic concept which is often used for explanations of language change, but is notoriously difficult to operationalize and measure. Assuming that the reconstruction of a syntactic structure by a parser can be used as a rough model of the understanding of a sentence by a human hearer, I propose a method for estimating redundancy. The key idea is to compare performances of a parser on a given treebank before and after artificially removing all information about a certain grammeme from the morphological annotation. The change in performance can be used as an estimate for the redundancy of the grammeme. I perform an experiment, applying MaltParser to an Old Church Slavonic treebank to estimate grammeme redundancy in Proto-Slavic. The results show that those Old Church Slavonic grammemes within the case, number and tense categories that were estimated as most redundant are those that disappeared in modern Russian. Moreover, redundancy estimates serve as a good predictor of case grammeme frequencies in modern Russian. The small sizes of the samples do not allow to make definitive conclusions for number and tense. 
Meaning conveyance is bottlenecked by the linguistic conventions shared among interlocutors. One possibility to convey non-conventionalized meaning is to employ known expressions in such a way that the intended meaning can be abduced from them. This, in turn, can give rise to ambiguity. We investigate this process with a focus on its use for semantic coordination and show it to be conducive to fast agreement on novel meaning under a mutual expectation to exploit semantic structure. We argue this to be a motivation for the crosslinguistic pervasiveness of systematic ambiguity. 
Children’s overextension errors in word usage can yield insights into the underlying representation of meaning. We simulate overextension patterns in the domain of color with two word-learning models, and look at the contribution of three possible factors: perceptual properties of the colors, typological prevalence of certain color groupings into categories (as a proxy for cognitive naturalness), and color term frequency. We ﬁnd that the perceptual features provide the strongest predictors of the error pattern observed during development, and can effectively rule out color term frequency as an explanation. Typological prevalence is shown to correlate strongly with the perceptual dimensions of color, and hence provides no effect over and above the perceptual dimensions. 
Infant-directed speech (IDS) is thought to play a key role in determining infant language acquisition. It is thus important to describe how computational models of infant language acquisition behave when given an input of IDS, as compared to adult-directed speech (ADS). In this paper, we explore how an acoustic motif discovery algorithm fares when presented with speech from both registers. Results show small but signiﬁcant differences in performance, with lower recall and lower cluster collocation in IDS than ADS, but a higher cluster purity in IDS. Overall, these results are inconsistent with a view suggesting that IDS is acoustically clearer than ADS in a way that systematically facilitates lexical recognition. Similarities and differences with human infants’ word segmentation are discussed. 
We address the question whether children can acquire mature use of higher-level grammatical choices from the linguistic input, given only general prior knowledge and learning biases. We do so on the basis of a case study with the dative alternation in English, building on a study by de Marneffe et al. (2012) who model the production of the dative alternation by seven young children, using data from the Child Language Data Exchange System corpus. Using mixed-effects logistic modelling on the aggregated data of these children, De Marneffe et al. report that the children’s choices can be predicted both by their own utterances and by child-directed speech. Here we bring the computational modeling down to the individual child, using memory-based learning and incremental learning curve studies. We observe that for all children, their dative choices are best predicted by a model trained on childdirected speech. Yet, models trained on two individual children for which sufﬁcient data is available are about as accurate. Furthermore, models trained on the dative alternations of these children provide approximations of dative alternations in caregiver speech that are about as accurate as training and testing on caregiver data only. 
Recently there has been a lot of interest in testing the processing predictions of a speciﬁc top-down parser for Minimalist grammars (Stabler, 2013). Most of this work relies on memory-based difﬁculty metrics that relate the shape of the parse tree to processing behavior. We show that none of the difﬁculty metrics proposed so far can explain why subject relative clauses are more easily processed than object relative clauses in Chinese, Korean, and Japanese. However, a minor tweak to how memory load is determined is sufﬁcient to fully capture the data. This result thus lends further support to the hypothesis that very simple notions of resource usage are powerful enough to explain a variety of processing phenomena. 
This paper shows how the parsing problem for general Abstract Categorial Grammars can be reduced to the provability problem for Multiplicative Exponential Linear Logic. It follows essentially a similar reduction by Kanazawa, who has shown how the parsing problem for second-order Abstract Categorial Grammars reduces to datalog queries. 
The implications of a speciﬁc pseudometric on the collection of languages over a ﬁnite alphabet are explored. In distinction from an approach in (Calude et al., 2009) that relates to collections of inﬁnite or bi-inﬁnite sequences, the present work is based on an adaptation of the “Besicovitch” pseudometric introduced by Besicovitch (1932) and elaborated in (Cattaneo et al., 1997) in the context of cellular automata. Using this pseudometric to form a metric quotient space, we study its properties and draw conclusions about the location of certain well-understood families of languages in the language space. We ﬁnd that topologies, both on the space of formal languages itself and upon quotient spaces derived from pseudometrics on the language space, may offer insights into the relationships, and in particular the distance, between languages over a common alphabet. 
In this paper we revisit the issue of copredication from the perspective of modern type theories. Speciﬁcally, we look at: a) the counting properties of dot-types, and b) the case of a complex dot-type that has remained unsolved in the literature, i.e. that of newspaper. As regards a), we show that the account proposed in (Luo, 2010) for dot-types makes the correct predictions as regards counting. In order to verify this, we implement the account in the Coq proof-assistant and check that the desired inferences follow. Then, we look at the case of b), the case of a dot-type which is both resource and context sensitive. We propose a further resource sensitive version of the dottype, in effect a linear dot-type. This along with local coercions can account for the behaviour attested. 
We discuss the model theory of two popular approaches to lexical semantics and their relation to transcendental logic. 
The categorical compositional distributional model of Coecke et al. (2010) provides a linguistically motivated procedure for computing the meaning of a sentence as a function of the distributional meaning of the words therein. The theoretical framework allows for reasoning about compositional aspects of language and offers structural ways of studying the underlying relationships. While the model so far has been applied on the level of syntactic structures, a sentence can bring extra information conveyed in utterances via intonational means. In the current paper we extend the framework in order to accommodate this additional information, using Frobenius algebraic structures canonically induced over the basis of ﬁnite-dimensional vector spaces. We detail the theory, provide truth-theoretic and distributional semantics for meanings of intonationally-marked utterances, and present justiﬁcations and extensive examples. 
Morphoid type theory (MTT) is a typetheoretic foundation for mathematics supporting the concept of isomorphism and the substitution of isomorphics. Unlike homotopy type theory (HoTT), which also supports isomorphism, morphoid type theory is a direct extension of classical predicate calculus and avoids the intuitionistic constructs of propositionsas-types, path induction and squashing. Although HoTT is capable of supporting classical inference, MTT’s thoroughly classical treatment is expected to be more comfortable for those who take a Platonic or realist approach to the practice of mathematics. 
Several algorithms have been proposed to learn different subclasses of context-free grammars based on the idea generically called distributional learning. Those techniques have been applied to many formalisms richer than context-free grammars like multiple context-free grammars, simple contextfree tree grammars and others. The learning algorithms for those different formalisms are actually quite similar to each other. We in this paper give a uniform view on those algorithms. 
Strong learning of context-free grammars is the problem of learning a grammar which is not just weakly equivalent to a target grammar but isomorphic or structurally equivalent to it. This is closely related to the problem of deﬁning a canonical grammar for the language. The current proposal for strong learning of a small class of CFGs uses grammars whose nonterminals correspond to congruence classes of the language, in particular to a subset of those that satisfy a primality condition. Here we extend this approach to larger classes of CFGs where the nonterminals correspond instead to closed sets of strings; to elements of the syntactic concept lattice. We present two different classes of canonical context-free grammars. One is based on all of the primes in the lattice: the other, more suitable for strong learning algorithms is based on a subset of primes that are irreducible in a certain sense. 
This paper characterizes a subclass of subsequential string-to-string functions called Output Strictly Local (OSL) and presents a learning algorithm which provably learns any OSL function in polynomial time and data. This algorithm is more efﬁcient than other existing ones capable of learning this class. The OSL class is motivated by the study of the nature of string-to-string transformations, a cornerstone of modern phonological grammars. 
An error-driven phonotactic learner is trained on a stream of licit phonological forms. Each piece of training data counts as a winner in terms of Optimality Theory. In order to test its current grammar, the learner needs to compare the current winner with a properly chosen loser. This paper advocates a new subroutine for the choice of the loser, based on the idea of minimizing the “distance” from the given winner. 
Autosegmental phonology represents words with graph structures. This paper introduces a way of reasoning about autosegmental graphs as strings of concatenated graph primitives. The main result shows that the sets of autosegmental graphs so generated obey two important, putatively universal, constraints in phonological theory provided that the graph primitives also obey these constraints. These constraints are the Obligatory Contour Principle and the No Crossing Constraint. Thus, these constraints can be understood as being derived from a ﬁnite basis under concatenation. This contrasts with (and complements) earlier analyses of autosegmental representations, where these constraints were presented as axioms of the grammatical system. Empirically motivated examples are provided. 
Syntactic analyses describe grouping operations that explain how words are combined to form utterances. The nature of these operations depends on the approach. In a constituency-based approach, grouping operations are ordered, or stratiﬁed, part-whole relations. In a dependency-based approach, grouping operations identify a governor (or head), i.e. they are directed hierarchical relations between words. It is possible to convert a constituency tree into a dependency tree by dereifying the nodes, by identifying the governor and by removing the stratiﬁcation of the part-whole relations. Polygraphs combine the two types of information into a single structure and are therefore a more powerful formalism. By relaxing constraints, polygraphs also allow to underspecify both kinds of information. Keywords: syntactic structure, immediate constituents analysis, dependency tree, headedness, phrase structure tree, polygraph, reiﬁcation, stratiﬁcation, underspeciﬁcation. 
This paper presents a successful approach for domain adaptation of a dependency parser via self-training. We improve parsing accuracy for out-of-domain texts with a self-training approach that uses conﬁdence-based methods to select additional training samples. We compare two conﬁdence-based methods: The ﬁrst method uses the parse score of the employed parser to measure the conﬁdence into a parse tree. The second method calculates the score differences between the best tree and alternative trees. With these methods, we were able to improve the labeled accuracy score by 1.6 percentage points on texts from a chemical domain and by 0.6 on average on texts of three web domains. Our improvements on the chemical texts of 1.5% UAS is substantially higher than improvements reported in previous work of 0.5% UAS. For the three web domains, no positive results for self-training have been reported before. 
The machine learning-based approaches that dominate natural language processing research require massive amounts of labeled training data. Active learning has the potential to substantially reduce the human effort needed to prepare this data by allowing annotators to focus on only the most informative training examples. This paper shows that active learning can be used for domain adaptation of dependency parsers, not just in single-domain settings. We also show that entropy-based query selection strategies can be combined with partial annotation to annotate informative examples in the new domain without annotating full sentences. Simulations are common in work on active learning, but we measured the actual time needed for manual annotation of data to better frame the results obtained in our simulations. We evaluate query strategies based on both full and partial annotation in several domains, and ﬁnd that they reduce the amount of in-domain training data needed for domain adaptation by up to 75% compared to random selection. We found that partial annotation delivers better indomain performance for the same amount of human effort than full annotation. 
Wide-coverage resources for lexicalized grammars have been obtained by converting the existing treebanks into collections of derivations. Additional annotations to the source treebank can be used to improve these derivations. A treebank annotation called the NTT treebank was used for this paper to improve a CCGbank for Japanese. The source treebank of the CCGbank itself is created by automatically converting chunk-dependencies, but the CCGbank contains errors caused by noisier phrase structures and a lack of linguistic information, which is difﬁcult to represent in chunk-dependency. The NTT treebank provides cleaner trees and functional and semantic information, e.g., coordinations and predicate-argument structures. The effect of the improvement process is empirically evaluated in terms of the changes in the dependency relations extracted from the resulting derivations. 
We propose to use Graph Rewriting for parsing syntactic dependencies. We present a system of rewriting rules dedicated to French and we evaluate it by parsing the SEQUOIA corpus. 
In this paper we gauge the utility of general-purpose, open-domain semantic parsing for textual entailment recognition by combining graph-structured meaning representations with semantic technologies and formal reasoning tools. Our approach achieves high precision, and in two case studies we show that when reasoning over n-best analyses from the parser the performance of our system reaches stateof-the-art for rule-based textual entailment systems. 
In this paper we propose a framework for procedural text understanding. Procedural texts are relatively clear without modality nor dependence on viewpoints, etc. and have many potential applications in artiﬁcial intelligence. Thus they are suitable as the ﬁrst target of natural language understanding. As our framework we extend parsing technologies to connect important concepts in a text. Our framework ﬁrst tokenizes the input text, a sequence of sentences, then recognizes important concepts like named entity recognition, and ﬁnally connect them like a sentence parser but dealing all the concepts in the text at once. We tested our framework on cooking recipe texts annotated with a directed acyclic graph as their meaning. We present experimental results and evaluate our framework. 
Parsers have evolved signiﬁcantly in the last decades, but currently big and accurate improvements are needed to enhance their performance. ParTes, a test suite in Spanish and Catalan for parsing evaluation, aims to contribute to this situation by pointing to the main factors that can decisively improve the parser performance.  Features Domain  HP  EAGLES TSNLP  general speciﬁc general  Goal Languages  parsing English  grammar checkers English  Annotation minimal minimal  NLP software English, German, French robust  Content  syntax  taxonomy (extra-)linguistic of errors  Table 1: HP, EAGLES & TSNLP features  
Coordinate structures pose difﬁculties in dependency parsers. In this paper, we propose a set of parsing rules speciﬁcally designed to handle coordination, which are intended to be used in combination with Eisner and Satta’s dependency rules. The new rules are compatible with existing similarity-based approaches to coordination structure analysis, and thus the syntactic and semantic similarity of conjuncts can be incorporated to the parse scoring function. Although we are yet to implement such a scoring function, we analyzed the time complexity of the proposed rules as well as their coverage of the Penn Treebank converted to the Stanford basic dependencies. 
We introduce interpolation of trained MSTParser models as a resource combination method for multi-source delexicalized parser transfer. We present both an unweighted method, as well as a variant in which each source model is weighted by the similarity of the source language to the target language. Evaluation on the HamleDT treebank collection shows that the weighted model interpolation performs comparably to weighted parse tree combination method, while being computationally much less demanding. 
We study non-deterministic oracles for training non-projective beam search parsers with swap transitions. We map out the spurious ambiguities of the transition system and present two non-deterministic oracles as well as a static oracle that minimizes the number of swaps. An evaluation on 10 treebanks reveals that the difference between static and non-deterministic oracles is generally insigniﬁcant for beam search parsers but that non-deterministic oracles can improve the accuracy of greedy parsers that use swap transitions. 
We propose solutions to enhance the Inside-Outside Recursive Neural Network (IORNN) reranker of Le and Zuidema (2014). Replacing the original softmax function with a hierarchical softmax using a binary tree constructed by combining output of the Brown clustering algorithm and frequency-based Huffman codes, we signiﬁcantly reduce the reranker’s computational complexity. In addition, enriching contexts used in the reranker by adding subtrees rooted at (ancestors’) cousin nodes, the accuracy is increased. 
We present a method for ﬁnding the best tree approximation parse of a dependency digraph for a given sentence, with respect to a dataset of semantic digraphs as a computationally efﬁcient and accurate alternative to DAG parsing. We present a training algorithm that learns the spanning subtree parses with the highest scores with respect to the data, and consider the output of this algorithm a description of the best tree approximations for digraphs of sentences from similar data. With the results from this approach, we acquire some important insights on the limits of solely data-driven tree approximation approaches to semantic dependency DAG parsing, and their rule-based, pre-processed tree approximation counterparts. 
The CKY algorithm is an important component in many natural language parsers. We propose a novel type of constraint for context-free parsing called independence constraints. Based on the concept of independence between words, we show how these constraints can be used to reduce the work done in the CKY algorithm. We demonstrate a classiﬁer which can be used to identify boundaries between independent words in a sentence using only surface features, and show that it can be used to speed up a CKY parser. We investigate the trade-off between speed and accuracy, and indicate directions for improvement. 
This article proposes a syntactic parsing strategy based on a dependency grammar containing both formal rules and a compression technique that reduces the complexity of those rules. Compression parsing is mainly driven by the single-head constraint of Dependency Grammar, and can be seen as an alternative method to the well-known constructive strategy. The compression algorithm simpliﬁes the input sentence by progressively removing from it the dependent tokens as soon as binary syntactic dependencies are recognized. The performance of our system was compared to a deterministic parser based on supervised learning: MaltParser. Both systems were applied on several test sets of sentences in Spanish and Portuguese, from a variety of different domains and genres. Results showed that our parsing method keeps a similar performance through related languages and different domains, while MaltParser, as most supervised methods, turns out to be very dependent on the text domain used to train the system. 
Supertagging was recently proposed to provide syntactic features for statistical dependency parsing, contrary to its traditional use as a disambiguation step. We conduct a broad range of controlled experiments to compare this speciﬁc application of supertagging with another method for providing syntactic features, namely stacking. We ﬁnd that in this context supertagging is a form of stacking. We furthermore show that (i) a fast parser and a sequence labeler are equally beneﬁcial in supertagging, (ii) supertagging/stacking improve parsing also in a cross-domain setting, and (iii) there are small gains when combining supertagging and stacking, but only if both methods use different tools. The important consideration is therefore not the method but rather the diversity of the tools involved. 
This paper explores the notion of lexicon embedded syntax: syntactic structures that are preassembled in natural language lexicons. Section 1 proposes a lexicological perspective on (dependency) syntax: ﬁrst, it deals with the well-known problem of lexicon-grammar dichotomy, then introduces the notion of lexicon embedded syntax and, ﬁnally, presents the lexical models this discussion is based on: lexical systems, as implemented in the English and French Lexical Networks. Two cases of lexicon embedded syntax are then treated: the syntax of idioms, section 2, and the syntax of collocations, section 3. Section 4 concludes on the possible exploitation of syntactic structures that can be extracted from lexical systems. 
The paper reports experiences of automatically converting the dependency analysis of the LinES English-Swedish parallel treebank to universal dependencies (UD). The most tangible result is a version of the treebank that actually employs the relations and parts-of-speech categories required by UD, and no other. It is also more complete in that punctuation marks have received dependencies, which is not the case in the original version. We discuss our method in the light of problems that arise from the desire to keep the syntactic analyses of a parallel treebank internally consistent, while available monolingual UD treebanks for English and Swedish diverge somewhat in their use of UD annotations. Finally, we compare the output from the conversion program with the existing UD treebanks. 
In this paper, we present a method of improving quality of machine translation (MT) evaluation of Czech sentences via targeted paraphrasing of reference sentences on a deep syntactic layer. For this purpose, we employ NLP framework Treex and extend it with modules for targeted paraphrasing and word order changes. Automatic scores computed using these paraphrased reference sentences show higher correlation with human judgment than scores computed on the original reference sentences. 
This paper is meant as a brief description of the Romanian syntax within the dependency framework, more specifically within the Universal Dependency (UD) framework, and is the result of a volunteer activity of mapping two independently created Romanian dependency treebanks to the UD specifications. This mapping process is not trivial, as concessions have to be made and solutions need to be found for various language specific phenomena. We highlight the specific characteristics of the UD relations in Romanian and argument the need for other relations. If they have already been defined for (an)other language(s) in the UD project, we adopt them. 
We study a group of adverbials that are composed of a preposition and a noun denoting an emotion or an inner state, such as v jarosti ‘in a rage’, s udovol’stviem ‘with pleasure’, ot radosti ‘out of joy’, s gorja ‘out of grief’, na udivlenie ‘to the surprise of’, k dosade ‘to one’s disappointment’ etc. Being collocations, they occupy an intermediate position between free phrases and idioms. On the one hand, some of them are simple adverbial derivatives of nouns and therefore inherit some of their properties. On the other hand, they may have specific properties of their own. Two types of properties of the adverbials are studied: the actantial properties in their correlation with the properties of the source nouns, and the semantics proper. At the end a case study of the adverbials of the gratitude field is given. We show that adverbial derivatives can be shifted in the dependency structure from the subordinate clause to the main one. 1. Introduction We proceed from the obvious assumption that adverbial derivatives refer to the same situation as the source lexical unit (LU). This implies that, given the semantic structure with predicate P, our linguistic description should be able to produce a syntactic structure in which P is realized by means of an adverbial derivative of P and determine possible syntactic positions for LUs that correspond to semantic actants of P. And, the other way round, given sentences such as John replied by a nod and John nodded in reply, we should be able to discover that in both cases the semantic actants of ‘reply’ are ‘John’ and ‘his nod’. Thus, our aim consists in describing semantic and syntactic properties of adverbial derivatives in their correlation with the source LU. For each predicate, we need to know its possible syntactic realizations (e.g. ‘reply’ --> to reply – in  reply) along with semantic modifications associated with them. For each syntactic realization, we should specify possible ways of valency filling of the LU. The main difference between this approach and traditional valency dictionaries is that we concentrate on adverbial derivatives of predicates in their correlation with the source LU unit and take into consideration a much larger range of possible realizations of their semantic actants. We study a group of nouns that denote emotions and inner states (EIS nouns). They are often used in specific adverbial prepositional phrases – v jarosti ‘in a rage’, s udovol’stviem ‘with pleasure’, ot radosti ‘out of joy’, s gorja ‘out of grief’, na udivlenie ‘to the surprise of’, k dosade ‘to one’s disappointment’ etc. The phrases usually mean that a person is in this state or that this state is the cause or a consequence of some other state or event. For brevity, we will call such phrases EIS adverbials. Russian explanatory dictionaries usually treat EIS adverbials as free phrases and attribute all their peculiarities, if any, to specific properties of corresponding prepositions. For example, the recent Active dictionary of Russian (ADR 2014), which provides deeply elaborated semantic definitions, lists among the senses of preposition v 'in', sense v 4.1 which «is used to denote the state A2 of a person A1 or his relationship A2 with other people»: On byl v sil'nom razdraženii (v polnom izumlenii, v upoenii, v ekstaze). V jarosti pnul sobačonku. ‘He was in a temper (in utter surprise, in ecstasy). In a rage, he kicked the dog’. Other detailed descriptions of semantics of Russian prepositions used in EIS adverbials can be found in Iomdin 1990-91, Iordanskaja-Mel’čuk 1996, Levontina 2004. However, even the most precise and detailed description of prepositions does not fully account for all peculiarities of adverbials. We intend to show that EIS adverbials manifest a number of features that are not derivable from the properties of prepo-  38 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 38–47, Uppsala, Sweden, August 24–26 2015.  sitions or nouns alone but appear only in their combination. Special attention will be paid to semantic and syntactic properties of the adverbials. In section 2 we will explain what we basically mean by adverbial derivatives and describe their certain properties relevant for our study. Section 3 will characterize EIS adverbials of different types. In section 4 we demonstrate a case study related to adverbials of the field of gratitude. We will conclude in 5. 2. Adverbial derivatives. We consider EIS adverbials as adverbial derivatives of corresponding nouns. An adverbial derivative of lexical unit (LU) L is a LU or a phrase that has the same or a similar meaning to L and has an adverbial syntactic function, which means that it is primarily used as a verb modifier. For more details on syntactic derivatives in general and adverbial derivatives in particular we refer the reader to Boguslavsky 2014. In Russian, there are three major types of adverbial derivatives: a) grammatical derivatives that can be derived from virtually any verb (deverbal adverbs, deepričastija); cf. (1a), b) lexico-syntactic derivatives (prepositional phrases) derived from nouns; cf. (1b), and c) lexical derivatives (adverbs); cf. (1c). The last two cases can be described as values of the lexical function Advi. (1a) Oni razgljadyvali kartinki, radujas' kak deti. ‘they were examining the pictures rejoicing like children’. (1b) Ja s bolšoj radostju prinimaju vaše priglašenie. ‘I accept your invitation with great joy’. (1c) Deti radostno prinjalis' narjažat' jolku. ‘the kids merrily began to decorate the Christmas tree’. Deverbal adverbs retain the lexical meaning and syntactic properties of the source LU to a greater extent than other types of adverbial derivatives. They serve to express a secondary predication attached to the main one. Their most salient feature is that their subject is always coreferential with the subject of the main clause and is elided from the syntactic structure. As a rule, prepositional phrases and adverbs also retain the lexical meaning of the source word, but they can manifest noticeable semantic modifications.  As far as the actantial structure of adverbials is concerned, it is necessary to distinguish between three types of valency slots in the semantic definition of a LU depending on the syntactic position of the argument with respect to its predicate (Boguslavsky 2003)1. We call a valency slot of lexeme L ACTIVE if in the syntactic structure of the sentence it is filled by a word syntactically subordinated to L. Active valency slots are instantiated with syntactic actants. We call a valency slot PASSIVE if it is filled by a lexeme that syntactically subordinates L. Finally, we call it DISCONTINUOUS if there is no direct syntactic link between L and the word filling this slot. To give an example, the valency slots of the verb to precede are active because in the prototypical sentence (2a) The conference preceded the workshop its actants syntactically depend on the verb. However, if one compares (2a) with the sentence (2b) The conference was before the workshop we will see that, from the purely semantic point of view, the preposition before denotes the same situation as the verb to precede - the situation of the temporal precedence of one event with respect to the other. This situation has at least two participants: an event that takes place earlier and another one that takes place later. These participants can be systematically expressed in a sentence with the given word and therefore the preposition before has the same semantic rights to have valency slots as the verb to precede. The only difference between these slots concerns their syntactic realization. In case of the verb, both slots are filled with phrases which are syntactically subordinated to the verb in the dependency tree (i.e. with the subject and with the direct object) and therefore they are active. With the preposition it is different: one of the slots is also filled with a subordinated NP (before the workshop) whereas the other is filled with a phrase which syntactically subordinates the preposition (the conference was before), which makes this slot passive. Discontinous valency filling can be illustrated by quantifiers, cf. (3): (3) All the papers [Q] were revised [P]. 
We present a dependency annotation scheme for Finnish which aims at respecting the multilayered nature of language. We ﬁrst tackle the annotation of surfacesyntactic structures (SSyntS) as inspired by the Meaning-Text framework. Exclusively syntactic criteria are used when deﬁning the surface-syntactic relations tagset. Our annotation scheme allows for a direct mapping between surface-syntax and a more semantics-oriented representation, in particular predicate-argument structures. It has been applied to a corpus of Finnish, composed of 2,025 sentences related to weather conditions. 
We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy. The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference. We propose an efﬁcient decoding algorithm based on particle ﬁltering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 
This paper investigates the relation between the number of full valency frames (we do not distinguish between complements and optional adjuncts, both are taken into account) of a verb and the number of its synonyms. It is shown that for Czech verbs from the Prague Dependency Treebank it holds “the greater the full valency of a verb, the more synonyms the verb has”. 
This article presents a new approach of using dependency treebanks in theoretical syntactic research: The view of dependency treebanks as combined networks. This allows the usage of advanced tools for network analysis that quite easily provide novel insight into the syntactic structure of language. As an example of this approach, we will show how the network approach can provide an interesting angle to discuss the degree of connectivity of Chinese syntactic categories, which it is not so easy to detect from the original treebank. 
We present a system for verbal Word Sense Disambiguation (WSD) that is able to exploit additional information from parallel texts and lexicons. It is an extension of our previous WSD method (Dušek et al., 2014), which gave promising results but used only monolingual features. In the follow-up work described here, we have explored two additional ideas: using English-Czech bilingual resources (as features only – the task itself remains a monolingual WSD task), and using a “hybrid” approach, adding features extracted both from a parallel corpus and from manually aligned bilingual valency lexicon entries, which contain subcategorization information. Albeit not all types of features proved useful, both ideas and additions have led to signiﬁcant improvements for both languages explored. 
Using recently available dependency corpora, we present novel measures of a key quantitative property of language, word order freedom: the extent to which word order in a sentence is free to vary while conveying the same meaning. We discuss two topics. First, we discuss linguistic and statistical issues associated with our measures and with the annotation styles of available corpora. We ﬁnd that we can measure reliable upper bounds on word order freedom in head direction and the ordering of certain sisters, but that more general measures of word order freedom are not currently feasible. Second, we present results of our measures in 34 languages and demonstrate a correlation between quantitative word order freedom of subjects and objects and the presence of nominative-accusative case marking. To our knowledge this is the ﬁrst large-scale quantitative test of the hypothesis that languages with more word order freedom have more case marking (Sapir, 1921; Kiparsky, 1997). 
This paper proposes a new dependency-based analysis of coordination that generalizes over existing analyses by combining symmetrical and asymmetrical analyses of coordination into a DAG structure. The new joint structure is shown to be theoretically grounded in the notion of connections between words just as the formal definition of other types of dependencies. Beside formalizations of shared dependents (including right-node raising), paradigmatic adverbs, and embedded coordinations, a completely new formalization of non-constituent coordination is proposed. 
The Universal Stanford Dependencies (USD) subordinates function words to content words. Auxiliaries, adpositions and subordinators are positioned as dependents of full verbs and nouns, respectively. Such an approach to the syntax of natural languages is contrary to most work in theoretical syntax in the past 35 years, regardless of whether this work is constituency- or dependency-based. A substantial amount of evidence delivers a strong argument for the more conventional approach, which subordinates full verbs to auxiliaries and nouns to adpositions. This contribution demonstrates that the traditional approach to the dependency status of auxiliary verbs is motivated by many empirical considerations, and hence USD cannot be viewed as modeling the syntax of natural languages in a plausible way. 
One easily observable aspect of language variation is the order of words. In human and machine natural language processing, it is often claimed that parsing freeorder languages is more difﬁcult than parsing ﬁxed-order languages. In this study on Latin and Ancient Greek, two wellknown and well-documented free-order languages, we propose syntactic correlates of word order freedom. We apply our indicators to a collection of dependencyannotated texts of different time periods. On the one hand, we conﬁrm a trend towards more ﬁxed-order patterns in time. On the other hand, we show that a dependency-based measure of the ﬂexibility of word order is correlated with the parsing performance on these languages. 
The goal of the present contribution is to put under scrutiny the language phenomenon commonly called ellipsis or deletion, especially from the point of view of its representation in the underlying syntactic level of a dependency based syntactic description. We ﬁrst give a brief account of the treatment of ellipsis in some present day dependency-based accounts of this phenomenon (Sect. 1). The core of the paper is the treatment of ellipsis within the framework of the dependency-based formal multi-level description of language called Functional Generative Description: after an attempt at a typology of ellipsis (Sect. 2) we describe in detail some selected types of grammatical ellipsis in Czech (Sect. 3). In Sect. 4 we brieﬂy summarize the results of our analysis. 
Non-projectivity is an important theoretical and computational concept that has been investigated extensively in the dependency grammar/parsing paradigms. However, from a human sentence processing perspective, non-projectivity has received very little attention. In this paper, we look at existing work and propose new factors related to processing non-projective conﬁguration. We argue that (a) counter to the claims in the psycholinguistic literature (Levy et al, 2012), different aspects of prediction maintenance can lead to higher processing cost for a non-projective dependency, (b) parsing strategies can interact with the expectation for a nonprojective dependency, and (c) memory (re)activation can explain processing cost in certain non-projective conﬁgurations. 
This paper addresses the question if the Focus0 and Neg0 functional heads posited by phrase structural, generative accounts of Hungarian should also be recognized in a dependency-based description of the language. It is argued that the “identificational focus” of a Hungarian clause indeed behaves like a “derived main predicate” (cf. É. Kiss 2007), as suggested by two-clause paraphrases and the fact that its assertion can be independently negated. In DG, Hudson’s (2003) “mutual dependency” based analysis of wh-questions provides a way of capturing this intuition; however, it does so by lifting the acyclicity constraint on dependency hierarchies (Nivre 2004: 9). To avoid this potentially problematic move, I propose an alternative whereby the primacy of the finite verb and the primacy of other (focussed, interrogative or negative) expressions can be linked to separate dimensions of description. The concept of dimensions adopted in the paper is formally similar to XDG’s related notion (Debusmann et al. 2004). In content, however, it is closer to Halliday’s (1994, 2004) understanding of the term. 
With a dependency grammar, this study provides a uniﬁed method for calculating the syntactic complexity in linear and hierarchical dimensions. Two metrics, mean dependency distance (MDD) and mean hierarchical distance (MHD), one for each dimension, are adopted. Some results from the Czech-English dependency treebank are revealed: (1) Positive asymmetries in the distributions of the two metrics are observed in English and Czech, which indicates both languages prefer the minimalization of structural complexity in each dimension. (2) There are signiﬁcantly positive correlations between sentence length (SL), MDD, and MHD. For longer sentences, English prefers to increase the MDD, while Czech tends to enhance the MHD. (3) A trade-off relationship of syntactic complexity in two dimensions is shown between the two languages. English tends to reduce the complexity of production in the hierarchical dimension, whereas Czech prefers to lessen the processing load in the linear dimension. (4) The threshold of the MDD2 and MHD2 in English and Czech is 4. 
 This paper discusses the adaptation of the  Stanford typed dependency model (de  Marneffe and Manning 2008), initially  designed for English, to the requirements of  typologically different languages from the  viewpoint of practical parsing. We argue for  a framework of functional dependency  grammar that is based on the idea of  parallelism between syntax and semantics.  There is a twofold challenge: (1) specifying  the annotation scheme in order to deal with  the morphological and syntactic peculiarities  of each language and (2) maintaining cross-  linguistically consistent annotations to ensure  homogenous analysis for similar linguistic  phenomena. We applied a number of  modifications to the original Stanford scheme  in an attempt to capture the language-specific  grammatical features present in  heterogeneous CoNLL-encoded data sets for  German, Dutch, French, Spanish, Brazilian  Portuguese, Russian, Polish, Indonesian, and  Traditional Chinese. From a multilingual  perspective, we discuss features such as  subject and object verb complements,  comparative  phrases,  expletives,  reduplication, copula elision, clitics and  adpositions.  
This paper scrutinizes various dependency-based representations of the syntax of function words, such as prepositions. The focus is on the underlying formal object used to encode the linguistic analyses and its relation to the corresponding linguistic theory. The polygraph structure is introduced: it consists of a generalization of the concept of graph that allows edges to be vertices of other edges. Such a structure is used to encode dependency-based analyses that are founded on two kinds of morphosyntactic criteria: presence constraints and distributional constraints. 
Complex predicates with light verbs have proven to be very challenging for syntactic theories, particularly due to the tricky distribution of valency complementations of light verbs and predicative nouns (or other predicative units) in their syntactic structure. We propose a theoretically adequate and economical representation of complex predicates with Czech light verbs based on a division of their description between the lexicon and the grammar. We demonstrate that a close interplay between these two components makes the analysis of the deep and surface syntactic structures of complex predicates reliable and efﬁcient. 
Despite the recent advances in parsing, signiﬁcant efforts are needed to improve the current parsers performance, such as the enhancement of the argument/adjunct recognition. There is evidence that verb subcategorization frames can contribute to parser accuracy, but a number of issues remain open. The main aim of this paper is to show how subcategorization frames acquired from a syntactically annotated corpus and organized into ﬁne-grained classes can improve the performance of two rulebased dependency grammars. 
Recently, there has been great interest both in the development of cross-linguistically applicable annotation schemes and in the application of syntactic parsers at web scale to create parsebanks of online texts. The combination of these two trends to create massive, consistently annotated parsebanks in many languages holds enormous potential for the quantitative study of many linguistic phenomena, but these opportunities have been only partially realized in previous work. In this work, we take a key step toward universal web parsebanks through a single-language case study introducing the ﬁrst retrainable parser applied to the Universal Dependencies representation and its application to create a Finnish web-scale parsebank. We further integrate this data into an online dependency search system and demonstrate its applicability by showing linguistically motivated search examples and by using the dependency syntax information to analyze the language of the web corpus. We conclude with a discussion of the requirements of extending from this case study on Finnish to create consistently annotated web-scale parsebanks for a large number of languages. 
Full recovery of argument structure information for question answering or information extraction requires that parsers can analyse long-distance dependencies. Previous work on statistical dependency parsing has used post-processing or additional training data to tackle this complex problem. We evaluate an alternative approach to recovering long-distance dependencies. This approach uses a two-level parsing model to recover both grammatical dependencies, such as subject and object, and full argument structure. We show that this two-level approach is competitive, while also providing useful semantic role information. 
The paper focuses on the notion of (surface) syntactic subject in Serbian within the syntactic dependency framework of the MeaningText linguistic theory. Properties of the subject are described and its text implementations illustrated with data from a variety of contemporary texts. 
This contribution provides a historical overview of the analysis of function words in surface syntactic dependency hierarchies. Starting with Tesnière (1959), the overview progresses through some prominent voices in the history of DG (Mel'čuk 1958, 1963, Hays 1964, Matthews 1981, Schubert 1987, Maxwell and Schubert 1989, Hudson 1976, 1984, etc.). The overview establishes that the analysis of prepositions has been almost unanimous: they are positioned as heads over their nouns. There has been more variation concerning the status of auxiliary verbs, although most DG grammarians have viewed them as heads over their content verbs. Concerning determiners, the dominant position is that they are dependents under their nouns, although there are a couple of prominent voices that assume the opposite stance. 
This contribution delivers two messages: 1) that the tests for constituents that are widely employed in linguistics and syntax textbooks are more congruent with dependency-based syntax than with constituency-based syntax and 2) that these same tests support the conventional analysis of function words, that is, the analysis that takes most function words (auxiliary verbs, adpositions, subordinators) to be heads over the content words with which they cooccur. The latter issue is important at present, since a recent annotation scheme is choosing to subordinate all function words to the content words with which they coocur. 
 This contribution examines the descriptive and resultative de-constructions in Mandarin Chinese, e.g. Wǒ pǎo-de hěn kuài ‘I run very fast’. There is a longstanding debate about this construction. The primary point of dispute concerns the main predicate: Is the first predicate the root of the sentence, i.e. pǎo-de ‘run’, or is the second predicate the root, i.e. kuài ‘fast’? We demonstrate here that from a dependency grammar (DG) perspective, the second predicate should be taken as the root. A number of diagnostics support this conclusion: 1) yes/no-questions with ma, 2) position of the negation bù, 3) omission, 4) placement of the adverb yě ‘also’, 5) ne-questions, and 6) modal insertion. The conclusion is important for the development of DG as applied to the syntax of Mandarin, since many basic questions about Mandarin sentence structure have not yet been examined from a DG perspective.  
Much work on ellipsis has been conducted using data from English, and many widely acknowledged types of ellipsis exist in English. The extent to which the named ellipsis mechanisms exist in other languages is, though, often not clear. This manuscript surveys ellipsis in Mandarin Chinese using a dependency-based approach to syntax. It probes to see which ellipsis mechanisms exist in Mandarin. The survey demonstrates that gapping, stripping, pseudogapping, sluicing, and comparative deletion do not exist in Mandarin (or are highly restricted) and that VP-ellipsis, answer ellipsis, and N-ellipsis are all arguably present. Furthermore, zero anaphora is frequent in Mandarin, whereas it is absent from English (or highly restricted). The catena unit is pillar of the account, since the elided material of ellipsis is a catena. 
We compare two annotation styles, Prague dependencies and Universal Stanford Dependencies, in their adequacy for parsing. We speciﬁcally focus on comparing the adposition attachment style, used in these two formalisms, applied in multisource cross-lingual delexicalized dependency parser transfer performed by parse tree combination. We show that in our setting, converting the adposition annotation to Stanford style in the Prague style training treebanks leads to promising results. We ﬁnd that best results can be obtained by parsing the target sentences with parsers trained on treebanks using both of the adposition annotation styles in parallel, and combining all the resulting parse trees together after having converted them to the Stanford adposition style (+0.39% UAS over Prague style baseline). The score improvements are considerably more significant when using a smaller set of diverse source treebanks (up to +2.24% UAS over the baseline). 
The paper introduces a new annotation of discourse relations in the Prague Dependency Treebank (PDT), i.e. the annotation of the so called secondary connectives (mainly multiword phrases like the condition is, that is the reason why, to conclude, this means etc.). Firstly, the paper concentrates on theoretical introduction of these expressions (mainly with respect to primary connectives like and, but, or, too etc.) and tries to contribute to the description and definition of discourse connectives in general (both primary and secondary). Secondly, the paper demonstrates possibilities of annotations of secondary connectives in large corpora (like PDT). The paper describes general annotation principles for secondary connectives used in PDT for Czech and compares the results of this annotation with annotation of primary connectives in PDT. In this respect, the main aim of the paper is to introduce a new type of discourse annotation that could be adopted also by other languages. 
We present a dependency parser for Persian, called ParsPer, developed using the graph-based parser in the Mate Tools. The parser is trained on the entire Uppsala Persian Dependency Treebank with a speciﬁc conﬁguration that was selected by MaltParser as the best performing parsing representation. The treebank’s syntactic annotation scheme is based on Stanford Typed Dependencies with extensions for Persian. The results of the ParsPer evaluation revealed a best labeled accuracy over 82% with an unlabeled accuracy close to 87%. The parser is freely available and released as an open source tool for parsing Persian. 
This paper investigates the potential of deﬁning a parsing representation for English data in Universal Dependencies, a crosslingual dependency scheme. We investigate structural transformations that change the choices of headedness in the dependency tree. The transformations make auxiliaries, copulas, subordinating conjunctions and prepositions heads, while in UD they are dependents of a lexical head. We show experimental results for the performance of MaltParser, a data-driven transition-based parser, on the product of each transformation. While some transformed representations favor performance, inverting the transformations to obtain UD for the ﬁnal product propagates errors, in part due to the nature of lexical-head representations. This prevents the transformations from being profitably used to improve parser performance in that representation. 
The notion of catena was introduced originally to represent the syntactic structure of multiword expressions with idiosyncratic semantics and non-constituent structure. Later on, several other phenomena (such as ellipsis, verbal complexes, etc.) were formalized as catenae. This naturally led to the suggestion that a catena can be considered a basic unit of syntax. In this paper we present a formalization of catenae and the main operations over them for modelling the combinatorial potential of units in dependency grammar. 
This paper analyses several points of interlingual dependency mismatch on the material of a parallel Czech-English dependency treebank. Particularly, the points of alignment mismatch between the valency frame arguments of the corresponding verbs are observed and described. The attention is drawn to the question whether such mismatches stem from the inherent semantic properties of the individual languages, or from the character of the used linguistic theory. Comments are made on the possible shifts in meaning. The authors use the ﬁndings to make predictions about possible machine translation implementation of the data. 
This paper presents cross-lingual models for dependency parsing using the ﬁrst release of the universal dependencies data set. We systematically compare annotation projection with monolingual baseline models and study the effect of predicted PoS labels in evaluation. Our results reveal the strong impact of tagging accuracy especially with models trained on noisy projected data sets. This paper quantiﬁes the differences that can be observed when replacing gold standard labels and our results should inﬂuence application developers that rely on crosslingual models that are not tested in realistic scenarios. 
Lexical-semantic knowledges sources are a stock item in the language technologist’s toolbox, having proved their practical worth in many and diverse natural language processing (NLP) applications. In linguistics, lexical semantics comes in many ﬂavors, but in the NLP world, wordnets reign more or less supreme. There has been some promising work utilizing Roget-style thesauruses instead, but wider experimentation is hampered by the limited availability of such resources. The work presented here is a ﬁrst step in the direction of creating a freely available Roget-style lexical resource for modern Swedish. Here, we explore methods for automatic disambiguation of interresource mappings with the longer-term goal of utilizing similar techniques for automatic enrichment of lexical-semantic resources. 
When working on a lexical resource, such as Swedish FrameNet (SweFN), assumptions based on linguistic theories are made, and methodological directions based upon them are taken. These directions often need to be revised when not beforehand foreseen problems arise. One assumption that was made already in the early development stages of SweFN was that each lexical entry from the reference lexicon, SALDO, would evoke only one semantic frame in SweFN. If a lexical entry evoked more than one frame, it entailed more than one sense and therefore required a new entry in the lexicon. As work progressed, this inclination towards splitting, in the perpetual lumpers and splitters discussion (Kilgarriff, 1999), proved to be progressively untenable. This paper will give an account of the problems which were encountered and suggestions for solutions on polysemy issues forcing a discussion on lumping or splitting. 
The paper describes a supervised approach for the detection of the most frequent senses of words on the basis of RuThes thesaurus, which is a large linguistic ontology for Russian. Due to the large number of monosemous multiword expressions and the set of RuThes relations it is possible to calculate several context features for ambiguous words and to study their contribution to a supervised model for detecting frequent senses. 
This paper describes the extraction of information on lethal events from the Swedish version of Wikipedia. The information searched includes the persons’ cause of death, origin, and profession. We carried out the extraction using a processing pipeline of available tools for Swedish including a part-of-speech tagger, a dependency parser, and manually-written extraction rules. We also extracted structured semantic data from the Wikidata store that we combined with the information retrieved from Wikipedia. Eventually, we gathered a database of facts that covers both sources: Wikipedia and Wikidata. 
We present the results of a coarse-grained sense annotation task on verbs, nouns and adjectives across six textual domains in Danish. We present the domain-wise differences in intercoder agreement and discuss how the applicability and validity of the sense inventory vary depending on domain. We ﬁnd that domain-wise agreement is not higher in very canonical or edited text. In fact, newswire text and parliament speeches have lower agreement than blogs and chats, probably because the language of these text types is more complex and uses more abstract concepts. We further observe that domains differ in their sense distribution. For instance, newswire and magazines stand out as having a high focus on persons, and discussion fora typically include a restricted number of senses dependent on specialized topics. We anticipate that these ﬁndings can be exploited in automatic sense tagging when dealing with domain shift. 
Automated scoring systems which evaluate content require robust ways of dealing with form errors. The work presented in this paper is set in the context of scoring learners’ responses to listening comprehension items included in a placement test of German as a foreign language. Based on a corpus of over 3000 responses to 17 questions, by test takers of different language proﬁciencies, we perform a quantitative analysis of the diversity in misspellings. We evaluate the performance of an off-the-shelf open source spell-checker on our data showing that around 45% of the reported non-word errors are not correctly accounted for, that is, they are either falsely identiﬁed as misspelt or the spellchecker is unable to identify the intended word. We propose to address misspellings in computer-based scoring of constructed response items by means of phonetic normalization. Learner responses transcribed into Soundex codes and into two encodings borrowed from historical linguistics (ASJP and Dolgopolsky’s sound classes) are compared to transcribed reference answers using string distance measures. We show that reliable correlation with teachers’ scores can be obtained, however, similarity thresholds are item-speciﬁc. 
Talebob (Speech Bob) is a newly developed interactive CALL-tool for training Danish speech with special regard to the pronunciation of highly idiomatic phrases. Talebob is currently being tested in primary schools in Nuuk, Hafnarfjörður and Tórshavn (where Danish is taught as a L2). The purpose of the current paper is twofold. We first introduce Talebob in its publicly available version, commenting on its linguistic, technical, and didactic principles. Secondly, we present our current plans and goals for the next version of Talebob focusing on linguistic and educational perspectives. Taking Talebob II as a point of departure, we wish to invite a discussion of ICALL as a means of modernizing the L2 educational programmes in the Nordic area. 
This paper proposes a framework for modeling and analyzing differences between texts written by different subgroups of learners of English as a Foreign Language (organized according to native language (L1) and proﬁciency level). Using frequency vectors of both POS-trigrams and mixed POS and function word trigrams, we compare learner language variants both to each other and to native English, German, and Chinese texts. We introduce the trigram usage factor metric for identifying sequences that are especially characteristic of a particular subgroup of learners. We show that distance between learner English and native English decreases with proﬁciency. Next we compare the distance between learner English and other native languages. Finally, we show that automatic proﬁciency classiﬁcation beneﬁts from using L1-speciﬁc classiﬁers. 
This paper proposes integration of three open source utilities: brat web annotation tool, Freeling suite of linguistic analyzers and Aspell spellchecker. We demonstrate how their combination can be used to preannotate texts in a learner corpus of English essays with potential errors and ease human annotators’ work. Spellchecker alerts and morphological analyzer tagging probabilities are used to detect students’ possible errors of most typical sorts. F-measure for the developed pre-annotation framework with regard to human annotation is 0.57, which already makes the system a substantial help to human annotators, but at the same time leaves room for further improvement. 
Automatic short-answer grading promises improved student feedback at reduced teacher effort both during and after instruction. Automated grading is, however, controversial in high-stakes testing and complex systems can be difﬁcult to set up by non-experts, especially for frequently changing questions. We propose a versatile, domain-independent system that assists manual grading by pre-sorting answers according to their similarity to a reference answer. We show near state-ofthe-art performance on the task of automatically grading the answers from CREG (Meurers et al., 2011). To evaluate the grader assistance task, we present CSSAG (Computer Science Short Answers in German), a new corpus of German computer science questions answered by natives and highly-proﬁcient non-natives. On this corpus, we demonstrate the positive inﬂuence of answer sorting on the slowest-graded, most complex-to-assess questions. 
This paper describes porting Oahpa, a set of advanced interactive language learning programs, to two new languages both of which spoken in Estonia – Estonian and Vo˜ro. Our programs offer a platform where the user can practice vocabulary and the generation of morphologically complex forms both in isolation and within sentential contexts. An overview of the Oahpa system and its two important building blocks – the morphological ﬁnite state transducer and the pedagogical lexicon – is given. The development of morphological ﬁnite state transducers for Estonian and Vo˜ro, as well as tailoring the speciﬁc transducers for pedagogical purposes are described. The adaptation of both Estonian and Vo˜ro Oahpa to the target user groups is also discussed. 
We want to build machines that read, and make inferences based on what was read. A long line of the work in the ﬁeld has focussed on approaches where language is converted (possibly using machine learning) into a symbolic and relational representation. A reasoning algorithm (such as a theorem prover) then derives new knowledge from this representation. This allows for rich knowledge to captured, but generally suffers from two problems: acquiring sufﬁcient symbolic background knowledge and coping with noise and uncertainty in data. Probabilistic logics (such as Markov Logic) offer a solution, but are known to often scale poorly. In recent years a third alternative emerged: latent variable models in which entities and relations are embedded in vector spaces (and represented "distributional"). Such approaches scale well and are robust to noise, but they raise their own set of questions: What type of inferences do they support? What is a proof in embeddings? How can explicit background knowledge be injected into embeddings? In this talk I ﬁrst present our work on latent variable models for machine reading, using ideas from matrix factorisation as well as both closed and open information extraction. Then I will present recent work we conducted to address the questions of injecting and extracting symbolic knowledge into/from models based on embeddings. In particular, I will show how one can rapidly build accurate relation extractors through combining logic and embeddings. Bio Dr. Riedel is a Senior Lecturer in the Department of Computer Science at University College London, leading the Machine Reading lab. He received his MSc and PhD (in 2009) in Computer Science from the University of Edinburgh. He was a researcher at the University of Tokyo, and a postdoc with Andrew McCallum at the University of Massachusetts Amherst. He is an Allen Distinguished Investigator, a Marie Curie CIG fellow, was a ﬁnalist for the Microsoft Research Faculty Award in 2013 and recently received a Google Focused Research award. Sebastian is generally interested in the intersection of NLP and machine learning, and particularly interested in teaching machines to read, and to reason with what was read.  Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015)  xv   
This paper details the design of the lexical and syntactic layers of a new annotated corpus of Swedish contemporary texts. In order to make the corpus adaptable into a variety of representations, the annotation is of a hybrid type with head-marked constituents and function-labeled edges, and with a rich annotation of non-local dependencies. The source material has been taken from public sources, to allow the resulting corpus to be made freely available. 
We present a case study on supervised classiﬁcation of Swedish pseudocoordination (SPC). The classiﬁcation is attempted on the type-level with data collected from two data sets: a blog corpus and a ﬁction corpus. Two small experiments were designed to evaluate the feasability of this task. The ﬁrst experiment explored a classiﬁer’s ability to discriminate pseudo-coordinations from ordinary verb coordinations, given a small labeled data set created during the experiment. The second experiment evaluated how well the classiﬁer performed at detecting and ranking SPCs in a set of unlabeled verb coordinations, to investigate if it could be used as a semi-automatic discovery procedure to ﬁnd new SPCs. 
We describe the creation of a new Danish resource for automated coarse-grained word sense disambiguation of running text (supersense tagging, SST). Based on corpus evidence we expand the sense inventory to incorporate new lexical classes. We add tags for verbal satellites like collocates, particles and reﬂexive pronouns, to give account for the satellite-framing properties of Danish. Finally, we evaluate the quality of our expanded sense inventory in terms of variation in F1 on a stateof-the-art SST system. The SST systems uses type constraints and achieves performance just under the upper bound of interannotator agreement. The initial release is a 1,500-sentence corpus covering six genres, made available under an open-source license.1 
This paper discusses methodological strengths and shortcomings of the Constraint Grammar paradigm (CG), showing how the classical CG formalism can be extended to achieve greater expressive power and how it can be enhanced and hybridized with techniques from other parsing paradigms. We present a new, largely theory­independent CG framework and rule compiler (CG­3), that allows the linguist to write CG rules incorporating different types of linguistic information and methodology from a wide range of parsing approaches, covering not only CG's native topological technique, but also dependency grammar, phrase structure grammar and unification grammar. In addition, we allow the integration of statistical­numerical constraints and non­discrete tag and string sets. 
This article presents a study of lemmatisation of ﬂexible multiword expressions in Lithuanian. An approach based on syntactic analysis designed for multiword term lemmatisation was adapted for a broader range of MWEs taken from the Dictionary of Lithuanian Nominal Phrases. In the present analysis, the main lemmatisation errors are identiﬁed and some improvements are proposed. It shows that automatic lemmatisation can be improved by taking into account the whole set of grammatical forms for each MWE. It would allow selecting the optimal grammatical form for lemmatisation and identifying some grammatical restrictions. 
In this paper we describe a question interpretation module designed as a part of a Question Answering Dialogue System (QADS) which is used for an interactive quiz application. Question interpretation is achieved in applying a sequence of classiﬁcation, information extraction, query formalization and query expansion tasks. The process of a question classiﬁcation is performed based on a domain-speciﬁc taxonomy of semantic roles and relations. Our taxonomy was designed in accordance with the real spoken dialogue data. The SVM-based classiﬁer is trained to predict the Expected Answer Type (EAT) with the precision of 82%. In order to retrieve a correct answer, focus word(-s) are extracted to augment the EAT identiﬁed by the system. Our hybrid algorithm for the extraction of focus words demonstrates the accuracy of 94.6%. EAT together with focus words are formalized in a query, which is further expanded with the synonyms from WordNet. The expanded query facilitates the search and retrieval of the information that is necessary to generate the system’s responses. 
We present a study in which we seek to interpret spatial references that are part of in-situ route descriptions. Our aim is to resolve these references to actual entities and places in the city using a crowdsourced geographic database (OpenStreetMap). We discuss the problems related to this task, and present a possible automatic reference resolution method that can ﬁnd the correct referent in 68% of the cases using features that are easily computable from the map. 
We present a new approach to word sense disambiguation derived from recent ideas in distributional semantics. The input to the algorithm is a large unlabeled corpus and a graph describing how senses are related; no sense-annotated corpus is needed. The fundamental idea is to embed meaning representations of senses in the same continuous-valued vector space as the representations of words. In this way, the knowledge encoded in the lexical resource is combined with the information derived by the distributional methods. Once this step has been carried out, the sense representations can be plugged back into e.g. the skip-gram model, which allows us to compute scores for the different possible senses of a word in a given context. We evaluated the new word sense disambiguation system on two Swedish test sets annotated with senses deﬁned by the SALDO lexical resource. In both evaluations, our system soundly outperformed random and ﬁrst-sense baselines. Its accuracy was slightly above that of a wellknown graph-based system, while being computationally much more efﬁcient. 
Talebob ("Speech Bob") is an interactive language learning tool for pupils (10+ years) helping them practice their pronunciation of simple, highly frequent phrases in Danish. Talebob's feedback is based on acoustic measurements (for pitch and intensity), presented to the user as helpful instructions for improvement. Talebob is currently being tested in schools in Nuuk, Hafnarfjörður and Tórshavn where Danish is taught as a second language (L2); we present some preliminary results. We conclude with a discussion of the didactic relevance of Talebob and computer-assisted language learning in general, exploiting the ITcuriosity of modern pupils. 
This paper reports the ﬁrst authorship attribution results based on the effect of the author set size using automatic computational methods for the Lithuanian language. The aim is to determine how fast authorship attribution results are deteriorating while the number of candidate authors is gradually increasing: i.e. starting from 3, going up to 5, 10, 20, 50, and 100. Using supervised machine learning techniques we also investigated the inﬂuence of different features (lexical, character, morphological, etc.) and language types (normative parliamentary speeches and non-normative forum posts). The experiments revealed that the effectiveness of the method and feature types depends more on the language type rather than on the number of candidate authors. The content features based on word lemmas are the most useful type for the normative texts, due to the fact that Lithuanian is a highly inﬂective, morphologically and vocabulary rich language. The character features are the most accurate type for forum posts, where texts are too complicated to be effectively processed with external morphological tools. 
Natural language processing (NLP) tools are often developed with the intention of easing human processing, a goal which is hard to measure. Eye movements in reading are known to reﬂect aspects of the cognitive processing of text (Rayner et al., 2013). We explore how eye movements reﬂect aspects of reading that are of relevance to NLP system evaluation and development. This becomes increasingly relevant as eye tracking is becoming available in consumer products. In this paper we present an analysis of the differences between reading automatic sentence compressions and manually simpliﬁed newswire using eye-tracking experiments and readers’ evaluations. We show that both manual simpliﬁcation and automatic sentence compression provide texts that are easier to process than standard newswire, and that the main source of difﬁculty in processing machine-compressed text is ungrammaticality. Especially the proportion of regressions to previously read text is found to be sensitive to the differences in human- and computer-induced complexity. This ﬁnding is relevant for evaluation of automatic summarization, simpliﬁcation and translation systems designed with the intention of facilitating human reading. 
This paper describes a semi-supervised approach to improving statistical dependency parsing using dependency-based word clusters. After applying a baseline parser to unlabeled text, clusters are induced using K-means with word features based on the dependency structures. The parser is then re-trained using information about the clusters, yielding improved parsing accuracy on a range of different data sets, including WSJ and the English Web Treebank. We report improved results using both in-domain and out-of-domain data, and also include a comparison with using n-gram–based Brown clustering. 
We model the problem of monolingual textual alignment as a Quadratic Assignment Problem (QAP) which simultaneously maximizes the global lexicosemantic and syntactic similarities of two sentence-level texts. Because QAP is an NP-complete problem, we propose a branch-and-bound approach to efﬁciently ﬁnd an optimal solution. When compared with other methods and studies, our results are competitive. 
This paper investigates sentence compression for automatic subtitle generation using supervised machine learning. We present a method for sentence compression as well as discuss generation of training data from compressed Finnish sentences, and different approaches to the problem. The method we present outperforms state-of-the-art baseline in both automatic and human evaluation. On real data, 44.9% of the sentences produced by the compression algorithm have been judged to be useable as-is or after minor edits. 
The paper describes the results of an empirical study of integrating bigram collocations and similarities between them and unigrams into topic models. First of all, we propose a novel algorithm PLSASIM that is a modiﬁcation of the original algorithm PLSA. It incorporates bigrams and maintains relationships between unigrams and bigrams based on their component structure. Then we analyze a variety of word association measures in order to integrate top-ranked bigrams into topic models. All experiments were conducted on four text collections of diﬀerent domains and languages. The experiments distinguish a subgroup of tested measures that produce top-ranked bigrams, which demonstrate signiﬁcant improvement of topic models quality for all collections, when integrated into PLSA-SIM algorithm. 
In this paper we explore the idea of using verb valency information to improve verb phrase extraction from historical text. As a case study, we perform experiments on Early Modern Swedish data, but the approach could easily be transferred to other languages and/or time periods as well. We show that by using verb valency information in a post-processing step to the verb phrase extraction system, it is possible to remove improbable complements extracted by the parser and insert probable complements not extracted by the parser, leading to an increase in both precision and recall for the extracted complements. 
There has been substantial recent interest in annotation schemes that can be applied consistently to many languages. Building on several recent efforts to unify morphological and syntactic annotation, the Universal Dependencies (UD) project seeks to introduce a cross-linguistically applicable part-of-speech tagset, feature inventory, and set of dependency relations as well as a large number of uniformly annotated treebanks. We present Universal Dependencies for Finnish, one of the ten languages in the recent ﬁrst release of UD project treebank data. We detail the mapping of previously introduced annotation to the UD standard, describing speciﬁc challenges and their resolution. We additionally present parsing experiments comparing the performance of a stateof-the-art parser trained on a languagespeciﬁc annotation schema to performance on the corresponding UD annotation. The results show improvement compared to the source annotation, indicating that the conversion is accurate and supporting the feasibility of UD as a parsing target. The introduced tools and resources are available under open licenses from http://bionlp.utu.fi/ud-finnish.html. 
We evaluate the effectiveness of ﬁnitestate tools we developed for automatically annotating word stress in Russian unrestricted text. This task is relevant for computer-assisted language learning and text-to-speech. To our knowledge, this is the ﬁrst study to empirically evaluate the results of this task. Given an adequate lexicon with speciﬁed stress, the primary obstacle for correct stress placement is disambiguating homographic wordforms. The baseline performance of this task is 90.07%, (known words only, no morphosyntactic disambiguation). Using a constraint grammar to disambiguate homographs, we achieve 93.21% accuracy with minimal errors. For applications with a higher threshold for errors, we achieved 96.15% accuracy by incorporating frequency- based guessing and a simple algorithm for guessing the stress position on unknown words. These results highlight the need for morphosyntactic disambiguation in the word stress placement task for Russian, and set a standard for future research on this task. 
We present a novel interactive approach for the visual analysis of intonation contours. Audio data are processed algorithmically and presented to researchers through interactive visualizations. To this end, we automatically analyze the data using machine learning in order to ﬁnd groups or patterns. These results are visualized with respect to meta-data. We present a ﬂexible, interactive system for the analysis of prosodic data. Using realworld application examples, one containing preprocessed, the other raw data, we demonstrate that our system enables researchers to interact dynamically with the data at several levels and by means of different types of visualizations, thus arriving at a better understanding of the data via a cycle of hypothesis generation and testing that takes full advantage of our visual processing abilities. 
This paper presents several modiﬁcations of the standard annotation projection algorithm for syntactic structures in crosslingual dependency parsing. Our approach reduces projection noise and includes efﬁcient data sub-set selection techniques that have a substantial impact on parser performance in terms of labeled attachment scores. We test our techniques on data from the Universal Dependency Treebank and demonstrate the improvements on a number of language pairs. We also look at treebank translation including syntaxbased models and data combination techniques that push the performance even further. We achieve absolute improvements of up to over seven points in labeled attachment scores pushing the state-of-the art in cross-lingual dependency parsing for all language pairs tested in our experiments. 
In this paper, we report on a two-part experiment aiming to assess and compare the performance of two types of automatic speech recognition (ASR) systems on two different computational platforms when used to augment dictation workflows. The experiment was performed with a sample of speakers of three major languages and with different linguistic profiles: non-native English speakers; non-native French speakers; and native Spanish speakers. The main objective of this experiment is to examine ASR performance in translation dictation (TD) and medical dictation (MD) workflows without manual transcription vs. with transcription. We discuss the advantages and drawbacks of a particular ASR approach in different computational platforms when used by various speakers of a given language, who may have different accents and levels of proficiency in that language, and who may have different levels of competence and experience dictating large volumes of text, and with ASR technology. Lastly, we enumerate several areas for future research. 
For the purposes of computational dialectology or other geographically bound text analysis tasks, texts must be annotated with their or their authors’ location. Many texts are locatable but most have no explicit annotation of place. This paper describes a series of experiments to determine how positionally annotated microblog posts can be used to learn location indicating words which then can be used to locate blog texts and their authors. A Gaussian distribution is used to model the locational qualities of words. We introduce the notion of placeness to describe how locational words are. We ﬁnd that modelling word distributions to account for several locations and thus several Gaussian distributions per word, deﬁning a ﬁlter which picks out words with high placeness based on their local distributional context, and aggregating locational information in a centroid for each text gives the most useful results. The results are applied to data in the Swedish language. 
This paper presents a rule-based method for converting between colloquial Finnish and standard Finnish. The method relies upon a small number of orthographical rules combined with a large language model of standard Finnish for ranking the possible conversions. Aside from this contribution, the paper also presents an evaluation corpus consisting of aligned sentences in colloquial Finnish, orthographically-standardised colloquial Finnish and standard Finnish. The method we present outperforms the baseline of simply treating colloquial Finnish as standard Finnish, but is outperformed by a phrase-based MT system trained by the evaluation corpus. The paper also presents preliminary results which show promise for using normalisation in the machine translation task. 
Statistical analysis of parliamentary roll call votes is an important topic in political science as it reveals ideological positions of members of parliament and factions. However, these positions depend on the issues debated and voted upon as well as on attitude towards the governing coalition. Therefore, analysis of carefully selected sets of roll call votes provides deeper knowledge about members of parliament behavior. However, in order to classify roll call votes according to their topic automatic text classiﬁers have to be employed, as these votes are counted in thousands. In this paper we present results of an ongoing research on thematic classiﬁcation of roll call votes of the Lithuanian Parliament. Also, this paper is a part of a larger project aiming to develop the infrastructure designed for monitoring and analyzing roll call voting in the Lithuanian Parliament. 
This paper describes ongoing work related to the analysis of spoken utterance transcripts and estimating the speaker’s attitude towards the whole dialogue on the basis of their opinions expressed by utterances. Using the standard technology used in sentiment analysis, we report promising results which can be linked to the conversational participants’ self-evaluation of their experience of the interaction. 
In this paper we explore how word vectors built using word2vec can be used to improve the performance of a classiﬁer during Named Entity Recognition. Thereby, we discuss the best integration of word embeddings into the classiﬁcation problem and consider the effect of the size of the unlabelled dataset on performance, reaching the unexpected result that for this particular task increasing the amount of unlabelled data does not necessarily increase the performance of the classiﬁer. 
This article describes a real (nonsynthetic) active-learning experiment to obtain supersense annotations for Danish. We compare two instance selection strategies, namely lowest-prediction conﬁdence (MAX), and sampling from the conﬁdence distribution (SAMPLE). We evaluate their performance during the annotation process, across domains for the ﬁnal resulting system, as well as against in-domain adjudicated data. The SAMPLE strategy yields competitive models that are more robust than the overly length-biased selection criterion of MAX. 
Can relations described by English nounnoun compounds be adequately captured by prepositions? We attempt to answer this question in a data-driven way, using gamiﬁcation to annotate a set of about a thousand noun-noun compound examples. Annotators could make a choice out of ﬁve prepositions generated with the help of paraphrases found in the Google ngram corpus. We show that there is substantial agreement among the players of our linguistic annotation game, and that their answers differ in about 50% of raw frequency counts of the Google n-gram corpus. Prepositions can be used to describe the majority of the implicit relations present in noun-noun compounds, but not all relations are captured by natural prepositions and some compounds are not easy to paraphrase with the use of a preposition. 
We report on results from using the multivariate readability model SVIT to classify texts into various levels. We investigate how the language features integrated in the SVIT model can be transformed to values on known criteria like vocabulary, grammatical ﬂuency and propositional knowledge. Such text criteria, sensitive to content, readability and genre in combination with the proﬁle of a student’s reading ability form the base of individually adapted texts. The procedure of levelling texts into different stages of complexity is presented along with results from the ﬁrst cycle of tests conducted on 8th grade students. The results show that SVIT can be used to classify texts into different complexity levels. 
We have used a novel Bayesian model of joint word alignment and part of speech (PoS) annotation transfer to enrich the Swedish Sign Language Corpus with PoS tags. The annotations were then handcorrected in order to both improve annotation quality for the corpus, and allow the empirical evaluation presented herein. 
Tree kernels have been used as an efﬁcient solution for many tasks, but are difﬁcult to calculate. To address this problem, in this paper we introduce the Positional Sufﬁx Trees: a novel data structure devised to store tree structures, as well as the MFTK and EFTK algorithms, which use them to estimate Subtree and Subspace Tree Kernels. Results show that the Positional Sufﬁx Tree can store large amounts of trees in a scalable fashion, and that our algorithms are up to 22 times faster than the state-ofthe-art approach. 
This paper describes a knowledge-based approach to word-sense disambiguation using a lexical-semantic resource, SALDO. This hierarchically organized lexicon deﬁning senses in terms of other related senses has not been previously explored for this purpose. The proposed method is based on maximizing the overlap between associated word senses of nouns and verbs co-occuring within a sentence. The results of a small-scale experiment using this method are also reported. Overall, the approach proved more efﬁcient for nouns, since not only was the accuracy score higher for this category (56%) than for verbs (46%), but for nouns in 22% more of the cases was a sense overlap found. As a result of an in-depth analysis of the predictions, we identiﬁed a number of ways the system could be modiﬁed or extended for an improved performance. 
In the Latvian language, one word can have tens or even hundreds of surface forms. This is a serious problem for large vocabulary speech recognition. Inclusion of every form in vocabulary will make it intractable, but, on the other hand, even with a vocabulary of 400K, the out-ofvocabulary (OOV) rate will be very high. In this paper, the authors investigate the possibility of using sub-word vocabularies where words are split into frequent and common parts. The results of our experiment show that this allows to significantly reduce the OOV rate. 
This paper describes work in progress. We experiment with training a state-of-the-art tagger, Stagger, on a new gold standard, MIM-GOLD, for the PoS tagging of Icelandic. We compare the results to results obtained using a previous gold standard, IFD. Using MIM-GOLD, tagging accuracy is considerably lower, 92.76% compared to 93.67% accuracy for IFD. We analyze and classify the errors made by the tagger in order to explain this difference. We ﬁnd that inconsistencies and incorrect tags in MIM-GOLD may account for this difference. 
At the National Library of Norway, we are currently developing a service comparable to the Google Ngram Viewer (Michel et al., 2010; Lin et al., 2012; Aiden and Michel, 2013) called NB Ngram. It is based on all books and newspapers digitized up to and including 2013, as part of the large scale digitization project at the National Library of Norway. Uni-, bi- and trigams have been generated on the basis of this text corpus containing some 34 billion words. In this paper, we sketch the background of NB N-gram and illustrate some applications of it. 
This paper contains a description of the Corpus of American Norwegian Speech, a new tool for heritage language research. We present the background for its existence, the linguistic contents and its main technical features. The demonstration will show the corpus in use, focussing on problems that are specific to heritage language research, and how the corpus can be searched to provide relevant data. 
 Boxer is a semantic parser for English texts with many input and output possibilities, and various ways to perform meaning analysis based on Discourse Representation Theory. This involves the various ways that meaning representations can be computed, as well as their possible semantic ingredients.  
We use hfst-pmatch (Lindén et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor. We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text. 
 tory1. It can also be installed directly via Python’s pip package manager2.  discoursegraphs is a Python-based converter for linguistic annotation formats which facilitates the combination of several, heterogeneous layers of annotation of a document into a uniﬁed graph representation. The library supports a range of syntax and discourse-related formats and was successfully used to revise and merge a multilayered corpus (Stede and Neumann, 2014). 
This demonstration presents a freely available open source lexical database omorfi. Omorfi is a mature lexicographical database project, started out as a single-person single-purpose free open source morphological analyser project, omorfi has since grown to be used in variety of applications including spell-checking, statistical and rule-based machine translation, treebanking, joint syntactic and morphological parsing, poetry generation, information extraction. In this demonstration we hope to show both the variety of end-user facing applications as well as the tools and interfaces for computational linguists to make the best use of a developing product. We show a shallow database arrangement that has allowed a great variety of contributors from different projects to extend the lexical database while not breaking the continued use of existing end-applications. We hope to show both the best current practices for lexical data management and software engineering with regards to continuous external project integration of a constantly developing product. As case examples we show some of the integrations with following applications: Voikko spell-checking for Windows, Mac OS X, Linux and Android, statistical machine translation pipelines with moses, rule-based machine translation with apertium and traditional xerox style morphological analysis and generation. morphological segmentation, as well as application programming interfaces for python and Java.  
We present a rule based automatic text simpliﬁcation tool for Swedish. The tool is designed to facilitate experimentation with various simpliﬁcation techniques. The architecture of the tool is inspired by and partly built on a previous text simpliﬁcation tool for Swedish, CogFLUX. New functionality, new operation types, and new simpliﬁcation operations were added. 
As the volume of documents on the Web increases, technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services such as Twitter and Facebook is useful because it contains a lot of location-speciﬁc information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document. Previous studies on location disambiguation have tackled this problem on the basis of word sense disambiguation, and did not make use of location-speciﬁc clues. In this paper, we propose a method for location disambiguation that takes advantage of the following two clues: spatial proximity and temporal consistency. We conﬁrm the effectiveness of these clues through experiments on Twitter tweets with GPS information. 
Paraphrase Identiﬁcation and Semantic Similarity are two different yet well related tasks in NLP. There are many studies on these two tasks extensively on structured texts in the past. However, with the strong rise of social media data, studying these tasks on unstructured texts, particularly, social texts in Twitter is very interesting as it could be more complicated problems to deal with. We investigate and ﬁnd a set of simple features which enables us to achieve very competitive performance on both tasks in Twitter data. Interestingly, we also conﬁrm the signiﬁcance of using word alignment techniques from evaluation metrics in machine translation in the overall performance of these tasks. 
Machine Translation system accuracies are often brought down due to inaccurate Language Detection (LD) of input phrases. The Language detection accuracy is further affected when the inputs are short and contain ungrammatical phrases, especially in a multilingual mobile game setting. Chat messages in mobile games are often short as they are typed on mobile devices and contain slang as a common communication preference. Previous work has shown that LD systems have a drop in accuracy when the inputs are short messages instead of long ones. This paper targets LD for short chat messages in mobile games. We propose a novel LD system which integrates text-based and user-based methods to achieve signiﬁcantly better performance over current state-of-the-art LD systems. 
One way in which marketers gain insights about consumers is by identifying the occasions in which consumers use their products and which are invoked by their products. Identifying occasions helps in consumer segmentation, answering why consumers purchase a product, and where and when they use it. Additionally, the types of occasions a consumer participates in and the social settings surrounding those occasions provide insights into the consumer’s personality and sociocultural self. Insights such as these are required for understanding consumer behavior, which marketers need to better design and sell their products. In this paper, we describe a methodology for extracting and categorizing occasions from product reviews, product descriptions, and forum posts. We examine using a maximum entropy markov model (MEMM) and a linear chain conditional random ﬁeld (CRF) for extraction and ﬁnd the CRF results in a 72.4% F1-measure. Extracted occasions are categorized as one of six high-level types (Celebratory, Special, Seasonal, Temporal, Weather-Related, and Other) using a support vector machine with an 88.5% macroaveraged F1-measure. 
Determining explicit user characteristics based on interactions on Social Media is a crucial task in developing recommendation and social polling solutions. For this purpose, rule based and N-gram based techniques have been proposed to develop user profiles, but they are only fit for detecting user attributes that can be classified by a relatively simple logic or rely on the presence of a large amount of training data. In this paper, we propose a general purpose, end-to-end architecture for text analytics, and demonstrate its effectiveness for analytics based on tweets with a relatively small training set. By performing unsupervised feature learning and deep learning over labeled and unlabeled tweets, we are able to learn in a more generalizable way than N-gram techniques. Our proposed hidden layer sharing approach makes it possible to efficiently transfer knowledge between related NLP tasks. This approach is extensible, and can learn even more from metadata available about Social Media users. For the task of user age prediction over a relatively small corpus, we demonstrate 38.3% error reduction over single task baselines, a total of 44.7% error reduction with the incorporation of two related tasks, and achieve 90.1% accuracy when useful metadata is present. 
Large-scale data resources needed for progress toward natural language understanding are not yet widely available and typically require considerable expense and expertise to create. This paper addresses the problem of developing scalable approaches to annotating semantic frames and explores the viability of crowdsourcing for the task of frame disambiguation. We present a novel supervised crowdsourcing paradigm that incorporates insights from human computation research designed to accommodate the relative complexity of the task, such as exemplars and real-time feedback. We show that non-experts can be trained to perform accurate frame disambiguation, and can even identify errors in gold data used as the training exemplars. Results demonstrate the efﬁcacy of this paradigm for semantic annotation requiring an intermediate level of expertise. 
Return-on-Investment (ROI) is a costconscious approach to active learning (AL) that considers both estimates of cost and of beneﬁt in active sample selection. We investigate the theoretical conditions for successful cost-conscious AL using ROI by examining the conditions under which ROI would optimize the area under the cost/beneﬁt curve. We then empirically measure the degree to which optimality is jeopardized in practice when the conditions are violated. The reported experiments involve an English part-of-speech annotation task. Our results show that ROI can indeed successfully reduce total annotation costs and should be considered as a viable option for machine-assisted annotation. On the basis of our experiments, we make recommendations for beneﬁt estimators to be employed in ROI. In particular, we ﬁnd that the more linearly related a beneﬁt estimate is to the true beneﬁt, the better the estimate performs when paired in ROI with an imperfect cost estimate. Lastly, we apply our analysis to help explain the mixed results of previous work on these questions. 
Generics are linguistic expressions that make statements about or refer to kinds, or that report regularities of events. Non-generic expressions make statements about particular individuals or speciﬁc episodes. Generics are treated extensively in semantic theory (Krifka et al., 1995). In practice, it is often hard to decide whether a referring expression is generic or non-generic, and to date there is no data set which is both large and satisfactorily annotated. Such a data set would be valuable for creating automatic systems for identifying generic expressions, in turn facilitating knowledge extraction from natural language text. In this paper we provide the next steps for such an annotation endeavor. Our contributions are: (1) we survey the most important previous projects annotating genericity, focusing on resources for English; (2) with a new agreement study we identify problems in the annotation scheme of the largest currentlyavailable resource (ACE-2005); and (3) we introduce a linguistically-motivated annotation scheme for marking both clauses and their subjects with regard to their genericity. (4) We present a corpus of MASC (Ide et al., 2010) and Wikipedia texts annotated according to our scheme, achieving substantial agreement. 
In this paper, we present design and construction of the ﬁrst Italian corpus for automatic and semi–automatic text simpliﬁcation. In line with current approaches, we propose a new annotation scheme speciﬁcally conceived to identify the typology of changes an original sentence undergoes when it is manually simpliﬁed. Such a scheme has been applied to two aligned Italian corpora, containing original texts with corresponding simpliﬁed versions, selected as representative of two different manual simpliﬁcation strategies and addressing different target reader populations. Each corpus was annotated with the operations foreseen in the annotation scheme, covering different levels of linguistic description. Annotation results were analysed with the ﬁnal aim of capturing peculiarities and differences of the different simpliﬁcation strategies pursued in the two corpora. 
Understanding the structure of scientiﬁc discourse is of paramount importance for the development of appropriate Natural Language Processing tools able to extract and summarize information from research articles. In this paper we present an annotated corpus of scientiﬁc discourse in the domain of Computer Graphics. We describe the way we built our corpus by designing an annotation schema and relying on three annotators for manually classifying all sentences into the deﬁned categories. Our corpus constitutes a semantically rich resource for scientiﬁc text mining. In this respect, we also present the results of our initial experiments of automatic classiﬁcation of sentences into the 5 main categories in our corpus. 
Aspect-based opinion summarization is the task of automatically generating a summary for some aspects of a speciﬁc topic from a set of opinions. In most cases, to evaluate the quality of the automatic summaries, it is necessary to have a reference corpus of human summaries to analyze how similar they are. The scarcity of corpora in that task has been a limiting factor for many research works. In this paper, we introduce OpiSums-PT, a corpus of extractive and abstractive summaries of opinions written in Brazilian Portuguese. We use this corpus to analyze how similar human summaries are and how people take into account the issues of aspect coverage and sentiment orientation to generate manual summaries. The results of these analyses show that human summaries are diversiﬁed and people generate summaries only for some aspects, keeping the overall sentiment orientation with little variation. 
Code-switching, where a speaker switches between languages mid-utterance, is frequently used by multilingual populations worldwide. Despite its prevalence, limited effort has been devoted to develop computational approaches or even basic linguistic resources to support research into the processing of such mixedlanguage data. We present a user-centric approach to collecting code-switched utterances from social media posts, and develop language universal guidelines for the annotation of codeswitched data. We also present results for several baseline language identiﬁcation models on our corpora and demonstrate that language identiﬁcation in code-switched text is a difﬁcult task that calls for deeper investigation. 
This paper presents a discussion of the problems surrounding the task of annotating geographical entities on microblogs and reports the preliminary results of our efforts to annotate Japanese microblog texts. Unlike prior work, we not only annotate geographical location entities but also facility entities, such as stations, restaurants, shopping stores, hospitals and schools. We discuss ways in which to build a gazetteer, the types of ambiguities that need to be considered, reasons why the annotator tends to disagree, and the problems that need to be solved to automate the task of annotating the geographical entities. All the annotation data and the annotation guidelines are publicly available for research purposes from our web site. 
The potential of processing user-generated texts freely available on the web is widely recognized, but due to the non-canonical nature of the language used in the web, it is not possible to process these data using conventional methodologies designed for well-edited formal texts. Procedures for properly annotating raw web data have not been as extensively researched as those for annotating well-edited texts, as also evident from the viewpoint of Turkish language processing. Moreover, there is a considerable shortage of human-annotated corpora derived from Turkish web data. The ITU Web Treebank is the ﬁrst attempt for a diverse corpus compiled from Turkish texts found on the web. In this paper, we ﬁrst present our survey of the non-canonical aspects of the language used in the Turkish web. Next, we discuss in detail the annotation procedure followed in the ITU Web Treebank, revised for compatibility with the language of the web. Finally, we describe the web-based annotation tool following this procedure, on which the treebank was annotated. 
Translation process data contains noncanonical features such as incomplete word tokens, non-sequential string modifications and syntactically deficient structures. While these features are often removed for the final translation product, they are present in the unfolding text (i.e. intermediate translation versions). This paper describes tools developed to semi-automatically process intermediate versions of translation data to facilitate quantitative analysis of linguistic means employed in translation strategies. We examine the data from a translation experiment with the help of these tools. 
English prepositions are extremely frequent and extraordinarily polysemous. In some usages they contribute information about spatial, temporal, or causal roles/relations; in other cases they are institutionalized, somewhat arbitrarily, as case markers licensed by a particular governing verb, verb class, or syntactic construction. To facilitate automatic disambiguation, we propose a general-purpose, broadcoverage taxonomy of preposition functions that we call supersenses: these are coarse and unlexicalized so as to be tractable for efﬁcient manual annotation, yet capture crucial semantic distinctions. Our resource, including extensive documentation of the supersenses, many example sentences, and mappings to other lexical resources, will be publicly released. Prepositions are perhaps the most beguiling yet pervasive lexicosyntactic class in English. They are everywhere; their functional versatility is dizzying and largely idiosyncratic (1). They are nearly invisible, yet indispensable for situating the where, when, why, and how of events. In a way, prepositions are the bastard children of lexicon and grammar, rising to the occasion almost whenever a noun-noun or verbnoun relation is needed and neither subject nor object is appropriate. Consider the many uses of the word to, just a few of which are illustrated in (1):1 (1) a. My cake is to die for. b. If you want I can treat you to some. c. How about this: you go to the store d. to buy ingredients. e. Then if you give the recipe to me f. I’m happy to make the batter g. and put it in the oven for 30 to 40 minutes h. so you’ll arrive to the sweet smell of chocolate. i. That sounds good to me. j. That’s all there is to it. 1Though inﬁnitival to is traditionally not considered a preposition, we allow it to be labeled with a supersense if the inﬁnitival clause serves as a PURPOSE (as in (1d)) or FUNCTION. See §2.  Sometimes a preposition speciﬁes a relationship between two entities or quantities, as in (1g). In other scenarios it serves a case-marking sort of function, marking a complement or adjunct—principally to a verb (1b–1e, 1h, 1i), but also to an argument-taking noun or adjective (1f). Further, it is not always possible to separate the semantic contribution of the preposition from that of other words in the sentence. As amply demonstrated in the literature, prepositions play a key role in multiword expressions (Baldwin and Kim, 2010), as in (1a, 1b, 1j). An adequate descriptive annotation scheme for prepositions must deal with these messy facts. Following a brief discussion of existing approaches to preposition semantics (§1), this paper offers a new approach to characterizing their functions at a coarsegrained level. Our scheme is intended to apply to almost all preposition tokens, though some are excluded on the grounds that they belong to a larger multiword expression or are purely syntactic (§2). The rest of the paper is devoted to our coarse semantic categories, supersenses (§3).2 Many of these categories are based on previous proposals—primarily, Srikumar and Roth (2013a) (so-called preposition relations) and VerbNet (thematic roles; Bonial et al., 2011; Hwang, 2014, appendix C)—but we organize them into a hierarchy and motivate a number of new or altered categories that make the scheme more robust. Because prepositions are so frequent, so polysemous, and so crucial in establishing relations, we believe that a wide variety of NLP applications (including knowledge base construction, reasoning about events, summarization, paraphrasing, and translation) stand to beneﬁt from automatic disambiguation of preposition supersenses. 2Supersense inventories have also been described for nouns and verbs (Ciaramita and Altun, 2006; Schneider et al., 2012; Schneider and Smith, 2015) and adjectives (Tsvetkov et al., 2014). Other inventories characterize semantic functions expressed via morphosyntax: e.g., tense/aspect (Reichart and Rappoport, 2010), deﬁniteness (Bhatia et al., 2014, also hierarchical).  112  Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 112–123, Denver, Colorado, June 5, 2015. c 2015 Association for Computational Linguistics  A wiki documenting our scheme in detail can be accessed at http://tiny.cc/prepwiki. It maps ﬁnegrained preposition senses to our supersenses, along with numerous examples. The wiki is conducive to browsing and to exporting the structure and examples for use elsewhere (e.g., in an annotation tool). From our experience with pilot annotations, we believe that the scheme is fairly stable and broadly applicable. 
This paper presents a resource and the associated annotation process used in a project of interlinking Czech and English verbal translational equivalents based on a parallel, richly annotated dependency treebank containing also valency and semantic roles, namely the Prague Czech-English Dependency Treebank. One of the main aims of this project is to create a high-quality and relatively large empirical base which could be used both for linguistic comparative research as well as for natural language processing applications, such as machine translation or cross-language sense disambiguation. This paper describes the resulting lexicon, CzEngVallex, and the process of building it, as well some interesting observations and statistics already obtained. 
We present our correction annotation guidelines to create a manually corrected nonnative (L2) Arabic corpus. We develop our approach by extending an L1 large-scale Arabic corpus and its manual corrections, to include manually corrected non-native Arabic learner essays. Our overarching goal is to use the annotated corpus to develop components for automatic detection and correction of language errors that can be used to help Standard Arabic learners (native and non-native) improve the quality of the Arabic text they produce. The created corpus of L2 text manual corrections is the largest to date. We evaluate our guidelines using inter-annotator agreement and show a high degree of consistency. 
The importance of balancing linguistic considerations, annotation practicalities, and end user needs in developing language annotation guidelines is discussed. Maintaining a clear view of the various goals and fostering collaboration and feedback across levels of annotation and between corpus creators and corpus users is helpful in determining this balance. Annotating non-canonical language brings additional challenges that serve to highlight the necessity of keeping these goals in mind when creating corpora. Introduction Context is important – both the linguistic context of a specific annotation and also the external context of the project as a whole affect what type of annotation scheme can be developed, what kind of annotation can be done, and what the balance of existing and new will need to be in an annotation scheme. Non-canonical language can make the usual linguistic and situational context considerations for annotation even more relevant: how broad the context is (word, sentence, document, conversation, world knowledge), how much that context affects the feature that is being annotated, and whether it is possible for an annotator to take that context into account. In addition, particularly when developing large corpora as part of projects with a  
 majority of the language we process and produce  As researchers developing robust NLP for a wide range of text types, we are often confronted with the prejudice that annotation of non-canonical language (whatever that means) is somehow more arbitrary than an-  through the course of a day is very different from newswire and textbooks, be it spoken language, literature, or social media text. Why, then, is newswire considered more standard or more canonical than other text types? Ob-  notation of canonical language. To investigate this, we present a small annotation study where annotators were asked, with minimal guidelines, to identify main predicates and arguments in sentences across ﬁve different domains, ranging from newswire to Twitter. Our study indicates that (at least such) annotation of non-canonical language is not harder. How-  viously, this may simply be because journalists are trained writers and produce fewer errors. But think, for a minute, about languages in which no newspapers are written. What, then, is canonical language? Can spoken language be canonical? Or is newswire called canonical, because, historically, it is what corpora are made of, and the only data that was avail-  ever, we also observe that agreements in social media domains correlate less with model conﬁdence, suggesting that maybe annotators disagree for different reasons when annotating social media data.  able to the NLP community for a long time? This discussion is more than a ﬁght of words. The use of the word ‘canonical’ alludes to the fact that non-canonical language presents a challenge to the NLP community, but a lot of the reason for NLP  
We examine some non-canonical annotation categories that license missing material (ellipses and enumerations). In extending these categories to learner data, the distinctions seem to require an annotator to determine whether a sentence is grammatical or not when deciding between particular analyses. We unpack the assumptions surrounding the annotation of learner language and how these particular phenomena compare to competing analyses, pointing out the implications for annotation practice and second language analysis. 
The present paper describes an attempt to create an interoperable scheme using existing annotations of textual phenomena across languages and genres including non-canonical ones. Such a kind of analysis requires annotated multilingual resources which are costly. Therefore, we make use of annotations already available in the resources for English, German and Czech. As the annotations in these corpora are based on different conceptual and methodological backgrounds, we need an interoperable scheme that covers existing categories and at the same time allows a comparison of the resources. In this paper, we describe how this interoperable scheme was created and which problematic cases we had to consider. The resulting scheme is supposed to be applied in the future to explore contrasts between the three languages under analysis, for which we expect the greatest differences in the degree of variation between non-canonical and canonical language. 
This paper reports on an eﬀort to develop a linguistically-informed annotation scheme for sluicing (Ross, 1969), ellipsis that leaves behind a wh-phrase. We describe a scheme for annotating the elided content, both in terms of a free text representation and its degree of correspondence with its antecedent. We demonstrate that we can achieve reasonable IAA (α between .78 and .88 across eight annotation types) and describe some of the novel patterns that have arisen from this eﬀort.  
Detecting and analyzing causal language is essential to extracting semantic relationships. To that end, we present an annotation scheme for English causal language (not metaphysical causality), and discuss two methodologies for annotation. The ﬁrst uses only a coding manual to train annotators in distinguishing causal from non-causal language. To address low inter-coder agreement, we adopted a second methodology, in which we ﬁrst created a causal language constructicon based on corpus analysis, then required annotators only to annotate instances based on the constructicon. (This resembles the methodology used for annotating the FrameNet and PropBank corpora.) Our contributions, in addition to the annotation scheme itself, are methodological: we discuss when constructicon-based methodology is appropriate, and address the validity of annotation schemes that require expertlevel metalinguistic awareness. 
Traditional supervised learning approaches to common NLP tasks depend heavily on manual annotation, which is labor intensive and time consuming, and often suffer from data sparseness. In this paper we show how to mitigate the problems in short text classiﬁcation (STC) through word embeddings – distributional representations of words learned from large unlabeled data. The word embeddings are trained from the entire English Wikipedia text. We assume that a short text document is a speciﬁc sample of one distribution in a Bayesian framework. A Gaussian process approach is used to model the distribution of words. The task of classiﬁcation becomes a simple problem of selecting the most probable Gaussian distribution. This approach is compared with those based on the classical maximum entropy (MaxEnt) model and the Latent Dirichlet Allocation (LDA) approach. Our approach achieved better performance and also showed advantages in dealing with unseen words. 
Up to now, relation extraction systems have made extensive use of features generated by linguistic analysis modules. Errors in these features lead to errors of relation detection and classiﬁcation. In this work, we depart from these traditional approaches with complicated feature engineering by introducing a convolutional neural network for relation extraction that automatically learns features from sentences and minimizes the dependence on external toolkits and resources. Our model takes advantages of multiple window sizes for ﬁlters and pre-trained word embeddings as an initializer on a non-static architecture to improve the performance. We emphasize the relation extraction problem with an unbalanced corpus. The experimental results show that our system signiﬁcantly outperforms not only the best baseline systems for relation extraction but also the state-of-the-art systems for relation classiﬁcation. 
We present an ad hoc concept modeling approach using distributional semantic models to identify fine-grained entities and their relations in an online search setting. Concepts are generated from user-defined seed terms, distributional evidence, and a relational model over concept distributions. A dimensional indexing model is used for efficient aggregation of distributional, syntactic, and relational evidence. The proposed semi-supervised model allows concepts to be defined and related at varying levels of granularity and scope. Qualitative evaluations on medical records, intelligence documents, and open domain web data demonstrate the efficacy of our approach. 
Graph-based dependency parsing algorithms commonly employ features up to third order in an attempt to capture richer syntactic relations. However, each level and each feature combination must be deﬁned manually. Besides that, input features are usually represented as huge, sparse binary vectors, offering limited generalization. In this work, we present a deep architecture for dependency parsing based on a convolutional neural network. It can examine the whole sentence structure before scoring each head/modiﬁer candidate pair, and uses dense embeddings as input. Our model is still under ongoing work, achieving 91.6% unlabeled attachment score in the Penn Treebank. 
Short text clustering has become an increasing important task with the popularity of social media, and it is a challenging problem due to its sparseness of text representation. In this paper, we propose a Short Text Clustering via Convolutional neural networks (abbr. to STCC), which is more beneﬁcial for clustering by considering one constraint on learned features through a self-taught learning framework without using any external tags/labels. First, we embed the original keyword features into compact binary codes with a localitypreserving constraint. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, with the output units ﬁtting the pre-trained binary code in the training process. After obtaining the learned representations, we use K-means to cluster them. Our extensive experimental study on two public short text datasets shows that the deep feature representation learned by our approach can achieve a signiﬁcantly better performance than some other existing features, such as term frequency-inverse document frequency, Laplacian eigenvectors and average embedding, for clustering. 
We present a method for the detection and representation of polysemous nouns, a phenomenon that has received little attention in NLP. The method is based on the exploitation of the semantic information preserved in Word Embeddings. We ﬁrst prove that polysemous nouns instantiating a particular sense alternation form a separate class when clustering nouns in a lexicon. Such a class, however, does not include those polysemes in which a sense is strongly predominant. We address this problem and present a sense index that, for a given pair of lexical classes, deﬁnes the degree of membership of a noun to each class: polysemy is hence implicitly represented as an intermediate value on the continuum between two classes. We ﬁnally show that by exploiting the information provided by the sense index it is possible to accurately detect polysemous nouns in the dataset. 
We tackle the question: how much supervision is needed to achieve state-of-the-art performance in part-of-speech (POS) tagging, if we leverage lexical representations given by the model of Brown et al. (1992)? It has become a standard practice to use automatically induced “Brown clusters” in place of POS tags. We claim that the underlying sequence model for these clusters is particularly well-suited for capturing POS tags. We empirically demonstrate this claim by drastically reducing supervision in POS tagging with these representations. Using either the bit-string form given by the algorithm of Brown et al. (1992) or the (less well-known) embedding form given by the canonical correlation analysis algorithm of Stratos et al. (2014), we can obtain 93% tagging accuracy with just 400 labeled words and achieve state-of-the-art accuracy (> 97%) with less than 1 percent of the original training data. 
We propose a novel approach to learning distributed representations of variable-length text sequences in multiple languages simultaneously. Unlike previous work which often derive representations of multi-word sequences as weighted sums of individual word vectors, our model learns distributed representations for phrases and sentences as a whole. Our work is similar in spirit to the recent paragraph vector approach but extends to the bilingual context so as to efﬁciently encode meaning-equivalent text sequences of multiple languages in the same semantic space. Our learned embeddings achieve state-of-theart performance in the often used crosslingual document classiﬁcation task (CLDC) with an accuracy of 92.7 for English to German and 91.5 for German to English. By learning text sequence representations as a whole, our model performs equally well in both classiﬁcation directions in the CLDC task in which past work did not achieve. 
Recent interest in distributed vector representations for words has resulted in an increased diversity of approaches, each with strengths and weaknesses. We demonstrate how diverse vector representations may be inexpensively composed into hybrid representations, effectively leveraging strengths of individual components, as evidenced by substantial improvements on a standard word analogy task. We further compare these results over different sizes of training sets and ﬁnd these advantages are more pronounced when training data is limited. Finally, we explore the relative impacts of the differences in the learning methods themselves and the size of the contexts they access. 
We present a simple method to learn continuous representations of dependency substructures (links), with the motivation of directly working with higher-order, structured embeddings and their hidden relationships, and also to avoid the millions of sparse, template-based word-cluster features in dependency parsing. These link embeddings allow a signiﬁcantly smaller and simpler set of unary features for dependency parsing, while maintaining improvements similar to state-of-the-art, n-ary word-cluster features, and also stacking over them. Moreover, these link vectors (made publicly available) are directly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydeﬁned features, and also stacks over them. 
We present the architecture of a deep learning pipeline for natural language processing. Based on this architecture we built a set of tools both for creating distributional vector representations and for performing specific NLP tasks. Three methods are available for creating embeddings: feedforward neural network, sentiment specific embeddings and embeddings based on counts and Hellinger PCA. Two methods are provided for training a network to perform sequence tagging, a window approach and a convolutional approach. The window approach is used for implementing a POS tagger and a NER tagger, the convolutional network is used for Semantic Role Labeling. The library is implemented in Python with core numerical processing written in C++ using parallel linear algebra library for efficiency and scalability. 
Vector representations for language has been shown to be useful in a number of Natural Language Processing tasks. In this paper, we aim to investigate the effectiveness of word vector representations for the problem of Aspect Based Sentiment Analysis. In particular, we target three sub-tasks namely aspect term extraction, aspect category detection, and aspect sentiment prediction. We investigate the effectiveness of vector representations over different text data and evaluate the quality of domain-dependent vectors. We utilize vector representations to compute various vectorbased features and conduct extensive experiments to demonstrate their effectiveness. Using simple vector based features, we achieve F1 scores of 79.91% for aspect term extraction, 86.75% for category detection, and the accuracy 72.39% for aspect sentiment prediction. 
We explore new methods of improving Curriculum Vitæ (CV) parsing for German documents by applying recent research on the application of word embeddings in Natural Language Processing (NLP). Our approach integrates the word embeddings as input features for a probabilistic sequence labeling model that relies on the Conditional Random Field (CRF) framework. Best-performing word embeddings are generated from a large sample of German CVs. The best results on the extraction task are obtained by the model which integrates the word embeddings together with a number of hand-crafted features. The improvements are consistent throughout different sections of the target documents. The effect of the word embeddings is strongest on semi-structured, out-of-sample data. 
 We replicate the syntactic experiments of Mikolov et al. (2013b) on English, and expand them to include morphologically complex languages. We learn vector representations for Dutch, French, German, and Spanish with the WORD2VEC tool, and investigate to what extent inﬂectional information is preserved across vectors. We observe that the accuracy of vectors on a set of syntactic analogies is inversely correlated with the morphological complexity of the language. 
Matrix factorization of knowledge bases in universal schema has facilitated accurate distantlysupervised relation extraction. This factorization encodes dependencies between textual patterns and structured relations using lowdimensional vectors deﬁned for each entity pair; although these factors are effective at combining evidence for an entity pair, they are inaccurate on rare pairs, or for relations that depend crucially on the entity types. On the other hand, tensor factorization is able to overcome these shortcomings when applied to link prediction by maintaining entity-wise factors. However these models have been unsuitable for universal schema. In this paper we ﬁrst present an illustration on synthetic data that explains the unsuitability of tensor factorization to relation extraction with universal schemas. Since the beneﬁts of tensor and matrix factorization are complementary, we then investigate two hybrid methods that combine the beneﬁts of the two paradigms. We show that the combination can be fruitful: we handle ambiguously phrased relations, achieve gains in accuracy on real-world relations, and demonstrate that entity embeddings encode entity types. 
Categorical compositional distributional models unify compositional formal semantic models and distributional models by composing phrases with tensor-based methods from vector representations. For the tensor-based compositions, Milajevs et al. (2014) showed that word vectors obtained from the continuous bag-of-words (CBOW) model are competitive with those from co-occurrence based models. However, because word vectors from the CBOW model are trained assuming additive interactions between context words, the word composition used for the training mismatches to the tensor-based methods used for evaluating the actual compositions including pointwise multiplication and tensor product of context vectors. In this work, we show whether the word embeddings from extended CBOW models using multiplication or tensor product between context words, reﬂecting the actual composition methods, can show better performance than those from the baseline CBOW model in actual tasks of compositions with multiplication or tensor-based methods. 
Recent work in learning bilingual representations tend to tailor towards achieving good performance on bilingual tasks, most often the crosslingual document classiﬁcation (CLDC) evaluation, but to the detriment of preserving clustering structures of word representations monolingually. In this work, we propose a joint model to learn word representations from scratch that utilizes both the context coocurrence information through the monolingual component and the meaning equivalent signals from the bilingual constraint. Speciﬁcally, we extend the recently popular skipgram model to learn high quality bilingual representations efﬁciently. Our learned embeddings achieve a new state-of-the-art accuracy of 80.3 for the German to English CLDC task and a highly competitive performance of 90.7 for the other classiﬁcation direction. At the same time, our models outperform best embeddings from past bilingual representation work by a large margin in the monolingual word similarity evaluation.1 
This paper presents a case study of using distributed word representations, word2vec in particular, for improving performance of Named Entity Recognition for the eCommerce domain. We also demonstrate that distributed word representations trained on a smaller amount of in-domain data are more effective than word vectors trained on very large amount of out-of-domain data, and that their combination gives the best results. 
Word embeddings have recently proven useful in a number of different applications that deal with natural language. Such embeddings succinctly reﬂect semantic similarities between words based on their sentence-internal contexts in large corpora. In this paper, we show that information extraction techniques provide valuable additional evidence of semantic relationships that can be exploited when producing word embeddings. We propose a joint model to train word embeddings both on regular context information and on more explicit semantic extractions. The word vectors obtained from such an augmented joint training show improved results on word similarity tasks, suggesting that they can be useful in applications that involve word meanings. 
The majority of research on Arabic Named Entity Recognition (NER) addresses the the task for newswire genre, where the language used is Modern Standard Arabic (MSA), however, the need to study this task in social media is becoming more vital. Social media is characterized by the use of both MSA and Dialectal Arabic (DA), with often code switching between the two language varieties. Despite some common characteristics between MSA and DA, there are signiﬁcant differences between which result in poor performance when MSA targeting systems are applied for NER in DA. Additionally, most NER systems rely primarily on gazetteers, which can be more challenging in a social media processing context due to an inherent low coverage. In this paper, we present a gazetteers-free NER system for Dialectal data that yields an F1 score of 72.68% which is an absolute improvement of ≈ 2 − 3% over a comparable state-ofthe-art gazetteer based DA-NER system. 
In this paper we compare the performance of three approaches for estimating the latent weights of terms for scientiﬁc document summarization, given the document and a set of citing documents. The ﬁrst approach is a termfrequency (TF) vector space method utilizing a nonnegative matrix factorization (NNMF) for dimensionality reduction. The other two are language modeling approaches for predicting the term distributions of human-generated summaries. The language model we build exploits the key sections of the document and a set of citing sentences derived from auxiliary documents that cite the document of interest. The parameters of the model may be set via a minimization of the Jensen-Shannon (JS) divergence. We use the OCCAMS algorithm (Optimal Combinatorial Covering Algorithm for Multi-document Summarization) to select a set of sentences that maximizes the term-coverage score while minimizing redundancy. The results are evaluated with standard ROUGE metrics, and the performance of the resulting methods achieve ROUGE scores exceeding those of the average human summarizer. 
Only very few users disclose their physical locations, which may be valuable and useful in applications such as marketing and security monitoring; in order to automatically detect their locations, many approaches have been proposed using various types of information, including the tweets posted by the users. It is not easy to infer the original locations from textual data, because text tends to be noisy, particularly in social media. Recently, deep learning techniques have been shown to reduce the error rate of many machine learning tasks, due to their ability to learn meaningful representations of input data. We investigate the potential of building a deep-learning architecture to infer the location of Twitter users based merely on their tweets. We ﬁnd that stacked denoising auto-encoders are well suited for this task, with results comparable to state-of-the-art models. 
Accurate metaphor detection remains an open challenge. In this paper, we explore a new type of clue for disambiguating terms that may be used metaphorically or literally in an online medical support community. In particular, we investigate the inﬂuence of situational factors on propensity to employ the metaphorical sense of words when they can be used to illustrate the emotion behind the experience of the event. Speciﬁcally we consider the experience of stressful illness-related events in a poster’s recent history as situational factors. We evaluate the positive impact of automatically extracted cancer events on a metaphor detection task using data from an online cancer forum. We also provide a discussion of speciﬁc associations between events and metaphors, such as journey with diagnosis or warrior with chemotherapy. 
We present a supervised machine learning system for word-level classiﬁcation of all content words in a running text as being metaphorical or non-metaphorical. The system provides a substantial improvement upon a previously published baseline, using re-weighting of the training examples and using features derived from a concreteness database. We observe that while the ﬁrst manipulation was very effective, the second was only slightly so. Possible reasons for these observations are discussed.  Data News Fiction Academic Conversation Essay Set A Essay Set B  #Texts 49 11 12 18 85 79  content tokens 18,519 17,836 29,469 15,667 21,838 22,662  % metaphors 18% 14% 13% 7% 11% 12 %  Table 1: The sizes of the datasets used in this study, and the proportion of metaphors. Content tokens are nouns, adjectives, adverbs, and verbs.  
Concreteness and imageability have long been held to play an important role in the meanings of ﬁgurative expressions. Recent work has implemented this idea in order to detect metaphors in natural language discourse. Yet, a relatively unexplored dimension of metaphor is the role of affective meanings. In this paper, we will show how combining concreteness, imageability and sentiment scores, as features at different linguistic levels, improves performance in such tasks as automatic detection of metaphor in discourse. By gradually reﬁning these features through descriptive studies, we found the best performing classiﬁer for our task to be random forests. Further reﬁning of our classiﬁers for part-ofspeech, led to very promising results, with F 1 scores of .744 for nouns,.799 for verbs, .811 for prepositions. We suggest that our approach works by capturing to some degree the complex interactions between external sensory information (concreteness), information about internal experience (imageability), and relatively subjective meanings (sentiment), in the use of metaphorical expressions in natural language. 
Language is the main communication device to represent the environment and share a common understanding of the world that we perceive through our sensory organs. Therefore, each language might contain a great amount of sensorial elements to express the perceptions both in literal and ﬁgurative usage. To tackle the semantics of ﬁgurative language, several conceptual properties such as concreteness or imegeability are utilized. However, there is no attempt in the literature to analyze and beneﬁt from the sensorial elements for ﬁgurative language processing. In this paper, we investigate the impact of sensorial features on metaphor identiﬁcation. We utilize an existing lexicon associating English words to sensorial modalities and propose a novel technique to automatically discover these associations from a dependency-parsed corpus. In our experiments, we measure the contribution of the sensorial features to the metaphor identiﬁcation task with respect to a state of the art model. The results demonstrate that sensorial features yield better performance and show good generalization properties. 
This paper describes a system that makes use of a repository of formalized frames and metaphors to automatically detect, categorize, and analyze expressions of metaphor in corpora. The output of this system can be used as a basis for making further refinements to the system, as well as supporting deep semantic analysis of metaphor expressions in corpora. This in turn provides a way to ground and test empirical conceptual metaphor theory, as well as serving as a means to gain insights into the ways conceptual metaphors are expressed in language. 
Metaphor is a cognitive phenomenon exhibited in language, where one conceptual domain (the target) is thought of in terms of another (the source). The ﬁrst level of metaphor interpretation is the mapping of linguistic metaphors to pairs of source and target concepts. Based on the abductive approach to metaphor interpretation proposed by Hobbs (1992) and implemented in the open-source Metaphor-ADP system (Ovchinnikova et al., 2014), we present work to automatically learn knowledge bases to support high-precision conceptual metaphor mapping in English, Spanish, Farsi, and Russian. 
In this article, we outline a novel approach to the automated analysis of cross-cultural conflicts through the discovery and classification of the metaphors used by the protagonist parties involved in the conflict. We demonstrate the feasibility of this approach on a prototypical conflict surrounding the appropriate management and oversight of gun-ownership in the United States. In addition, we present a way of incorporating sociolinguistic measures of influence in discourse to draw further insights from complex data. The results presented in this article should be considered as illustrative of the types of analyses that can be obtained using our methodology; however, no attempt was made to rigorously validate the specific findings reported here. We address open issues such as how our approach could be generalized to analyze cross-cultural conflicts around the world. 
Metaphor processing has been a heated topic in NLP. Cognitive properties of a word are important in metaphor understanding and generation. But data collected automatically tend to be reduced in both quantity and quality. This paper introduces CogBank a database of Chinese concepts and their associated cognitive properties. The database was constructed using simile templates to extract millions of “word-property” pairs via search engine over the World Wide Web. A method of manual check and correction was then implemented, resulting in the current CogBank database which contains 232,590 “word-property” pairs. CogBank also provides various search and visualization services for observing and comparing associations between concepts and properties. 
Metaphor is a fundamentally antagonistic way of viewing and describing the world. Metaphors ask us to see what is not there, so as to remake the world to our own liking and to suit our own lexicons. But if metaphors clash with the world as it is, they can also clash with each other. Each metaphor represents a stance from which to view a topic, and though some stances are mutually compatible, many more are naturally opposed to each other. So while we cringe at a clumsily mixed metaphor, there is real value to be had from a deliberate opposition of conceptual metaphors. Such contrasts reveal the limits of a particular worldview, and allow us to extract humorous insight from each opposition. We present here an automatic approach to the framing of antagonistic metaphors, embodied in a metaphor-generating Twitterbot named @MetaphorMagnet. 
Statistical Machine Translation has come a long way improving the translation quality of a range of different linguistic phenomena. With negation however, techniques proposed and implemented for improving translation performance on negation have simply followed from the developers’ beliefs about why performance is worse. These beliefs, however, have never been validated by an error analysis of the translation output. In contrast, the current paper shows that an informative empirical error analysis can be formulated in terms of (1) the set of semantic elements involved in the meaning of negation, and (2) a small set of string-based operations that can characterise errors in the translation of those elements. Results on a Chinese-to-English translation task conﬁrm the robustness of our analysis cross-linguistically and the basic assumptions can inform an automated investigation into the causes of translation errors. Conclusions drawn from this analysis should guide future work on improving the translation of negative sentences. 
In this paper, we present a corpus study investigating the use of the ﬁllers a¨h (uh) and a¨hm (uhm) in informal spoken German youth language and in written text from social media. Our study shows that ﬁlled pauses occur in both corpora as markers of hesitations, corrections, repetitions and unﬁnished sentences, and that the form as well as the type of the ﬁllers are distributed similarly in both registers. We present an analysis of ﬁllers in written microblogs, illustrating that a¨h and a¨hm are used intentionally and can add a subtext to the message that is understandable to both author and reader. We thus argue that ﬁlled pauses in user-generated content from social media are words with extrapropositional meaning.  Barr, 2001; Clark and Fox Tree, 2002), or as strategic devices, e.g. as ﬂoor-holders or turn-taking signals (Maclay and Osgood, 1959; Rochester, 1973; Beattie, 1983). Filled pauses can function as discourse-structuring devices, but they can also express extra-propositional aspects of meaning beyond the propositional content of the utterance, e.g. as markers of uncertainty or politeness (Fischer, 2000; Barr, 2001; Arnold et al., 2003). Examples (1)-(6) illustrate the use of FP to mark repetitions (1), repairs (2), breaks (3) and hesitations (4) (the last one often used to bridge word ﬁnding problems). FPs can also express astonishment (5), excitement or negative sentiment (6). Extralinguistic reasons also come into play, such as the lack of concentration due to fatigue or distraction, which might lead to a higher ratio of FP in the discourse.  
We propose a compositional method to assess the factuality of biomedical events extracted from the literature. The composition procedure relies on the notion of semantic embedding and a ﬁne-grained classiﬁcation of extrapropositional phenomena, including modality and valence shifting, and a dictionary based on this classiﬁcation. The event factuality is computed as a product of the extra-propositional operators that have scope over the event. We evaluate our approach on the GENIA event corpus enriched with certainty level and polarity annotations. The results indicate that our approach is effective in identifying the certainty level component of factuality and is less successful in recognizing the other element, negative polarity. 
Level of committed belief is a modality in natural language, it expresses a speak-er/writers belief in a proposition. Initial work exploring this phenomenon in the literature both from a linguistic and computational modeling perspective shows that it is a challenging phenomenon to capture, yet of great interest to several downstream NLP applications. In this work, we focus on identifying relevant features to the task of determining the level of committed belief tagging in two corpora speciﬁcally annotated for the phenomenon: the LU corpus and the FactBank corpus. We perform a thorough analysis comparing tagging schemes, infrastructure machinery, feature sets, preprocessing schemes and data genres and their impact on performance in both corpora. Our best results are an F1 score of 75.7 on the FactBank corpus and 72.9 on the smaller LU corpus. 
NegEx is a popular rule-based system used to identify negated concepts in clinical notes. This system has been reported to perform very well by numerous studies in the past. In this paper, we demonstrate the use of kernel methods to extend the performance of NegEx. A kernel leveraging the rules of NegEx and its output as features, performs as well as the rule-based system. An improvement in performance is achieved if this kernel is coupled with a bag of words kernel. Our experiments show that kernel methods outperform the rule-based system, when evaluated within and across two different open datasets. We also present the results of a semi-supervised approach to the problem, which improves performance on the data.  showed no signs of malignancy” has the concept ‘malignancy’ which was looked for in the patient, but was not observed to be present. The task of negation detection is to identify whether a given concept is negated or afﬁrmed in a sentence. NegEx (Chapman et al., 2001) is a rule-based system developed to detect negated concepts in the clinical domain and has been extensively used in the literature. In this paper, we show that a kernel-based approach can map this rule-based system into a machine learning system and extends its performance. We validate the generalization capabilities of our approach by evaluating it across datasets. Finally, we demonstrate that a semi-supervised approach can also achieve an improvement over the baseline rulebased system, a valuable ﬁnding in the clinical domain where annotated data is expensive to generate.  
Many signiﬁcant challenges exist for the mental health ﬁeld, but one in particular is a lack of data available to guide research. Language provides a natural lens for studying mental health – much existing work and therapy have strong linguistic components, so the creation of a large, varied, language-centric dataset could provide signiﬁcant grist for the ﬁeld of mental health research. We examine a broad range of mental health conditions in Twitter data by identifying self-reported statements of diagnosis. We systematically explore language differences between ten conditions with respect to the general population, and to each other. Our aim is to provide guidance and a roadmap for where deeper exploration is likely to be fruitful. 
Analyzing symptoms of schizophrenia has traditionally been challenging given the low prevalence of the condition, affecting around 1% of the U.S. population. We explore potential linguistic markers of schizophrenia using the tweets1 of self-identiﬁed schizophrenia sufferers, and describe several natural language processing (NLP) methods to analyze the language of schizophrenia. We examine how these signals compare with the widelyused LIWC categories for understanding mental health (Pennebaker et al., 2007), and provide preliminary evidence of additional linguistic signals that may aid in identifying and getting help to people suffering from schizophrenia. 
Mental illnesses, such as depression and post traumatic stress disorder (PTSD), are highly underdiagnosed globally. Populations sharing similar demographics and personality traits are known to be more at risk than others. In this study, we characterise the language use of users disclosing their mental illness on Twitter. Language-derived personality and demographic estimates show surprisingly strong performance in distinguishing users that tweet a diagnosis of depression or PTSD from random controls, reaching an area under the receiveroperating characteristic curve – AUC – of around .8 in all our binary classiﬁcation tasks. In fact, when distinguishing users disclosing depression from those disclosing PTSD, the single feature of estimated age shows nearly as strong performance (AUC = .806) as using thousands of topics (AUC = .819) or tens of thousands of n-grams (AUC = .812). We also ﬁnd that differential language analyses, controlled for demographics, recover many symptoms associated with the mental illnesses in the clinical literature. 
Alzheimer’s Disease, as other mental and neurological disorders, is difficult to diagnose since it affects several cognitive abilities shared with other impairments. Current diagnostic mainly consists of neuropsychological tests and history obtained from the patient and relatives. In this paper we propose a methodology for the characterization of probable AD based on the computational cognitive modeling of a language function in order to capture the internal mechanisms of the impaired brain. Parameters extracted from the model allow a better characterization of this illness than using only behavioral data. 
Motivational Interviewing (MI) is an efﬁcacious treatment for substance use disorders and other problem behaviors (Lundahl and Burke, 2009). However, little is known about the speciﬁc mechanisms that drive therapeutic change. A growing body of research has focused on coding within-session language to better understand how therapist and patient language mutually inﬂuence each other and predict successful (or unsuccessful) treatment outcomes. These studies typically use human raters, requiring considerable ﬁnancial, time, and training costs for conducting such research. This paper describes the development and testing of a recursive neural network (RNN) model for rating 78,977 therapist and patient talk turns across 356 MI sessions. We assessed the accuracy of RNNs in predicting human ratings for client speech and compared them to standard n-gram models. The RNN model showed improvement over ngram models for some codes, but overall, all of the models performed well below human reliability, demonstrating the difﬁculty of the task. 
The use of language to convey emotional experience is of signiﬁcant importance to the process of psychotherapy, the diagnosis of problems with emotion and memory, and more generally to any communication that aims to evoke a feeling in the recipient. Bucci’s theory of the referential process (1997) concerns three phases whereby a person activates emotional, or bodily experience (Arousal Phase), conveys the images and events associated with it (Symbolizing Phase), and reconsiders them (Reorganizing Phase). The Symbolizing Phase is the major focus of this study and is operationalized by a measure called referential activity (RA) based on judges’ ratings of the concreteness, speciﬁcity, clarity and imagery of language style. Computational models of RA have previously been created in several languages, however, due to the complexity of modeling RA, different modeling strategies have been employed for each language and common features that predict RA across languages are not well understood. Working from previous computational models developed in English and Italian, this study speciﬁes a new model of predictors common to both languages that correlates between r = .36 and.45 with RA. The components identiﬁed support the construct validity of the referential process and may facilitate the development of measures in other languages.  
Major depressive disorder is one of the most burdensome and debilitating diseases in the United States. In this pilot study, we present a new annotation scheme representing depressive symptoms and psycho-social stressors associated with major depressive disorder and report annotator agreement when applying the scheme to Twitter data. 
Topic models can yield insight into how depressed and non-depressed individuals use language differently. In this paper, we explore the use of supervised topic models in the analysis of linguistic signal for detecting depression, providing promising results using several models. 
Quantitative analysis of clinical language samples is a powerful tool for assessing and screening developmental language impairments, but requires extensive manual transcription, annotation, and calculation, resulting in error-prone results and clinical underutilization. We describe a system that performs automated morphological analysis needed to calculate statistics such as the mean length of utterance in morphemes (MLUM), so that these statistics can be computed directly from orthographic transcripts. Estimates of MLUM computed by this system are closely comparable to those produced by manual annotation. Our system can be used in conjunction with other automated annotation techniques, such as maze detection. This work represents an important first step towards increased automation of language sample analysis, and towards attendant benefits of automation, including clinical greater utilization and reduced variability in care delivery. 
Restrictive and repetitive behavior (RRB) is a core symptom of autism spectrum disorder (ASD) and are manifest in language. Based on this, we expect children with autism to talk about fewer topics, and more repeatedly, during their conversations. We thus hypothesize a higher semantic overlap ratio between dialogue turns in children with ASD compared to those with typical development (TD). Participants of this study include children ages 48, 44 with TD and 25 with ASD without language impairment. We apply several semantic similarity metrics to the children’s dialogue turns in semi-structured conversations with examiners. We ﬁnd that children with ASD have signiﬁcantly more semantically overlapping turns than children with TD, across different turn intervals. These results support our hypothesis, and could provide a convenient and robust ASD-speciﬁc behavioral marker. 
Background: Verbal fluency tasks, which require producing as many words in response to a cue in a fixed time, are widely used within clinical neuropsychology and in neuropsychological research. Although semantic word lists can be elicited, typically only the number of words related to the cue is interpreted thus ignoring any structure in the word sequences. Automated language techniques can provide a much needed framework for extracting and charting useful semantic relations in healthy individuals and understanding how cortical disorders disrupt these knowledge structures and the retrieval of information from them. Methods: One minute, animal category verbal fluency tests from 150 participants consisting of healthy individuals, patients with schizophrenia, and patients with bipolar disorder were transcribed. We discuss the issues involved in building and evaluating semantic frameworks and developing robust features to analyze this data. Specifically we investigate a Latent Semantic Analysis (LSA) semantic space to obtain semantic features, such as pairwise semantic similarity and clusters. Results and Discussion: An in-depth analysis of the framework is presented, and then results from two measures based on LSA semantic similarity illustrate how these automated techniques provide additional, clinically useful information beyond word list cardinality.  
Describing troubling events and images and reﬂecting on their emotional meanings are central components of most psychotherapies. The computer system described here tracks the occurrence and intensity of narration or imagery within transcribed therapy sessions and over the course of treatments; it likewise tracks the extent to which language denoting appraisal and logical thought occurs. The Discourse Attributes Analysis Program (DAAP) is a computer text analysis system that uses several dictionaries, including the Weighted Referential Activity Dictionary (WRAD), designed to detect verbal communication of emotional images and events, and the Reﬂection Dictionary (REF), designed to detect verbal communication denoting cognitive appraisal, as well as other dictionaries. For each dictionary and each turn of speech, DAAP uses a moving weighted average of dictionary weights, together with a fold-over procedure, to produce a smooth density function that graphically illustrates the rise and fall of each underlying psychological variable. These density functions are then used to produce several new measures, including measures of the vividness of descriptions of images or events, and a measure of the extent to which descriptions of events or images and reﬂection on their meaning occur separately. 
Syntactic priming from comprehension to production has been shown to be robust: we are more likely to repeat structures that we have previously heard. Many current models do not distinguish between comprehension and production. Here we contrast human language processing with two variants of a Bayesian belief updating model. In the ﬁrst model, production-to-production priming (i.e. selfpriming) is as strong as comprehension-toproduction priming. In the second, both individuals who self-prime and those who do not are exposed to a syntactic construction via comprehension. Our results suggest that when production-to-production priming is as robust as comprehension-to-production priming, then speakers who self-prime are simultaneously less likely to be primed by input from comprehension and demonstrate different distributions of responses than speakers who do not self-prime. The computational model accords with recent results demonstrating no self-priming, and provides evidence for an account of syntactic priming that distinguishes between production and comprehension input.  2006; Reitter et al., 2011). This means that syntactic priming can occur without another person being present, from production to production (selfpriming). Unfortunately, little experimental research has assessed the degree to which priming occurs from production to production in a controlled context. The primary phenomena that are of interest to research on syntactic priming are as follows: (1) Do we incrementally and cumulatively adapt to our linguistic environment, changing how we talk based on what we hear (Jaeger & Snider, 2013; Chang et al., 2006; Kaschak et al., 2012; Reitter et al., 2011; Pickering & Garrod, 2013)? (2) Do we change how we talk more when we encounter less probable structures (Jaeger & Snider, 2013; Chang et al., 2006)? (3) How long-lasting is syntactic priming (Kaschak, 2007; Bock, 1986)? And ﬁnally: (4) Are we more likely to repeat the structures that we ourselves have said, as predicted by models that unify priming in comprehension and production (Pickering & Garrod, 2013; Chang et al., 2006)? The model presented here accounts for these phenomena and makes additional predictions about the potential ramiﬁcations of self-priming on linguistic representations and efﬁcient communication.  
Linguistic alignment, such as lexical and syntactic alignment, is a universal phenomenon inﬂuencing dialogue participants in online conversations. While adaptation can occur at lexical, syntactic and pragmatic levels, relationships between alignments at multiple levels are neither theoretically nor empirically well understood. In this study, we ﬁnd that community members show pragmatic alignment on social support type, distinguishing emotional and informational support, both of which provide beneﬁts to members. We also ﬁnd that lexical alignment is correlated with emotional support. This ﬁnding can contribute to our understanding of the linguistic signature of different types of support as well as the theory of Interactive Alignment in dialogue. 
The “uniform information density” (UID) hypothesis proposes that language producers aim for a constant rate of information ﬂow within a message, and research on monologue-like written texts has found evidence for UID in production. We consider conversational messages, using a large corpus of tweets, and look for UID behavior. We do not ﬁnd evidence of UID behavior, and even ﬁnd context effects that are opposite that of previous, monologue-based research. We propose that a more collaborative conception of information density and careful consideration of channel noise may be needed in the informationtheoretic framework for conversation. 
In conversation, speakers tend to echo the linguistic style of the person they are interacting with. This paper contributes to a body of work that addresses how this linguistic style coordination is affected by the social context in which the interaction occurs. In particular, we investigate the effect that an agent’s social network centrality has on the coordination exhibited in replies to their utterances. We ﬁnd that linguistic coordination is positively correlated with social network centrality and that this effect is greater than previous results showing a similar connection between statusbased power and linguistic coordination. We conjecture that the social value of coordination may reside in the wish to conform to the linguistic norms of a community. 
Distributional Semantic Models (DSMs) have been successful at modeling the meaning of individual words, with interest recently shifting to compositional structures, i.e., phrases and sentences. Network-based DSMs represent and handle semantics via operators applied on word neighborhoods, i.e., semantic graphs containing a target’s most similar words. We extend network-based DSMs to address compositionality using an activation model (motivated by psycholinguistics) that operates on the fused neighborhoods of variable size activation. The proposed method is evaluated against and combined with the lexical function method proposed by (Baroni and Zamparelli, 2010). We show that, by fusing a network-based with a lexical function model, performance gains can be achieved. 
While several data sets for evaluating thematic ﬁt of verb-role-ﬁller triples exist, they do not control for verb polysemy. Thus, it is unclear how verb polysemy affects human ratings of thematic ﬁt and how best to model that. We present a new dataset of human ratings on high vs. low-polysemy verbs matched for verb frequency, together with high vs. low-frequency and well-ﬁtting vs. poorly-ﬁtting patient roleﬁllers. Our analyses show that low-polysemy verbs produce stronger thematic ﬁt judgements than verbs with higher polysemy. Roleﬁller frequency, on the other hand, had little effect on ratings. We show that these results can best be modeled in a vector space using a clustering technique to create multiple prototype vectors representing different “senses” of the verb. 
Linguistic alignment has emerged as an important property of conversational language and a driver of mutual understanding in dialogue. While various computational measures of linguistic alignment in corpus and experimental data have been devised, a systematic evaluation of them is missing. In this study, we ﬁrst evaluate the sensitivity and distributional properties of three measures, indiscriminate local linguistic alignment (LLA), Spearman’s correlation coefﬁcient (SCC), and repetition decay (RepDecay). Then we apply them in a study of interactive alignment and individual differences to see how well they conform to the Interactive Alignment Model (IAM), and how well they can reveal the individual differences in alignment propensity. Our results suggest that LLA has the overall best performance. 
Models of language acquisition are typically evaluated against a “gold standard” meant to represent adult linguistic knowledge, such as orthographic words for the task of speech segmentation. Yet adult knowledge is rarely the target knowledge for the stage of acquisition being modeled, making the gold standard an imperfect evaluation metric. To supplement the gold standard evaluation metric, we propose an alternative utility-based metric that measures whether the acquired knowledge facilitates future learning. We take the task of speech segmentation as a case study, assessing previously proposed models of segmentation on their ability to generate output that (i) enables creation of language-speciﬁc segmentation cues that rely on stress patterns, and (ii) assists the subsequent acquisition task of learning word meanings. We ﬁnd that behavior that maximizes gold standard performance does not necessarily maximize the utility of the acquired knowledge, highlighting the beneﬁt of multiple evaluation metrics. 
While reading times are often used to measure working memory load, frequency effects (such as surprisal or n-gram frequencies) also have strong confounding effects on reading times. This work uses a naturalistic audio corpus with magnetoencephalographic (MEG) annotations to measure working memory load during sentence processing. Alpha oscillations in posterior regions of the brain have been found to correlate with working memory load in non-linguistic tasks (Jensen et al., 2002), and the present study extends these ﬁndings to working memory load caused by syntactic center embeddings. Moreover, this work ﬁnds that frequency effects in naturally-occurring stimuli do not signiﬁcantly contribute to neural oscillations in any frequency band, which suggests that many modeling claims could be tested on this sort of data even without controlling for frequency effects. 
Neuroimaging while participants listen to audiobooks provides a rich data source for theories of incremental parsing. We compare nested regression models of these data. These mixed-effects models incorporate linguistic predictors at various grain sizes ranging from part-of-speech bigrams, through surprisal on context-free treebank grammars, to incremental node counts in trees that are derived by Minimalist Grammars. The ﬁne-grained structures make an independent contribution over and above coarser predictors. However, this result only obtains with time courses from anterior temporal lobe (aTL). In analogous time courses from inferior frontal gyrus, only n-grams improve upon a non-syntactic baseline. These results support the idea that aTL does combinatoric processing during naturalistic story comprehension, processing that bears a systematic relationship to linguistic structure. 
Accurate identiﬁcation of phrasal translation equivalents is critical to both phrase-based and syntax-based machine translation systems. We show that the extraction of many phrasal translation equivalents is made impossible by word alignments done without taking syntactic structures into consideration. To address the problem, we propose a new annotation scheme where word alignment and the alignment of non-terminal nodes (i.e., phrases) are done simultaneously to avoid conﬂicts between word alignments and syntactic structures. Relying on this new alignment approach, we construct a Hierarchically Aligned Chinese-English Parallel Treebank (HACEPT), and show that all phrasal translation equivalents can be automatically extracted based on the phrase alignments in HACEPT. 
The quality of statistical machine translation performed with phrase based approaches can be increased by permuting the words in the source sentences in an order which resembles that of the target language. We propose a class of recurrent neural models which exploit sourceside dependency syntax features to reorder the words into a target-like order. We evaluate these models on the Germanto-English language pair, showing significant improvements over a phrase-based Moses baseline, obtaining a quality similar or superior to that of hand-coded syntactical reordering rules. 
Statistical Machine Translation systems show considerably worse performance in translating negative sentences than positive ones (Fancellu and Webber, 2014; Wetzel and Bond, 2012). Various techniques have addressed the problem of translating negation, but their underlying assumptions have never been validated by a proper error analysis. A related paper (Fancellu and Webber, 2015) reports on a manual error analysis of the kinds of errors involved in translating negation. The present paper presents ongoing work to discover their causes by considering which, if any, are induction, search or model errors. We show that standard oracle decoding techniques provide little help due to the locality of negation scope and their reliance on a single reference. We are working to address these weaknesses using a chart analysis based on oracle hypotheses, guided by the negation elements contained in a source span and by how these elements are expected to be translated at each decoding step. Preliminary results show chart analysis is able to give a more in-depth analysis of the above errors and better explains the results of the manual analysis. 
This paper argues in favor of a linguisticallyinformed error classiﬁcation for SMT to identify system weaknesses and map them to possible syntactic, semantic and structural ﬁxes. We propose a scheme which includes both linguistic-oriented error categories as well as SMT-oriented edit errors, and evaluate an English-Spanish system and an English Basque system developed for a Q&A scenario in the IT domain. The classiﬁcation, in our use-scenario, reveals great potential for ﬁxes from lexical semantics techniques involving entity handling for IT-related names and user interface strings, word sense disambiguation for terminology, as well as argument structure for prepositions and syntactic parsing for various levels of reordering. 
Lexical false friends (FF) are the phenomena where words that look the same, do not have the same meaning or lexical usage. FF impose several challenges to statistical machine translation. We present a methodology which exploits word context modeling as well as information provided by word alignments for identifying false friends and choosing the right sense for them in the context. We show that our approach enhances SMT lexical choice for false friends across language variants. We demonstrate that our approach reduces word error rate (WER) and position independent error rate (PER) for Egyptian-English SMT by 0.6% and 0.1% compared to the baseline. 
We present an initial experiment in integrating a disambiguation step in MT evaluation. We show that accounting for sense distinctions helps METEOR establish better sense correspondences and improves its correlation with human judgments of translation quality. 
Translation of named-entities (NEs) is an issue in SMT. In this paper we analyze the errors when translating NEs with a SMT system from English to Spanish. We train on Europarl and test on News Commentary, focusing on entities correctly recognized by an automatic NE recognition system. The automatic systems translate around 85% NEs correctly, leaving a small margin for improving performance. In addition, we implement a purpose-build NE translator and integrate it in the SMT system, yielding a small but signiﬁcant improvement in BLEU score. Our analysis shows that, contrary to similar systems translating from Chinese to English, there was no improvement in NE translation, prompting further work. 
We use a morphology-aware SMT system which ﬁrst translates into a lemmatized representation with a component to generate fully inﬂected forms in a second step, see Toutanova et al. (2008) and Fraser et al. (2012). The inﬂection step requires the modeling of the grammatical case of noun phrases, which corresponds to determining the syntactic function. Weller et al. (2013) describe modeling case in SMT; we extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the “empty” preposition). The presented work is similar to that of Agirre et al. (2009), but is applied to a fully statistical MT system. A detailed presentation of our work including a full literature survey can be found in Weller et al. (2015). 
We describe a N-best reranking model based on features that combine sourceside dependency syntactical information and segmentation and alignment information. Speciﬁcally, we consider segmentation-aware ”phrase dependency” features. 
Semantic knowledge has been adopted recently for SMT preprocessing, decoding and evaluation, in order to be able to compare sentences based on their meaning rather than on mere lexical and syntactic similarity. Little attention has been paid to semantic knowledge in the context of integrating fuzzy matches from a translation memory with SMT. We present work in progress which focuses on semantics-based pretranslation before decoding in SMT. This involves applying fuzzy matching metrics based on lexical semantics and semantic roles, aligning parse trees based on semantic roles, and pretranslating matching source sentence parts using aligned tree nodes. 
 Morphological segmentation is an effective strategy for addressing difﬁculties caused by morphological complexity. In this study, we use an English-to-Arabic test bed to determine what steps and components of a phrase-based statistical machine translation pipeline beneﬁt the most from segmenting the target language. We test several scenarios that differ primarily in when desegmentation is applied, showing that the most important criterion for success in segmentation is to allow the system to build target words from morphemes that span phrase boundaries. We also investigate the impact of segmented and unsegmented target language models (LMs) on translation quality. We show that an unsegmented LM is helpful according to BLEU score, but also leads to a drop in the overall usage of compositional morphology, bringing it to well below the amount observed in human references. 
We describe 2 improvements to ChineseEnglish PropBank predicate-argument structure alignment. Taking advantage of the recently expanded PropBank English nominal and adjective predicate annotation (Bonial et al., 2014), we performed predicateargument alignments between both verb and nominal/adjective predicates in Chinese and English. Using our alignment system, this increased the number of aligned predicateargument structures by 24.5% on the parallel Xinhua News corpus. We also improved the PropBank alignment system using expectation-maximization (EM) techniques. By collecting Chinese-English predicate-topredicate and argument type-to-argument type alignment probabilities and iteratively improving the alignment output using these probabilities on a large unannotated parallel corpora, we improved the predicate alignment performance by 1 F point when using all automatic SRL and word alignment inputs. 
The paper describes the results of an empirical study of integrating bigram collocations and similarities between them and unigrams into topic models. First of all, we propose a novel algorithm PLSA-SIM that is a modiﬁcation of the original algorithm PLSA. It incorporates bigrams and maintains relationships between unigrams and bigrams based on their component structure. Then we analyze a variety of word association measures in order to integrate top-ranked bigrams into topic models. All experiments were conducted on four text collections of different domains and languages. The experiments distinguish a subgroup of tested measures that produce topranked bigrams, which demonstrate signiﬁcant improvement of topic models quality for all collections, when integrated into PLSASIM algorithm. 
We present a novel approach for the identiﬁcation of multiword expressions (MWEs). The methodology extracts a large set of recurring syntactic fragments from a given treebank using a Tree-Kernel method. Diﬀerently from previous studies, the expressions underlying these fragments are arbitrarily long and can include intervening gaps. In the initial study we use these fragments to identify MWEs as a parsing task (in a supervised manner) as proposed by Green et al. (2011). Here we obtain a small improvement over previous results. In the second part, we compare various association measures in reranking the expressions underlying these fragments in an unsupervised fashion. We show how a newly deﬁned measure (Log Inside Ratio) based on statistical parsing techniques is able to outperform classical association measures in the French data. 
Support-verb constructions (i.e., multiword expressions combining a semantically light verb with a predicative noun) are problematic for standard statistical machine translation systems, because SMT systems cannot distinguish between literal and idiomatic uses of the verb. We work on the German to English translation direction, for which the identiﬁcation of support-verb constructions is challenging due to the relatively free word order of German. We show that we achieve improved translation quality for verb-object supportverb constructions by marking the verbs when occuring in such constructions. Additional evaluations revealed that our systems produce more correct verb translations than a contrastive baseline system without verb markup. 
Scarcity of multiword expression data sets raises a fundamental challenge to evaluating the systems that deal with these linguistic structures. In this work we attempt to address this problem for a subclass of multiword expressions by producing a large data set annotated by experts and validated by common statistical measures. We present a set of 1048 noun-noun compounds annotated as non-compositional, compositional, conventionalized and not conventionalized. We build this data set following common trends in previous work while trying to address some of the well known issues such as small number of annotated instances, quality of the annotations, and lack of availability of true negative instances. 
The focus of this work is statistical idiosyncrasy (or collocational weight) as a discriminant property of multiword expressions. We formalize and model this property, compile a 2-class data set of MWE and non-MWE examples, and evaluate our models on this data set. We present a possible empirical implementation of collocational weight and study its effects on identiﬁcation and extraction of MWEs. Our models prove to be more effective than baselines in identifying noun-noun MWEs. 
We present a domain-independent clusteringbased approach for automatic extraction of multiword expressions (MWEs). The method combines statistical information from a general-purpose corpus and texts from Wikipedia articles. We incorporate association measures via dimensions of data points to cluster MWEs and then compute the ranking score for each MWE based on the closest exemplar assigned to a cluster. Evaluation results, achieved for two languages, show that a combination of association measures gives an improvement in the ranking of MWEs compared with simple counts of cooccurrence frequencies and purely statistical measures. 
One of the major motivations for a constructionbased approach to syntax is that a given rule of syntactic formation can often be associated with more than one semantic speciﬁcation. For example, a pair of expressions like purple plum and alleged thief call on different rules of semantic combination. The ﬁrst involves something related to intersection of sets: a purple plum is a member of the set of purple things and of the set of plums. But an alleged thief is not a member of the intersection of the set of thieves and the set of alleged things. Indeed, that intersection is empty, since only a proposition can be alleged and a thief is never a proposition. Constructional approaches recognize as instances of compositionality cases in which two different meanings for the same syntactic form are licensed by two different collections of form-meaning licensors, i.e., by two different collections of constructions. Construction-based grammars are nevertheless compositional in the usual sense: if you know the meanings of the words and you know all the rules that combine words and phrases into larger formal units, while simultaneously combining the meanings of the smaller units into the meanings of the larger ones, then you know the forms and meanings of all the larger units, including all the sentences. Constructional approaches focus on the fact that there are many such rules, and especially on the rules that assign meanings to complex structures. Such approaches do not draw a theoretical distinction between those rules thought to be in the core and those considered peripheral. The construction grammarian conceives of a language as a continuum of generality of expressions; a construction grammar 
This paper introduces NEMWEL, a system that performs Never-Ending MultiWord Expressions Learning. Instead of using a static corpus and classiﬁer, NEMWEL applies supervised learning on automatically crawled news texts. Moreover, it uses its own results to periodically retrain the classiﬁer, bootstrapping on its own results. In addition to a detailed description of the system’s architecture and its modules, we report the results of a manual evaluation. It shows that NEMWEL is capable of learning new expressions over time with improved precision. 
In this paper, we present the ﬁrst attempt to integrate predicted compositionality scores of multiword expressions into automatic machine translation evaluation, in integrating compositionality scores for English noun compounds into the TESLA machine translation evaluation metric. The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact. 
We argue that many multi-word domain terms are not (and should not be regarded as) strictly atomic, especially from a parser’s point of view. We introduce the notion of Lexical Kernel Units (LKUs), and discuss some of their essential properties. LKUs are building blocks for lexicalizations of domain concepts, and as such, can be used for compositional derivation of an open-ended set of domain terms. Beneﬁts from such an approach include reduction in size of the domain lexicon, improved coverage for domain terms, and improved accuracy for parsing. 
Past approaches to translate a phrase in a language L1 to a language L2 using a dictionarybased approach require grammar rules to restructure initial translations. This paper introduces a novel method without using any grammar rules to translate a given phrase in L1, which does not exist in the dictionary, to L2. We require at least one L1–L2 bilingual dictionary and n-gram data in L2. The average manual evaluation score of our translations is 4.29/5.00, which implies very high quality. 
Multiword expressions (MWEs) present particular and distinctive semantic properties, hence their automatic extraction receives special attention from the natural language processing (NLP) and corpus linguistics community, and is still an active research area. Unfortunately, the creation of necessary resources for this task is quite rigorous and many languages suffer from the lack of these; as in the case for Turkish. This study presents our MWE annotations on recently introduced Turkish Treebanks, which focuses on annotating various types of linguistic units and expressions, including named entities, numerical expressions, idiomatic phrases, verb phrases with auxiliaries and duplications. The paper aims to provide a benchmark and pave the way towards further MWE extraction research for Turkish. To this end, the paper also introduces our experimental results with seven baseline approaches, a dependency parser and a previously introduced rule-based extractor on these annotated corpora. Our highest performances achieved over these resources are about 60% F-scores. 
Verb senses are often assumed to distinguish among different conceptual event categories. However, senses misrepresent the number of event categories expressed both within and across languages and event categories may be “named” by more than a word, i.e. a multi-word expression. Determining the nature and number of event categories in an event description requires an understanding of the parameters relevant for categorization. We propose a set of parameters for use in creating a Gold Standard of event categories and apply them to a corpus sample of 2000 sentences across 10 verbs. In doing so, we find an asymmetry between subjects and direct objects in their contributions to distinguishing event categories. We then explore methods of automating event categorization to approximate our Gold Standard through the use of hierarchical clustering and Latent Semantic Analysis (Deerwester et al., 1990). 
Multiword expressions (MWEs) are vexing for linguists, psycholinguists and computational linguists, as they are hard to deﬁne, detect and parse. However, previous studies have not taken into account the cognitive constraints under which MWEs are produced or comprehended. We present a new modality for studying MWEs, keystroke dynamics. We ask subjects to respond to a variety of questions, varying in the level of cognitive demand required to generate an answer. In each response, a subject’s pause time preceding each word – within and outside an MWE – can illuminate distinct differences in required effort across tasks. By taking advantage of high-precision keystroke loggers, we show that MWEs produced under greater cognitive demands are produced more slowly, at a rate more similar to free expressions. We hypothesize that increasingly burdensome cognitive demands diminish the capacity of lexical retrieval, and cause MWE production to slow. 
Though the multiword lexicon has long been of interest in computational linguistics, most relevant work is targeted at only a small portion of it. Our work is motivated by the needs of learners for more comprehensive resources reﬂecting formulaic language that goes beyond what is likely to be codiﬁed in a dictionary. Working from an initial sequential segmentation approach, we present two enhancements: the use of a new measure to promote the identiﬁcation of lexicalized sequences, and an expansion to include sequences with gaps. We evaluate using a novel method that allows us to calculate an estimate of recall without a reference lexicon, showing that good performance in the second enhancement depends crucially on the ﬁrst, and that our lexicon conforms much more with human judgment of formulaic language than alternatives. 
Using clues from event semantics to solve coreference, we present an “event template” approach to cross-document event coreference resolution on news articles. The approach uses a pairwise model, in which event information is compared along ﬁve semantically motivated slots of an event template. The templates, ﬁlled in on the sentence level for every event mention from the data set, are used for supervised classiﬁcation. In this study, we determine granularity of events and we use the grain size as a clue for solving event coreference. We experiment with a newly-created granularity ontology employing granularity levels of locations, times and human participants as well as event durations as features in event coreference resolution. The granularity ontology is available for research. Results show that determining granularity along semantic event slots, even on the sentence level exclusively, improves precision and solves event coreference with scores comparable to those achieved in related work. 
In this paper, we propose a novel approach for Word Sense Disambiguation (WSD) of verbs that can be applied directly in the event mention detection task to classify event types. By using the PropStore, a database of relations between words, our approach disambiguates senses of verbs by utilizing the information of verbs that appear in similar syntactic contexts. Importantly, the resource our approach requires is only a word sense dictionary, without any annotated sentences or structures and relations between different senses (as in WordNet). Our approach can be extended to disambiguate senses of words for parts of speech besides verbs. 
In this paper we propose a scheme for annotating opposition relations among verb frames in lexical resources. The scheme is tested on the T-PAS resource, an inventory of typed predicate argument structures for Italian, conceived for both linguistic research and computational tasks. After discussing opposition relations from a linguistic point of view and listing the tags we decided to use, we report the results of the experiment we performed to test the annotation scheme, in terms of interannotation agreement and linguistic analysis of annotated data. 
We propose a new kind of event structure representation for computational linguistics, based on the theoretical framework of FirstPhase Syntax (Ramchand, 2008). We show that the approach not only gives a theoretically well-motivated set of subevents and related semantic roles, it also posits the levels of representation needed for analyzing a linguistic phenomenon that has repeatedly caused problems in computational systems, namely the treatment of complex predication. In particular, we look at V+V complex predicates in Urdu/Hindi and show that Ramchand’s subevent decomposition implemented in a VerbNet-style resource allows for a consistent semantic analysis of these complex events. We also show how the proposed event representation can be added to existing resources in the language, in particular the Hindi-Urdu Treebank and Hindi PropBank. 
This paper presents a Retrospective Event Detection algorithm, called Eventy-Topic Detection (ETD), which automatically generates topics that describe events in a large, temporal text corpus. Our approach leverages the structure of the topic modeling framework, specifically the Latent Dirichlet Allocation (LDA), to generate topics which are then later labeled as Eventy-Topics or non-Eventy-Topics. The system ﬁrst runs daily LDA topic models, then calculates the cosine similarity between the topics of the daily topic models, and then runs our novel Bump-Detection algorithm. Similar topics labeled as an Eventy-Topic are then grouped together. The algorithm is demonstrated on two Terabyte sized corpuses - a Reuters News corpus and a Twitter corpus. Our method is evaluated on a human annotated test set. Our algorithm demonstrates its ability to accurately describe and label events in a temporal text corpus. 
Causality is an important relation among events and entities. Embedded causal structures represent an important class, expressing complex causal chains; but they are traditionally diﬃcult to uncover automatically. In this paper we propose a method for the eﬃcient identiﬁcation and extraction of embedded causal relations with minimal supervision, by combining a representation of structured language data with modiﬁed prototype theory speciﬁcally suited to the data type. We then utilize a form of genetic algorithm speciﬁcally adapted for our purpose to locate the likely candidate linguistic structures that contain causal chains. With this procedure, we were able to identify many embedded structures with complex causal chains in two corpora of diﬀerent genres, applying this algorithm as a ranking procedure for all structures in the data. We obtained 79.5% percision for top quantiles of both of our datasets (BNC & novels). 
 1.1 The Event Nugget Detection Task  Event Mention detection is the ﬁrst step in textual event understanding. Proper evaluation is important for modern natural language processing tasks. In this paper, we present our evaluation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements.  As deﬁned in Mitamura (2014), event nugget detection involves identifying semantic meaningful units (mention span detection) that refer to an event1. The task also requires a system to identify other attributes (attribute detection). In this pilot study, the attributes are event type and realis status. (1) President Obama will nominate [realis: Other type: Personnel.Nominate] John Kerry for Secretary of State. (2) He carried out the assassination [realis: Actual type: Life.Die] .  
Event identification plays a crucial role in several natural language processing applications such as information extraction, question answering, and text analysis. In this paper, we describe a novel approach for analyzing events, their distribution, and the event mentions from a corpus of unlabeled business-based technical documents—a specific genre. In order to infer such mentions, we analyze the subject-verbobject structure for semi-automatically extracting several lexical, syntactic, and semantic features for each event mention from the corpus. Extracting event mentions allows us to cast grouping together the mentions with same features and propose properties leading to the differences of the specific genre. The obtained results are used for supporting an eventcentered processing level, from an automated machine for processing texts. 
ports the TAC KBP pilot evaluation for Event Nugget Detection as part of the DEFT program. In this paper, we introduce the notion of event nugget and how event nuggets are annotated in the corpus. We discuss the issues that arose in the process of developing TAC KBP Event Guidelines, because they are important challenges for manual annotation and impact the quality of annotation for gold standard creation. Two major issues are (1) determining if an event meets the event type/subtype definitions and (2) deciding which words should be tagged within the span of a multiword event nugget that represents a single event. We provide screen images of our annotation tool in order to give a complete picture of the annotation process. Finally, we present statistics to explain the characteristics of the corpus, such as the size of the corpus and the distribution of event type/subtypes. We discuss consistency analysis of inter-annotator agreement in terms of single word, multi-word continuous, and multi-word discontinuous event nuggets.  
In this paper we show how our semantic parser (Knowledge Parser or K-Parser) identiﬁes various kinds of event mentions in the input text. The types include recursive (complex) and non recursive event mentions. KParser outputs each event mention in form of an acyclic graph with root nodes as the verbs that drive those events. The children nodes of the verbs represent the entities participating in the events, and their conceptual classes. The on-line demo of the system is available at http://kparser.org 
We describe the evolution of the Entities, Relations and Events (ERE) annotation task, created to support research and technology development within the DARPA DEFT program. We begin by describing the specification for Light ERE annotation, including the motivation for the task within the context of DEFT. We discuss the transition from Light ERE to a more complex Rich ERE specification, enabling more comprehensive treatment of phenomena of interest to DEFT. 
 provide different information opens new perspec-  We describe a system for event extraction across documents and languages. We developed a framework for the interoperable semantic interpretation of mentions of events, participants, locations and time, as well as  tives to study the role of these sources in reporting on what happened in the world. When we consider news written in different languages this perspective becomes more complex but also more interesting. For such a cross-document and cross-lingual per-  the relations between them. Furthermore, we use a common RDF model to represent instances of events and normalised entities and dates. We convert multiple mentions of the same event in English, Spanish and Dutch to a single representation. We thus resolve crossdocument event and entity coreference within a language but also across languages. We  spective it is essential to deﬁne a semantically interoperable approach that can handle the large variation of event expressions within and across languages. In this paper, we report on a system to derive interoperable event representations across documents and across languages. In particular, we focus on English, Spanish and Dutch. Firstly, we developed Nat-  tested our system on a Wikinews corpus of 120 English articles that have been manually translated to Spanish and Dutch. We report on the cross-lingual cross-document event and entity extraction comparing the Spanish and Dutch output with respect to English.  ural Language Processing (NLP) pipelines for interpreting mentions of events and event components in text in a uniform way and, secondly, we developed a method to derive instance representations for these interpretations in RDF that is agnostic for the linguistic forms of expression. We report on the eval-  
We present a novel technique to identify emerging or important topics mentioned on social media. A sudden increase in related posts can indicate an occurrence of an external event. Assuming that the sequence of posts is a homogeneous Poisson process, this sudden change can be modeled using the Gamma distribution. Our Gamma curve ﬁtter is used to return a set of emerging topics. We demonstrate our algorithm on Twitter data and evaluate empirically using the Reuters News Archive and manual inspection. Our experimental results show that our algorithm provides a good picture of the emerging topics discussed on Twitter. 
Accessing historical texts is often a challenge because readers either do not know the historical language, or they are challenged by the technological hurdle when such texts are available digitally. Merging corpus linguistic methods and digital technology can provide novel ways of representing historical texts digitally and providing a simpler access. In this paper, we describe a multi-dimensional parallel Old Occitan-English corpus, in which word alignment serves as the basis for search capabilities as well as for the transfer of annotations. We show how parallel alignment can help overcome some challenges of historical manuscripts. Furthermore, we apply a resource-light method of building an emotion annotation via parallel alignment, thus showing that such annotations are possible without speaking the historical language. Finally, using visualization tools, such as ANNIS and GoogleViz, we demonstrate how the emotion analysis can be queried and visualized dynamically in our parallel corpus, thus showing that such information can be made accessible with low technological barriers. 
The analysis of sound and sonic devices in poetry is the focus of much poetic scholarship, and poetry scholars are becoming increasingly interested in the role that computation might play in their research. Since the nature of such sonic analysis is unique, the associated tasks are not supported by standard text analysis techniques. We introduce a formalism for analyzing sonic devices in poetry. In addition, we present RhymeDesign, an open-source implementation of our formalism, through which poets and poetry scholars can explore their individual notion of rhyme. 
We propose an approach to detecting the rhetorical ﬁgure called chiasmus, which involves the repetition of a pair of words in reverse order, as in “all for one, one for all”. Although repetitions of words are common in natural language, true instances of chiasmus are rare, and the question is therefore whether a computer can effectively distinguish a chiasmus from a random criss-cross pattern. We argue that chiasmus should be treated as a graded phenomenon, which leads to the design of an engine that extracts all criss-cross patterns and ranks them on a scale from prototypical chiasmi to less and less likely instances. Using an evaluation inspired by information retrieval, we demonstrate that our system achieves an average precision of 61%. As a by-product of the evaluation we also construct the ﬁrst annotated corpus of chiasmi. 
In this paper, we investigate whether longstanding literary theories about nineteenthcentury British novels can be veriﬁed using computational techniques. Elson et al. (2010) previously introduced the task of computationally validating such theories, extracting conversational networks from literary texts. Revisiting their work, we conduct a closer reading of the theories themselves, present a revised and expanded set of hypotheses based on a divergent interpretation of the theories, and widen the scope of networks for validating this expanded set of hypotheses. 
This paper introduces a software tool, GutenTag, which is aimed at giving literary researchers direct access to NLP techniques for the analysis of texts in the Project Gutenberg corpus. We discuss several facets of the tool, including the handling of formatting and structure, the use and expansion of metadata which is used to identify relevant subcorpora of interest, and a general tagging framework which is intended to cover a wide variety of future NLP modules. Our hope that the shared ground created by this tool will help create new kinds of interaction between the computational linguistics and digital humanities communities, to the beneﬁt of both. 
We present a manually annotated word alignment of Franz Kafka’s “Verwandlung” and use this as a controlled test case to assess the principled usefulness of word alignment as an additional information source for the (monolingually motivated) identiﬁcation of literary characters, focusing on the technically wellexplored task of co-reference resolution. This pilot set-up allows us to illustrate a number of methodological components interacting in a modular architecture. In general, co-reference resolution is a relatively hard task, but the availability of word-aligned translations can provide additional indications, as there is a tendency for translations to explicate underspeciﬁed or vague passages. 
We study perceptions of literariness in a set of contemporary Dutch novels. Experiments with machine learning models show that it is possible to automatically distinguish novels that are seen as highly literary from those that are seen as less literary, using surprisingly simple textual features. The most discriminating features of our classiﬁcation model indicate that genre might be a confounding factor, but a regression model shows that we can also explain variation between highly literary novels from less literary ones within genre. 
 In this paper we present a specific application of SPARSAR, a system for poetry analysis and TextToSpeech “expressive reading”. We will focus on the graphical output organized at three macro levels, a Phonetic Relational View where phonetic and phonological features are highlighted; a Poetic Relational View that accounts for a poem rhyming and metrical structure; and a Semantic Relational View that shows semantic and pragmatic relations in the poem. We will also discuss how colours may be used appropriately to account for the overall underlying attitude expressed in the poem, whether directed to sadness or to happiness. This is done following traditional approaches which assume that the underlying feeling of a poem is strictly related to the sounds conveyed by the words besides their meaning. This will be shown using part of Shakespeare’s Sonnets.  
Burrows’s Delta is the most established measure for stylometric difference in literary authorship attribution. Several improvements on the original Delta have been proposed. However, a recent empirical study showed that none of the proposed variants constitute a major improvement in terms of authorship attribution performance. With this paper, we try to improve our understanding of how and why these text distance measures work for authorship attribution. We evaluate the effects of standardization and vector normalization on the statistical distributions of features and the resulting text clustering quality. Furthermore, we explore supervised selection of discriminant words as a procedure for further improving authorship attribution. 
This paper investigates how literature could be used as a means to expand our understanding of history. By applying macroanalytic techniques we are aiming to investigate how women enter literature and particularly which functions do they assume, their working patterns and if we can spot differences in how often male and female characters are mentioned with various types of occupational titles (vocation) in Swedish literary texts. Modern historiography, and especially feminist and women’s history has emphasized a relative invisibility of women’s work and women workers. The reasons to this are manifold, and the extent, the margin of error in terms of women’s work activities is of course hard to assess. Therefore, vocation identification can be used as an indicator for such exploration and we present a hybrid system for automatic annotation of vocational signals in 19th century Swedish prose fiction. Beside vocations, the system also assigns gender (male, female or unknown) to the vocation words, a prerequisite for the goals of the study and future in-depth explorations of the corpora. 
Coreference resolution (CR) is a key task in the automated analysis of characters in stories. Standard CR systems usually trained on newspaper texts have difficulties with literary texts, even with novels; a comparison with newspaper texts showed that average sentence length is greater in novels and the number of pronouns, as well as the percentage of direct speech is higher. We report promising evaluation results for a rule-based system similar to [Lee et al. 2011], but tailored to the domain which recognizes coreference chains in novels much better than CR systems like CorZu. Rule-based systems performed best on the CoNLL 2011 challenge [Pradhan et al. 2011]. Recent work in machine learning showed similar results as rule-based systems [Durett et al. 2013]. The latter has the advantage that its explanation component facilitates a fine grained error analysis for incremental refinement of the rules. 
Several computational linguistics techniques are applied to analyze a large corpus of Spanish sonnets from the 16th and 17th centuries. The analysis is focused on metrical and semantic aspects. First, we are developing a hybrid scansion system in order to extract and analyze rhythmical or metrical patterns. The possible metrical patterns of each verse are extracted with language-based rules. Then statistical rules are used to resolve ambiguities. Second, we are applying distributional semantic models in order to, on one hand, extract semantic regularities from sonnets, and on the other hand to group together sonnets and poets according to these semantic regularities. Besides these techniques, in this position paper we will show the objectives of the project and partial results.  We have two general objectives: ﬁrst, we will try to extract regular patterns from the overall period; and second, in order to analyze each author inside the broad literary context in which they wrote (Garc´ıa Berrio, 2000), we will look for chains of relationships between them. Nowadays both objectives are focused on metrical and semantic aspects of Spanish Golden Age Sonnets. In this position paper we will present the computational linguistic techniques used to achieve these objectives. Next section shows how a large corpus of Spanish Golden-Age sonnets has been compiled and annotated; Section 3 describes a hybrid scansion system developed to extract metrical patterns; Section 4 presents how we use distributional semantic models to extract semantic patterns from the corpus; ﬁnally, Section 5 shows some preliminar conclusions.  
Current machine translation (MT) techniques are continuously improving. In speciﬁc areas, post-editing (PE) can enable the production of high-quality translations relatively quickly. But is it feasible to translate a literary work (ﬁction, short story, etc) using such an MT+PE pipeline? This paper offers an initial response to this question. An essay by the American writer Richard Powers, currently not available in French, is automatically translated and post-edited and then revised by non-professional translators. In addition to presenting experimental evaluation results of the MT+PE pipeline (MT system used, automatic evaluation), we also discuss the quality of the translation output from the perspective of a panel of readers (who read the translated short story in French, and answered a survey afterwards). Finally, some remarks of the ofﬁcial French translator of R. Powers, requested on this occasion, are given at the end of this article. 
We explore the feasibility of applying machine translation (MT) to the translation of literary texts. To that end, we measure the translatability of literary texts by analysing parallel corpora and measuring the degree of freedom of the translations and the narrowness of the domain. We then explore the use of domain adaptation to translate a novel between two related languages, Spanish and Catalan. This is the ﬁrst time that speciﬁc MT systems are built to translate novels. Our best system outperforms a strong baseline by 4.61 absolute points (9.38% relative) in terms of BLEU and is corroborated by other automatic evaluation metrics. We provide evidence that MT can be useful to assist with the translation of novels between closely-related languages, namely (i) the translations produced by our best system are equal to the ones produced by a professional human translator in almost 20% of cases with an additional 10% requiring at most 5 character edits, and (ii) a complementary human evaluation shows that over 60% of the translations are perceived to be of the same (or even higher) quality by native speakers. 
Language proﬁciency tests are a useful tool for evaluating learner progress, if the test difﬁculty ﬁts the level of the learner. In this work, we describe a generalized framework for test difﬁculty prediction that is applicable to several languages and test types. In addition, we develop two ranking strategies for candidate evaluation inspired by automatic solving methods based on language model probability and semantic relatedness. These ranking strategies lead to signiﬁcant improvements for the difﬁculty prediction of cloze tests. 
Automated scoring systems used for the evaluation of spoken or written responses in language assessments need to balance good empirical performance with the interpretability of the scoring models. We compare several methods of feature selection for such scoring systems and show that the use of shrinkage methods such as Lasso regression makes it possible to rapidly build models that both satisfy the requirements of validity and intepretability, crucial in assessment contexts as well as achieve good empirical performance. 
This paper presents an investigation of score prediction for the Organization dimension of an assessment of analytical writing in response to text. With the long-term goal of producing feedback for students and teachers, we designed a task-dependent model that aligns with the scoring rubric and makes use of the source material. Our experimental results show that our rubric-based model performs as well as baselines on datasets from grades 6-8. On shorter and noisier essays from grades 5-6, the rubric-based model performs better than the baselines. Further, we show that the baseline model (lexical chaining) can be improved if we extend it with information from the source text for shorter and noisier data. 
In this paper, we describe a morphological analyzer for learner Hungarian, built upon limited grammatical knowledge of Hungarian. The rule-based analyzer requires very few resources and is ﬂexible enough to do both morphological analysis and error detection, in addition to some unknown word handling. As this is work-in-progress, we demonstrate its current capabilities, some areas where analysis needs to be improved, and an initial foray into how the system output can support the analysis of interlanguage grammars. 
This work investigates linguistically motivated features for automatically scoring a spoken picture-based narration task. Speciﬁcally, we build scoring models with features for story development, language use and task relevance of the response. Results show that combinations of these features outperform a baseline system that uses state of the art speechbased features, and that best results are obtained by combining the linguistic and speech features. 
The task of Native Language Identiﬁcation (NLI) is typically solved with machine learning methods, and systems make use of a wide variety of features. Some preliminary studies have been conducted to examine the effectiveness of individual features, however, no systematic study of feature interaction has been carried out. We propose a function to measure feature independence and analyze its effectiveness on a standard NLI corpus. 
In this work, we investigate whether the analysis of opinion expressions can help in scoring persuasive essays. For this, we develop systems that predict holistic essay scores based on features extracted from opinion expressions, topical elements, and their combinations. Experiments on test taker essays show that essay scores produced using opinion features are indeed correlated with human scores. Moreover, we ﬁnd that combining opinions with their targets (what the opinions are about) produces the best result when compared to using only opinions or only topics. 
A key aspect of cognitive diagnostic models is the speciﬁcation of the Q-matrix associating the items and some underlying student attributes. In many data-driven approaches, test items are mapped to the underlying, latent knowledge components (KC) based on observed student performance, and with little or no input from human experts. As a result, these latent skills typically focus on modeling the data accurately, but may be hard to describe and interpret. In this paper, we focus on the problem of describing these knowledge components. Using a simple probabilistic model, we extract, from the text of the test items, some keywords that are most relevant to each KC. On a small dataset from the PSLC datashop, we show that this is surprisingly effective, retrieving unknown skill labels in close to 50% of cases. We also show that our method clearly outperforms typical baselines in speciﬁcity and diversity. 
Automatic evaluation of written responses to content-focused assessment items (automated short answer scoring) is a challenging educational application of natural language processing. It is often addressed using supervised machine learning by estimating models to predict human scores from detailed linguistic features such as word n-grams. However, training data (i.e., human-scored responses) can be difﬁcult to acquire. In this paper, we conduct experiments using scored responses to 44 prompts from 5 diverse datasets in order to better understand how training set size and other factors relate to system performance. We believe this will help future researchers and practitioners working on short answer scoring to answer practically important questions such as, “How much training data do I need?” 
We present a log-linear ranking model for interpreting questions in a virtual patient dialogue system and demonstrate that it substantially outperforms a more typical multiclass classiﬁer model using the same information. The full model makes use of weighted and concept-based matching features that together yield a 15% error reduction over a strong lexical overlap baseline. The accuracy of the ranking model approaches that of an extensively handcrafted pattern matching system, promising to reduce the authoring burden and make it possible to use conﬁdence estimation in choosing dialogue acts; at the same time, the effectiveness of the concept-based features indicates that manual development resources can be productively employed with the approach in developing concept hierarchies. 
Short answer scoring systems typically use regular expressions, templates or logic expressions to detect the presence of speciﬁc terms or concepts among student responses. Previous work has shown that manually developed regular expressions can provide effective scoring, however manual development can be quite time consuming. In this work we present a new approach that uses word-order graphs to identify important patterns from humanprovided rubric texts and top-scoring student answers. The approach also uses semantic metrics to determine groups of related words, which can represent alternative answers. We evaluate our approach on two datasets: (1) the Kaggle Short Answer dataset (ASAP-SAS, 2012), and (2) a short answer dataset provided by Mohler et al. (2011). We show that our automated approach performs better than the best performing Kaggle entry and generalizes as a method to the Mohler dataset. 
This paper reports on the development and the initial evaluation of a dictation&spelling prototype exercise for second language (L2) learners of Swedish based on text-to-speech (TTS) technology. Implemented on an already existing Intelligent Computer-Assisted Language Learning (ICALL) platform, the exercise has not only served as a test case for TTS in L2 environment, but has also shown a potential to train listening and orthographic skills, as well as has become a way of collecting learner-specific spelling errors into a database. Exercise generation re-uses well-annotated corpora, lexical resources, and text-to-speech technology with an accompanying talking head.  In the past five decades the area of NLP has witnessed intensive development in Sweden. However, ICALL has remained rather on the periphery of NLP community interests. Among the directions in which ICALL research developed in Sweden, one can name supportive writing systems (Bigert et al., 2005; Östling et al., 2013); exercise generators (Bick 2001, 2005; Borin & Saxena, 2004; Volodina et al., 2014); tutoring systems (Wik 2004, 2011; Wik & Hjalmarsson, 2009). As can be seen, the number of directions for Swedish ICALL projects is relatively small. Given the potential that NLP holds for CALL community, this fact is rather surprising, if not remarkable.  
We present the Jinan Chinese Learner Corpus, a large collection of L2 Chinese texts produced by learners that can be used for educational tasks. The present work introduces the data and provides a detailed description. Currently, the corpus contains approximately 6 million Chinese characters written by students from over 50 different L1 backgrounds. This is a large-scale corpus of learner Chinese texts which is freely available to researchers either through a web interface or as a set of raw texts. The data can be used in NLP tasks including automatic essay grading, language transfer analysis and error detection and correction. It can also be used in applied and corpus linguistics to support Second Language Acquisition (SLA) research and the development of pedagogical resources. Practical applications of the data and future directions are discussed. 
Automated short answer scoring is increasingly used to give students timely feedback about their learning progress. Building scoring models comes with high costs, as stateof-the-art methods using supervised learning require large amounts of hand-annotated data. We analyze the potential of recently proposed methods for semi-supervised learning based on clustering. We ﬁnd that all examined methods (centroids, all clusters, selected pure clusters) are mainly effective for very short answers and do not generalize well to severalsentence responses. 
This paper explores the annotation and classiﬁcation of students’ revision behaviors in argumentative writing. A sentence-level revision schema is proposed to capture why and how students make revisions. Based on the proposed schema, a small corpus of student essays and revisions was annotated. Studies show that manual annotation is reliable with the schema and the annotated information helpful for revision analysis. Furthermore, features and methods are explored for the automatic classiﬁcation of revisions. Intrinsic evaluations demonstrate promising performance in high-level revision classiﬁcation (surface vs. text-based). Extrinsic evaluations demonstrate that our method for automatic revision classiﬁcation can be used to predict a writer’s improvement. 
We introduce a novel framework based on the probabilistic model for emotion wording assistance. The example sentences from the online dictionary, Vocabulary.com are utilized as the training data; and the writings in a designed ESL’s writing task are the testing corpus. The emotion events are captured by extracting patterns of the example sentences. Our approach learns the joint probability of contextual emotion events and the emotion words from the training corpus. After extracting patterns in the testing corpus, we then aggregate their probabilities to suggest the emotion word that describes the ESL’s context most appropriately. We evaluate the proposed approach by the NDCG@5 of the suggested words for the writings in the testing corpus. The experiment result shows our approach can more appropriately suggest the emotion words compared to SVM, PMI and two representative on-line reference tools, PIGAI and Thesaurus.com. 
This paper describes RevUP which deals with automatically generating gap-ﬁll questions. RevUP consists of 3 parts: Sentence Selection, Gap Selection & Multiple Choice Distractor Selection. To select topicallyimportant sentences from texts, we propose a novel sentence ranking method based on topic distributions obtained from topic models. To select gap-phrases from each selected sentence, we collected human annotations, using the Amazon Mechanical Turk, on the relative relevance of candidate gaps. This data is used to train a discriminative classiﬁer to predict the relevance of gaps, achieving an accuracy of 81.0%. Finally, we propose a novel method to choose distractors that are semantically similar to the gap-phrase and have contextual ﬁt to the gap-ﬁll question. By crowdsourcing the evaluation of our method through the Amazon Mechanical Turk, we found that 94% of the distractors selected were good. RevUP ﬁlls the semantic gap left open by previous work in this area, and represents a signiﬁcant step towards automatically generating quality tests for teachers and self-motivated learners. 
Providing writing feedback to English language learners (ELLs) helps them learn to write better, but it is not clear what type or how much information should be provided. There have been few experiments directly comparing the effects of different types of automatically generated feedback on ELL writing. Such studies are difﬁcult to conduct because they require participation and commitment from actual students and their teachers, over extended periods of time, and in real classroom settings. In order to avoid such difﬁculties, we instead conduct a crowdsourced study on Amazon Mechanical Turk to answer questions concerning the effects of type and amount of writing feedback. We ﬁnd that our experiment has several serious limitations but still yields some interesting results. 
We examine different ensemble methods, including an oracle, to estimate the upper-limit of classiﬁcation accuracy for Native Language Identiﬁcation (NLI). The oracle outperforms state-of-the-art systems by over 10% and results indicate that for many misclassiﬁed texts the correct class label receives a signiﬁcant portion of the ensemble votes, often being the runner-up. We also present a pilot study of human performance for NLI, the ﬁrst such experiment. While some participants achieve modest results on our simpliﬁed setup with 5 L1s, they did not outperform our NLI system, and this performance gap is likely to widen on the standard NLI setup. 
A quasi-experimental study compared the effects of feedback condition on eighth-grade students’ writing motivation and writing achievement. Four classes of eighth-graders were assigned to a combined feedback condition in which they received feedback on their writing from their teacher and from an automated essay evaluation (AEE) system called PEGWriting®. Four other eighth-grade classes were assigned to a teacher feedback condition, in which they solely received feedback from their teacher via GoogleDocs. Results indicated that students in the combined PEGWriting+Teacher Feedback condition received feedback more quickly and indicated that they were more likely to solve problems in their writing. Equal writing quality was achieved between feedback groups even though teachers in the PEGWriting condition spent less time providing feedback to students than in the GoogleDocs condition. Results suggest that PEGWriting enabled teachers to offload certain aspects of the feedback process and promoted greater independence and persistence for students. 1. Introduction In the 21st century, possessing strong writing skills is essential for success in K-12 education, college acceptance and completion, and stable gainful employment (National Commission on Writing, 2004, 2005). Yet, more than two-thirds of students in grades four, eight, and twelve fail to achieve grade-level proficiency in writing, as indicated by recent performance on the National Assessment of Educational Progress (NCES, 2012; Salahu-Din, Persky, & Miller, 2008). Without sufficient writing skills, students are at-risk of performing worse in school, suffering lower grades, and experiencing school dropout (Graham & Perin, 2007).  One effective method for improving students’ writing skills is providing instructional feedback (Graham, McKeown, Kiuhara, & Harris, 2012; Graham & Perin, 2007). Struggling writers, in particular, need targeted instructional feedback because they tend to produce shorter, lessdeveloped, and more error-filled texts than their peers (Troia, 2006). However, instructional feedback is often difficult and time-consuming for teachers to provide. Indeed, educators in the primary and secondary grades report that the timecosts of evaluating writing are so prohibitive that they rarely assign more than one or two paragraphs of writing (Cutler & Graham, 2008; Kiuhara, Graham, & Hawken, 2009). Consequently, educators are increasingly relying on automated essay evaluation (AEE) systems (Warschauer & Grimes, 2008) to provide students with immediate feedback in the form of essay ratings and individualized suggestions for improving an essay—i.e., automated feedback. Previous research on AEE indicates that, in isolation of teacher feedback, automated feedback appears to support modest improvements in students’ writing quality. Findings from studies of ETS’s Criterion® (Kellogg, Whiteford, & Quinlan; Shermis, Wilson Garvan, & Diao, 2008), Pearson’s Summary Street (Franzke, Kintsch, Caccamise, Johnson, & Dooley, 2005; Wade-Stein & Kintsch, 2004), and Measurement Incorporated’s PEGWriting® system (Wilson & Andrada, in press; Wilson, Olinghouse, & Andrada, 2014), indicate that automated feedback assists students in improving the overall quality of their essays while concomitantly reducing the frequency of their mechanical errors. Less research has explored the effects of AEE on writing motivation. However, in two studies, Warschauer and Grimes (2008; 2010), found that  179  Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 179–189, Denver, Colorado, June 4, 2015. c 2015 Association for Computational Linguistics  teachers and students who had used ETS’ Criterion or Pearson’s My Access programs, agreed that AEE had positive effects on student motivation. Teachers also reported that the AEE systems saved them time on grading, and to be more selective about the feedback they gave. 1.1 Study purpose The purpose of the present study was to extend previous research in the following ways. First, previous studies of AEE have focused on the use of automated feedback in isolation of teacher feedback, despite the intended use of such systems for complementing, not replacing, teacher feedback (Kellogg et al., 2010). To date, no research has evaluated the effects of a combined AEE-andteacher-feedback condition against a teacherfeedback-only condition. Furthermore, studies have employed a weak control condition, typically a nofeedback condition, to test the effects of AEE on writing quality. Furthermore, additional research is needed regarding the possible effects of AEE on writing motivation. Theoretical models of writing (e.g., Hayes, 2006; 2012), and empirical research (e.g., Graham, Berninger, & Fan, 2007) underscore the importance of a student’s motivation and dispositions towards writing for promoting writing achievement. As AEE systems become more widely-used, it is important for stakeholders to know the degree, and limitations, of their effect on these affective dimensions of writing ability. Therefore, the present study compared a combined teacher-plus-AEE feedback condition to a teacher-feedback-only condition with regards to their effect on eighth-grade students’ writing motivation and writing quality. The combined feedback condition utilized an AEE system called PEGWriting. The teacher-feedback-only condition utilized the comments function of GoogleDocs to provide students with feedback. We hypothesized that students in the combined feedback condition would report greater motivation due to PEGWriting’s capacity to provide immediate feedback in the form of essay ratings and individualized suggestions for feedback. With respect to writing quality, it was difficult to generate a priori hypotheses given the aforementioned  limitations of previous research. Exploratory analyses considered whether students in the combined feedback condition outperformed their peers on measures of writing quality, or whether quality was commensurate across groups. 2. Methods 2.1 Setting and Participants This study was conducted in a middle school in an urban school district in the mid-Atlantic region of the United States. The district serves approx. 10,000 students in 10 elementary schools, three middle schools, and one high school. In this district, 43% of students are African-American, 20% are Hispanic/Latino, and 33% White. Approximately 9% of students are English Language Learners, and 50% of students come from low income families. Two eighth-grade English Language Arts (ELA) teachers agreed to participate in this study. The teachers were experienced, having taught for a total of 12 and 19 years, respectively. One teacher had earned a Master’s degree and the other was in the process of earning it (Bachelor’s +21 credits). Each teacher taught a total of four class periods of ELA per day. Given that the school did not use academic tracking and each class exhibited a range of reading and writing ability, teachers were asked to randomly select two classes from each of their schedules to assign to the combined automated-and-teacherfeedback condition (hereafter referred to as PEG+Teacher), and two classes to assign to a teacher-feedback-only condition (hereafter referred to as GoogleDocs). Thus, teachers instructed classes assigned to both feedback condition. A total of 74 students were assigned to the PEG+Teacher condition and 77 students to the GoogleDocs condition. Though classes were randomly assigned to feedback conditions, the study sampled intact classes, resulting in a quasiexperimental design. Table 1 reports demographics for each sample. Chi-Square and t-tests confirmed that the groups were equal with respect to all variables. In addition, all students received freelunch. No students received special education services.  180  Gender (n) Male Female Race (n) Hispanic/Latino African American White Asian Unreported ELL (n) Age (months) M SD  PEG + Teacher 41 33 20 31 22 1 0 2 169.03 5.90  GoogleDocs 38 39 20 24 30 1 2 0 169.51 4.90  Table 1: Demographics of Study Participants  2.2 Description of PEGWriting  PEGWriting is a web-based formative writing assessment program developed by Measurement Incorporated (MI). It is designed to provide students and teachers with an efficient and reliable method of scoring student writing in order to promote students’ writing skills. PEGWriting is built around an automated essay scoring engine called PEG, or Project Essay Grade. PEG was developed by Ellis Batten Page (Page, 1966; 1994; 2003) and acquired by MI in 2002. PEG uses a combination of techniques such as natural language processing, syntactic analysis, and semantic analysis to measure more than 500 variables that are combined in a regression-based algorithm that predicts human holistic and analytic essay ratings. A number of empirical studies have established the reliability and criterion validity of PEG’s essay ratings (Kieth, 2003; Shermis, 2014; Shermis, Koch, Page, Keith, & Harrington, 2002). Students and teachers access PEGWriting by visiting www.pegwriting.com and inputting their individual username and passwords. Teachers can assign system-created prompts in narrative, argumentative, or informative genres. They can also create and embed their own prompts, which can use words, documents, images, videos, or even music as stimuli. Once a prompt is assigned, students can select from several embedded graphic organizers to support their brainstorming and prewriting  activities. After prewriting, students have up to 60 minutes to complete and submit their drafts for evaluation by PEG. Once submitted, students immediately receive essay ratings for six traits of writing ability: idea development, organization, style, sentence structure, word choice, and conventions. Each of these traits is scored on a 1-5 scale and combined to form an Overall Score ranging from 6-30. In addition, students receive feedback on grammar and spelling, as well as traitspecific feedback that encourages students to review and evaluate their text with regard to the features of that specific trait. Students also receive customized links to PEGWriting’s skill-building mini-lessons. These lessons are multimedia interactive lessons on specific writing skills such as elaboration, organization, or sentence variety. Once students receive their feedback, they may revise and resubmit their essays up to a total of 99 times—the default limit is 30—and receive new essay ratings, error corrections, and trait-specific feedback. Teachers are also able to provide students with feedback by embedding comments within the students’ essays or through summary comments located in a text box following the PEG-generated trait-specific feedback. Students may also leave comments for their teacher using a similar function. 2.3 Study Procedures After classes were assigned to feedback conditions, all students completed a pretest writing motivation survey (Piazza & Siebert, 2008; see Section 2.4). Then, teachers began instruction in their districtassigned curriculum module on memoir writing. Teachers introduced the key features of memoir writing to all their classes. During this initial instructional phase, students in the PEG+Teacher condition were given an opportunity to learn how to use PEGWriting. Earlier in the school year, the first author trained the two teachers on the use PEGWriting during three 30 minute training sessions. Then, teachers subsequently trained their students how to use the program in one 45 minute class period following completion of the pretest writing motivation survey. Teachers then assigned their district-created writing prompt for the memoir unit, which read: We have all had interesting life experiences. Some are good, funny, or exciting, while  181  others are bad, sad, or devastating. Choose one experience from your life and tell the story. Once you have chosen your topic, you may choose to turn it into a scary story, drama, elaborate fiction, science fiction, comedy, or just tell it how it is. Be sure to organize your story and elaborate on your details. Your audience wasn’t there so you need to tell them every little detail. Students then proceeded to brainstorm, organize their ideas, and draft their memoirs using the technology available to them. Students in the PEG+Teacher condition used the built-in graphic organizers to plan out their memoirs. Students in the GoogleDocs condition used teacher-provided graphic organizers. Subsequent class periods were devoted to drafting, revising, and editing the memoirs. During this time, teachers delivered mini-lessons on features of memoir writing such as “Show, not tell,” “Using dialogue in memoirs,” and “Using transitions.” Both teachers kept a log of their instructional activities, documenting that they delivered the same instruction as each other and to each of the classes they taught. Teachers were instructed to review and provide feedback on their students’ writing a minimum of one, and a maximum of two times, across both conditions. Teachers were allowed to provide feedback as they normally would, commenting on those aspects of students’ text which they deemed necessary. They gave feedback to students in the GoogleDocs condition by (a) directly editing students’ texts, and (b) providing comments similar to the comment feature in Microsoft Word. Since students in the PEG+Teacher feedback condition were already receiving feedback from PEG, teachers could supplement the feedback with additional comments as they deemed necessary. Feedback was delivered in the form of embedded comments (similar to the GoogleDocs condition) and in the form of summary comments. Students in this condition were allowed to receive as much feedback from PEG as they wished by revising and resubmitting their memoir to PEG for evaluation. But, the amount of teacher feedback was held constant across conditions. At the conclusion of the instructional period (approx. three weeks), students submitted the final drafts of their memoir. Then, students were  administered a post-test writing motivation survey that mirrored the initial survey with additional items that specifically asked about their perceptions of the feedback they received. Teachers also completed a brief survey regarding their experiences providing feedback via PEGWriting and GoogleDocs. 2.4 Study Measures Writing Motivation was assessed using the Writing Disposition Scale (WDS; Piazza & Siebert, 2008), which consisted of 11 Likert-scale items that are combined to form three subscales measuring the constructs of confidence, persistence, and passion. Cronbach’s Alpha was reported as .89 for the entire instrument, and .81, .75, and .91, respectively for the three subscales (Piazza & Siebert, 2008). The WDS was administered at pretest and at posttest. The posttest administration of the WDS also include additional researcher-developed items asking students to share their perceptions of the feedback they received. These items included Likert-scale ratings followed by an open-ended response option. Writing quality was assessed using the PEG Overall Score, PEG trait scores, and teacher grades. Details on the PEG Overall Score and the PEG trait scores are found in Section 2.2. Teacher grades were generated by using a primary trait narrative rubric developed by the local school district. The rubric evaluated ten traits of personal narrative writing, each on a 0-10 scale. Final grades were assigned by totaling students’ scores on each of the ten traits (range: 0-100). Traits assessed included: the presence of a compelling introduction; logical organization; establishment of a setting, narrator, and point of view; effective conclusion which reflects on the life event; sufficient details and description; effective use of figurative language and dialogue; presence of accurate sentence structure; strong and vivid word choice; and absence of errors of spelling, punctuation, and usage. 2.5 Data Analysis Non-parametric analyses were used to estimate differences between feedback conditions on individual items of the Writing Dispositions Scale (WDS). A series of one-way analysis of variance (ANOVA) were used to estimate differences between groups on the Confidence, Persistence, and  182  PEG+Teacher  Item  SA A N D SD  1. My written work is among the  8 17 34 13 2  best in the class.  2. Writing is fun for me.  4 25 29 8 8  3. I take time to try different  3 33 23 13 2  possibilities in my writing.  4. I would like to write more in  2 11 25 22 14  school.  5. I am NOT a good writer.  3 12 23 20 16  6. Writing is my favorite subject in 3 6 24 31 10  school.  7. I am will to spend time on long 3 21 21 14 15  papers.  8. If I have choices during free  0 4 13 27 30  time, I usually select writing.  9. I always look forward to writing 3 10 27 21 13  class.  10. I take time to solve problems 11 34 17 8 4  in my writing.  11. Writing is easy for me.  10 24 31 2 7  GoogleDocs SA A N D SD 9 13 39 10 5 10 23 15 18 10 7 35 20 10 4 5 18 18 17 18 6 8 28 25 9 3 11 22 22 18 8 23 13 19 13 
This paper identiﬁes computational challenges in restructuring encyclopedic resources (like Wikipedia or thesauri) to reorder concepts with the goal of helping learners navigate through a concept network without getting trapped in circular dependencies between concepts. We present approaches that can help content authors identify regions in the concept network, that after editing, would have maximal impact in terms of enhancing the utility of the resource to learners. 
In this paper, we propose to use active learning for training classiﬁers to judge the quality of gap-ﬁll questions. Gap-ﬁll questions are widely used for assessments in education contexts because they can be graded automatically while offering reliable assessment of learners’ knowledge level if appropriately calibrated. Active learning is a machine learning framework which is typically used when unlabeled data is abundant but manual annotation is slow and expensive. This is the case in many Natural Language Processing tasks, including automated question generation, which is our focus. A key task in automated question generation is judging the quality of the generated questions. Classiﬁers can be built to address this task which typically are trained on human labeled data. Our evaluation results suggest that the use of active learning leads to accurate classiﬁers for judging the quality of gap-ﬁll questions while keeping the annotation costs in check. We are not aware of any previous effort that uses active learning for question evaluation. 
Various measures have been used to evaluate the effectiveness of automated text scoring (ATS) systems with respect to a human gold standard. However, there is no systematic study comparing the efﬁcacy of these metrics under different experimental conditions. In this paper we ﬁrst argue that measures of agreement are more appropriate than measures of association (i.e., correlation) for measuring the effectiveness of ATS systems. We then present a thorough review and analysis of frequently used measures of agreement. We outline desirable properties for measuring the effectiveness of an ATS system, and experimentally demonstrate using both synthetic and real ATS data, that some commonly used measures (e.g., Cohen’s kappa) lack these properties. Finally, we identify the most appropriate measures of agreement and present general recommendations for best evaluation practices. 
Automated scoring of student essays is increasingly used to reduce manual grading effort. State-of-the-art approaches use supervised machine learning which makes it complicated to transfer a system trained on one task to another. We investigate which currently used features are task-independent and evaluate their transferability on English and German datasets. We ﬁnd that, by using our task-independent feature set, models transfer better between tasks. We also ﬁnd that the transfer works even better between tasks of the same type. 
This paper presents a novel approach to error correction in content words in learner writing focussing on adjective–noun (AN) combinations. We show how error patterns can be used to improve the performance of the error correction system, and demonstrate that our approach is capable of suggesting an appropriate correction within the top two alternatives in half of the cases and within top 10 alternatives in 71% of the cases, performing with an M RR of 0.5061. We then integrate our error correction system with a state-of-the-art content word error detection system and discuss the results. 
Marking student responses to short answer questions raises particular issues for human markers, as well as for automatic marking systems. In this paper we present the Amati system, which aims to help human markers improve the speed and accuracy of their marking. Amati supports an educator in incrementally developing a set of automatic marking rules, which can then be applied to larger question sets or used for automatic marking. We show that using this system allows markers to develop mark schemes which closely match the judgements of a human expert, with the beneﬁts of consistency, scalability and traceability afforded by an automated marking system. We also consider some difﬁcult cases for automatic marking, and look at some of the computational and linguistic properties of these cases. 
In this work, we explore applications of automatic essay scoring (AES) to a corpus of essays written by college freshmen and discuss the challenges we faced. While most AES systems evaluate highly constrained writing, we developed a system that handles open-ended, long-form writing. We present a novel corpus for this task, containing more than 3,000 essays and drafts written for a freshman writing course. We describe statistical analysis of the corpus and identify problems with automatically scoring this type of data. Finally, we demonstrate how to overcome grader bias by using a multi-task setup, and predict scores as well as human graders on a different dataset. Finally, we discuss how AES can help teachers assign more uniform grades. 
This paper presents the results of an annotation study focused on the ﬁne-grained analysis of argumentation structures in scientiﬁc publications. Our new annotation scheme speciﬁes four types of binary argumentative relations between sentences, resulting in the representation of arguments as small graph structures. We developed an annotation tool that supports the annotation of such graphs and carried out an annotation study with four annotators on 24 scientiﬁc articles from the domain of educational research. For calculating the inter-annotator agreement, we adapted existing measures and developed a novel graphbased agreement measure which reﬂects the semantic similarity of different annotation graphs. 
This paper presents preliminary work on identification of argumentation schemes, i.e., identifying premises, conclusion and name of argumentation scheme, in arguments for scientific claims in genetics research articles. The goal is to develop annotation guidelines for creating corpora for argumentation mining research. This paper gives the specification of ten semantically distinct argumentation schemes based on analysis of argumentation in several journal articles. In addition, it presents an empirical study on readers’ ability to recognize some of the argumentation schemes. 
Argument mining studies in natural language text often use lexical (e.g. n-grams) and syntactic (e.g. grammatical production rules) features with all possible values. In prior work on a corpus of academic essays, we demonstrated that such large and sparse feature spaces can cause difﬁculty for feature selection and proposed a method to design a more compact feature space. The proposed feature design is based on post-processing a topic model to extract argument and domain words. In this paper we investigate the generality of this approach, by applying our methodology to a new corpus of persuasive essays. Our experiments show that replacing n-grams and syntactic rules with features and constraints using extracted argument and domain words signiﬁcantly improves argument mining performance for persuasive essays. 
We advocate a relation based approach to Argumentation Mining. Our focus lies on the extraction of argumentative relations instead of the identiﬁcation of arguments, themselves. By classifying pairs of sentences according to the relation that holds between them we are able to identify sentences that may be factual when considered in isolation, but carry argumentative meaning when read in context. We describe scenarios in which this is useful, as well as a corpus of annotated sentence pairs we are developing to provide a testbed for this approach. 
Park and Cardie (2014) proposed a novel task of automatically identifying appropriate types of support for propositions comprising online user comments, as an essential step toward automated analysis of the adequacy of supporting information. While multiclass Support Vector Machines (SVMs) proved to work reasonably well, they do not exploit the sequential nature of the problem: For instance, veriﬁable experiential propositions tend to appear together, because a personal narrative typically spans multiple propositions. According to our experiments, however, Conditional Random Fields (CRFs) degrade the overall performance, and we discuss potential ﬁxes to this problem. Nonetheless, we observe that the F1 score with respect to the unveriﬁable proposition class is increased. Also, semi-supervised CRFs with posterior regularization trained on 75% labeled training data can closely match the performance of a supervised CRF trained on the same training data with the remaining 25% labeled as well. 
Automatic generation of arguments is an important task that can be useful for many applications. For instance, the ability to generate coherent arguments during a debate can be useful when determining strengths of supporting evidence. However, with limited technologies that automatically generate arguments, the development of computational models for debates, as well as other areas, is becoming increasingly important. For this task, we focused on a promising argumentation model: the Toulmin model. The Toulmin model is both well-structured and general, and has been shown to be useful for policy debates. In this preliminary work we attempted to generate, with a given topic motion keyword or phrase, Toulmin model arguments by developing a computational model for detecting arguments spanned across multiple documents. This paper discusses our subjective results, observations, and future work. 
Argument extraction is the task of identifying arguments, along with their components in text. Arguments can be usually decomposed into a claim and one or more premises justifying it. The proposed approach tries to identify segments that represent argument elements (claims and premises) on social Web texts (mainly news and blogs) in the Greek language, for a small set of thematic domains, including articles on politics, economics, culture, various social issues, and sports. The proposed approach exploits distributed representations of words, extracted from a large non-annotated corpus. Among the novel aspects of this work is the thematic domain itself which relates to social Web, in contrast to traditional research in the area, which concentrates mainly on law documents and scientiﬁc publications. The huge increase of social web communities, along with their user tendency to debate, makes the identiﬁcation of arguments in these texts a necessity. In addition, a new manually annotated corpus has been constructed that can be used freely for research purposes. Evaluation results are quite promising, suggesting that distributed representations can contribute positively to the task of argument extraction. 
Argumentation mining and stance classiﬁcation were recently introduced as interesting tasks in text mining. In this paper, a novel framework for argument tagging based on topic modeling is proposed. Unlike other machine learning approaches for argument tagging which often require large set of labeled data, the proposed model is minimally supervised and merely a one-to-one mapping between the pre-deﬁned argument set and the extracted topics is required. These extracted arguments are subsequently exploited for stance classiﬁcation. Additionally, a manuallyannotated corpus for stance classiﬁcation and argument tagging of online news comments is introduced and made available. Experiments on our collected corpus demonstrate the beneﬁts of using topic-modeling for argument tagging. We show that using Non-Negative Matrix Factorization instead of Latent Dirichlet Allocation achieves better results for argument classiﬁcation, close to the results of a supervised classiﬁer. Furthermore, the statistical model that leverages automatically-extracted arguments as features for stance classiﬁcation shows promising results. 
The paper discusses the architecture and development of an Argument Workbench, which is a interactive, integrated, modular tool set to extract, reconstruct, and visualise arguments. We consider a corpora with dispersed information across texts, making it essential to conceptually search for argument elements, topics, and terminology. The Argument Workbench is a processing cascade, developed in collaboration with DebateGraph. The tool supports an argument engineer to reconstruct arguments from textual sources, using information processed at one stage as input to a subsequent stage of analysis, and then building an argument graph. We harvest and preprocess comments; highlight argument indicators, speech act and epistemic terminology; model topics; and identify domain terminology. We use conceptual semantic search over the corpus to extract sentences relative to argument and domain terminology. The argument engineer uses the extracts for the construction of arguments in DebateGraph. 
The main goal of argumentation mining is to analyze argumentative structures within an argument-rich document, and reason about their composition. Recently, there is also interest in the task of simply detecting claims (sometimes called conclusion) in general documents. In this work we ask how this set of detected claims can be augmented further, by adding to it the negation of each detected claim. This presents two NLP problems: how to automatically negate a claim, and when such a negated claim can plausibly be used. We present ﬁrst steps into solving both these problems, using a rule-based approach for the former and a statistical one towards the latter. 
We propose a sentence ordering method to help compose persuasive opinions for debating. In debate texts, support of an opinion such as evidence and reason typically follows the main claim. We focused on this claimsupport structure to order sentences, and developed a two-step method. First, we select from among candidate sentences a ﬁrst sentence that is likely to be a claim. Second, we order the remaining sentences by using a ranking-based method. We tested the effectiveness of the proposed method by comparing it with a general-purpose method of sentence ordering and found through experiment that it improves the accuracy of ﬁrst sentence selection by about 19 percentage points and had a superior performance over all metrics. We also applied the proposed method to a constructive speech generation task. 
Argumentation mining obviously involves ﬁnding support relations between statements, but many interesting instances of argumentation also contain counter-considerations, which the author mentions in order to preempt possible objections by the readers. A counterconsideration in monologue text thus involves a switch of perspective toward an imaginary opponent. We present a classiﬁcation approach to classifying counter-considerations and apply it to two different corpora: a selection of very short argumentative texts produced in a text generation experiment, and a set of newspaper commentaries. As expected, the latter pose more difﬁculties, which we investigate in a brief error anaylsis. 
Online debates sparkle argumentative discussions from which generally accepted arguments often emerge. We consider the task of unsupervised identiﬁcation of prominent argument in online debates. As a ﬁrst step, in this paper we perform a cluster analysis using semantic textual similarity to detect similar arguments. We perform a preliminary cluster evaluation and error analysis based on cluster-class matching against a manually labeled dataset. 
We investigate the characteristics of factual and emotional argumentation styles observed in online debates. Using an annotated set of FACTUAL and FEELING debate forum posts, we extract patterns that are highly correlated with factual and emotional arguments, and then apply a bootstrapping methodology to ﬁnd new patterns in a larger pool of unannotated forum posts. This process automatically produces a large set of patterns representing linguistic expressions that are highly correlated with factual and emotional language. Finally, we analyze the most discriminating patterns to better understand the deﬁning characteristics of factual and emotional arguments. 
In this paper, we look at three different methods of extracting the argumentative structure from a piece of natural language text. These methods cover linguistic features, changes in the topic being discussed and a supervised machine learning approach to identify the components of argumentation schemes, patterns of human reasoning which have been detailed extensively in philosophy and psychology. For each of these approaches we achieve results comparable to those previously reported, whilst at the same time achieving a more detailed argument structure. Finally, we use the results from these individual techniques to apply them in combination, further improving the argument structure identiﬁcation. 
Arbitrary n-ary relations (n ≥ 1) can in principle be realized through binary relations obtained by a reiﬁcation process that introduces new individuals to which the additional arguments are linked via accessor properties. Modern ontologies which employ standards such as RDF and OWL have mostly obeyed this restriction, but have struggled with it nevertheless. Additional arguments for representing, e.g., valid time, grading, uncertainty, negation, trust, sentiment, or additional verb roles (for ditransitive verbs and adjuncts) are often better modeled in relation and information extraction systems as direct arguments of the relation instance, instead of being hidden in deep structures. In order to address non-binary relations directly, ontologies must be extended by Cartesian types, ultimately leading to an extension of the standard entailment rules for RDFS and OWL. In order to support ontology construction, ontology editors such as Prote´ge´ have to be adapted as well.  
This paper introduces a dialogue-based ontology authoring interface. The aim of this interface is to simplify the ontology authoring process for the users. The design of the interface is based on the insights that have emerged from research into human language and explorations of authoring activities in Prote´ge´. We will discuss our initial ﬁndings regarding ontology authoring patterns and how we aim at modelling user’s goals and intentions. We will also discuss the challenges arising whilst generating interesting and comprehensible feedback. 
Software requirements describe functional and non-functional aspects of a software system and form the basis for the development process. Accordingly, requirements of existing systems can provide insights regarding the re-usability of already implemented software artifacts. To facilitate direct comparison between requirements of existing and to be developed systems, we propose to automatically map requirements in natural language text to structured semantic representations. For this task, we adapt techniques from semantic role labeling to a high-level ontology that deﬁnes concepts and relations for describing static software functionalities. The proposed method achieves a precision and recall of 77.9% and 74.5%, respectively, on an annotated software requirements dataset and signiﬁcantly outperforms two baselines that are based on lexical and syntactic patterns. 
In this article we look at how the use of ontologies can assist in analysing polysemy in natural languages. We develop a model, the Lexical-Sense-Ontology model (LSO), to represent the interaction between a lexicon and ontology, based on lemon. We use the LSO model to show how default rules can be used to represent semi-productivity in polysemy as well as discussing the kinds of ontological information that are useful for studying polysemy. 
In order to analyze and synthesize spatial language in Brazilian Portuguese automatically, a machine needs a linguistic model that ﬁts the sort of wordings that adult Brazilians express and can understand. In this paper, we analyzed a corpus of spatial actions and relations manually using the categories of the Generalized Upper Model (GUM) and veriﬁed how far this linguistic model covered the instances of our corpus. We found uncovered spatial relations and contextualization constraints and, based on these ﬁndings, we proposed (a) a reformulation of GUM’s typology for relational ﬁgures as well as (b) the notion of relational stability as a culture-speciﬁc contextualization constraint. 
Intuitively absurd but logically consistent sets of statements are common in publicly available OWL datasets. This article proposes an original and fully automated method to point at erroneous axioms in a consistent OWL knowledge base, by weakening it in order to improve its compliance with linguistic evidence gathered from natural language texts. A score for evaluating the compliance of subbases of the input knowledge base is proposed, as well as a trimming algorithm to discard potentially erroneous axioms. The whole approach is evaluated on two real datasets, with automatically retrieved web pages as a linguistic input.  Introduction  As they grow in size, knowledge bases (KBs) tend to contain statements which may make sense individually but, when taken together, violate common sense intuitions. As an illustration, consider the following set Ω of Description Logics (DL) formulas, issued from DBpedia (Mendes et al., 2012) :  Ex 1. Ω = {  (1) owningCompany(Smithsonian Networks, Smithsonian Institution),  (2) doctoralAdvisor(Thaddeus S.C. Lowe, Smithsonian Institution),  (3) doctoralAdvisor(Nick Katz, Bernard Dwork),  (4)  ∀doctoralAdvisor.Person,  (5)  ∀owningCompany.Company }  From (1), (2), (4) and (5), the individual Smithsonian Institution must be an instance of both Company and Person, which may seem counterintuitive, and indeed does not correspond to the overall understanding of these two concepts within DBpedia. This kind of issue is common among OWL datasets, which should not be a surprise. The most conventional way of spotting this kind of errors in OWL is by checking consistency or coherence1 of the input KB, but negation (or cardinality restriction) is underused in practice. As an illustration, according to the LODstats survey tool (Auer et al., 2012), which provides statistics about a sample of the Linked Open Data (LOD) cloud, the two most standard OWL constructs expressing negation, namely owl:disjointWith and owl:complementOf, have been observed 333 times and twice respectively, against more that 89 000 occurrences for owl:subClassOf. Let us assume that Ω is part of a larger KB K, for instance a subset of DBpedia extracted for a speciﬁc application, or a set of OWL statements aggregated from multiple sources. Assume also that there are several other instances of Person and Company according to K and, to keep the example simple, that Smithsonian Institution, Bernard Dwork, doctoralAdvisor, and owningCompany do not appear in K \Ω. If most instances of Person and Company according to K are respectively human beings and companies, one can expect the term “the Smithsonian Institution” to appear with linguistic contexts which tend to characterize terms denoting other instances of Company according to K (e.g. 1in the DL sense, i.e. the satisﬁability of all atomic concepts  the context “X was established”), but not other instances of Person (like “X was born in”). Similarly, “Bernard Dwork” should appear with contexts which are characteristic of terms denoting other instances of Person according to K. In other words, by checking the overall compliance of K with some linguistic input, it should be possible to identify some undesirable (Person(Smithsonian Institution)) and desirable (Company(Smithsonian Institution), Person(Bernard Dwork)) consequences of it. The next problem consists in determining how K can be weakened in order to discard the former, but keep the latter. Even if one focuses here (for readability) on weakening Ω only, there are several options available. The view adopted here, which is also the most common in the knowledge base debugging literature, is that some axiom(s) of Ω should be discarded, but none of them unnecessarily. Then the only solution in this example consists in discarding (2). The article investigates the applicability of such a trimming mechanism to moderately large input KBs (up to a few thousand statements), using automatically gathered web pages or snippets as linguistic input. To our knowledge, this is the ﬁrst attempt to use linguistic evidence in order to automatically weaken an existing KB instead of extending it. Section 1 reviews existing works in two closely related ﬁelds, KB extraction from texts and KB debugging, whereas section 2 introduces some conventions. Section 3 deﬁnes a score which evaluates the compliance with the linguistic data of any subbase of the input KB. Section 4 proposes an algorithm to trim the input KB based on this score. Section 5 evaluates the approach with two datasets. 
Restricting the spread of sensitive information is important in domains ranging from commerce to military operations. In this position paper, we propose research aimed at exploring techniques for privacy enforcement when humans are the recipient of — possibly obfuscated — information. Such obfuscation could be considered to be a white lie, and we argue that determining what information to share and whether it should be obfuscated must be done through controlled query evaluation, which depends on each agent’s risk/beneﬁt evaluation. We concentrate on machine-human interactions, and note that appropriate speciﬁc natural language interfaces need to be developed to handle obfuscation. We propose a solution for creating controlled query evaluation mechanisms based on robust approaches for data representation under uncertainty, viz. SDL-Lite, and propose using ITA Controlled English for natural language generation so as to handle subjectivity. We present the general architecture for our approach, with a focus on the relationship between formal ontology, controlled query evaluation, and natural language. 
Logical tradition establishes three basic modal meanings: alethic, epistemic and deontic, related, respectively, to the notions of truth, knowledge and conduct. Following this tradition, linguistics takes into account the notions of necessity, possibility and obligation to define modal types. However, as well as there is no unanimously adopted definition of modality, there is also no consensus of which categories should be encompassed under the label “modal”. In linguistic classical literature, studies organize in diverse ways the dimensions concerned to this phenomenon, depending on the theoretical frame. Among different approaches, the opposition epistemic and non-epistemic holds, and the values contrasted with the first vary considerably. For instance, Van der Auwera and Plungian (1998) distinguish participant-internal and participant-external modality and volition, evaluation, ability and capacity are other values considered (Palmer, 1986). Although most of the literature is centered on verbal expressions of modality (mostly semi-auxiliary verbs like may/might, should, can/could), studies on adverbs and modality have also been carried out for English. Several schemes for the annotation of modality have been proposed, mainly for English, and vary according to their objectives: some are strictly focused on modality while others are concerned with the identification of belief, subjectivity, factuality, as a source of data for applications in computational linguistics. Our objective in this paper is to make a contrastive analysis of two modality schemes that have recently been developed and implemented for Portuguese, and to go a little further, by suggesting a standardization of these two proposals, to be soon implemented. We also consider that this proposal could be applied to other languages than Portuguese (regarding their particularities), given its broad scope to written and oral data. Indeed, although more modality schemes have become available in the recent years, their contrastive study is hampered by the diversity of modal values that are included, and so is the evaluation of tools for the automatic annotation of modality. The two schemes for Portuguese differ mainly in what concerns the text type that they apply to: the scheme proposed for European Portuguese (Hendrickx et al., 2012a; 2012b) has been designed and applied over written texts, while the scheme for Brazilian Portuguese targets spontaneous speech data (Ávila and Mello, 2013; Ávila, 2014). We will revise and compare the two schemes in section 3, report on their application to corpora in section 4 and attempt a unified perspective in section 5.  2. Related work The growing interest on separating facts from speculations results from the importance of this task to NLP applications, as information extraction (Kartunnen and Zaennen, 2005); uncertainty modelling of clinical texts (Mowery et al., 2012); question answering (Saurí et al., 2006); classification of hedges (Medlock and Briscoe, 2007; Morante and Daelemans, 2009); and sentiment analysis (Wiebe et al., 2005). Annotating modality, in order to allow its automatic recognition, includes identifying modal indexes, classifying them in a given typology (e.g. in epistemic and non-epistemic meanings), defining its source and its semantic scope. Many projects that have been developed for the annotation of modal expressions focus mostly on English and on modal auxiliaries. Some highlight the relationship between modality and negation (Morante and Sporleder, 2012; Baker et al., 2012), the annotation of modal verbs meanings (Ruppenhofer and Rehbein, 2012), or the construction of automatic taggers (Baker et al., 2010). There are also annotation efforts undertaken for other languages, such as the work on Chinese (Cui and Chi, 2013), European Portuguese (Hendrickx et al., 2012; Mendes et al., 2013), and Brazilian Portuguese spoken data (Ávila and Mello, 2013; Ávila, 2014). Opposed to classical linguistic typologies of modality, these schemes describe in detail which elements in the text are actually involved in the expression of modality and their roles. These are the subject of the modality (source) and the elements in its scope (target/scope/focus). Other schemes (Baker et al., 2010; Matsuyoshi et al., 2010; Sauri et al., 2006) also determine the relation between sentences in text, identifying temporal and conditional relations between events or the evaluation of the degree of relevance of some information within a text, rather than classifying modal values. For more contrastive information on the existing annotation schemes, see an overview in Nissim et al. (2013). 3. Annotation schemes for Portuguese While the modal scheme for EP has been designed and applied to written texts, the modal scheme for BP is designed for spontaneous speech, and it is more theoretically-oriented. Modality is taken in enunciative terms (Bally, 1932), that is, it stands for the point of view of a subject who evaluates the locutory material in a given utterance in a communicative act. The scheme follows the Language Into Act Theory (Cresti, 2000), which takes the utterance as its reference unit, and considers the scope of the modality to be the information unit (Tucci, 2007). Both schemes converge in what concerns the elements that are not marked in the modal scheme, namely mood and tense. Nor do these schemes address factuality or a larger category of subjectivity and emotion. Due to their work on speech, Ávila and Mello (2013) and Ávila (2014) also distinguish modality, which is marked lexically and grammatically, from the pragmatic categories of illocution and attitude, which are carried by prosodic cues. As the three categories are often confused in their definition in the linguistic studies tradition, Mello and Raso (2011), through experimental investigation and observation of empirical data, suggest that modality is restricted to the semantic domain, although interrelated and projected into the pragmatic one. The same illocution can be modalized in different ways and performed with different attitudes, without affecting the illocutionary level. EP (Hendrickx et al., 2012) combines a practical annotation with a theoretically-oriented perspective mainly based on the work of Van der Auwera and Plungian (1998). The scheme includes the values Epistemic, Deontic and Participant-internal, but differs in two fundamental aspects from the proposals of these two authors: Participant-external modality is not considered as an independent type, but rather as a subtype of deontic modality; and several other values are considered, namely Evaluation, Volition, and, following Baker et al. (2010), Effort and Success. The sub-values for Epistemic express the conceptualizer’s perspective regarding the truth of the state of affair that is reported: Possibility, Knowledge, Belief, Doubt, Interrogative; the sub-values for Deontic modality express Obligation and Permission. The participant-internal modality has the sub-values Necessity and Capacity (internal necessity or internal capacity of the speaker, subject or other participant in the situation). Commissives and Evidentials are not annotated as a separate value but instead tagged, respectively, as a type of Deontic_obligation and Epistemic_belief (supported by evidence).  Declaratives are not included, and this is justified by the fact that they represent the unmarked level of modality (Oliveira, 1988), just as in the BP scheme. The BP scheme (based on the latest revision of the guidelines in Ávila, 2014) considers a three-category scheme of Epistemic, Deontic and Dynamic modality, inspired by Palmer (1986). Epistemic modality carries seven sub-values: knowledge, belief, possibility, probability, necessity (here the conceptualizer presents what is said as a necessity, based on previous knowledge (só pode ser doido ‘he can only be crazy, he has to be crazy)) and verification (the conceptualizer regards a state of affair as uncertain (olha aí se nũ tem ninguém ‘check over there if there is no one’). Deontic modality encompasses four sub-values: obligation, permission, prohibition and necessity (the conceptualizer expresses his or someone elses needs). Finally, dynamic modality comprises the subvalues ability and volition/intention. Table 1 presents a comparison of the modal values that are considered in the EP and the BP modal schemes: equivalent modal values (or sub-values) are presented in the same row, regardless of their designation. Both schemes are organized in terms of main and secondary modal values. The table also provides frequency of modal values in each corpus (see discussion in section 4). Most of the modal values are included in both schemes: it is the case of Epistemic_possibility, Epistemic_Knowledge, Epistemic_belief, Deontic_obligation, Deontic_Permission, Capacity/Ability, Volition. There are some cases of mismatch: the contexts tagged with the sub-value Epistemic_necessity in BP seem to be close to the value Deontic_obligation in EP; the Deontic_prohibition value in BP is most probably annotated as a Deontic_permission with negative polarity in EP; and Participant_internal_necessity in EP is covered by Deontic_necessity in BP (see arrows in Table 1). Two sub-values seem to have no equivalent: Epistemic_probability only occurs in BP and Epistemic_interrogative only occurs in EP. Besides those sub-values, three main values in the EP scheme are absent in BP: Evaluation, Effort and Success (there is however a partial equivalence for Success: when success is related to an internal capacity (e.g. verb conseguir ‘achieve’) it is tagged as Dynamic_ability in BP). The EP and BP schemes share the same components and their approach is described as very similar to the OntoSem (McShane et al., 2005) annotation scheme for modality (Nirenburg and McShane, 2008). These are the main components: the Trigger is the lexical item that carries modality; the Source of the modality is the conceptualizer, i.e., the individual whose perspective and view point is being reported (this might be the speaker, the addressee, or another entity in the discourse); Source of the event mention is the producer of the text or the speaker; the Target is the expression in the scope of the trigger. The BP scheme also considers a Target-dependent component to encompass the cases in which the target, in a given utterance, is not explicit, but it can be recoverable in the referential chain of the text. The two different types of sources are marked up to capture cases where the conceptualizer of the modality is not the producer of the text or speech. While the components of both schemes are practically the same, their conceptualization and application differ according to options in the delimitation of Trigger and Target and, mainly, to the text type which is annotated. For instance, the EP scheme follows a “min-max strategy” (Farkas et al., 2010) in which the Trigger is tagged as a single element whenever possible and the Target is tagged maximally (covering possible discontinuous sequences), while the BP scheme frequently selects multiword triggers. But the most significant difference falls on the Target component. The identification of the limits of the target is always a challenge, especially in what concerns consistency between annotators. In written texts the scope of the target is of a syntactic nature and the EP scheme specifies that syntactic boundaries should be respected. In spoken data, the target is in the scope of an information unit (IU) which may assume different functions: Comment (expresses the illocutionary force of the utterance), Topic (specifies the locus of application of the illocutionary force of the Comment), Parenthetical (expresses metalinguistic integration of the utterance) or Locutive Introducer (signals pragmatic suspension of the hic et nunc and introduces a meta-illocution). The BP scheme takes into account, for the annotation of the trigger and the target, the information unit in which they occur: Comment (COM), Topic (TOP), Parenthetical (PAR) or Locutive Introducer (INT). Example (1), taken from Ávila and Mello (2013), illustrates the differences in terms of target delimitation (for an explanation of the transcription symbols, refer to the authors’ paper). The utterance in (1) comprises three different tone units, and the target of the trigger tem que ‘has to/must’, in the second unit, is restringir também. It leaves out the direct object of the verb isso because it is outside this  information unit (defined prosodically). The same sequence in the EP scheme would take as target restringir também isso. (1) é / [a <gente] [tem que]> <[restringir também] / isso> // Yeah / we have to restrict too / this // The EP scheme includes a polarity feature on the trigger and on the target that describes the polarity of both components and allows to deal with dual negation (Quaresma et al., 2014), and also a feature Ambiguity on the trigger component to describe cases where two or more modal values are valid in the context. The authors are conscious of the importance of dealing with negation and of the possibility to create an independent markup scheme for polarity, that interacts with the modality scheme (e.g. Morante, 2010) or to deal with both in a unified scheme (e.g. Baker et al., 2012). The approach taken leans towards the second option, although very tentatively. A specific study in the interaction between modal triggers and focus (the exclusive particle só ‘only’) was also addressed by the EP scheme (Mendes et al., 2013). 4. Application to corpora The EP scheme has been applied to a corpus of 158.553 tokens, composed of 2000 sentences of written texts extracted from the written subpart of the Reference Corpus of Contemporary Portuguese (Généreux et al., 2012), a highly diverse corpus of 312 million words covering a large variety of textual genres and Portuguese varieties. A list of 40 Portuguese lemma verbs covering each modal value was the starting point for the extraction of the corpus sample and equal sets of single sentences for each modal type were randomly selected. Subsequently, the annotation covered all modal triggers found in the sentences. The BP scheme was applied to a sample from the C-ORAL-BRASIL I, an informal corpus of 139 texts, already published (Raso and Mello, 2012). The sample for modality annotation covers a sub-corpus of 20 texts with an average of 1,500 words each, thereby totally 31,318 words; 5,484 utterances and 9,825 tone units, divided into monologues, dialogues and conversations, distributed in familiar/private and public interactional contexts. The modal cues in both schemes are not restricted to modal auxiliaries, but rather take into consideration a large set of cues, such as propositional verbs, adverbs, adjectives, periphrastic forms and conditionals, and also nouns and interrogative clauses, in EP. In both projects, the annotation was performed with the MMAX2 annotation software tool (Müller and Strube, 2006), which is free, platform-independent, written in java and produces stand-off annotation1. In the BP annotation, the identification of modal markers was manually undertaken by three annotators working independently and qualitatively validated through group discussions, and the files were later annotated in the MMAX2 tool with the full scheme by one single annotator. In EP, the annotation was done by one annotator and all difficult cases were discussed with a second annotator. A small inter-annotator agreement (IAA) using Kappa-statistic (Cohen, 1960) was conducted over the EP corpus, with 50 sentences and two annotators, resulted in a kappa value of 0.65 for the trigger and 0.85 for modal value (Hendrickx et al., 2012). A follow-up study on the identification of modal triggers in the context of an exclusive adverb was also the subject of an IAA that reported a higher score for trigger identification (0.85), a similar score for modal value (0.83) and included the target component, which attained a score of 0.64. These results are considered in line with those reported for English (Matsuyoshi et al., 2010). In the set of 1946 sentences (158.553 tokens) of the EP corpus, 2377 triggers were tagged, while in the 20 texts sample of the BP corpus (31,318 words), 781 triggers were tagged with modality. The triggers of the EP corpus cover 2511 modal values due to 135 ambiguous cases, marked with more than one modal value. The frequency of each modal value in both corpora is provided in Table 1. The comparison of the data is not straightforward. Several factors hinder frequency comparisons: first of all, the EP corpus was selected from a list of 40 modal verbs and even if the list tried to balance the verbs per modal value, the corpus is to a certain extent biased, as assumed by the authors; the set of modal 
Most existing annotation schemes for hedging were created to aid in the automatic identiﬁcation of hedges in formal language styles, such as used in scholarly prose. Language with informal tone, typical in much web content, poses a challenge and provides illuminating case studies for the analysis of the use of hedges. We have analysed conversations from a web forum and identiﬁed the manners individuals express hedging through expressions which differ slightly regarding to their lexical form from hedges used in formal writing. Based on these observations, we propose an annotation scheme composed of three main categories of hedges where the main class comprises ﬁrst person epistemic expressions that explicitly note an individual’s involvement in what they express. We provide here an overview of our insights obtained by annotating a dataset of web forum posts according to this scheme. These observations will be useful in the design of automatic methods for the detection of hedges in texts in informal language.  
This paper summarizes the research that is leading to ISO standard 24617-6, which describes the approach to semantic annotation that characterizes the ISO semantic annotation framework (SemAF). It investigates the consequences and the risks of the SemAF strategy of developing separate annotation schemes for certain classes of semantic phenomena, with the long-term aim to combine these schemes into a single, wide- coverage scheme for semantic annotation. The principles are discussed for linguistic annotation in general and semantic annotation in particular that underly the SemAF effort. The notions of abstract syntax and concrete syntax are described with their relation to the speciﬁcation of a metamodel and the semantics of annotations. Overlaps between the annotation schemes deﬁned in SemAF parts are discussed, as well as semantic phenomena that cut across these schemes. 
We report an annotation experiment aiming at assessing the use of a single functional taxonomy of sense relations for discourse markers in spoken and written data. We start by presenting an operational deﬁnition of the category of DMs and its application to identify tokens of DMs in corpora. We then present an original annotation experiment making use of a uniﬁed taxonomy to annotate written and spoken data in English and French. In this experiment, we test the reliability of the annotations made separately by two annotators and the applicability of the tag set across two languages in the spoken and written modes. Our experiment leads us to conclude that: i) spoken data is not more difﬁcult to annotate than written data in terms of inter-annotator agreement, ii) recurrent problems are found across the two languages and modes, iii) the reliability of the annotation scheme is improved by the use of more explicit instructions and training. 
This paper presents a language for the semantic annotation of images, focusing on event types, their participants, and their spatial and orientational conﬁgurations. This language, ImageML, is a self-contained layered speciﬁcation language, building on top of ISOspace, as well as some elements from Spatial Role Labeling and SpatialML. An annotation language characterizing such features surrounding an event and its various aspects could play a signiﬁcant role in structured image retrieval, and a mapping of annotated semantic entities and the image’s low-level features will likely assist event recognition and description generation tasks. 
Relations that hold between discourse segments can, but need not, be made explicit by means of discourse connectives. Even though the explicit signaling of discourse relations is optional, not all relations can be easily conveyed implicitly. It has been proposed that readers and listeners have certain expectations about discourse and that discourse relations that are in line with these expectations (default) are more often implicit than the ones that are not (non-default). In this paper, we analyze the implicitation of discourse relations from a multilingual perspective. Using an annotation scheme for discourse relations based on Sanders, Spooren, & Noordman (1992), we distinguish between default and non-default discourse relations to predict the amount of implicit translations per relation in parallel corpora from four language pairs. We argue that the existing hypotheses about reader expectations are not sufficient to explain default discourse relations and propose that the rate of implicitation of discourse relations is governed by cognitive complexity: default discourse relations are cognitively simple within the framework of basic categories of discourse relations. 1. Introduction Discourse connectives like but and because in English are often used to explicitly mark discourse relations such as ‘cause’ and ‘concession’ that hold between two discourse segments (Halliday & Hasan, 1976; Mann & Thompson, 1988; Sanders, Spooren, & Noordman, 1992; Knott & Dale, 1994). In addition, connectives are important for text processing, comprehension and memorization (e.g. Britton et al., 1982; Caron, Micko, & Thüring, 1988; Haberlandt, 1982; Millis, Golding, & Barker, 1995; Sanders & Noordman, 2000). Despite their usefulness, connectives are not indispensable for the communication of discourse relations, as they can often be left implicit, in which case the relation can be reconstructed by inference. The causal relation conveyed by the connective because in (1) can for instance still be inferred in the absence of this connective, as in (2). (1) John is happy because he won the race. (2) John is happy. He won the race. Not all discourse relations, however, are equally easy to infer in the absence of a connective. For example, if the concessive connective although in (3) is removed, as in (4), the original coherence relation between the two segments is lost. In (4), the second segment is expected to be explaining the first one, but the semantic content of the relation clashes with this expectation, as the fact of losing the race is not a likely reason for being happy. (3) John is happy although he lost the race. (4) ? John is happy. He lost the race. Sanders (2005) proposed the “causality-by-default hypothesis” for the interpretation of implicit relations, which states that hearers by default expect two segments in a discourse to be causally related. This may explain the causal interpretation triggered by the implicit relation in (4). There are, however, restrictions to this causality-by-default principle. Most importantly, the propositional content of the two segments (clauses) has to allow for a causal interpretation. Murray’s (1995; 1997) 
Large-scale linguistic resources that provide relational information about predicates and their arguments are indispensable tools for a wide rage of NLP applications, where the participants of a certain event expressed by a predicate need to be detected. In particular, hand-annotated corpora combining semantic and syntactic information constitute the backbone for the development of probabilistic models that automatically identify the semantic relationships conveyed by sentential constituents in text, as in the case of Semantic Role Labeling (Gildea and Jurafsky, 2002). Even if attempts of standardization of semantic role annotations are being developed (cf. the LIRICS project, Petukhova and Bunt 2008), controversial points are still present. In this paper we examine a problematic semantic role, the Instrument role, which presents differences in deﬁnition and causes problems of attribution. Particularly, it is not clear whether to assign this role to inanimate entities occurring as subjects or not. This problem is especially relevant 1- because of its treatment in practical annotation and semantic role labeling, 2- because it affects the whole deﬁnition of semantic roles. We propose that inanimate nouns denoting instruments in subject positions are not instantiations of the Instrument role, but are Cause, Agent or Theme. Ambiguities in the annotation of these cases are due to confusion between semantic roles and ontological types associated with event participants. 
In annotating measure expressions such as three days and about 123 km, two recently published ISO standards, ISO-TimeML (ISO, 2012b) and ISOspace (ISO, 2014a), show some inconsistencies, as pointed out in ISO SemAF Principles (ISO, 2014c), a third ISO standards on semantic annotation to be published soon. Other than terminological or semantic inconsistencies introduced in ISO SemAF Principles, there are some formal inconsistencies between or within these standards. This paper attempts to resolve such inconsistencies by proposing some minimally possible modiﬁcations into the annotation schemes of those two standards. Despite these modiﬁcations, the interoperability between these standards is preserved, each retaining its own annotation scheme for either temporal or spatial information involving measures. An attempt is also made to partially merge ISO-TimeML and ISOspace as a step towards the integration of ISO SemAF standards into a modularly usable general annotation scheme for the semantic annotation of language. 
Abstract This paper introduces LX-SenseAnnotator, a user-friendly interface tool for manual word sense annotation. The demonstration will show how input texts are loaded by the tool, the options available to the annotator for displaying and browsing texts, and how word senses are displayed and manually assigned. The ﬂexibility of LX-SenseAnnotator, including the support of a variety of languages and the handling of pre-processed texts with different tagsets, will also be addressed. 
This paper describes some of the research conducted with the aim to develop a proposal for an ISO standard for the annotation of semantic relations in discourse. A range of theoretical approaches and annotation efforts were analysed for their commonalities and their differences, in order to deﬁne a clear delineation of the scope of the ISO effort and to give it a solid theoretical and empirical basis. A set of 20 core discourse relations was identiﬁed as indispensible for an annotation standard, and these relations were provided with clear deﬁnitions and illustrative examples from existing corpora. The ISO principles for linguistic annotation in general and semantic annotation in particular were applied to design a markup language (DRelML) for discourse relation annotation. 
Annotating the semantics of time in language is important. THYME (Styler et al., 2014) is a recent temporal annotation standard for clinical texts. This paper examines temporal expressions in the ﬁrst major corpus released under this standard. It investigates where the standard has proven difﬁcult to apply, and gives a series of recommendations regarding temporal annotation in this important domain. 
This paper presents the semantic annotation process of a corpus of spoken conversation transcriptions recorded in the Paris transport authority call-centre. The semantic model used is a FrameNet model developed for the French language. The methodology proposed for the rapid annotation of this corpus is a semi-supervised process where syntactic dependency annotations are used in conjunction with a semantic lexicon in order to generate frame candidates for each turn of a conversation. This ﬁrst hypotheses generation is followed by a rule-based decision module in charge of ﬁltering and removing ambiguities in the frames generated. These rules are very speciﬁc, they don’t need to generalize to other examples as the ﬁnal goal of this study is limited to the annotation of this given corpus, on which a statistical frame parser will ﬁnally be trained. This paper describes this methodology and give examples of annotations obtained. A ﬁrst evaluation of the quality of the corpus obtained is also given on a small gold corpus manually labeled. 
Accurate parse ranking requires semantic information, since a sentence may have many candidate parses involving common syntactic constructions. In this paper, we propose a probabilistic framework for incorporating distributional semantic information into a maximum entropy parser. Furthermore, to better deal with sparse data, we use a modiﬁed version of Latent Dirichlet Allocation to smooth the probability estimates. This LDA model generates pairs of lemmas, representing the two arguments of a semantic relation, and can be trained, in an unsupervised manner, on a corpus annotated with semantic dependencies. To evaluate our framework in isolation from the rest of a parser, we consider the special case of prepositional phrase attachment ambiguity. The results show that our semantically-motivated feature is effective in this case, and moreover, the LDA smoothing both produces semantically interpretable topics, and also improves performance over raw co-occurrence frequencies, demonstrating that it can successfully generalise patterns in the training data. 
Resolving attachment ambiguities is a pervasive problem in syntactic analysis. We propose and investigate an approach to resolving prepositional phrase attachment that centers around the ways of incorporating semantic knowledge derived from the lexico-semantic ontologies such as VERBNET and WORDNET.  
Adjectives are one of the major contributors to conveying subjective meaning to sentences. Understanding the semantics of adjectives is essential for understanding natural language sentences. In this paper we propose a novel approach for learning different properties that an adjective can describe, corresponding to the ‘attribute concepts’, which is not currently available in existing linguistic resources. We accomplish this by reading adjective glosses and bootstrapping attribute concepts from a seed of adjectives. We show that we can learn new attribute concepts using adjective glosses of WordNet adjectives with high accuracy as compared with human annotation on a test set. 
While continuous word vector representations enjoy increasing popularity, it is still poorly understood (i) how reliable they are for other languages than English, and (ii) to what extent they encode deep semantic relatedness such as paradigmatic relations. This study presents experiments with continuous word vectors for English and German, a morphologically rich language. For evaluation, we use both published and newly created datasets of morpho-syntactic and semantic relations. Our results show that (i) morphological complexity causes a drop in accuracy, and (ii) continuous representations lack the ability to solve analogies of paradigmatic relations. 
As part of our ongoing work on grounding in dialogue, we present a corpus-based investigation of intention-level clariﬁcation requests. We propose to reﬁne existing theories of grounding by considering two distinct types of intention-related conversational problems: intention recognition and intention adoption. This distinction is backed-up by an annotation experiment conducted on a corpus assembled with a novel method for automatically retrieving potential requests for clariﬁcation.  
Multimodal semantic models attempt to ground distributional semantics through the integration of visual or perceptual information. Feature norms provide useful insight into human concept acquisition, but cannot be used to ground large-scale semantics because they are expensive to produce. We present an automatic method for predicting feature norms for new concepts by learning a mapping from a text-based distributional semantic space to a space built using feature norms. Our experimental results show that we are able to generalise feature-based concept representations, which opens up the possibility of developing large-scale semantic models grounded in a proxy for human perceptual data. 
Predicting the (distributional) meaning of derivationally related words (read / read+er) from one another has recently been recognized as an instance of distributional compositional meaning construction. However, the properties of this task are not yet well understood. In this paper, we present an analysis of two such composition models on a set of German derivation patterns (e.g., -in, durch-). We begin by introducing a rank-based evaluation metric, which reveals the task to be challenging due to speciﬁc properties of German (compounding, capitalization). We also ﬁnd that performance varies greatly between patterns and even among base-derived term pairs of the same pattern. A regression analysis shows that semantic coherence of the base and derived terms within a pattern, as well as coherence of the semantic shifts from base to derived terms, all signiﬁcantly impact prediction quality. 
The semantic complexity of a quantiﬁer can be deﬁned as the computational complexity of the ﬁnite model checking problem induced by its semantics. This paper describes a preliminary study to understand if quantiﬁer distribution in corpora can be to some extent predicted or explained by semantic complexity. We show that corpora distributions for English are signiﬁcantly skewed towards quantiﬁers of low complexity and that this bias can be described in some cases by a power law. 
Following earlier work in multimodal distributional semantics, we present the ﬁrst results of our efforts to build a perceptually grounded semantic model. Rather than using images, our models are built on sound data collected from freesound.org. We compare three models: one bag-of-words model based on user-provided tags, a model based on audio features, using a ‘bag-of-audio-words’ approach and a model that combines the two. Our results show that the models are able to capture semantic relatedness, with the tag-based model scoring higher than the sound-based model and the combined model. However, capturing semantic relatedness is biased towards language-based models. Future work will focus on improving the sound-based model, ﬁnding ways to combine linguistic and acoustic information, and creating more reliable evaluation data. 
Extracting meaning from images is a challenging task that has generated much interest in recent years. In domains such as medicine, image understanding requires special expertise. Experts’ eye movements can act as pointers to important image regions, while their accompanying spoken language descriptions, informed by their knowledge and experience, call attention to the concepts and features associated with those regions. In this paper, we apply an unsupervised alignment technique, widely used in machine translation to align parallel corpora, to align observers’ eye movements with the verbal narrations they produce while examining an image. The resulting alignments can then be used to create a database of low-level image features and high-level semantic annotations corresponding to perceptually important image regions. Such a database can in turn be used to automatically annotate new images. Initial results demonstrate the feasibility of a framework that draws on recognized bitext alignment algorithms for performing unsupervised automatic semantic annotation of image regions. Planned enhancements to the methods are also discussed. 
We present a cross-lingual method for determining NP structures. More speciﬁcally, we try to determine whether the semantics of tripartite noun compounds in context requires a left or right branching interpretation. The system exploits the difference in word position between languages as found in parallel corpora. We achieve a bracketing accuracy of 94.6%, signiﬁcantly outperforming all systems in comparison and comparable to human performance. Our system generates large amounts of high-quality bracketed NPs in a multilingual context that can be used to train supervised learners. 
This paper addresses the task of semantic class learning by introducing a new methodology to identify the set of semantic classes underlying an aggregate of instances (i.e, a set of nominal phrases observed as a particular semantic role in a collection of text documents). The aim is to identify a set of semantically coherent (i.e., interpretable) and general enough classes capable of accurately describing the full extension that the set of instances is intended to represent. Thus, the set of learned classes is then used to devise a generative model for entity categorization tasks such as semantic class induction. The proposed methods are completely unsupervised and rely on an (unlabeled) open-domain collection of text documents used as the source of background knowledge. We demonstrate our proposal on a collection of news stories. Speciﬁcally, we model the set of classes underlying the predicate arguments in a Proposition Store built from the news. The experiments carried out show signiﬁcant improvements over a (baseline) generative model of entities based on latent classes that is deﬁned by means of Hierarchical Dirichlet Processes. 
We introduce a robust statistical approach to realization from Minimal Recursion Semantics representations. The approach treats realization as a translation problem, transforming the Dependency MRS graph representation to a surface string. Translation is based on a Synchronous Context-Free Grammar that is automatically extracted from a large corpus of parsed sentences. We have evaluated the new approach on the Wikiwoods corpus, where it shows promising results.1 
About half of the discourse relations annotated in Penn Discourse Treebank (Prasad et al., 2008) are not explicitly marked using a discourse connective. But we do not have extensive theories of when or why a discourse relation is marked explicitly or when the connective is omitted. Asr and Demberg (2012a) have suggested an information-theoretic perspective according to which discourse connectives are more likely to be omitted when they are marking a relation that is expected or predictable. This account is based on the Uniform Information Density theory (Levy and Jaeger, 2007), which suggests that speakers choose among alternative formulations that are allowed in their language the ones that achieve a roughly uniform rate of information transmission. Optional discourse markers should thus be omitted if they would lead to a trough in information density, and be inserted in order to avoid peaks in information density. We here test this hypothesis by observing how far a speciﬁc cue, negation in any form, affects the discourse relations that can be predicted to hold in a text, and how the presence of this cue in turn affects the use of explicit discourse connectives.  
This paper explores theoretical issues in constructing an adequate probabilistic semantics for natural language. Two approaches are contrasted. The ﬁrst extends Montague Semantics with a probability distribution over models. It has nice theoretical properties, but does not account for the ubiquitous nature of ambiguity; moreover inference is NP-hard. An alternative approach is described in which a sequence of pairs of sentences and truth values is generated randomly. By sacriﬁcing some of the nice theoretical properties of the ﬁrst approach it is possible to model ambiguity naturally; moreover inference now has polynomial time complexity. Both approaches provide a compositional semantics and account for the gradience of semantic judgements of belief and inference.1 
As a format for describing the meaning of natural language sentences, probabilistic logic combines the expressivity of ﬁrst-order logic with the ability to handle graded information in a principled fashion. But practical probabilistic logic frameworks usually assume a ﬁnite domain in which each entity corresponds to a constant in the logic (domain closure assumption). They also assume a closed world where everything has a very low prior probability. These assumptions lead to some problems in the inferences that these systems make. In this paper, we show how to formulate Textual Entailment (RTE) inference problems in probabilistic logic in a way that takes the domain closure and closed-world assumptions into account. We evaluate our proposed technique on three RTE datasets, on a synthetic dataset with a focus on complex forms of quantiﬁcation, on FraCas and on one more natural dataset. We show that our technique leads to improvements on the more natural dataset, and achieves 100% accuracy on the synthetic dataset and on the relevant part of FraCas. 
This paper investigates the representation of proper names in distributional semantics. We deﬁne three properties we expect names to display: uniqueness (being a unique entity), instantiation (being an instance of a relevant kind) and individuality (being separable from the subspace of concepts). We show that taking a standard distribution as the representation of a name does not satisfy those properties particularly well. We propose an alternative method to compute a name vector, which relies on re-weighting the distribution of the appropriate named entity type – in effect, producing an individual out of a kind. We illustrate the behaviour of such representations over some characters from two English novels. 
Motivated by theories of language development we investigate the contribution of affect to lexical semantics in the context of distributional semantic models (DSMs). The relationship between semantic and affective spaces is computationally modeled for the task of semantic similarity computation between words. It is shown that affective spaces contain salient information for lexical semantic tasks. We further investigate speciﬁc semantic relationships where affective information plays a prominent role. The relations between semantic similarity and opposition are studied in the framework of a binary classiﬁcation problem applied for the discrimination of synonyms and antonyms. For the case of antonyms, the use of affective features results in 33% relative improvement in classiﬁcation accuracy compared to the use of semantic features. 
The present paper reports on the results of automatic noun compound interpretation for English using a deep neural network classiﬁer and a selection of publicly available word embeddings to represent the individual compound constituents. The task at hand consists of identifying the semantic relation that holds between the constituents of a compound (e.g. WHOLE+PART_OR_MEMBER_OF in the case of ‘robot arm’, LOCATION in the case of ‘hillside home’). The experiments reported in the present paper use the noun compound dataset described in Tratz (2011), a revised version of the dataset used by Tratz and Hovy (2010) for training their Maximum Entropy classiﬁer. Our experiments yield results that are comparable to those reported in Tratz and Hovy (2010) in a crossvalidation setting, but outperform their system on unseen compounds by a large margin. 
Interpreting an utterance sometimes depends on the presence and nature of non-linguistic actions. In this paper, we motivate and develop a semantic model of embodied interaction in which the contribution that non-linguistic events make to the content of the interaction is dependent on their rhetorical connections to other actions, both linguistic and non-linguistic. We support our claims with concrete examples from a corpus of online chats, comparing annotations of the linguistic-only content against annotations in which non-linguistic events in the context are taken into account. 
A large part of human communication involves referring to entities in the world, and often these entities are objects that are visually present for the interlocutors. A computer system that aims to resolve such references needs to tackle a complex task: objects and their visual features must be determined, the referring expressions must be recognised, extra-linguistic information such as eye gaze or pointing gestures must be incorporated — and the intended connection between words and world must be reconstructed. In this paper, we introduce a discriminative model of reference resolution that processes incrementally (i.e., word for word), is perceptually-grounded, and improves when interpolated with information from gaze and pointing gestures. We evaluated our model and found that it performed robustly in a realistic reference resolution task, when compared to a generative model. 
Truly interactive dialogue systems need to construct meaning on at least a word-by-word basis. We propose desiderata for incremental semantics for dialogue models and systems, a task not heretofore attempted thoroughly. After laying out the desirable properties we illustrate how they are met by current approaches, comparing two incremental semantic processing frameworks: Dynamic Syntax enriched with Type Theory with Records (DS-TTR) and Robust Minimal Recursion Semantics with incremental processing (RMRS-IP). We conclude these approaches are not signiﬁcantly different with regards to their semantic representation construction, however their purported role within semantic models and dialogue models is where they diverge.  
We introduce s-graph grammars, a new grammar formalism for computing graph-based semantic representations. Semantically annotated corpora which use graphs as semantic representations have recently become available, and there have been a number of data-driven systems for semantic parsing that can be trained on these corpora. However, it is hard to map the linguistic assumptions of these systems onto more classical insights on semantic construction. S-graph grammars use graphs as semantic representations, in a way that is consistent with more classical views on semantic construction. We illustrate this with a number of hand-written toy grammars, sketch the use of s-graph grammars for data-driven semantic parsing, and discuss formal aspects. 
With the recent resurgence of interest in semantic annotation of corpora for improved semantic parsing, we observe a tendency which we view as ill-advised, to conﬂate sentence meaning and speaker meaning into a single mapping, whether done by annotators or by a parser. We argue instead for the more traditional hypothesis that sentence meaning, but not speaker meaning, is compositional, and accordingly that NLP systems would beneﬁt from reusable, automatically derivable, taskindependent semantic representations which target sentence meaning, in order to capture exactly the information in the linguistic signal itself. We further argue that compositional construction of such sentence meaning representations affords better consistency, more comprehensiveness, greater scalability, and less duplication of effort for each new NLP application. For concreteness, we describe one well-tested grammar-based method for producing sentence meaning representations which is efﬁcient for annotators, and which exhibits many of the above beneﬁts. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method. 
Computationally detecting the accepting/rejecting force of an utterance in dialogue is often a complex process. In this paper we focus on a class of utterances we call pragmatic rejections, whose rejection force arises only by pragmatic means. We deﬁne the class of pragmatic rejections, present a novel corpus of such utterances, and introduce a formal model to compute what we call rejectionsby-implicature. To investigate the perceived rejection force of pragmatic rejections, we conduct a crowdsourcing experiment and compare the experimental results to a computational simulation of our model. Our results indicate that models of rejection should capture partial rejection force.  
In conversation, interlocutors routinely indicate whether something said or done has been processed and integrated. Such feedback includes backchannels such as ‘okay’ or ‘mhm’, the production of a next relevant turn, and repair initiation via clariﬁcation requests. Importantly, such feedback can be produced not only at sentence/turn boundaries, but also sub-sententially. In this paper, we extend an existing model of incremental semantic processing in dialogue, based around the Dynamic Syntax (DS) grammar framework, to provide a low-level, integrated account of backchannels, clariﬁcation requests and their responses; demonstrating that they can be accounted for as part of the core semantic structure-building mechanisms of the grammar, rather than via higher level pragmatic phenomena such as intention recognition, or treatment as an “unoﬃcial” part of the conversation. The end result is an incremental model in which words, not turns, are seen as procedures for contextual update and backchannels serve to align participant semantic processing contexts and thus ease the production and interpretation of subsequent conversational actions. We also show how clariﬁcation requests and their following responses and repair can be modelled within the same DS framework, wherein the divergence and re-alignment eﬀort in participants’ semantic processing drives conversations forward. 
In this paper, we present a dynamic semantics for dialogue in terms of commitments. We use this to provide a model theoretic treatment of ambiguity and its effects on the evolutions of commitments as a dialogue proceeds. Our ﬁrst semantics ensures common commitments and has a simple logic for which we provide a complete axiomatization. On the other hand, our semantics poses difﬁculties for the analysis of particular dialogue moves, in particular acknowledgments, and of disputes. We provide a second semantics that addresses these difﬁculties. 
SystemVerilog assertion (SVA) is widely used for verifying properties of hardware designs. This paper presents a new method of generating SVAs from natural language assertion descriptions. For capturing the temporal semantics in natural language descriptions, we develop a new logical form called simple interval temporal logic (SIT L) which can deal formally with temporal constructions such as temporal prepositions. Furthermore, SIT L makes the transformations from natural language descriptions to SVAs possible. Thus, we build transformation rules to map our logic into SVAs. Our systematic experimental investigation on AXI bus protocol in ARM (2010) suggest that our method is applicable for generating SVAs from natural language descriptions. Keywords: Temporal semantics; temporal prepositions; natural language descriptions; SystemVerilog assertions 
The growing size, heterogeneity and complexity of databases demand the creation of strategies to facilitate users and systems to consume data. Ideally, query mechanisms should be schema-agnostic, i.e. they should be able to match user queries in their own vocabulary and syntax to the data, abstracting data consumers from the representation of the data. This work provides an informationtheoretical framework to evaluate the semantic complexity involved in the query-database communication, under a schema-agnostic query scenario. Different entropy measures are introduced to quantify the semantic phenomena involved in the user-database communication, including structural complexity, ambiguity, synonymy and vagueness. The entropy measures are validated using natural language queries over Semantic Web databases. The analysis of the semantic complexity is used to improve the understanding of the core semantic dimensions present at the query-data matching process, allowing the improvement of the design of schema-agnostic query mechanisms and deﬁning measures which can be used to assess the semantic uncertainty or difﬁculty behind a schema-agnostic querying task. Semantic Complexity, Entropy, Schema-agnostic Queries, Database Queries, Databases 
In this paper we use a deep auto-encoder for extractive query-based summarization. We experiment with different input representations in order to overcome the problems stemming from sparse inputs characteristic to linguistic data. In particular, we propose constructing a local vocabulary for each document and adding a small random noise to the input. Also, we propose using inputs with added noise in an Ensemble Noisy Auto-Encoder (ENAE) that combines the top ranked sentences from multiple runs on the same input with different added noise. We test our model on a publicly available email dataset that is speciﬁcally designed for text summarization. We show that although an auto-encoder can be a quite effective summarizer, adding noise to the input and running a noisy ensemble can make improvements. 
Language data for the Tesseract OCR system currently supports recognition of a number of languages written in Indic writing scripts. An initial study is described to create comparable data for Tesseract training and evaluation based on two approaches to character segmentation of Indic scripts; logical vs. visual. Results indicate further investigation of visual based character segmentation language data for Tesseract may be warranted. 
 (2) the selected learning algorithm, and (3) the  This study investigates the use of unsupervised features derived from word embedding approaches and novel sequence representation approaches for improving clinical information extraction systems. Our results corroborate previous ﬁndings that indicate that the use of word embeddings signiﬁcantly improve the effectiveness of concept extraction models; however, we further determine the inﬂuence that the corpora used to generate such features have. We also demonstrate the promise of sequence-based unsupervised features for further improving concept extraction.  quality of features generated from the data. In recent years, clinical information extraction and retrieval challenges like i2b2 (Uzuner et al., 2011) and ShARe/CLEF (Suominen et al., 2013) have provided annotated data which can be used to apply and evaluate different machine learning approaches (e.g., supervised and semi-supervised). Conditional Random Fields (CRFs) (Lafferty et al., 2001) has shown to be the state-of-the-art supervised machine learning approach for this clinical task. A wide range of features has been leveraged to improve the effectiveness of concept extraction systems, including hand-crafted grammatical, syntactic, lexical, morphological and orthographical features (de Bruijn et al., 2011; Tang  
Relation extraction is the task of extracting predicate-argument relationships between entities from natural language text. This paper investigates whether background information about entities available in knowledge bases such as FreeBase can be used to improve the accuracy of a state-of-the-art relation extraction system. We describe a simple and effective way of incorporating FreeBase’s notable types into a state-of-the-art relation extraction system (Riedel et al., 2013). Experimental results show that our notable typebased system achieves an average 7.5% weighted MAP score improvement. To understand where the notable type information contributes the most, we perform a series of ablation experiments. Results show that the notable type information improves relation extraction more than NER labels alone across a wide range of entity types and relations. 
This study is a pilot research that explores the effectiveness of a likelihood ratio (LR)-based forensic voice comparison (FVC) system built on non-native speech production. More specifically, it looks at native Hong Kong Cantonese-speaking male productions of English vowels, and the extent to which FVC can work on these speakers. 15 speakers participated in the research, involving two noncontemporaneous recording sessions with six predetermined target words – “hello”, “bye”, “left”, “right”, “yes”, and “no”. Formant frequency values were measured from the trajectories of the vowels and surrounding segments. These trajectories were modelled using discrete cosine transforms for each formant (F1, F2 and F3), and the coefficient values were used as feature vectors in the LR calculations. LRs were calculated using the multivariate-kernel-density method. The results are reported along two metrics of performance, namely the log-likelihood-ratio cost and 95% credible intervals. The six bestperforming word-specific outputs are presented and compared. We find that FVC can be built using L2 speech production, and the results are comparable to similar systems built on native speech. 
We present a clustering approach for documents returned by a PubMed search, which enable the organisation of evidence underpinning clinical recommendations for Evidence Based Medicine. Our approach uses a combination of document similarity metrics, which are fed to an agglomerative hierarchical clusterer. These metrics quantify the similarity of published abstracts from syntactic, semantic, and statistical perspectives. Several evaluations have been performed, including: an evaluation that uses ideal documents as selected and clustered by clinical experts; a method that maps the output of PubMed to the ideal clusters annotated by the experts; and an alternative evaluation that uses the manual clustering of abstracts. The results of using our similarity metrics approach shows an improvement over K-means and hierarchical clustering methods using TFIDF. 
Historical newspapers are an important resource in humanities research, providing the source materials about people and places in historical context. The Trove collection in the National Library of Australia holds a large collection of digitised newspapers dating back to 1803. This paper reports on some work to apply named-entity recognition (NER) to data from Trove with the aim of supplying useful data to Humanities researchers using the HuNI Virtual Laboratory. We present an evaluation of the Stanford NER system on this data and discuss the issues raised when applying NER to the 155 million articles in the Trove archive. We then present some analysis of the results including a version published as Linked Data and an exploration of clustering the mentions of certain names in the archive to try to identify individuals. 
A central task in clinical information extraction is the classiﬁcation of sentences to identify key information in publications, such as intervention and outcomes. Surface tokens and part-of-speech tags have been the most commonly used feature types for this task. In this paper we evaluate the use of word representations, induced from approximately 100m tokens of unlabelled in-domain data, as a form of semi-supervised learning for this task. We take an approach based on unsupervised word clusters, using the Brown clustering algorithm, with results showing that this method outperforms the standard features. We inspect the induced word representations and the resulting discriminative model features to gain further insights about this approach. 
Building comprehensive language models using latent semantic analysis (LSA) requires substantial processing power. At the ideal parameters suggested in the literature (for an overview, see Bradford, 2008) it can take up to several hours, or even days, to complete. For linguistic researchers, this extensive processing time is inconvenient but tolerated— but when LSA is deployed in commercial software targeted at non-specialists, these processing times become untenable. One way to reduce processing time is to reduce the number of dimensions used to build the model. While the existing research has found that the model’s reliability starts to degrade as dimensions are reduced, the point at which reliability becomes unacceptably poor varies greatly depending on the application. Therefore, in this paper, we set out to determine the lowest number of LSA dimensions that can still produce an acceptably reliable language model for our particular application: Lex, a visual cohesion analysis tool. We found that, across all three texts that we analysed, the cohesion-relevant visual motifs created by Lex start to become apparent and consistent at 50 retained dimensions.  
Risk assessment is a crucial activity for ﬁnancial institutions because it helps them to determine the amount of capital they should hold to assure their stability. Flawed risk assessment models could return erroneous results that trigger a misuse of capital by banks and in the worst case, their collapse. Robust models need large amounts of data to return accurate predictions, the source of which is text-based ﬁnancial documents. Currently, bank staff extract the relevant data by hand, but the task is expensive and timeconsuming. This paper explores a machine learning approach for information extraction of credit risk attributes from ﬁnancial documents, modelling the task as a named-entity recognition problem. Generally, statistical approaches require labelled data for learn the models, however the annotation task is expensive and tedious. We propose a solution for domain adaption for NER based on out-of-domain data, coupled with a small amount of in-domain data. We also developed a ﬁnancial NER dataset from publicly-available ﬁnancial documents. 
The utility of using morphological features in part-of-speech (POS) tagging is well established in the literature. However, the usefulness of exploiting information about POS tags for morphological segmentation is less clear. In this paper we study the POS-dependent morphological segmentation in the Adaptor Grammars framework. We experiment with three different scenarios: without POS tags, with gold-standard tags and with automatically induced tags, and show that the segmentation F1-score improves when the tags are used. We show that the gold-standard tags lead to the biggest improvement as expected. However, using automatically induced tags also brings some improvement over the tagindependent baseline. 
Some revisions of documents can change the meaning of passages, while others merely re-phrase or improve style. In a multi-author workﬂow, assisting readers to assess whether a revision changes meaning or not can be useful in prioritising revision. One challenge in this is how to detect and represent the revision changes in a meaningful way to assist users in assessing the impact of revision changes. This paper explores a segmentation approach which utilises the syntactic context of revisions to support assessment of signiﬁcant changes. We observe that length of normalised edit distance or Word Error Rate (WER) correlates better to the significance of the revision changes at sentence level compared to general sentence similarity approaches. We show that our proposed method, SAVeS, supports improved analysis of change signiﬁcance through alignment of segments rather than words. SAVeS can be used as the basis for a computational approach to identify signiﬁcant revision changes. 
This study compared three topic models trained on three versions of a news corpus. The ﬁrst model was generated from the raw news corpus, the second was generated from the lemmatised version of the news corpus, and the third model was generated from the lemmatised news corpus reduced to nouns only. We found that the removing all words except nouns improved the topics’ semantic coherence. Using the measures developed by Lau et al (2014), the average observed topic coherence improved 6% and the average word intrusion detection improved 8% for the noun only corpus, compared to modelling the raw corpus. Similar improvements on these measures were obtained by simply lemmatising the news corpus, however, the model training times are faster when reducing the articles to the nouns only. 
Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature word vectors have been used to obtain high performance in many natural language processing (NLP) tasks. In this paper, we present a new approach by incorporating word vectors to directly optimize the maximum a posteriori (MAP) estimation in a topic model. Preliminary results show that the word vectors induced from the experimental corpus can be used to improve the assignments of topics to words. Keywords: MAP estimation, LDA, Topic model, Word vectors, Topic coherence 
Interpreting event mentions in text is central to many tasks from scientiﬁc research to intelligence gathering. We present an event trigger detection system and explore baseline conﬁgurations. Speciﬁcally, we test whether it is better to use a single multi-class classiﬁer or separate binary classiﬁers for each label. The results suggest that binary SVM classiﬁers outperform multi-class maximum entropy by 6.4 points F-score. Brown cluster and WordNet features are complementary with more improvement from WordNet features. 
One of the most common lexical transformations between cognates in French and English is the presence or absence of a terminal “e”. However, many other transformations exist, such as a vowel with a circumﬂex corresponding to the vowel and the letter s. Our algorithms tested the effectiveness of taking the entire English and French lexicons from Treetagger, deaccenting the French lexicon, and taking the intersection of the two. Words shorter than 6 letters were excluded from the list, and a set of lexical transformations were also used prior to intersecting, to increase the potential pool of cognates. The result was 15% above the baseline cognate list in the initial test set, but only 1% above it in the ﬁnal test set. However, its accuracy was consistant at about 37% for both test sets. 
This paper describes our approaches to paraphrase recognition in Twitter organized as task 1 in Semantic Evaluation 2015. Lots of approaches have been proposed to address the paraphrasing task on conventional texts ( surveyed in (Madnani and Dorr, 2010)). In this work we examined the effectiveness of various linguistic features proposed in traditional paraphrasing task on informal texts, (i.e., Twitter), for example, string based, corpus based, and syntactic features, which served as input of a classiﬁcation algorithm. Besides, we also proposed novel features based on distributed word representations, which were learned using deep learning paradigms. Results on test dataset show that our proposed features improve the performance by a margin of 1.9% in terms of F1-score and our team ranks third among 10 teams with 38 systems. 
We use referential translation machines (RTMs) for predicting the semantic similarity of text. RTMs are a computational model effectively judging monolingual and bilingual similarity while identifying translation acts between any two data sets with respect to interpretants. RTMs pioneer a language independent approach to all similarity tasks and remove the need to access any task or domain speciﬁc information or resource. RTMs become the 2nd system out of 13 systems participating in Paraphrase and Semantic Similarity in Twitter, 6th out of 16 submissions in Semantic Textual Similarity Spanish, and 50th out of 73 submissions in Semantic Textual Similarity English. 
This paper reports our submissions to semantic textual similarity task, i.e., task 2 in Semantic Evaluation 2015. We built our systems using various traditional features, such as string-based, corpus-based and syntactic similarity metrics, as well as novel similarity measures based on distributed word representations, which were trained using deep learning paradigms. Since the training and test datasets consist of instances collected from various domains, three different strategies of the usage of training datasets were explored: (1) use all available training datasets and build a uniﬁed supervised model for all test datasets; (2) select the most similar training dataset and separately construct a individual model for each test set; (3) adopt multi-task learning framework to make full use of available training sets. Results on the test datasets show that using all datasets as training set achieves the best averaged performance and our best system ranks 15 out of 73. 
 2 Cubes for English STS  In Semantic Textual Similarity, systems rate the degree of semantic equivalence on a graded scale from 0 to 5, with 5 being the most similar. For the English subtask, we present a system which relies on several resources for token-to-token and phrase-to-phrase similarity to build a data-structure which holds all the information, and then combine the information to get a similarity score. We also participated in the pilot on Interpretable STS, where we apply a pipeline which ﬁrst aligns tokens, then chunks, and ﬁnally uses supervised systems to label and score each chunk alignment. 
In this paper, we present a comment labeling system based on a deep learning strategy. We treat the answer selection task as a sequence labeling problem and propose recurrent convolution neural networks to recognize good comments. In the recurrent architecture of our system, our approach uses 2-dimensional convolutional neural networks to learn the distributed representation for question-comment pair, and assigns the labels to the comment sequence with a recurrent neural network over CNN. Compared with the conditional random fields based method, our approach performs better performance on Macro-F1 (53.82%), and achieves the highest accuracy (73.18%), F1-value (79.76%) on predicting the Good class in this answer selection challenge. 
In this paper we present an unsupervised approach to word sense disambiguation based on evolutionary game theory. In our algorithm each word to be disambiguated is represented as a node on a graph and each sense as a class. The algorithm performs a consistent class assignment of senses according to the similarity information of each word with the others, so that similar words are constrained to similar classes. The dynamics of the system are formulated in terms of a non-cooperative multiplayer game, where the players are the data points to decide their class memberships and equilibria correspond to consistent labeling of the data. 
Unstructured clinical notes are rich sources for valuable patient information. Information extraction techniques can be employed to extract this valuable information, which in turn can be used to discover new knowledge. Named entity recognition and normalization are the basic tasks involved in information extraction. In this paper, identification of disorder named entities and the mapping of identified disorder entities to SNOMED-CT terminology using UMLS Metathesaurus is presented. A supervised linear chain conditional random field model based on sets of features to predict disorder mentions is used in conjunction with MetaMap to identify and normalize disorders. Error analysis conclude that recall of the developed system can be significantly increased by adding more features during model development and also by using a frame based approach for handling disjoint entities. 
We implemented an end-to-end system for disorder identiﬁcation and slot ﬁlling. For identifying spans for both disorders and their attributes, we used a linear chain conditional random ﬁeld (CRF) approach coupled with cTAKES for pre-processing. For combining disjoint disorder spans, ﬁnding relations between attributes and disorders, and attribute normalization, we used l2-regularized l2-loss linear support vector machine (SVM) classiﬁcation. Disorder CUIs were identiﬁed using a back-off approach to YTEX lookup (CUAB1) or NLM UTS API (CUAB2) if the target text was not found in the training data. Our best system utilized UMLS semantic type features for disorder/attribute span identiﬁcation and the NLM UTS API for normalization. It was ranked 12th in Task 1 (disorder identiﬁcation) and 6th in Task 2b (disorder identiﬁcation and slot ﬁlling) with a weighted F Measure of 0.711. 
The LT3 system perceives ABSA as a task consisting of three main subtasks, which have to be tackled incrementally, namely aspect term extraction, classiﬁcation and polarity classiﬁcation. For the ﬁrst two steps, we see that employing a hybrid terminology extraction system leads to promising results, especially when it comes to recall. For the polarity classiﬁcation, we show that it is possible to gain satisfying accuracies, even on out-ofdomain data, with a basic model employing only lexical information. 
 For candidate aspect extraction, we focus on the double  use WordNet [28] to automatically generate a list of general propagation method [26] which is based on the following  words using three typical general words “thing,” “person,” observations. The ﬁrst is that it is easy to identify (a priori)  and “place” as knowledge that  saeegUdesn.FerBRayl GwexoStred:ndiIsindngeortnmhetailDflyyPinnmotgetahnCodaaspwteeictght,otwhreeies  aanensxetdtisoTfthaoarpt igonpeioitnnsiowninowrdCosrdussuscathroe amussu“eagrlolyoRda”sesvaoncidieawt“ebdsawd,”ithetacs.pTechtes  obtain a major improvement in the precision with almost no (opinion targets) under certain syntactic relations. For example,  drop in recall on a widely used benchmark data set.  in the sentence “This camera is good,” “good” is an opinion  In summary, we make two contributions: (1) We propose to employ Answer Set ProgramAmnindger(AsoSnP)K–auaevrariant of Logic Programming – toInimstpitleumteenotfsIynnftaocrtmicaaltiacpspr–oaUchFbRaGseSd amJasovpraeecitmeleepxgltearmanctetinaotnnadt.ioOenfuﬁrhcaiiesmnaatp,uPbleoaokmunratdteonu5it1teAa0thriloael@nsignireooensnf l–foythf.Re8ucSoDrfdu–Prele.BgsmT,srehaw.tezhhbioiAldlreSiPas based implementation can process about 3000 sentences per  word. The “camera,” a noun modiﬁed by “good,” is clearly an aspect. TherefVorievifaronme aP.gMiveonrseeitraof opinion words, we can dsyenritvaecItaincssecttliutouefsteacsaopnefchItenslfipnoertxmetrrmaatcsitconsfes–wynUatsaFpcReticcGtsrSeflraotmionths.eSeixmtrialacrtleyd, aspects, anPdornteowAolpeignrieon–wRoSrd–s Bfroramzitlhe extracted aspects. This provpiagvaitioannepr@ocienssf.counftinrugess .ubntril no more opinion words or aspects can be extracted.  second, while the Java implementation only processes about 300 sentences per second. The preciseness and simplicity of the logic programming rules enable the sharing of knowledge used in aspect extraction and the reproducibility of experimental results. (2) We introduce the concept of general words based on WordNet and augmenAt bthsetraDcPt method with the knowledge that general words normally should not be taken as aspects, whicThhirsesuplatpseirn rmepoorretsacocnuroauter apsapreticctipeaxttiroanctionn. Again, the generSael mwEovrdasl-2an01d5neTwaskn1o2w,lewdhgiechcawn abse dneavtoutreadlly  Dependency grammar is adopted to represent the syntactic relations used in the propagation. See the picture below for an example of the dependency tree for the sentence “The phone has a good screen.”  ¡  ¢  £  ¤  ¥  ¦    ¥  ¦  ¨  ©    §  ¢          ©  implemented usitnogAAspSePc.t-Based Sentiment Analysis. Partic-                  The remaininipganotfs twheereparepqeur iriesdotrogaindieznetdifyasthfeolclaotwegso: rwy e present backgro(uenndtitaynadndrealatttreidbutwe)o,rktheinopSienciotinontarIgIeta,nadndan overview of ourthleogpioclaprriotygroafmcmusitnogmearpprerovaiecwhs.inTSheecstyiosntemIII. The ASP rules wtoe ibmuipltlermeleienst othneclDasPsiﬁmceathioond aflogroreitxhtmrascttiong explicit aspects airdeendteifsycraibspeedcitncaSteecgtoiorinesIVan.dOounr aneswet aopfpruroleasch to aspect prunintgo isdepnrteifsyenttheedoipninSioenctitoanrgeVt.. We proespeonste tahe experiments in Stewcoti-opnhaVseI calnadssciﬁocnactliuodneapthperopaacphefroarncdatedgisocruyss future work in SiedcetniotinﬁcVatIiIo.n and use a simple method for po- larity detection. Our results outperform the II. BAbaCsKelGinReOiUnNmDanAyNcDasReEs,LwAThiEcDh WmeOaRnsKour sys- In this sectiotnemwecoinutldrobdeucuesetdheasbasnicaslteorfnaastipveectfoerxatrsapcetciot n and Answer SetcPlarsosgiﬁracmatmioinn.g.      ¡  ¨  ¨    anoFtAihgeudrriewre1oc:rtdEdxweapimtehnpoldueetonafcnyaydiaendpddeiicntaidoteennsacltyhwtarotereod(nsLeiinuwtehoteraidlr.,dd2ee0pp1ee3nn)dd.esnocny path or they both depend on a third word directly. The DrOePplatpiotnaimioroctneaintsctheiwopcgodaaorntrdecysdmonaaairsnnkeiddeaeasptrlhsslouelsmoaumnrebildteytytahtsodcokildbrsaeescvfsatruidoﬁldnjmceeeacprttaeTiivbonaelndsese.knatoOnc1id2eupsara(srsAapsysiesnscpgtcteesomcemntrorpoulrensxs. or Bnaosuend pShernatsiems.enTthAusnatlhyesisp)o.teFnotriaml oPrOeSdettaagilss foonr thoipsinion wotradssk,arpeleJaJse(ardejfeecrtitvoesP),onJtJiRki (ectomalp. a(r2a0ti1v5e).adOjeucrtivsyes)- and JJSte(msucpoermlabtiivneesacdljaescstiivﬁecsa)tiwonhialelgtohroisthemfos,r caosrpeefcetrsenacree NN (sinregsuolalur tinoonuntos)olasn, danNdNaSsy(npltuarcatlicnpoaurnsse)r.. TOhneedoefpeonudrency  
This paper presents a supervised Aspect Based Sentiment Analysis (ABSA) system. Our aim is to develop a modular platform which allows to easily conduct experiments by replacing the modules or adding new features. We obtain the best result in the Opinion Target Extraction (OTE) task (slot 2) using an off-the-shelf sequence labeler. The target polarity classiﬁcation (slot 3) is addressed by means of a multiclass SVM algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80. 
 2 Data, Resources and Tools  The HLT-FBK system is a suite of SVMsbased classiﬁcation models for extracting time expressions, events and temporal relations, each with a set of features obtained with the NewsReader NLP pipeline. HLT-FBK’s best system runs ranked 1st in all three domains, with a recall of 0.30 over all domains. Our attempts on increasing recall by considering all SRL predicates as events as well as utilizing event co-reference information in extracting temporal links result in signiﬁcant improvements. 
The 2015 Clinical TempEval Challenge addressed the problem of temporal reasoning in the clinical domain by providing an annotated corpus of pathology and clinical notes related to colon cancer patients. The challenge consisted of six subtasks: TIMEX3 and event span detection, TIMEX3 and event attribute classiﬁcation, document relation time and narrative container relation classiﬁcation. Our BluLab team participated in all six subtasks. For the TIMEX3 and event subtasks, we developed a ClearTK support vector machine pipeline using mainly simple lexical features along with information from rule-based systems. For the relation subtasks, we employed a conditional random ﬁelds classiﬁcation approach, with input from a rule-based system for the narrative container relation subtask. Our team ranked ﬁrst for all TIMEX3 and event subtasks, as well as for the document relation subtask. 
This paper proposes neural networks for integrating compositional and non-compositional sentiment in the process of sentiment composition, a type of semantic composition that optimizes a sentiment objective. We enable individual composition operations in a recursive process to possess the capability of choosing and merging information from these two types of sources. We propose our models in neural network frameworks with structures, in which the merging parameters can be learned in a principled way to optimize a well-deﬁned objective. We conduct experiments on the Stanford Sentiment Treebank and show that the proposed models achieve better results over the model that lacks this ability. 
We are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture. The extension allows information low in parse trees to be stored in a memory register (the ‘memory cell’) and used much later higher up in the parse tree. This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies. Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank. 
A range of approaches to the representation of lexical semantics have been explored within Computational Linguistics. Two of the most popular are distributional and knowledgebased models. This paper proposes hybrid models of lexical semantics that combine the advantages of these two approaches. Our models provide robust representations of synonymous words derived from WordNet. We also make use of WordNet’s hierarcy to reﬁne the synset vectors. The models are evaluated on two widely explored tasks involving lexical semantics: lexical similarity and Word Sense Disambiguation. The hybrid models are found to perform better than standard distributional models and have the additional beneﬁt of modelling polysemy. 
As they grow in size, OWL ontologies tend to comprise intuitively incompatible statements, even when they remain logically consistent. This is true in particular of lightweight ontologies, especially the ones which aggregate knowledge from different sources. The article investigates how distributional semantics can help detect and repair violation of common sense in consistent ontologies, based on the identiﬁcation of consequences which are unlikely to hold if the rest of the ontology does. A score evaluating the plausibility for a consequence to hold with regard to distributional evidence is deﬁned, as well as several methods in order to decide which statements should be preferably amended or discarded. A conclusive evaluation is also provided, which consists in extending an input ontology with randomly generated statements, before trying to discard them automatically. 
This research describes the development of a supervised classiﬁer of English Caused Motion Constructions (CMCs) (e.g. The goalie kicked the ball into the ﬁeld). Consistent identiﬁcation of CMCs is a necessary step to a correct interpretation of semantics for sentences where the verb does not conform to the expected semantics of the verb (e.g. The crowd laughed the clown off the stage). We expand on a previous study on the classiﬁcation CMCs (Hwang et al., 2010) to show that CMCs can be successfully identiﬁed in the corpus data. In this paper, we present the classiﬁer and the series of experiments carried out to improve its performance. 
Word Sense Disambiguation has been stuck for many years. In this paper we explore the use of large-scale crowdsourcing to cluster senses that are often confused by non-expert annotators. We show that we can increase performance at will: our in-domain experiment involving 45 highly polysemous nouns, verbs and adjective (9.8 senses on average), yields an average accuracy of 92.6 using a supervised classiﬁer for an average polysemy of 6.1. Our proposal has the advantage of being cost-effective and being able to produce different levels of granularity. Our analysis shows that the error reduction with respect to ﬁnegrained senses is higher, and manual inspection show that the clusters are sensible when compared to those of OntoNotes and WordNet Supersenses. 
We propose a novel method to learn negation expressions in a specialized (medical) domain. In our corpus, negations are annotated as ‘ﬂat’ text spans. This allows for some infelicities in the mark-up of the ground truth, making it less than perfectly aligned with the underlying syntactic structure. Nonetheless, the negations thus captured are correct in intent, and thus potentially valuable. We succeed in training a model for detecting the negated predicates corresponding to the annotated negations, by re-mapping the corpus to anchor its ‘ﬂat’ annotation spans into the predicate argument structure. Our key idea—re-mapping the negation instance spans to more uniform syntactic nodes—makes it possible to re-frame the learning task as a simpler one, and to leverage an imperfect resource in a way which enables us to learn a high performance model. We achieve high accuracy for negation detection overall, 87%. Our re-mapping scheme can be constructively applied to existing ﬂatly annotated resources for other tasks where syntactic context is vital. 
The terms “belief” and “factuality” both refer to the intention of the writer to present the propositional content of an utterance as ﬁrmly believed by the writer, not ﬁrmly believed, or having some other status. This paper presents an ongoing annotation effort and an associated evaluation. 
Explicit Semantic Analysis (ESA) utilizes the Wikipedia knowledge base to represent the semantics of a word by a vector where every dimension refers to an explicitly deﬁned concept like a Wikipedia article. ESA inherently assumes that Wikipedia concepts are orthogonal to each other, therefore, it considers that two words are related only if they co-occur in the same articles. However, two words can be related to each other even if they appear separately in related articles rather than cooccurring in the same articles. This leads to a need for extending the ESA model to consider the relatedness between the explicit concepts (i.e. Wikipedia articles in Wikipedia based implementation) for computing textual relatedness. In this paper, we present NonOrthogonal ESA (NESA) which represents more ﬁne grained semantics of a word as a vector of explicit concept dimensions, where every such concept dimension further constitutes a semantic vector built in another vector space. Thus, NESA considers the concept correlations in computing the relatedness between two words. We explore different approaches to compute the concept correlation weights, and compare these approaches with other existing methods. Furthermore, we evaluate our model NESA on several word relatedness benchmarks showing that it outperforms the state of the art methods. 
Named entity disambiguation is the task of linking entity mentions to their intended referent, as represented in a Knowledge Base, usually derived from Wikipedia. In this paper, we combine local mention context and global hyperlink structure from Wikipedia in a probabilistic framework. Our results show that the two models of context, namely, words in the context and hyperlink pathways to other entities in the context, are complementary. We test our method in eight datasets, improving the state-of-the-art results in ﬁve, without any tuning, showing that it is robust to out-ofdomain scenarios. When tuning combination weights, we match the best reported results on the widely-used AIDA-CoNLL test-b. 
This paper addresses the question of how document classiﬁers can exploit implicit information about document similarity to improve document classiﬁer accuracy. We infer document similarity using simple n-gram overlap, and demonstrate that this improves overall document classiﬁcation performance over two datasets. As part of this, we ﬁnd that collective classiﬁcation based on simple iterative classiﬁers outperforms the more complex and computationally-intensive dual classiﬁer approach. 
Keyphrase extraction is a fundamental technique in natural language processing. It enables documents to be mapped to a concise set of phrases that can be used for indexing, clustering, ontology building, auto-tagging and other information organization schemes. Two major families of unsupervised keyphrase extraction algorithms may be characterized as statistical and graph-based. We present a hybrid statistical-graphical algorithm that capitalizes on the heuristics of both families of algorithms and is able to outperform the state of the art in unsupervised keyphrase extraction on several datasets. 
We induce semantic association networks from translation relations in parallel corpora. The resulting semantic spaces are encoded in a single reference language, which ensures cross-language comparability. As our main contribution, we cluster the obtained (crosslingually comparable) lexical semantic spaces. We ﬁnd that, in our sample of languages, lexical semantic spaces largely coincide with genealogical relations. To our knowledge, this constitutes the ﬁrst large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 
In this paper, we propose the use of word sense disambiguation and latent semantic features to automatically identify a person’s perspective from his/her written text. We run an Amazon Mechanical Turk experiment where we ask Turkers to answer a set of constrained and open-ended political questions drawn from the American National Election Studies (ANES). We then extract the proposed features from the answers to the open-ended questions and use them to predict the answer to one of the constrained questions, namely, their preferred Presidential Candidate. In addition to this newly created dataset, we also evaluate our proposed approach on a second standard dataset of “Ideological-Debates”. This latter dataset contains topics from four domains: Abortion, Creationism, Gun Rights and GayRights. Experimental results show that using word sense disambiguation and latentsemantics, whether separately or combined, beats the majority and random baselines on the cross-validation and held-out-test sets for both the ANES and the four domains of the “Ideological Debates” datasets. Moreover combining both feature sets outperforms a stronger unigram-only classiﬁcation system. 
Annotation efforts have resulted in the availability of a number of corpora with rhetorical relation information. The corpora, unfortunately, are annotated under different theoretical approaches and have different hierarchies of relations. In addition, new sets of rhetorical relations have been proposed to account for language variation. The types of relations, however, tend to overlap or be related in speciﬁc ways. We believe that differences across approaches are minimal, and a uniﬁed set of relations that works across languages is possible. This paper details a new taxonomy of relations organized in four top level-classes with a total of 26 relations. We propose a mapping between existing annotations and show that our taxonomy is robust across theories, and can be applied to multiple languages. 
The Practical Lexical Function model (PLF) is a recently proposed compositional distributional semantic model which provides an elegant account of composition, striking a balance between expressiveness and robustness and performing at the state-of-the-art. In this paper, we identify an inconsistency in PLF between the objective function at training and the prediction at testing which leads to an overcounting of the predicate’s contribution to the meaning of the phrase. We investigate two possible solutions of which one (the exclusion of simple lexical vector at test time) improves performance signiﬁcantly on two out of the three composition datasets. 
Based on the hypothesis that frame-semantic parsing and event extraction are structurally identical tasks, we retrain SEMAFOR, a stateof-the-art frame-semantic parsing system to predict event triggers and arguments. We describe how we change SEMAFOR to be better suited for the new task and show that it performs comparable to one of the best systems in event extraction. We also describe a bias in one of its models and propose a feature factorization which is better suited for this model.  Figure 1 illustrates an example. The sentence contains two events, DIE and ATTACK, triggered by “died” and “ﬁred”, respectively. For DIE, the roles victim, instrument, and place are ﬁlled with the arguments “cameraman”, “American tank”, and “Baghdad”, respectively. For ATTACK, the role target has two arguments, namely “cameraman” and “Palestine hotel”, the roles instrument, and place have the arguments, “American tank”, and “Baghdad”, respectively. Three arguments are shared. One of them, “cameraman”, plays different roles in the events, namely victim of DIE and target of ATTACK.  
We investigate from the competence standpoint two recent models of lexical semantics, algebraic conceptual representations and continuous vector models. Characterizing what it means for a speaker to be competent in lexical semantics remains perhaps the most signiﬁcant stumbling block in reconciling the two main threads of semantics, Chomsky’s cognitivism and Montague’s formalism. As Partee (1979) already notes (see also Partee 2013), linguists assume that people know their language and that their brain is ﬁnite, while Montague assumed that words are characterized by intensions, formal objects that require an inﬁnite amount of information to specify. In this paper we investigate two recent models of lexical semantics that rely exclusively on ﬁnite information objects: algebraic conceptual representations (ACR) (Wierzbicka, 1985; Kornai, 2010; Gordon et al., 2011), and continuous vector space (CVS) models which assign to each word a point in ﬁnitedimensional Euclidean space (Bengio et al., 2003; Turian et al., 2010; Pennington et al., 2014). After a brief introduction to the philosophical background of these and similar models, we address the hard questions of competence, starting with learnability in Section 2; the ability of ﬁnite networks or vectors to replicate traditional notions of lexical relatedness such as synonymy, antonymy, ambiguity, polysemy, etc. in Section 3; the interface to compositional semantics in Section 4; and language-speciﬁcity and  universality in Section 5. Our survey of the literature is far from exhaustive: both ACR and CVS have deep roots, with signiﬁcant precursors going back at least to Quillian (1968) and Osgood et al. (1975) respectively, but we put the emphasis on the computational experiments we ran (source code and lexica available at github.com/kornai/4lang). 
The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results. 
 will yield information on the semantic relation (if  The lexical semantic relationships between word pairs are key features for many NLP tasks. Most approaches for automatically classifying related word pairs are hindered by data sparsity because of their need to observe two words co-occurring in order to detect the lexical relation holding between them. Even when mining very large corpora, not every related word pair co-occurs. Using novel representations based on graphs and word embeddings, we present two systems that are able to predict relations between words, even when these are never found in the same sentence in a given corpus. In two experiments, we demonstrate superior performance of both approaches over the state of the art, achieving signiﬁcant gains  any) between them. Given a set of example word pairs having some relation, relation-speciﬁc patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-speciﬁc patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classiﬁcation system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipﬁan distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit  in recall.  on the number of instances that they can classify.  As an alternative to requiring co-occurrence, other  
 et al., 2015) that standardizes important aspects like  annotation types, preprocessing, and knowledge re-  A major problem in research on Textual Entailment (TE) is the high implementation effort for TE systems. Recently, interoperable standards for annotation and preprocessing have been proposed. In contrast, the algorithmic level remains unstandardized, which makes component re-use in this area very difﬁcult in practice. In this paper, we introduce multi-level  sources, it largely ignores the algorithmic level. In fact, TE algorithms themselves are generally not designed to be extensible or interoperable. Therefore, changes to the algorithms – like adding support for a new language or for new analysis aspect – are often very involved, if not impossible. This often forces the next generation of TE researchers to develop and  alignments as a central, powerful representation for TE algorithms that encourages modular, reusable, multilingual algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages.  implement their own core algorithms from scratch. In this paper, we address this problem by propos- ing a schema for TE algorithms that revolves around a central representation layer called multi-level alignment geared towards encoding the relevant information for deciding entailment. The use of multi-level alignments encourages a modular, extensible devel-  
Complex interactions among the meanings of words are important factors in the function that maps word meanings to phrase meanings. Recently, compositional distributional semantics models (CDSM) have been designed with the goal of emulating these complex interactions; however, experimental results on the effectiveness of CDSM have been difﬁcult to interpret because the current metrics for assessing them do not control for the confound of lexical information. We present a new method for assessing the degree to which CDSM capture semantic interactions that dissociates the inﬂuences of lexical and compositional information. We then provide a dataset for performing this type of assessment and use it to evaluate six compositional models using both co-occurrence based and neural language model input vectors. Results show that neural language input vectors are consistently superior to co-occurrence based vectors, that several CDSM capture substantial compositional information, and that, surprisingly, vector addition matches and is in many cases superior to purpose-built paramaterized models.  a different color to the ﬁrst two and a political afﬁliation to the third. This is an example of a common phenomenon in natural language in which the meaning of a whole expression is not derived from a simple concatenation of its parts, but is composed by interactions among their meanings. Cognitive and computer scientists have pointed out this complexity and proposed various models for accommodating it (Kintsch, 2001; Mitchell and Lapata, 2010; Socher et al., 2013). A dominant modeling approach seeks to learn functions that combine word representations derived from the distributional structure of large natural language corpora (Deerwester et al., 1990; Landauer and Dumais, 1997). Because the word representations to be combined and the compositional functions are generated based on the distributions of words in corpora, these models have been dubbed compositional distributional semantic models, or CDSM (Marelli et al., 2014). CDSM produce ﬁxed-dimensional vector representations of arbitrary sentences and phrases, and the foundational principle of these models is, stated simply, that semantically similar phrases should have vector representations that are close together in the vector space.  
The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson’s canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called “Dinners from Hell.” Our models learn narrative chains, script-like structures that we evaluate with the “narrative cloze” task (Chambers and Jurafsky, 2008). 
An English entity linking (EL) workflow is presented, which combines the annotations of five public open source EL services. The annotations are combined through a weighted voting scheme inspired by the ROVER method, which had not been previously tested on EL outputs. The combined results improved over each individual system’s results, as evaluated on four different golden sets. 
This paper reports an approach to automatically generate a lexical resource to support incremental semantic role labeling annotation in Portuguese. The data come from the corpus Propbank-Br (Propbank of Brazilian Portuguese) and from the lexical resource of English Propbank, as both share the same structure. In order to enable the strategy, we added extra annotation to Propbank-Br. This approach is part of a previous decision to invert the process of implementing a Propbank project, by first annotating a core corpus and only then generating a lexical resource to enable further annotation tasks. The reasoning behind such inversion is to explore the task empirically before distributing the annotation task and to provide simultaneously: 1) a first training corpus for SRL in Brazilian Portuguese and 2) annotated examples to compose a lexical resource to support SRL. The main contribution of this paper is to point out to what extent linguistic effort may be reduced, thereby speeding up the construction of a lexical resource to support SRL for less resourced languages. The corpus Propbank-Br, with the extra annotation described herein, is publicly available. 
 are often negated by parse errors. With the recent  We describe a topic model based approach for selectional preference. Using the topic features generated by an LDA model on the extracted predicate-arguments over the Chinese Gigaword corpus, we show improvement to our state-of-the-art Chinese SRL system by 2.34 F1 points on arguments of nominal pred-  extension of PropBank SRL to nominal and adjective predicates, preposition relationships, light-verb constructions, and abstract meaning representation (Bonial et al., 2014; Banarescu et al., 2013), it may be time to revisit SP for SRL. We hypothesize that SP will provide a greater beneﬁt to nominal SRL, especially on a language with lower parsing accuracy.  icates, 0.40 F1 point on arguments of verb predicates, and 0.66 F1 point overall. More over, similar gains were achieved on out-ofgenre test data, as well as on English SRL using the same technique.  In this paper, we apply SP to Chinese SRL (which has few morphological clues that impacts parsing accuracy) for arguments of both verb and nominal predicates using Chinese Gigaword. Our hypothesis, that SP will provide a greater beneﬁt for nomi-  
With the increasing automation of health care information processing, it has become crucial to extract meaningful information from textual notes in electronic medical records. One of the key challenges is to extract and normalize entity mentions. State-of-the-art approaches have focused on the recognition of entities that are explicitly mentioned in a sentence. However, clinical documents often contain phrases that indicate the entities but do not contain their names. We term those implicit entity mentions and introduce the problem of implicit entity recognition (IER) in clinical documents. We propose a solution to IER that leverages entity deﬁnitions from a knowledge base to create entity models, projects sentences to the entity models and identiﬁes implicit entity mentions by evaluating semantic similarity between sentences and entity models. The evaluation with 857 sentences selected for 8 different entities shows that our algorithm outperforms the most closely related unsupervised solution. The similarity value calculated by our algorithm proved to be an effective feature in a supervised learning setting, helping it to improve over the baselines, and achieving F1 scores of .81 and .73 for different classes of implicit mentions. Our gold standard annotations are made available to encourage further research in the area of IER. 
Semantic role labeling has become a key module for many language processing applications such as question answering, information extraction, sentiment analysis, and machine translation. To build an unrestricted semantic role labeler, the ﬁrst step is to develop a comprehensive proposition bank. However, creating such a bank is a costly enterprise, which has only been achieved for a handful of languages. In this paper, we describe a technique to build proposition banks for new languages using distant supervision. Starting from PropBank in English and loosely parallel corpora such as versions of Wikipedia in different languages, we carried out a mapping of semantic propositions we extracted from English to syntactic structures in Swedish using named entities. We trained a semantic parser on the generated Swedish propositions and we report the results we obtained. Using the CoNLL 2009 evaluation script, we could reach the scores of 52.25 for labeled propositions and 62.44 for the unlabeled ones. We believe our approach can be applied to train semantic role labelers for other resource-scarce languages. 
Hypernymy relation acquisition has been widely investigated, especially because taxonomies, which often constitute the backbone structure of semantic resources are structured using this type of relations. Although lots of approaches have been dedicated to this task, most of them analyze only the written text. However relations between not necessarily contiguous textual units can be expressed, thanks to typographical or dispositional markers. Such relations, which are out of reach of standard NLP tools, have been investigated in well speciﬁed layout contexts. Our aim is to improve the relation extraction task considering both the plain text and the layout. We are proposing here a method which combines layout, discourse and terminological analyses, and performs a structured prediction. We focused on textual structures which correspond to a well deﬁned discourse structure and which often bear hypernymy relations. This type of structure encompasses titles and sub-titles, or enumerative structures. The results achieve a precision of about 60%. 
 syntactic process as a derivation which is a tree  This paper proposes a method of incrementally constructing semantic representations. Our method is based on Steedman’s Combinatory Categorial Grammar (CCG), which has a transparent correspondence between the syn-  structure. Our method constructs a CCG derivation by applying operations used in incremental phrase structure parsing. Each intermediate data structure constructed by the operations represents partial information of some derivation. Our method obtains a  tax and semantics. In our method, a derivation for a sentence is constructed in an incremental fashion and the corresponding semantic representation is derived synchronously. Our method uses normal form CCG derivation. This is the difference between our approach and previous ones. Previous approaches use most left-branching derivation called incre-  semantic representation from the intermediate structure. Since the obtained semantic representations conform to the CCG semantic construction, we can expect that incremental semantic interpretation is realized by applying a CCG-based semantic analysis such as (Bos, 2008). This paper is organized as follows: Section  mental derivation, but they cannot process coordinate structures incrementally. Our method overcomes this problem.  2 brieﬂy explains Combinatory Categorial Grammar. Section 3 gives an overview of previous work of CCG-based incremental parsing and discusses  
 re-ranking based on multiple syntactic analyses. Un-  We describe a semantic role labeler with stateof-the-art performance and low computational requirements, which uses convolutional and time-domain neural networks. The system is designed to work with features derived from a dependency parser output. Various system options and architectural details are discussed.  fortunately, these approaches have a number of nontrivial limitations including the computational cost of the syntactic parsing and the sparse nature of the complex features on which they rely. This latter limitation is particularly critical since it leads to significant degradation in performance when the trained system is applied to texts from new domains.  Incremental experiments were run to explore the beneﬁts of adding increasingly more complex dependency-based features to the system; results are presented for both in-domain and out-of-domain datasets.  However, recent results using multilayer neural networks and pre-trained word embeddings have demonstrated high performance using a much smaller number of minimalist features. The architecture described by Collobert et al. [2011] com-  
Discourse deixis is a linguistic phenomenon in which pronouns have verbal or clausal, rather than nominal, antecedents. Studies have estimated that between 5% and 10% of pronouns in non-conversational data are discourse deictic. However, current coreference resolution systems ignore this phenomenon. This paper presents an automatic system for the detection and resolution of discourse-deictic pronouns. We introduce a two-step approach that ﬁrst recognizes instances of discourse-deictic pronouns, and then resolves them to their verbal antecedent. Both components rely on linguistically motivated features. We evaluate the components in isolation and in combination with two state-of-the-art coreference resolvers. Results show that our system outperforms several baselines, including the only comparable discourse deixis system, and leads to small but statistically signiﬁcant improvements over the full coreference resolution systems. An error analysis lays bare the need for a less strict evaluation of this task. 
Readability depends on many factors ranging from shallow features like word length to semantic ones like coherence. We introduce novel graph-based coherence features based on frequent subgraphs and compare their ability to assess the readability of Wall Street Journal articles. In contrast to Pitler and Nenkova (2008) some of our graph-based features are signiﬁcantly correlated with human judgments. We outperform Pitler and Nenkova (2008) in the readability ranking task by more than 5% accuracy thus establishing a new state-of-the-art on this dataset. 
 In this article, I present Japanese local and long-distance scrambling and restrictions to this phenomenon. I will argue that Japanese scrambling is too complex to be adequately represented with TAG. Instead, I will use a variant of TAG, namely TLMCTAG. Subsequently, I also will propose to regard other scrambling languages, such as German, or Russian, in complexity classes, which is basically driven by the derivational power of each TAG formalism. This classiﬁcation, though remains peripheric.  
This paper is concerned with theoretical considerations of commercial content analysis software, namely Linguistic Inquiry and Word Count (LIWC), developed by social psychologists at the University of Texas. LIWC is widely cited and forms the basis of many research papers from a range of disciplines. Here, LIWC is taken as an example of a context-independent, word-counting approach to text analysis, and the strengths and potential pitfalls of such a methodology are discussed. It is shown that text analysis software is constrained not only by its functions, but also by its underlying theoretical assumptions. The paper offers recommendations for good practice in software commercialisation and application, stressing the importance of transparency and acknowledgement of biases.  aims to illustrate the ways in which text analysis software is tightly bound to its theoretical basis, and the practical implications this can have on usage and performance. We find this a timely discussion due to the diverse range of researchers now turning to linguistic analysis software without necessarily understanding its construction. The paper is organised as follows. Section 2 outlines the LIWC system and its development, followed by some of its perceived theoretical assumptions in Section 3. Section 4 describes some previous experiments using LIWC in Franklin (2015). Section 5 revisits LIWC's theoretical assumptions, and offers suggestions for good practice in software commercialisation and application. Section 6 concludes the paper. 2 LIWC  
We propose to model a collection of documents by means of topic-speciﬁc domain dependency graphs (DDGs). We use LDA topic modeling to detect topics underlying a mixed-domain dataset and select topically pure documents from the collection. We aggregate counts of words and their dependency relations per topic, weigh them with Tf-Idf and produce a DDG by selecting the highest-ranked words and their dependency relations. We demonstrate an implementation of the approach on the task of identifying product aspects for aspect-oriented sentiment analysis. A large corpus of Amazon reviews is used to identify product aspects by applying syntactic ﬁltering to the DDG. Evaluation on a small set of cameras reviews demonstrate a good precision of our method. To our knowledge, this is the ﬁrst method that ﬁnds product-class speciﬁc aspects in mutli-domain collections in an unsupervised fashion. 
In this paper we describe a method to morphologically segment highly agglutinating and inﬂectional languages from Dravidian family. We use nested Pitman-Yor process to segment long agglutinated words into their basic components, and use a corpus based morpheme induction algorithm to perform morpheme segmentation. We test our method in two languages, Malayalam and Kannada and compare the results with Morfessor Categories-MAP. 
This paper presents initial research into the use of easy-read articles written for people with cognitive disabilities as a gold standard for the evaluation of the output of text simplification systems. We investigate the compliance of the easyread documents available on the Web with guidelines for development of easyread material, as well as their suitability as a gold standard for simple documents for two types of populations in particular: adult readers with autism and readers with mild intellectual disability (MID). The results indicate an overall good level of compliance with the guidelines and suggest that easy-read documents are a suitable resource for evaluation of accessible documents produced for adults with autism or MID. 
Part-of-Speech (POS) tagging is a key step in many NLP algorithms. However, tweets are difﬁcult to POS tag because there are many phenomena that frequently appear in Twitter that are not as common, or are entirely absent, in other domains: tweets are short, are not always written maintaining formal grammar and proper spelling, and abbreviations are often used to overcome their restricted lengths. Arabic tweets also show a further range of linguistic phenomena such as usage of different dialects, romanised Arabic and borrowing foreign words. In this paper, we present an evaluation and a detailed error analysis of stateof-the-art POS taggers for Arabic when applied to Arabic tweets. The accuracy of standard Arabic taggers is typically excellent (96-97%) on Modern Standard Arabic (MSA) text; however, their accuracy declines to 49-65% on Arabic tweets. Further, we present our initial approach to improve the taggers’ performance. By doing some improvements based on observed errors, we are able to reach 79% tagging accuracy. 
The vast information related to products and services available online, of both objective and subjective nature, can be used to provide contextualized suggestions and guidance to possible new customers. User feedback and comments left on different shopping websites, portals and social media have become a valuable resource, and text analysis methods have become an invaluable tool to process this kind of data. A lot of business use-cases have applied sentiment analysis in order to gauge people’s response to a service or product, or to support customers with reaching a decision when choosing such a product. Although methods and techniques in this area abound, the majority only address a handful of natural languages at best. In this paper, we describe a lexiconbased sentiment analysis method designed around the Persian language. An evaluation of the developed GATE pipeline shows an encouraging overall accuracy of up to 69%. 
The automatic development of terminological databases, especially in a standardized format, has a crucial aspect for multiple applications related to technical and scientiﬁc knowledge that requires semantic and terminological descriptions covering multiple domains. In this context, we have, in this paper, two challenges: the ﬁrst is the automatic extraction of terms in order to build a terminological database, and the second challenge is their normalization into a standardized format. To deal with these challenges, we propose an approach based on a cascade of transducers performed using CasSys tool of the Unitex linguistic platform that beneﬁts from both: the success of the rule-based approach for the extraction of terms, and the performance of the TMF standard for the representation of terms. We have tested and evaluated our approach on an Arabic scientiﬁc and technical corpus for the Elevator domain and the results are very encouraging. 
 • similarities in URLs;  This paper presents a statistical model for measuring structural similarity between webpages from bilingual websites. Starting from basic assumptions we derive the model and propose an algorithm to estimate its parameters in unsupervised manner. Statistical approach appears to benefit the structural similarity measure: in the task of distinguishing parallel webpages from bilingual websites our languageindependent model demonstrates an Fscore of 0.94–0.99 which is comparable to the results of language-dependent methods involving content similarity measures. 
Community Question Answering websites (CQA) have a growing popularity as a way of providing and searching of information. CQA attract users as they provide a direct and rapid way to ﬁnd the desired information. As recognizing good questions can improve the CQA services and the user’s experience, the current study focuses on question quality instead. Speciﬁcally, we predict question quality and investigate the features which inﬂuence it. The inﬂuence of the question tags, length of the question title and body, presence of a code snippet, the user reputation and terms used to formulate the question are tested. For each set of dependent variables, Ridge regression models are estimated. The results indicate that the inclusion of terms in the models improves their predictive power. Additionally, we investigate which lexical terms determine high and low quality questions. The terms with the highest and lowest coefﬁcients are semantically analyzed. The analysis shows that terms predicting high quality are terms expressing, among others, excitement, negative experience or terms regarding exceptions. Terms predicting low quality questions are terms containing spelling errors or indicating off-topic questions and interjections. 
In this paper we present our approach to automatically identify the subjectivity, polarity and irony of Italian Tweets. Our system which reaches and outperforms the state of the art in Italian is well adapted for different domains since it uses abstract word features instead of bag of words. We also present experiments carried out to study how Italian Sentiment Analysis systems react to domain changes. We show that bag of words approaches commonly used in Sentiment Analysis do not adapt well to domain changes. 
 efit from Wikipedia articles. Therefore, names of  people, which are part of proper nouns, appear fre-  Transducers namely transducer cascades are  quently in the Arabic Wikipedia. More efforts by  used in several NLP-applications such as Arabic named entity recognition (ANER). To experiment and evaluate an ANER process, a weight coverage corpus is necessary. In this paper, we propose an ANER method based on transducer cascade. The proposed transducer cascade is generated with the CasSys tool integrated in Unitex linguistic platform. The experimentation of our method is done on a Wikipe-  NLP-researchers are concentrated on this type. Person names are considered as the most challenging task for Arabic. In this context, our objective is to propose, using the rule-based approach, a transducer cascade for the recognition of personality’s names. In this approach, we benefit from the robustness of transducers and exploit the free resource, Wikipedia.  dia corpus. The Wikipedia text format is ob-  The recognition requires the identification of dic-  tained with Kiwix tool. The experiment results  tionaries, a list of trigger words and extraction  are satisfactory based on calculated measures.  rules allowing the development of a set of trans-  Keywords: Cascade of transducers, Wikipe- dia, Arabic named entities, Unitex, CasSys  ducers acting on the corpus with a certain logic. The present paper is composed of six sections. The second section presents previous work de-  1. Introduction  scribing the developed systems for the recognizing of the personality names. The third section is  Transducers can play an important role in the Information Extraction (IE) namely in the Named Entity Recognition (NER). At the same time, transducers can extract and classify the Arabic Named Entity (ANE). Generally, the use of transducers is realized in well defined succession that  dedicated to describing the categorization of person’s names. The fourth section devoted to detail the proposed method that is implemented by using CasSys system. The experiment is presented and evaluated in section five. Finally, we give a conclusion and some perspectives.  is called cascade (Friburger and Maurel, 2004). In fact, the identification of necessary transduc-  2.  Previous Work  ers is not an easy task because several linguistic There are several work treating the ANER based  phenomenacan interact (Shaalan, 2014; Ben Mes- on several approaches among which we cite the  mia and al, 2015).  work of (Shaalan and Raza, 2007). In this work,  The free resource Wikipedia is an important in- the authors proposed an ANER system based on  formation source. Indeed, several text processing the rule-based approach. This system called  48 Proceedings of Recent Advances in Natural Language Processing, pages 48–54, Hissar, Bulgaria, Sep 7–9 2015.  PERA is composed of three components: gazet-  Several systems based on cascades were devel-  teers, local grammars and a filtration mechanism. oped in NLP that touch essentially the following  PERA is applied to the ACE and ATB datasets. domains: parsing, information extraction and  In (Mesfar, 2007), the author developed a sys- translation. Among the systems constracted for  tem identifying ANE of many types such as per- the IE task, we cite the following work.  son names. This system consists of a tokenizer, a In EU project FACILE, (Ciravegna and Lavelli,  morphological analyzer and a NE finder. The sys- 1999) implemented a module based on three  tem is evaluated by using the of news corpus ex- transducers cascades. These cascades contain  tracted from le journal “Le monde diplomatique”. transducers representing respectively empirical,  In (Elsebai et al, 2009), the authors proposed a regular and default rules.  rule-based system that integrates pattern matching  CasEN, the system developed by (Maurel and  with morphological analysis to extract Arabic per- al., 2011) uses lexical resources and transducers  son names. This system is evaluated by using acting together on texts by insertions, deletions or  news articles extracted from Aljazeera website. substitutions.  In (Fehri and al., 2011), authors developed a For Arabic, (Ben Mesmia and al, 2015) devel-  rule-based system to recognize ANE for sport’s oped a transducer cascade allowing the recogni-  domain such as place names and player’s names. tion of ANE more precisely the dates. This cas-  This system is composed of a set of dictionaries, cade is generated by the CasSys that is module  syntactic patterns and transducers implemented available under the Unitex platform.  with the linguistic platform NooJ.  In (Aboaoga and Aziz, 2013), the authors intro- 3. Typology of Arabic Person’s Names  duced a rule-based system that extracts Arabic person names. The system is composed of three steps: the preprocessing (tokenization, data cleaning and sentence splitting), the automatic ANE tagging and the application of rules to the Arabic texts in order to extract ANEs that do not exist in the built dictionaries. The domains covered by this system are sports, politics and economics.  The Arabic names may have variations related to origin of country, religion, culture, level of formality and even personal preference. In this section, we present firstly our study corpus. Secondly, we give the categorization of person names. We explain also phenomena that are related to their recognition.  In (Elsebai, 2008), the author developed a sys-  3.1 Corpus of Study  tem adopting statistical approach for ANER. This system allows the recognition of Arabic proper names using heuristics. Heuristics based on a set of key-words rather than complex grammars and statistical techniques. The system is evaluated by using news articles extracted from the Aljazeera television website. In (Shaalan and Oudah, 2014), the authors pro-  The corpus of study was collected from Arabic Wikipedia through Arabic kiwix1 tool. It regroups a number of texts from 19 Arabic countries and contains text files for a cumulative 79 659 tokens. This corpus allows us to identify the forms that will be transformed into extraction rules and transformed later in transducers.  posed a system based on hybrid approach. This system, which is capable of recognizing 11 types of Arabic named entities such as person names, is applied to ANERcorp standard dataset. According the study made by Shaalan (2014), systems which are developed for the ANER, are essentially based on restraint domains. Namely in the NER, the use of transducer cascade is very frequent. A cascade is defined as a succession of transducers applied to text in a specific order to convert or extract patterns. Each transducer of the cascade uses the results of the previous transducer (Maurel and al., 2009).  3.2 Categorization of Person’s Names In general, an Arabic name can contain five parts, which follow no particular order: the ism, kunya, nasab, laqab, and nisba (Shaalan, 2014). The ism is the first name. These are the names given to children at their birth. Male isms are such names as “`bd allah2 / Abdullah”, “`aadl / Adel”, “Hsyn / Hussein”. Men’s isms are sometimes preceded by one of the attributes of Allah such as “’aaHmd / Ahmed”, “mHmwd / Mahmoud” but this practice is declining, especially in areas influenced by Western practices, such as Lebanon,  
This paper presents a Constraint Grammarbased pedagogical proofing tool for Danish. The system recognizes not only spelling errors, but also grammatical errors in otherwise correctly spelled words, and categorizes errors for WORD-integrated pedagogical comments. Possible spelling corrections are prioritized from context, and grammatical corrections generated by a morphological module. The system uses both phonetic similarity measures and traditional Levenshtein-distances, and has a special focus on compounding/splitting errors common in modern Danish. As a classical spell-checker DanProof achieves F-Scores over 95, and F=88 if compounding correction is included. With the maximal set of error types, 2/3 of all errors are found in school essays, and precision is 91.7%. 
This article tackles the Authorship Attribution task according to the language independence issue. We propose an alternative of variable length character n-grams features in supervised methods: maximal repeats in strings. When character ngrams are by essence redundant, maximal repeats are a condensed way to represent any substring of a corpus. Our experiments show that the redundant aspect of n-grams contributes to the efﬁciency of character-based techniques. Therefore, we introduce a new way to weight features in vector based classiﬁer by introducing n-th order maximal repeats (maximal repeats detected in a set of maximal repeats). The experimental results show higher performance with maximal repeats, with less data than n-grams based approach (approximately divided by a factor of 10). 
Event Detection (ED), one aspect of Information Extraction, involves identifying instances of speciﬁed types of events in text. Much of the research on ED has been based on the speciﬁcations of the 2005 ACE [Automatic Content Extraction] event task1, and the associated annotated corpus. However, as the event instances in the ACE corpus are not evenly distributed, some frequent expressions involving ACE events do not appear in the training data, adversely affecting performance. In this paper, we demonstrate the effectiveness of a Pattern Expansion technique to import frequent patterns extracted from external corpora to boost ED performance. The experimental results show that our pattern-based system with the expanded patterns can achieve 70.4% (with 1.6% absolute improvement) F-measure over the baseline, an advance over current state-of-the-art systems. 
Event Detection (ED) is an Information Extraction task which involves identifying instances of speciﬁed types of events in text. Most recent research on Event Detection relies on pattern-based or featurebased approaches, trained on annotated corpora, to recognize combinations of event triggers, arguments, and other contextual information. These combinations may each appear in a variety of linguistic forms. Not all of these event expressions will have appeared in the training data, thus adversely affecting ED performance. In this paper, we demonstrate the effectiveness of Dependency Regularization techniques to generalize the patterns extracted from the training data to boost ED performance. The experimental results on the ACE 2005 corpus show that our pattern-based system with the expanded patterns can achieve 70.49% (with 2.57% absolute improvement) F-measure over the baseline, which advances the state-of-the-art for such systems. 
Authorship analysis is an important task for different text applications, for example in the field of digital forensic text analysis. Hence, we propose an authorship analysis method that compares the average similarity of a text of unknown authorship with all the text of an author. Using this idea, a text that was not written by an author, would not exceed the average of similarity with known texts and only the text of unknown authorship would be considered as written by the author, if it exceeds the average of similarity obtained between texts written by him. The experiments were realized using the data provided in PAN 2014 competition for Spanish articles for the task of authorship verification. We realize experiments using different similarity functions and 17 linguistics features. We analyze the results obtained with each pair function-features against the baseline of the competition. Additionally, we introduce a text filtering phase that delete all the sample text of an author that are more similar to the samples of other author, with the idea to reduce confusion or non-representative text, and finally we analyze new experiments to compare the results with the data obtained without filtering. Keywords: Authorship detection, Author identification, similarity measures, linguistic features. 
Efﬁcient music information retrieval (MIR) require to have meta information about music along with content based information in the knowledge base. Discussion forums on music are rich sources of information gathered from a wider audience. Taking into consideration the nature of text in these web resources, the yield of relation extraction is quite dependent on resolving the entity references in the document. Among the few music forums dealing with Indian classical music, rasikas.org (rasikas, 2015) having rich information about artistes, raga and other music concepts is taken for our study. The forum posts generally contain anaphoric references to the main topic of the thread or any other entity in the discourse. In this paper we focus on coreference resolution for short discourse noisy text like that of forum posts. Since grammatical roles capture relation between mentions in a discourse, those features extracted from dependency parsing are widely explored along with semantic compatibility feature. On investigation of issues, the need for integrating known dependencies between features emerged. A Bayesian network with predeﬁned network structure is evaluated, since a Bayesian belief network enacts a probabilistic rule based system. To the extent possible the superior behaviour of Bayesian network over SVM is analysed. 
In this paper we investigate how readability varies between texts originally written in English and texts translated into English. For quantiﬁcation, we analyze several factors that are relevant in assessing readability – shallow, lexical and morpho-syntactic features – and we employ the widely used Flesch-Kincaid formula to measure the variation of the readability level between original English texts and texts translated into English. Finally, we analyze whether the readability features have enough discriminative power to distinguish between originals and translations. 
We present ongoing work in linguistic processing of hashtags in Twitter text, with the goal of supplying normalized hashtag content to be used in more complex natural language processing (NLP) tasks. Hashtags represent collectively shared topic designators with considerable surface variation that can hamper semantic interpretation. Our normalization scripts allow for the lexical consolidation and segmentation of hashtags, potentially leading to improved semantic classification. 
Brown clustering, an unsupervised hierarchical clustering technique based on ngram mutual information, has proven useful in many NLP applications. However, most uses of Brown clustering employ the same default conﬁguration; the appropriateness of this conﬁguration has gone predominantly unexplored. Accordingly, we present information for practitioners on the behaviour of Brown clustering in order to assist hyper-parametre tuning, in the form of a theoretical model of Brown clustering utility. This model is then evaluated empirically in two sequence labelling tasks over two text types. We explore the dynamic between the input corpus size, chosen number of classes, and quality of the resulting clusters, which has an impact for any approach using Brown clustering. In every scenario that we examine, our results reveal that the values most commonly used for the clustering are sub-optimal. 
Determining the temporal order of events in a text is difﬁcult. However, it is crucial to the extraction of narratives, plans, and context. We suggest that a simple, established framework of tense and aspect provides a viable model for ordering a subset of events and times in a given text. Using this framework, we investigate extracting features that represent temporal information and integrate these in a machine learning approach. These features improve event-event ordering. 
Linguistic annotation is time-consuming and expensive. One common annotation task is to mark entities – such as names of people, places and organisations – in text. In a document, many segments of text often contain no entities at all. We show that these segments are worth skipping, and demonstrate a technique for reducing the amount of entity-less text examined by annotators, which we call “preempting”. This technique is evaluated in a crowdsourcing scenario, where it provides downstream performance improvements for the same size corpus. 
Product review mining is an important task that can beneﬁt both businesses and consumers. Lately a number of models combining collaborative ﬁltering and content analysis to model reviews have been proposed, among which the Hidden Factors as Topics (HFT) model is a notable one. In this work, we propose a new model on top of HFT to separate product properties and aspects. Product properties are intrinsic to certain products (e.g. types of cuisines of restaurants) whereas aspects are dimensions along which products in the same category can be compared (e.g. service quality of restaurants). Our proposed model explicitly separates the two types of latent factors but links both to product ratings. Experiments show that our proposed model is effective in separating product properties from aspects. 
Summarizing opinions expressed in online forums can potentially beneﬁt many people. However, special characteristics of this problem may require changes to standard text summarization techniques. In this work, we present our initial attempt at extractive summarization of opinionated online forum threads. Given the nature of user generated content in online discussion forums, we hypothesize that besides relevance, text quality and subjectivity also play important roles in deciding which sentences are good summary sentences. We therefore construct an annotated corpus to facilitate our study of extractive summarization of online discussion forums. We deﬁne a set of features to capture relevance, text quality and subjectivity, and empirically test their usefulness in choosing summary sentences. Using unpaired Student’s t-test, we ﬁnd that sentence length and number of sentiment words have high correlations with good summary sentences. Finally we propose some simple modiﬁcations to a standard Integer Linear Programming based summarization framework to incorporate these features. 
We investigate in this paper the degree of overlap between synonym sets of translated word pairs across three languages: French, English and Romanian. We use for this purpose a French Synonym Dictionary, a Romanian Synonym Dictionary, Princeton’s WordNet and Google Translate API. We build a database containing pairs of (translated) words from the three languages, along with their corresponding synonym sets. We use it in order to gain insight into the synonym overlap for each language pair, and thus, into their degree of common concept lexicalization, by various queries. While the overall percentage of common synonyms is (expectedly) quite small (averaging ~6% across all language pairs), the percentage of hard synonyms pairs (pairs that have at least one common synonym), reaching ~62%, is signiﬁcant. This is encouraging for further use of this special kind of word translated pairs in tasks such as automatic enhancement of lexical databases (such as WordNet) for less resourced languages such as Romanian, based on corresponding English versions of these lexical databases. Another interesting query topic was obtaining distributions of hard synonym pairs, function of their part of speech: hard synonyms were most frequent among verbs for English, and among adjectives for Romanian and French. Keywords: cross-lingual synonyms, French, Romanian, database 
This paper investigates the use of semantic preferences for ontology population. It draws on a new resource, the Pattern Dictionary of English Verbs, which lists semantic categories expected in each syntactic slot of a verb pattern. Knowledge of semantic preferences is used to drive and control bootstrapped pattern extraction techniques on the EnClueWeb09 corpus with the aim of identifying common nouns belonging to twelve semantic types. Evaluation reveals that syntactic patterns perform better than lexical and surface patterns, at the same time raising issues about assessing ontology population candidates out of context. 
The present research exploits the large amount of linguistic resources developed into the Lexicon-grammar paradigm in the domain of the Opinion Mining. Grounded on the Semantic Predicates theory, the proposed system is able to automatically match the syntactic structures selected by special classes of verbs, indicating positive or negative Sentiment, Opinion or Physical acts, with the semantic frames evoked by the same lexical items. This methods has been tested on a large dataset composed of short texts, such as tweets and news headings. 
In this paper we propose an approach for identifying syntactic behaviours related to lexical items and linking them to the meanings. This approach is based on the analysis of the textual content presented in LMF normalized dictionaries by means of Definition and Context classes. The main particularity of these contents is their large availability and their semantically control due to their attachment to the meanings, which promotes the effective links between the syntactic behaviours and the meaning. In order to test the performance of the proposed approach, we tested it on an available Arabic LMF normalized dictionary. The experiment treats 9,800 verbs and allows us to evaluate the identified syntactic behaviours as well as their links to the meanings. 
Deﬁnition Extraction (DE) is the task to extract textual deﬁnitions from naturally occurring text. It is gaining popularity as a prior step for constructing taxonomies, ontologies, automatic glossaries or dictionary entries. These ﬁelds of application motivate greater interest in well-formed encyclopedic text from which to extract deﬁnitions, and therefore DE for academic or lay discourse has received less attention. In this paper we propose a weakly supervised bootstrapping approach for identifying textual deﬁnitions with higher linguistic variability than the classic encyclopedic genus-et-differentia deﬁnition, and take the domain of Natural Language Processing as a use case. We also introduce a novel set of features for DE and explore their relevance. Evaluation is carried out on two datasets that reﬂect opposed ways of expressing deﬁnitional knowledge. 
This paper contributes a joint embedding model for predicting relations between a pair of entities in the scenario of relation inference. It differs from most standalone approaches which separately operate on either knowledge bases or free texts. The proposed model simultaneously learns low-dimensional vector representations for both triplets in knowledge repositories and the mentions of relations in free texts, so that we can leverage the evidence both resources to make more accurate predictions. We use NELL to evaluate the performance of our approach, compared with cutting-edge methods. Results of extensive experiments show that our model achieves signiﬁcant improvement on relation extraction. 
We explore the impact of adding distributional knowledge to a state-of-the-art coreference resolution system. By integrating features based on word and context expansions from a distributional thesaurus (DT), automatically mined IS-A relationships and shallow syntactical clues into the Berkeley system (Durrett and Klein, 2013), we are able to increase its F1 score on bridging mentions, i.e. coreferent mentions with non-identical heads, by 8.29 points. Our semantic features improve over the Web-based features of Bansal and Klein (2012). Since bridging mentions are a hard but infrequent class of coreference, this leads to merely small improvements in the overall system. 
The problem of classifying text with respect to belonging to a document or a meta-document is formulated and its application areas are proposed. An algorithm is proposed for document classification tasks where counts of words is insufficient do differentiate between such abstract classes of text as metalanguage and object-level. We extend the parse tree kernel method from the level of individual sentences towards the level of paragraphs, based on anaphora, rhetoric structure relations and communicative actions linking phrases in different sentences. Tree kernel learning technique is applied to these extended trees to leverage of additional discourse-related information. We evaluate our approach in the domain of action-plan documents. 
Vector-space models derived from corpora are an effective way to learn a representation of word meaning directly from data, and these models have many uses in practical applications. A number of unsupervised approaches have been proposed to automatically learn representations of word senses directly from corpora, but since these methods use no information but the words themselves, they sometimes miss distinctions that could be possible to make if more information were available. In this paper, we present a general framework that we call context enrichment that incorporates external information during the training of multi-sense vector-space models. Our approach is agnostic as to which external signal is used to enrich the context, but in this work we consider the use of translations as the source of enrichment. We evaluated the models trained using the translation-enriched context using several similarity benchmarks and a word analogy test set. In all our evaluations, the enriched model outperformed the purely word-based baseline soundly. 
This article describes a method which allows acquiring artifact nouns in French automatically by extracting predicateargument structures. Two strategies are presented: the supervised strategy and the semi-supervised strategy. In the supervised method, the semantic classes of artifact nouns are recognized by identifying the predicate-argument structures with the syntactic patterns of the given predicates. In the semi-supervised method, the extraction of predicate-argument structures is carried out from a semantic class of artifact nouns given in advance. The predicate candidates obtained from extracted predicate-argument structures are then intersected. Next, the syntactic patterns of predicates are automatically learned by probabilistic calculation. With the acquired predicates and the learned syntactic patterns, more artifact nouns are identiﬁed. 
This paper describes the implementation, improvement and evaluation of the machine translation (MT) system proposed by Jackov (2014) when used as a feature-rich part-ofspeech (POS) tagger for Bulgarian. The system does not rely on POS tagging for morphological disambiguation. Instead, all ambiguities are considered in parsing hypotheses that are scored and the best one is used for tagging. The system does not use automatic training on annotated corpora. Manually and automatically compiled linguistic resources are used for hypothesis derivation and scoring. BulTreeBank manually annotated corpus (Simov and Osenova, 2004) was used for evaluation, error detection and improvement. 
In this paper, we show an approach to extracting different types of constraint rules from a dependency treebank. Also, we show an approach to integrating these constraint rules into a dependency data-driven parser, where these constraint rules inform parsing decisions in speciﬁc situations where a set of parsing rule (which is induced from a classiﬁer) may recommend several recommendations to the parser. Our experiments have shown that parsing accuracy could be improved by using different sets of constraint rules in combination with a set of parsing rules. Our parser is based on the arc-standard algorithm of MaltParser but with a number of extensions, which we will discuss in some detail. 
The paper reports work on collecting and annotating code-mixed English-Hindi social media text (Twitter and Facebook messages), and experiments on automatic tagging of these corpora, using both a coarse-grained and a ﬁne-grained part-ofspeech tag set. We compare the performance of a combination of language speciﬁc taggers to that of applying four machine learning algorithms to the task (Conditional Random Fields, Sequential Minimal Optimization, Naïve Bayes and Random Forests), using a range of different features based on word context and wordinternal information. 
In this paper we present an approach for analysis of sentiments and emotions in image tagging using SentiWordNet as an external linguistic resource of emotional words. Our aim is to design and implement algorithms that assess the emotions and polarity given a set of image tags. The approach is not limited to object analysis only (considering informational keywords) but deals with the involvement tags and employs some techniques used for sentiment analysis in social networks. We consider the issue of tag sense disambiguation when image keywords are mapped to SentiWordNet. The Lesk algorithm helps to identify correctly the meaning of about 50% of the ambiguous single keywords of 200 images. The total number of tags we process is about 10,000. Calculating a "sentiment score" for each im- age, the system classifies images into three classes (positive, negative, neutral). These classes are compared to emotional assessments done (i) by humans and (ii) by training of a SVM classifier that provides the baseline of 69.7% precision, 29.9% recall and 41.8% Fmeasure. Our approach works with 63.53% precision, 58.7% recall and 61.02% Fmeasure. The experiments are performed using the annotations of the industrial auto-tagging platform Imagga that identifies automatically image objects with high precision. 
We present a system for ﬁne-grained sentiment analysis in Bulgarian movie reviews. As this is pioneering work for this combination of language and sentiment granularity, we create suitable, freely available resources: a dataset of movie reviews with ﬁne-grained scores, and a sentiment polarity lexicon. We further compare experimentally the performance of classiﬁcation, regression and ordinal regression in a 3-way, 5-way and 11-way classiﬁcation setups, using as features not only the text from the reviews, but also contextual information in the form of metadata, e.g., movie length, director, actors, genre, country, and various scores: IMDB, Cinexio, and user-average. The results show that adding contextual information yields strong performance gains. 
There tends to be a substantial proportion of reviews that include explicit textual comparisons between the reviewed item and another product. To the extent that such comparisons can be captured reliably by automatic means, they can provide an extremely helpful input to support a process of choice. As the small amount of available training data limits the development of robust systems to automatically detect comparisons, this paper investigates how to use semi-supervised strategies to expand a small set of labeled sentences. Speciﬁcally, we use structural alignment, a method that starts out from a seed set of manually annotated data and ﬁnds similar unlabeled sentences to which the labels can be projected. We present several adaptations of the method to our task of comparison detection and show that adding the found expansion sentences slightly improves over a non-expanded baseline in low-resource settings, i.e., when a very small amount of training data is available. 
In this article we present the result of the recent research in the recognition of Polish temporal expressions. The temporal information extracted from the text plays major role in many information extraction systems, like question answering, event recognition or discourse analysis. We prepared a broad description of Polish temporal expressions, called PLIMEX. It is based on the state-of-the-art solutions for English, mostly TimeML speciﬁcation. This solution can be used for the extraction of events and their attributes, in order to anchor events in time and to reason about the persistence of events. We prepared the annotation guidelines and we annotated all documents in Polish Corpus of Wrocław University of Technology (KPWr) using our speciﬁcation. Here we describe results achieved by Liner2 machine learning system, adapted to recognise Polish temporal expressions. 
We developed a system to extract Japanese anime-related words, i.e., Japanese NEs (named entities) in the anime-related domain. Since the NEs in the area, such as the titles of anime or the names of characters, were domain-speciﬁc, we started by building a tagged corpus and then used it for the experiments. We examined to see if the existing corpora were useful to improve the results. The experiments conducted using Conditional Random Fields showed that the effect of domain adaptation varied according to the genres of the corpora, but the ﬁltering of the source data not only reduced the time for training but also assisted in the domain adaptation work. 
 This paper revisits the work of (Kuncham et al., 2015) which developed a statistical sandhi splitter (SSS) for agglutinative languages that was tested for Telugu and Malayalam languages. Handling compound words is a major challenge for Natural Language Processing (NLP) applications for agglutinative languages. Hence, in this paper we concentrate on testing the effect of SSS on the NLP applications like Machine Translation, Dialogue System and Anaphora Resolution and show that the accuracy of these applications is consistently improved by using SSS. We shall also discuss in detail the performance of SSS on these applications.  
Many events referred to on Twitter are of a periodic nature, characterized by roughly constant time intervals in between occurrences. Examples are annual music festivals, weekly television programs, and the full moon cycle. We propose a system that can automatically identify periodic events from Twitter in an unsupervised and open-domain fashion. We ﬁrst extract events from the Twitter stream by associating terms that have a high probability of denoting an event to the exact date of the event. We compare a timelinebased and a calendar-based approach to detecting periodic patterns from the event dates that are connected to these terms. After applying event extraction on over four years of Dutch tweets and scanning the resulting events for periodic patterns, the calendar-based approach yields a precision of 0.76 on the 500 top-ranked periodic events, while the timeline-based approach scores 0.63. 
Sentiment analysis from a text requires amongst others having a polarity lexical resource. We designed LikeIt, a GWAP (Game With A Purpose) that allows to attribute a positive, negative or neutral value to a term, and thus obtain a resulting polarity for most of the terms of the freely available lexical network of the JeuxDeMots project. We present a quantitative analysis of data obtained through our approach, together with the comparison method we developed to validate them qualitatively. 
In medical imaging domain, digitized data is rapidly expanding Therefore it is of major interest for radiologists to be able to do an efficient and accurate extraction of imaging and clinical data (radiology reports) which are essential for a rigorous diagnosis and for a better management of patients. In daily practice, radiology reports are written using a nonstandardized language which is often ambiguous and noisy. The queries of radiological images can be greatly facilitated through textual indexing of associated reports. In order to improve the quality of the analysis of such reports, it is desirable to specify an index enlargement algorithm based on spreading activations over a general lexical-semantic network. In this paper, we present such an algorithm along with its qualitative evaluation. 
We present extensive evaluations comparing the performance of taxonomy-based and corpus-based approaches on SimLex999. The results conﬁrm our hypothesis that taxonomy-based approaches are more suitable to identify similarity. We introduce two new measures of evaluation that show that all measures perform well on a coarse-grained evaluation and that it is not always clear which approach is most suitable when a similarity score is used as a threshold. This leads us to conclude that the inferior performance of corpus-based approaches may not (always) matter. 
In this paper, we explore the IBM Model with a 0-norm prior to the semantic parsing which parses a sentence to its corresponding meaning representation, and compare two supervised probabilistic Combinatory Categorial Grammar (PCCG) online learning approaches that are Uniﬁcation-Based Learning (UBL) method and Factored Uniﬁcation-Based Learning (FUBL) one. Specially, we extend manually GeoQuery and ATIS datasets from English to Chinese pinyinformat string. The experiment on such benchmark datasets in both English and Chinese with two different meaning representations (i.e., lambda-calculus and variable-free expressions) demonstrates that both methods adopted this IBM Model with 0-norm outperform trivially those that used the IBM Model without 0norm, and also shows small improvements of around 0.1% ∼ 0.7% of F1 for the two algorithms on nearly all conditions. 
Existing semantic parsing research has steadily improved accuracy on a few domains and their corresponding meaning representations. In this paper, we present a novel supervised semantic parsing algorithm, which includes the lexicon extension and the syntactic supervision. This algorithm adopts a large-scale knowledge base from the open-domain Freebase to construct efﬁcient, rich Combinatory Categorial Grammar (CCG) lexicon in order to supplement the inadequacy of its manually-annotated training dataset in the small closed-domain while allows for the syntactic supervision from the dependency-parsed sentences to penalize the ungrammatical semantic parses. Evaluations on both benchmark closed-domain datasets demonstrate that this approach learns highly accurate parser, whose parsing performance beneﬁts greatly from the open-domain CCG lexicon and syntactic constraint. 
Non-standard language as it appears in user-generated content has recently attracted much attention. This paper proposes that non-standardness comes in two basic varieties, technical and linguistic, and develops a machine-learning method to discriminate between standard and nonstandard texts in these two dimensions. We describe the manual annotation of a dataset of Slovene user-generated content and the features used to build our regression models. We evaluate and discuss the results, where the mean absolute error of the best performing method on a three-point scale is 0.38 for technical and 0.42 for linguistic standardness prediction. Even when using no language-dependent information sources, our predictor still outperforms an OOVratio baseline by a wide margin. In addition, we show that very little manually annotated training data is required to perform good prediction. Predicting standardness can help decide when to attempt to normalise the data to achieve better annotation results with standard tools, and provide linguists who are interested in nonstandard language with a simple way of selecting only such texts for their research. 
In this paper we describe a semi-automated approach to extend morphological lexicons by deﬁning the prediction of the correct inﬂectional paradigm and the lemma for an unknown word as a supervised ranking task trained on an already existing lexicon. While most ranking approaches rely only on heuristics based on a single information source, our predictor uses hundreds of features calculated on the candidate stem, corpus evidence and statistics calculated from the existing lexicon. On the example of the Croatian language we show that our approach signiﬁcantly outperforms a heuristic-based baseline, yielding correct candidates in 77% of cases on the ﬁrst position and in 95% of cases on the ﬁrst ﬁve positions. 
While there is a strong intuition that word alignments (e.g. synonymy, hyperonymy) play a relevant role in recognizing textto-text semantic inferences (e.g. textual entailment, semantic similarity), this intuition is often not reﬂected in the system performances and there is a general need of a deeper comprehension of the role of lexical resources. This paper provides an empirical analysis of the dependencies between data-sets, lexical resources and algorithms that are commonly used in text-to-text inference tasks. We deﬁne a resource impact index, based on lexical alignments between pairs of texts, and show that such index is signiﬁcantly correlated with the performance of different textual entailment algorithms. The result is an operational, algorithm-independent, procedure for predicting the performance of a class of available RTE algorithms. 
We present a work to evaluate the hypothesis that automatic evaluation metrics developed for Machine Translation (MT) systems have signiﬁcant impact on predicting semantic similarity scores in Semantic Textual Similarity (STS) task for English, in light of their usage for paraphrase identiﬁcation. We show that different metrics may have different behaviors and signiﬁcance along the semantic scale [0-5] of the STS task. In addition, we compare several classiﬁcation algorithms using a combination of different MT metrics to build an STS system; consequently, we show that although this approach obtains state of the art result in paraphrase identiﬁcation task, it is insufﬁcient to achieve the same result in STS. 
We present a study of Native Language Identiﬁcation (NLI) using data from learners of Norwegian, a language not yet used for this task. NLI is the task of predicting a writer’s ﬁrst language using only their writings in a learned language. We ﬁnd that three feature types, function words, part-of-speech n-grams and a hybrid part-of-speech/function word mixture n-gram model are useful here. Our system achieves an accuracy of 79% against a baseline of 13% for predicting an author’s L1. The same features can distinguish non-native writing with 99% accuracy. We also ﬁnd that part-of-speech n-gram performance on this data deviates from previous NLI results, possibly due to the use of manually post-corrected tags. 
Conditional Random Fields (CRFs) have been proven to be very useful in many sequence labelling tasks from the ﬁeld of natural language processing, including named entity recognition (NER). The advantage of CRFs over other statistical models (like Hidden Markov Models) is that they can utilize a large set of features describing a sequence of observations. On the other hand, CRFs potential function is deﬁned as a linear combination of features, what means, that it cannot model relationships between combinations of input features and output labels. This limitation can be overcome by deﬁning the relationships between atomic features as complex features before training the CRFs. In the paper we present the experimental results of automatic generation of complex features for the named entity recognition task for Polish. A rule-induction algorithm called RIPPER is used to generate a set of rules which are latter transformed into a set of complex features. The extended set of features is used to train a CRFs model. 
The extraction of domain terminology is a task that is increasingly used for different application processes of natural language such as the information recovery, the creation of specialized corpus, question-answering systems, the creation of ontologies and the automatic classification of documents. This task of the extraction of domain terminology is generally performed by generating patterns. In literature we could find that the patterns which are used to extract such terminology often change from one domain to another, it means the intervention of human experts to the generation and validation of these patterns. This article deals with a methodology for automatic obtaining patterns (Basic Patterns and Definitory Verbal Patterns) for extracting domain terminology and minimizing the manual work of the experts. The obtained methodology was evaluated in the computer science domain obtaining a 97 percent in the case of the values of the basic patterns and a 98 percent of the definitory verbal patterns. Then the methodology was tested in three other domains with similar results, Agricultural Engineering (a 96 percent of the basic patterns and a 97 percent of the definitory verbal patterns), Veterinary Medicine (98% of the basic pattern and the definitory verbal patterns) and Agronomy (96% of the basic pattern and the definitory verbal patterns), showing that methodology can be applied in any specialty curriculum documents. 
Multi-word expressions evade a closed deﬁnition. Linguists and computational linguists rely on intuition or build lists of MWE types; while practical, that is scientiﬁcally and aesthetically unsatisfying. Without presuming to solve a daunting theoretical problem, we propose a decision procedure which steers a lexicographer toward acceptance or rejection of an N-gram as a lexical unit: a decision tree classiﬁes N-grams as MWE or not MWE. It will succeed if it agrees with the native speakers’ judgment. We need a small, linguistically credible set of features, to contend with the multiplicity of adequate trees. Decision tree induction works with a ﬁxed set of annotated classiﬁcation examples, but the lexical material for MWE recognition is too large to make annotation feasible. We rely on small-scale statistically signiﬁcant sampling, and on intuition. Of a few decision trees produced by informed trial and error, we select one we consider best in our circumstances. That tree, deployed in a large-scale wordnet construction project, allowed us to gather dependable statistics on its usefulness in lexicographers’ work. Our goal: systematic expansion of a wordnet by tens of thousands of MWEs in a manner as free of personal biases as possible. 
 Some languages do not have enough labeled data to obtain good discourse parsing, specially in the relation identiﬁcation step, and the additional use of unlabeled data is a plausible solution. A workﬂow is presented that uses a semi-supervised learning approach. Instead of only a predeﬁned additional set of unlabeled data, texts obtained from the web are continuously added. This obtains near human perfomance (0.79) in intra sentential rhetorical relation identiﬁcation. An experiment for English also shows improvement using a similar workﬂow. 
Recently, Web forums have been invaded by opinion manipulation trolls. Some trolls try to inﬂuence the other users driven by their own convictions, while in other cases they can be organized and paid, e.g., by a political party or a PR agency that gives them speciﬁc instructions what to write. Finding paid trolls automatically using machine learning is a hard task, as there is no enough training data to train a classiﬁer; yet some test data is possible to obtain, as these trolls are sometimes caught and widely exposed. In this paper, we solve the training data problem by assuming that a user who is called a troll by several different people is likely to be such, and one who has never been called a troll is unlikely to be such. We compare the proﬁles of (i) paid trolls vs. (ii) “mentioned” trolls vs. (iii) non-trolls, and we further show that a classiﬁer trained to distinguish (ii) from (iii) does quite well also at telling apart (i) from (iii). 
News reports, social media streams, blogs, digitized archives and books are part of a plethora of reading sources that people face every day. This raises the question of how to best generate automatic summaries. Many existing methods for extracting summaries rely on comparing the similarity of two sentences in some way. We present new ways of measuring this similarity, based on sentiment analysis and continuous vector space representations, and show that combining these together with similarity measures from existing methods, helps to create better summaries. The ﬁnding is demonstrated with MULTSUM, a novel summarization method that uses ideas from kernel methods to combine sentence similarity measures. Submodular optimization is then used to produce summaries that take several different similarity measures into account. Our method improves over the state-of-the-art on standard benchmark datasets; it is also fast and scale to large document collections, and the results are statistically signiﬁcant. 
This paper describes the importance of introducing a phrase-based language model in the process of machine translation. In fact, nowadays SMT are based on phrases for translation but their language models are based on classical ngrams. In this paper we introduce a phrase-based language model (PBLM) in the decoding process to try to match the phrases of a translation table with those predicted by a language model. Furthermore, we propose a new way to retrieve phrases and their corresponding translation by using the principle of conditional mutual information. The SMT developed will be compared to the baseline one in terms of BLEU, TER and METEOR. The experimental results show that the introduction of PBLM in the translation decoding improve the results. 
Distributed representations of words have boosted the performance of many Natural Language Processing tasks. However, usually only one representation per word is obtained, not acknowledging the fact that some words have multiple meanings. This has a negative effect on the individual word representations and the language model as a whole. In this paper we present a simple model that enables recent techniques for building word vectors to represent distinct senses of polysemic words. In our assessment of this model we show that it is able to effectively discriminate between words’ senses and to do so in a computationally efﬁcient manner. 
The manual identiﬁcation of terminology from specialized corpora is a complex task that needs to be addressed by ﬂexible tools, in order to facilitate the construction of multilingual terminologies which are the main resources for computer-assisted translation tools, machine translation or ontologies. The automatic terminology extraction tools developed so far either use a proprietary code or an open source code, that is limited to certain software functionalities. To automatically extract terms from specialized corpora for different purposes such as constructing dictionaries, thesauruses or translation memories, we need open source tools to easily integrate new functionalities to improve term selection. This paper presents TBXTools, a free automatic terminology extraction tool that implements linguistic and statistical methods for multiword term extraction. The tool allows the users to easily identify multiword terms from specialized corpora and also, if needed, translation candidates from parallel corpora. In this paper we present the main features of TBXTools along with evaluation results for term extraction, both using statistical and linguistic methodology, for several corpora. 
Wordnet is a standard semantic resource for several Natural Language Processing tasks and it is available for an increasing number of languages. The Croatian Wordnet (CroWN) was a relatively small resource with 10.026 synsets and 31.367 synset-variant pairs covering only 45.91% of the so-called Core WordNet. Comparing these ﬁgures with the size of the Princeton WordNet for English version 3.0, that has 117,659 synsets and 206,975 synset-variant pairs, it is clear that the CroWN should be expanded. First experiments for the expansion of the CroWN were performed using the WN-Toolkit, a set of Python programs for wordnet creation and expansion using dictionary, Babelnet and parallel-corpora based strategies. The WN-Toolkit was previously successfully applied to other languages as Spanish, Catalan and Galician. After this ﬁrst expansion, CroWN reached 70.63% of the core wordnet. In the second step we used CroDeriv, a derivational database for Croatian and the manual creation of 1,457 synset-variant pairs until reaching 100% of the Core WordNet. After second step was completed, CroWN reached 23,137 synsets and 47,931 synset-lemma pairs. 
Attribute information in a natural language query is one of the key features for converting a natural language query into a Structured Query Language1 (SQL) in Natural Language Interface to Database systems. In this paper, we explore the task of classifying the attributes present in a natural language query into different SQL clauses in a SQL query. In particular, we investigate the effectiveness of various features and Conditional Random Fields for this task. Our system uses a statistical classiﬁer trained on manually prepared data. We report our results on three different domains and also show how our system can be used for generating a complete SQL query. 
We describe an algorithm for automatic classiﬁcation of idiomatic and literal expressions. Our starting point is that idioms and literal expressions occur in different contexts. Idioms tend to violate cohesive ties in local contexts, while literals are expected to ﬁt in. Our goal is to capture this intuition using a vector representation of words. We propose two approaches: (1) Compute inner product of context word vectors with the vector representing a target expression. Since literal vectors predict well local contexts, their inner product with contexts should be larger than idiomatic ones, thereby telling apart literals from idioms; and (2) Compute literal and idiomatic scatter (covariance) matrices from local contexts in word vector space. Since the scatter matrices represent context distributions, we can then measure the difference between the distributions using the Frobenius norm. We provide experimental results validating the proposed techniques. 
The paper focuses on selecting an optimal set of the Multiword Expressions Extraction methods used as a tool during wordnet expansion. Wordnet multiword lexical units are a broad class and it is difﬁcult to ﬁnd a single extraction method fulﬁlling the task. Many extraction association measures were tested on very large corpora and a very large wordnet, namely plWordNet. Several new measures are proposed and compared with selected methods in the literature. Two ways of combining measures into ensembles were analysed too. We showed that method selection and the tuning of their parameters can be transferred between two large corpora. The comparison of the extracted collocations with the huge set of plWordNet multiword lexical units revealed that the performance of the methods is much below the optimistic levels reported in the literature. However, the carefully selected set and combination of the methods can be a valuable tool for lexicographers. 
This study is about the development of a learner-focused text readability indexing tool for second language learners (L2) of English. Student essays are used to calibrate the system, making it capable of providing realistic approximation of L2s’ actual reading ability spectrum. The system aims to promote self-directed (i.e. selfstudy) language learning and help even those L2s who can not afford formal education. In this paper, we provide a comparative review of two vectorial semantics-based algorithms, namely, Latent Semantic Indexing (LSI) and Concept Indexing (CI) for text content analysis. Since these algorithms rely on the bag-of-words approach and inherently lack grammar-related analysis, we augment them by incorporating Part-of-Speech (POS) n-gram features to approximate syntactic complexity of the text documents. Based on the results, CI-based features outperformed LSI-based features in most of the experiments. Without the integration of POS n-gram features, the difference between their mean exact agreement accuracies (MEAA) can reach as high as 23%, in favor of CI. It has also been proven that the performance of both algorithms can be further enhanced by combining POS bi-gram features, yielding as high as 95.1% and 91.9% MEAA values for CI and LSI, respectively. 
It is generally acknowledged that collocations in the sense of idiosyncratic word cooccurrences are a challenge in the context of second language learning. Advanced miscollocation correction is thus highly desirable. However, state-of-the-art “collocation checkers” are merely able to detect a possible miscollocation and then offer as correction suggestion a list of collocations of the given keyword retrieved automatically from a corpus. No more targeted correction is possible since state-ofthe-art collocation checkers are not able to identify the type of the miscollocation. We suggest a classiﬁcation of the main types of lexical miscollocations by US American learners of Spanish and demonstrate its performance. 
Similarity between natural language texts, sentences in terms of meaning, known as textual entailment, is a generic problem in the area of computational linguistics. In the last few years researchers worked on various aspects of textual entailment problem, but mostly restricted to English language. Here in this paper we present a method for measuring the semantic similarity of Bengali tweets using WordNet. Moreover we defined partial textual entailment (PTE) as in real data partial entailment cases are equally prevalent with the complete/direct entailment. Although by definition entailment is a directional relationship, but here we consider entail- ment more as semantic similarity. Keywords: Semantic similarity; WordNet; Synonym; 
In recent years, theoretical and computational linguistics has paid much attention to linguistic items that form scales. In NLP, much research has focused on ordering adjectives by intensity (tiny < small). Here, we address the task of automatically ordering English adverbs by their intensifying or diminishing effect on adjectives (e.g. extremely small < very small). We experiment with 4 different methods: 1) using the association strength between adverbs and adjectives; 2) exploiting scalar patterns (such as not only X but Y); 3) using the metadata of product reviews; 4) clustering. The method that performs best is based on the use of metadata and ranks adverbs by their scaling factor relative to unmodiﬁed adjectives. 
Developing a large vocabulary automatic speech recognition system is a very difficult task, due to the high variations in domain and acoustic variability. This task is even more difficult for the Latvian language, which is very rich morphologically and in which one word can have dozens of surface forms. Although there is some research on speech recognition for Latvian, Latvian ASR remains behind “big” languages such as English, German etc. In order to improve the performance of Latvian ASR, it is important to understand what errors does it make and why. In this paper, the authors analyze the most common errors of Latvian ASR. Based on this, baseline system WER is improved from 30.94% to 28.43%. 
This paper describes a novel approach to ﬁnd evidence for implicit semantic roles. Our data-driven models generalize over large amounts of explicit annotations only, in order to acquire information about implicit roles. We establish a generic background knowledge base of probablistic predicate-role co-occurrences in an unsupervised manner, and estimate thresholds which trigger the prediction of a missing role. Our approach outperforms the stateof-the-art in terms of recognition rate and offers a more ﬂexible alternative to rulebased solutions which rely on costly, language and domain-speciﬁc lexica. 
This paper describes an Example-Based Machine Translation prototype and presents an evaluation of the impact of using a domainspecific vocabulary on its performance. This prototype is based on a hybrid approach which needs only monolingual texts in the target language and consists to combine translation candidates returned by a cross-language search engine with translation hypotheses provided by a finite-state transducer. The results of this combination are evaluated against a statistical language model of the target language in order to obtain the n-best translations. To measure the performance of this hybrid approach, we achieved several experiments using corpora on two domains from the European Parliament proceedings (Europarl) and the European Medicines Agency documents (Emea). The obtained results show that the proposed approach outperforms the state-of-the-art Statistical Machine Translation system Moses when texts to translate are related to the specialized domain. 
Topic segmentation traditionally relies on lexical cohesion measured through word re-occurrences to output a dense segmentation, either linear or hierarchical. In this paper, a novel organization of the topical structure of textual content is proposed. Rather than searching for topic shifts to yield dense segmentation, we propose an algorithm to extract topically focused fragments organized in a hierarchical manner. This is achieved by leveraging the temporal distribution of word re-occurrences, searching for bursts, to skirt the limits imposed by a global counting of lexical reoccurrences within segments. Comparison to a reference dense segmentation on varied datasets indicates that we can achieve a better topic focus while retrieving all of the important aspects of a text. 
In this paper we present an approach for the enrichment of WSD knowledge bases with data-driven relations from a gold standard corpus (annotated with word senses, valency information, syntactic analyses, etc.). We focus on Bulgarian as a use case, but our approach is scalable to other languages as well. For the purpose of exploring such methods, the Personalized Page Rank algorithm was used. The reported results show that the addition of new knowledge improves the accuracy of WSD with approximately 10.5%. 
Our current work analyses relations between sentiments and activity of authors of online InVitro Fertilization forums. We focus on two types of active authors: those who start new discussions and those who post significantly more messages than other authors. By incorporating authors‟ activity information into a domain-specific lexical representation of messages, we were able to improve multi-class classification of sentiments by 9% for Support Vector Machines and by 15.3 % for Conditional Random Fields. 
In recent years, several studies have approached the Text Simpliﬁcation (TS) task as a machine translation (MT) problem. They report promising results in learning how to translate from ‘original’ to ‘simpliﬁed’ language using the standard phrasebased translation model. However, our results indicate that this approach works well only when the training dataset consists mostly of those sentence pairs in which the simpliﬁed sentence is already very similar to its original. Our ﬁndings suggest that the standard phrase-based approach might not be appropriate to learn strong simpliﬁcations which are needed for certain target populations. 
In this paper, we explore statistical machine translation (SMT) approaches to automatic text simpliﬁcation (ATS) for Spanish. First, we compare the performances of the standard phrase-based (PB) and hierarchical (HIERO) SMT models in this speciﬁc task. In both cases, we build two models, one using the TS corpus with “light” simpliﬁcations and the other using the TS corpus with “heavy” simpliﬁcations. Next, we compare the two best systems with the state-of-the-art text simpliﬁcation system for Spanish (Simplext). Our results, based on an extensive human evaluation, show that the SMT-based systems perform equally as well as, or better than, Simplext, despite the very small datasets used for training and tuning. 
This paper presents a multilingual corpus of news, annotated with event metadata information. The events in our corpus are from the domain of violence, natural and man made disasters. The main goal of the corpus is automatic evaluation of event detection and extraction systems in different languages. As a use case, we take a rulebased event extraction system, extend it to cover a new language, Czech in our case, and evaluate it on the corpus. We explain what needs to be done to cover a new language, especially learning domain-speciﬁc dictionaries and event extraction patterns. The evaluation of the Czech system can be viewed as a starting point for further research into the evaluation of multilingual event extraction systems, which is an important stage during the development of such systems. The comparison of the performance for the Czech and English systems indicates the importance for multilingual event extraction evaluation. 
The paper addresses the task of automatic interpretation of semantic relation in noun compounds. The problem has been attempted with both Ontology-based and Statistical approaches, but both approaches having their own limitations. We present a novel VSMbased statistical model which represents each relation with a weighted vector of prepositional and verbal paraphrases. The model ranks the paraphrases on their relevance and assigns higher weights to more relevant paraphrases. The performance of the model is compared with the Ontology model and the results are quite encouraging. We ﬁnally propose a Hybrid of the two models which compares on par with the best performing systems on Nastase and Szpakowicz (2003) dataset. 
Typically, only a very limited amount of in-domain data is available for training the language model component of an Handwritten Text Recognition (HTR) system for historical data. One has to rely on a combination of in-domain and out-ofdomain data to develop language models. Accordingly, domain adaptation is a central issue in language modeling for HTR. We pursue a topic modeling approach to handle this issue, and propose two algorithms based on this approach. The ﬁrst algorithm relies on posterior inference for topic modeling to construct a language model adapted to the development set, and the second algorithm proceeds by iterative selection, using a new ranking criterion, of topic-dependent language models. Our experimental results show that both approaches clearly outperform a strong baseline method. 
Our goal is to facilitate named entity recognition in Bulgarian texts by extending the coverage of DBpedia (http://www.dbpedia.org/) for Bulgarian. For this task we have trained translation Moses models to transliterate foreign names to Bulgarian. The training sets were obtained by extracting the names of all people, places and organizations from DBpedia and its extension Airpedia (http://www. airpedia.org/). Our approach is extendable to other languages with small DBpedia coverage. 
Clustering words is a widely used technique in statistical natural language processing. It requires syntactic, semantic, and contextual features. Especially, semantic clustering is gaining a lot of interest. It consists in grouping a set of words expressing the same idea or sharing the same semantic properties. In this paper, we present a new method to integrate semantic classes in a Statistical Machine Translation (SMT) context to improve the Arabic-English translation quality. In our method, we first apply a semantic word clustering algorithm for English. We then project the obtained semantic word classes from the English side to the Arabic side. This projection is based on available word alignments provided by the alignment step using GIZA++ tool. Finally, we apply a new process to incorporate semantic classes in order to improve the SMT quality. The experimental results show that introducing semantic word classes achieves 4 % of relative improvement on the BLEU score for the Arabic → English translation task. 
In the current era of online interactions, both positive and negative experiences are abundant on the Web. As in real life, negative experiences can have a serious impact on youngsters. Recent studies have reported cybervictimization rates among teenagers that vary between 20% and 40%. In this paper, we focus on cyberbullying as a particular form of cybervictimization and explore its automatic detection and ﬁne-grained classiﬁcation. Data containing cyberbullying was collected from the social networking site Ask.fm. We developed and applied a new scheme for cyberbullying annotation, which describes the presence and severity of cyberbullying, a post author’s role (harasser, victim or bystander) and a number of ﬁne-grained categories related to cyberbullying, such as insults and threats. We present experimental results on the automatic detection of cyberbullying and explore the feasibility of detecting the more ﬁne-grained cyberbullying categories in online posts. For the ﬁrst task, an F-score of 55.39% is obtained. We observe that the detection of the ﬁne-grained categories (e.g. threats) is more challenging, presumably due to data sparsity, and because they are often expressed in a subtle and implicit way. 
There is a great deal of knowledge available on the Web, which represents a great opportunity for automatic, intelligent text processing and understanding, but the major problems are ﬁnding the legitimate sources of information and the fact that search engines provide page statistics not occurrences. This paper presents a new, domain independent, general-purpose idiom identiﬁcation approach. Our approach combines the knowledge of the Web with the knowledge extracted from dictionaries. This method can overcome the limitations of current techniques that rely on linguistic knowledge or statistics. It can recognize idioms even when the complete sentence is not present, and without the need for domain knowledge. It is currently designed to work with text in English but can be extended to other languages. 
We present a case study on the role of syntactic structures towards resolving the Semantic Textual Similarity (STS) task. Although various approaches have been proposed, the research of using syntactic information to determine the semantic similarity is a relatively under-researched area. At the level of syntactic structure, it is not clear how signiﬁcant the syntactic structure contributes to the overall accuracy of the task. In this paper, we analyze the impact of syntactic structure towards the overall performance and its behavior in different score ranges of the STS semantic scale. 
This paper presents our investigation of the ability of 33 readability indices to account for the reading comprehension difﬁculty posed by texts for people with autism. The evaluation by autistic readers of 16 text passages is described, a process which led to the production of the ﬁrst text collection for which readability has been evaluated by people with autism. We present the ﬁndings of a study to determine which of the 33 indices can successfully discriminate between the difﬁculty levels of the text passages, as determined by our reading experiment involving autistic participants. The discriminatory power of the indices is further assessed through their application to the FIRST corpus which consists of 25 texts presented in their original form and in a manually simpliﬁed form (50 texts in total), produced speciﬁcally for readers with autism. 
Community question answering sites provide us convenient and interactive platforms for problem solving and knowledge sharing, which are attracting an increasing number of users. Accordingly, it will be very common that different people have the same user name. When a query question is given, some potential answer providers would be recommended to the asker in the form of user name. However, some user names are ambiguous and not unique in the community. To help question askers match the ambiguous user names with the right people, in this paper, we propose to disambiguate same-name users by ranking their tag-based relevance to a query question. Empirical studies on three community question answering datasets demonstrate that our method is effective for disambiguating user names in community question answering. 
One aspect of ontology learning methods is the discovery of relations in textual data. One kind of such relations are causal relations. Our aim is to discover causations described in texts such as recipes and manuals. There is a lot of research on causal relations discovery that is based on grammatical patterns. These patterns are, however, rarely discovered in textual instructions (such as recipes) with short and simple sentence structure. Therefore we propose an approach that makes use of time series to discover causal relations. We distinguish causal relations from correlation by assuming that one word causes another only if it precedes the second word temporally. To test the approach, we compared the discovered by our approach causal relations to those obtained through grammatical patterns in 20 textual instructions. The results showed that our approach has an average recall of 41% compared to 13% obtained with the grammatical patterns. Furthermore the discovered by the two approaches causal relations are usually disjoint. This indicates that the approach can be combined with grammatical patterns in order to increase the number of causal relations discovered in textual instructions. 
The applications of plWordNet, a very large wordnet for Polish, do not yet include work on sentiment and emotions. We present a pilot project to annotate plWordNet manually with sentiment polarity values and basic emotion values. We work with lexical units, plWordNet’s basic building blocks.1 So far, we have annotated about 30,000 nominal and adjectival LUs. The resulting lexicon is already one of the largest sentiment and emotion resources, in particular among those based on wordnets. We opted for manual annotation to ensure high accuracy, and to provide a reliable starting point for future semi-automated expansion. The paper lists the principal assumptions, outlines the annotation process, and introduces the resulting resource, plWordNetemo. We discuss the selection of the material for the pilot study, show the distribution of annotations across the wordnet, and consider the statistics, including interannotator agreement and the resolution of disagreement. 
The rise in Arabic usage within various social media platforms, and notably in Twitter, has led to a growing interest in building Arabic Natural Language Processing (NLP) applications capable of dealing with informal colloquial Arabic, as it is the most commonly used form of Arabic in social media. The unique characteristics of the Arabic language make the extraction of Arabic named entities a challenging task, to which, the nature of tweets adds new dimensions. The majority of previous research done on Arabic NER focused on extracting entities from the formal language, namely Modern Standard Arabic (MSA). However, the unstructured nature of the colloquial language used in tweets degrades the performance of NER systems developed to support formal MSA text. In this paper, we focus on the task of Arabic persons‟ names recognition. Specifically, we introduce an approach to extract Arabic persons‟ names from tweets without employing any morphological analysis or languagedependent features. The proposed approach adopts a rule-based model combined with a statistical one. This approach uses unsupervised learning of patterns and clustered dictionaries as constrains to identify a person‟s name and resolve its ambiguity. Our approach outperforms the best reported result in the literature on the same test set by an increase of 19.6% in the F-score. 
We address the task of parsing semantically indeterminate expressions, for which several correct structures exist that do not lead to differences in meaning. We present a novel non-deterministic structure transfer method that accumulates all structural information based on cross-lingual word distance derived from parallel corpora. Our system’s output is a ranked list of trees. To evaluate our system, we adopted common IR metrics. We show that our system outperforms previous cross-lingual structure transfer methods signiﬁcantly. In addition, we illustrate that tree accumulation can be used to combine partial evidence across languages to form a single structure, thereby making use of sparse parallel data in an optimal way. 
The Federal Open Market Committee (FOMC) is a committee within the central banking system of the US and decides on the target rate. Analyzing the positions of its members is a challenge even for experts with a deep knowledge of the ﬁnancial domain. In our work, we aim at automatically determining opinion groups in transcriptions of the FOMC discussions. We face two main challenges: ﬁrst, the positions of the members are more complex as in common opinion mining tasks because they have more dimensions than pro or contra. Second, they cannot be learned as there is no labeled data available. We address the challenge using graph clustering methods to group the members, including the similarity of their speeches as well as agreement and disagreement they show towards each other in discussions. We show that our approach produces stable opinion clusters throughout successive meetings and correlates with positions of speakers on a dove-hawk scale estimated by experts. 
Little work from the Natural Language Processing community has targeted the role of quantities in Natural Language Understanding. This paper takes some key steps towards facilitating reasoning about quantities expressed in natural language. We investigate two different tasks of numerical reasoning. First, we consider Quantity Entailment, a new task formulated to understand the role of quantities in general textual inference tasks. Second, we consider the problem of automatically understanding and solving elementary school math word problems. In order to address these quantitative reasoning problems we first develop a computational approach which we show to successfully recognize and normalize textual expressions of quantities. We then use these capabilities to further develop algorithms to assist reasoning in the context of the aforementioned tasks.
Identifying and linking named entities across information sources is the basis of knowledge acquisition and at the heart of Web search, recommendations, and analytics. An important problem in this context is cross-document co-reference resolution (CCR): computing equivalence classes of textual mentions denoting the same entity, within and across documents. Prior methods employ ranking, clustering, or probabilistic graphical models using syntactic features and distant features from knowledge bases. However, these methods exhibit limitations regarding run-time and robustness. This paper presents the CROCS framework for unsupervised CCR, improving the state of the art in two ways. First, we extend the way knowledge bases are harnessed, by constructing a notion of semantic summaries for intra-document co-reference chains using co-occurring entity mentions belonging to different chains. Second, we reduce the computational cost by a new algorithm that embeds sample-based bisection, using spectral clustering or graph partitioning, in a hierarchical clustering process. This allows scaling up CCR to large corpora. Experiments with three datasets show significant gains in output quality, compared to the best prior methods, and the run-time efficiency of CROCS.
We present a dynamic programming algorithm for efficient constrained inference in semantic role labeling. The algorithm tractably captures a majority of the structural constraints examined by prior work in this area, which has resorted to either approximate methods or off-the-shelf integer linear programming solvers. In addition, it allows training a globally-normalized log-linear model with respect to constrained conditional likelihood. We show that the dynamic program is several times faster than an off-the-shelf integer linear programming solver, while reaching the same solution. Furthermore, we show that our structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora.
We introduce Sprite, a family of topic models that incorporates structure into model priors as a function of underlying components. The structured priors can be constrained to model topic hierarchies, factorizations, correlations, and supervision, allowing Sprite to be tailored to particular settings. We demonstrate this flexibility by constructing a Sprite-based model to jointly infer topic hierarchies and author perspective, which we apply to corpora of political debates and online reviews. We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks.
Online discussion forums and community question-answering websites provide one of the primary avenues for online users to share information. In this paper, we propose text mining techniques which aid users navigate troubleshooting-oriented data such as questions asked on forums and their suggested solutions. We introduce Bayesian generative models of the troubleshooting data and apply them to two interrelated tasks: (a) predicting the complexity of the solutions (e.g., plugging a keyboard in the computer is easier compared to installing a special driver) and (b) presenting them in a ranked order from least to most complex. Experimental results show that our models are on par with human performance on these tasks, while outperforming baselines based on solution length or readability.
Grammars for machine translation can be materialized on demand by finding source phrases in an indexed parallel corpus and extracting their translations. This approach is limited in practical applications by the computational expense of online lookup and extraction. For phrase-based models, recent work has shown that on-demand grammar extraction can be greatly accelerated by parallelization on general purpose graphics processing units (GPUs), but these algorithms do not work for hierarchical models, which require matching patterns that contain gaps. We address this limitation by presenting a novel GPU algorithm for on-demand hierarchical grammar extraction that is at least an order of magnitude faster than a comparable CPU algorithm when processing large batches of sentences. In terms of end-to-end translation, with decoding on the CPU, we increase throughput by roughly two thirds on a standard MT evaluation dataset. The GPU necessary to achieve these improvements increases the cost of a server by about a third. We believe that GPU-based extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput.
Natural language meanings allow speakers to encode important real-world distinctions, but corpora of grounded language use also reveal that speakers categorize the world in different ways and describe situations with different terminology. To learn meanings from data, we therefore need to link underlying representations of meaning to models of speaker judgment and speaker choice. This paper describes a new approach to this problem: we model variability through uncertainty in categorization boundaries and distributions over preferred vocabulary. We apply the approach to a large data set of color descriptions, where statistical evaluation documents its accuracy. The results are available as a Lexicon of Uncertain Color Standards (LUX), which supports future efforts in grounded language understanding and generation by probabilistically mapping 829 English color descriptions to potentially context-sensitive regions in HSV color space.
Most approaches to relation extraction, the task of extracting ground facts from natural language text, are based on machine learning and thus starved by scarce training data. Manual annotation is too expensive to scale to a comprehensive set of relations. Distant supervision, which automatically creates training data, only works with relations that already populate a knowledge base (KB). Unfortunately, KBs such as FreeBase rarely cover event relations (e.g. {``}person travels to location{''}). Thus, the problem of extracting a wide range of events {---} e.g., from news streams {---} is an important, open challenge. This paper introduces NewsSpike-RE, a novel, unsupervised algorithm that discovers event relations and then learns to extract them. NewsSpike-RE uses a novel probabilistic graphical model to cluster sentences describing similar events from parallel news streams. These clusters then comprise training data for the extractor. Our evaluation shows that NewsSpike-RE generates high quality training sentences and learns extractors that perform much better than rival approaches, more than doubling the area under a precision-recall curve compared to Universal Schemas.
Inferring the information structure of scientific documents is useful for many NLP applications. Existing approaches to this task require substantial human effort. We propose a framework for constraint learning that reduces human involvement considerably. Our model uses topic models to identify latent topics and their key linguistic features in input documents, induces constraints from this information and maps sentences to their dominant information structure categories through a constrained unsupervised model. When the induced constraints are combined with a fully unsupervised model, the resulting model challenges existing lightly supervised feature-based models as well as unsupervised models that use manually constructed declarative knowledge. Our results demonstrate that useful declarative knowledge can be learned from data with very limited human involvement.
Entity disambiguation with Wikipedia relies on structured information from redirect pages, article text, inter-article links, and categories. We explore whether web links can replace a curated encyclopaedia, obtaining entity prior, name, context, and coherence models from a corpus of web pages with links to Wikipedia. Experiments compare web link models to Wikipedia models on well-known conll and tac data sets. Results show that using 34 million web links approaches Wikipedia performance. Combining web link and Wikipedia models produces the best-known disambiguation accuracy of 88.7 on standard newswire test data.
Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.
The role of language models in SMT is to promote fluent translation output, but traditional n-gram language models are unable to capture fluency phenomena between distant words, such as some morphological agreement phenomena, subcategorisation, and syntactic collocations with string-level gaps. Syntactic language models have the potential to fill this modelling gap. We propose a language model for dependency structures that is relational rather than configurational and thus particularly suited for languages with a (relatively) free word order. It is trainable with Neural Networks, and not only improves over standard n-gram language models, but also outperforms related syntactic language models. We empirically demonstrate its effectiveness in terms of perplexity and as a feature function in string-to-tree SMT from English to German and Russian. We also show that using a syntactic evaluation metric to tune the log-linear parameters of an SMT system further increases translation quality when coupled with a syntactic language model.
As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown{\ldots}) attracting most attention. By building on the recent {``}zero-shot learning{''} approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available. Our approach relies on two key observations. First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives. Second, objects and attributes come together in pictures (the same thing is a dog and it is brown). We can thus achieve better attribute (and object) label retrieval by treating images as {``}visual phrases{''}, and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun. Our approach performs comparably to a method exploiting manual attribute annotation, it out-performs various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition.
Lexical semantic models provide robust performance for question answering, but, in general, can only capitalize on direct evidence seen during training. For example, monolingual alignment models acquire term alignment probabilities from semi-structured data such as question-answer pairs; neural network language models learn term embeddings from unstructured text. All this knowledge is then used to estimate the semantic similarity between question and answer candidates. We introduce a higher-order formalism that allows all these lexical semantic models to chain direct evidence to construct indirect associations between question and answer texts, by casting the task as the traversal of graphs that encode direct term associations. Using a corpus of 10,000 questions from Yahoo! Answers, we experimentally demonstrate that higher-order methods are broadly applicable to alignment and language models, across both word and syntactic representations. We show that an important criterion for success is controlling for the semantic drift that accumulates during graph traversal. All in all, the proposed higher-order approach improves five out of the six lexical semantic models investigated, with relative gains of up to +13{\%} over their first-order variants.
Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.
Lexical embeddings can serve as useful representations for words for a variety of NLP tasks, but learning embeddings for phrases can be challenging. While separate embeddings are learned for each word, this is infeasible for every phrase. We construct phrase embeddings by learning how to compose word embeddings using features that capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use.
Supervised methods can achieve high performance on NLP tasks, such as Named Entity Recognition (NER), but new annotations are required for every new domain and/or genre change. This has motivated research in minimally supervised methods such as semi-supervised learning and distant learning, but neither technique has yet achieved performance levels comparable to those of supervised methods. Semi-supervised methods tend to have very high precision but comparatively low recall, whereas distant learning tends to achieve higher recall but lower precision. This complementarity suggests that better results may be obtained by combining the two types of minimally supervised methods. In this paper we present a novel approach to Arabic NER using a combination of semi-supervised and distant learning techniques. We trained a semi-supervised NER classifier and another one using distant learning techniques, and then combined them using a variety of classifier combination schemes, including the Bayesian Classifier Combination (BCC) procedure recently proposed for sentiment analysis. According to our results, the BCC model leads to an increase in performance of 8 percentage points over the best base classifiers.
We present an approach to learning a model-theoretic semantics for natural language tied to Freebase. Crucially, our approach uses an open predicate vocabulary, enabling it to produce denotations for phrases such as {``}Republican front-runner from Texas{''} whose semantics cannot be represented using the Freebase schema. Our approach directly converts a sentence{'}s syntactic CCG parse into a logical form containing predicates derived from the words in the sentence, assigning each word a consistent semantics across sentences. This logical form is evaluated against a learned probabilistic database that defines a distribution over denotations for each textual predicate. A training phase produces this probabilistic database using a corpus of entity-linked text and probabilistic matrix factorization with a novel ranking objective function. We evaluate our approach on a compositional question answering task where it outperforms several competitive baselines. We also compare our approach against manually annotated Freebase queries, finding that our open predicate vocabulary enables us to answer many questions that Freebase cannot.
Simple Wikipedia has dominated simplification research in the past 5 years. In this opinion paper, we argue that focusing on Wikipedia limits simplification research. We back up our arguments with corpus analysis and by highlighting statements that other researchers have made in the simplification literature. We introduce a new simplification dataset that is a significant improvement over Simple Wikipedia, and present a novel quantitative-comparative approach to study the quality of simplification data resources.
Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature vector representations of words have been used to obtain high performance in many NLP tasks. In this paper, we extend two different Dirichlet multinomial topic models by incorporating latent feature vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus. Experimental results show that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.
Recent research on entity linking (EL) has introduced a plethora of promising techniques, ranging from deep neural networks to joint inference. But despite numerous papers there is surprisingly little understanding of the state of the art in EL. We attack this confusion by analyzing differences between several versions of the EL problem and presenting a simple yet effective, modular, unsupervised system, called Vinculum, for entity linking. We conduct an extensive evaluation on nine data sets, comparing Vinculum with two state-of-the-art systems, and elucidate key aspects of the system that include mention extraction, candidate generation, entity type prediction, entity coreference, and coherence.
Discourse relations bind smaller linguistic units into coherent texts. Automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked arguments. A more subtle challenge is that it is not enough to represent the meaning of each argument of a discourse relation, because the relation may depend on links between lowerlevel components, such as entity mentions. Our solution computes distributed meaning representations for each discourse argument by composition up the syntactic parse tree. We also perform a downward compositional pass to capture the meaning of coreferent entity mentions. Implicit discourse relations are then predicted from these two representations, obtaining substantial improvements on the Penn Discourse Treebank.
The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive semantic resource, consisting of a list of phrase pairs with (heuristic) confidence estimates. However, it is still unclear how it can best be used, due to the heuristic nature of the confidences and its necessarily incomplete coverage. We propose models to leverage the phrase pairs from the PPDB to build parametric paraphrase models that score paraphrase pairs more accurately than the PPDB{'}s internal scores while simultaneously improving its coverage. They allow for learning phrase embeddings as well as improved word embeddings. Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.
Space-delimited words in Turkish and Hebrew text can be further segmented into meaningful units, but syntactic and semantic context is necessary to predict segmentation. At the same time, predicting correct syntactic structures relies on correct segmentation. We present a graph-based lattice dependency parser that operates on morphological lattices to represent different segmentations and morphological analyses for a given input sentence. The lattice parser predicts a dependency tree over a path in the lattice and thus solves the joint task of segmentation, morphological analysis, and syntactic parsing. We conduct experiments on the Turkish and the Hebrew treebank and show that the joint model outperforms three state-of-the-art pipeline systems on both data sets. Our work corroborates findings from constituency lattice parsing for Hebrew and presents the first results for full lattice parsing on Turkish.
Corpus-based distributional semantic models capture degrees of semantic relatedness among the words of very large vocabularies, but have problems with logical phenomena such as entailment, that are instead elegantly handled by model-theoretic approaches, which, in turn, do not scale up. We combine the advantages of the two views by inducing a mapping from distributional vectors of words (or sentences) into a Boolean structure of the kind in which natural language terms are assumed to denote. We evaluate this Boolean Distributional Semantic Model (BDSM) on recognizing entailment between words and sentences. The method achieves results comparable to a state-of-the-art SVM, degrades more gracefully when less training data are available and displays interesting qualitative properties.
We present a model of unsupervised phonological lexicon discovery{---}the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model{'}s behavior and the kinds of linguistic structures it learns.
Machine learning approaches to coreference resolution vary greatly in the modeling of the problem: while early approaches operated on the mention pair level, current research focuses on ranking architectures and antecedent trees. We propose a unified representation of different approaches to coreference resolution in terms of the structure they operate on. We represent several coreference resolution approaches proposed in the literature in our framework and evaluate their performance. Finally, we conduct a systematic analysis of the output of these approaches, highlighting differences and similarities.
Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy. These differences were proven useful for statistical machine translation. However, it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on. We show that this is indeed the case, in a variety of evaluation scenarios. We then show that unsupervised classification is highly accurate on this task. We suggest a method for determining the correct labels of the clustering outcomes, and then use the labels for voting, improving the accuracy even further. Moreover, we suggest a simple method for clustering in the challenging case of mixed-domain datasets, in spite of the dominance of domain-related features over translation-related ones. The result is an effective, fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy.
The observed pronunciations or spellings of words are often explained as arising from the {``}underlying forms{''} of their morphemes. These forms are latent strings that linguists try to reconstruct by hand. We propose to reconstruct them automatically at scale, enabling generalization to new words. Given some surface word types of a concatenative language along with the abstract morpheme sequences that they express, we show how to recover consistent underlying forms for these morphemes, together with the (stochastic) phonology that maps each concatenation of underlying forms to a surface form. Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finite-state machines with trainable weights. We define training and evaluation paradigms for the task of surface word prediction, and report results on subsets of 7 languages.
Frame semantic representations have been useful in several applications ranging from text-to-scene generation, to question answering and social network analysis. Predicting such representations from raw text is, however, a challenging task and corresponding models are typically only trained on a small set of sentence-level annotations. In this paper, we present a semantic role labeling system that takes into account sentence and discourse context. We introduce several new features which we motivate based on linguistic insights and experimentally demonstrate that they lead to significant improvements over the current state-of-the-art in FrameNet-based semantic role labeling.
Structural kernels are a flexible learning paradigm that has been widely used in Natural Language Processing. However, the problem of model selection in kernel-based methods is usually overlooked. Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained. In contrast, Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods. In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes. Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search. The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods.
We present the first large-scale, corpus based verification of Dowty{'}s seminal theory of proto-roles. Our results demonstrate both the need for and the feasibility of a property-based annotation scheme of semantic relationships, as opposed to the currently dominant notion of categorical roles.
We show how to train the fast dependency parser of Smith and Eisner (2008) for improved accuracy. This parser can consider higher-order interactions among edges while retaining O(n3) runtime. It outputs the parse with maximum expected recall{---}but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by back-propagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as others have done for loopy CRFs (Domke, 2010; Stoyanov et al., 2011; Domke, 2011; Stoyanov and Eisner, 2012). The resulting parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood.
We present Plato, a probabilistic model for entity resolution that includes a novel approach for handling noisy or uninformative features, and supplements labeled training data derived from Wikipedia with a very large unlabeled text corpus. Training and inference in the proposed model can easily be distributed across many servers, allowing it to scale to over 107 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets.
We present a novel hierarchical distance-dependent Bayesian model for event coreference resolution. While existing generative models for event coreference resolution are completely unsupervised, our model allows for the incorporation of pairwise distances between event mentions {---} information that is widely used in supervised coreference models to guide the generative clustering processing for better event clustering both within and across documents. We model the distances between event mentions using a feature-rich learnable distance function and encode them as Bayesian priors for nonparametric clustering. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods for both within- and cross-document event coreference resolution.
We present DefIE, an approach to large-scale Information Extraction (IE) based on a syntactic-semantic analysis of textual definitions. Given a large corpus of definitions we leverage syntactic dependencies to reduce data sparsity, then disambiguate the arguments and content words of the relation strings, and finally exploit the resulting information to organize the acquired relations hierarchically. The output of DefIE is a high-quality knowledge base consisting of several million automatically acquired semantic relations.
Semantic parsers conventionally construct logical forms bottom-up in a fixed order, resulting in the generation of many extraneous partial logical forms. In this paper, we combine ideas from imitation learning and agenda-based parsing to train a semantic parser that searches partial logical forms in a more strategic order. Empirically, our parser reduces the number of constructed partial logical forms by an order of magnitude, and obtains a 6x-9x speedup over fixed-order parsing, while maintaining comparable accuracy.
We study the generalization of maximum spanning tree dependency parsing to maximum acyclic subgraphs. Because the underlying optimization problem is intractable even under an arc-factored model, we consider the restriction to noncrossing dependency graphs. Our main contribution is a cubic-time exact inference algorithm for this class. We extend this algorithm into a practical parser and evaluate its performance on four linguistic data sets used in semantic dependency parsing. We also explore a generalization of our parsing framework to dependency graphs with pagenumber at most k and show that the resulting optimization problem is NP-hard for k {\mbox{$\geq$}} 2.
We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the original input. This paraphrase can be used to disambiguate the meaning representation via verification using a language model that calculates the probability of each paraphrase.
This paper formalizes the problem of solving multi-sentence algebraic word problems as that of generating and scoring equation trees. We use integer linear programming to generate equation trees and score their likelihood by learning local and global discriminative models. These models are trained on a small set of word problems and their answers, without any manual annotation, in order to choose the equation that best matches the problem text. We refer to the overall system as Alges. We compare Alges with previous work and show that it covers the full gamut of arithmetic operations whereas Hosseini et al. (2014) only handle addition and subtraction. In addition, Alges overcomes the brittleness of the Kushman et al. (2014) approach on single-equation problems, yielding a 15{\%} to 50{\%} reduction in error.
 Heng Ji Computer Science Dept. Rensselaer Polytechnic Institute Troy, NY 12180, USA jih@rpi.edu  Yizhou Sun College of Computer and Information Science Northeastern University Boston, MA 02115, USA yzsun@ccs.neu.edu  
 Elisabetta Jezek University of Pavia, IT jezek@unipv.it Octavian Popescu IBM Research, US o.popescu@us.ibm.com  
In this paper, we demonstrate a system implementation of a framework for computer assisted pronunciation training for second language learner (L2). This framework supports an iterative improvement of the automatic pronunciation error recognition and classiﬁcation by allowing integration of annotated error data. The annotated error data is acquired via an annotation tool for linguists. This paper will give a detailed description of the annotation tool and explains the error types. Furthermore, it will present the automatic error recognition method and the methods for automatic visual and audio feedback. This system demonstrates a novel approach to interactive and individualized learning for pronunciation training. 
Semantic annotated parallel corpora, though rare, play an increasingly important role in natural language processing. These corpora provide valuable data for computational tasks like sense-based machine translation and word sense disambiguation, but also to contrastive linguistics and translation studies. In this paper we present the ongoing development of a web-based corpus semantic annotation environment that uses the Open Multilingual Wordnet (Bond and Foster, 2013) as a sense inventory. The system includes interfaces to help coordinating the annotation project and a corpus browsing interface designed specifically to meet the needs of a semantically annotated corpus. The tool was designed to build the NTU-Multilingual Corpus (Tan and Bond, 2012). For the past six years, our tools have been tested and developed in parallel with the semantic annotation of a portion of this corpus in Chinese, English, Japanese and Indonesian. The annotation system is released under an open source license (MIT). 
We present a novel approach to the selective annotation of large corpora through the use of machine learning. Linguistic search engines used to locate potential instances of an infrequent phenomenon do not support ranking the search results. This favors the use of high-precision queries that return only a few results over broader queries that have a higher recall. Our approach introduces a classiﬁer used to rank the search results and thus helping the annotator focus on those results with the highest potential of being an instance of the phenomenon in question, even in low-precision queries. The classiﬁer is trained in an in-tool fashion, except for preprocessing relying only on the manual annotations done by the users in the querying tool itself. To implement this approach, we build upon CSniper1, a web-based multi-user search and annotation tool. 
Kernel-based learning algorithms have been shown to achieve state-of-the-art results in many Natural Language Processing (NLP) tasks. We present KELP, a Java framework that supports the implementation of both kernel-based learning algorithms and kernel functions over generic data representation, e.g. vectorial data or discrete structures. The framework has been designed to decouple kernel functions and learning algorithms: once a new kernel function has been implemented it can be adopted in all the available kernelmachine algorithms. The platform includes different Online and Batch Learning algorithms for Classiﬁcation, Regression and Clustering, as well as several Kernel functions, ranging from vector-based to structural kernels. This paper will show the main aspects of the framework by applying it to different NLP tasks. 
We present ICARUS for intonation, an interactive tool to browse and search automatically derived descriptions of fundamental frequency contours. It offers access to tonal features in combination with other annotation layers like part-ofspeech, syntax or coreference and visualizes them in a highly customizable graphical interface with various playback functions. The built-in search allows multilevel queries, the construction of which can be done graphically or textually, and includes the ability to search F0 contours based on various similarity measures. 
In this demo paper, we present NEED4Tweet, a Twitterbot for named entity extraction (NEE) and disambiguation (NED) for Tweets. The straightforward application of state-of-the-art extraction and disambiguation approaches on informal text widely used in Tweets, typically results in signiﬁcantly degraded performance due to the lack of formal structure; the lack of sufﬁcient context required; and the seldom entities involved. In this paper, we introduce a novel framework that copes with the introduced challenges. We rely on contextual and semantic features more than syntactic features which are less informative. We believe that disambiguation can help to improve the extraction process. This mimics the way humans understand language. 
We present the Visual Entity Explorer (VEX), an interactive tool for visually exploring and analyzing the output of entity linking systems. VEX is designed to aid developers in improving their systems by visualizing system results, gold annotations, and various mention detection and entity linking error types in a clear, concise, and customizable manner. 
Patterns extracted from dependency parses of sentences are a major source of knowledge for most state-of-the-art relation extraction systems, but can be of low quality in distantly supervised settings. We present a linguistic annotation tool that allows human experts to analyze and categorize automatically learned patterns, and to identify common error classes. The annotations can be used to create datasets that enable machine learning approaches to pattern quality estimation. We also present an experimental pattern error analysis for three semantic relations, where we ﬁnd that between 24% and 61% of the learned dependency patterns are defective due to preprocessing or parsing errors, or due to violations of the distant supervision assumption. 
We describe a well-performed semantic role labeling system that further extracts concepts (smaller semantic expressions) from unstructured natural language sentences language independently. A dual-layer semantic role labeling (SRL) system is built using Chinese Treebank and Propbank data. Contextual information is incorporated while labeling the predicate arguments to achieve better performance. Experimental results show that the proposed approach is superior to CoNLL 2009 best systems and comparable to the state of the art with the advantage that it requires no feature engineering process. Concepts are further extracted according to templates formulated by the labeled semantic roles to serve as features in other NLP tasks to provide semantically related cues and potentially help in related research problems. We also show that it is easy to generate a different language version of this system by actually building an English system which performs satisfactory.  node of the parse tree; those defined in FrameNet (Ruppenhofer et al., 2006) are the finest but most expressive. Each set provides semantic information. As long as the semantic relationship between terms derives from their semantic role labels, we are able to determine whether they should be extracted from the current sentence to construct a concept. The word concept usually refers to an abstract or general idea inferred or derived from specific instances. Therefore, the extraction of concepts from text is often defined as extracting terms that are in some way related to one another. These terms could be predefined by people in resources such as ontologies, or they could be typical words in texts. In this paper, we view concepts as the continuous or discontinuous meaningful units in a sentence and hence they are tightly related to semantic roles. We propose a dual-layer semantic role labeling system which provides extracted concepts according to the reported labels, and then demonstrate the functions of this system. Experimental results will show the merit of the proposed framework. 2 Related Work  
This paper presents a pipeline for aspectbased sentiment analysis of Chinese texts in the automotive domain. The input to the pipeline is a string of Chinese characters; the output is a set of relationships between evaluations and their targets. The main goal is to demonstrate how knowledge about sentence structure can increase the precision, insight value and granularity of the output. We formulate the task of sentiment analysis in two steps, namely unit identiﬁcation and relation extraction. In unit identiﬁcation, we identify fairly well-delimited linguistic units which describe features, emotions and evaluations. In relation extraction, we discover the relations between evaluations and their “target” features. 
We present cort, a modular toolkit for devising, implementing, comparing and analyzing approaches to coreference resolution. The toolkit allows for a uniﬁed representation of popular coreference resolution approaches by making explicit the structures they operate on. Several of the implemented approaches achieve state-ofthe-art performance. 
We present SCHNA¨ PPER, a web toolkit for Exploratory Relation Extraction (ERE). The tool allows users to identify relations of interest in a very large text corpus in an exploratory and highly interactive fashion. With this tool, we demonstrate the easeof-use and intuitive nature of ERE, as well as its applicability to large corpora. We show how users can formulate exploratory, natural language-like pattern queries that return relation instances. We also show how automatically computed suggestions are used to guide the exploration process. Finally, we demonstrate how users create extractors with SCHNA¨ PPER once a relation of interest is identiﬁed. 
Wordnets play a central role in many natural language processing tasks. This paper introduces a multilingual editing system for the Open Multilingual Wordnet (OMW: Bond and Foster, 2013). Wordnet development, like most lexicographic tasks, is slow and expensive. Moving away from the original Princeton Wordnet (Fellbaum, 1998) development workflow, wordnet creation and expansion has increasingly been shifting towards an automated and/or interactive system facilitated task. In the particular case of human edition/expansion of wordnets, a few systems have been developed to aid the lexicographers’ work. Unfortunately, most of these tools have either restricted licenses, or have been designed with a particular language in mind. We present a webbased system that is capable of multilingual browsing and editing for any of the hundreds of languages made available by the OMW. All tools and guidelines are freely available under an open license. 
In this paper, we present our Crossword Puzzle Resolution System (SACRY), which exploits syntactic structures for clue reranking and answer extraction. SACRY uses a database (DB) containing previously solved CPs in order to generate the list of candidate answers. Additionally, it uses innovative features, such as the answer position in the rank and aggregated information such as the min, max and average clue reranking scores. Our system is based on WebCrow, one of the most advanced systems for automatic crossword puzzle resolution. Our extensive experiments over our two million clue dataset show that our approach highly improves the quality of the answer list, enabling the achievement of unprecedented results on the complete CP resolution tasks, i.e., accuracy of 99.17%. 
Lexical Simpliﬁcation consists in replacing complex words in a text with simpler alternatives. We introduce LEXenstein, the ﬁrst open source framework for Lexical Simpliﬁcation. It covers all major stages of the process and allows for easy benchmarking of various approaches. We test the tool’s performance and report comparisons on different datasets against the state of the art approaches. The results show that combining the novel Substitution Selection and Substitution Ranking approaches introduced in LEXenstein is the most effective approach to Lexical Simpliﬁcation.  set of scripts designed for the training and testing of ranking models provided by (Jauhar and Specia, 2012)1. However, they cover only one step of the process. In an effort to tackle this issue, we present LEXenstein: a framework for Lexical Simpliﬁcation development and benchmarking. LEXenstein is an easy-to-use framework that provides simpliﬁed access to many approaches for several sub-tasks of the LS pipeline, which is illustrated in Figure 1. Its current version includes methods for the three main sub-tasks in the pipeline: Substitution Generation, Substitution Selection and Substitution Ranking.  
Annotations are increasingly created and shared online and connected with web resources such as databases of real-world entities. Recent collaborative efforts to provide interoperability between online annotation tools and resources have introduced the Open Annotation (OA) model, a general framework for representing annotations based on web standards. Building on the OA model, we propose to share annotations over a minimal web interface that conforms to the Representational State Transfer architectural style and uses the JSON for Linking Data representation (JSON-LD). We introduce tools supporting this approach and apply it to several existing annotation clients and servers, demonstrating direct interoperability between tools and resources that were previously unable to exchange information. The speciﬁcation and tools are available from http://restoa.github.io/. 
This paper reports on and demonstrates META-SHARE/QT21, a prototype implementation of a data sharing and annotation service platform, which was based on the META-SHARE infrastructure. META-SHARE, which has been designed for sharing datasets and tools, is enhanced with a processing layer for annotating textual content with appropriate NLP services that are documented with the appropriate metadata. In META-SHARE/QT21 pre-defined processing workflows are offered to the users; each workflow is a pipeline of atomic NLP services/tools (e.g. sentence splitting, part-of-speech tagging). Currently, workflows for annotating monolingual and bilingual resources of various formats are provided (e.g. XCES, TXT, TMX). From the legal framework point of view, a simple operational model is adopted by which only openly licensed datasets can be processed by openly licensed services. 
This paper introduces a web-based visualization framework for graph-based distributional semantic models. The visualization supports a wide range of data structures, including term similarities, similarities of contexts, support of multiword expressions, sense clusters for terms and sense labels. In contrast to other browsers of semantic resources, our visualization accepts input sentences, which are subsequently processed with languageindependent or language-dependent ways to compute term-context representations. Our web demonstrator currently contains models for multiple languages, based on different preprocessing such as dependency parsing and n-gram context representations. These models can be accessed from a database, the web interface and via a RESTful API. The latter facilitates the quick integration of such models in research prototypes. 
We introduce an argument generation system in debating, one that is based on sentence retrieval. Users can specify a motion such as This house should ban gambling, and a stance on whether the system agrees or disagrees with the motion. Then the system outputs three argument paragraphs based on “values” automatically decided by the system. The “value” indicates a topic that is considered as a positive or negative for people or communities, such as health and education. Each paragraph is related to one value and composed of about seven sentences. An evaluation over 50 motions from a popular debate website showed that the generated arguments are understandable in 64 paragraphs out of 150. 
This paper presents QUEST++ , an open source tool for quality estimation which can predict quality for texts at word, sentence and document level. It also provides pipelined processing, whereby predictions made at a lower level (e.g. for words) can be used as input to build models for predictions at a higher level (e.g. sentences). QUEST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that QUEST++ achieves state-of-the-art performance. 
Word alignment (WA) between a pair of sentences in the same or different languages is a key component of many natural language processing tasks. It is commonly used for identifying the translation relationships between words and phrases in parallel sentences from two different languages. WA-Continuum is a tool designed for the visualisation of WAs. It was initially built to aid research studying WAs and ways to improve them. The tool relies on the automated mark-up of WAs, as typically produced by WA tools. Different from most previous work, it presents the alignment information graphically in a WA matrix that can be easily understood by users, as opposed to text connected by lines. The key features of the tool are the ability to visualise WA matrices for multiple parallel aligned sentences simultaneously in a single place, coupled with powerful search and selection components to ﬁnd and inspect particular sentences as required. 
We describe the design, development, and API of ODIN (Open Domain INformer), a domainindependent, rule-based event extraction (EE) framework. The proposed EE approach is: simple (most events are captured with simple lexico-syntactic patterns), powerful (the language can capture complex constructs, such as events taking other events as arguments, and regular expressions over syntactic graphs), robust (to recover from syntactic parsing errors, syntactic patterns can be freely mixed with surface, token-based patterns), and fast (the runtime environment processes 110 sentences/second in a real-world domain with a grammar of over 200 rules). We used this framework to develop a grammar for the biochemical domain, which approached human performance. Our EE framework is accompanied by a web-based user interface for the rapid development of event grammars and visualization of matches. The ODIN framework and the domain-speciﬁc grammars are available as open-source code. 
To better organize and understand online news information, we propose Storybase1, a knowledge base for news events that builds upon Wikipedia current events and daily Web news. It ﬁrst constructs stories and their timelines based on Wikipedia current events and then detects and links daily news to enrich those Wikipedia stories with more comprehensive events. We encode events and develop efﬁcient event clustering and chaining techniques in an event space. We demonstrate Storybase with a news events search engine that helps ﬁnd historical and ongoing news stories and inspect their dynamic timelines. 
This paper describes WriteAhead, a resource-rich, Interactive Writing Environment that provides L2 learners with writing prompts, as well as ”get it right” advice, to helps them write ﬂuently and accurately. The method involves automatically analyzing reference and learner corpora, extracting grammar patterns with example phrases, and computing dubious, overused patterns. At run-time, as the user types (or mouses over) a word, the system automatically retrieves and displays grammar patterns and examples, most relevant to the word. The user can opt for patterns from a general corpus, academic corpus, learner corpus, or commonly overused dubious patterns found in a learner corpus. WriteAhead proactively engages the user with steady, timely, and spot-on information for effective assisted writing. Preliminary experiments show that WriteAhead fulﬁlls the design goal of fostering learner independence and encouraging self-editing, and is likely to induce better writing, and improve writing skills in the long run. 
We present a new toolkit - NiuParser for Chinese syntactic and semantic analysis. It can handle a wide range of Natural Language Processing (NLP) tasks in Chinese, including word segmentation, partof-speech tagging, named entity recognition, chunking, constituent parsing, dependency parsing, and semantic role labeling. The NiuParser system runs fast and shows state-of-the-art performance on several benchmarks. Moreover, it is very easy to use for both research and industrial purposes. Advanced features include the Software Development Kit (SDK) interfaces and a multi-thread implementation for system speed-up. 
Spoken dialogue systems (SDS) are rapidly appearing in various smart devices (smartphone, smart-TV, in-car navigating system, etc). The key role in a successful SDS is a spoken language understanding (SLU) component, which parses user utterances into semantic concepts in order to understand users’ intentions. However, such semantic concepts and their structure are manually created by experts, and the annotation process results in extremely high cost and poor scalability in system development. Therefore, the dissertation focuses on improving SDS generalization and scalability by automatically inferring domain knowledge and learning structures from unlabeled conversations through a matrix factorization (MF) technique. With the automatically acquired semantic concepts and structures, we further investigate whether such information can be utilized to effectively understand user utterances and then show the feasibility of reducing human effort during SDS development. 
This paper presents a method to improve the translation of polysemous nouns, when a previous occurrence of the noun as the head of a compound noun phrase is available in a text. The occurrences are identiﬁed through pattern matching rules, which detect XY compounds followed closely by a potentially coreferent occurrence of Y , such as “Nordwand . . . Wand”. Two strategies are proposed to improve the translation of the second occurrence of Y : re-using the cached translation of Y from the XY compound, or post-editing the translation of Y using the head of the translation of XY . Experiments are performed on Chinese-toEnglish and German-to-French statistical machine translation, over the WIT3 and Text+Berg corpora respectively, with 261 XY /Y pairs each. The results suggest that while the overall BLEU scores increase only slightly, the translations of the targeted polysemous nouns are signiﬁcantly improved. 
In the proposed doctoral work we will design an end-to-end approach for the challenging NLP task of text-level discourse parsing. Instead of depending on mostly hand-engineered sparse features and independent components for each subtask, we propose a uniﬁed approach completely based on deep learning architectures. To train more expressive representations that capture communicative functions and semantic roles of discourse units and relations between them, we will jointly learn all discourse parsing subtasks at different layers of our architecture and share their intermediate representations. By combining unsupervised training of word embeddings with our layer-wise multi-task learning of higher representations we hope to reach or even surpass performance of current state-of-the-art methods on annotated English corpora. 
In most of the dependency parsing studies, dependency relations within a sentence are often presented as a tree structure. Whilst the tree structure is sufﬁcient to represent the surface relations, deep dependencies which may result to multi-headed relations require more general dependency structures, namely Directed Acyclic Graphs (DAGs). This study proposes a new dependency DAG parsing approach which uses a dynamic oracle within a shift-reduce transitionbased parsing framework. Although there is still room for improvement on performance with more feature engineering, we already obtain competitive performances compared to static oracles as a result of our initial experiments conducted on the ITU-METU-Sabancı Turkish Treebank (IMST). 
Social media has attracted attention because of its potential for extraction of information of various types. For example, information collected from Twitter enables us to build useful applications such as predicting an epidemic of inﬂuenza. However, using text information from social media poses challenges for event detection because of the unreliable nature of user-generated texts, which often include counter-factual statements. Consequently, this study proposes the use of modality features to improve disease event detection from Twitter messages, or “tweets”. Experimental results demonstrate that the combination of a modality dictionary and a modality analyzer improves the F1-score by 3.5 points. 
We have constructed two research resources of Japanese lexical simpliﬁcation. One is a simpliﬁcation system that supports reading comprehension of a wide range of readers, including children and language learners. The other is a dataset for evaluation that enables open discussions with other systems. Both the system and the dataset are made available providing the ﬁrst such resources for the Japanese language. 
 Abstract Meaning Representation (AMR) is a semantic representation language used to capture the meaning of English sentences. In this work, we propose an AMR parser based on dependency parse rewrite rules. This approach transfers dependency parses into AMRs by integrating the syntactic dependencies, semantic arguments, named entity and co-reference information. A dependency parse to AMR graph aligner is also introduced as a preliminary step for designing the parser. 
Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and provide an evaluation, demonstrating its reliability and robustness. Based on our procedure and taking the RG-65 word similarity dataset as a reference, we release two high-quality Spanish and Farsi (Persian) monolingual datasets, and ﬁfteen cross-lingual datasets for six languages: English, Spanish, French, German, Portuguese, and Farsi. 
Computing pairwise word semantic similarity is widely used and serves as a building block in many tasks in NLP. In this paper, we explore the embedding of the shortest-path metrics from a knowledge base (Wordnet) into the Hamming hypercube, in order to enhance the computation performance. We show that, although an isometric embedding is untractable, it is possible to achieve good non-isometric embeddings. We report a speedup of three orders of magnitude for the task of computing Leacock and Chodorow (LCH) similarity while keeping strong correlations (r = .819, ρ = .826). 
In recent years, there has been an increasing interest in learning a distributed representation of word sense. Traditional context clustering based models usually require careful tuning of model parameters, and typically perform worse on infrequent word senses. This paper presents a novel approach which addresses these limitations by ﬁrst initializing the word sense embeddings through learning sentencelevel embeddings from WordNet glosses using a convolutional neural networks. The initialized word sense embeddings are used by a context clustering based model to generate the distributed representations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 
Distributional semantic models have trouble distinguishing strongly contrasting words (such as antonyms) from highly compatible ones (such as synonyms), because both kinds tend to occur in similar contexts in corpora. We introduce the multitask Lexical Contrast Model (mLCM), an extension of the effective Skip-gram method that optimizes semantic vectors on the joint tasks of predicting corpus contexts and making the representations of WordNet synonyms closer than that of matching WordNet antonyms. mLCM outperforms Skip-gram both on general semantic tasks and on synonym/antonym discrimination, even when no direct lexical contrast information about the test words is provided during training. mLCM also shows promising results on the task of learning a compositional negation operator mapping adjectives to their antonyms. 
In this paper, we address semi-supervised sentiment learning via semi-stacking, which integrates two or more semi-supervised learning algorithms from an ensemble learning perspective. Specifically, we apply metalearning to predict the unlabeled data given the outputs from the member algorithms and propose N-fold cross validation to guarantee a suitable size of the data for training the meta-classifier. Evaluation on four domains shows that such a semi-stacking strategy performs consistently better than its member algorithms. 
 different topics. However, most existing sequen-  We present a general framework for incorporating sequential data and arbitrary features into language modeling. The general  tial data modeling methods are not capable of incorporating the information from both the topic and the author’s identity. More generally, there is no sufﬁciently ﬂexible sequential model that al-  framework consists of two parts: a hidden  lows incorporating an arbitrary set of features.  Markov component and a recursive neural network component. We demonstrate the effectiveness of our model by applying it to a speciﬁc application: predicting topics and sentiments in dialogues. Experiments on real data demonstrate that our method is substantially more accurate than previous methods.  In this paper, we present a Deep Markov Neural Network (DMNN) for incorporating sequential data and arbitrary features into language modeling. Our method learns from general sequential observations. It is also capable of taking the ordering of words into account, and collecting information from arbitrary features associated with the context. Comparing to traditional HMM-based  
Predicting the helpfulness of product reviews is a key component of many ecommerce tasks such as review ranking and recommendation. However, previous work mixed review helpfulness prediction with those outer layer tasks. Using nontext features, it leads to less transferable models. This paper solves the problem from a new angle by hypothesizing that helpfulness is an internal property of text. Purely using review text, we isolate review helpfulness prediction from its outer layer tasks, employ two interpretable semantic features, and use human scoring of helpfulness as ground truth. Experimental results show that the two semantic features can accurately predict helpfulness scores and greatly improve the performance compared with using features previously used. Cross-category test further shows the models trained with semantic features are easier to be generalized to reviews of different product categories. The models we built are also highly interpretable and align well with human annotations. 
 There have been many recent advances in the structure and measurement of distributed language models: those that map from words to a vector-space that is rich in information about word choice and composition. This vector-space is the distributed language representation. The goal of this note is to point out that any distributed representation can be turned into a classiﬁer through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also ﬁnd that it performs as well as or better than complex purpose-built algorithms.  
We explore using relevant tweets of a given news article to help sentence compression for generating compressive news highlights. We extend an unsupervised dependency-tree based sentence compression approach by incorporating tweet information to weight the tree edge in terms of informativeness and syntactic importance. The experimental results on a public corpus that contains both news articles and relevant tweets show that our proposed tweets guided sentence compression method can improve the summarization performance signiﬁcantly compared to the baseline generic sentence compression method. 
The validity of applying paraphrase rules depends on the domain of the text that they are being applied to. We develop a novel method for extracting domainspeciﬁc paraphrases. We adapt the bilingual pivoting paraphrase method to bias the training data to be more like our target domain of biology. Our best model results in higher precision while retaining complete recall, giving a 10% relative improvement in AUC. 
Simpliﬁcation of lexically complex texts, by replacing complex words with their simpler synonyms, helps non-native speakers, children, and language-impaired people understand text better. Recent lexical simpliﬁcation methods rely on manually simpliﬁed corpora, which are expensive and time-consuming to build. We present an unsupervised approach to lexical simpliﬁcation that makes use of the most recent word vector representations and requires only regular corpora. Results of both automated and human evaluation show that our simple method is as effective as systems that rely on simpliﬁed corpora. 
This paper describes an experiment to elicit referring expressions from human subjects for research in natural language generation and related ﬁelds, and preliminary results of a computational model for the generation of these expressions. Unlike existing resources of this kind, the resulting data set - the Zoom corpus of natural language descriptions of map locations - takes into account a domain that is signiﬁcantly closer to real-world applications than what has been considered in previous work, and addresses more complex situations of reference, including contexts with different levels of detail, and instances of singular and plural reference produced by speakers of Spanish and Portuguese. 
We present an experiment to compare a standard, minimally distinguishing algorithm for the generation of relational referring expressions with two alternatives that produce overspeciﬁed descriptions. The experiment shows that discrimination which normally plays a major role in the disambiguation task - is also a major inﬂuence in referential overspeciﬁcation, even though disambiguation is in principle not relevant. 
This paper is the ﬁrst to examine the effect of prosodic features on coreference resolution in spoken discourse. We test features from different prosodic levels and investigate which strategies can be applied. Our results on the basis of manual prosodic labelling show that the presence of an accent is a helpful feature in a machine-learning setting. Including prosodic boundaries and determining whether the accent is the nuclear accent further increases results. 
Discourse parsing is the process of discovering the latent relational structure of a long form piece of text and remains a signiﬁcant open challenge. One of the most difﬁcult tasks in discourse parsing is the classiﬁcation of implicit discourse relations. Most state-of-the-art systems do not leverage the great volume of unlabeled text available on the web–they rely instead on human annotated training data. By incorporating a mixture of labeled and unlabeled data, we are able to improve relation classiﬁcation accuracy, reduce the need for annotated data, while still retaining the capacity to use labeled data to ensure that speciﬁc desired relations are learned. We achieve this using a latent variable model that is trained in a reduced dimensionality subspace using spectral methods. Our approach achieves an F1 score of 0.485 on the implicit relation labeling task for the Penn Discourse Treebank. 
A wide array of natural dialogue discourse can be found on the internet. Previous attempts to automatically determine disagreement between interlocutors in such dialogue have mostly relied on n-gram and grammatical dependency features taken from respondent text. Agreement-disagreement classiﬁers built upon these baseline features tend to do poorly, yet have proven difﬁcult to improve upon. Using the Internet Argument Corpus, which comprises quote and response post pairs taken from an online debate forum with human-annotated agreement scoring, we introduce semantic environment features derived by comparing quote and response sentences which align well. We show that this method improves classiﬁer accuracy relative to the baseline method namely in the retrieval of disagreeing pairs, which improves from 69% to 77%. 
Two recent approaches have achieved state-of-the-art results in image captioning. The ﬁrst uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the ﬁrst time by using the same state-ofthe-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments. 
In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation. 
We propose IMAGINET, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Mimicking an important aspect of human language learning, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases. 
We exploit the visual properties of concepts for lexical entailment detection by examining a concept’s generality. We introduce three unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, signiﬁcantly outperforming a competitive frequencybased baseline. 
A language lexicon can be divided into four main strata, depending on origin of words: core vocabulary words, fully- and partiallyassimilated foreign words, and unassimilated foreign words (or transliterations). This paper focuses on translation of fullyand partially-assimilated foreign words, called “borrowed words”. Borrowed words (or loanwords) are content words found in nearly all languages, occupying up to 70% of the vocabulary. We use models of lexical borrowing in machine translation as a pivoting mechanism to obtain translations of out-of-vocabulary loanwords in a lowresource language. Our framework obtains substantial improvements (up to 1.6 BLEU) over standard baselines. 
The inability to model long-distance dependency has been handicapping SMT for years. Speciﬁcally, the context independence assumption makes it hard to capture the dependency between translation rules. In this paper, we introduce a novel recurrent neural network based rule sequence model to incorporate arbitrary long contextual information during estimating probabilities of rule sequences. Moreover, our model frees the translation model from keeping huge and redundant grammars, resulting in more efﬁcient training and decoding. Experimental results show that our method achieves a 0.9 point BLEU gain over the baseline, and a signiﬁcant reduction in rule table size for both phrase-based and hierarchical phrase-based systems. 
 S=M  This paper explores a simple discriminative preordering model for statistical machine translation. Our model traverses binary constituent trees, and classiﬁes whether children of each node should be reordered. The model itself is not extremely novel, but herein we introduce a new procedure to determine oracle labels so as to maximize Kendall’s τ . Experiments in Japanese-to-English translation revealed that our simple method is comparable with, or superior to, state-of-the-art methods in translation accuracy. 
A lightweight, human-in-the-loop evaluation scheme for machine translation (MT) systems is proposed. It extrinsically evaluates MT systems using human subjects’ scores on second language ability test problems that are machine-translated to the subjects’ native language. A largescale experiment involving 320 subjects revealed that the context-unawareness of the current MT systems severely damages human performance when solving the test problems, while one of the evaluated MT systems performed as good as a human translation produced in a context-unaware condition. An analysis of the experimental results showed that the extrinsic evaluation captured a different dimension of translation quality than that captured by manual and automatic intrinsic evaluation. 
Precisely evaluating the quality of a translation against human references is a challenging task due to the ﬂexible word ordering of a sentence and the existence of a large number of synonyms for words. This paper proposes to evaluate translations with distributed representations of words and sentences. We study several metrics based on word and sentence representations and their combination. Experiments on the WMT metric task shows that the metric based on the combined representations achieves the best performance, outperforming the state-of-the-art translation metrics by a large margin. In particular, training the distributed representations only needs a reasonable amount of monolingual, unlabeled data that is not necessary drawn from the test domain. 
Downstream processing of machine translation (MT) output promises to be a solution to improve translation quality, especially when the MT system’s internal decoding process is not accessible. Both rule-based and statistical automatic postediting (APE) methods have been proposed over the years, but with contrasting results. A missing aspect in previous evaluations is the assessment of different methods: i) under comparable conditions, and ii) on different language pairs featuring variable levels of MT quality. Focusing on statistical APE methods (more portable across languages), we propose the ﬁrst systematic analysis of two approaches. To understand their potential, we compare them in the same conditions over six language pairs having English as source. Our results evidence consistent improvements on all language pairs, a relation between the extent of the gain and MT output quality, slight but statistically signiﬁcant performance differences between the two methods, and their possible complementarity. 
Replicated Softmax model, a well-known undirected topic model, is powerful in extracting semantic representations of documents. Traditional learning strategies such as Contrastive Divergence are very inefﬁcient. This paper provides a novel estimator to speed up the learning based on Noise Contrastive Estimate, extended for documents of variant lengths and weighted inputs. Experiments on two benchmarks show that the new estimator achieves great learning efﬁciency and high accuracy on document retrieval and classiﬁcation. 
We present a simple yet effective unsupervised domain adaptation method that can be generally applied for different NLP tasks. Our method uses unlabeled target domain instances to induce a set of instance similarity features. These features are then combined with the original features to represent labeled source domain instances. Using three NLP tasks, we show that our method consistently outperforms a few baselines, including SCL, an existing general unsupervised domain adaptation method widely used in NLP. More importantly, our method is very easy to implement and incurs much less computational cost than SCL. 
In sentence modeling and classiﬁcation, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To combine deep learning with linguistic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classiﬁcation tasks, and achieves the highest published accuracy on TREC. 
Text regression has traditionally been tackled using linear models. Here we present a non-linear method based on a deep convolutional neural network. We show that despite having millions of parameters, this model can be trained on only a thousand documents, resulting in a 40% relative improvement over sparse linear models, the previous state of the art. Further, this method is ﬂexible allowing for easy incorporation of side information such as document meta-data. Finally we present a novel technique for interpreting the effect of different text inputs on this complex non-linear model. 
Log-bilinear language models such as SkipGram and GloVe have been proven to capture high quality syntactic and semantic relationships between words in a vector space. We revisit the relationship between SkipGram and GloVe models from a machine learning viewpoint, and show that these two methods are easily merged into a uniﬁed form. Then, by using the uniﬁed form, we extract the factors of the conﬁgurations that they use differently. We also empirically investigate which factor is responsible for the performance difference often observed in widely examined word similarity and analogy tasks. 
In this paper, we apply the concept of pretraining to hidden-unit conditional random ﬁelds (HUCRFs) to enable learning on unlabeled data. We present a simple yet effective pre-training technique that learns to associate words with their clusters, which are obtained in an unsupervised manner. The learned parameters are then used to initialize the supervised learning process. We also propose a word clustering technique based on canonical correlation analysis (CCA) that is sensitive to multiple word senses, to further improve the accuracy within the proposed framework. We report consistent gains over standard conditional random ﬁelds (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 
Automatic resolution of Crossword Puzzles (CPs) heavily depends on the quality of the answer candidate lists produced by a retrieval system for each clue of the puzzle grid. Previous work has shown that such lists can be generated using Information Retrieval (IR) search algorithms applied to the databases containing previously solved CPs and reranked with tree kernels (TKs) applied to a syntactic tree representation of the clues. In this paper, we create a labelled dataset of 2 million clues on which we apply an innovative Distributional Neural Network (DNN) for reranking clue pairs. Our DNN is computationally efﬁcient and can thus take advantage of such large datasets showing a large improvement over the TK approach, when the latter uses small training data. In contrast, when data is scarce, TKs outperform DNNs. 
With massively parallel corpora of hundreds or thousands of translations of the same text, it is possible to automatically perform typological studies of language structure using very large language samples. We investigate the domain of word order using multilingual word alignment and high-precision annotation transfer in a corpus with 1144 translations in 986 languages of the New Testament. Results are encouraging, with 86% to 96% agreement between our method and the manually created WALS database for a range of different word order features. Beyond reproducing the categorical data in WALS and extending it to hundreds of other languages, we also provide quantitative data for the relative frequencies of different word orders, and show the usefulness of this for language comparison. Our method has applications for basic research in linguistic typology, as well as for NLP tasks like transfer learning for dependency parsing, which has been shown to beneﬁt from word order information. 
A deﬁning symptom of autism spectrum disorder (ASD) is the presence of restricted and repetitive activities and interests, which can surface in language as a perseverative focus on idiosyncratic topics. In this paper, we use semantic similarity measures to identify such idiosyncratic topics in narratives produced by children with and without ASD. We ﬁnd that neurotypical children tend to use the same words and semantic concepts when retelling the same narrative, while children with ASD, even when producing accurate retellings, use different words and concepts relative not only to neurotypical children but also to other children with ASD. Our results indicate that children with ASD not only stray from the target topic but do so in idiosyncratic ways according to their own restricted interests. 
We consider the task of identifying and labeling the semantic arguments of a predicate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach augments an existing model with features derived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4% absolute increase in F1 versus the original model. 
This paper addresses a novel task of semantically analyzing the comparative constructions inherent in attributive superlative expressions against structured knowledge bases (KBs). The task can be deﬁned in two-fold: ﬁrst, selecting the comparison dimension against a KB, on which the involved items are compared; and second, determining the ranking order, in which the items are ranked (ascending or descending). We exploit Wikipedia and Freebase to collect training data in an unsupervised manner, where a neural network model is then learnt to select, from Freebase predicates, the most appropriate comparison dimension for a given superlative expression, and further determine its ranking order heuristically. Experimental results show that it is possible to learn from coarsely obtained training data to semantically characterize the comparative constructions involved in attributive superlative expressions. 
Multi-modal semantics has relied on feature norms or raw image data for perceptual input. In this paper we examine grounding semantic representations in olfactory (smell) data, through the construction of a novel bag of chemical compounds model. We use standard evaluations for multi-modal semantics, including measuring conceptual similarity and cross-modal zero-shot learning. To our knowledge, this is the ﬁrst work to evaluate semantic similarity on representations grounded in olfactory data. 
We present a novel scheme for wordbased Japanese typed dependency parser which integrates syntactic structure analysis and grammatical function analysis such as predicate-argument structure analysis. Compared to bunsetsu-based dependency parsing, which is predominantly used in Japanese NLP, it provides a natural way of extracting syntactic constituents, which is useful for downstream applications such as statistical machine translation. It also makes it possible to jointly decide dependency and predicate-argument structure, which is usually implemented as two separate steps. We convert an existing treebank to the new dependency scheme and report parsing results as a baseline for future research. We achieved a better accuracy for assigning function labels than a predicate-argument structure analyzer by using grammatical functions as dependency label. 
We present KLcpos3, a language similarity measure based on Kullback-Leibler divergence of coarse part-of-speech tag trigram distributions in tagged corpora. It has been designed for multilingual delexicalized parsing, both for source treebank selection in single-source parser transfer, and for source treebank weighting in multi-source transfer. In the selection task, KLcpos3 identiﬁes the best source treebank in 8 out of 18 cases. In the weighting task, it brings +4.5% UAS absolute, compared to unweighted parse tree combination. 
Recent work on supertagging using a feedforward neural network achieved signiﬁcant improvements for CCG supertagging and parsing (Lewis and Steedman, 2014). However, their architecture is limited to considering local contexts and does not naturally model sequences of arbitrary length. In this paper, we show how directly capturing sequence information using a recurrent neural network leads to further accuracy improvements for both supertagging (up to 1.9%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text. 
We deﬁne a dynamic oracle for the Covington non-projective dependency parser. This is not only the ﬁrst dynamic oracle that supports arbitrary non-projectivity, but also considerably more efﬁcient (O(n)) than the only existing oracle with restricted non-projectivity support. Experiments show that training with the dynamic oracle signiﬁcantly improves parsing accuracy over the static oracle baseline on a wide range of treebanks. 
We present a novel solution to improve the performance of Chinese word segmentation (CWS) using a synthetic word parser. The parser analyses the internal structure of words, and attempts to convert out-of-vocabulary words (OOVs) into in-vocabulary ﬁne-grained sub-words. We propose a pipeline CWS system that ﬁrst predicts this ﬁne-grained segmentation, then chunks the output to reconstruct the original word segmentation standard. We achieve competitive results on the PKU and MSR datasets, with substantial improvements in OOV recall. 
We present a simple method for learning part-of-speech taggers for languages like Akawaio, Aukan, or Cakchiquel – languages for which nothing but a translation of parts of the Bible exists. By aggregating over the tags from a few annotated languages and spreading them via wordalignment on the verses, we learn POS taggers for 100 languages, using the languages to bootstrap each other. We evaluate our cross-lingual models on the 25 languages where test sets exist, as well as on another 10 for which we have tag dictionaries. Our approach performs much better (20-30%) than state-of-the-art unsupervised POS taggers induced from Bible translations, and is often competitive with weakly supervised approaches that assume high-quality parallel corpora, representative monolingual corpora with perfect tokenization, and/or tag dictionaries. We make models for all 100 languages available. 
Distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 
In contrast with traditional relation extraction, which only considers a ﬁxed set of relations, Open Information Extraction (Open IE) aims at extracting all types of relations from text. Because of data sparseness, Open IE systems typically ignore lexical information, and instead employ parse trees and Part-of-Speech (POS) tags. However, the same syntactic structure may correspond to different relations. In this paper, we propose to use a lexicalized tree kernel based on the word embeddings created by a neural network model. We show that the lexicalized tree kernel model surpasses the unlexicalized model. Experiments on three datasets indicate that our Open IE system performs better on the task of relation extraction than the stateof-the-art Open IE systems of Xu et al. (2013) and Mesquita et al. (2013). 
We propose a new approach to the task of ﬁne grained entity type classiﬁcations based on label embeddings that allows for information sharing among related labels. Speciﬁcally, we learn an embedding for each label and each feature such that labels which frequently co-occur are close in the embedded space. We show that it outperforms state-of-the-art methods on two ﬁne grained entity-classiﬁcation benchmarks and that the model can exploit the ﬁner-grained labels to improve classiﬁcation of standard coarse types. 
We examine a key task in biomedical text processing, normalization of disorder mentions. We present a multi-pass sieve approach to this task, which has the advantage of simplicity and modularity. Our approach is evaluated on two datasets, one comprising clinical reports and the other comprising biomedical abstracts, achieving state-of-the-art results. 
Semantic applications typically extract information from intermediate structures derived from sentences, such as dependency parse or semantic role labeling. In this paper, we study Open Information Extraction’s (Open IE) output as an additional intermediate structure and ﬁnd that for tasks such as text comprehension, word similarity and word analogy it can be very effective. Speciﬁcally, for word analogy, Open IE-based embeddings surpass the state of the art. We suggest that semantic applications will likely beneﬁt from adding Open IE format to their set of potential sentencelevel structures. 
Pronouns are frequently dropped in Chinese sentences, especially in informal data such as text messages. In this work we propose a solution to recover dropped pronouns in SMS data. We manually annotate dropped pronouns in 684 SMS ﬁles and apply machine learning algorithms to recover them, leveraging lexical, contextual and syntactic information as features. We believe this is the ﬁrst work on recovering dropped pronouns in Chinese text messages. 
We give an algorithm for disambiguating generic versus referential uses of secondperson pronouns in restaurant reviews in Chinese. Reviews in this domain use the ‘you’ pronoun 你 either generically or to refer to shopkeepers, readers, or for selfreference in reported conversation. We ﬁrst show that linguistic features of the local context (drawn from prior literature) help in disambigation. We then show that document-level features (n-grams and document-level embeddings)— not previously used in the referentiality literature— actually give the largest gain in performance, and suggest this is because pronouns in this domain exhibit ‘one-senseper-discourse’. Our work highlights an important case of discourse effects on pronoun use, and may suggest practical implications for audience extraction and other sentiment tasks in online reviews. 
We propose an unsupervised probabilistic model for zero pronoun resolution. To our knowledge, this is the first such model that (1) is trained on zero pronouns in an unsupervised manner; (2) jointly identifies and resolves anaphoric zero pronouns; and (3) exploits discourse information provided by a salience model. Experiments demonstrate that our unsupervised model significantly outperforms its state-of-the-art unsupervised counterpart when resolving the Chinese zero pronouns in the OntoNotes corpus. 
 Co-Simrank is a useful Simrank-like mea- sure of similarity based on graph structure. The existing method iteratively computes each pair of Co-Simrank score from a dot product of two Pagerank vectors, entailing O(log(1/ǫ)n3) time to compute all pairs of Co-Simranks in a graph with n nodes, to attain a desired accuracy ǫ. In this study, we devise a model, Co-Simmate, to speed up the retrieval of all pairs of Co-Simranks to O(log2(log(1/ǫ))n3) time. Moreover, we show the optimality of Co-Simmate among other hop-(uk) variations, and integrate it with a matrix decomposition based method on singular graphs to attain higher efﬁciency. The viable experiments verify the superiority of Co-Simmate to others.  
In this paper, we present a test collection for mathematical information retrieval composed of real-life, researchlevel mathematical information needs. Topics and relevance judgements have been procured from the on-line collaboration website MathOverﬂow by delegating domain-speciﬁc decisions to experts on-line. With our test collection, we construct a baseline using Lucene’s vectorspace model implementation and conduct an experiment to investigate how prior extraction of technical terms from mathematical text can affect retrieval efﬁciency. We show that by boosting the importance of technical terms, statistically signiﬁcant improvements in retrieval performance can be obtained over the baseline. 
Many queries in web search are ambiguous or multifaceted. Identifying the major senses or facets of queries is very important for web search. In this paper, we represent the major senses or facets of queries as subtopics and refer to indentifying senses or facets of queries as query subtopic mining, where query subtopic are represented as a number of clusters of queries. Then the challenges of query subtopic mining are how to measure the similarity between queries and group them semantically. This paper proposes an approach for mining subtopics from query log, which jointly learns a similarity measure and groups queries by explicitly modeling the structure among them. Compared with previous approaches using manually defined similarity measures, our approach produces more desirable query subtopics by learning a similarity measure. Experimental results on real queries collected from a search engine log confirm the effectiveness of the proposed approach in mining query subtopics. 
 Existing studies have utilized Wikipedia for various knowledge acquisition tasks. However, no attempts have been made to explore multi-level topic knowledge contained in Wikipedia articles’ Contents tables. The articles with similar subjects are grouped together into Wikipedia categories. In this work, we propose novel methods to automatically construct comprehensive topic hierarchies for given categories based on the structured Contents tables as well as corresponding unstructured text descriptions. Such a hierarchy is important for information browsing, document organization and topic prediction. Experimental results show our proposed approach, incorporating both the structural and textual information, achieves high quality category topic hierarchies. 
Short texts usually encounter data sparsity and ambiguity problems in representations for their lack of context. In this paper, we propose a novel method to model short texts based on semantic clustering and convolutional neural network. Particularly, we ﬁrst discover semantic cliques in embedding spaces by a fast clustering algorithm. Then, multi-scale semantic units are detected under the supervision of semantic cliques, which introduce useful external knowledge for short texts. These meaningful semantic units are combined and fed into convolutional layer, followed by max-pooling operation. Experimental results on two open benchmarks validate the effectiveness of the proposed method. 
We study the event detection problem using convolutional neural networks (CNNs) that overcome the two fundamental limitations of the traditional feature-based approaches to this task: complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features. The experimental results show that the CNNs outperform the best reported feature-based systems in the general setting as well as the domain adaptation setting without resorting to extensive external resources. 
The task of event trigger labeling is typically addressed in the standard supervised setting: triggers for each target event type are annotated as training data, based on annotation guidelines. We propose an alternative approach, which takes the example trigger terms mentioned in the guidelines as seeds, and then applies an eventindependent similarity-based classiﬁer for trigger labeling. This way we can skip manual annotation for new event types, while requiring only minimal annotated training data for few example events at system setup. Our method is evaluated on the ACE-2005 dataset, achieving 5.7% F1 improvement over a state-of-the-art supervised system which uses the full training data. 
Methods for name matching, an important component to support downstream tasks such as entity linking and entity clustering, have focused on alphabetic languages, primarily English. In contrast, logogram languages such as Chinese remain untested. We evaluate methods for name matching in Chinese, including both string matching and learning approaches. Our approach, based on new representations for Chinese, improves both name matching and a downstream entity clustering task. 
We repurpose network security hardware to perform language identiﬁcation and language modeling tasks. The hardware is a deterministic pushdown transducer since it executes regular expressions and has a stack. One core is 2.4 times as fast at language identiﬁcation and 1.8 to 6 times as fast at part-of-speech language modeling. 
 alignment and projected tags are potentially noisy,  We propose an approach to cross-lingual named entity recognition model transfer without the use of parallel corpora. In addition to global de-lexicalized features, we introduce multilingual gazetteers that are generated using graph propagation, and cross-lingual word representation mappings without the use of parallel data. We target the e-commerce domain, which is challenging due to its unstructured and noisy nature. The experiments have shown that our approaches beat the strong MT baseline, where the English model is transferred to two languages: Spanish and Chinese.  making the trained models sub-optimal. Instead of projecting noisy labels explicitly, Wang and Manning (2014) project posterior marginals expectations as soft constraints. Das and Petrov (2011) projected POS tags from source language types to target language trigarms using graph propagation and used the projected label distribution to train robust POS taggers. Secondly, the availability of such bitext is limited especially for resourcepoor languages and domains, where it is often the case that available resources are moderately-sized monolingual/comparable corpora and small bilingual dictionaries. Instead, we seek a direct transfer approach (Figure 1) to cross-lingual NER (also classiﬁed as transductive transfer learning (Pan and Yang,  
 in computer vision (Wang et al., 2011b), bioinfor-  matics (Wang et al., 2013), natural language un-  In this paper, we propose an 1-norm  derstanding (Wang et al., 2011a), to name a few.  Symmetric Nonnegative Matrix Tri-  Compared to many traditional clustering meth-  Factorization ( 1 S-NMTF) framework  ods, such as K-means clustering, NMF has better  to cluster multi-type relational data by  mathematical interpretation, which usually lead  utilizing their interrelatedness. Due to  to improved accuracy on clustering (Ding et al.,  introducing the 1-norm distances in our  2010). Traditional clustering algorithms concen-  new objective function, the proposed ap-  trate on dealing with homogeneous data, in which  proach is robust against noise and outliers,  all the data belong to one single type (Wang et al.,  which are inherent in multi-relational data.  2011d). To deal with the richer data structures in  We also derive the solution algorithm and  modern real-world applications, symmetric Non-  rigorously analyze its correctness and  negative Matrix Tri-Factorization (NMTF)(Wang  convergence. The promising experimental  et al., 2011c) have demonstrated its effectiveness  results of the algorithm applied to text  on simultaneous clustering of multi-type relational  clustering on IMDB dataset validate the  data by utilizing the interrelatedness among differ-  proposed approach.  ent data types.  
Labeled data is not readily available for many natural language domains, and it typically requires expensive human effort with considerable domain knowledge to produce a set of labeled data. In this paper, we propose a simple unsupervised system that helps us create a labeled resource for categorical data (e.g., a document set) using only ﬁfteen minutes of human input. We utilize the labeled resources to discover important insights about the data. The entire process is domain independent, and demands no prior annotation samples, or rules speciﬁc to an annotation. 
We increase the lexical coverage of FrameNet through automatic paraphrasing. We use crowdsourcing to manually ﬁlter out bad paraphrases in order to ensure a high-precision resource. Our expanded FrameNet contains an additional 22K lexical units, a 3-fold increase over the current FrameNet, and achieves 40% better coverage when evaluated in a practical setting on New York Times data. 
Nowadays, there are a lot of natural language processing pipelines that are based on training data created by a few experts. This paper examines how the proliferation of the internet and its collaborative application possibilities can be practically used for NLP. For that purpose, we examine how the German version of Wiktionary can be used for a lemmatization task. We introduce IWNLP, an opensource parser for Wiktionary, that reimplements several MediaWiki markup language templates for conjugated verbs and declined adjectives. The lemmatization task is evaluated on three German corpora on which we compare our results with existing software for lemmatization. With Wiktionary as a resource, we obtain a high accuracy for the lemmatization of nouns and can even improve on the results of existing software for the lemmatization of nouns. 
Measuring word relatedness is an important ingredient of many NLP applications. Several datasets have been developed in order to evaluate such measures. The main drawback of existing datasets is the focus on single words, although natural language contains a large proportion of multiword terms. We propose the new TR9856 dataset which focuses on multi-word terms and is signiﬁcantly larger than existing datasets. The new dataset includes many real world terms such as acronyms and named entities, and further handles term ambiguity by providing topical context for all term pairs. We report baseline results for common relatedness methods over the new data, and exploit its magnitude to demonstrate that a combination of these methods outperforms each individual method. 
We present a new release of the Paraphrase Database. PPDB 2.0 includes a discriminatively re-ranked set of paraphrases that achieve a higher correlation with human judgments than PPDB 1.0’s heuristic rankings. Each paraphrase pair in the database now also includes ﬁnegrained entailment relations, word embedding similarities, and style annotations. 
Identifying the type of relationship between words provides a deeper insight into the history of a language and allows a better characterization of language relatedness. In this paper, we propose a computational approach for discriminating between cognates and borrowings. We show that orthographic features have discriminative power and we analyze the underlying linguistic factors that prove relevant in the classiﬁcation task. To our knowledge, this is the ﬁrst attempt of this kind. 
We describe the ﬁrst version of the Media Frames Corpus: several thousand news articles on three policy issues, annotated in terms of media framing. We motivate framing as a phenomenon of study for computational linguistics and describe our annotation process. 
We introduce Discriminative BLEU (∆BLEU), a novel metric for intrinsic evaluation of generated text in tasks that admit a diverse range of possible outputs. Reference strings are scored for quality by human raters on a scale of [−1, +1] to weight multi-reference BLEU. In tasks involving generation of conversational responses, ∆BLEU correlates reasonably with human judgments and outperforms sentence-level and IBM BLEU in terms of both Spearman’s ρ and Kendall’s τ . 
In Tibetan, as words are written consecutively without delimiters, finding unknown word boundary is difficult. This paper presents a hybrid approach for Tibetan unknown word identification for offline corpus processing. Firstly, Tibetan named entity is preprocessed based on natural annotation. Secondly, other Tibetan unknown words are extracted from word segmentation fragments using MTC, the combination of a statistical metric and a set of context sensitive rules. In addition, the preliminary experimental results on Tibetan News Corpus are reported. Lexicon-based Tibetan word segmentation system SegT with proposed unknown word extension mechanism is indeed helpful to promote the performance of Tibetan word segmentation. It increases the F-score of Tibetan word segmentation by 4.15% on random-selected test set. Our unknown word identification scheme can find new words in any length and in any field. 
We propose two improvements on lexical association used in embedding learning: factorizing individual dependency relations and using lexicographic knowledge from monolingual dictionaries. Both proposals provide low-entropy lexical cooccurrence information, and are empirically shown to improve embedding learning by performing notably better than several popular embedding models in similarity tasks. 
Data-driven representation learning for words is a technique of central importance in NLP. While indisputably useful as a source of features in downstream tasks, such vectors tend to consist of uninterpretable components whose relationship to the categories of traditional lexical semantic theories is tenuous at best. We present a method for constructing interpretable word vectors from hand-crafted linguistic resources like WordNet, FrameNet etc. These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We analyze their performance on state-of-the-art evaluation methods for distributional models of word vectors and ﬁnd they are competitive to standard distributional approaches. 
In this article, we ﬁrst propose to exploit a new criterion for improving distributional thesauri. Following a bootstrapping perspective, we select relations between the terms of similar nominal compounds for building in an unsupervised way the training set of a classiﬁer performing the reranking of a thesaurus. Then, we evaluate several ways to combine thesauri reranked according to different criteria and show that exploiting the complementary information brought by these criteria leads to signiﬁcant improvements. 
It has been extensively observed that languages minimise the distance between two related words. Dependency length minimisation effects are explained as a means to reduce memory load and for effective communication. In this paper, we ask whether they hold in typically short spans, such as noun phrases, which could be thought of being less subject to efﬁciency pressure. We demonstrate that minimisation does occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies. 
Many NLP tools for English and German are based on manually annotated articles from the Wall Street Journal and Frankfurter Rundschau. The average readers of these two newspapers are middle-aged (55 and 47 years old, respectively), and the annotated articles are more than 20 years old by now. This leads us to speculate whether tools induced from these resources (such as part-of-speech taggers) put older language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region. 
Biterm Topic Model (BTM) is designed to model the generative process of the word co-occurrence patterns in short texts such as tweets. However, two aspects of BTM may restrict its performance: 1) user individualities are ignored to obtain the corpus level words co-occurrence patterns; and 2) the strong assumptions that two co-occurring words will be assigned the same topic label could not distinguish background words from topical words. In this paper, we propose Twitter-BTM model to address those issues by considering user level personalization in BTM. Firstly, we use user based biterms aggregation to learn user speciﬁc topic distribution. Secondly, each user’s preference between background words and topical words is estimated by incorporating a background topic. Experiments on a large-scale real-world Twitter dataset show that Twitter-BTM outperforms several stateof-the-art baselines. 
In this paper, we propose the new ﬁxedsize ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a ﬁxed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNNLMs can signiﬁcantly outperform not only the standard ﬁxed-input FNN-LMs but also the popular recurrent neural network (RNN) LMs. 
This paper proposes a new unsupervised method for decomposing a multi-author document into authorial components. We assume that we do not know anything about the document and the authors, except the number of the authors of that document. The key idea is to exploit the difference in the posterior probability of the Naive-Bayesian model to increase the precision of the clustering assignment and the accuracy of the classiﬁcation process of our method. Experimental results show that the proposed method outperforms two state-of-the-art methods. 
Topic Model such as Latent Dirichlet Allocation(LDA) makes assumption that topic assignment of different words are conditionally independent. In this paper, we propose a new model Extended Global Topic Random Field (EGTRF) to model non-linear dependencies between words. Speciﬁcally, we parse sentences into dependency trees and represent them as a graph, and assume the topic assignment of a word is inﬂuenced by its adjacent words and distance-2 words. Word similarity information learned from large corpus is incorporated to enhance word topic assignment. Parameters are estimated efﬁciently by variational inference and experimental results on two datasets show EGTRF achieves lower perplexity and higher log predictive probability. 
Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the stateof-the-art models on this task. 
Rumours on social media exhibit complex temporal patterns. This paper develops a model of rumour prevalence using a point process, namely a log-Gaussian Cox process, to infer an underlying continuous temporal probabilistic model of post frequencies. To generalize over different rumours, we present a multi-task learning method parametrized by the text in posts which allows data statistics to be shared between groups of similar rumours. Our experiments demonstrate that our model outperforms several strong baseline methods for rumour frequency prediction evaluated on tweets from the 2014 Ferguson riots. 
Recently, a variety of representation learning approaches have been developed in the literature to induce latent generalizable features across two domains. In this paper, we extend the standard hidden Markov models (HMMs) to learn distributed state representations to improve cross-domain prediction performance. We reformulate the HMMs by mapping each discrete hidden state to a distributed representation vector and employ an expectationmaximization algorithm to jointly learn distributed state representations and model parameters. We empirically investigate the proposed model on cross-domain part-ofspeech tagging and noun-phrase chunking tasks. The experimental results demonstrate the effectiveness of the distributed HMMs on facilitating domain adaptation. 
The usefulness of translation quality estimation (QE) to increase productivity in a computer-assisted translation (CAT) framework is a widely held assumption (Specia, 2011; Huang et al., 2014). So far, however, the validity of this assumption has not been yet demonstrated through sound evaluations in realistic settings. To this aim, we report on an evaluation involving professional translators operating with a CAT tool in controlled but natural conditions. Contrastive experiments are carried out by measuring post-editing time differences when: i) translation suggestions are presented together with binary quality estimates, and ii) the same suggestions are presented without quality indicators. Translators’ productivity in the two conditions is analysed in a principled way, accounting for the main factors (e.g. differences in translators’ behaviour, quality of the suggestions) that directly impact on time measurements. While the general assumption about the usefulness of QE is veriﬁed, signiﬁcance testing results reveal that real productivity gains can be observed only under speciﬁc conditions. 
We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difﬁcult categories, and gradually build the ability of representing phrases and sentencelevel contexts by using training examples from easy to difﬁcult. Experimental results show that our approach signiﬁcantly outperforms the baseline system by up to 1.4 BLEU points. 
Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a speciﬁc threshold are useful to improve translation quality. Compared with previous work, our method may more effectively and efﬁciently exploit helpful word reordering information. 
In this paper we present the UNRAVEL toolkit: It implements many of the recently published works on decipherment, including decipherment for deterministic ciphers like e.g. the ZODIAC-408 cipher and Part two of the BEALE ciphers, as well as decipherment of probabilistic ciphers and unsupervised training for machine translation. It also includes data and example conﬁguration ﬁles so that the previously published experiments are easy to reproduce. 
In Statistical Machine Translation, some complex features are still difﬁcult to integrate during decoding and usually used through the reranking of the k-best hypotheses produced by the decoder. We propose a translation table partitioning method that exploits the result of this reranking to iteratively guide the decoder in order to produce a new k-best list more relevant to some complex features. We report experiments on two translation domains and two translations directions which yield improvements of up to 1.4 BLEU over the reranking baseline using the same set of complex features. On a practical viewpoint, our approach allows SMT system developers to easily integrate complex features into decoding rather than being limited to their use in one-time k-best list reranking. 
Domain adaptation is an active ﬁeld of research in statistical machine translation (SMT), but so far most work has ignored the distinction between the topic and genre of documents. In this paper we quantify and disentangle the impact of genre and topic differences on translation quality by introducing a new data set that has controlled topic and genre distributions. In addition, we perform a detailed analysis showing that differences across topics only explain to a limited degree translation performance differences across genres, and that genre-speciﬁc errors are more attributable to model coverage than to suboptimal scoring of translation candidates. 
A joint-space model for cross-lingual distributed representations generalizes language-invariant semantic features. In this paper, we present a matrix cofactorization framework for learning cross-lingual word embeddings. We explicitly deﬁne monolingual training objectives in the form of matrix decomposition, and induce cross-lingual constraints for simultaneously factorizing monolingual matrices. The cross-lingual constraints can be derived from parallel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classiﬁcation show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 
 Pivot translation allows for translation of language pairs with little or no parallel data by introducing a third language for which data exists. In particular, the triangulation method, which translates by combining source-pivot and pivot-target translation models into a source-target model, is known for its high translation accuracy. However, in the conventional triangulation method, information of pivot phrases is forgotten and not used in the translation process. In this paper, we propose a novel approach to remember the pivot phrases in the triangulation stage, and use a pivot language model as an additional information source at translation time. Experimental results on the Europarl corpus showed gains of 0.4-1.2 BLEU points in all tested combinations of languages1. 
In this article, we discuss the challenges of document summarization for the blind and visually impaired people and then propose a new system called BrailleSUM to produce better summaries for the blind and visually impaired people. Our system considers the factor of braille length of each sentence in news articles into the ILPbased summarization method. Evaluation results on a DUC dataset show that BrailleSUM can produce shorter braille summaries than existing methods, meanwhile, it does not sacrifice the content quality of the summaries. 
This paper presents a novel task, namely the automatic identiﬁcation of ageappropriate ratings of a musical track, or album, based on its lyrics. Details are provided regarding the construction of a dataset of lyrics from 12,242 tracks across 1,798 albums along with age-appropriate ratings obtained from various web resources, along with results from various text classiﬁcation experiments. The best accuracy of 71.02% for classifying albums by age groups is achieved by combining vector space model and psycholinguistic features. 
How do we know which grammatical error correction (GEC) system is best? A number of metrics have been proposed over the years, each motivated by weaknesses of previous metrics; however, the metrics themselves have not been compared to an empirical gold standard grounded in human judgments. We conducted the ﬁrst human evaluation of GEC system outputs, and show that the rankings produced by metrics such as MaxMatch and I-measure do not correlate well with this ground truth. As a step towards better metrics, we also propose GLEU, a simple variant of BLEU, modiﬁed to account for both the source and the reference, and show that it hews much more closely to human judgments. 
Languages using Chinese characters are mostly processed at word level. Inspired by recent success of deep learning, we delve deeper to character and radical levels for Chinese language processing. We propose a new deep learning technique, called “radical embedding”, with justiﬁcations based on Chinese linguistics, and validate its feasibility and utility through a set of three experiments: two in-house standard experiments on short-text categorization (STC) and Chinese word segmentation (CWS), and one in-ﬁeld experiment on search ranking. We show that radical embedding achieves comparable, and sometimes even better, results than competing methods. 
We present and evaluate a method for automatically detecting sentence fragments in English texts written by non-native speakers. Our method combines syntactic parse tree patterns and parts-of-speech information produced by a tagger to detect this phenomenon. When evaluated on a corpus of authentic learner texts, our best model achieved a precision of 0.84 and a recall of 0.62, a statistically signiﬁcant improvement over baselines using non-parse features, as well as a popular grammar checker. 
Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the inﬂuence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classiﬁers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the ﬁrst quantitative evidence that text contains signals that can be exploited to detect drunk-texting. 
Expert ﬁnding on social media beneﬁts both individuals and commercial services. In this paper, we exploit a 5-level tree representation to model the posts on social media and cast the expert ﬁnding problem to the matching problem between the learned user tree and domain tree. We enhance the traditional approximate tree matching algorithm and incorporate word embeddings to improve the matching result. The experiments conducted on Sina Microblog demonstrate the effectiveness of our work. 
Online social networks nowadays have the worldwide prosperity, as they have revolutionized the way for people to discover, to share, and to diffuse information. Social networks are powerful, yet they still have Achilles Heel: extreme data sparsity. Individual posting documents, (e.g., a microblog less than 140 characters), seem to be too sparse to make a difference under various scenarios, while in fact they are quite different. We propose to tackle this speciﬁc weakness of social networks by smoothing the posting document language model based on social regularization. We formulate an optimization framework with a social regularizer. Experimental results on the Twitter dataset validate the effectiveness and efﬁciency of our proposed model. 
We propose a label propagation approach to geolocation prediction based on Modiﬁed Adsorption, with two enhancements: (1) the removal of “celebrity” nodes to increase location homophily and boost tractability; and (2) the incorporation of text-based geolocation priors for test users. Experiments over three Twitter benchmark datasets achieve state-of-theart results, and demonstrate the effectiveness of the enhancements. 
 Voice is more important than Fukushima tonight”. Ex-  This paper proposes an approach to capture  plicit opposition can also arise from an explicit positive/negative contrast between a subjective proposition  the pragmatic context needed to infer irony in  and a situation that describes an undesirable activity or  tweets. We aim to test the validity of two main hypotheses: (1) the presence of negations, as an internal propriety of an utterance, can help  state. For instance, in “ I love when my phone turns the volume down automatically” the writer assumes that every one expects its cell phone to ring loud enough  to detect the disparity between the literal and the intended meaning of an utterance, (2) a tweet containing an asserted fact of the form  to be heard. In (b), irony is due to an implicit opposition between a lexicalized proposition P describing an event or state and a pragmatic context external to the  N ot(P1) is ironic if and only if one can assess the absurdity of P1. Our ﬁrst results are encouraging and show that deriving a pragmatic contextual model is feasible.  utterance in which P is false or is not likely to happen. In other words, the writer asserts or afﬁrms P while he intends to convey P such that P = N ot(P ) or P = P . The irony occurs because the writer believes  
This paper presents an email importance corpus annotated through Amazon Mechanical Turk (AMT). Annotators annotate the email content type and email importance for three levels of hierarchy (senior manager, middle manager and employee). Each email is annotated by 5 turkers. Agreement study shows that the agreed AMT annotations are close to the expert annotations. The annotated dataset demonstrates difference in proportions of content type between different levels. An email importance prediction system is trained on the dataset and identiﬁes the unimportant emails at minimum 0.55 precision with only text-based features. 
Compared with carefully edited prose, the language of social media is informal in the extreme. The application of NLP techniques in this context may require a better understanding of word usage within social media. In this paper, we compute a word embedding for a corpus of tweets, comparing it to a word embedding for Wikipedia. After learning a transformation of one vector space to the other, and adjusting similarity values according to term frequency, we identify words whose usage differs greatly between the two corpora. For any given word, the set of words closest to it in a particular embedding provides a characterization for that word’s usage within the corresponding corpora. 
Human labeled corpus is indispensable for the training of supervised word segmenters. However, it is time-consuming and laborintensive to label corpus manually. During the process of typing Chinese text by Pingyin, people usually need to type "space" or numeric keys to choose the words due to homophones, which can be viewed as a cue for segmentation. We argue that such a process can be used to build a labeled corpus in a more natural way. Thus, in this paper, we investigate Natural Typing Annotations (NTAs) that are potential word delimiters produced by users while typing Chinese. A detailed analysis on over three hundred user-produced texts containing NTAs reveals that highquality NTAs mostly agree with gold segmentation and, consequently, can be used for improving the performance of supervised word segmentation model in out-of-domain. Experiments show that a classification model combined with a voting mechanism can reliably identify the high-quality NTAs texts that are more readily available labeled corpus. Furthermore, the NTAs might be particularly useful to deal with out-of-vocabulary (OOV) words such as proper names and neo-logisms.  manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia. There has been work on making use of both unlabeled data (Li and Sun, 2009; Sun and Xu, 2011; Wang et al., 2011; Qiu et al., 2014) and Wikipedia (Jiang et al., 2013; Liu et al., 2014;) to improve segmentation. But none of them notice the segmentation information produced by users while typing Chinese. Chinese is unique due to its logographic writing system. Chinese users cannot directly type in Chinese words using a QWERTY keyboard. Input methods have been proposed to assist users to type in Chinese words (Chen, 1997). Substantial information has been produced, but not recorded and stored during text typing process.  
We study the problem of predicting tense in Chinese conversations. The unique challenges include: (1) Chinese verbs do not have explicit lexical or grammatical forms to indicate tense; (2) Tense information is often implicitly hidden outside of the target sentence. To tackle these challenges, we ﬁrst propose a set of novel sentence-level (local) features using rich linguistic resources and then propose a new hypothesis of “One tense per scene” to incorporate scene-level (global) evidence to enhance the performance. Experimental results demonstrate the power of this hybrid approach, which can serve as a new and promising benchmark. 
This paper presents a universal morphological feature schema that represents the ﬁnest distinctions in meaning that are expressed by overt, afﬁxal inﬂectional morphology across languages. This schema is used to universalize data extracted from Wiktionary via a robust multidimensional table parsing algorithm and feature mapping algorithms, yielding 883,965 instantiated paradigms in 352 languages. These data are shown to be effective for training morphological analyzers, yielding signiﬁcant accuracy gains when applied to Durrett and DeNero’s (2013) paradigm learning framework. 
Given a discourse tree for a text as a candidate answer to a compound query, we propose a rule system for valid and invalid occurrence of the query keywords in this tree. To be a valid answer to a query, its keywords need to occur in a chain of elementary discourse unit of this answer so that these units are fully ordered and connected by nucleus – satellite relations. An answer might be invalid if the queries’ keywords occur in the answer's satellite discourse units only. We build the rhetoric map of an answer to prevent it from firing by queries whose keywords occur in non-adjacent areas of the Answer Map. We evaluate the improvement of search relevance by filtering out search results not satisfying the proposed rule system, demonstrating a 4% increase of accuracy with respect to the nearest neighbor learning approach which does not use the discourse tree structure. 
Retrieving similar questions in online Q&A community sites is a difﬁcult task because different users may formulate the same question in a variety of ways, using different vocabulary and structure. In this work, we propose a new neural network architecture to perform the task of semantically equivalent question retrieval. The proposed architecture, which we call BOW-CNN, combines a bag-ofwords (BOW) representation with a distributed vector representation created by a convolutional neural network (CNN). We perform experiments using data collected from two Stack Exchange communities. Our experimental results evidence that: (1) BOW-CNN is more effective than BOW based information retrieval methods such as TFIDF; (2) BOW-CNN is more robust than the pure CNN for long texts. 
We demonstrate signiﬁcant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a signiﬁcant performance gradient for the use of linguistic structure in machine comprehension. 
In this paper, we present an approach that address the answer sentence selection problem for question answering. The proposed method uses a stacked bidirectional Long-Short Term Memory (BLSTM) network to sequentially read words from question and answer sentences, and then outputs their relevance scores. Unlike prior work, this approach does not require any syntactic parsing or external knowledge resources such as WordNet which may not be available in some domains or languages. The full system is based on a combination of the stacked BLSTM relevance model and keywords matching. The results of our experiments on a public benchmark dataset from TREC show that our system outperforms previous work which requires syntactic features and external knowledge resources. 
We propose a simple yet effective approach to learning bilingual word embeddings (BWEs) from non-parallel document-aligned data (based on the omnipresent skip-gram model), and its application to bilingual lexicon induction (BLI). We demonstrate the utility of the induced BWEs in the BLI task by reporting on benchmarking BLI datasets for three language pairs: (1) We show that our BWE-based BLI models signiﬁcantly outperform the MuPTM-based and context-counting models in this setting, and obtain the best reported BLI results for all three tested language pairs; (2) We also show that our BWE-based BLI models outperform other BLI models based on recently proposed BWEs that require parallel data for bilingual training. 
In recent years, distributional models (DMs) have shown great success in representing lexical semantics. In this work we show that the extent to which DMs represent semantic knowledge is highly dependent on the type of knowledge. We pose the task of predicting properties of concrete nouns in a supervised setting, and compare between learning taxonomic properties (e.g., animacy) and attributive properties (e.g., size, color). We employ four state-of-the-art DMs as sources of feature representation for this task, and show that they all yield poor results when tested on attributive properties, achieving no more than an average F-score of 0.37 in the binary property prediction task, compared to 0.73 on taxonomic properties. Our results suggest that the distributional hypothesis may not be equally applicable to all types of semantic information. 
Several compositional distributional semantic methods use tensors to model multi-way interactions between vectors. Unfortunately, the size of the tensors can make their use impractical in large-scale implementations. In this paper, we investigate whether we can match the performance of full tensors with low-rank approximations that use a fraction of the original number of parameters. We investigate the effect of low-rank tensors on the transitive verb construction where the verb is a third-order tensor. The results show that, while the low-rank tensors require about two orders of magnitude fewer parameters per verb, they achieve performance comparable to, and occasionally surpassing, the unconstrained-rank tensors on sentence similarity and verb disambiguation tasks. 
In this paper, we present a model for improved discriminative semantic parsing. The model addresses an important limitation associated with our previous stateof-the-art discriminative semantic parsing model – the relaxed hybrid tree model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/. 
A question may be asked not only to elicit information, but also to make a statement. Questions serving the latter purpose, called rhetorical questions, are often lexically and syntactically indistinguishable from other types of questions. Still, it is desirable to be able to identify rhetorical questions, as it is relevant for many NLP tasks, including information extraction and text summarization. In this paper, we explore the largely understudied problem of rhetorical question identiﬁcation. Speciﬁcally, we present a simple n-gram based language model to classify rhetorical questions in the Switchboard Dialogue Act Corpus. We ﬁnd that a special treatment of rhetorical questions which incorporates contextual information achieves the highest performance. 
This paper proposes a novel lifelong learning (LL) approach to sentiment classiﬁcation. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we ﬁrst discuss LL in general and then LL for sentiment classiﬁcation in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods signiﬁcantly, which demonstrates that lifelong learning is a promising research direction. 
The relationship between context incongruity and sarcasm has been studied in linguistics. We present a computational system that harnesses context incongruity as a basis for sarcasm detection. Our statistical sarcasm classiﬁers incorporate two kinds of incongruity features: explicit and implicit. We show the beneﬁt of our incongruity features for two text forms - tweets and discussion forum posts. Our system also outperforms two past works (with Fscore improvement of 10-20%). We also show how our features can capture intersentential incongruity. 
Code-switching is commonly used in the free-form text environment, such as social media, and it is especially favored in emotion expressions. Emotions in codeswitching texts differ from monolingual texts in that they can be expressed in either monolingual or bilingual forms. In this paper, we ﬁrst utilize two kinds of knowledge, i.e. bilingual and sentimental information to bridge the gap between different languages. Moreover, we use a term-document bipartite graph to incorporate both bilingual and sentimental information, and propose a label propagation based approach to learn and predict in the bipartite graph. Empirical studies demonstrate the effectiveness of our proposed approach in detecting emotion in code-switching texts. 
Humans are idiosyncratic and variable: towards the same topic, they might hold different opinions or express the same opinion in various ways. It is hence important to model opinions at the level of individual users; however it is impractical to estimate independent sentiment classiﬁcation models for each user with limited data. In this paper, we adopt a modelbased transfer learning solution – using linear transformations over the parameters of a generic model – for personalized opinion analysis. Extensive experimental results on a large collection of Amazon reviews conﬁrm our method signiﬁcantly outperformed a user-independent generic opinion model as well as several state-ofthe-art transfer learning algorithms. 
In this paper, we propose a ﬂexible principle-based approach (PBA) for reader-emotion classiﬁcation and writing assistance. PBA is a highly automated process that learns emotion templates from raw texts to characterize an emotion and is comprehensible for humans. These templates are adopted to predict reader-emotion, and may further assist in emotional resonance writing. Results demonstrate that PBA can effectively detect reader-emotions by exploiting the syntactic structures and semantic associations in the context, thus outperforming wellknown statistical text classiﬁcation methods and the state-of-the-art reader-emotion classiﬁcation method. Moreover, writers are able to create more emotional resonance in articles under the assistance of the generated emotion templates. These templates have been proven to be highly interpretable, which is an attribute that is difﬁcult to accomplish in traditional statistical methods. 
Most cross-lingual sentiment classiﬁcation (CLSC) research so far has been performed at sentence or document level. Aspect-level CLSC, which is more appropriate for many applications, presents the additional difﬁculty that we consider subsentential opinionated units which have to be mapped across languages. In this paper, we extend the possible cross-lingual sentiment analysis settings to aspect-level speciﬁc use cases. We propose a method, based on constrained SMT, to transfer opinionated units across languages by preserving their boundaries. We show that cross-language sentiment classiﬁers built with this method achieve comparable results to monolingual ones, and we compare different cross-lingual settings. 
Compared to the categorical approach that represents affective states as several discrete classes (e.g., positive and negative), the dimensional approach represents affective states as continuous numerical values on multiple dimensions, such as the valence-arousal (VA) space, thus allowing for more fine-grained sentiment analysis. In building dimensional sentiment applications, affective lexicons with valence-arousal ratings are useful resources but are still very rare. Therefore, this study proposes a weighted graph model that considers both the relations of multiple nodes and their similarities as weights to automatically determine the VA ratings of affective words. Experiments on both English and Chinese affective lexicons show that the proposed method yielded a smaller error rate on VA prediction than the linear regression, kernel method, and pagerank algorithm used in previous studies. 
Dialog state tracking is a key component of many modern dialog systems, most of which are designed with a single, welldeﬁned domain in mind. This paper shows that dialog data drawn from different dialog domains can be used to train a general belief tracking model which can operate across all of these domains, exhibiting superior performance to each of the domainspeciﬁc models. We propose a training procedure which uses out-of-domain data to initialise belief tracking models for entirely new domains. This procedure leads to improvements in belief tracking performance regardless of the amount of in-domain data available for training the model. 
Dialogue Management (DM) is a key issue in Spoken Dialogue System (SDS). Most of the existing studies on DM use Dialogue Act (DA) to represent semantic information of sentence, which might not represent the nuanced meaning sometimes. In this paper, we model DM based on sentence clusters which have more powerful semantic representation ability than DAs. Firstly, sentences are clustered not only based on the internal information such as words and sentence structures, but also based on the external information such as context in dialogue via Recurrent Neural Networks. Additionally, the DM problem is modeled as a Partially Observable Markov Decision Processes (POMDP) with sentence clusters. Finally, experimental results illustrate that the proposed DM scheme is superior to the existing one. 
In this paper, we introduce the task of selecting compact lexicon from large, noisy gazetteers. This scenario arises often in practice, in particular spoken language understanding (SLU). We propose a simple and effective solution based on matrix decomposition techniques: canonical correlation analysis (CCA) and rank-revealing QR (RRQR) factorization. CCA is ﬁrst used to derive low-dimensional gazetteer embeddings from domain-speciﬁc search logs. Then RRQR is used to ﬁnd a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of > 50% over randomly selected lexicon. 
We investigate the impact of listener’s gaze on predicting reference resolution in situated interactions. We extend an existing model that predicts to which entity in the environment listeners will resolve a referring expression (RE). Our model makes use of features that capture which objects were looked at and for how long, reﬂecting listeners’ visual behavior. We improve a probabilistic model that considers a basic set of features for monitoring listeners’ movements in a virtual environment. Particularly, in complex referential scenes, where more objects next to the target are possible referents, gaze turns out to be beneﬁcial and helps deciphering listeners’ intention. We evaluate performance at several prediction times before the listener performs an action, obtaining a highly signiﬁcant accuracy gain. 
The intelligent personal assistant software such as the Apple’s Siri and Samsung’s S-Voice has been issued these days. This paper introduces a novel Spoken Language Understanding (SLU) module to predict user’s intention for determining system actions of the intelligent personal assistant software. The SLU module usually consists of several connected recognition tasks on a pipeline framework, whereas the proposed SLU module simultaneously recognizes four recognition tasks on a recognition framework using Conditional Random Fields (CRF). The four tasks include named entity, speech-act, target and operation recognition. In the experiments, the new simultaneous recognition method achieves the higher performance of 4% and faster speed of about 25% than other method using a pipeline framework. By a significance test, this improvement is considered to be statistically significant as a p-value of smaller than 0.05. 
In the last few years, there has been a growing number of studies addressing the Text Simpliﬁcation (TS) task as a monolingual machine translation (MT) problem which translates from ‘original’ to ‘simple’ language. Motivated by those results, we investigate the inﬂuence of quality vs quantity of the training data on the effectiveness of such a MT approach to text simpliﬁcation. We conduct 40 experiments on the aligned sentences from English Wikipedia and Simple English Wikipedia, controlling for: (1) the similarity between the original and simpliﬁed sentences in the training and development datasets, and (2) the sizes of those datasets. The results suggest that in the standard PB-SMT approach to text simpliﬁcation the quality of the datasets has a greater impact on the system performance. Additionally, we point out several important differences between cross-lingual MT and monolingual MT used in text simpliﬁcation, and show that BLEU is not a good measure of system performance in text simpliﬁcation task. 
In this paper, we propose the concept of summary prior to deﬁne how much a sentence is appropriate to be selected into summary without consideration of its context. Different from previous work using manually compiled documentindependent features, we develop a novel summary system called PriorSum, which applies the enhanced convolutional neural networks to capture the summary prior features derived from length-variable phrases. Under a regression framework, the learned prior features are concatenated with document-dependent features for sentence ranking. Experiments on the DUC generic summarization benchmarks show that PriorSum can discover different aspects supporting the summary prior and outperform state-of-the-art baselines. 
Timeline generation is a summarisation task which transforms a narrative, roughly chronological input text into a set of timestamped summary sentences, each expressing an atomic historical event. We present a methodology for evaluating systems which create such timelines, based on a novel corpus consisting of 36 humancreated timelines. Our evaluation relies on deep semantic units which we call historical content units. An advantage of our approach is that it does not require human annotation of new system summaries. 
 Coverage maximization with bigram concepts is a state-of-the-art approach to unsupervised extractive summarization. It has been argued that such concepts are adequate and, in contrast to more linguistic concepts such as named entities or syntactic dependencies, more robust, since they do not rely on automatic processing. In this paper, we show that while this seems to be the case for a commonly used newswire dataset, use of syntactic and semantic concepts leads to signiﬁcant improvements in performance in other domains.  
Training a high-accuracy dependency parser requires a large treebank. However, these are costly and time-consuming to build. We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages. We exploit cues from a different source language in order to guide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method. 
We propose a method for semantic structure analysis of noun phrases using Abstract Meaning Representation (AMR). AMR is a graph representation for the meaning of a sentence, in which noun phrases (NPs) are manually annotated with internal structure and semantic relations. We extract NPs from the AMR corpus and construct a data set of NP semantic structures. We also propose a transition-based algorithm which jointly identiﬁes both the nodes in a semantic structure tree and semantic relations between them. Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points. 
We report improved AMR parsing results by adding a new action to a transitionbased AMR parser to infer abstract concepts and by incorporating richer features produced by auxiliary analyzers such as a semantic role labeler and a coreference resolver. We report ﬁnal AMR parsing results that show an improvement of 7% absolute in F1 score over the best previously reported result. Our parser is available at: https://github.com/ Juicechuan/AMRParsing 
We propose a neural network model for scalable generative transition-based dependency parsing. A probability distribution over both sentences and transition sequences is parameterised by a feedforward neural network. The model surpasses the accuracy and speed of previous generative dependency parsers, reaching 91.1% UAS. Perplexity results show a strong improvement over n-gram language models, opening the way to the efﬁcient integration of syntax into neural models for language generation. 
Nearly all work in unsupervised grammar induction aims to induce unlabeled dependency trees from gold part-of-speechtagged text. These clean linguistic classes provide a very important, though unrealistic, inductive bias. Conversely, induced clusters are very noisy. We show here, for the ﬁrst time, that very limited human supervision (three frequent words per cluster) may be required to induce labeled dependencies from automatically induced word clusters. 
Ezafe construction is an idiosyncratic phenomenon in the Persian language. It is a good indicator for phrase boundaries and dependency relations but mostly does not appear in the text. In this paper, we show that adding information about Ezafe construction can give 4.6% relative improvement in dependency parsing and 9% relative improvement in shallow parsing. For evaluation purposes, Ezafe tags are manually annotated in the Persian dependency treebank. Furthermore, to be able to conduct experiments on shallow parsing, we develop a dependency to shallow phrase structure convertor based on the Persian dependencies. 
Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efﬁciently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT’14. 
Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A signiﬁcant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT’14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the ﬁrst to surpass the best result achieved on a WMT’14 contest task. 
The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding, our speciﬁcally designed convolution+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a uniﬁed representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve signiﬁcant improvements over the previous NNJM by up to +1.08 BLEU points on average. 
We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various nonlocal translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in signiﬁcant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and ChineseEnglish translation over a state-of-the-art system that already includes neural network features. 
The Visual Dependency Representation (VDR) is an explicit model of the spatial relationships between objects in an image. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to ﬁnd the objects mentioned in a given description using a state-of-the-art object detector, and to use successful detections to produce training data. The description of an unseen image is produced by ﬁrst predicting its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-ofthe-art multimodal deep neural network in images depicting actions. 
The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education, and robotics. However, prior work on the text to 3D scene generation task has used manually speciﬁed object categories and language that identiﬁes them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground textual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the ﬁdelity and plausibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments. 
We present MultiGranCNN, a general deep learning architecture for matching text chunks. MultiGranCNN supports multigranular comparability of representations: shorter sequences in one chunk can be directly compared to longer sequences in the other chunk. MultiGranCNN also contains a ﬂexible and modularized match feature component that is easily adaptable to different types of chunk matching. We demonstrate stateof-the-art performance of MultiGranCNN on clause coherence and paraphrase identiﬁcation tasks. 
Massive open online courses (MOOCs) are redeﬁning the education system and transcending boundaries posed by traditional courses. With the increase in popularity of online courses, there is a corresponding increase in the need to understand and interpret the communications of the course participants. Identifying topics or aspects of conversation and inferring sentiment in online course forum posts can enable instructor interventions to meet the needs of the students, rapidly address course-related issues, and increase student retention. Labeled aspect-sentiment data for MOOCs are expensive to obtain and may not be transferable between courses, suggesting the need for approaches that do not require labeled data. We develop a weakly supervised joint model for aspectsentiment in online courses, modeling the dependencies between various aspects and sentiment using a recently developed scalable class of statistical relational models called hinge-loss Markov random ﬁelds. We validate our models on posts sampled from twelve online courses, each containing an average of 10,000 posts, and demonstrate that jointly modeling aspect with sentiment improves the prediction accuracy for both aspect and sentiment. 
This paper considers the problem of embedding Knowledge Graphs (KGs) consisting of entities and relations into lowdimensional vector spaces. Most of the existing methods perform this task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. In this paper, aiming at further discovering the intrinsic geometric structure of the embedding space, we propose Semantically Smooth Embedding (SSE). The key idea of SSE is to take full advantage of additional semantic information and enforce the embedding space to be semantically smooth, i.e., entities belonging to the same semantic category will lie close to each other in the embedding space. Two manifold learning algorithms Laplacian Eigenmaps and Locally Linear Embedding are used to model the smoothness assumption. Both are formulated as geometrically based regularization terms to constrain the embedding task. We empirically evaluate SSE in two benchmark tasks of link prediction and triple classiﬁcation, and achieve signiﬁcant and consistent improvements over state-of-the-art methods. Furthermore, SSE is a general framework. The smoothness assumption can be imposed to a wide variety of embedding models, and it can also be constructed using other information besides entities’ semantic categories. 
Word embeddings have recently gained considerable popularity for modeling words in different Natural Language Processing (NLP) tasks including semantic similarity measurement. However, notwithstanding their success, word embeddings are by their very nature unable to capture polysemy, as different meanings of a word are conﬂated into a single representation. In addition, their learning process usually relies on massive corpora only, preventing them from taking advantage of structured knowledge. We address both issues by proposing a multifaceted approach that transforms word embeddings to the sense level and leverages knowledge from a large semantic network for effective semantic similarity measurement. We evaluate our approach on word similarity and relational similarity frameworks, reporting state-of-the-art performance on multiple datasets. 
Contrasting meaning is a basic aspect of semantics. Recent word-embedding models based on distributional semantics hypothesis are known to be weak for modeling lexical contrast. We present in this paper the embedding models that achieve an F-score of 92% on the widely-used, publicly available dataset, the GRE “most contrasting word” questions (Mohammad et al., 2008). This is the highest performance seen so far on this dataset. Surprisingly at the ﬁrst glance, unlike what was suggested in most previous work, where relatedness statistics learned from corpora is claimed to yield extra gains over lexicon-based models, we obtained our best result relying solely on lexical resources (Roget’s and WordNet)—corpora statistics did not lead to further improvement. However, this should not be simply taken as that distributional statistics is not useful. We examine several basic concerns in modeling contrasting meaning to provide detailed analysis, with the aim to shed some light on the future directions for this basic semantics modeling problem. 
Online debate forums present a valuable opportunity for the understanding and modeling of dialogue. To understand these debates, a key challenge is inferring the stances of the participants, all of which are interrelated and dependent. While collectively modeling users’ stances has been shown to be effective (Walker et al., 2012c; Hasan and Ng, 2013), there are many modeling decisions whose ramiﬁcations are not well understood. To investigate these choices and their effects, we introduce a scalable uniﬁed probabilistic modeling framework for stance classiﬁcation models that 1) are collective, 2) reason about disagreement, and 3) can model stance at either the author level or at the post level. We comprehensively evaluate the possible modeling choices on eight topics across two online debate corpora, ﬁnding accuracy improvements of up to 11.5 percentage points over a local classiﬁer. Our results highlight the importance of making the correct modeling choices for online dialogues, and having a uniﬁed probabilistic modeling framework that makes this possible. 
Entity classiﬁcation, like many other important problems in NLP, involves learning classiﬁers over sparse highdimensional feature spaces that result from the conjunction of elementary features of the entity mention and its context. In this paper we develop a low-rank regularization framework for training maxentropy models in such sparse conjunctive feature spaces. Our approach handles conjunctive feature spaces using matrices and induces an implicit low-dimensional representation via low-rank constraints. We show that when learning entity classiﬁers under minimal supervision, using a seed set, our approach is more effective in controlling model capacity than standard techniques for linear classiﬁers. 
Vector space representation of words has been widely used to capture ﬁne-grained linguistic regularities, and proven to be successful in various natural language processing tasks in recent years. However, existing models for learning word representations focus on either syntagmatic or paradigmatic relations alone. In this paper, we argue that it is beneﬁcial to jointly modeling both relations so that we can not only encode different types of linguistic properties in a uniﬁed way, but also boost the representation learning due to the mutual enhancement between these two types of relations. We propose two novel distributional models for word representation using both syntagmatic and paradigmatic relations via a joint training objective. The proposed models are trained on a public Wikipedia corpus, and the learned representations are evaluated on word analogy and word similarity tasks. The results demonstrate that the proposed models can perform signiﬁcantly better than all the state-of-the-art baseline methods on both tasks. 
We present paired learning and inference algorithms for signiﬁcantly reducing computation and increasing speed of the vector dot products in the classiﬁers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high conﬁdence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early conﬁdence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and transition-based dependency parsing. On the typical benchmarking datasets we can preserve POS tagging accuracy above 97% and parsing LAS above 88.5% both with over a ﬁve-fold reduction in run-time, and NER F1 above 88 with more than 2x increase in speed. 
Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z) → containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classiﬁer by 11%, and a method leveraging pre-trained embeddings by 7%. 
Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods. 
We present results on using stacking to ensemble multiple systems for the Knowledge Base Population English Slot Filling (KBP-ESF) task. In addition to using the output and conﬁdence of each system as input to the stacked classiﬁer, we also use features capturing how well the systems agree about the provenance of the information they extract. We demonstrate that our stacking approach outperforms the best system from the 2014 KBPESF competition as well as alternative ensembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the performance of stacking. 
This paper presents a generative model to event schema induction. Previous methods in the literature only use head words to represent entities. However, elements other than head words contain useful information. For instance, an armed man is more discriminative than man. Our model takes into account this information and precisely represents it using probabilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically conﬁrm this enhancement. 
 Simultaneous translation is a method to reduce the latency of communication through machine translation (MT) by dividing the input into short segments before performing translation. However, short segments pose problems for syntaxbased translation methods, as it is difﬁcult to generate accurate parse trees for sub-sentential segments. In this paper, we perform the ﬁrst experiments applying syntax-based SMT to simultaneous translation, and propose two methods to prevent degradations in accuracy: a method to predict unseen syntactic constituents that help generate complete parse trees, and a method that waits for more input when the current utterance is not enough to generate a ﬂuent translation. Experiments on English-Japanese translation show that the proposed methods allow for improvements in accuracy, particularly with regards to word order of the target sentences. 
We present an efﬁcient incremental topdown parsing method for preordering based on Bracketing Transduction Grammar (BTG). The BTG-based preordering framework (Neubig et al., 2012) can be applied to any language using only parallel text, but has the problem of computational efﬁciency. Our top-down parsing algorithm allows us to use the early update technique easily for the latent variable structured Perceptron algorithm with beam search, and solves the problem. Experimental results showed that the topdown method is more than 10 times faster than a method using the CYK algorithm. A phrase-based machine translation system with the top-down method had statistically signiﬁcantly higher BLEU scores for 7 language pairs without relying on supervised syntactic parsers, compared to baseline systems using existing preordering methods. 
We present a method for predicting machine translation output quality geared to the needs of computer-assisted translation. These include the capability to: i) continuously learn and self-adapt to a stream of data coming from multiple translation jobs, ii) react to data diversity by exploiting human feedback, and iii) leverage data similarity by learning and transferring knowledge across domains. To achieve these goals, we combine two supervised machine learning paradigms, online and multitask learning, adapting and unifying them in a single framework. We show the effectiveness of our approach in a regression task (HTER prediction), in which online multitask learning outperforms the competitive online single-task and pooling methods used for comparison. This indicates the feasibility of integrating in a CAT tool a single QE component capable to simultaneously serve (and continuously learn from) multiple translation jobs involving different domains and users. 
Lexical selection is crucial for statistical machine translation. Previous studies separately exploit sentence-level contexts and documentlevel topics for lexical selection, neglecting their correlations. In this paper, we propose a context-aware topic model for lexical selection, which not only models local contexts and global topics but also captures their correlations. The model uses target-side translations as hidden variables to connect document topics and source-side local contextual words. In order to learn hidden variables and distributions from data, we introduce a Gibbs sampling algorithm for statistical estimation and inference. A new translation probability based on distributions learned by the model is integrated into a translation system for lexical selection. Experiment results on NIST ChineseEnglish test sets demonstrate that 1) our model signiﬁcantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality. 
Understanding open-domain text is one of the primary challenges in NLP. Machine comprehension evaluates the system’s ability to understand text through a series of question-answering tasks on short pieces of text such that the correct answer can be found only in the given text. For this task, we posit that there is a hidden (latent) structure that explains the relation between the question, correct answer, and text. We call this the answer-entailing structure; given the structure, the correctness of the answer is evident. Since the structure is latent, it must be inferred. We present a uniﬁed max-margin framework that learns to ﬁnd these hidden structures (given a corpus of question-answer pairs), and uses what it learns to answer machine comprehension questions on novel texts. We extend this framework to incorporate multi-task learning on the different subtasks that are required to perform machine comprehension. Evaluation on a publicly available dataset shows that our framework outperforms various IR and neuralnetwork baselines, achieving an overall accuracy of 67.8% (vs. 59.9%, the best previously-published result.) 
Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in cQA archives aims to ﬁnd the existing questions that are semantically equivalent or relevant to the queried questions. However, the lexical gap problem brings about new challenge for question retrieval in cQA. In this paper, we propose to learn continuous word embeddings with metadata of category information within cQA pages for question retrieval. To deal with the variable size of word embedding vectors, we employ the framework of ﬁsher kernel to aggregated them into the ﬁxedlength vectors. Experimental results on large-scale real world cQA data set show that our approach can signiﬁcantly outperform state-of-the-art translation models and topic-based models for question retrieval in cQA. 
Answering natural language questions over a knowledge base is an important and challenging task. Most of existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning manner. We use FREEBASE as the knowledge base and conduct extensive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves better or comparable performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn. 
Zero-shot methods in language, vision and other domains rely on a cross-space mapping function that projects vectors from the relevant feature space (e.g., visualfeature-based image representations) to a large semantic word space (induced in an unsupervised way from corpus data), where the entities of interest (e.g., objects images depict) are labeled with the words associated to the nearest neighbours of the mapped vectors. Zero-shot cross-space mapping methods hold great promise as a way to scale up annotation tasks well beyond the labels in the training data (e.g., recognizing objects that were never seen in training). However, the current performance of cross-space mapping functions is still quite low, so that the strategy is not yet usable in practical applications. In this paper, we explore some general properties, both theoretical and empirical, of the cross-space mapping function, and we build on them to propose better methods to estimate it. In this way, we attain large improvements over the state of the art, both in cross-linguistic (word translation) and cross-modal (image labeling) zero-shot experiments. 
Over the last two decades, numerous algorithms have been developed that successfully capture something of the semantics of single words by looking at their distribution in text and comparing these distributions in a vector space model. However, it is not straightforward to construct meaning representations beyond the level of individual words – i.e. the combination of words into larger units – using distributional methods. Our contribution is twofold. First of all, we carry out a largescale evaluation, comparing different composition methods within the distributional framework for the cases of both adjectivenoun and noun-noun composition, making use of a newly developed dataset. Secondly, we propose a novel method for composition, which generalises the approach by Baroni and Zamparelli (2010). The performance of our novel method is also evaluated on our new dataset and proves competitive with the best methods. 
An elementary way of using language is to refer to objects. Often, these objects are physically present in the shared environment and reference is done via mention of perceivable properties of the objects. This is a type of language use that is modelled well neither by logical semantics nor by distributional semantics, the former focusing on inferential relations between expressed propositions, the latter on similarity relations between words or phrases. We present an account of word and phrase meaning that is perceptually grounded, trainable, compositional, and ‘dialogueplausible’ in that it computes meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automatically transcribed utterances. 
This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufﬁcient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system1 achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages. 
Most existing graph-based parsing models rely on millions of hand-crafted features, which limits their generalization ability and slows down the parsing speed. In this paper, we propose a general and effective Neural Network model for graph-based dependency parsing. Our model can automatically learn high-order feature combinations using only atomic features by exploiting a novel activation function tanhcube. Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 
We present structured perceptron training for neural network transition-based dependency parsing. We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences. Given this ﬁxed network representation, we learn a ﬁnal layer using the structured perceptron with beam-search decoding. On the Penn Treebank, our parser reaches 94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge is the best accuracy on Stanford Dependencies to date. We also provide indepth ablative analysis to determine which aspects of our model provide the largest gains in accuracy. 
We propose a technique for learning representations of parser states in transitionbased dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks— the stack LSTM. Like the conventional stack data structures used in transitionbased parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efﬁcient parsing model that captures three facets of a parser’s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance. 
Relation triples produced by open domain information extraction (open IE) systems are useful for question answering, inference, and other IE tasks. Traditionally these are extracted using a large set of patterns; however, this approach is brittle on out-of-domain text and long-range dependencies, and gives no insight into the substructure of the arguments. We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classiﬁer which learns to extract self-contained clauses from longer sentences. We then run natural logic inference over these short clauses to determine the maximally speciﬁc arguments for each candidate triple. We show that our approach outperforms a state-of-the-art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task. 
A standard pipeline for statistical relational learning involves two steps: one ﬁrst constructs the knowledge base (KB) from text, and then performs the learning and reasoning tasks using probabilistic ﬁrst-order logics. However, a key issue is that information extraction (IE) errors from text affect the quality of the KB, and propagate to the reasoning task. In this paper, we propose a statistical relational learning model for joint information extraction and reasoning. More speciﬁcally, we incorporate context-based entity extraction with structure learning (SL) in a scalable probabilistic logic framework. We then propose a latent context invention (LCI) approach to improve the performance. In experiments, we show that our approach outperforms state-of-the-art baselines over three real-world Wikipedia datasets from multiple domains; that joint learning and inference for IE and SL signiﬁcantly improve both tasks; that latent context invention further improves the results. 
Prepositional phrases (PPs) express crucial information that knowledge base construction methods need to extract. However, PPs are a major source of syntactic ambiguity and still pose problems in parsing. We present a method for resolving ambiguities arising from PPs, making extensive use of semantic knowledge from various resources. As training data, we use both labeled and unlabeled data, utilizing an expectation maximization algorithm for parameter estimation. Experiments show that our method yields improvements over existing methods including a state of the art dependency parser. 
Comparisons in text, such as in online reviews, serve as useful decision aids. In this paper, we focus on the task of identifying whether a comparison exists between a speciﬁc pair of entity mentions in a sentence. This formulation is transformative, as previous work only seeks to determine whether a sentence is comparative, which is presumptuous in the event the sentence mentions multiple entities and is comparing only some, not all, of them. Our approach leverages not only lexical features such as salient words, but also structural features expressing the relationships among words and entity mentions. To model these features seamlessly, we rely on a dependency tree representation, and investigate the applicability of a series of tree kernels. This leads to the development of a new context-sensitive tree kernel: Skip-node Kernel (SNK). We further describe both its exact and approximate computations. Through experiments on real-life datasets, we evaluate the effectiveness of our kernel-based approach for comparison identiﬁcation, as well as the utility of SNK and its approximations. 
The last few years have seen a surge in the number of accurate, fast, publicly available dependency parsers. At the same time, the use of dependency parsing in NLP applications has increased. It can be difﬁcult for a non-expert to select a good “off-the-shelf” parser. We present a comparative analysis of ten leading statistical dependency parsers on a multi-genre corpus of English. For our analysis, we developed a new web-based tool that gives a convenient way of comparing dependency parser outputs. Our analysis will help practitioners choose a parser to optimize their desired speed/accuracy tradeoff, and our tool will help practitioners examine and compare parser output. 
Semantic role labeling (SRL) is crucial to natural language understanding as it identiﬁes the predicate-argument structure in text with semantic labels. Unfortunately, resources required to construct SRL models are expensive to obtain and simply do not exist for most languages. In this paper, we present a two-stage method to enable the construction of SRL models for resourcepoor languages by exploiting monolingual SRL and multilingual parallel data. Experimental results show that our method outperforms existing methods. We use our method to generate Proposition Banks with high to reasonable quality for 7 languages in three language families and release these resources to the research community. 
We propose a cross-lingual framework for ﬁne-grained opinion mining using bitext projection. The only requirements are a running system in a source language and word-aligned parallel data. Our method projects opinion frames from the source to the target language, and then trains a system on the target language using the automatic annotations. Key to our approach is a novel dependency-based model for opinion mining, which we show, as a byproduct, to be on par with the current state of the art for English, while avoiding the need for integer programming or reranking. In cross-lingual mode (English to Portuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 
Cross-lingual sentiment analysis is a task of identifying sentiment polarities of texts in a low-resource language by using sentiment knowledge in a resource-abundant language. While most existing approaches are driven by transfer learning, their performance does not reach to a promising level due to the transferred errors. In this paper, we propose to integrate into knowledge transfer a knowledge validation model, which aims to prevent the negative inﬂuence from the wrong knowledge by distinguishing highly credible knowledge. Experiment results demonstrate the necessity and effectiveness of the model. 
The sentiment classiﬁcation performance relies on high-quality sentiment resources. However, these resources are imbalanced in different languages. Cross-language sentiment classiﬁcation (CLSC) can leverage the rich resources in one language (source language) for sentiment classiﬁcation in a resource-scarce language (target language). Bilingual embeddings could eliminate the semantic gap between two languages for CLSC, but ignore the sentiment information of text. This paper proposes an approach to learning bilingual sentiment word embeddings (BSWE) for English-Chinese CLSC. The proposed BSWE incorporate sentiment information of text into bilingual embeddings. Furthermore, we can learn high-quality BSWE by simply employing labeled corpora and their translations, without relying on largescale parallel corpora. Experiments on NLP&CC 2013 CLSC dataset show that our approach outperforms the state-of-theart systems. 
We present a new factoid-annotated dataset for evaluating content models for scientiﬁc survey article generation containing 3,425 sentences from 7 topics in natural language processing. We also introduce a novel HITS-based content model for automated survey article generation called HITSUM that exploits the lexical network structure between sentences from citing and cited papers. Using the factoid-annotated data, we conduct a pyramid evaluation and compare HITSUM with two previous state-of-the-art content models: C-Lexrank, a network based content model, and TOPICSUM, a Bayesian content model. Our experiments show that our new content model captures useful survey-worthy information and outperforms C-Lexrank by 4% and TOPICSUM by 7% in pyramid evaluation. 
We present a novel syntax-based natural language generation system that is trainable from unaligned pairs of input meaning representations and output sentences. It is divided into sentence planning, which incrementally builds deep-syntactic dependency trees, and surface realization. Sentence planner is based on A* search with a perceptron ranker that uses novel differing subtree updates and a simple future promise estimation; surface realization uses a rule-based pipeline from the Treex NLP toolkit. Our ﬁrst results show that training from unaligned data is feasible, the outputs of our generator are mostly ﬂuent and relevant. 
 Texts Candidate Extraction  We propose an event-driven model for headline generation. Given an input document, the system identiﬁes a key event chain by extracting a set of structural events that describe them. Then a novel multi-sentence compression algorithm is used to fuse the extracted events, generating a headline for the document. Our model can be viewed as a novel combination of extractive and abstractive headline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems. 
In natural language understanding (NLU), a user utterance can be labeled differently depending on the domain or application (e.g., weather vs. calendar). Standard domain adaptation techniques are not directly applicable to take advantage of the existing annotations because they assume that the label set is invariant. We propose a solution based on label embeddings induced from canonical correlation analysis (CCA) that reduces the problem to a standard domain adaptation task and allows use of a number of transfer learning techniques. We also introduce a new transfer learning technique based on pretraining of hidden-unit CRFs (HUCRFs). We perform extensive experiments on slot tagging on eight personal digital assistant domains and demonstrate that the proposed methods are superior to strong baselines. 
Spoken dialogue systems (SDS) typically require a predeﬁned semantic ontology to train a spoken language understanding (SLU) module. In addition to the annotation cost, a key challenge for designing such an ontology is to deﬁne a coherent slot set while considering their complex relations. This paper introduces a novel matrix factorization (MF) approach to learn latent feature vectors for utterances and semantic elements without the need of corpus annotations. Speciﬁcally, our model learns the semantic slots for a domain-speciﬁc SDS in an unsupervised fashion, and carries out semantic parsing using latent MF techniques. To further consider the global semantic structure, such as inter-word and inter-slot relations, we augment the latent MF-based model with a knowledge graph propagation model based on a slot-based semantic graph and a word-based lexical graph. Our experiments show that the proposed MF approaches produce better SLU models that are able to predict semantic slots and word patterns taking into account their relations and domain-speciﬁcity in a joint manner. 
Automatic speech recognition (ASR) outputs often contain various disﬂuencies. It is necessary to remove these disﬂuencies before processing downstream tasks. In this paper, an efﬁcient disﬂuency detection approach based on right-to-left transitionbased parsing is proposed, which can efﬁciently identify disﬂuencies and keep ASR outputs grammatical. Our method exploits a global view to capture long-range dependencies for disﬂuency detection by integrating a rich set of syntactic and disﬂuency features with linear complexity. The experimental results show that our method outperforms state-of-the-art work and achieves a 85.1% f-score on the commonly used English Switchboard test set. We also apply our method to in-house annotated Chinese data and achieve a signiﬁcantly higher f-score compared to the baseline of CRF-based approach. 
Non-linear models recently receive a lot of attention as people are starting to discover the power of statistical and embedding features. However, tree-based models are seldom studied in the context of structured learning despite their recent success on various classiﬁcation and ranking tasks. In this paper, we propose S-MART, a tree-based structured learning framework based on multiple additive regression trees. S-MART is especially suitable for handling tasks with dense features, and can be used to learn many different structures under various loss functions. We apply S-MART to the task of tweet entity linking — a core component of tweet information extraction, which aims to identify and link name mentions to entities in a knowledge base. A novel inference algorithm is proposed to handle the special structure of the task. The experimental results show that S-MART significantly outperforms state-of-the-art tweet entity linking systems. 
We propose that entity queries are generated via a two-step process: users ﬁrst select entity facts that can distinguish target entities from the others; and then choose words to describe each selected fact. Based on this query generation paradigm, we propose a new entity representation model named as entity factoid hierarchy. An entity factoid hierarchy is a tree structure composed of factoid nodes. A factoid node describes one or more facts about the entity in different information granularities. The entity factoid hierarchy is constructed via a factor graph model, and the inference on the factor graph is achieved by a modiﬁed variant of Multiple-try Metropolis algorithm. Entity retrieval is performed by decomposing entity queries and computing the query likelihood on the entity factoid hierarchy. Using an array of benchmark datasets, we demonstrate that our proposed framework signiﬁcantly improves the retrieval performance over existing models. 
Document enrichment focuses on retrieving relevant knowledge from external resources, which is essential because text is generally replete with gaps. Since conventional work primarily relies on special resources, we instead use triples of Subject, Predicate, Object as knowledge and incorporate distributional semantics to rank them. Our model ﬁrst extracts these triples automatically from raw text and converts them into real-valued vectors based on the word semantics captured by Latent Dirichlet Allocation. We then represent these triples, together with the source document that is to be enriched, as a graph of triples, and adopt a global iterative algorithm to propagate relevance weight from source document to these triples so as to select the most relevant ones. Evaluated as a ranking problem, our model signiﬁcantly outperforms multiple strong baselines. Moreover, we conduct a task-based evaluation by incorporating these triples as additional features into document classiﬁcation and enhances the performance by 3.02%. 
We characterize a class of indirect answers to yes/no questions, alternative answers, where information is given that is not directly asked about, but which might nonetheless address the underlying motivation for the question. We develop a model rooted in game theory that generates these answers via strategic reasoning about possible unobserved domain-level user requirements. We implement the model within an interactive question answering system simulating real estate dialogue. The system learns a prior probability distribution over possible user requirements by analyzing training dialogues, which it uses to make strategic decisions about answer selection. The system generates pragmatically natural and interpretable answers which make for more efﬁcient interactions compared to a baseline. 
While recent years have seen a surge of interest in automated essay grading, including work on grading essays with respect to particular dimensions such as prompt adherence, coherence, and technical quality, there has been relatively little work on grading the essay dimension of argument strength, which is arguably the most important aspect of argumentative essays. We introduce a new corpus of argumentative student essays annotated with argument strength scores and propose a supervised, feature-rich approach to automatically scoring the essays along this dimension. Our approach signiﬁcantly outperforms a baseline that relies solely on heuristically applied sentence argument function labels by up to 16.1%. 
We study the problem of summarizing DAG-structured topic hierarchies over a given set of documents. Example applications include automatically generating Wikipedia disambiguation pages for a set of articles, and generating candidate multi-labels for preparing machine learning datasets (e.g., for text classiﬁcation, functional genomics, and image classiﬁcation). Unlike previous work, which focuses on clustering the set of documents using the topic hierarchy as features, we directly pose the problem as a submodular optimization problem on a topic hierarchy using the documents as features. Desirable properties of the chosen topics include document coverage, speciﬁcity, topic diversity, and topic homogeneity, each of which, we show, is naturally modeled by a submodular function. Other information, provided say by unsupervised approaches such as LDA and its variants, can also be utilized by deﬁning a submodular function that expresses coherence between the chosen topics and this information. We use a large-margin framework to learn convex mixtures over the set of submodular components. We empirically evaluate our method on the problem of automatically generating Wikipedia disambiguation pages using human generated clusterings as ground truth. We ﬁnd that our framework improves upon several baselines according to a variety of standard evaluation metrics including the Jaccard Index, F1 score and NMI, and moreover, can be scaled to extremely large scale problems.  
We study the problem of explaining relationships between pairs of knowledge graph entities with human-readable descriptions. Our method extracts and enriches sentences that refer to an entity pair from a corpus and ranks the sentences according to how well they describe the relationship between the entities. We model this task as a learning to rank problem for sentences and employ a rich set of features. When evaluated on a large set of manually annotated sentences, we ﬁnd that our method signiﬁcantly improves over state-of-the-art baseline models. 
An event chronicle provides people with an easy and fast access to learn the past. In this paper, we propose the ﬁrst novel approach to automatically generate a topically relevant event chronicle during a certain period given a reference chronicle during another period. Our approach consists of two core components – a timeaware hierarchical Bayesian model for event detection, and a learning-to-rank model to select the salient events to construct the ﬁnal chronicle. Experimental results demonstrate our approach is promising to tackle this new problem. 
People create morphs, a special type of fake alternative names, to achieve certain communication goals such as expressing strong sentiment or evading censors. For example, “Black Mamba”, the name for a highly venomous snake, is a morph that Kobe Bryant created for himself due to his agility and aggressiveness in playing basketball games. This paper presents the ﬁrst end-to-end context-aware entity morph decoding system that can automatically identify, disambiguate, verify morph mentions based on speciﬁc contexts, and resolve them to target entities. Our approach is based on an absolute “cold-start” - it does not require any candidate morph or target entity lists as input, nor any manually constructed morph-target pairs for training. We design a semi-supervised collective inference framework for morph mention extraction, and compare various deep learning based approaches for morph resolution. Our approach achieved signiﬁcant improvement over the state-of-the-art method (Huang et al., 2013), which used a large amount of training data. 1 
In this paper, we present a novel approach to joint word sense disambiguation (WSD) and entity linking (EL) that combines a set of complementary objectives in an extensible multi-objective formalism. During disambiguation the system performs continuous optimization to ﬁnd optimal probability distributions over candidate senses. The performance of our system on nominal WSD as well as EL improves state-ofthe-art results on several corpora. These improvements demonstrate the importance of combining complementary objectives in a joint model for robust disambiguation. 
Extracted keyphrases can enhance numerous applications ranging from search to tracking the evolution of scientiﬁc discourse. We present SCHBASE, a hierarchical database of keyphrases extracted from large collections of scientiﬁc literature. SCHBASE relies on a tendency of scientists to generate new abbreviations that “extend” existing forms as a form of signaling novelty. We demonstrate how these keyphrases/concepts can be extracted, and their viability as a database in relation to existing collections. We further show how keyphrases can be placed into a semantically-meaningful “phylogenetic” structure and describe key features of this structure. The complete SCHBASE dataset is available at: http://cond.org/schbase.html. 
Aspect extraction and sentiment analysis of reviews are both important tasks in opinion mining. We propose a novel sentiment and aspect extraction model based on Restricted Boltzmann Machines to jointly address these two tasks in an unsupervised setting. This model reﬂects the generation process of reviews by introducing a heterogeneous structure into the hidden layer and incorporating informative priors. Experiments show that our model outperforms previous state-of-the-art methods. 
We study the application of word embeddings to generate semantic representations for the domain adaptation problem of relation extraction (RE) in the tree kernelbased method. We systematically evaluate various techniques to generate the semantic representations and demonstrate that they are effective to improve the generalization performance of a tree kernel-based relation extractor across domains (up to 7% relative improvement). In addition, we compare the tree kernel-based and the feature-based method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 
In the current fast-paced world, people tend to possess limited knowledge about things from the past. For example, some young users may not know that Walkman played similar function as iPod does nowadays. In this paper, we approach the temporal correspondence problem in which, given an input term (e.g., iPod) and the target time (e.g. 1980s), the task is to find the counterpart of the query that existed in the target time. We propose an approach that transforms word contexts across time based on their neural network representations. We then experimentally demonstrate the effectiveness of our method on the New York Times Annotated Corpus. 
Identifying negative or speculative narrative fragments from fact is crucial for natural language processing (NLP) applications. Previous studies on negation and speculation identification in Chinese language suffers much from two problems: corpus scarcity and the bottleneck in fundamental Chinese information processing. To resolve these problems, this paper constructs a Chinese corpus which consists of three sub-corpora from different resources. In order to detect the negative and speculative cues, a sequence labeling model is proposed. Moreover, a bilingual cue expansion method is proposed to increase the coverage in cue detection. In addition, this paper presents a new syntactic structure-based framework to identify the linguistic scope of a cue, instead of the traditional chunking-based framework. Experimental results justify the usefulness of our Chinese corpus and the appropriateness of our syntactic structure-based framework which obtained significant improvement over the stateof-the-art on negation and speculation identification in Chinese language. * 
The path ranking algorithm (PRA) has been recently proposed to address relational classiﬁcation and retrieval tasks at large scale. We describe Cor-PRA, an enhanced system that can model a larger space of relational rules, including longer relational rules and a class of ﬁrst order rules with constants, while maintaining scalability. We describe and test faster algorithms for searching for these features. A key contribution is to leverage backward random walks to efﬁciently discover these types of rules. An empirical study is conducted on the tasks of graph-based knowledge base inference, and person named entity extraction from parsed text. Our results show that learning paths with constants improves performance on both tasks, and that modeling longer paths dramatically improves performance for the named entity extraction task. 
In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs λ-calculus; (2) enable a probabilistic semantic parsing schema to learn the lambda-calculus representation of manipulation action from an annotated action corpus of videos; (3) use (1) and (2) to develop a system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation. 
Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance. In this paper, we propose a more ﬁne-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The ﬁrst one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classiﬁcation and link prediction. Evaluation results show that our approach outperforms stateof-the-art methods. 
In this paper, we ﬁrst explore the role of inter-annotator agreement statistics in grammatical error correction and conclude that they are less informative in ﬁelds where there may be more than one correct answer. We next created a dataset of 50 student essays, each corrected by 10 different annotators for all error types, and investigated how both human and GEC system scores vary when different combinations of these annotations are used as the gold standard. Upon learning that even humans are unable to score higher than 75% F0.5, we propose a new metric based on the ratio between human and system performance. We also use this method to investigate the extent to which annotators agree on certain error categories, and ﬁnd that similar results can be obtained from a smaller subset of just 10 essays. 
Traditional approaches to word sense disambiguation (WSD) rest on the assumption that there exists a single, unambiguous communicative intention underlying every word in a document. However, writers sometimes intend for a word to be interpreted as simultaneously carrying multiple distinct meanings. This deliberate use of lexical ambiguity—i.e., punning— is a particularly common source of humour. In this paper we describe how traditional, language-agnostic WSD approaches can be adapted to “disambiguate” puns, or rather to identify their double meanings. We evaluate several such approaches on a manually sense-annotated collection of English puns and observe performance exceeding that of some knowledge-based and supervised baselines. 
Meaning of a word varies from one domain to another. Despite this important domain dependence in word semantics, existing word representation learning methods are bound to a single domain. Given a pair of source-target domains, we propose an unsupervised method for learning domain-speciﬁc word representations that accurately capture the domainspeciﬁc aspects of word semantics. First, we select a subset of frequent words that occur in both domains as pivots. Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, pivots that appear in a document must accurately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method signiﬁcantly outperforms competitive baselines including the state-of-theart domain-insensitive word representations, and reports best sentiment classiﬁcation accuracies for all domain-pairs in a benchmark dataset. 
Semantic representation lies at the core of several applications in Natural Language Processing. However, most existing semantic representation techniques cannot be used effectively for the representation of individual word senses. We put forward a novel multilingual concept representation, called MUFFIN, which not only enables accurate representation of word senses in different languages, but also provides multiple advantages over existing approaches. MUFFIN represents a given concept in a uniﬁed semantic space irrespective of the language of interest, enabling cross-lingual comparison of different concepts. We evaluate our approach in two different evaluation benchmarks, semantic similarity and Word Sense Disambiguation, reporting state-of-the-art performance on several standard datasets. 
Extra-linguistic factors inﬂuence language use, and are accounted for by speakers and listeners. Most natural language processing (NLP) tasks to date, however, treat language as uniform. This assumption can harm performance. We investigate the effect of including demographic information on performance in a variety of text-classiﬁcation tasks. We ﬁnd that by including age or gender information, we consistently and signiﬁcantly improve performance over demographic-agnostic models. These results hold across three text-classiﬁcation tasks in ﬁve languages. 
In order to build psycholinguistic models of processing difﬁculty and evaluate these models against human data, we need highly accurate language models. Here we speciﬁcally consider surprisal, a word’s predictability in context. Existing approaches have mostly used n-gram models or more sophisticated syntax-based parsing models; this largely does not account for effects speciﬁc to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic conversational data. An interesting ﬁnding is that the training data for the semantic model also plays a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at ﬁrst counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. 
Latent variable topic models such as Latent Dirichlet Allocation (LDA) can discover topics from text in an unsupervised fashion. However, scaling the models up to the many distinct topics exhibited in modern corpora is challenging. “Flat” topic models like LDA have difﬁculty modeling sparsely expressed topics, and richer hierarchical models become computationally intractable as the number of topics increases. In this paper, we introduce efﬁcient methods for inferring large topic hierarchies. Our approach is built upon the Sparse Backoff Tree (SBT), a new prior for latent topic distributions that organizes the latent topics as leaves in a tree. We show how a document model based on SBTs can effectively infer accurate topic spaces of over a million topics. We introduce a collapsed sampler for the model that exploits sparsity and the tree structure in order to make inference efﬁcient. In experiments with multiple data sets, we show that scaling to large topic spaces results in much more accurate models, and that SBT document models make use of large topic spaces more effectively than ﬂat LDA. 
Language modeling (LM) involves determining the joint probability of words in a sentence. The conditional approach is dominant, representing the joint probability in terms of conditionals. Examples include n-gram LMs and neural network LMs. An alternative approach, called the random ﬁeld (RF) approach, is used in whole-sentence maximum entropy (WSME) LMs. Although the RF approach has potential beneﬁts, the empirical results of previous WSME models are not satisfactory. In this paper, we revisit the RF approach for language modeling, with a number of innovations. We propose a trans-dimensional RF (TDRF) model and develop a training algorithm using joint stochastic approximation and trans-dimensional mixture sampling. We perform speech recognition experiments on Wall Street Journal data, and ﬁnd that our TDRF models lead to performances as good as the recurrent neural network LMs but are computationally more efﬁcient in computing sentence probability. 
Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at capturing semantic regularities in language. In this paper we replace LDA’s parameterization of “topics” as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space. This encourages the model to group words that are a priori known to be semantically related into topics. To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decompositions of covariance matrices of the posterior predictive distributions. We further derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis–Hastings step. Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS). Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA. Quantitatively, our technique outperforms existing models at dealing with OOV words in held-out documents. 
We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is ﬂexible, allows for efﬁcient learning and classiﬁcation, and yields correlation with humans that rivals the state of the art. 
We achieve signiﬁcant improvements in several syntax-based machine translation experiments using a string-to-tree variant of multi bottom-up tree transducers. Our new parameterized rule extraction algorithm extracts string-to-tree rules that can be discontiguous and non-minimal in contrast to existing algorithms for the tree-to-tree setting. The obtained models signiﬁcantly outperform the string-to-tree component of the Moses framework in a large-scale empirical evaluation on several known translation tasks. Our linguistic analysis reveals the remarkable beneﬁts of discontiguous and non-minimal rules. 
Modern statistical machine translation (SMT) systems usually use a linear combination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner, which might limit the expressive power of the model and lead to a under-ﬁt model on the current data. In this paper, we propose a nonlinear modeling for the quality of translation hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear models. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model. 
We introduce into Bayesian decipherment a base distribution derived from similarities of word embeddings. We use Dirichlet multinomial regression (Mimno and McCallum, 2012) to learn a mapping between ciphertext and plaintext word embeddings from non-parallel data. Experimental results show that the base distribution is highly beneﬁcial to decipherment, improving state-of-the-art decipherment accuracy from 45.8% to 67.4% for Spanish/English, and from 5.1% to 11.2% for Malagasy/English. 
The quality of statistical machine translation performed with phrase based approaches can be increased by permuting the words in the source sentences in an order which resembles that of the target language. We propose a class of recurrent neural models which exploit source-side dependency syntax features to reorder the words into a target-like order. We evaluate these models on the German-to-English and Italian-toEnglish language pairs, showing signiﬁcant improvements over a phrasebased Moses baseline. We also compare with state of the art German-toEnglish pre-reordering rules, showing that our method obtains similar or better results. 
Deception detection has been formulated as a supervised binary classiﬁcation problem on single documents. However, in daily life, millions of fraud cases involve detailed conversations between deceivers and victims. Deceivers may dynamically adjust their deceptive statements according to the reactions of victims. In addition, people may form groups and collaborate to deceive others. In this paper, we seek to identify deceptive groups from their conversations. We propose a novel subgroup detection method that combines linguistic signals and signed network analysis for dynamic clustering. A social-elimination game called Killer Game is introduced as a case study1. Experimental results demonstrate that our approach signiﬁcantly outperforms human voting and state-of-theart subgroup detection methods at dynamically differentiating the deceptive groups from truth-tellers. 
Stubs on Wikipedia often lack comprehensive information. The huge cost of editing Wikipedia and the presence of only a limited number of active contributors curb the consistent growth of Wikipedia. In this work, we present WikiKreator, a system that is capable of generating content automatically to improve existing stubs on Wikipedia. The system has two components. First, a text classiﬁer built using topic distribution vectors is used to assign content from the web to various sections on a Wikipedia article. Second, we propose a novel abstractive summarization technique based on an optimization framework that generates section-speciﬁc summaries for Wikipedia stubs. Experiments show that WikiKreator is capable of generating well-formed informative content. Further, automatically generated content from our system have been appended to Wikipedia stubs and the content has been retained successfully proving the effectiveness of our approach. 
Using natural language to write programs is a touchstone problem for computational linguistics. We present an approach that learns to map natural-language descriptions of simple “if-then” rules to executable code. By training and testing on a large corpus of naturally-occurring programs (called “recipes”) and their natural language descriptions, we demonstrate the ability to effectively map language to code. We compare a number of semantic parsing approaches on the highly noisy training data collected from ordinary users, and ﬁnd that loosely synchronous systems perform best. 
We develop an approach for generating deep (i.e, high-level) comprehension questions from novel text that bypasses the myriad challenges of creating a full semantic representation. We do this by decomposing the task into an ontologycrowd-relevance workflow, consisting of first representing the original text in a low-dimensional ontology, then crowdsourcing candidate question templates aligned with that space, and finally ranking potentially relevant templates for a novel region of text. If ontological labels are not available, we infer them from the text. We demonstrate the effectiveness of this method on a corpus of articles from Wikipedia alongside human judgments, and find that we can generate relevant deep questions with a precision of over 85% while maintaining a recall of 70%.  and Ha, 2003; Schwartz, 2004), both as a means of providing self-assessments directly to students and as a tool to help teachers with question authoring. Much work to date has focused on questions based on a single sentence of the text (Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014), and the ideal of creating deep, conceptual questions has remained elusive. In this work, we hope to take a significant step towards this challenge by approaching the problem in a somewhat unconventional way.  
This paper presents the NL2KR platform to build systems that can translate text to different formal languages. It is freelyavailable1, customizable, and comes with an Interactive GUI support that is useful in the development of a translation system. Our key contribution is a userfriendly system based on an interactive multistage learning algorithm. This effective algorithm employs Inverse-λ, Generalization and user provided dictionary to learn new meanings of words from sentences and their representations. Using the learned meanings, and the Generalization approach, it is able to translate new sentences. NL2KR is evaluated on two standard corpora, Jobs and GeoQuery and it exhibits state-of-the-art performance on both of them. 
We investigate multiple many-to-many alignments as a primary step in integrating supplemental information strings in string transduction. Besides outlining DP based solutions to the multiple alignment problem, we detail an approximation of the problem in terms of multiple sequence segmentations satisfying a coupling constraint. We apply our approach to boosting baseline G2P systems using homogeneous as well as heterogeneous sources of supplemental information. 
In this paper, we propose a syllable-based method for tweet normalization to study the cognitive process of non-standard word creation in social media. Assuming that syllable plays a fundamental role in forming the non-standard tweet words, we choose syllable as the basic unit and extend the conventional noisy channel model by incorporating the syllables to represent the word-to-word transitions at both word and syllable levels. The syllables are used in our method not only to suggest more candidates, but also to measure similarity between words. Novelty of this work is three-fold: First, to the best of our knowledge, this is an early attempt to explore syllables in tweet normalization. Second, our proposed normalization method relies on unlabeled samples, making it much easier to adapt our method to handle non-standard words in any period of history. And third, we conduct a series of experiments and prove that the proposed method is advantageous over the state-of-art solutions for tweet normalization. 
Most previous work of text normalization on informal text made a strong assumption that the system has already known which tokens are non-standard words (NSW) and thus need normalization. However, this is not realistic. In this paper, we propose a method for NSW detection. In addition to the information based on the dictionary, e.g., whether a word is out-ofvocabulary (OOV), we leverage novel information derived from the normalization results for OOV words to help make decisions. Second, this paper investigates two methods using NSW detection results for named entity recognition (NER) in social media data. One adopts a pipeline strategy, and the other uses a joint decoding fashion. We also create a new data set with newly added normalization annotation beyond the existing named entity labels. This is the ﬁrst data set with such annotation and we release it for research purpose. Our experiment results demonstrate the effectiveness of our NSW detection method and the beneﬁt of NSW detection for NER. Our proposed methods perform better than the state-of-the-art NER system. 
Many high level natural language processing problems can be framed as determining if two given sentences are a rewriting of each other. In this paper, we propose a class of kernel functions, referred to as type-enriched string rewriting kernels, which, used in kernel-based machine learning algorithms, allow to learn sentence rewritings. Unlike previous work, this method can be fed external lexical semantic relations to capture a wider class of rewriting rules. It also does not assume preliminary syntactic parsing but is still able to provide a uniﬁed framework to capture syntactic structure and alignments between the two sentences. We experiment on three different natural sentence rewriting tasks and obtain state-of-the-art results for all of them. 
Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the ﬁrst SP learning method that simultaneously draws knowledge from text, images and videos, using image and video descriptions to obtain visual features. Our results show that it outperforms linguistic and visual models in isolation, as well as the existing SP induction approaches. 
 Existing methods for Japanese predicate  argument structure (PAS) analysis identify  case arguments of each predicate without  considering interactions between the tar-  get PAS and others in a sentence. However, the argument structures of the predicates in a sentence are semantically related to each other. This paper proposes new methods for Japanese PAS analysis to jointly identify case arguments of all predicates in a sentence by (1) modeling multiple PAS interactions with a bipar-  Figure 1: An example of Japanese PAS. The English translation is “Because ϕi caught a cold, Ii skipped school.”. The upper edges are dependency relations, and the under edges are case arguments. “NOM” and “ACC” represents the nominative and accusative arguments, respectively. “ϕi” is a zeropronoun, referring to the antecedent “watashii”.  tite graph and (2) approximately search-  ing optimal PAS combinations. Perform-  ing experiments on the NAIST Text Cor-  The case role label “NOM” and “ACC” respec-  pus, we demonstrate that our joint analysis  tively represents the nominative and accusative  methods substantially outperform a strong baseline and are comparable to previous  roles, and ϕi represents a zero-pronoun. There are two predicates “hiita (caught)” and “yasunda  work.  (skipped)”. For the predicate “yasunda (skipped)”,  “watashii-wa (Ii)” is the “skipper”, and “gakko-wo  (school)” is the “entity skipped”. It is easy to iden-  tify these arguments, since syntactic dependency  
We introduce C-PHRASE, a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree, from single words to full sentences. C-PHRASE outperforms the state-of-theart C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models. 
 The Abstract Meaning Representation (AMR) is a representation for opendomain rich semantics, with potential use in ﬁelds like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous approach, and can be learned with a simple classiﬁer. We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by 3 F1 on both the LDC2013E117 and LDC2014T12 datasets. 
 Text: “get the cup, fill it with water and then microwave the cup”  We focus on the task of interpreting complex natural language instructions to a robot, in which we must ground high-level commands such as microwave the cup to low-level actions such as grasping. Previous approaches that learn a lexicon during training have inadequate coverage at test time, and pure search strategies cannot handle the exponential search space. We propose a new hybrid approach that leverages the environment to induce new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute, mix, arrange to complex logical forms, each containing up to four predicates. 
This paper studies the use of structural representations for learning relations between pairs of short texts (e.g., sentences or paragraphs) of the kind: the second text answers to, or conveys exactly the same information of, or is implied by, the ﬁrst text. Engineering effective features that can capture syntactic and semantic relations between the constituents composing the target text pairs is rather complex. Thus, we deﬁne syntactic and semantic structures representing the text pairs and then apply graph and tree kernels to them for automatically engineering features in Support Vector Machines. We carry out an extensive comparative analysis of stateof-the-art models for this type of relational learning. Our ﬁndings allow for achieving the highest accuracy in two different and important related tasks, i.e., Paraphrasing Identiﬁcation and Textual Entailment Recognition. 
Neural network methods have achieved promising results for sentiment classiﬁcation of text. However, these models only use semantics of texts, while ignoring users who express the sentiment and products which are evaluated, both of which have great inﬂuences on interpreting the sentiment of text. In this paper, we address this issue by incorporating user- and product- level information into a neural network approach for document level sentiment classiﬁcation. Users and products are modeled using vector space models, the representations of which capture important global clues such as individual preferences of users or overall qualities of products. Such global evidence in turn facilitates embedding learning procedure at document level, yielding better text representations. By combining evidence at user-, product- and documentlevel in a uniﬁed neural framework, the proposed model achieves state-of-the-art performances on IMDB and Yelp datasets1. 
Central to many sentiment analysis tasks are sentiment lexicons (SLs). SLs exhibit polarity inconsistencies. Previous work studied the problem of checking the consistency of an SL for the case when the entries have categorical labels (positive, negative or neutral) and showed that it is NPhard. In this paper, we address the more general problem, in which polarity tags take the form of a continuous distribution in the interval [0, 1]. We show that this problem is polynomial. We develop a general framework for addressing the consistency problem using linear programming (LP) theory. LP tools allow us to uncover inconsistencies efﬁciently, paving the way to building SL debugging tools. We show that previous work corresponds to 0-1 integer programming, a particular case of LP. Our experimental studies show a strong correlation between polarity consistency in SLs and the accuracy of sentiment tagging in practice. 
Automatically detecting verbal irony (roughly, sarcasm) in online content is important for many practical applications (e.g., sentiment detection), but it is difﬁcult. Previous approaches have relied predominantly on signal gleaned from word counts and grammatical cues. But such approaches fail to exploit the context in which comments are embedded. We thus propose a novel strategy for verbal irony classiﬁcation that exploits contextual features, speciﬁcally by combining noun phrases and sentiment extracted from comments with the forum type (e.g., conservative or liberal) to which they were posted. We show that this approach improves verbal irony classiﬁcation performance. Furthermore, because this method generates a very large feature space (and we expect predictive contextual features to be strong but few), we propose a mixed regularization strategy that places a sparsity-inducing 1 penalty on the contextual feature weights on top of the 2 penalty applied to all model coefﬁcients. This increases model sparsity and reduces the variance of model performance. 
Predicting emotion categories, such as anger, joy, and anxiety, expressed by a sentence is challenging due to its inherent multi-label classification difficulty and data sparseness. In this paper, we address above two challenges by incorporating the label dependence among the emotion labels and the context dependence among the contextual instances into a factor graph model. Specifically, we recast sentence-level emotion classification as a factor graph inferring problem in which the label and context dependence are modeled as various factor functions. Empirical evaluation demonstrates the great potential and effectiveness of our proposed approach to sentencelevel emotion classification. 1 
A review text is normally represented as a bag-of-words (BOW) in sentiment classiﬁcation. Such a simpliﬁed BOW model has fundamental deﬁciencies in modeling some complex linguistic phenomena such as negation. In this work, we propose a dual-view co-training algorithm based on dual-view BOW representation for semisupervised sentiment classiﬁcation. In dual-view BOW, we automatically construct antonymous reviews and model a review text by a pair of bags-of-words with opposite views. We make use of the original and antonymous views in pairs, in the training, bootstrapping and testing process, all based on a joint observation of two views. The experimental results demonstrate the advantages of our approach, in meeting the two co-training requirements, addressing the negation problem, and enhancing the semi-supervised sentiment classiﬁcation efﬁciency. 
This paper tackles the issue of the detection of user’s verbal expressions of likes and dislikes in a human-agent interaction. We present a system grounded on the theoretical framework provided by (Martin and White, 2005) that integrates the interaction context by jointly processing agent’s and user’s utterances. It is designed as a rule-based and bottom-up process based on a symbolic representation of the structure of the sentence. This article also describes the annotation campaign – carried out through Amazon Mechanical Turk – for the creation of the evaluation dataset. Finally, we present all measures for rating agreement between our system and the human reference and obtain agreement scores that are equal or higher than substantial agreements. 
In this paper, we address the problem of evaluating spontaneous speech using a combination of machine learning and crowdsourcing. Machine learning techniques inadequately solve the stated problem because automatic speakerindependent speech transcription is inaccurate. The features derived from it are also inaccurate and so is the machine learning model developed for speech evaluation. To address this, we post the task of speech transcription to a large community of online workers (crowd). We also get spoken English grades from the crowd. We achieve 95% transcription accuracy by combining transcriptions from multiple crowd workers. Speech and prosody features are derived by force aligning the speech samples on these highly accurate transcriptions. Additionally, we derive surface and semantic level features directly from the transcription. To demonstrate the efﬁcacy of our approach we performed experiments on an expert–graded speech sample of 319 adult non–native speakers. Using these features in a regression model, we are able achieve a Pearson correlation of 0.76 with expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. This work is timely given the huge requirement of spoken English training and assessment. 
ROVER is a widely used method to combine the output of multiple automatic speech recognition (ASR) systems. Though effective, the basic approach and its variants suffer from potential drawbacks: i) their results depend on the order in which the hypotheses are used to feed the combination process, ii) when applied to combine long hypotheses, they disregard possible differences in transcription quality at local level, iii) they often rely on word conﬁdence information. We address these issues by proposing a segment-based ROVER in which hypothesis ranking is obtained from a conﬁdence-independent ASR quality estimation method. Our results on English data from the IWSLT2012 and IWSLT2013 evaluation campaigns signiﬁcantly outperform standard ROVER and approximate two strong oracles. 
Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Longshort term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a ﬁrst step toward generating coherent text units from neural models, our work has the potential to signiﬁcantly impact natural language generation and summarization1. 
Complex conjunctions and determiners are often considered as pretokenized units in parsing. This is not always realistic, since they can be ambiguous. We propose a model for joint dependency parsing and multiword expressions identiﬁcation, in which complex function words are represented as individual tokens linked with morphological dependencies. Our graphbased parser includes standard secondorder features and verbal subcategorization features derived from a syntactic lexicon.We train it on a modiﬁed version of the French Treebank enriched with morphological dependencies. It recognizes 81.79% of ADV+que conjunctions with 91.57% precision, and 82.74% of de+DET determiners with 86.70% precision. 
The performance of discriminative constituent parsing relies crucially on feature engineering, and effective features usually have to be carefully selected through a painful manual process. In this paper, we propose to automatically learn a set of effective features via neural networks. Speciﬁcally, we build a feedforward neural network model, which takes as input a few primitive units (words, POS tags and certain contextual tokens) from the local context, induces the feature representation in the hidden layer and makes parsing predictions in the output layer. The network simultaneously learns the feature representation and the prediction model parameters using a back propagation algorithm. By pre-training the model on a large amount of automatically parsed data, and then ﬁne-tuning on the manually annotated Treebank data, our parser achieves the highest F1 score at 86.6% on Chinese Treebank 5.1, and a competitive F1 score at 90.7% on English Treebank. More importantly, our parser generalizes well on cross-domain test sets, where we signiﬁcantly outperform Berkeley parser by 3.4 points on average for Chinese and 2.5 points for English. 
Dependency parsers are usually evaluated on attachment accuracy. Whilst easily interpreted, the metric does not illustrate the cascading impact of errors, where the parser chooses an incorrect arc, and is subsequently forced to choose further incorrect arcs elsewhere in the parse. We apply arc-level constraints to MSTparser and ZPar, enforcing the correct analysis of speciﬁc error classes, whilst otherwise continuing with decoding. We investigate the direct and indirect impact of applying constraints to the parser. Erroneous NP and punctuation attachments cause the most cascading errors, while incorrect PP and coordination attachments are frequent but less inﬂuential. Punctuation is especially challenging, as it has long been ignored in parsing, and serves a variety of disparate syntactic roles. 
In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 
Constituent parsing is typically modeled by a chart-based algorithm under probabilistic context-free grammars or by a transition-based algorithm with rich features. Previous models rely heavily on richer syntactic information through lexicalizing rules, splitting categories, or memorizing long histories. However enriched models incur numerous parameters and sparsity issues, and are insufﬁcient for capturing various syntactic phenomena. We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure. Our transition-based neural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters. 
Given a set of basic binary features, we propose a new L1 norm SVM based feature selection method that explicitly selects the features in their polynomial or tree kernel spaces. The efﬁciency comes from the anti-monotone property of the subgradients: the subgradient with respect to a combined feature can be bounded by the subgradient with respect to each of its component features, and a feature can be pruned safely without further consideration if its corresponding subgradient is not steep enough. We conduct experiments on the English dependency parsing task with a third order graph-based parser. Beneﬁting from the rich features selected in the tree kernel space, our model achieved the best reported unlabeled attachment score of 93.72 without using any additional resource. 
Incremental parsing is the task of assigning a syntactic structure to an input sentence as it unfolds word by word. Incremental parsing is more difﬁcult than fullsentence parsing, as incomplete input increases ambiguity. Intuitively, an incremental parser that has access to semantic information should be able to reduce ambiguity by ruling out semantically implausible analyses, even for incomplete input. In this paper, we test this hypothesis by combining an incremental TAG parser with an incremental semantic role labeler in a discriminative framework. We show a substantial improvement in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score. 
We present an extension to incremental shift-reduce parsing that handles discontinuous constituents, using a linear classiﬁer and beam search. We achieve very high parsing speeds (up to 640 sent./sec.) and accurate results (up to 79.52 F1 on TiGer). 
Neural probabilistic parsers are attractive for their capability of automatic feature combination and small data sizes. A transition-based greedy neural parser has given better accuracies over its linear counterpart. We propose a neural probabilistic structured-prediction model for transition-based dependency parsing, which integrates search and learning. Beam search is used for decoding, and contrastive learning is performed for maximizing the sentence-level log-likelihood. In standard Penn Treebank experiments, the structured neural parser achieves a 1.8% accuracy improvement upon a competitive greedy neural parser baseline, giving performance comparable to the best linear parser. 
Treebanks are key resources for developing accurate statistical parsers. However, building treebanks is expensive and timeconsuming for humans. For domains requiring deep subject matter expertise such as law and medicine, treebanking is even more difﬁcult. To reduce annotation costs for these domains, we develop methods to improve cross-domain parsing inference using paraphrases. Paraphrases are easier to obtain than full syntactic analyses as they do not require deep linguistic knowledge, only linguistic ﬂuency. A sentence and its paraphrase may have similar syntactic structures, allowing their parses to mutually inform each other. We present several methods to incorporate paraphrase information by jointly parsing a sentence with its paraphrase. These methods are applied to state-of-the-art constituency and dependency parsers and provide significant improvements across multiple domains. 
This paper investigates the problem of cross-lingual dependency parsing, aiming at inducing dependency parsers for low-resource languages while using only training data from a resource-rich language (e.g. English). Existing approaches typically don’t include lexical features, which are not transferable across languages. In this paper, we bridge the lexical feature gap by using distributed feature representations and their composition. We provide two algorithms for inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space. Consequently, both lexical features and non-lexical features can be used in our model for cross-lingual transfer. Furthermore, our framework is able to incorporate additional useful features such as cross-lingual word clusters. Our combined contributions achieve an average relative error reduction of 10.9% in labeled attachment score as compared with the delexicalized parser, trained on English universal treebank and transferred to three other languages. It also signiﬁcantly outperforms McDonald et al. (2013) augmented with projected cluster features on identical data. 
How we teach and learn is undergoing a revolution, due to changes in technology and connectivity. Education may be one of the best application areas for advanced NLP techniques, and NLP researchers have much to contribute to this problem, especially in the areas of learning to write, mastery learning, and peer learning. In this paper I consider what happens when we convert natural language processors into natural language coaches. 
This paper proposes a novel approach for incorporating discourse information into machine comprehension applications. Traditionally, such information is computed using off-the-shelf discourse analyzers. This design provides limited opportunities for guiding the discourse parser based on the requirements of the target task. In contrast, our model induces relations between sentences while optimizing a task-speciﬁc objective. This approach enables the model to beneﬁt from discourse information without relying on explicit annotations of discourse structure during training. The model jointly identiﬁes relevant sentences, establishes relations between them and predicts an answer. We implement this idea in a discriminative framework with hidden variables that capture relevant sentences and relations unobserved during training. Our experiments demonstrate that the discourse aware model outperforms state-of-the-art machine comprehension systems.1 
There is a growing interest in researching null instantiations, which are those implicit semantic arguments. Many of these implicit arguments can be linked to referents in context, and their discoveries are of great beneﬁts to semantic processing. We address the issue of automatically identifying and resolving implicit arguments in Chinese discourse. For their resolutions, we present an approach that combines the information about overtly labeled arguments and frame-to-frame relations deﬁned by FrameNet. Experimental results on our created corpus demonstrate the effectiveness of our approach. 
This paper describes a novel sequence labeling method for identifying generic expressions, which refer to kinds or arbitrary members of a class, in discourse context. The automatic recognition of such expressions is important for any natural language processing task that requires text understanding. Prior work has focused on identifying generic noun phrases; we present a new corpus in which not only subjects but also clauses are annotated for genericity according to an annotation scheme motivated by semantic theory. Our contextaware approach for automatically identifying generic expressions uses conditional random ﬁelds and outperforms previous work based on local decisions when evaluated on this corpus and on related data sets (ACE-2 and ACE-2005). 
This work develops a new statistical understanding of word embeddings induced from transformed count data. Using the class of hidden Markov models (HMMs) underlying Brown clustering as a generative model, we demonstrate how canonical correlation analysis (CCA) and certain count transformations permit efﬁcient and effective recovery of model parameters with lexical semantics. We further show in experiments that these techniques empirically outperform existing spectral methods on word similarity and analogy tasks, and are also competitive with other popular methods such as WORD2VEC and GLOVE. 
Existing distributed representations are limited in utilizing structured knowledge to improve semantic relatedness modeling. We propose a principled framework of embedding entities that integrates hierarchical information from large-scale knowledge bases. The novel embedding model associates each category node of the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method. 
A recent distributional approach to wordanalogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we ﬁnd that performance on this task can be related to orthogonality within the space. Explicitly designing such structure into a neural network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within English Wikipedia text to enable this decomposition can produce substantial improvements on semantic-similarity, posinduction and word-analogy tasks. 
We consider the problem of building scalable semantic parsers for Freebase, and present a new approach for learning to do partial analyses that ground as much of the input text as possible without requiring that all content words be mapped to Freebase concepts. We study this problem on two newly introduced large-scale noun phrase datasets, and present a new semantic parsing model and semi-supervised learning approach for reasoning with partial ontological support. Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. 
We propose a novel semantic parsing framework for question answering using a knowledge base. We deﬁne a query graph that resembles subgraphs of the knowledge base and can be directly mapped to a logical form. Semantic parsing is reduced to query graph generation, formulated as a staged search problem. Unlike traditional approaches, our method leverages the knowledge base in an early stage to prune the search space and thus simpliﬁes the semantic matching problem. By applying an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences, our system outperforms previous methods substantially, and achieves an F1 measure of 52.5% on the WEBQUESTIONS dataset. 
How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours. 
In this paper, we introduce Long ShortTerm Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in the memory block structure, the model could handle interactions between words through a ﬂexible compositional function. Experiments on a public noisy labelled data show that our model outperforms several feature-engineering approaches, with the result comparable to the current best data-driven technique. According to the evaluation on a generated negation phrase test set, the proposed architecture doubles the performance of non-neural model based on bag-of-word features. Furthermore, words with special functions (such as negation and transition) are distinguished and the dissimilarities of words with opposite sentiment are magniﬁed. An interesting case study on negation expression processing shows a promising potential of the architecture dealing with complex sentiment phrases. 
The goal of this research is to build a model to predict stock price movement using sentiments on social media. A new feature which captures topics and their sentiments simultaneously is introduced in the prediction model. In addition, a new topic model TSLDA is proposed to obtain this feature. Our method outperformed a model using only historical prices by about 6.07% in accuracy. Furthermore, when comparing to other sentiment analysis methods, the accuracy of our method was also better than LDA and JST based methods by 6.43% and 6.07%. The results show that incorporation of the sentiment information from social media can help to improve the stock prediction. 
Recursive neural network is one of the most successful deep learning models for natural language processing due to the compositional nature of text. The model recursively composes the vector of a parent phrase from those of child words or phrases, with a key component named composition function. Although a variety of composition functions have been proposed, the syntactic information has not been fully encoded in the composition process. We propose two models, Tag Guided RNN (TGRNN for short) which chooses a composition function according to the part-ofspeech tag of a phrase, and Tag Embedded RNN/RNTN (TE-RNN/RNTN for short) which learns tag embeddings and then combines tag and word embeddings together. In the ﬁne-grained sentiment classiﬁcation, experiment results show the proposed models obtain remarkable improvement: TG-RNN/TE-RNN obtain remarkable improvement over baselines, TE-RNTN obtains the second best result among all the top performing models, and all the proposed models have much less parameters/complexity than their counterparts. 
 In this paper, we introduce a new method for the problem of unsupervised dependency parsing. Most current approaches are based on generative models. Learning the parameters of such models relies on solving a non-convex optimization problem, thus making them sensitive to initialization. We propose a new convex formulation to the task of dependency grammar induction. Our approach is discriminative, allowing the use of different kinds of features. We describe an efﬁcient optimization algorithm to learn the parameters of our model, based on the Frank-Wolfe algorithm. Our method can easily be generalized to other unsupervised learning problems. We evaluate our approach on ten languages belonging to four different families, showing that our method is competitive with other state-of-the-art methods. 
Syntactic annotation is a hard task, but it can be made easier by allowing annotators ﬂexibility to leave aspects of a sentence underspeciﬁed. Unfortunately, partial annotations are not typically directly usable for training parsers. We describe a method for imputing missing dependencies from sentences that have been partially annotated using the Graph Fragment Language, such that a standard dependency parser can then be trained on all annotations. We show that this strategy improves performance over not using partial annotations for English, Chinese, Portuguese and Kinyarwanda, and that performance competitive with state-of-the-art unsupervised and weakly-supervised parsers can be reached with just a few hours of annotation. 
Work in grammar induction should help shed light on the amount of syntactic structure that is discoverable from raw word or tag sequences. But since most current grammar induction algorithms produce unlabeled dependencies, it is difﬁcult to analyze what types of constructions these algorithms can or cannot capture, and, therefore, to identify where additional supervision may be necessary. This paper provides an in-depth analysis of the errors made by unsupervised CCG parsers by evaluating them against the labeled dependencies in CCGbank, hinting at new research directions necessary for progress in grammar induction. 
We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date. 
We propose a cross-lingual framework for learning coreference resolvers for resource-poor target languages, given a resolver in a source language. Our method uses word-aligned bitext to project information from the source to the target. To handle task-speciﬁc costs, we propose a softmax-margin variant of posterior regularization, and we use it to achieve robustness to projection errors. We show empirically that this strategy outperforms competitive cross-lingual methods, such as delexicalized transfer with bilingual word embeddings, bitext direct projection, and vanilla posterior regularization. 
We introduce the Hierarchical Ideal Point Topic Model, which provides a rich picture of policy issues, framing, and voting behavior using a joint model of votes, bill text, and the language that legislators use when debating bills. We use this model to look at the relationship between Tea Party Republicans and “establishment” Republicans in the U.S. House of Representatives during the 112th Congress. 
Many existing knowledge bases (KBs), including Freebase, Yago, and NELL, rely on a ﬁxed ontology, given as an input to the system, which deﬁnes the data to be cataloged in the KB, i.e., a hierarchy of categories and relations between them. The system then extracts facts that match the predeﬁned ontology. We propose an unsupervised model that jointly learns a latent ontological structure of an input corpus, and identiﬁes facts from the corpus that match the learned structure. Our approach combines mixed membership stochastic block models and topic models to infer a structure by jointly modeling text, a latent concept hierarchy, and latent semantic relationships among the entities mentioned in the text. As a case study, we apply the model to a corpus of Web documents from the software domain, and evaluate the accuracy of the various components of the learned ontology. 
Most existing topic models make the bagof-words assumption that words are generated independently, and so ignore potentially useful information about word order. Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipeline approach, restricted attention to bigrams, or resulted in models whose inference does not scale to large corpora. This paper studies how to simultaneously learn both collocations and their topic assignments. We present an efﬁcient reformulation of the Adaptor Grammar-based topical collocation model (AG-colloc) (Johnson, 2010), and develop a point-wise sampling algorithm for posterior inference in this new formulation. We further improve the efﬁciency of the sampling algorithm by exploiting sparsity and parallelising inference. Experimental results derived in text classiﬁcation, information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of documents while maintaining the good performance of the AG-colloc model. 
Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains signiﬁcant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. 
A key problem in semantic parsing with graph-based semantic representations is graph parsing, i.e. computing all possible analyses of a given graph according to a grammar. This problem arises in training synchronous string-to-graph grammars, and when generating strings from them. We present two algorithms for graph parsing (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 
Current distributed representations of words show little resemblance to theories of lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) and relations (e.g., synonymy and hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with. Most importantly, we ﬁnd that they outperform the original vectors on benchmark tasks. 
In this paper, we propose a general framework to incorporate semantic knowledge into the popular data-driven learning process of word embeddings to improve the quality of them. Under this framework, we represent semantic knowledge as many ordinal ranking inequalities and formulate the learning of semantic word embeddings (SWE) as a constrained optimization problem, where the data-derived objective function is optimized subject to all ordinal knowledge inequality constraints extracted from available knowledge resources such as Thesaurus and WordNet. We have demonstrated that this constrained optimization problem can be efﬁciently solved by the stochastic gradient descent (SGD) algorithm, even for a large number of inequality constraints. Experimental results on four standard NLP tasks, including word similarity measure, sentence completion, name entity recognition, and the TOEFL synonym selection, have all demonstrated that the quality of learned word vectors can be signiﬁcantly improved after semantic knowledge is incorporated as inequality constraints during the learning process of word embeddings. 
We add an interpretable semantics to the paraphrase database (PPDB). To date, the relationship between phrase pairs in the database has been weakly deﬁned as approximately equivalent. We show that these pairs represent a variety of relations, including directed entailment (little girl/girl) and exclusion (nobody/someone). We automatically assign semantic entailment relations to entries in PPDB using features derived from past work on discovering inference rules from text and semantic taxonomy induction. We demonstrate that our model assigns these relations with high accuracy. In a downstream RTE task, our labels rival relations from WordNet and improve the coverage of a proof-based RTE system by 17%. 
We present a constituent shift-reduce parser with a structured perceptron that ﬁnds the optimal parse in a practical runtime. The key ideas are new feature templates that facilitate state merging of dynamic programming and A* search. Our system achieves 91.1 F1 on a standard English experiment, a level which cannot be reached by other beam-based systems even with large beam sizes.1 
This paper is concerned with building CCG-grounded, semantics-oriented deep dependency structures with a data-driven, factorization model. Three types of factorization together with different higherorder features are designed to capture different syntacto-semantic properties of functor-argument dependencies. Integrating heterogeneous factorizations results in intractability in decoding. We propose a principled method to obtain optimal graphs based on dual decomposition. Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result. which yields a signiﬁcant improvement over the best published result in the literature. Our implementation is available at http://www.icst. pku.edu.cn/lcwm/grass. 
We propose a convolutional neural network, named genCNN, for word sequence prediction. Different from previous work on neural networkbased language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a ﬁxed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy speciﬁcally designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the state-ofthe-arts with big margins. 
We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoderdecoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming stateof-the-arts in the same setting, including retrieval-based and SMT-based models. 
We propose an abstraction-based multidocument summarization framework that can construct new sentences by exploring more ﬁne-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method ﬁrst constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-ofthe-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation. 
Automatic timeline summarization (TLS) generates precise, dated overviews over (often prolonged) events, such as wars or economic crises. One subtask of TLS selects the most important dates for an event within a certain time frame. Date selection has up to now been handled via supervised machine learning approaches that estimate the importance of each date separately, using features such as the frequency of date mentions in news corpora. This approach neglects interactions between different dates that occur due to connections between subevents. We therefore suggest a joint graphical model for date selection. Even unsupervised versions of this model perform as well as supervised state-of-theart approaches. With parameter tuning on training data, it outperforms prior supervised models by a considerable margin. 
During crises such as natural disasters or other human tragedies, information needs of both civilians and responders often require urgent, specialized treatment. Monitoring and summarizing a text stream during such an event remains a difﬁcult problem. We present a system for update summarization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection, increasing the quality of the updates. We use novel, disaster-speciﬁc features for salience prediction, including geo-locations and language models representing the language of disaster. Our evaluation on a standard set of retrospective events using ROUGE shows that salience prediction provides a signiﬁcant improvement over other approaches. 
 2014), there is limited research on the relationship  In this paper we present the task of unsupervised prediction of speakers’ acceptability judgements. We use a test set generated from the British National Corpus (BNC) containing both grammatical sentences and sentences containing a variety of syntactic infelicities introduced by round trip machine translation. This set was annotated for acceptability judgements through crowd sourcing. We trained a variety of unsupervised language models on the original BNC, and tested them to see the extent to which they could predict mean speakers’ judgements on the test set. To map probability to acceptability, we experimented with several normalisation functions to neutralise the effects of sentence length and word frequencies. We found encouraging results with the unsupervised models predicting acceptability across two different datasets. Our methodology is highly portable to other domains and languages, and the approach has potential implications for the representation and the acquisition of linguistic knowledge.  between acceptability and probability. In this paper, we consider the the task of unsupervised prediction of acceptability. Speakers have robust intuitions about acceptability, and acceptability has been consistently rated on various scales (Sprouse and Almeida, 2012). The acceptability of a sentence appears to be relatively unaffected by its length (within certain bounds), or the frequency of its words, properties that we have conﬁrmed experimentally. By contrast sentence probability does depend on these factors. To ﬁlter the effects of sentence length and word frequency, we devise normalising functions to map the probability of a sentence (inferred by our unsupervised language models) to an acceptability score. Keller (2001) and Lau et al. (2014) present evidence that acceptability exhibits gradience. Accordingly, we treat acceptability as a continuous variable here. We train a variety of unsupervised models for the acceptability prediction task, and we assess the performance of these models by measuring the correlation between their normalised acceptability scores and the mean crowdsourced acceptability judgements on a set of test sentences. There are a number of NLP tasks to which our  
Framing is a sophisticated form of discourse in which the speaker tries to induce a cognitive bias through consistent linkage between a topic and a speciﬁc context (frame). We build on political science and communication theory and use probabilistic topic models combined with time series regression analysis (autoregressive distributed-lag models) to gain insights about the language dynamics in the political processes. Processing four years of public statements issued by members of the U.S. Congress, our results provide a glimpse into the complex dynamic processes of framing, attention shifts and agenda setting, commonly known as ‘spin’. We further provide new evidence for the divergence in party discipline in U.S. politics. 
We propose a language production model that uses dynamic discourse information to account for speakers’ choices of referring expressions. Our model extends previous rational speech act models (Frank and Goodman, 2012) to more naturally distributed linguistic data, instead of assuming a controlled experimental setting. Simulations show a close match between speakers’ utterances and model predictions, indicating that speakers’ behavior can be modeled in a principled way by considering the probabilities of referents in the discourse and the information conveyed by each word. 
Interpersonal relations are ﬁckle, with close friendships often dissolving into enmity. In this work, we explore linguistic cues that presage such transitions by studying dyadic interactions in an online strategy game where players form alliances and break those alliances through betrayal. We characterize friendships that are unlikely to last and examine temporal patterns that foretell betrayal. We reveal that subtle signs of imminent betrayal are encoded in the conversational patterns of the dyad, even if the victim is not aware of the relationship’s fate. In particular, we ﬁnd that lasting friendships exhibit a form of balance that manifests itself through language. In contrast, sudden changes in the balance of certain conversational attributes—such as positive sentiment, politeness, or focus on future planning—signal impending betrayal. 
The development and proliferation of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is health surveillance, e.g., predicting the outbreak of an epidemic by recognizing diseases and symptoms from text messages posted on social media platforms. In this paper, we propose a novel task that is crucial and generic from the viewpoint of health surveillance: estimating a subject (carrier) of a disease or symptom mentioned in a Japanese tweet. By designing an annotation guideline for labeling the subject of a disease/symptom in a tweet, we perform annotations on an existing corpus for public surveillance. In addition, we present a supervised approach for predicting the subject of a disease/symptom. The results of our experiments demonstrate the impact of subject identiﬁcation on the effective detection of an episode of a disease/symptom. Moreover, the results suggest that our task is independent of the type of disease/symptom. 
In this paper, we model conversational roles in terms of distributions of turn level behaviors, including conversation acts and stylistic markers, as they occur over the whole interaction. This work presents a lightly supervised approach to inducing role deﬁnitions over sets of contributions within an extended interaction, where the supervision comes in the form of an outcome measure from the interaction. The identiﬁed role deﬁnitions enable a mapping from behavior proﬁles of each participant in an interaction to limited sized feature vectors that can be used effectively to predict the teamwork outcome. An empirical evaluation applied to two Massive Open Online Course (MOOCs) datasets demonstrates that this approach yields superior performance in learning representations for predicting the teamwork outcome over several baselines. 
Many existing deep learning models for natural language processing tasks focus on learning the compositionality of their inputs, which requires many expensive computations. We present a simple deep neural network that competes with and, in some cases, outperforms such models on sentiment analysis and factoid question answering tasks while taking only a fraction of the training time. While our model is syntactically-ignorant, we show signiﬁcant improvements over previous bag-of-words models by deepening our network and applying a novel variant of dropout. Moreover, our model performs better than syntactic models on datasets with high syntactic variance. We show that our model makes similar errors to syntactically-aware models, indicating that for the tasks we consider, nonlinearly transforming the input is more important than tailoring a network to incorporate word order and syntax. 
Traditional learning to rank methods learn ranking models from training data in a batch and ofﬂine learning mode, which suffers from some critical limitations, e.g., poor scalability as the model has to be retrained from scratch whenever new training data arrives. This is clearly nonscalable for many real applications in practice where training data often arrives sequentially and frequently. To overcome the limitations, this paper presents SOLAR — a new framework of Scalable Online Learning Algorithms for Ranking, to tackle the challenge of scalable learning to rank. Speciﬁcally, we propose two novel SOLAR algorithms and analyze their IR measure bounds theoretically. We conduct extensive empirical studies by comparing our SOLAR algorithms with conventional learning to rank algorithms on benchmark testbeds, in which promising results validate the efﬁcacy and scalability of the proposed novel SOLAR algorithms. 
In this paper, we consider the task of text categorization as a graph classiﬁcation problem. By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part – its main core – speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classiﬁcation datasets show statistically signiﬁcant higher accuracy and macro-averaged F1-score compared to baseline approaches. 
We present a novel, count-based approach to obtaining inter-lingual word representations based on inverted indexing of Wikipedia. We present experiments applying these representations to 17 datasets in document classiﬁcation, POS tagging, dependency parsing, and word alignment. Our approach has the advantage that it is simple, computationally efﬁcient and almost parameter-free, and, more importantly, it enables multi-source crosslingual learning. In 14/17 cases, we improve over using state-of-the-art bilingual embeddings. 
In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve signiﬁcantly higher translation quality over individually learned model in both situations on the data sets publicly available. 
This paper proposes an embedding matching approach to Chinese word segmentation, which generalizes the traditional sequence labeling framework and takes advantage of distributed representations. The training and prediction algorithms have linear-time complexity. Based on the proposed model, a greedy segmenter is developed and evaluated on benchmark corpora. Experiments show that our greedy segmenter achieves improved results over previous neural network-based word segmenters, and its performance is competitive with state-of-the-art methods, despite its simple feature set and the absence of external resources for training. 
 BME S  Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering. However, the previous neural models cannot extract the complicated feature compositions as the traditional methods with discrete features. In this paper, we propose a gated recursive neural network (GRNN) for Chinese word segmentation, which contains reset and update gates to incorporate the complicated combinations of the context characters. Since GRNN is relative deep, we also use a supervised layer-wise training method to avoid the problem of gradient diffusion. Experiments on the benchmark datasets show that our model outperforms the previous neural network models as well as the state-of-the-art methods. 
Tracking topics on social media streams is non-trivial as the number of topics mentioned grows without bound. This complexity is compounded when we want to track such topics against other fast moving streams. We go beyond traditional small scale topic tracking and consider a stream of topics against another document stream. We introduce two tracking approaches which are fully applicable to true streaming environments. When tracking 4.4 million topics against 52 million documents in constant time and space, we demonstrate that counter to expectations, simple single-pass clustering can outperform locality sensitive hashing for nearest neighbour search on streams. 
We propose a nonparametric Bayesian model for joint unsupervised word segmentation and part-of-speech tagging from raw strings. Extending a previous model for word segmentation, our model is called a Pitman-Yor Hidden SemiMarkov Model (PYHSMM) and considered as a method to build a class n-gram language model directly from strings, while integrating character and word level information. Experimental results on standard datasets on Japanese, Chinese and Thai revealed it outperforms previous results to yield the state-of-the-art accuracies. This model will also serve to analyze a structure of a language whose words are not identiﬁed a priori. 
In order to effectively utilize multiple datasets with heterogeneous annotations, this paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously, and to facilitate discussion we use Chinese part-ofspeech (POS) tagging as our case study. The key idea is to bundle two sets of POS tags together (e.g. “[NN, n]”), and build a conditional random ﬁeld (CRF) based tagging model in the enlarged space of bundled tags with the help of ambiguous labelings. To train our model on two non-overlapping datasets that each has only one-side tags, we transform a one-side tag into a set of bundled tags by considering all possible mappings at the missing side and derive an objective function based on ambiguous labelings. The key advantage of our coupled model is to provide us with the ﬂexibility of 1) incorporating joint features on the bundled tags to implicitly learn the loose mapping between heterogeneous annotations, and 2) exploring separate features on one-side tags to overcome the data sparseness problem of using only bundled tags. Experiments on benchmark datasets show that our coupled model signiﬁcantly outperforms the state-ofthe-art baselines on both one-side POS tagging and annotation conversion tasks. The codes and newly annotated data are released for non-commercial usage.1 ∗Correspondence author. 1http://hlt.suda.edu.cn/˜zhli  
We present AutoExtend, a system to learn embeddings for synsets and lexemes. It is ﬂexible in that it can take any word embeddings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efﬁciency and parallelizability. We use WordNet as a lexical resource, but AutoExtend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disambiguation tasks. 
This paper proposes a math operation (e.g., Summation, Addition, Subtraction, Multiplication, Division, etc.) oriented approach to explain how the answers are obtained for math word problems. Based on the reasoning chain given by the inference engine, we search each math operator involved. For each math operator, we generate one sentence. Since explaining math operation does not require complicated syntax, we adopt a specific template to generate the text for each kind of math operator. To the best of our knowledge, this is the first explanation generation that is specifically tailored to solving the math word problem. Keywords: Explanation Generation, Math Word Problem Explanation, Machine Reading 1. Introduction Since Big Data mainly aims to explore the correlation between surface features but not their underlying causality relationship (Mayer-Schönberger & Cukier, 2013), the “Big Mechanism” program1 has been proposed by DARPA to find out “why” behind the big data. However, the pre-requisite for it is that the machine can read each document and learn its associated knowledge, which is the task of Machine Reading (MR) (Strassel et al., 2010). Therefore, the Natural Language and Knowledge Processing Group (under the Institute of Information Science) of Academia Sinica formally launched a 3-year MR project (from January 2015) to attack this problem. Since a domain-independent MR system is difficult to build, the Math Word Problem (MWP) (Mukherjee & Garain, 2008) is chosen as our first test case to study MR. The main reason for that is that it not only adopts less complicated syntax but also requires less amount of domain knowledge; therefore, the researcher can focus more on text understanding and  Institute of Information Science , Academia Sinica 128 Academia Road, Section 2, Nankang, Taipei 11529, Taiwan E-mail: { joecth; lyc; kysu}@iis.sinica.edu.tw 
The large amount of text on the Internet cause people hard to understand the meaning in a short limit time. Topic models (e.g. LDA and PLSA) has been proposed to summarize the long text into several topic terms. In the recent years, the short text media such as tweet is very popular. However, directly applies the transitional topic model on the short text corpus usually gating non-coherent topics. Because there is no enough words to discover the word co-occurrence pattern in a short document. The Bi-term topic model (BTM) has been proposed to improve this problem. However, BTM just consider simple bi-term frequency which cause the generated topics are dominated by common words. In this paper, we solve the problem of the frequent bi-term in BTM. Thus, we proposed an improvement of word co-occurrence method to enhance the topic models. We apply the word co-occurrence information to the BTM. The experimental result that show our PMI-β-BTM gets well result in the both of regular short news title text and the noisy tweet text. Moreover, there are two advantages in our method. We do not need any external data and our proposed methods are based on the original topic model that we did not modify the model itself, thus our methods can easily apply to some other existing BTM based models. Keywords: Short Text, Topic Model, Document Clustering, Document Classification 1. Introduction With the advancement of information and communication technology, the information we obtained is very abundant and multivariate. Especially, in the recent 15 years, many type of the Internet media grow up so that people can get large amount of the information in a short time. These internet media include Wikipedia, blogs and the recently popular social medial  Department of Computer Science and Information Engineering, National Cheng Kung University E-mail: gbchen@ikmlab.csie.ncku.edu.tw; hykao@mail.ncku.edu.tw  46  Guan-Bin Chen & Hung-Yu Kao  such as Twitter, Facebook et.al. Generally, the articles/documents in the Wikipedia, and blogs are usually the long text and have the complete content. While the short text social media, such as Twitter, become very popular in the recent years. The reason is that these short text social media provide a very convenient way to share the people feeling and thinking. Generally, these Internet media deliver the people thinking by using the text. However, the large amount of text on the Internet cause people hard to understand the meaning in a short limit time. To solve the problem, many document summarization technologies have been proposed. Among them, topic models summarize the context in large amount of documents into several topic terms. By reading these topic terms, people will understand the content in a short time. Topic model can be performed by the vector space model or the probability model. In the recent years, the probability models such as Probabilistic Latent Semantic Analysis (pLSA) (Hofmann, 1999) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are very popular because the probability models base on the document generation process. The inspirations of the document generation process come from the human written articles. When a person writes an article, he or she will inspire some thinking in mind, then extend these thinking into some related words. Finally, they write down these words to complete an article. Probability topic models simulate the behavior of above document generating process. In the view of the vectorization of the probability topic models, when we have a text corpus, we have known the documents and its words distribution by statistic the word vector. Then, the probability topic models split the document-word matrix into the document-topic and topic-word matrices. The distribution of the document-topic matrix describes that the degree of each document belongs each topic while the topic-word matrix describes the degree of each word belongs each topic. The “topic” in these two matrices is the latent factor as the human thinking. In essence, the topic models capture the word co-occurrence information and these highly co-occurrence words are put together to compose a topic (Divya et al., 2013; Mimno et al., 2011). So, the key to find out high quality topics is that the corpus must contain a large amount of word co-occurrence information and the topic model has the ability to correctly capture the amount of the word co-occurrence. However, the traditional topic models work well in the long text corpus but work poorly in short text corpus. The reason is that the original intention of LDA is designed to model the long text corpus. Exactly, LDA capture the word co-occurrence in document-level (Divya et al., 2013; Yan et al., 2013), but there are no enough words to well judge the word co-occurrence in document-level in a short text document. Figure 1 is an example which shows the difference of the topic model in between the long text and short text corpus. In the long text corpus, each document provides a lot of word co-occurrence information, so that LDA can well capture these information to discover the high quality topics. While in the short text document, there are no enough words in a  Word Co-occurrence Augmented Topic Model in Short Text  47  single document to discover the word co-occurrence information.  General Documents Twitch Plays Pokémon is a social exWpWiethirtihtmhteheenadtavdavanancndecmemecnehtnatonfonfineiflnofroomrnmataiottinhoneanadvndicdocemoommmunuinciactaiotinontetcehcnhonloolgoyg,y, hesatirncetfhraoeormmwiianndtfigsooornmwuwaretceiboeonsdbittaweainetTetedwomibsitptamctinhue,tcdohcaoipbsnulasnmiydsuatcniGhnt aganambdouefmndualtnivt arainadte. mEuslpteivcaiarilalyte. nFtrheeEasrkpee'csceinaatlnlyydeianrNst,hienmtraeenncyedntoty'ypseeasPrsoo,fkmtéhamenyoInntyteprvensiedtoefmotehdeiaIngterronwets mupedisao gtrhoawt s eguosapelmuerpsceassntohgbtrehoytaultagprpgheaeorpastmlhieneogcuancnthcogaofenmtthnmleearilang'snfeodrcamsmhaaotstuioennnrt otionof mabthys.heoirnt ftoimrdmeia.sttiroiTbnouiptniiocan short Thteimceo.ncept was developed by ...  ...  Topics stream 0.15 channel 0.45 video 0.40  social  0.12  experiment 0.15  crowdsource 0.53  ...  twitch 0.12 game 0.35 video 0.53  Short Text Documents (Tweets)  David @GuysWithPride This is an apple. HAAA  Topic distribution  ...  No enough words  Topics apple 0.45 banana 0.25 fruit 0.15 food 0.13 chicken 0.36 ... ... haaa 0.12 hi 0.35 noooo 0.53  Figure 1. An example of LDA in the long text and short text corpus To overcome above problems in short text, many researchers consider a simpler topic model, mixture of unigrams model. Mixture of unigrams model samples topics in global corpus level (Nigam et al., 2000; Zhao et al., 2011). More specifically, the word co-occurrence in document-level means that the amount of the word co-occurrence relation comes from a single document. On the contrary, the word co-occurrence in corpus-level means that the amount of the word co-occurrence relation comes from a full corpus which contains many documents. Mixture of unigrams overcomes the lack of words in the short text documents. Further, Xiaohui Yan et al. proposed the Bi-term Topic Model (BTM) (Yan et al.,  48  Guan-Bin Chen & Hung-Yu Kao  2013; Cheng et al., 2014) which directly model the word co-occurrence and use the corpus-level bi-term to overcome the lack of the text information problem. A bi-term is an unordered word pair co-occurring in a short text document. The major advantage of BTM is that 1) BTM model the word co-occurrence by using the explicit bi-term, and 2) BTM aggregate these word co-occurrence patterns in the corpus for topic discovering (Yan et al., 2013; Cheng et al., 2014). BTM abandons the document-level directly. A topic in BTM contains several bi-term and a bi-term crosses many documents. BTM emphasizes that the co-occurrence information comes from all bi-terms in whole corpus. However, BTM will make the common words be performed excessively because the frequency of bi-term comes from the whole corpus instead of a short document.        wi  wwij   w.j..  ...  wi  w.j..     ...   Figure 2. The graphical representation of the PMI-β-BTM In this paper, we solve the frequent bi-term problem in BTM. We propose an approach base on BTM. For the problem in BTM, a simple and intuitive solution is to use pointwise mutual information (PMI) (Church & Hanks, 1990) to decrease the statistical amount of the frequent words in whole corpus. With respect to the frequency of bi-term, the PMI can normalize the score by each single word frequency in the bi-term. Otherwise, the priors in the topic models usually set symmetric. This symmetric priors mean that there is not any preference of words in any specific topic (Wallach et al., 2009). An intuitive idea is that why not adopt some word co-occurrence information in priors to restrict the generated topics. Base on above two ideas, we propose a novel prior adjustment method, PMI-β priors, which first use the PMI to mine the word co-occurrence from the whole corpus. Then, we transform such PMI scores to the priors of BTM. Figure 2 shows the graphical representation of the PMI-β-BTM. In summary, the proposed approach enhance the amount of the word co-occurrence and  Word Co-occurrence Augmented Topic Model in Short Text  49  also based on the original topic model. Basing on the original topic model means we did not modify the model itself, thus our methods can easily apply to some other existing BTM based models, to overcome the short text problem without any modification. To test the performance of our two methods completely, we prepare two different types of short text corpus for the experiments. One is the tweet text and another is the news title. The context of news title dataset is regular and formal while the text in tweet usually contain many noise. Experimental results show our PMI-β priors method is better than the BTM in both tweet and news title datasets. The remaining of this paper shows below. In Section 2, we show the survey of some traditional topic models and the previous works of topic model to overcome the short text. Section 3 shows our proposed PMI-β priors and the re-organized document methods. The experiment results show in Section 4. Finally, we conclude this research in Section 5. 2. Related Work 2.1 The Survey of the Traditional Topic Models for Normal Text Topic Model is a method to find out the hidden semantic topics from the observed documents in the text corpus. Topic Models have been researched several years. Generally, topic model can be performed by the vector space model or the probability model. The early one of the vector space topic model, Latent Semantic Analysis (LSA) (Landauer et al., 1998), uses the singular value decomposition (SVD) to find out the latent topic. However, LSA does not model the polysemy well and the cost of SVD is very high (Hofmann, 1999; Blei et al., 2003). Afterward, Thomas Hofmann proposed the one-document-multi-topics model, probabilistic Latent Semantic Analysis (pLSA) (Hofmann, 1999). pLSA bases on the document generation process which like the human writing. However, the numerous parameters of pLSA cause the overfitting problem and pLSA does not define the generation of the unknown documents. In 2003, Blei et al. proposed a well-known Latent Dirichlet Allocation (LDA) (Blei et al., 2003), LDA use the prior probability in Bayes theory to extents pLSA and simplify the parameters estimate process in pLSA. Also, the non-zero priors let LDA have the ability to infer the new documents. However, there are some drawbacks in LDA. First, LDA works under the bag-of-word model hypothesis. In the bag-of-word model, each word of the document is no order and independent of others (Wallach, 2006). The hypothesis compared with the human writing behavior is unreasonable (Divya et al., 2013). Second, LDA emphasizes the relations between topics are week, but actually, the topics may have hierarchical structure. Third, LDA requires the large number of articles and well-structured long articles to get the high quality topics. Apply LDA on the short text or uncompleted sentences corpus usually get poor results. The  50  Guan-Bin Chen & Hung-Yu Kao  fourth drawback is that in spite of the LDA has the concept of the prior probabilities but LDA priors generally set the symmetric values in each prior vector, like <0.1> or <0.01>. The symmetric prior means no bias of each words in the specific topic (Wallach et al., 2009). In this situation, the priors only provide the smooth technology to avoid the zero probability and the model only use the statistical information from the data to discover the hidden topics. To overcome above four drawbacks, many researchers propose new modify models. Such as N-gram Topic Model (Wang et al., 2007) and HMM-LDA (Griffiths et al., 2004) provide the context modeling. Wei Li et al. proposed the Pachinko Allocation Model (PAM) (Li & McCallum, 2006) which adds the super topic concept and make the topic have the hierarchical structure. Otherwise, Zhiyuan Chen et al. apply the must-link and cannot-link information to guide the document generation process which words must or not to be put into a topic (Chen & Liu, 2014). 2.2 Topic Models for Short Text With the rise of social media in recent years, topic models have been utilized for social media analysis. For example, some researches apply topic models in social media for event tracking (Lin et al., 2010), content characterizing (Zhao et al., 2011; Ramage et al., 2010), and content recommendation (Chen et al., 2010; Phelan et al., 2009). However, to share people thinking conveniently, the context is usually short. These short text contexts make topic models hard to discover the amount of word co-occurrence. For the short text corpus, there are three directions to overcome the insufficient of the word co-occurrence problem. One is using the external resources to guide the model generation, another is aggregating several short texts into a long text, and the other is improving the model to satisfy the short text properties. For the first direction, Phan et al. (Phan et al., 2008) proposed a framework that adopt the large external resources (such as Wiki and blog) to deal with the data sparsity problem. R.Z. Michal et al. proposed an author topic model (Rosen-Zvi et al., 2004) which adopt the user information and make the model suitable for specific users. Jin et al. proposed the Dual-LDA model (Jin et al., 2011), it use not only the short text corpus but also the related long text corpus to generate topics, respectively. The generation process use the long text to help the short text modeling. If the quality of the external long text or knowledge base is high, the generated topic quality will be improve. However, we cannot always obtain the related long text to guide short text and the related long text is very domain specific. So, using external resources is not suitable for the general short text dataset. In addition to adopt the long text, Hong et al. aggregate the tweets which shared the same words and get better results than the original tweet text (Hong & Davison, 2010). For the model improvement, Wayne et al. use the mixture of unigrams model to model the tweets topics from whole corpus text (Zhao et al., 2011). Their experimental results verify  Word Co-occurrence Augmented Topic Model in Short Text  51  that the mixture of unigram model can discover more coherent topics than LDA in the short text corpus. Further, Xiaohui Yan et al. proposed the Bi-term Topic Model (BTM) (Yan et al., 2013; Cheng et al., 2014) which directly model the word co-occurrence and use the corpus level bi-term to overcome the lack of the text information problem. A bi-term is a word pair containing a co-occur relation in this two words. The advantage is that BTM can model the general text without any domain specific external data. Comparing with the mixture of unigram, BTM is a special case of the mixture of unigram. They both model the corpus level topic but BTM generates two words (bi-term) every time the generation process. However, BTM discovers the word co-occurrence just by considering the bi-term frequency. The bi-term frequency will be failed to judge the word co-occurrence when the bi-term frequency is high but one of the frequency of two words in a bi-term is high and another is low. 3. The Word Co-occurrence Augmented Methods Topic models learn topics base on the amount of the word co-occurrence in the documents. The word co-occurrence is a degree which describes how often the two words appear together. BTM, discovers topics from bi-terms in the whole corpus to overcome the lack of local word co-occurrence information. However, BTM will make the common words be performed excessively because BTM identifies the word co-occurrence information by the bi-term frequency in corpus-level. Thus, we propose a PMI-β priors methods on BTM. Our PMI-β priors method can adjust the co-occurrence score to prevent the common words problem. Next, we will describe the detail of our method of PMI-β priors. We first describe the detail of BTM. First, we introduce the notation of “bi-term”. Bi-term is the word pair co-occurring in the short text. Any two distinct words in a document construct a bi-term. For example, a document with three terms will generate three bi-term (Yan et al., 2013):  t1,t2,t3   t1,t2 , t2,t3 , t1,t3 .  (1)  Note that each bi-term is unordered. For a real case example, we have a document and the context is “I visit apple store”. Because “I” is a stop-word, we remove it. The remaining three terms “visit”, “apple” and “store” will generate three bi-terms “visit apple”, “apple store”, and “visit store”. We generate all possible bi-terms for each document and put all bi-terms in the bi-term set B. Second, we describe the parameter estimation of the BTM. The aim of the parameter estimation of BTM is to estimate the topic assignment z, the corpus-topic posteriori distribution  and the topic-word posteriori distribution . But the Gibbs sampling can integrate  and  due to use the conjugate priors. Thus, the only one parameter z should be  52  Guan-Bin Chen & Hung-Yu Kao  estimate. Clearly, we should assign a suitable topic for each bi-term. The Gibbs sampling equation shows below:  P(z  k | zb,B,α,β)    ,  (2)  where z is the topic assignment, k means the kth topic, B is the bi-term set,  is the corpus-topic prior distribution and β is the topic-word prior distribution. The  and  in Eq. (2) show following:    (nk ,b  k ) K  ,  (3)   (nk ,b  k )  k 1  (nw1   w1 )  (nw2   w2 )      k ,b  V   (n  wt    k   wt  )    V   k ,b (nwt  k  ,    wt )  (4)  t1 k ,b  k  t1 k ,b  k  where V is the number of unique words in the corpus, nk,-b is the statistical count for the document-topic distribution, and nkw,tb is the statistical count for the document-topic distribution. When the frequency of bi-term is high the two terms in this bi-term tend to be put into the same topic. Otherwise, to overcome the lack of words in a single document BTM abandons the document-level directly. A topic in BTM contains several bi-term and a bi-term crosses many documents. BTM emphasizes that the co-occurrence information comes from all bi-terms in whole corpus.  However, just consider the frequency of bi-term in corpus-level will generate the topics which contain too many common words. To solve this problem, we consider the Pointwise Mutual Information (PMI) (Church & Hanks, 1990). Since the PMI score not only considers the co-occurrence frequency of the two words, but also normalizes by the single word frequency. Thus, we want to apply PMI score in the original BTM. A suitable way to apply PMI scores is modifying the priors in the BTM. The reason is that the priors modifying will not increase the complexity in the generation model and very intuitive. Clearly, there are two kinds of priors in BTM which are β-prior and β-priors. The β-prior is a corpus-topic bias without the data. While the β-priors are topic-word biases without the data. Applying the PMI score to the β-priors is the only one choice because we can adjust the degree of the word co-occurrence by modifying the distributions in the β-priors. For example, we assume that a topic contains three words “pen”, “apple” and “banana”. In the symmetric priors, we set <0.1, 0.1, 0.1> which means no bias of these three words, while we can apply <0.1, 0.5, 0.5> to enhance the word co-occurrence of “apple” and “banana”. Thus the topic will prefer to put the “apple” and “banana” together in the topic sampling step.  Word Co-occurrence Augmented Topic Model in Short Text  53  Short text data  Preprocess Remove stop-word Stemming  Mining Priors Knowledge Pointwise Mutual Information  News Title  Word Vectors  Bi-term Topic Model priors priors Gibbs Sampling  Topic Model  Figure 3. The PMI-β priors approach  Figure 3 shows our PMI-β-priors approach. After pre-procession, we first calculate the  PMI score of each bi-term <wx, wy> as  PMI(wx , wy )    log  p(wx,wy ) p(wx ) p(wy )  ,  (5)  Because the priors can view as an additional statistics count of the target probability, the value ordinarily should be greater than or equal to zero. Thus, we adjust the value of NPMI to [0, 2] by adding one as:  NPMI(wx , wy )    PMI(wx , wy )  log p(wx , wy )  1.  (6)  After getting the NPMI scores, we transform these scores to meet the β-priors. Let βSYM is the original symmetric β-priors and the PMI β-priors, denote βPMI, define as    wx ,w PMI  y    SYM   0.1 NPMI(wx , wy ) .  (7)  There is a constant value 0.1 in Eq. (7). This constant value 0.1 prevent the target probability being dominated by the priors. The partial of the word co-occurrence information should still be captured by the original model and the priors provide the additional information to enhance the word co-occurrence in the model. The following shows how we apply PMI-β -priors into the BTM. We apply the βPMI of w1 and w2 in Eq. (6) and the new equation of 　 shows below:  54  Guan-Bin Chen & Hung-Yu Kao  (nw1   w1,w2 ) (nw2   w1,w2 )      k ,b  V   (n  wt  PMI   wt  )    k ,b  V   (n  wt  PMI  .    wt )  (8)  t1 k ,b  k  t1 k ,b  k  Finally, we sample topic assignments by Gibbs sampling (Liu, 1994) approach.  4. Experiments  How to justly evaluate the quality of the topic model is still a problem. The reason is that the topic model is an unsupervised method. There are no prominent words or labels can directly assign to each topic. Thus, many researchers apply topic model in other applications, such as clustering, classification and information retrieval (Blei et al., 2003; Yan et al., 2013). In classification task, instead of using the original word vectors to identify the document categories, it use the reduced vectors which generating from the topic model. The topic model plays as a dimensional reduction role and the classification result shows how well the model to represent the original features. Topic model can also look as the document clustering approach by just considering a document assign to which topic(s). In this paper, we evaluate topic models by clustering and classification tasks. Otherwise, to make our experiment more robust, we adopt two different types of short text dataset - Twitter2011 and ETtoday Chinese news title. The properties of these two corpus are different. The text of ETtoday Chinese news title is very regular, while the text of Twitter2011 usually contains emotional words, simplified texts and some unformed words. For example, “haha” is the emotional word, and “agreeeee” is the unformed word.  Table 1 shows the statistics of short text datasets. The number of average words per document is not more than ten words. The number of documents in each class are shown in Figure 4. The property of both two dataset is skew. The skew dataset may cause the results that the fewer documents are dominated by the larger one. In summary, the challenges of these two datasets are not only the short text problem but also the unbalance category. The top-3 classes in the Twitter2011 dataset are “#jan25”, “#superbowl” and “#sotu”. And the top-3 classes in the ETtoday News Title dataset are “entertainment”, “physical” and “political”.  Table 1. The Statistics of Two Short Text Datasets  Property  Twitter2011  ETtoday News title  The number of documents  49,461  17,814  The number of domains  50  25  The number of distinct words  30,421  31,217  Avg. words per document  5.92  9.25  Word Co-occurrence Augmented Topic Model in Short Text  55  #Docs  #Docs  7000 6000 5000 4000 3000 2000 1000 0 1 5 9 13 17 21 25 29 33 37 41 45 49 Class ID for Twitter2011 Dataset 3000 2500 2000 1500 1000 500 0 1 5 9 13 17 21 25 Class ID for ETtoday Dataset Figure 4. The number of documents in each class 4.1 Experimental Setup All of the experiments were done on the Intel i7 3.4 GHz CPU and 16G memory PC. All of the pre-process and topic models were written by JAVA code. The parameters  priors and the base β priors of topic models are all set <0.1>. The number of iterations in Gibbs sampling is set 1,000. To make our results more reliable, we run each experiments 10 times and average these scores. For the clustering experiment, we first get the document-topic posteriori probability distribution  and we use the highest probability topic P(z|d) as the cluster assignment for each document in . For the classification experiment, we divide our dataset into five parts in which four parts for training and one for testing. After training the topic model, we fix the topic-word distribution and then we re-infer document-topic posteriori probability  56  Guan-Bin Chen & Hung-Yu Kao  distribution  of all original short text documents. Instead of using the original word vectors to do the classification task, we take this re-inferred posteriori probability distribution  as the reduced feature matrix. Finally we use this reduced feature matrix to classify the documents by LIBLINEAR1. We compare our methods with the previous topic models: 1) LDA, 2) Mixture of unigrams, and 3) BTM. In addition to the above three topic models, we also compare with our PCA-β priors methods. We use the principal component analysis (PCA) to discover the whole corpus principal component. Then, we transform the principal component to the topic-word prior distribution.  4.2 Evaluation Criteria In this part, we list three criteria for the clustering experiment and one for classification. In the clustering experiment, let  = {1, 2, ... , K} is the output cluster labels, and C = {c1, c2, ... , cp} is the gold standard labels of the documents. We first describe the three criteria for the clustering.  　Purity Purity is a simple and transparent measure which perform the accuracy of all cluster assignments as the following equation:  Purity(,C)    max kj  k N  c  j  ,  (9)  where N is the total number of documents. Note that the high purity is easy to achieve when the number of clusters is large. In particular, purity is 1 if each document gets its own cluster.  　Normalized Mutual Information (NMI)  NMI score is based on the information theory. Let I(, C) denotes the mutual information between the output cluster  and the gold standard cluster C. The mutual information of NMI is normalized by each entropy denoted H( and H(C). This normalization can avoid the influence of the number of clusters. The equation of NMI shows following:  NMI(, C)    I(, C) [H()  H(C)]  2  ,  (10)  where C,and H denote:  
The rapidly increasing availability of multimedia associated with spoken documents on the Internet has prompted automatic spoken document summarization to be an important research subject. Thus far, the majority of existing work has focused on extractive spoken document summarization, which selects salient sentences from an original spoken document according to a target summarization ratio and concatenates them to form a summary concisely, in order to convey the most important theme of the document. On the other hand, there has been a surge of interest in developing representation learning techniques for a wide variety of natural language processing (NLP)-related tasks. However, to our knowledge, they are largely unexplored in the context of extractive spoken document summarization. With the above background, this study explores a novel use of both word and sentence representation techniques for extractive spoken document summarization. In addition, three variants of sentence ranking models building on top of such representation techniques are proposed. Furthermore, extra information cues like the prosodic features extracted from spoken documents, apart from the lexical features, are also employed for boosting the summarization performance. A series of experiments conducted on the MATBN broadcast news corpus indeed reveal the performance merits of our proposed summarization methods in relation to several state-of-the-art baselines. Keywords: Spoken Document, Extractive Summarization, Word Representation, Sentence Representation, Prosodic Feature 1. 緒論 巨量資料充斥著現今的世界，在全球資訊網(World Wide Web)中已存在有數十億篇網頁， 並且以指數的倍數持續成長著。為此，人們必須仰賴及時摘要各類資訊的自動化工具， 以 減 緩 資 訊 過 載 (Information Overload) 的 問 題 。 這 些 迫 切 的 需 求 促 使 了 自 動 摘 要 (Automatic Summarization)技術的蓬勃發展(Luhn, 1958)。自動摘要技術可概分為節錄式 (Extractive)摘要以及抽象式(Abstractive)摘要。前者主要是依據特定的摘要比例，從原始 的文件中選取重要的語句子集(Sentence Subset)，透過該語句子集簡潔地表示原始文件的 大致內容；而後者是在完全理解文件內容之後，重新撰寫產生摘要來代表原始文件的內 容。雖然抽象式摘要是最為貼近人們日常撰寫摘要的形式，但其涉及深層的自然語言處  節錄式語音文件摘要使用表示法學習技術  67  理能力(Mitra et al., 1997)，較為困難許多；目前大多數的研究主要集中在節錄式摘要的 自動產生(Jones, 1999)。除了傳統的文字文件外，多媒體文件亦迅速地在世界各地傳播， 例如語音郵件、會議錄音、電視新聞以及課程演講等；因此，語音文件摘要(Spoken Document Summarization)自然地成為近年來的一項受關注的研究議題。本論文主要探討 節錄式語音文件摘要，其目標是基於定量的摘要比例(Summarization Ratio)，從多媒體內 容所對應的語音文件中選取能夠表示其內容主題資訊之語句，讓使用者可以迅速地理解 多媒體內容的主要意涵。 本論文延續過去學者的經驗、成果以及貢獻，針對語音文件的特性及困難點，進一 步地深入研究語音文件摘要方法與特徵使用，希望藉由自動語音文件摘要技術的改進， 使自動摘要的結果能更適切地詮釋語音文件內容及主題。本論文提出兩個主要的研究貢 獻：(1)基於表示法學習(Representation Learning)技術，本論文提出三種排序模型(即統計 式模型、機率式模型以及圖論式模型)，嘗試將表示法學習技術運用於語音文件摘要任務 之中。(2)除了利用文件中的文字資訊外，本論文進一步地結合語音文件中的各種韻律特 徵，期望增進摘要系統的成效。 本論文的後續安排如下：第二節首先簡介當前主要的文件摘要方法。第三節介紹本 論文所探究的表示法學習技術。第四節介紹本論文提出的三種排序模型於表示法學習之 技術。第五節簡介語音文件的多種特徵。第六節介紹實驗語料及摘要評估之方法。第七 節說明實驗結果及其分析。第八節為本論文之結論與未來展望。 2. 基礎文件摘要方法之簡介 2.1 以文件結構為基礎之摘要方法 摘要單位元(例如詞彙或語句)在文件中的位置資訊(Positional Information)可以作為文件 尋找重要語句選取的判斷依據。一般而言，位於段落開頭的摘要單位元通常有較高的重 要性及代表性。因此，前導(LEAD)摘要方法是根據摘要比例選取文件前 M %的部分作為 摘要(Hajime & Manabu, 2000)。這種摘要的方式簡單且直覺，但僅適用於特定的結構內 容，並且文件需遵循某種編排方式。對於某些沒有特定結構編排的文件，或是缺乏文件 結構的語音文件內容有可能會不適用。 2.2 以統計值為基礎之摘要方法 A. 向量空間模型(Vector Space Model, VSM) 向 量 空 間 模 型 已 被 廣 泛 地 應 用 於 資 訊 檢 索 中 ， 用 以 估 測 使 用 者 查 詢 (Query) 與 文 件 (Document)之間的相關程度(Salton & Lesk, 1968)。節錄式語音文件摘要任務可以被視為 是一個資訊檢索的問題，重要語句的選取是以句子與語音文件內容的相關程度而定 ；亦 即將文件內容視為查詢來檢索 最相關的 m 個語句。因此，可以將文件表示成向量 D ，文 件中的每一語句表示為向量 St ，透過餘弦相似度(Cosine Similarity)計算，就可估測兩者  68  施凱文 等  之間的相似性程度。雖然直接地利用文件中的文字資訊已被證明在許多自然語言處理的 相關問題中可以獲得一定的成效，但這樣的方法忽略了隱藏於文字中的語意資訊 (Semantic Information)。為了有效地運用這些語意資訊，最早於 2001 年開始有學者提出 潛藏語意的概念應用於文件摘要的方法(Gong & Liu, 2001)。  B. 最大邊際關聯法(Maximum Margin Relevance, MMR) 最大邊際關聯法是於 1998 年被提出的自動摘要準則(Carbonell & Goldstein, 1998)。該準 則是以遞迴的方式一次一句地挑選可能的摘要語句，其考量的重點不僅是希望所選取出 來的摘要語句與文件的關聯性分數要高，更進一步地考慮了候選語句與已被選取的摘要 語句之間的重複性分數要低。如此一來，摘要系統選取出的語句不僅可以代表文件中的 重要主題，更可以充分的涵蓋文件中的各個面向。直到今日，最大邊際關聯法仍為節錄 式摘要方法常使用的重要準則；它也常是一個主要的基礎系統，做為新摘要方法發展時 效能比較與評估的依據。  2.3 以機率式模型為基礎之摘要方法  A. 單連語言模型(Unigram Language Model, ULM)  語言模型被廣泛地應用於語音辨識(Speech Recognition)與機器翻譯(Machine Translation) 等方面，學者 Ponte 等人在 1998 年時將其運用於資訊檢索的問題中(Ponte & Croft, 1998)。 由於我們可以將節錄式語音文件摘要任務視為一個資訊檢索的問題，即當給予一篇文件 D 時，希望對文件中的語句 S 依照機率值 P(S|D)進行排序；藉由貝式定理的推導，可得 (Chen et al., 2009)：  |  |  ∝|  (1)  其中 P(D)對於每一語句皆相同，故可忽略。而我們假設每一語句 S 的事前機率 P(S)為一 個均勻分佈(Uniform Distribution)，因此 P(S)亦可忽略。值得一提的是，由於文件中的語 句通常較為簡短，不容易建立一個準確的模型來完整地描述每一語句的內容涵意。為此， 有 研 究 學 者 陸 續 提 出 各 式 較 為 強 健 性 的 語 言 模 型 ， 例 如 關 聯 模 型 (Relevance Model)(Lavrenko & Croft, 2001)等，期望可以改善此一問題。關聯模型的優點在於融入關 聯文件中的資訊，藉此豐富語句模型使得更準確地表達語句的主題特性，以提升摘要的 成效。  B. Okapi Best Match 25 (BM25) Okapi BM25 是於 1994 年由學者 Robertson 等人所提出的權重計算公式，是現今資訊檢 索模型中最著名的機率式檢索模型之一。其權重計算方式主要是將詞頻對文件長度作正 規化，有效降低因文件長度不同而產生的檢索誤差(Robertson & Jones, 1976; Robertson &  節錄式語音文件摘要使用表示法學習技術  69  Walker, 1994; Robertson et al., 1996)。當利用該方法於文件摘要任務時，我們首先對文件  的詞序列  … | | 計算出每個詞 與語句 S 之間的相似性分數，接著將每個詞  對於語句 S 的相似性分數進行加權求和，進而得到文件 D 與語句 S 的相似性分數，公式  如下：  25 ,  ,  ,  
The performance of an automatic speech recognition (ASR) system often deteriorates sharply due to the interference from varying environmental noise. As such, the development of effective and efficient robustness techniques has long been a challenging research subject in the ASR community. In this article, we attempt to obtain noise-robust speech features through modulation spectrum processing of the original speech features. To this end, we explore the use of nonnegative matrix factorization (NMF) and its extensions on the magnitude modulation spectra of speech features so as to distill the most important and noise-resistant information cues that can benefit the ASR performance. The main contributions include three aspects: 1) we leverage the notion of sparseness to obtain more localized and parts-based representations of the magnitude modulation spectra with fewer basis vectors; 2) the prior knowledge of the similarities among training utterances is taken into account as an additional constraint during the NMF derivation; and 3) the resulting encoding vectors of NMF are further normalized so as to further enhance their robustness of representation. A series of experiments conducted on the Aurora-2 benchmark task demonstrate that our methods can deliver remarkable improvements over the baseline NMF method and achieve performance on par with or better than several widely-used robustness methods. Keywords: Speech Recognition, Language Model, Concept Information, Model Adaptation 1. 研究動機 大多數的自動語音辨識系統，在不受雜訊干擾的理想實驗室環境下，皆可獲得良好的辨 識效果；但是在真實的日常環境中，往往因為環境中諸多複雜因素的影響，造成訓練環 境與測試環境存在不匹配問題，使得此系統之辨識精確率大幅度降低。造成環境不匹配 問題的因素有語者變異、加成性背景雜訊、摺積性通道雜訊及其他語者發音的干擾等。 本研究探討語音辨識之強健性技術，希望降低上述因素所帶來的負面影響，進而使語音 辨識系統在實際應用時仍能保有一定的效能表現。 當前所發展出的各種語音強健技術大致可分為三種類型(Lin et al., 2009; Chu et al., 2011)：第一種類型為以聲學模型(Acoustic Model)為基礎之強健性技術(Model-Based Techniques)，此類方法大多是期望透過少量在測試環境所錄製的調適語料來對聲學模型  調變頻譜分解技術於強健語音辨識之研究  89  進行調整，使聲學模型可以近似於輸入含雜訊語音的機率分布參數，達到降低環境不匹 配 所 造 成 影 響 的 目 的 。 第 二 類 是 以 語 音 特 徵 為 基 礎 之 強 健 性 技 術 (Feature-Based Techniques)。此類方法期望經過適當的正規化處理後，能使含雜訊語音與其原始乾淨語 音趨於一致。最後第三類型為綜合式強健性技術，即同時在特徵處理和模型訓練兩階段 做改善。 本論文將探討以語音特徵為基礎之強健性技術。其研究的議題主要圍繞在對何種空 間正規化？以及在該空間應如何正規化？典型方法是將時間序列域(Temporal Domain)上 的語音特徵視為是隨機變數(Random Variable)的樣本(Samples)，利用觀測到樣本去估測 隨機變數之統計特性，進而對語音特徵時間序列做線性或非線性的轉換，使其在部分或 整 體 之 統 計 特 性 能 經 過 正 規 化 的 處 理 。 常 見 的 方 法 有 統 計 圖 等 化 法 (Histogram Equalization, HEQ)(Torre et al., 2005)、倒頻譜平均值減去法(Cepstral Mean Subtraction, CMS)(Furui, 1981)以及倒頻譜平均數與變異數正規化法(Cepstral Mean and Variance Normalization, CMVN)(Vikki & Laurila, 1998)。上述方法所利用的統計資訊仍有所不足， 無法觀察出明確的時序結構(Temporal Structure)改變。特徵參數時間序列之調變頻譜 (Modulation Spectrum)為一有效描繪時空結構之媒介，相對於時間序列域之語音特徵正規 化法的觀念而言，可能具有更廣泛的分析面向。例如，人們發出的聲音大多集中在調變 頻譜的低頻處，因為發聲器官限制了語速。加成性的噪音則可能會反應在每個頻率，那 些在高頻或與人聲不同的頻帶的資訊就可以被區分出來。近年來在調變頻譜域的語音強 健性研究相當熱門，學者們致力於正規化特徵參數之時空結構，藉由強化語音特徵之調 變頻譜來提升語音特徵的強健性。相關的技術包括了調變頻譜統計圖等化法(Spectral Histogram Equalization, SHE)(Sun et al., 2007)、分頻式調變頻譜統計正規化法(Sub-Band Modulation Spectrum Compensation)(Huang et al., 2009) 與 其 它 一 系 列 資 料 導 向 (Data-Driven)之時間序列濾波器法(Xiao et al., 2008; Hermansky & Morgan, 1994)等。 本論文旨在探究使用非負矩陣分解(Nonnegative Matrix Factorization, NMF)以及一些 改進方法來正規化調變頻譜強度成分，以獲得較具強健性的語音特徵。首先，結合稀疏 性的概念，期望能夠求取到具調變頻譜局部性的資訊以及重疊較少的 NMF 基底向量表 示。其次，基於局部不變性的概念，希望發音內容相似的語句之調變頻譜強度成分，在 NMF 空間有越相近的向量表示以維持語句間的關聯程度。再者，在測試階段經由正規化 NMF 之編碼向量，更進一步提升語音特徵之強健性。最後，我們也結合上述三種 NMF 的改進方法。此外，也嘗試將我們所提出的改進方法與一些現有的特徵強健技術做比較 和結合，以驗證這些改進方法之實用性。 2. 調變頻譜正規化法 2.1 調變頻譜之簡介 對於任一特定維度語音頻譜特徵所成的時間序列 x[n]而言，其調變頻譜定義如下：  90  張庭豪 等  ，0  2  (1)  其中，n 與 k 依序為音框索引與調變頻率索引，DFT 為離散傅立葉轉換(Discrete Fourier Transform, DFT)，X[k]代表語音特徵時間序列 x[n]的調變頻譜。由式(1)可看出調變頻譜 可以被用來廣泛地分析語句中語音特徵隨時間變化的資訊。而 X[k]頻譜序列可視為一種 對於原始語音訊號作降低取樣(Down-Sampling)後的調變訊號(由訊號取樣率轉至音框取 樣率)，此序列即為所屬語音特徵時間序列之調變頻譜(Modulation Spectrum)。由式(1)可 知，調變頻譜 X[k]之最高頻率與特徵序列 x[n]之取樣頻率(音框取樣率)有關。例如，在一 般設定下，若音框取樣率為 100 Hz，則最高調變頻率為 50 Hz。 過去已有不少學者研究語音特徵之調變頻譜的特性，發現了調變頻譜中的低頻成分 是比高頻成分還要重要的特性(Kanedera et al., 1997)。同時，調變頻譜之低頻成分(約 1Hz 至 16Hz)對於語音辨識正確率也有密切的關係，潛藏有重要的語意資訊。其中，最重要 的是位於 4 Hz 附近，有學者指出，4 Hz 是人耳聽覺最為敏感之調變頻率(Hermansky, 1998)； 另有學者也認為，4 Hz 為人類大腦皮層感知之重要調變頻率(Greenberg, 1997)。當語音訊 號受到雜訊影響時，其語音特徵時間序列會受到影響而失真，及其調變頻譜也會跟著受 到牽連。很多學者提出作用在調變頻譜的正規化法，以改善調變頻譜受到雜訊干擾的影 響。因此，我們可將許多發展在語音特徵時間序列的正規化法應用在調變頻譜使其正規 化；而正規化的對象是對其調變頻譜強度(Magnitude)成分|X[k]|來進行處理，並保持其相 位角不變θ[k]=∠X[k]的部分。接著，經處理後被更新的強度成分會與原始相位成分結合， 再藉由反傅立葉轉換(Inverse Discrete Fourier Transform, IDFT)來求得新的語音特徵時間 序列。若調變頻譜的強度能夠被有效的正規化，便能夠有效解決雜訊產生的環境不匹配 問題，使自動語音辨識系統在使用新的語音特徵的情況下能夠獲得較佳的辨識率。以下 將會簡單回顧一些常見的調變頻譜正規化法。  2.2 調變頻譜平均正規化法(Spectral Mean Normalization, SMN)  假設當各種音素在一般環境中分布的比例接近一致時，每一維度語音特徵的調變頻譜之 平均值應該為一個定值(Huang et al., 2009)：  ||  (2)  在式(2)中，| |為原始的調變頻譜強度成分， 為單一語句的調變頻譜強度成分之平  均值， 為所有訓練語句的調變頻譜強度成分之平均值，而  便是更新過後的調變頻  譜強度成分。  2.3 調 變 頻 譜 平 均 與 變 異 數 正 規 化 法 (Spectral Mean and Variance Normalization, SMVN) 除了要正規化調變頻譜強度成分之平均值外，也可同時正規化其標準差(Huang et al., 2009)。假設特徵向量參數之平均值與變異數在一般環境中分布的比例接近一致時，我們  調變頻譜分解技術於強健語音辨識之研究  91  可以同時對其平均值和標準差來進行正規化： || (3)  在式(3)中， 與 為單一語句的調變頻譜強度成分之平均值與標準差； 與 為所有訓 練語句的調變頻譜強度成分之平均值與標準差， 便是更新過後的調變頻譜強度成分。  2.4 調變頻譜統計圖等化法(Spectral Histogram Equalization, SHE)  利用非線性的轉換(Nonlinear Transformation)，不僅將調變頻譜強度成分之平均值與標準 差(或變異數)作正規化，而是整體上使得訓練語句與測試語句的調變頻譜強度成分趨於 擁有同一個機率分布函數，正規化全部階層的動差(Sun et al., 2007)：  ||  (4)  在式(4)中， ‧ 為單一語句某一特徵維度的調變頻譜強度之累積分布函數(Cumulative  Distribution Function, CDF)， 則是利用所有訓練語句之調變頻譜強度所求得的對應之  參考累積分布函數，  便是更新過後的調變頻譜強度成分。  2.5 分頻段調變頻譜統計正規化法 此方法的概念是想要改進原始調變頻譜統計正規化法；原始調變頻譜統計正規化法是將 全部調變頻帶的頻譜強度值視為是屬於同一隨機變數的樣本(Samples)，且將之一併進行 正規化的動作。但是前面提到在語音辨識中，不同調變頻率的成分有不同的重要性，低 頻成分是比高頻成分還要相對重要的，因為語言的重要資訊較集中於低頻成分。因此， 有學者提出將調變頻帶分成許多子頻段，再分別對每一個子頻段的頻譜強度作上述所提 的調變頻譜正規化的方法，而不是單純直接對整個全部調變頻帶做處理(Huang et al., 2009)。因為要強調低調變頻率的重要性，所以在低頻部分的子頻段擁有較窄的頻寬，子 頻段的數量也比較多，而高調變頻率便持有相反的特性。由於能更細緻地分析與處理低 頻成分的資訊，過去的一些實驗數據顯示出將調變頻率分頻段來正規化的做法，能比全 頻帶正規化的方式獲得較好的效能。 3. 三種新穎NMF改進方法用於調變頻譜分解  3.1 傳統非負矩陣分解法(NMF) 在很多領域中如何尋找重要的潛藏資訊成分是個重要的議題，而基於非負矩陣分解法 (Nonnegative Matrix Factorization, NMF)(Lee & Seung, 1999)的技術可以被用於處理此議 題。顧名思義，此方法就是將非負的原始資料所成的矩陣進行分解，表示成兩個也是非 負的矩陣乘積，接著利用線性組合的特性來表示原始資料中各個樣本之目的。而其它常 見的線 性表示法有 主成分分析 (Principal Component Analysis, PCA)與獨立成分分析 (Independent Component Analysis, ICA)。非負矩陣分解法與這兩種線性表示法之差異就是  92  張庭豪 等  能夠提供非負的基底向量(Nonnegative Basis Vectors)，且也能夠擁有保證由基底向量組 合而成之資料也為非負的特性。非負矩陣分解法的另一個重要特性是想要學習以部分為 基礎(Parts-Based)之線性表示法來表示原始的資料，且此線性表示法是一個加法的組合模 式。這種以部分為基礎的概念方法擁有直觀的性質，而且對於一個特定任務來說，在與 其它分解方法相比下可以得到比較高的解釋性。過去有學者應用非負矩陣分解法在影像 處理的領域，例如人臉影樣可以用為五官等局部影像做為非負基底向量經由線性組合(線 性編碼)而產生。若是使用上述所提到的，例如 PCA，在分解舉證產生基底向量的過程中 可能會產生負值，這些負值在影像處理當中會難以解釋。而在語音領域方面，語音的特 徵值有正有負，所以較難以直接地使用非負矩陣分解法；直到近期有學者將非負矩陣分 解法用在分析調變頻譜強度以擷取重要語音特徵(Chu et al., 2011)，而可以得到了不錯的 強健性效果。NMF 的數學式表示如下：  V WH  WH  (5)  其中V ∈ 為一個非負矩陣，而兩個被分解出來的非負矩陣分別為W ∈ ，如圖 1 所示。  和H∈  圖 1. 非負矩陣分解法(NMF)示意圖 其中矩陣 W 所包的 K 行即為基底向量，矩陣 H 中的每一行則通常被稱為編碼向量 (Encoding)，有著權重的概念，與基底向量進行線性組合去近似資料矩陣 V。I 是每筆資 料向量的維度大小；J 為所有資料向量的個數；K 為基底向量的數量。參數 K 是可以自 行決定的，通常會選擇小於 I 與 J，但還是會有選擇的限制： (6) 式(6)是學者過去所提出更確切的基底向量個數選擇限制關係式。在非負矩陣分解法的方 法中，有著資料壓縮的概念，若是 K 的數目選擇得越少，代表壓縮的比率越高。因為我 們對資料進行了壓縮的動作，所以壓縮後的資料跟原始的資料來比較必定會有一些資料 是在壓縮過程中被遺失了。我們希望遺失的部分資料越少越好，所以可以定義減損函數 (Loss Function)來測量資料前後的相似度。測量由兩個因子矩陣 W 與 H 所重建的訊號Λ 與原始訊號 V 之間的距離，對分解結果與原始資料的近似程度作量化(Quantification)。  調變頻譜分解技術於強健語音辨識之研究  93  非負矩陣分解法常見的減損函數為歐氏距離(Euclidian Distance 或 Frobenius Norm)：  D V||WH ||V WH||  V WH  (7)  ,  D V||WH 是藉由歐氏距離所提出的減損函數。當重建訊號Λ與原始信號 V 相等時，則 D V||WH 0。另一個減損函數則是基於 KL 散度(Kullback-Leibler Divergence)：  D V||WH  V  ln  V WH  V  WH  (8)  ,  當原始信號 V 與重建訊號Λ相等時，D V||Λ 0。因為 KL 散度不具對稱性(Symmetric)， 因此減損函數值不能稱為兩個訊號之間的距離值(Distance)，而是兩訊號之間的差異值 (Divergence)，而 KL 散度也稱為相對熵(Relative Entropy)。  由於要將資料矩陣 V 分解成 W 與 H，而使誤差最小化。所以使用迭代更新規則將 W 與 H 更新去求得局部最小值(Local Minimum)。最起初提出的方法是使用梯度下降演 算法(Gradient Descent Algorithm)與加法迭代(Iteration)規則。後來又有學者提出乘法迭代 規則；乘性迭代規則能夠直接地賦予非負矩陣分解法之非負限制的特性。以下是乘法迭 代更新規則(Lee & Seung, 2000)：  Euclidian Distance 的乘法更新規則：  H ←H  WV W WH  W ←W  VH WHH  9  Kullback-Leibler Divergence 的乘法更新規則：  H  ←H  ∑ W V ⁄ WH ∑W  W  ∑ H V ⁄ WH ∑H  (10)  3.2 非平滑非負矩陣分解法(NSNMF) 非平滑非負矩陣分解法(Pascual-Montano et al., 2006)直接修改傳統非負矩陣分解法的模 型，利用模型的乘法性質，達到矩陣全面的稀疏，以能擷取更局部的資訊(如圖 2 所意示)。 非負矩陣分解法將資料矩陣分成兩個矩陣相乘，也就是基底矩陣乘以編碼矩陣。若在一 個矩陣中，其元素是非稀疏或平滑的，為了要補償最後兩個矩陣相乘之後能盡可能地近 似原始資料矩陣，這將會迫使另一個矩陣面臨稀疏或非平滑的情況。非平滑非負矩陣分 解法可以定義如下：  94  張庭豪 等  V WSH  在式(11)中，矩陣V ∈ 為資料矩陣；矩陣W ∈ 為基底矩陣；矩陣H ∈  矩陣；而矩陣S ∈  稱為平滑矩陣，其定義如下：  (11) 為編碼  圖 2. 非平滑非負矩陣分解法(NSNMF)示意圖  S 1 I 11  (12)  式(12)中 1 是一個元素都是 1 的向量，I 是單位矩陣，以及θ是一個用來控制整體稀疏程  度的參數，此參數θ滿足0  1的範圍中。對平滑矩陣 S 可以解釋為：假設 X 為一個  正的非零值向量，而 Y=SX 為轉換後的向量。如果θ=0，Y=X，意謂著向量 X 中沒有平  滑發生；如果θ=1，向量 Y 中所有的元素會變成一致的數值，此數值會等於向量 X 所有  元素的平均，這就是最平滑的向量。由上述可知參數θ用來控制平滑矩陣 S 的平滑程度。  由於模型的乘法性質，平滑矩陣 S 中若有強烈的平滑情況，將會迫使在基底向量與編碼  向量中造成強烈的稀疏，因此也可以說參數θ是用來控制整個非負矩陣分解法模型的稀  疏程度。特別的是，當參數θ=0 時，平滑矩陣 S 會等同於一個單位矩陣 I，此時模型會  回歸到傳統的非負矩陣分解法的模型。在此，我們將更進一步地說明整個非平滑非負矩  陣分解法的流程與乘法更新規則。首先，式(11)中非平滑非負矩陣分解法的模型可以等  價地寫成：  V WS H W SH  (13)  用括號來表示平滑矩陣S是先與哪個矩陣做相乘。若是平滑矩陣S先與基底矩陣W做相乘， 代表說基底矩陣W會變得平滑，這將會迫使編碼矩陣 H 變得稀疏；同樣地，若是平滑矩 陣S先與編碼矩陣H做相乘，代表說編碼矩陣H會變得平滑，這將會迫使基底矩陣W變得 稀疏。在非負矩陣分解與更新過程中，上述兩種情況將都會發生的，所以基底矩陣W與 編碼矩陣H都會被強制變為具稀疏性。相較於傳統非負矩陣分解法，非平滑非負矩陣分 解法的乘法迭代更新規則為：在更新編碼矩陣 H 時，將W換成 WS ；更新基底矩陣W時， 將H換成 SH 。  調變頻譜分解技術於強健語音辨識之研究  95  Euclidian Distance 的乘法更新規則：  H ←H  WS V WS WS H  W ←W  V SH W SH SH  (14)  Kullback-Leibler Divergence 的乘法更新規則：  H  ←H  ∑ WS V ⁄ WS H ∑  W  ←W  ∑ SH V ⁄ W SH ∑ SH  15  而其它部分的演算法流程同傳統非負矩陣分解法。  3.3 基於圖正則化非負矩陣分解法(GNMF)  基 於 圖 正 則 化 非 負 矩 陣 分 解 法 (Graph Regularized Non-negative Matrix Factorization, GNMF)(Cai et al., 2011)的主要目的在於保留資料的局部不變性(Locally Invariant)(Hadsell et al., 2006)，意指原本相鄰的資料向量經過降維或投影後仍然維持相鄰近。資料向量間 的遠近關係，或幾何結構資訊可以用ㄧ權重矩陣 E 表示，其維度是等於資料向量數量所 形成的方陣。最後將權重矩陣 E 納入減損函式中，做為編碼矩陣的正則項(Regularization Term)。  令  , … , 為編碼矩陣H的第 j 行， 可被視為是第 個資料向量相對於新  的基底矩陣W之新表示。在此我們討論較常見的歐式距離：  ,  (16)  此距離用來測量相對於新的基底矩陣W，而兩個資料向量 與 在低維度空間中表示之 間的差異(Dissimilarity)，距離函式值越大代表此兩個資料向量 與 彼此差異越大。  
Traditional way of conducting analyses of human behaviors is through manual observation. For example in couple therapy studies, human raters observe sessions of interaction between distressed couples and manually annotate the behaviors of each spouse using established coding manuals. Clinicians then analyze these annotated behaviors to understand the effectiveness of treatment that each couple receives. However, this manual observation approach is very time consuming, and the subjective nature of the annotation process can result in unreliable annotation. Our work aims at using machine learning approach to automate this process, and by using signal processing technique, we can bring in quantitative evidence of human behavior. Deep learning is the current state-of-art machine learning technique. This paper proposes to use stacked sparse autoencoder (SSAE) to reduce the dimensionality of the acoustic-prosodic features used in order to identify the key higher-level features. Finally, we use logistic regression (LR) to perform classification on recognition of high and low rating of six different codes. The method achieves an overall accuracy of 75% over 6 codes (husband’s average accuracy of 74.9%, wife’s average accuracy of 75%), compared to the previously-published study of 74.1% (husband’s average accuracy of 75%, wife’s average accuracy of 73.2%) (Black et al., 2013), a total improvement of 0.9%. Our proposed method achieves a higher classification rate by using much fewer number of features (10 times less than the previous work (Black et al., 2013)). Keywords: Deep Learning, Stacked Autoencoders, Couple Therapy, Human Behavior Analysis, Emotion Recognition 1. 緒論 人與人之間交談互動，常透過語言傳達彼此的想法，並在這交談過程中得知雙方的行為 反應。利用人為觀察來分析雙方行為反應，這部分最早常應用在心理學和精神學方面 (O’Brian et al., 1994)。人為行為觀察相當的成功研究在親密關係 (Karney & Bradbury, 1995) (Gonzaga et al., 2007)，即夫妻的行為是影響親密關係程度的因素之一。然而用於 人為觀察行為的方式存在一些困難，一方面太消耗時間，另一面也浪費成本。 如果能透過電腦工程的方式來取代人為觀察將大大提升效率，透過低層描述映射高 層描述來預測人類行為 (Schuller et al., 2007)，這項研究領域是正在不斷發展的一部分。 人類行為信號處理(Behavioral Signal Processing, BSP)目的在幫助連接信號科學和行為處 理的方法，建立在傳統的信號處理研究，如語音識別，面手部追蹤等等。相關顯著 BSP 研究已發產於以人為中心的提取音頻，視頻信號，來分析實際上人類行為或是情感方面 (Burkhardt et al., 2009; Devillers & Campbell, 2011)。  透過語音特徵建構基於堆疊稀疏自編碼器演算法之  109  婚姻治療中夫妻互動行為量表自動化評分系統  本論文利用 BSP 的基本思路應用在婚姻治療資料庫上面 (Christensen et al., 2004)， 婚姻治療資料庫會詳細說明在第二章。這個資料庫紀錄了夫妻在一段對話中談述了他們 所選擇婚姻中的問題。評分者在根據他們一段話的種種行為根據不同行為量表進行評分 (幽默行為、悲傷行為等等)。 延續上篇論文的研究內容來自動化分析夫妻一段對話的行為分數(Black et al., 2013)， 一段語音經過預處理，之後作聲音特徵擷取(acoustic feature extraction)，再使用機器學習 來作分類辨識，得到最後的準確率。其中，特徵擷取和機器學習的算法都會影響最後的 準確率，思考如何改進這些影響因素，對整體準確率的提升是一大重要的課題，也是我 們提出這篇論文的因素之一。 在特徵擷取方面，我們沿用三種低階語音特徵(Low Level Descriptors, LLDs)，語韻 (prosodic) LLDs、頻譜(spectrum) LLDs 和音質(voice quality) LLDs。切割三種說話者說 話區間(speaker domain)，丈夫說話區間、太太說話區間、和不分人說話區間。再來對應 各區間提取 20%語句，經過 7 種統計函數(functionals)，產生 2940 種特徵值。最後我們 利用非監督深度學習的做法來降維找出相對關鍵的主要特徵值表現。 深度學習在機器學習領域裡面是最近熱門的話題 (Hinton, 2006)。深度學習可看成是 一種資訊的表達方式，利用多層神經網絡，第一層輸入的數據學習之後，產生新的組合 輸出，輸出值為第二層的輸入值，再經由學習產生新的輸出值，依此類推重覆把每層的 資訊堆疊下去，透過這樣多層學習，可以得到對一個目標值好的特徵表示，相對準確率 就能有所提升。至今存在多種深度學習框架如深度神經網路(DNN)、深度信念網路(DBN) 和卷積神經網路(CNN)已被應用在語音 (Hinton et al., 2012)、影像辨識 (Smirnov et al., 2014)和手寫識別 (Perwej & Chaturvedi, 2011)等等。 我們利用深度學習中的堆疊稀疏自編碼器(stacked sparse autoencoder, SSAE) ，降低 特徵值維度，提升特徵值整體相關性，最後利用簡單 LR 辨識行為分數高低。此初期研 究結果顯示整體行為平均準確率 75%較之前研究使用 40479 維特徵值結合支持向量器 (support vector machine) (Black et al., 2013)提升了 0.9%。 以下簡述各章節的內容。第二章介紹本篇論文所使用的資料庫(database)，第三章介 紹我們使用的 SSAE 架構和其演算法，第四章介紹我們提出的系統架構和研究結果，第 五章節為結論。 2. 婚姻治療資料庫 為了測試我們提出方法的準確率，我們使用和之前論文相同的婚姻治療資料庫(couple therapy database)。以下簡單的介紹的資料庫相關內容：此資料庫的收集是基於研究綜合 行為夫婦治療(integrative behavioral couple therapy, IBCT)成效 (Christensen et al., 1995)。 資料內容針對 134 對夫妻，每對都長期患有婚姻的問題，如夫妻相處不融洽或是爭執。 治療內容為每對夫妻接受為期一年的治療，研究團隊再讓每對夫妻由太太和丈夫各 別選擇一個目前存在嚴重婚姻問題的題目來作為一段 10 分鐘對話，對話中沒有治療師和  110  陳柏軒與李祈均  研究團隊。透過這 10 分鐘的對話讓夫妻彼此了解雙方之間的問題並且試圖解決當前問 題。  每對夫妻皆會進行三個不同階段的對話，治療前、治療中和治療兩年後。透過這三 個時間點對話，再經由多位有專業背景的評分者經由兩個行為評分量表，基於社交互動 行為評分系統(Social Support Interaction Rating System, SSIRS) (Jones & Christensen, 1998) 和基於夫妻互動行為評分系統(Couples Interaction Rating System, CIRS) (Heavey et al., 2002)進行評分，依據評分結果來了解治療的成效。SSIRS 主要包含 19 種行為準則在四 個社交互動分類裡，情感(affectivity)、屈從服從(dominance/submission)、互動表現行為 (feature of interaction)和主題評價(topic definition)來作為評分的內容，CIRS 主要包含 13 種行為準則關於夫妻互動問題解決方面，如表 1。 表 1. 32 種人類行為準則包含在兩種行為量表 SSIRS 和 CIRS  Manual  Codes  SSIRS (Social Support Interaction Rating System) CIRS (Couples Interaction Rating System)  Global positive affect、global negative affect use of humor、sadness、anger/frustration、 belligerence/domineering、contempt/disgust、 tension/anxiety 、 defensiveness 、 affection 、 satisfaction 、 solicits partner suggestions 、 instrumental support offered 、 emotional support offered、submissive or dominant、topic a relationship issue、topic a personal issue、 discussion about husband、discussion about wife Acceptance of other、blame、responsibility for self、solicits partner perspective、states external origins、discussion、clearly defines problem、 offers solutions 、 negotiates 、 make agreements 、 pressures for change 、 withdraws 、avoidance  總共 32 個行為準則，每個行為評分區間為 1 到 9 分。同一對話中，丈夫與妻子會各別被 評分。1 為對這項行為所表現的程度最低，9 為對這項行為所表現的程度最高。評分者為 3 到 4 個，透過觀察夫妻 10 分鐘的影片來各別對 32 個行為進行評分。最後總共有 569 個 10 分鐘的會話，117 對夫妻在這個婚姻治療庫裡。 本篇論文延續上一篇論文所使用的 6 種行為來下去作分析，包含認同對方 (Acceptance of other)、責備行為(Blame)、夫妻之間正面的互動(Global positive affect)、夫 妻之間負面的互動(Global positive affect)、悲傷行為(sadness)、幽默表現行為(humor)，如  透過語音特徵建構基於堆疊稀疏自編碼器演算法之  111  婚姻治療中夫妻互動行為量表自動化評分系統  表 2。之所以會選擇這 6 種行為，因為和其他 26 種行為評分比起來，這 6 種有較高的評 分者認同度(Agreement)，認同度的計算方式為個別評分者的分數和其他評分者評分的平 均分數取相關係數(correlation)。其餘行為的認同度介於 0.4 和 0.7 之間，第五章節會比較 這 6 種行為預測準確率。 表 2. 對於 6 種行為準則的認同度(agreement)  Code  Agreement  Acceptance of other (acc)  0.751  Blame (bla)  0.788  Global positive affect (pos)  0.740  Global negative affect (neg)  0.798  Sadness (sad)  0.722  Use of humor (hum)  0.755  3. 研究方法 在本節, 我們首先簡單的介紹自編碼器(Autoencoder)和堆疊稀疏自編碼(Stacked Sparse Autoencoder, SSAE)基本架構以及本篇論文用到的演算法。  3.1 自編碼器(Autoencoder) 深度學習中自編碼器利用非監督學習方式 (Rubanov, 2000)，目標從高維度的輸入特徵值 學習到更具代表性的特徵值，最後透過解碼讓輸出值等於輸入值，基本的自編碼器架構 如圖 1。  圖 1. 自編碼器  112  陳柏軒與李祈均  從圖 1，輸入值 ， 1,2, … , ， ∈ ，隱藏層(hidden layer)中的 ，  ∈ ，權重矩陣(weight matrix) ∈  ，偏移向量(bias vector) ∈  子(factor)構成激活函數(activation function)，如式(1)。  1,2, … , ， 。由這些因  (1)  其中 權重矩陣  1/ 1  為 sigmoid function 。輸出值 ， 1,2, … , , ∈ ，  ∈  ，偏移向量 ∈ ，自編碼器輸出為式(2):  (2)  為了要求得權重矩陣  和  ， 偏移向量 和 ，假設一個樣本集為  , , , … , ，有 m 組樣本， 為樣本輸入特徵值， 為對應標籤值，利  用代價函數(cost function)，如式(3)。  ,  11 2  2  ,  (3)  式(3)中第一項為均方差項( sum-of-squares error term)，第二項為規則項(regularization term)，其中λ為權重衰減參數(weight decay parameter)，n 為自編碼器層數， 為第 l 層 節 點 數 ， 這 項 是 為 了 避 免 訓 練 過 程 發 生 過 擬 合 (overfitting) ， 之 後 我 們 利 用 反 向 傳 導 (back-propagation)演算法和 L-BFGS 優化算法 (Andrew & Gao, 2007)，重複疊代減小 , 值，最後得到 和 。 而為了讓輸入特徵值更有效的歸類群集並且不同特徵之間的區隔明顯， , 加 入稀疏項(sparsity term)如式(4)，取名為稀疏編碼器(sparse autoencoder) (Obst, 2014)。  ,  ,  ||  (4)  其中 ∑  log  
This paper proposes an automatic method to build a Chinese spelling check system. Confusion sets were expanded by using two language resources, Shuowen Jiezi and the Four-Corner codes, which improved the coverages of the confusion sets. Nine scoring functions which utilize the frequency data in the Google Ngram Datasets were proposed, where the idea of smoothing was also adopted. Thresholds were also decided in an automatic way. The final system achieved far better than our baseline system in CSC 2013 Evaluation Task. Keywords: Chinese Spelling Check, Confusion Set Expansion, Google Ngram Scoring Function. 1. Introduction Automatic spelling check is a basic and important technique in building NLP systems. It has been studied since 1960s as Blair (1960) and Damerau (1964) made the first attempt to solve the spelling error problem in English. Spelling errors in English can be grouped into two classes: non-word spelling errors and real-word spelling errors. A non-word spelling error occurs when the written string cannot be found in a dictionary, such as in “fly fron* Paris”. The typical approach is finding a list of candidates from a large dictionary by edit distance or phonetic similarity (Mitton, 1996; Deorowicz & Ciura, 2005; Carlson & Fette, 2007; Chen et al., 2007; Mitton, 2008; Whitelaw et al., 2009). A real-word spelling error occurs when one word is mistakenly used for another word, such as in “fly form* Paris”. Typical approaches include using confusion set (Golding & Roth, 1999; Carlson et al., 2001), contextual information (Verberne, 2002; Islam & Inkpen, 2009), and others (Pirinen & Linden, 2010; Amorim & Zampieri, 2013). Spelling error problem in Chinese is quite different. Because there is no word delimiter  Department of Computer Science and Engineering, National Taiwan Ocean University No. 2, Pei-Ning Road, Keelung, 20224 Taiwan E-mail: (cjlin, wcchu.cse)@ntou.edu.tw  24  Chuan-Jie Lin & Wei-Cheng Chu  in a Chinese sentence and almost every Chinese character can be considered as a one-character word, most of the errors are real-word errors. Although that an illegal-character error can happen where writing by hand, i.e. the written symbol is not a legal Chinese character and thus not collected in a dictionary, such an error cannot happen in a digital document because only legal Chinese characters can be typed or shown in computer. Spelling error problem in Chinese is defined as follows: given a sentence, find the locations of misused characters which result in wrong words, and propose the correct characters. There have been many attempts to solve the spelling error problem in Chinese (Chang, 1994; Zhang et al., 2000; Cucerzan & Brill, 2004; Li et al., 2006; Liu et al., 2008). Among them, lists of visually and phonologically similar characters play an important role in Chinese spelling check (Liu et al., 2011). Two Chinese spelling check evaluation projects have been held: Chinese Spelling Check Evaluation at SIGHAN Bake-off 2013 (Wu et al., 2013) and CLP-2014 Chinese Spelling Check Evaluation (Yu et al., 2014), including error detection and error correction subtasks. The tasks are organized based on some research works (Wu et al., 2010; Chen et al., 2011; Liu et al., 2011). Our baseline system participated in both tasks. This paper describes an extended system based on Chinese Spelling Check (shorten as CSC tasks hereafter) 2013 and 2014 datasets. This paper is organized as follows. Section 2 introduces our baseline system developed during Chinese Spelling Check Task 2013 and 2014. We sought new resources to expand confusion sets as described in Section 3. New scoring functions and threshold decision using Google Ngram frequencies to estimate the likelihood of passages were defined in Section 4. Section 5 shows experimental results with discussions and Section 6 concludes this paper. 2. Baseline System Description 2.1 System Architecture Figure 1 shows the architecture of our Chinese spelling checking system. A sentence under consideration is first word-segmented. Candidates of spelling errors are replaced by similar characters one by one. The newly created sentences are word segmented again. They are sorted according to sentence generation probabilities measured by word or POS bigram model. If a replacement results in a better sentence, spelling error is reported. In CSC tasks, the set of similar characters is called a confusion set. More information about confusion sets is given in Section 2.2.  A Study on Chinese Spelling Check Using  25  Confusion Sets and N-gram Statistics  Original sentence Segmented org sent  Word segmentation Similar character replacement  Replaced sentences  Word segmentation  Segmented rpl sent Top results  Filtering rules; N-gram probabilities (words or POS)  Figure 1. Architecture of NTOU Chinese Spelling Check System  There are two kinds of spelling-error candidates in our system: one-character words and two-character words. Their replacement procedures are different, as described in Section 2.3 and 2.4. Section 2.5 introduced two rules for filtering out unlikely replacements. N-gram probability models in our baseline system are described in Section 2.6. The procedure to decide locations of errors is given in Section 2.7. 2.2 Confusion Sets In SIGHAN7 Bake-off 2013 Chinese Spelling Check task, the organizers provided six kinds of confusion sets: 4 sets of phonologically similar characters and 2 sets of visually similar characters. The four sets of phonologically similar characters include characters with the same pronunciation in the same tone (同音同調, shorten as SPST hereafter), characters with the same pronunciation but in different tones (同音異調, shorten as SPDT hereafter), characters with similar pronunciations in the same tone (近音同調, shorten as DPST hereafter), and characters with similar pronunciations but in different tones (近音異調, shorten as DPDT hereafter). For example, phonologically similar characters to the character 情 (whose pronunciation is [qing2] and meaning is ‘feeling’) are:  26  Chuan-Jie Lin & Wei-Cheng Chu  SPST: 檠晴擎[qing2] SPDT: 青卿蜻傾輕鯖氫清[qing1] 頃請[qing3] 慶罄磬[qing4] DPST: 擒禽噙琴勤秦芹[qin2] DPDT: 精經驚睛…京[jing1] 頸景警…井[jing3] 竟靜競徑鏡…敬[jing4] 今筋斤津…金[jin1] 僅儘錦緊…謹 [jin3] 近進勁盡禁…浸[jin4] 親侵欽嶔[qin1] 寢[qin3] 沁撳[qin4] There are two confusion sets of visually-similar characters. The first one is the set of characters with the same radicals (部首) with the same number of strokes (筆劃) (同部首同筆 畫數, shorten as RStrk hereafter). For example, the radical of the character 情 is 心 (shown as 忄 inside the character) with 11 strokes. Characters belonging to the radical 心 with 11 strokes are: RStrk: 惋您悉惇惆悠患惦惚悼悽惘悸惟惜悻悴悵恿惕 The second visually-similar-character set collects characters with similar Cangjie codes (倉頡碼, shorten as CJie hereafter). Cangjie is a well-known code map of Chinese characters. Each Chinese character is encoded by a combination of at most 5 codes representing basic strokes in its visual structure. Characters who have similar Cangjie codes are likely visually similar. Liu et al. (2011) considered the information of surface structure and stroke similarity to create this confusion set. For example, the Cangjie code of the character 情 ([qing2], ‘feeling’) is PQMB, where “P 忄” denotes its radical part (忄) and “QMB キ一月” denotes its body part (青). So its similar characters are: CJie: 清[EQMB] 晴[AQMB] 倩[OQMB] 猜[KHQMB] 睛[BUQMB] 靖[YTQMB] 精[FDQMB] 蜻[LIQMB] 鯖[NFQMB] 菁[TQMB] 請[YRQMB] 青[QMB] 債[OQMC] 漬[EQMC] 嘖[RQMC] 磧[MRQMC] 積[HDQMC] 績[VFQMC] 蹟[QMQMC] 責[QMBUC] 2.3 One-Character Word Replacement After doing word segmentation on the original sentence, every one-character word is considered as candidate where error occurs. These candidates are one-by-one replaced by similar characters in their confusion sets to see if a new sentence is more acceptable.  A Study on Chinese Spelling Check Using  27  Confusion Sets and N-gram Statistics  Taking C1-1701-2 in the test set as an example. The original sentence is  ...嬰兒個數卻特續下滑...  and it is segmented as  ...嬰兒 個數 卻 特 續 下滑...  “卻”, “特” and “續” are one-character words so they are candidates of spelling errors. The confusion set of the character “卻” includes 腳欲叩卸... and the confusion set of the character “特” includes 持時恃峙侍... Replacing these one-character words with similar characters one-by-one will produce the following new sentences.  ...嬰兒個數腳特續下滑... ...嬰兒個數欲特續下滑... ...嬰兒個數卻持續下滑... (correct) ...嬰兒個數卻時續下滑... ......  (English meaning: 嬰兒 infant, 個數 number, 卻 but, 腳 foot, 欲 desire, 特 particular, 續 continue, 持續 keep, 時 time, 下滑 decrease) (Original sentence: infant number but special continue decrease ‘but the number of infants particularly continues to decrease’) (Correct sentence: 嬰兒個數卻持續下滑 ‘but the number of infants keeps decreasing’) 2.4 Two-Character Word Replacement Our observation on the training sets finds that some errors occur in two-character words, which means that a string containing an incorrect character is also a legal word. Examples are “ 身手” ([shen1-shou3], ‘skills’) versus “生 手” ([sheng1- shou3], ‘amateur’), and “人員” ([ren2-yuan2], ‘member’) vs. “人緣” ([ren2-yuan2], ‘relation’). To handle such kinds of spelling errors, we created confusion sets for all known words by the following method. The resource for creating word-level confusion set is Academia Sinica Balanced Corpus (ASBC for short hereafter, cf. Chen et al., 1996).  28  Chuan-Jie Lin & Wei-Cheng Chu  For each word appearing in ASBC, each character in the word is substituted with its similar characters one by one. If a newly created word also appears in ASBC, it is collected into the confusion set of this word. Take the word “人員” as an example. After replacing “人” or “員” with their similar characters, new strings 仁員, 壬員, …, 人緣, and 人 韻 are looked up in ASBC. Among them, only 人緣, 人猿, 人文, and 人俑 are legal words thus collected in 人員’s confusion set. For each two-character word, if it has a confusion set, similar words in the set one-by-one substitute the original word to see if a new sentence is more acceptable. Take ID=00058 in the Bakeoff 2013 CSC Datasets as an example. The original sentence is ... 在教室裡只要人員好... and it is segmented as ... 在 教室 裡 只要 人員 好... where “教室”, “只要”, and “人員” are multi-character words with confusion sets. By replacing 教室 with 教士, 教師…, replacing 只要 with 祇要, 只有, and replacing 人員 with 人緣, 人猿…, the following new sentences will be generated. ... 在教士裡只要人員好... ... 在教師裡只要人員好... ... 在教室裡祇要人員好... ... 在教室裡只要人緣好... (correct) ... 在教室裡只要人猿好... (English meaning: 在 in, 教室 classroom, 教士 priest, 教師 teacher, 裡 inside, 只要 as-long-as, 祇要 as-long-as (variant), 人員 member, 人緣 relations, 人猿 ape, 好 good) (Original Sentence: in classroom inside as-long-as member good ‘as long as there are good members in the classroom…’) (Correct sentence: 在教室裡只要人緣好 ‘in the classroom, as long as you have good relations with the others...’)  A Study on Chinese Spelling Check Using  29  Confusion Sets and N-gram Statistics  2.5 Filtering Rules Two filter rules are applied before error detection in order to discard apparently incorrect replacements. The rules are defined as follows.  Rule 1: No error in person names If a replacement results in a person name, discard it. Our word segmentation system performs named entity recognition at the same time. If the replacing similar character can be considered as a Chinese family name, the consequent characters might be merged into a person name. As most of the spelling errors do not occur in personal names, we simply ignore these replacements. Take C1-1701-2 as an example:  ...每 位 產 齡 婦女... (every QF pregnancy age woman ‘every woman in the age of pregnancy’)  “魏” is phonologically similar to “位” and is a Chinese family name. The newly created sentence is segmented as  ...每 魏產齡(PERSON) 婦女... (every Chan-Ling Wei woman: nonsense)  where “魏產齡” is recognized as a person name so this replacement is discarded. Rule 2: Stopword filtering For the one-character replacement, if the replaced (original) character is a personal anaphora (你 ‘you’ 我 ‘I’ 他 ‘he/she’) or numbers from 1 to 10 (一二三四五六七八九十), discard the replacement. We assume that a writer seldom misspell such words. Take B1-0122-2 as an example:  ...我 會 在 二 號 出口 等 你... (I will at two number exit wait you ‘I will wait for you at Exit No. 2’)  Although “二” is a one-character word, it is in our stoplist therefore no replacement is performed on this word.  30  Chuan-Jie Lin & Wei-Cheng Chu  2.6 N-Gram Probabilities A basic hypothesis is that a correct replacement will generate a “better” sentence which has higher probability than the original one. The likelihood of a passage being understandable can be estimated as sentence generation probability by language models. We tried smoothed word-unigram, word-bigram, and POS-bigram models in our baseline system. The training corpus used to build language models is ASBC. As usual, we use log probabilities instead. Besides applying rules in which the probabilities were compared directly, we also treated them as features to train a SVM classifier which guessed whether a replacement was correct or not. 2.7 Error Detection In our system, error detection and correction greatly rely on sentence generation probabilities. Therefore, all the newly created sentences should also be word segmented. If a new sentence results in a better word segmentation, it is very likely that the original character is misused and this replacement is correct. But if no replacement is better than the original sentence, it is reported as “no error”. The detail of our error detection algorithm is delivered here. The original sentence is first divided into several sub-sentences by six sentence-delimiting punctuation marks: comma, period, exclamation, question mark, colon, and semicolon. The following steps are performed on each sub-sentence, referred to as original passage hereafter. 1. Divide the original sentence into several passages by the sentence-delimiting punctuation marks 2. Perform word segmentation on the original passages 3. Measure the likelihood of the original passages by language models 4. For each one-character word in each original passage (1) Skip the word if it is a person name or a stopword (filtering rules) (2) Replace the word with its similar characters in the confusion sets to generate un-segmented passages, one new passage for one similar character (3) Perform word segmentation on the new passages 5. For each two-character word in each original passage (1) If the word appears in the two-character confusion set, replace the word with its similar words in the two-character confusion sets to generate un-segmented passages, one new passage for one similar word (2) Perform word segmentation on the new passages  A Study on Chinese Spelling Check Using  31  Confusion Sets and N-gram Statistics  6. Measure the likelihood of the new passages from step 4 and 5 by language models 7. If no new passage has a higher score than its original passage, report “no error” in this original passage 8. Consider only the new passage with the highest score (1) If its score comparing to the original one is not higher than a pre-defined threshold, report “no error” in this original passage (2) Otherwise, report the location and the similar character (or locations of similar characters in a two-syllable similar word) of the replacement which generates this new passage  3. Confusion Set Expansion  In our experience, the confusion sets provided by the task organizers do not cover all the errors. The error coverage of the confusion sets is depicted in Table 1, where TR means training set and TS means test set. The first 9 rows show the coverage of each confusion set, where set 0 to set 5 have been explained in Section 2.2. We can see that the SPST confusion set alone covers 70% of the errors in CSC 2013 datasets but only about half of the errors in CSC 2014 datasets. The second important confusion set is CJie, which covers 30% to 40% of the errors.  The last 10 rows of Table 1 show the coverage of the unions of confusion sets. The union of set 0~5 covers 94.59% of the errors. The union of set 0~3+5 has the same coverage as the union of set 0~5, which suggests that RStrk can be ignored.  In order to achieve better coverage, we used two resources to expand the confusion sets. One is Shuowen Jiezi and the other is the Four-Corner Encoding System.  Table 1. Error Coverage of Confusion Sets (%)  Confusion Set TR2013 TS2013 TR2014 TS2014  set0: SPST  70.09 72.13 47.92 47.41  set1: SPDT  15.10 17.50 46.52 47.03  set2: DPST  3.70  4.99  5.15 4.68  set3: DPDT  3.70  4.67  8.41  7.71  set4: RStrk  9.12  3.17  0.38  0.88  set5: CJie  40.46 36.18 29.72 31.10  set6: Cor4  14.81  6.89  1.84  1.52  set7: SWen1  17.09 19.24 11.48 12.64  set8: SWen2  18.23 19.64 11.91 12.90  32  Chuan-Jie Lin & Wei-Cheng Chu  set0+1 set0+1+2 set0+…+3 set0+…+4 set0+…+5 set0+…+6 set0+…+7 set0+…+8 set0+1+2+3+5 set0+1+2+3+5+7  74.93 78.35 79.20 87.75 94.59 96.01 97.15 97.15 94.59 97.15  78.23 83.06 83.85 86.94 93.27 93.67 94.54 94.54 93.27 94.54  71.89 76.55 81.55 81.76 83.86 84.22 84.58 84.60 83.86 84.58  72.57 76.61 82.05 82.30 84.58 84.70 85.59 85.59 84.58 85.59  3.1 Confusion Set from Shuowen Jiezi Shuowen Jiezi1 (說文解字) is a dictionary of Chinese characters. Xu Shen (許慎), author of this dictionary, analyzed the characters according to the six lexicographical categories (六書). One major category is phono-semantic compound characters (形聲), which were created by combining a radical (形符) with a phonetic component (聲符). Characters with same phonetic components were collected to expand confusion sets, because they are by definition phonologically and visually similar. For example, the following characters share the same phonetic component “寺” ([si4], ‘temple’) thus become confusion candidates (their actual pronunciation are given in brackets):  SWen: 侍[si4]持[chi2]恃[shi4]特[te4]時[shi2]...  It happens a phonetic component might not be atomic, which means it also has its own phonetic component. For example, 潔’s phonetic component is 絜, but 絜’s phonetic component is 丯. We tried two creation methods. The first one was created by collecting characters with the same phonetic component (referred to as SWen1), and the second one was the closure of SWen1 (referred to as SWen2). Set 7 and 8 in Table 1 represent SWen1 and SWen2. Although they alone do not provide good coverage, unions including SWen sets can cover up to 97.15% errors in CSC 2013 Training set. Closure set only cover one more error in CSC 2014 Training set. In order not to introduce too much noise, the closure SWen set is not recommended. 
This paper proposed a word usage classification for “De” in Chinese as a secondary language by rule induction algorithm. Learning of Chinese characters and tone adaption are both essential and hard tasks for non-native speakers. The frequent terms, defined in morphosyntatic particle “De” with three characters {的, 得, 地}, is hard to learn for foreign learners due to the similar pronunciation and meaning. This investment illustrates a data-driven algorithm to classify the usages about the morphosyntatic particle “De” in Chinese learning. Rule induction is one of the most important techniques to learn the knowledge from data. Since regularities hidden in data are frequently expressed in terms of rules, rule induction is one of the fundamental tools for natural language processing and obtains a significant improvement in character selection. By the automatic rule induction process, 32 rules are adopted here to classify the character usage in morphosyntatic particle “De.” According to the experimental results, we find the proposed method can provide good enough performance to classify the character usages for morphosyntatic particle “De.” Keywords: Rule Induction, Natural Language Processing, Secondary Language Learning, Classifier, Word Usage. 1. Introduction To learn Chinese as a foreign or second language is to study of the Chinese languages by non-native speakers and new learners. Increasing interested peoples in China learning from those outside has led to a corresponding interest in the study of Chinese as their second language, the official languages of mainland China and Taiwan. However, the learning of Chinese both within and outside China is not a recent phenomenon. Westerners started learning different Chinese languages in the 16th century. Within China, Mandarin became the  Department of Computer Science and Information Engineering, National Chiayi University, Chiayi city, Taiwan, ROC E-mail: Ralph@mail.ncyu.edu.tw  66  Jui-Feng Yeh & Chan-Kun Yeh  official language in early 20th century. According to the analysis of Summer Institute for Linguistics (SIL), there are near to seven thousands languages over the world nowadays. Among these languages, the top five languages are Chinese, English, Spanish, Bengali and Indian by their population sizes. As the first and second languages, Chinese occupies 14.8 percents populations to be the most used language. China’s growing global influence has prompted a surge of interest in learning Mandarin Chinese as a foreign language (CFL), and this trend is expected to continue. Therefore, the population to learn Chinese as the second language is increasing in the latest decades (Simpson, 2000). Compared to the alphabetic language, Chinese is more complex and hard to understand for non-native speakers due to its several thousand characters and complicated sentence structures. Due the historical evolution of Chinese is deep and far, there are some word usage is susceptible to the corresponding allusions. Therefore, it is hard for the second language learners without the Chinese cultural background to understand, handle and use with skill the Chinese words very well. Actually, there are many whereas many computer-assisted learning tools have been developed for learning English, support for CFL learners is relatively sparse, especially in terms of tools designed to automatically evaluate learners’ responses. Computer technologies are used to assist in language learning, the so-called Computer-Assisted Language Learning (CALL), has been invested in the latest decades. An investigation was proposed to the adoption of information and communication technology (ICT) for teachers of Chinese as a foreign language (CFL) in US universities (Lin et al., 2014). Yang (2011) emphasized an online situated language learning environment, for supporting the students, the teachers, and the teaching assistants (TAs) to communicate synchronously and asynchronously in and after class. Chen and Liu (2008) proposed a web-based synchronized multimedia lecture system based on WSML for the learners to learn Chinese as second language. They also compared the Web-CALL, IWiLL, and BRIX based systems for evaluating the proposed systems in Chinese learning/teaching (Chen & Liu, 2008). A user-centered design approach for learn Chinese as second language was invested for evaluating the web usability in (Huang et al., 2010). Lu et al. (2014) suggested the curriculum content design in learning Chinese as a second language. However, Chinese is rated as one of the most difficult languages to learn for people whose native language is English, together with Arabic, Japanese and Korean. There are many difficulties for foreigners to lean Chinese as their second language mainly caused by the special character set and tones in Chinese. Pronunciation cannot be obtained from its character directly. Although there are three aspects: text shape, pronunciation, meaning within one Chinese character. However, there are differences in pronunciation among the similar characters. Therefore, it is hard for the foreign learners to spell the correct Chinese words. For preventing the word segmentation error confusing the word boundaries, Bai et al. (2013) used  Automatic Classification of the  67  “De” Word Usage for Chinese as a Foreign Language  the inter-word spacing effects on the acquisition of new vocabulary for readers. One character with different pronunciations and meanings is hard to understand for non-native learners. Compared to other languages, the information of the Chinese character is overloaded. Besides, the number of the Chinese character is too large for a novice especially for the character with server usages. Tone is not easy to control in Chinese characters. The four tones are hard to enunciate for the non-native learners with a toneless source language. For example, the pitch trajectories for the secondary and third tones are one of the main obstacles for the learners. Accented pronunciation confuses the learners to obtain the standard. The pronunciation in Chinese is usually influenced by the speakers’ own dialect, since the speaker has learned the dialects before they use the Mandarin. The usages of mandarin usually are affected by the dialects significantly such as Wu, Hokenese, Haka, and Cantonese. The complex structure of the Chinese character makes the hinder for nonnative learners. Reading and writing are main learning activities and they are cross validation for assessment of the achievements to use the Chinese characters. However, the complex structure and too many strokes make it more difficult to understand the reading and writing for learners. The flexible grammar rules in Chinese are not easy to learn for nonnative speakers. Confucius has described the Chinese as “a language without solid grammar (文無定法)”since two thousand years ago. The flexibility in syntax makes Chinese to be one of the most various languages. The rich rhetoric in Chinese make it is interesting and hard to understand the grammatical rules. The influence by ancient writings, the word usage is more complex in Chinese. That is to say, the literary language used in ancient China and preserved today for formal occasions or conspicuous display. Without the culture background, the foreigner learners are not able to obtain the meaning and pronunciation about word preciously. The part of a word to which inflectional endings are attached, they are usually seen in alphabetical languages. Stem provides a good extension for word usage for language learners. However, the stems are hard to be obtained for non-native speaker, since the Chinese word with complex structure. The lexicon is hard to use for new learners. Actually, the design of Chinese lexicon aims at the user who is experienced in Chinese usage especially for the populations in home country. It is not friend for new learners. This makes it hard to study Chinese by oneself. For removing the barrier of learning Chinese as second language, more efforts are invested in Chinese character learning. Learning Chinese, which consists of more than ten thousands of characters composed of hundreds of basic writing units, presents such a challenge of orthographic learning for non-native speakers at the beginning stages of learning. A classroom was designed to extend previous research on how to support orthographic learning in (Chang et al., 2014). Chuang and Ku (2011) invested the effect of computer­based multimedia instruction with Chinese character recognition for foreign learners. Chen et al., (2013) proposed an approach for investigating the a radical-derived Chinese character  68  Jui-Feng Yeh & Chan-Kun Yeh  teaching strategy on enhancing Chinese as a Foreign Language (CFL) learners’ Chinese orthographic awareness based on statistical data from the Chinese Orthography Database Explorer established and used as an auxiliary teaching tool. Hsiao et al. (2013) designed and developed a Chinese character handwriting diagnosis and remedial instruction (CHDRI) system to improve the CFL learners’ ability in Chinese character writing. The CFL learners were given two tests based on the CHDRI system. One test focused on Chinese character handwriting to diagnose the CFL learners’ errors in the stroke order and their knowledge of Chinese characters, while the other test focused on the spatial structure of Chinese characters (Hsiao et al., 2013). Looi et al. (2009) Explored interactional moves in a CSCL environment for Chinese language learning. Chang et al. (2012) presented approach for error diagnosis of Chinese sentences for Chinese as second language (CSL) learners. A penalized probabilistic First-Order Inductive Learning (pFOIL) algorithm is presented for error diagnosis of Chinese sentences. The pFOIL algorithm composed with three parts: inductive logic programming (ILP), First-Order Inductive Learning (FOIL), and a penalized log-likelihood function for error diagnosis (Chang et al., 2012). Chinese is a tonal language; tone and pronunciation acquisition also plays an essential role for CSL learners. There are some research efforts were made for listening and speaking diagnosis (Hao, 2012; Chu et al., 2014; Chun et al., 2015; Hsiao et al., 2015). Since the learning for Chinese is not easy for non-native speakers. This drives us to the question what is the one to help the foreign learners. Indeed, the characters those are frequently used and mistake for each other usually confuse the foreign Chinese learners. The second language learners for Chinese usually are in the state of confusion about the usage of “De” (Jiang et al., 2012). Shi and Li (2002) analyzed the causal relationship between the establishment of classifier system and the grammatical issues of the particle “De”. Yip and Rimmington (2004) described that “De” is required to be present in the relative clause as modifier contexts for Chinese as second language (CSL) learners. Waltraud (2012) analyzed the insubordinate subordinator “De” in Mandarin Chinese. Paul (2012) compared the difference of “De” in Chinese and French. Li (2012) also compared the usage between “De” in Chinese and “E” in Taiwanese. This paper invested an automatic rule induction algorithm for classification of the usages of the morphosyntactic particle“De.” The confusing set about the morphosyntatic particle “De” is defined as the character set {的, 得, 地} in Chinese. Herein, the automatically classification about the morphosyntactic particle “De” is further defined as the process to decide which character is correct for using in Chinese. That is to say, we want to help the non-native learner to know which one is correct in the morphosyntatic particle “De” in Chinese. This paper is organized as follows. Section 2 describes the rule induction algorithm used for classify the usage of morphosyntactic particle “De” in detal. In Section 3, we analyze the  Automatic Classification of the  69  “De” Word Usage for Chinese as a Foreign Language  performance in experimental results of the proposed methods. Finally, Section 4 will illustrate the findings and draw the conclusion of this paper. 2. Rule Induction for Morphosyntactic Particle “De” Using the basic ideas of rough set theory, learning from examples module version 2 (LEM 2) is adopted as the rule induction algorithm based on corpus with semantic tagging. As we known, LEM 2 is one of rule induction methods in LERS data mining system, the flow chart is illustrated in Figure 1.  Figure 1. The LEM 2-based rule induction for the morphosyntatic particle “De.” This paper adopted the LEM 2 algorithm to natural language processing especially for Chinese information processing. For each input Chinese sentence, word segmentation is applied to obtain the word level tokens with part-of-speech (POS) tagging. The detection process for “De” is further used to select the sentence with “De.” The sentences without “De” are dropped in the post-processing here. For extracting the linguistic feature to decide which morphosyntactic particle is used in the sentence, the contextual attributes are defines according to word and part-of-speech based n-grams. The contextual attributes accompanied with morphosyntactic particle {的, 得, 地} to constructing the attribute-value pairs. All the attribute-value pairs gathered in training data are fed into LEM 2 rule induction algorithm to generate the rule set. Therefore, the rule set can be further used to decide the usage of the morphosyntactic particle {的, 得, 地}. Herein, the proposed method is divided into two parts, decision table construction and LEM 2 rule induction algorithm, are described dentally in Section 2.1 and 2.2 respectively.  70  Jui-Feng Yeh & Chan-Kun Yeh  2.1 Decision Table Construction Since the decision table is defined as a form for blocks of attribute-value pairs, the attribution plays an essential role in rule induction using LEM 2 algorithm. However, the sentence in natural language is not structural and fitting to the format of attribute. It is noteworthy how to transform the natural language into the attribute. That is to say, proposition extracted from sentence is one of the important issues for attribute. Herein, the contextual information surrounding the morphosyntactic particle “De” is used to form the attributes as shown in Figure 2.  Figure 2. The contextual attribute formulation using the word with POS information surrounding the target word “De.” Each sentence representing one case and the independent variables are called attributes. As shown in Figure 2, the surrounding words with part-of-speech will combined with their relative position for particle “De” will combine into considering to form the attributes. The values are defined as one of the single-character words {的, 得, 地} in particle set. Each attribute-value pair represents one sample of knowledge about a decision table or a property of cases. These attribute-value pairs and the corresponding blocks serve as a basis for rule induction. Similar to N-gram models, the utility of the proposed contextual features is closely linked with the observation window size. As we know, the longer word sequence can provide more information for predicting the next word in N-gram models. This phenomenon leads us to find the near optimal window size for the “De” classifier. However, we have observed the empirical results of the larger windows size. Here, the relative positions from -2 to +2 are included in the windows for obtaining the contextual attribute, because the performance is near to those by the larger windows size. This condition not to conform to our expectation and the reason should be the limitation of the training corpus. For the example shown in Figure 3, the related information in decision table is illustrated in Table 1. The sentence containing the word sequence “欣賞(enjoy) 美麗(beautiful) 的(De) 一幅(a) 畫(picture)” is illustrated as the case 1 shown in Figure 3. Basically, each case is obtained from one sentence. Actually, the number of cases is dependent on the number of the particle “De” in the sentence. An  Automatic Classification of the  71  “De” Word Usage for Chinese as a Foreign Language  example “特別(special) 的(De1) 愛(Love) 給(give/for) 特別(special) 的(De2) 您(you)” with two particles, the cases 2 and 3 is obtained from the same sentence. The cases 4 and 5 in Table 1 show the examples for “得(De)” and “地(De)” separately.  Figure 3. The contextual attribute formulation for the sentence containing “欣賞(enjoy) 美麗(beautiful) 的(De) 一幅(a) 畫(picture).”  Table 1. A decision table for the decision of “De.”  Attributes  case  W-2 POS-2 W-1 POS-1 W+1  POS+1 W+2 POS+2  
This paper presents an empirical study on the difficulties in learning Chinese as a second language based on learners’ corpora written by native English speakers and native Japanese speakers at CEFR-based A2 and B1 levels. The first part of this paper will discuss the procedures for how to collect learners’ corpora, proofread, establish an error tag system and annotate errors. Later it will focus on a significant difference in the production of “ 一 + Classifier” among the corpora of native English speakers and native Japanese speakers. The corpus of English native speakers displays an overuse of “ 一 + Classifier”, even in an atelic context like a negative construction or a conditional construction where a “ 一 + Classifier” should not occur. On the other hand, the corpus of Japanese native speakers displays a lack of “ 一 + Classifier”. This striking contrast is due to whether or not a determiner position exists in each language. Since English has a determiner position which accommodates an article, “a/an, the”, “this/that/my/your/~’s”, English-native learners tend to treat the “ 一 + Classifier” as an article although it does not appear in an atelic event structure. On the other hand, Japanese does not have any determiner position before a Noun Phrase, therefore it is assumed that  
Research on deep learning has experienced a surge of interest in recent years. Alongside the rapid development of deep learning related technologies, various 4  distributed representation methods have been proposed to embed the words of a vocabulary as vectors in a lower-dimensional space. Based on the distributed representations, it is anticipated to discover the semantic relationship between any pair of words via some kind of similarity computation of the associated word vectors. With the above background, this article explores a novel use of distributed representations of words for language modeling (LM) in speech recognition. Firstly, word vectors are employed to represent the words in the search history and the upcoming words during the speech recognition process, so as to dynamically adapt the language model on top of such vector representations. Second, we extend the recently proposed concept language model (CLM) by conduct relevant training data selection in the sentence level instead of the document level. By doing so, the concept classes of CLM can be more accurately estimated while simultaneously eliminating redundant or irrelevant information. On the other hand, since the resulting concept classes need to be dynamically selected and linearly combined to form the CLM model during the speech recognition process, we determine the relatedness of each concept class to the test utterance based the word representations derived with either the continue bag-of-words model (CBOW) or the skip-gram model (Skip-gram). Finally, we also combine the above LM methods for better speech recognition performance. Extensive experiments carried out on the MATBN (Mandarin Across Taiwan Broadcast News) corpus demonstrate the utility of our proposed LM methods in relation to several well-practiced baselines. Keywords: speech recognition, language modeling, deep learning, word representation, concept language model 一、 緒論 語言模型(Language Models, LM)不僅在語音辨識中扮演重要的角色，還可以應用 至資訊檢索、機器翻譯、手寫辨識以及文件摘要等不同任務之中，成為關鍵的組 成[1, 2]。在語音辨識過程中，我們通常會透過語言模型來補足聲學模型經常不 能充分應付同音異字或發音混淆的情況，並幫助語音辨識系統從眾多混淆的候選 詞序列假設(Candidate Word Sequence Hypotheses)中找出最有可能的結果[3, 4]。N 連(N-gram)語言模型為語音辨識之中最為常見的統計式語言模型，用來估測每一 個待預測詞彙在其先前緊鄰的 N-1 個詞彙已知的情況下出現的條件機率；假設每 一個詞彙出現的機率僅與它緊鄰的前 N-1 個詞彙相關，可以透過多項式分布 (Multinomial Distribution)來表示。然而 N 連語言模型僅能擷取短距離的詞彙規則 資訊，而無法考慮長距離的語句或篇章資訊；當詞序列越長時參數量越多，使得 N 連語言模型會有維度詛咒的問題。另一方面，N 連語言模型亦容易面臨訓練語 料與測試語料不匹配(Mismatch)而造成估測誤差。有鑑於此，近十幾年來許多動 態語言模型調適技術被提出，用以發展有效的語言模型輔助並彌補傳統 N 連 (N-gram)語言模型不足之處。常見的有快取模型(Cache Model)[5]，以及在資訊檢 索 領 域 的 主 題 模 型 (Topic Model)[6] 等 。 其 中 又 以 機 率 式 潛 藏 語 意 分 析 (Probabilistic Latent Semantic Analysis, PLSA)[7]以及其延伸狄利克里分配(Latent Dirichlet Allocation, LDA)[8]最為普遍被使用。 本論文旨在於發展新穎動態語言模型調適技術，用以輔助並彌補傳統 N 連 (N-gram)語言模型不足之處。首先，我們提出將分散式表示法之詞向量表示(Word 5  Representation or Embedding)應用於語音辨識的語言模型中使用。在語音辨識的 過程中，對於動態產生之歷史詞序列(Word History)與候選詞(Candidate Word)改 以詞向量表示的方式來建立其對應的語言模型，希望透過詞向量表示而能獲取到 更多詞彙間的語意資訊。其次，我們針對新近被提出的概念語言模型(Concept Language Model)加以改進；嘗試在調適語料中以句子的層次做模型訓練資料挑 選之依據，去掉多餘且不相關的資訊，使得經由調適語料中挑選出的概念類別更 為具代表性，而能幫助動態語言模型調適。另一方面，在選擇相關的概念類別來 動態組成概念語言模型時，而此是透過詞向量表示的方式來估算，其中詞向量表 示是由連續型模型(Continue Bag-of-Words Model)或是跳躍式模型(Skip-gram Model)生成，希望藉由詞向量表示記錄每一個概念類別內詞彙彼此間的語意關 係。最後，我們嘗試將上述兩種語言模型調適技術做結合。本論文是基於公視電 視新聞語料庫來進行中文大詞彙連續語音辨識 (Large Vocabulary Continuous Speech Recognition, LVCSR)實驗，比較本論文所提出語言模型調適技術與其它當 今常用語言模型調適技術之效能。 本論文的後續安排如下：第二節介紹詞向量表示法以及本論文嘗試將詞向量 表示應用於詞圖搜尋中之方法；第三節介紹將詞向量表示資訊融入概念語言模 型；第四節介紹實驗語料、實驗設定以及實驗結果分析；第五節則是結論及未來 展望。 二、 詞向量表示法應用於詞圖搜尋之中 在自然語言中，最常見也是最為直覺的詞表示方式為 One-hot Representation，亦 即將每個詞表示成一個很長的 N 維向量。其中 N 為詞彙的大小，而向量中僅有 其中一維的值為 1，用來表示當前的詞，其餘則表示為 0。此種表示方式是採用 稀疏的方式來儲存，並假設兩兩詞彙間彼此獨立，所以從此向量中並無法找出兩 兩詞彙之間的關係。 因此於 1986 年時，Hinton 提出了分散式表示法(Distributed Representation) [9] 做為詞的表示法，是透過前饋式類神經網路(Feed-Forward Neural Network)訓練而 成。這種向量表示是將詞表示成一個較低維度的實數向量。每個詞彙之間的關係 可以利用餘弦或是歐式距離計算找出兩個詞向量間的語意相似度，我們將這些詞 向量稱為詞表示法(Word Representation or Embedding)。 有鑑於使用傳統類神經網路語言模型來訓練詞向量會造成訓練時間過長， Tomas Mikolov 等 人 [10] 於 是 提 出 所 謂 的 連 續 型 詞 袋 模 型 (Continuous Bag-of-Words Model, CBOW)與跳躍式模型(Skip-Gram Model, SG)，這兩種模型 使 用 階 層 軟 式 最 大 化 (Hierarchical Soft-max, HS)[10] 以 及 負 例 採 樣 (Negative Sampling, NS) [11]方法來提高訓練的速度並改善訓練後詞向量的表示能力。 6  圖一、連續型詞袋模型示意圖  (一)、連續型模型  連續型詞袋模型(CBOW)與前饋式類神經網路類似，不同之處在於連續型詞袋模 型將非線性隱藏層(Non-Linear Hidden Layer)移除，並且在輸入層的所有單詞皆共 享隱藏層。如圖一所示，此模型包含三層，分別為輸入層、投影層、輸出層。已 知當前詞 wt 的上下文wt-2,wt-1,wt+1,wt+2 的情況下預測當前詞wt出現的機率。在此 目標函數為最大化訓練語料庫中所有詞彙平均的發生機率:  T-k 
鄭文惠↑  王宏甦∮  邱偉雲ǂ  Chao-Lin Liu Chun-Ning Chang Chu-Ting Hsu Wen-Huei Cheng Hongsu Wang Wei-Yuan Chiu †‡§國立政治大學資訊科學系、†國立政治大學語言學研究所 ↑ǂ國立政治大學中國文學系、∮美國哈佛大學 CBDB 計畫辦公室 {†chaolin, ‡101703004, §104753021, ↑whcheng}@nccu.edu.tw, ∮hongsuwang@fas.harvard.edu, ǂacwu0523@gmail.com  摘要 唐詩是中國文學極重要的一部分，由清代官方力量所編纂、收錄兩千餘位詩人所著， 內容四萬多首詩歌、包含超過三百萬字的《全唐詩》，無疑是研究唐詩最重要的資源 之一。本文作者採取共現詞彙和 distributional semantics 的分析角度，利用計算語言 學領域所發展的軟體工具，分析《全唐詩》的內容；就作者風格、詩歌內容，特別是 唐詩中的顏色詞彙深入探索。同時我們也利用資訊技術，發掘唐詩內容所攜帶的唐代 文人的社會網路，將研究成果擴大到歷史領域。另外，我們也藉由探勘唐詩中詞彙的 共現、搭配、對仗關係，發展一個簡單的對對聯的應用。透過這一系列的工作，我們 實踐了數位人文領域的初步理想，數位技術雖然尚且不足以直接被用來建立深度的人 文論述，但是透過相關的資訊檢索、文本分析和資料整合的服務，數位技術讓專家可 以比過去更加專注於深度議題的研究，而不需要花很多時間來蒐集基礎的研究資料。 Abstract1 The Complete Tang Poems (CTP) is the most important collection for studying Tang poetry, which in turn is arguably a very influential part of the Chinese literature. Our analyzing the CTP from the perspectives of antithesis2, collocation and distributional semantics offers some interesting overviews of the styles and imageries embedded in the works of some representative Tang poets. Our analyses include (1) a quantitative comparison of the uses of “wind” and “moon” in Li Bai’s and Du Fu’s works and (2) the functions of colors in Tang poems. In particular, we explored the appearances of “white” color, which is the most frequent color in Tang poems. Colors in static poems are like audios in motion pictures, so we thought the analyses could lead us to an important facet of the poems. In addition, we extracted social networks of poets from the poems, and built a simple couplet suggestion kit based on the textual analysis of the poems. 關鍵詞：數位人文、中國文學、全唐詩、詞彙語意、共現分析、文本分析、語料庫 分析、中國歷代人物傳記資料庫 Keywords: Digital humanities, Chinese literature, Quan-Tang-Shi, Distributional semantics,  
Figure 1. The block diagram of the proposed Math Word Problem Solver.  
Readability is basically concerned with readers’ comprehension of given textual materials: the higher the readability of a document, the easier the document can be understood. It may be affected by various factors, such as document length, word difficulty, sentence structure and whether the content of a document meets the prior knowledge of a reader or not. However, simple surface linguistic features cannot always account for these factors in an appropriate manner. To cater for this, we explore in this study a variety of extra features, including syntactic analysis, parts of speech, word embedding, semantic role features and well-written features. The experimental datasets are composed of two parts: one is textbooks of the Chinese language for elementary and junior high schools (K1 to K9) in Taiwan, compiled from three publishers in the academic year of 2009; the other is excellent extracurricular reading materials for students of elementary and junior high schools, collected by the Ministry of Culture in Taiwan. Two readability prediction models, viz. stepwise regression and support vector machine, are evaluated and compared, while the combination of these two models is also investigated so as to further enhance the accuracy of readability prediction. Experimental results reveal that our proposed approach can yield consistently better performance than traditional ones merely with simple surface linguistic features in evaluating text difficulty. Keywords: Readability, Textual Features, Stepwise Regression, Support Vector Machine 一、 緒論 可讀性（readability）是指閱讀材料能夠被讀者理解的程度[1]。可讀性高的文章 較容易被讀者理解。文章的可讀性與很多因素有關，如：文長、字詞難度、句法 結構、內容是否符合讀者的先備知識等，然而表淺的語言特徵並無法完全反映這 些複雜的成分。英文文本的可讀性研究行之有年，或以詞彙頻率列表，評量文章 難度、或將詞表作為參照，建置可讀性公式、或發展線上多文本特徵分析器[2]， 計算影響文章難易度的各類型指標，並提供數值化的結果；中文的可讀性研究則 屈指可數，或選用表淺的語言特徵建置可讀性公式[3, 4]，或將可讀性指標等當 成預測變項，以教科書的年級值當成效標，透過逐步迴歸（Stepwise Regression） 建置公式、或結合特徵選取方法與支援向量機（Support Vector Machine, SVM） 建立預測模型預測文本等級[1]。可讀性研究除了傳統的語言特徵，心理學上的 因素亦是值得考量之因素[5]。可讀性較高的文章除了能讓讀者較容易理解外， 亦應有較高的趣味性，增強閱讀印象，加快閱讀速度，令讀者有意願持續閱讀， 進而達成如輔助教學、文本推薦等特定目標。文本可讀性預測可依據讀者提供合 適的文本閱讀，以提高其理解程度，進而培養從小閱讀的習慣。而可讀性預測的 特徵仍有許多探討空間，結合不同模型以提高預測正確性亦為一研究面向。現今 資訊來源多元，非傳統文字文件，如圖片、音訊、影片等，皆可成為接收新知的 管道，故其可讀性預測亦是未來研究趨勢。然而因多媒體文本所包含的內容形式 與純文字文本之特性差異甚大，如何結合既有概念以探討新興領域之可讀性，所 面臨之挑戰將更加艱困。 72  由於中英文字在語言特徵上的差異極大，過去西方研究者在可讀性研究所採 用的特徵，是否適合中文可讀性評估有待商榷[1]。有鑑於可讀性研究的重要性， 以及可能發展的多元應用，本論文提出使用句法分析（Syntactic Analysis）、詞性 標記（Part-of-Speech, POS）、詞表示法（Word Embedding）、語意資訊（Semantic Information）與寫作程度（Well-written）等特徵，分析不同類型的特徵所代表之 意義，比對各類特徵與可讀性高低的關聯性，並將特徵彼此結合以提升可讀性預 測之正確性。藉由這些特徵，本論文透過逐步迴歸與支持向量機等兩種方式建立 可讀性模型，比較兩者用於測試國中小教科書及優良課外讀物之效能優劣，並期 望找出可讀性分類之重要因素。 本論文的後續安排如下：第二節說明可讀性的基本概念、回顧可讀性的歷史 與公式、分析可讀性的模型、探討可讀性的發展趨勢、介紹可讀性的應用層面。 第三節除解釋先前研究的特徵外，亦分別論述本論文所使用的各類特徵。第四節 為實驗資料與實驗結果的呈現。第五節為全文總結與未來研究展望。 二、 文獻探討 (一) 、可讀性基本概念介紹 可讀性是指閱讀材料能夠被讀者理解的程度（Dale & Chall, 1949; Klare, 1963, 2000; McLaughlin, 1969 ）。 Klare （ 1984 ） 認 為 可 讀 性 的 定 義 為 ： 易 識 別 性 圖一、可讀性研究發展歷史 73  （Legibility）、易閱讀性（Ease of Reading）、易理解性（Ease of Understanding） 等任何一種關於材料的特徵。可讀性的概念中，易理解性是在閱讀領域中比較通 用的用法[1]。 語言專家藉由不斷修正而得出的「可讀性公式」來計算可讀性的分數，並將 這些公式廣泛應用於對文本與讀者群體的閱讀水準加以匹配，然而可讀性公式無 法準確反映文本難度，只是給出一個「不錯的粗略估計」[6]。 (二) 、可讀性之歷史與公式 西方可讀性研究行之有年，早於 1950 年代時可讀性公式已百家爭鳴，近年 來更嘗試探討與文本更相關的凝聚性指標，及各指標間的關係；中文的可讀性研 究相對而言則屈指可數，早期僅運用表淺指標，發展一系列中文適讀性公式，近 期則有將小學教科書進行可讀性分類之探討[1]。可讀性研究概略發展歷史可參 照圖一。西方可讀性研究以發展測量公式為大宗，然而侷限於技術僅納入文本的 表淺語言特徵。第一個可讀性公式 The Lively & Pressey Method 利用詞表當成參 照，篩選出不同等級難度的詞彙當成文章難度指標，對後來的可讀性研究有重大 的影響。另外也有不少的可讀性公式將詞長與句長當成難度指標，納入可讀性公 式之計算。由表一可以看出可讀性公式著重於利用如詞彙與句長等淺顯的語言特 徵作為指標，有學者因此認為以這些語言特徵預測文本可讀性，並沒有強而有力 的證據。  公式名稱 Flesch Reading Ease （Flesch, 1948） New Reading Ease （Flesch, 1951） Gunning FOG （Gunning, 1952） Spache （Spache, 1953） Powers-Summer-Kearl （Power et al., 1958） Fry Graph （Fry, 1968） SMOG （McLaughlin, 1969）  計算公式 Reading ease = 206.876 - (1.015 × 平均句長) (84.6 × 平均音節數) Reading ease = 1.599 × 每百詞之單音節詞比率 - 1.015 × 每句平均詞數 - 31.517  採用指標 句長、音節數 單音節數、詞數  Grade level = 0.4 × (平均句長 + 100 × 總難詞詞數)  句長、難詞比率  Grade level = 0.839 + (0.086 × 難詞百分比) + (0.141 × 平均句長)  句長、難詞比率  Grade Level = -2.2029 + 0.0778 × 平均句長 + 句長、音節數  0.455 × 音節數  Reading Age = -2.7971 + 0.0778 × 平均句長 +  0.455 × 音節數  計算 3 篇 100 詞文章的平均句數與音節數；將數 句數、音節數 值在 Fry Graph 中做記號找出閱讀年級  SMOG Grade = 1.0430 ×  多音節詞數、句數 √三音節以上的詞數 × (320) + 3.1291 + 3.1291)  74  Grade Level = 20 - (單音節的詞數) 10  FORCAST （Caylor et al., 1973）  Reading Age = 25 - (單音節的詞數) years → 150 詞 10  音節數  Reading Age = 25 - (單音節的詞數) years 6.67  →  100 詞  Flesch Grade Level Grade Level = -15.59 + (0.39 × 平均句長) + (11.8 句長、音節數 （Kincaid et al., 1975） × 平均音節數)  The New Dale-Chall Grade Level = (0.1579 × 總難詞詞數) + (0.0496 × 平均 （Chall and Dale, 1995） 句長) + 3.6365  難詞比率、句長  表一、西方常見的可讀性公式與採用指標  中文可讀性研究以迴歸分析法發展可讀性公式，將可讀性指標逐一刪去，最 後只留下少數影響最大的指標。另外，亦有研究使用支援向量機建置之模型來預 估文章適合閱讀的年級（宋曜廷等人，2013）。由表二則可看出研究者多採用較 為表淺之指標建立公式。因此，傳統中文可讀性研究，在指標的選取上與拼音文 字系統常見的指標並無顯著差異。  公式名稱 Yang（1970） 陳世敏（1970） 荊溪昱（1992） 荊溪昱（1995） 宋曜廷等人 （2013）  計算式  採用指標  年級 = 0.1788 × 筆劃數超過 10 劃百分比 + 0.1432 × 平均句長 + 0.6375 × 難字百分比  筆劃、難字比率、句長  學期 = 14.95961 + 39.07746 × 詞彙數 - 2.48491 × 平均筆劃數 + 1.11506 × 句數  詞彙數、句數、筆劃數  年級 = （每句平均字數 + 難字數） × 0.7  句長、難字數  年級 = 5.43035627 + 0.00657347 × 課文長度 +  0.02443016 × 平均句長 - 5.56746245 × 常用字比 課文長度、句長、常用  率 + 1.38315091 × 詩歌體 - 1.07299966 × 對白 字比率、文體  文體  年級 = 8.76105604 + 0.00272438 × 課文長度 +  0.07866782 × 平均句長 - 8.9311010 × 常用字比 課文長度、句長、常用  率 + 0.42920182 × 詩歌體 + 3.23677141 × 文言 字比率、文體  文體  年級 = 4.53 + 0.01 × 難詞數 – 0.86 × 單句數 難詞數、單句數比率、  比率 – 1.45 × 實詞頻對數平均 + 0.02 × 人稱 實詞頻對數平均、人稱  代名詞數  代名詞數  表二、中文常見的可讀性公式與採用指標  75  (三) 、可讀性模型分析比較 傳統可讀性公式多為線性迴歸模型，納入不同的特徵為自變項，估算文章難度， 或提供公式估算文本適合閱讀的年級。迴歸分析（Regression Analysis）是一種 統計學上分析數據的方法，目的在於了解兩個或多個變數間是否相關，並建立數 學模型以便觀察特定變數來預測研究者感興趣的變數[7]。更明確地，迴歸分析 是利用依變數 Y 與自變數 X 之間的關係所建立的模型，期望找出一條最能夠代 表所有觀測資料的函數（迴歸估計式）[7]。而多元迴歸即為探討一個依變數和 多個自變數間的關係，如：Y = β0 + β1X1 + β2X2 + … + βnXn，其中 β0 為常數， β1, … ,βn 為迴歸係數[8]。 近年來，許多研究開始將可讀性議題視為一種機械學習的問題。藉由抽取自 文本的各類可讀性特徵，透過支援向量機建立預測模型後，就可用於預測測試資 料集文件之可讀性。支援向量機將原始資料轉換到更高的維度，利用在訓練資料 集中所謂的小樣本資料（Support Vectors）找到超平面，用以分類資料[1]。支援 向量機主要是在尋找具有最大邊界的超平面，因為其具有較高的分類準確性[9]。 目前支援向量機相關研究常使用由台大林智仁教授所開發的 LIBSVM[10]開放原 始碼軟體為工具，經由準備資料集、訓練模型、預測新資料所屬之類別等步驟， 得到測試之準確率。 (四) 、可讀性近來研究趨勢 隨著技術的進步，納入更多複雜的可讀性指標變得可行。Graesser 等人為了改良 傳統教科書的寫作方式，並提供符合學生閱讀能力的教材，發展了線上多文本特 徵分析器（Coh-Metrix）[2, 11]，可抽取多項文本特徵。 「中文文本自動化分析系統」[12]為 Coh-Metrix 之中文版本，由國立臺中教 育大學教育測驗統計研究所與特殊教育學系合作，參考 Coh-Metrix 分析建置的 指標應用於中文領域，結合中文詞彙與文章之特性，發展中文文本自動化分析指 標，以幫助使用者分析文章的特性作為讀本選擇之參考。 許多研究亦嘗試根據認知理論來分析文本的難度，積極探討與文本更相關的 進階指標，並發展新的方式自動化地處理文本，像 WordNe（t Fellbaum, 1998）[13]， 即分析詞、句子、段落及篇章等較大範圍的文本多層次之凝聚特性與文章難度的 關係[1]。相較於 WordNet，中文亦有類似的詞庫。中文詞彙網路（Chinese Wordnet） 計畫（黃居仁、謝舒凱，2010）[14]，目的是在提供完整的中文詞義（Sense）區 分與詞彙語意關係知識庫。 三、 特徵探討 (一) 、基礎特徵 本研究以〈中文文本可讀性探討：指標選取、模型建立與效度驗證〉[1]中 之指標為基礎，且經由宋曜廷等人發展的文本可讀性指標自動分析化系統 76  （Chinese Readability Index Explorer, CRIE）[15]擷取文章可讀性指標的數值。其 所包括的指標請參閱表三。其中負向連接詞如「然」、「卻」、「否則」等。 上述特徵為參考中西方文獻回顧，所發展適合中文特性的可讀性指標。然而 其所包含之深層類型指標仍較為稀少，故本研究以此為基礎，另外結合其他指標， 以期達到考慮文本難易度更深層次因素之目的。 (二) 、句法分析與詞性特徵 此節探討由 Feng 等人[16]所提出的句法分析（Syntactic Analysis）特徵及詞 性標記（Part-of-Speech, POS）特徵。其所包括的指標請參閱表四。 語法（Gram-mar）是語言單位的結構規則；也可以說：語法是詞、詞組、 子句、句子的結構和運用法則[17]。語法特性只有分析句子含意時才得以揭露， 因此句法分析就顯得相當重要。 詞性是以個別詞彙為對象，根據其語法作用，兼顧其意義，所分類得到的結 果[18]。由於中文語法特性的緣故，同一詞彙可能有不同詞性，如「縱橫交錯」 與「稍縱即逝」中的「縱」字因詞性不同，其意義也不同，故此種情況容易造成 理解上的困難。  類別 詞彙類指標 詞彙數量 詞彙豐富性 詞彙頻率 詞彙長度  指標編號與指標名稱 1. 字數 2. 詞數 3. 相異詞數比率 4. 實詞密度 5. 實詞頻對數平均 6. 難詞數 7. 低筆劃字元數 8. 中筆劃字元數 9. 高筆劃字元數 10. 字元平均筆畫數 11. 二字詞數 12. 三字詞數  定義 加總文章中的字數 計算文章中的詞數 相異詞數除以詞總數 實詞總數除以詞總數 計算文章的實詞在整個資料集出現的 頻率取對數後平均 加總文章中不在常用詞表的詞數 加總文章中筆劃數介於 1~10 筆劃的 字元數 加總文章中筆劃數介於 11~20 筆劃的 字元數 加總文章中筆劃數介於 21 筆劃以上 的字元數 計算文章中的字元平均筆劃數 加總文章中的二字元詞 加總文章中的三字元詞  77  語意類指標 句法類指標 文章凝聚性指標 指稱詞 連接詞  13. 實詞數 14. 否定詞 15. 複雜語意類別句子數 16. 句平均詞數 17. 單句數比率 18. 名詞片語修飾語數 19. 名詞片語比率  加總文章中的實詞數 加總文章中的否定詞數 加總文章中複雜語意句數 詞數除以句數 計算文章中的單句數比例 計算文章中名詞片語的修飾語平均數 計算文章中每句中名詞片語數與詞數 比之平均  20. 代名詞數 21. 人稱代名詞數 22. 連接詞數 23. 正向連接詞數 24. 負向連接詞數  加總文章中的代名詞 加總文章中的人稱代名詞 加總文章中的連接詞 加總文章中的正向連接詞 加總文章中的負向連接詞  表三、本研究採用之基礎特徵名稱與定義  類別  指標編號與指標名稱  1. Number of the NPs  2. Number of NPs per sentence  Parsed Syntactic Features  3. Number of the VPs  4. Number of VPs per sentence  5. Number of non-terminal nodes per parse tree  6. Fraction of tokens labeled as noun  POS-based Features  7. Fraction of tokens labeled as preposition 8. Number of noun tokens per sentence  9. Number of preposition tokens per sentence  表四、本研究採用之句法分析與詞性特徵名稱與定義  78  圖二、CBOW 與 Skip-gram 模型示意圖[21] (三) 、表示法特徵 要將自然語言的問題轉變成為機器學習的問題，首先便須把這些符號數學化。傳 統的做法為把每個詞表示成一個很長的向量，向量的維度是全部詞的數目，其中 除了該詞的維度值為 1，其餘皆為 0，這個向量就代表了當前的詞（One-hot Representation）[19]。 深 度 學 習 （ Deep Learning ） 領 域 中 則 利 用 分 散 式 表 示 法 （ Distributed Representation）的方式，將每一個詞以一個低維度的實數向量表示之，稱為詞表 示法（Word Representation or Word Embedding）[19]。此表示法向量中各維度皆 有值，因此讓兩個意思相近的詞在向量空間上的距離縮短。 Google 在 2013 年公開的 Word2Vec 工具[20]，即是用於求取詞向量表示法。 常見的詞向量表示法模型有兩種：連續型詞袋模型（Continuous Bag-of-Words, CBOW）與跳躍式模型（Skip-gram）。連續型詞袋模型的訓練目標是給定一個詞 的上下文，以預測這個詞出現的機率；在跳躍式模型中，訓練目標則是給定一個 詞，預測其上下文中的詞。由於許多研究指出跳躍式模型的效果較佳，故本研究 利用跳躍式模型訓練詞向量表示法及詞性向量表示法作為特徵。 (四) 、語意資訊特徵 本研究參考〈句結構樹中的語意角色〉[22]中之語意角色為指標，並利用中研院 之中文剖析系統將文章進行語意角色的擷取。其所包括的指標請參閱表五。 (五) 、寫作程度特徵 此節探討由 Louis 等人[23]所提出的優良寫作概念（Great Writing），並將其應用 於可讀性研究。其所包括的指標請參閱表六。其中 Visual nature of articles 類別是經 由將 ESP Game Dataset 英文標記資料，隨機抽取五十個單字並轉譯成中文作為描 述生動的詞彙。 79  類別  指標編號與指標名稱  1. apposition  修飾物體名詞  2. possessor 3. predicator  4. property  5. quantifier  6. companion  7. comparison  8. goal  9. topic  修飾事件動詞  10. addition 11. alternative  12. complement  13. conclusion  14. contrast  15. reason  表五、本研究採用之語意資訊特徵名稱與定義  類別 Visual nature of articles Beautiful language Affective content  指標編號與指標名稱 ESP Game Dataset（指標 1-50） 自行蒐集之優美詞彙及成語（指標 51-100） 台灣地區華人情緒與相關心理生理資料庫—中文情 緒詞常模研究[24]（指標 101-150）  表六、本研究採用之寫作程度特徵名稱與定義  四、 實驗設置與結果 (一) 、實驗資料 中小學國語文教科書選自 98 年度台灣 H、K、N 三大出版社所出版的 1~9 年級 （共 18 冊）審定版國中小國語文教科書。各版本在各年級的文章數詳見表七。 優良課外讀物選自文化部歷屆「中小學生優良課外讀物」獲選書籍[25]，以書單 中標示之適讀年齡為分類正確答案。各級別的文章數詳見表八。 (二) 、實驗設定 以下兩節實驗各分為兩部份：第一部份實驗以逐步迴歸方式，以年級當成效標變 項，24 個中文可讀性指標為預測變項，以 SPSS 22.0 軟體建立可讀性數學模型計 算各篇課文可讀性分數，以預測其屬於哪個年級。第二部份實驗中運用支援向量 機學習並預測資料類別。 80  (三) 、國語文教科書實驗  年級 = 11.701 - 5.362 × 領域實詞頻對數平均 + 0.176 × 負向連接詞數  + 0.167 × 句平均詞數 + 0.024 × 代名詞數  (1)  式(1)為國語文教科書之迴歸公式。在此先比較以不同區間劃定年級值之預測正 確性，結果如表九所示。其中之 0.0 意指若逐步迴歸之分數為 0~1 間即定為 1； 0.1 意指若逐步迴歸之分數為 0.1~1.1 間即定為 1，依此類推。由表九得知以 0.9~1.9 為區間劃分正確率最高，故以此測試文章所屬年級的正確性，預測結果如表十所 示。由表十可看出以三至六年級之預測正確性較高，且各年級分類結果皆偏向較 低年級，尤以七至九年級最為明顯，造成此結果的原因可能為所使用的特徵較為 表淺，對國小高年級與國中文章差異不大，故無法有效分類較高年級之文本。  年級 一 出版社  H  22  K  22  N  20  總數  64  二  三  四  五  六  七  八  28  28  28  33  27  31  32  28  28  29  36  27  28  29  28  24  29  30  29  31  30  84  80  86  99  83  90  91  表七、國語文教科書各年級文章數  九 總數 23 252 25 252 24 245 72 749  低年級  中年級  高年級  國中  總數  20  20  20  20  80  表八、優良課外讀物各級別文章數  區間  0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  
English of Academic Writing (EAW) is essential to the research community for sharing knowledge. Research documents using EAW, especially the abstract and introduction, may *此研究由科技部資助，編號為：MOST-103-2511-S-007-002-MY3 1通訊作者：徐嘉連 Jia-Lien Hsu (E-mail: alien@csie.fju.edu.tw) 87  follow a simple and succinct picture of the organizational patterns, called move. This paper introduces a method for computational analysis of move structures, the Background-PurposeMethod-Result-Conclusion in this paper, in abstracts and introductions of research documents, instead of manually time-consuming and labor-intensive analysis process. In our approach, sentences in a given abstract and introduction are automatically analyzed and labeled with a specific move (i.e., B-P-M-R-C in this paper) to reveal various rhetorical functions. As a result, it is expected that the automatic analytical tool for move structures will facilitate non-native speakers or novice writers to be aware of appropriate move structures and internalize relevant knowledge to improve their writing. In this paper, we propose a Bayesian approach to determine move tags for research articles. The approach consists of two phases, training phase and testing phase. In the training phase, we build a Bayesian model based on a couples of given initial patterns and the corpus, a subset of CiteSeerX. In the beginning, the priori probability of Bayesian model solely relies on initial patterns. Subsequently, with respect to the corpus, we process each document one by one: extract features, determine tags, and update the Bayesian model iteratively. In the testing phase, we compare our results with tags which are manually assigned by the experts. In our experiments, the promising accuracy of the proposed approach reaches 56%. Keyword: Academic English Writing, Assisted Writing, Move Tag Analysis 一、緒論 自然語言處理是近幾年學術所關心的議題，在科技尚未發展以前，語言處理幾乎靠人力檢 查與校正拼字與文法錯誤，但是靠人力，則會產生人為的失誤，意思是指並非人工檢查就表示 寫作的用詞與語法正確，所以採用機器學習來替代人工的方式，相較於機器學習，人工校正或 是處理文字相對花費較多時間。 英文是在學術上主要溝通的語言，所以非英語體系的國家，對於英文寫作這一部分相較之 下，發生文法與拼字的錯誤率會明顯提高，因此在資訊發達的世代，學術機構開始收集寫作資 料，譬如：英文檢定考的作文 (ETS)、學生寫的作文資料集 (CLEC) 與維基百科的編輯紀錄等 等，有這些語料集 (Corpus)，學者們開始從事多方面的語言處理與分析研究。利用語料庫，分 析英語的用法 (搭配詞、文法)，運用統計，找出大部分人們所使用的句法，嘗試著從數據當中 找到理論，藉此幫助學習，以及提升寫作上的效率。 在學術論文中，簡介此一章節，通常會描述：問題的背景、主要目的、解決方法、結果與 結論，此修詞結構的組成稱之為「文步」。在過去的研究中 [1–3]，針對論文簡介定義出四個 文步，包括：問題 (Problem)、方法 (Solution)、評估 (Evaluation) 與結論 (Conclusion) 等部 分。美國國家標準協會 (American National Standard Institute, ANSI) [4]，審核並規範寫作的 文步結構為目的 (Problem)、方法 (Method)、結果 (Result) 與結論 (Conclusion)。Swales [5] 定義在論文寫作依循的三大文步修辭結構 (Creating a Research Space, CARS)，包括：為建 立研究領域 (Establishing a research territory)、建立利基 (Establishing a niche)、占領利基 (Occupying the niche)，並在每一個文步修辭結構之下定義細節，藉此幫助描述文章內容。 特別針對學術英文寫作，Glasman-Deal [6] 提出寫作上文步模組，包括：介紹、方法、結 果、討論。Weissberg & Buker [7] 定義學術論文寫作文步為 BPMRC，即背景 (Background, B)、目的 (Purpose, P )、方法 (Method, M )、結果 (Result, R)、討論 (Conclusion, C)。 在本篇論文使用 Weissberg & Buker 提出文章的文步架構 (背景、目的、方法、結果、 結論)，利用大量的學術論文資料 (CiteSeerX) 與少量初始規則，訓練貝氏模型 (Bayesian approach)，學習如何判別句子所屬的文步。 為了得知訓練完畢的貝氏模型所提供文步的精確度，則利用單一篇新的學術論文簡介，透 過貝氏分類器進行文步標定，最終由人為判別文步的正確性。 本論文接著的部分會先探討相關研究 (Section 2)，進而描敘分類器自動學習標定文步的過 程 (Section 3)，與實驗設計、結果 (Section 4)。最後，討論未來的研究方向與結論 (Section 5)。 88  二、相關研究 隨著資訊發展，為了讓資訊交流快速，關於自然語言處理為相當重要的研究領域，在純文 字的應用包括機器翻譯、拼字校正、資料檢索等等。近年來學者對於學術論文或是期刊，有進 一步的研究 (Swales & Feak, 2004)。主要針對論文的段落與句子進行人為的分析研究，經過歸 納之後提出關於論文修辭的架構規則-「文步」。在本研究中，則是針對論文的「簡介」這一個 章節做分析，提出自動化分析論文文步結構的方法。 大部分論文簡介有著簡單文步結構–IMRD [8]，即為介紹 (Introduction)、方法 (Method)、 結果 (Result)、討論 (Discussion)，許多學者也定義出不同的論文文步結構，例如 Swales [5] 為 簡介此小節提出 CARS(Creating a Research Space) 模組，CARS 主要為 3 大文步並細分為 11 文步，使得許多學者使用 CARS 模組探討寫作上的修辭方法，Weissberg & Buker [7] 整理出 BPMRC 文步結構，即背景 (Background)、目的 (Purpose)、方法 (Method)、結果 (Result)、 結論 (Conclusion)，為學者與作者提供研究方向與寫作建議。 近幾年來，有許多學者採用不同機器學習的方式訓練文步分類器，例如 Teufel & Moens [9] 利用簡易的貝氏分類器 (Naive Bayesian Model, NBM) 透過修辭的狀態與關聯針對論文全文 進行文步分類。Ling [10] 提出隱馬可夫模型 (Hidden Markov Model, HMM) 利用統計機率去 做文步標註，Wu & Jason S. [11] 提出一套系統 (CARE)，利用 HMM 標記文步。Shimbo [3] 透過 MEDLINE，提出一套系統，讓使用者可以搜尋簡介特定的文步，此系統利用支撐向量機 (Suport Vector Machines, SVM)，系統將簡介分為四個部分，為目的、方法、結果、結論，每 個句子可以利用位置找出上下文，作為判別文步的依據。Yamamoto & Takagi [12] 將簡介中的 句子分為背景、目的、方法、結果、結論，訓練線性 SVM 找出動詞時態與相對的句子位置當 作分類依據，進行文步標註。 在本文當中，所採用的機器學習演算法為貝氏定理 (Bayesian)，貝氏定理在自然語言處理 上常被用於統計式翻譯 (Statistical Machine Translation, SMT)，在條件機率理論上，預測原 文被翻譯為譯文的方式，去做機器訓練 (Jia Xu, 2008) [13]，利用大量的論文資訊，運用貝氏 定理採取半監督式分析法，預測一篇簡介的句子，屬於何種文步來做討論。 與本文最相關的研究，為 Guan-Cheng Huang [14] 的論文研究，主要的區別為所採用的分 類架構有所不同，Guan-Cheng Huang 提出：背景（領域、缺口、前人研究）、本論文 (目的、 方法、結果）、討論（和前人研究的比較與對照）與文節結構（論文組織、圖表的指示、內容的 預告與回顧）等四種文步，而本篇所採用的文步為五種 (背景、目的、方法、結果、結論)，在 應用上，訓練文步分類器的演算法有些差別，本文是採用貝氏分類 (Bayesian) 而 Guan-Cheng Huang 提出最大熵模型 (Maximum Entropy, ME)，差別在於貝氏在運算的一開始需要先驗機 率條件，依據先驗條件推理出文步機率，而最大熵模型則不需先驗條件，所以會平均分佈，不 傾向於任何文步，但在訓練過程中接觸到其他訊息，則會調整文步的機率分佈。 相對於前人研究文步分析的文獻，在本文當中提出一套自動學習系統，利用專家已經歸納 的文步片語整理成 N-連詞 (n-gram)，以降低人工標示的成本，在訓練的過程中，利用文步特 徵，自動將句子標示，使得系統可以分類文步並從中擴充字詞，利用自動化文步標示而得到的 字詞，套用到英文輔助寫作系統，幫助學生寫學術論文。 三、方法 為了提供使用者在寫學術論文時，在不同章節 (文步) 可以使用較正確的字詞，我們必須擁 有大量已經被標註的文步字詞來做寫作上的提示，而人工自行標註字詞的文步需花費大量的時 間，因此，我們採取專家整理過的字詞透過自動學習的方法，省去人工標註所需花的時間，我 們將問題定義如下。 我們將句子經過 Genia Tagger 斷字之後，採用三種特徵訓練出貝氏模型 (OW, BF, BPC)， 以迭代 (bootstrapping) 的方法擴增貝氏模型，計算之後，將一篇文章當中的句子，單獨觀察 一種文步，找出在此文步機率最高的句子進行文步標註，避免一篇文章當中只有一種文步的情 形發生，將已被標定文步的句子分為 N-連詞 (S = {ng1, ng2, ... }) 回饋到初始表，藉以達到訓 練的效果。 在測試階段，則會選取一篇新的文章簡介，透過訓練完畢的模型結果，評估文步標註的精 確率。 在此章節，敘述我們所使用的演算法，包含貝氏定理所需要的先驗機率與文步的挑選，並 89  問題陳述 給定：以學術文章組成的語料集 (Corpus) 與初始訓練規則 (Initial pattern) 我們先計算一個初始模型 給定：一篇學術文章 D (D ∈ Corpus) 目標：為單一篇文章 (D = {S1, S2, ... }) 中每一句子 Si 判定句子文步 標上文步標籤 (move-tag = {B, P , M , R, C}) 同時，新增或更新規則 介紹系統架構圖與模組訓練的過程。 3.1 系統架構 圖 1: 系統架構 (System architecture) 參考系統架構圖 (圖 1)，分為兩大部分: 一部分是利用 CiteSeerX 語料庫，訓練貝氏分類 器，另一部分則是使用訓練好的貝氏分類器預測新文章的句子。 在訓練階段，從 Glasman-Deal 此書當中依照文步所提供的資訊，擷取 155 句 N-連詞當作 初始文步訓練規則，而在資料方面則是取專門收集學術論文簡介的語料庫 (CiteSeerX)，須先 將語料庫逐篇經過 Genia Tagger 進行斷字處理，利用程式將簡介中的句子分割成一句一句，然 後再把句子依照 Genia Tagger 提供的詞性標註 (Part of Speech, POS)、字根還原 (Base form) 與語意區塊 (chunk) 做預先處理，將處理過後的句子分為 N-連詞，依照初始文步當作依據，經 過分析將語料庫所提供句子字詞進行文步標註，並回饋到初始表當中，經過反覆訓練的過程， 擴增已被標記的 N-連詞當作下一次計算的依據。 在測試階段，選取新的一篇文章，一樣使用 Genia Tagger 進行預先處理，將訓練階段得到 大量被標記文步的 N-連詞，當作先驗資訊，測試文章經過計算之後，逐句所得的文步標籤是 否正確，進而得知方法的效率。 3.2 特徵選取 本文中，挑選 BPMRC 此文步架構當作句子分類類別，從語料庫逐篇處理句子，若篇幅當中的 句數少於五句，則會忽略不做處理。 而文章中的句子 (S) 會經過 Genia tagger 處理每個字詞 (W)，Genia tagger 會提供字詞的 一些特徵，例如詞性標記 (Part-of-Speech, POS)、意元集組 (Chunk)。例如一篇文章中其中一 句 (S1) 為”Glyoxysomal citrate synthase in pumpkin is synthesized as a precursor that has one 90  cleavable presequence at its N-terminal end.” 經過 enia tagger 分析之後 (表 1)，我們依照結 果，將句子整理成三種表達方式，分別為 1. 原始資料 (OW: Original word) 將句子保留原始資料，包含過去式、複數等等，但是捨去部分符號，使得句子只由單字 組成，所以原始句子 (S1)，將會轉成如下： ”Glyoxysomal citrate synthase in pumpkin is synthesized as a precursor that has one cleavable presequence at its N-terminal end.” 2. 字根還原 (BF: Base form) 將句子包含的單字，還原字根，使得句子被簡化，所以原始句子 (S1)，將會轉成如下： ”Glyoxysomal citrate synthase in pumpkin be synthesize as a precursor that have one cleavable presequence at its N-terminal end.” 3. 利用意元集合與詞性 (BPC: Base form & POS & Chunk) 透過 POS 給的規則，將代表數字 (CD、LS) 或是非英語單字 (FW) 的字詞換成標籤 (one→CD)，而符號 (SYM、$、:) 則是忽略，並考慮 Chunk，找出單字的前後屬性是否 為一個組合 Base form 使得單字可以統一，則句子會轉成如下： ”Glyoxysomal citrate synthase in pumpkin be synthesize as a precursor that have CD cleavable presequence at its N-terminal end.”  表 1: 將 S1 = ”Glyoxysomal...” ，經過 Genia Tagger 分析之結果  Original word Glyoxysomal citrate synthase in pumpkin is synthesized as one precursor that has … . (Period)  Base form Glyoxysomal citrate synthase in pumpkin be synthesize as one precursor that have … . (Period)  POS JJ NN NN IN NN VBZ VBN IN CD NN WDT VBZ … .  Chunk B-NP I-NP I-NP B-PP B-NP B-VP I-VP B-PP B-NP I-NP B-NP B-VP … O  Named entity (NE) B-protein I-protein O O O O O O O O O O … O  3.3 初始規則 針對初始 N-連詞的選用，採用 Glasman-Deal 所撰寫的教科書 [6]，此書歸納出在不同文步上 該如何建立一個寫作架構與在文步上所該使用的詞彙，從中挑選，我們將選取出的 N-連詞 (參 考表 2，依照文步給予初始值，比如”a basic issue for” 在書中的建議在背景 (B) 當中使用，所 以代表此 N-連詞在背景出現次數為 1(表 3)。利用被標註分類的 N-連詞，當作訓練資料，運用 貝氏定理計算而自動產生大量標註完的論文句子，將句子分為 N-連詞，回饋於初始值，當作 下一次訓練資料，而最後將訓練完的結果，進一步的分析。 所以在實驗當中，選出 155 個詞彙片語作為 N-連詞 (N-gram)，當作初始的特徵參數，其 初始句數的分布如表 4。 91  表 2: 從 Glasman-Deal 撰寫的書 [6] 所擷取出部分的初始規則 (Initial pattern) Pattern a basic issue for approach was developed by majority of the tests … in future it is  表 3: 將初始規則給予出現次數，稱之為 Count table (CT )  Pattern  BP M RC  a basic issue for  100  approach was developed by 0 1 0  majority of the tests  001  …  ………  in future it is  000  00 00 00 …… 01  3.4 貝氏方法 首先，貝氏定理需要有先驗機率，才能計算與分析，所以利用書本在文步架構上推薦的寫法， 當作貝氏的特徵參數，再來，將一篇文章的簡介當中的字詞經過處理，使得文章當中的特殊符 號不影響單字，利用貝氏定理計算文章中的每個句子去預測為背景、目的、方法、結果、結論 的機率，當一篇文章的所有句子都計算完畢，才從所有句子當中是關於背景此文步最大值的句 子標註為背景，已被標註的句子則不能重複被標註，當句子都被標註完，則將獲得辨識結果， 回饋到一開始的先驗特徵參數。 從 Corpus 取一篇簡介 (D1)，因為我們的分類模型為五個文步，若該篇簡介數句少於五， 則忽略該篇文章，而句數超過五句，就定為一篇完整的簡介，而進一步分析。如表 5為擷取的 一篇完整篇幅，接著將文章當中的一個句子 (S1) 分別計算出可能為背景 (B)、目的 (P )、方法 (M )、結果 (R)、結論 (C) 的機率。  3.4.1 計算文步機率  以 S1 為例，我們將分別計算文步的機率值。  P (move-tag|S1)  =  P (move-tag) × P (S1|move-tag) , P (S1)  when  move-tag  ∈  {B, P, M, R, C}  (1)  句子會計算每個文步的機率，由於每個文步的計算方法都相同，所以在往後的敘述將已背景 (B) 文步做代表。 而本文中給予的先驗特徵參數是給予 N-連詞，因為 S1 假設為一組獨立 N-連詞所組成 (S1 is approimated by set of n-grams as follows: {ng1, ng2, ...}) 所以要計算句子的所屬的文步機 率，在此將句子劃分為 N-連詞來做運算，例如 S1 近似為 n 個 N-連詞所組成。  S1 ← {ng1, ng2, ..., ngn}  (2)  92  表 4: 初始規則 (Initial pattern) 中，各種文步的次數分佈 move-tag B P M R C 次數 25 34 33 41 22  表 5: 範例文章 D1 = {S1, S2, ... , S6} S Sentence S1 Glyoxysomal citrate synthase in pumpkin is synthesized as one …. S2 To investigate the role of the presequence in the …. S3 Lmmunogold labeling and cell fractionation studies …. S4 The chimeric protein was transported to functionally …. S5 These observations indicated that the transport of …. S6 Site-directed mutagenesis of the conserved amino acids in ….  所以 S1 的機率定義為  P (S1) ≃ P (ng1) × P (ng2)... × P (ngn)  (3)  而 P (S1|B) 的條件機率定義為  P (S1|B) ≃ P (ng1|B) × P (ng2|B) × ...P (ngn|B)  (4)  根據上述的定義，將公式 (1)，定義為  P (B|S1)  ≃  P (B)  × P (ng1|B) × P (ng2|B) P (ng1) × P (ng2) × ...  ×  ...  (5)  3.4.2 計算 N-連詞機率 句子經過分割之後，得到其中一段 N-連詞 (ng1)，例如為”ng1: glyoxysomal citrate synthase in”，先計算 ng1 出現的機率。  n-gram  BPMRC  ng1 glyoxysomal citrate synthase in B1 P1 M1 R1 C1  ng2 a basic issue for  B2 P2 M2 R2 C2  ……  … …… … …  ngm role of the presequence  Bm Pm Mm Rm Cm  P (ng1)  =  ∑miB=11(+BiP+1  + M1 + Pi + Mi  R1 + + Ri  C1 + Ci)  (6)  則會判斷此 ng1 是否存在於初始表 (表 3)，若存在於初始表 (CT )，則會計算 N-連詞在該 文步 (B) 次數出現的機率。  93  P (ng1|B)  =  ∑mB1 i=1  Bi  (7)  若該 N-連詞不存在於初始表格或是在未曾出現於某文步，比如”a basic issue for” 此 N-連 詞不曾出現於結論 (C)，則會給予極小值 (δ = 10−8) 當作機率。將每個 N-連詞，對照初始規 則表 (CT )，因此 S1 透過運算則會得近似所屬的文步機率值。而各文步的機率，則是依照文步 次數做為依據。 由於句子組成單字的多寡，會影響計算上的公平性，所以我們將結果正規化。  normalized  (P (B|S1))  =  #  P (B|S1) of n-gram in  S1  (8)  3.4.3 文步標定 在文步標定上，在本文中，先將一篇文章 (D1) 中所有句子的各文步機率計算完畢，才逐句標 定文步。 而每個文步要標定幾個句子，則是依據給定的比例去做計算，由於句數不能為小數，所以 取四捨五入的方法。  表 6: 一篇文章中 (D1)，文步句數算法。  move-tag B P M C R  文章句數比例 0.15 × 6 = 0.9 0.20 × 6 = 1.2 0.30 × 6 = 1.8 0.15 × 6 = 0.9 6 − (1 + 1 + 2 + 1)  句數 1 1 2 1 1  為了避免某一文步造成多數制 (Majority rule) 結果，我們根據文步在 Corpus 內文寫的比 例多寡，依序標定文步 (以表 6為例，先標定 B 往後順序為 C → P → R → M )。由於我們是 由一篇文章判定句子文步，依照比例，我們先標定為 B 的句子。  ∵ B2 = max{B1, B2, ..., B6}  ∴ S2 ← B  (9)  若該句 (S2) 已經被標上標籤 (B)，則將句子移除序列中，經由表 6計算，文章當中為 B 的 內容為一句，則換標定下一個文步 C。  表 7: 經過第一次文步標定  Sentence B P M R C  S1  B1 P1 M1 R1 C1  S2  B1 P1 M1 R1 C1  S3  B3 P3 M3 R3 C3  S4  B4 P4 M4 R4 C4  S5  B5 P5 M5 R5 C5  S6  B6 P6 M6 R6 C6  94  反覆標定過程，將文章當中的句子標上文步。  ∵ C6 = max{C1, C3, ..., C6}  ∴ S6 ← C  (10)  表 8: 經過第二次標定  Sentence B P M R C  S1  B1 P1 M1 R1 C1  S2  B2 P2 M2 R2 C2  S3  B3 P3 M3 R3 C3  S4  B4 P4 M4 R4 C4  S5  B5 P5 M5 R5 C5  S6  B6 P6 M6 R6 C6  當一篇文章當中所包含的句子都已經標上文步，而我們也會根據結果，更新 CT 相對應 的規則 (表 3)。假設 S1 被標定為 B，而句子當中包含一個 N-連詞”ng: glyoxysomal citrate synthase in”，不存在 CT ，依照句子被標定的文步，新增 ng 至 CT 中並在 B 給予初始次數 (表 9)。  表 9: 新增規則  pattern (4-gram)  BP M RC  glyoxysomal citrate synthase in 1 0 0 0 0  …  …… … ……  若 ng 存在於 CT 中，則會依照 S1 被標定的結果，更新 ng 在該文步出現的次數 (表 10)。 請注意，在這步驟中，我們僅是將前一步驟中、用貝氏方法判定的文步結果（沒有人為介入 判定），加回 CT 中。我們並不立即判定所標定的文步是否為正確，而是以不斷迭代（iterative） 的方式，利用貝氏方法，來抓住訓練資料（training data）的特性。  表 10: 更新規則  pattern (4-gram)  B P M RC  glyoxysomal citrate synthase in Bi + 1 0 0 0 0  …  … ………  四、實驗 在本節中，我們將討論實驗設定與結果討論。  95  4.1 語料庫 本文針對輔助英文學術論文寫作，因此我們採用專門收集發表過的學術論文語料集 (CiteSeerX)1。CiteSeerX 是一個關於文獻的搜尋引擎，在 1997 年，由美國普林斯頓大學開發 CiteSeer，建立一個數位圖書館，由於 CiteSeer 只能收集公開的文件，使得所收集文章領域有 限，為了克服侷限性，針對系統架構重新定向 (CiteSeerX)，於 2007 年採用機器學習的方法， 自動辨識網路上存在的論文，然後依照索引標示文章，透過引文的影響，連接每篇文章。 CiteSeerX 總共擁有 138 萬多篇的文獻，主要的內容為科學領域（包含資工和生醫領域）， 而這些資料來源通常為 PDF 格式，經過自動辨識轉檔成文字，因此語料庫裏頭包含許多換行 連字符號、特殊符號等雜訊，所以在使用資料之前，我們透過文字處理，將冗餘的符號或是日 期格式捨去，進而得到較完善的一篇論文。  4.2 實驗設定  參考系統架構圖 (圖 1)，我們利用初始規則 (Initial pattern)，分析語料庫 (CiteSeerX) 提供的 文章，逐篇訓練語言模組，每當經過一千篇訓練的語言模組，則會測試精確度，在本論文當 中，取兩萬篇當訓練資料。 在測試階段，事先從語料庫隨機提出 20 篇尚未經過訓練的文章，經過專家逐句標註文步， 我們透過四個專家針對此 20 篇 (共 185 句) 逐句給予文步標籤，挑出其中三個人以上給予句子 的標籤相同來評估資料的準確性 (三人以上相同句數共 142 句)。 我們將 20 篇文章進行測試，將句子標上標籤。之後定義如下精確率，依照 142 句正確答 案，找出標上正確文步的句數。  # of sentences with correct move-tag  Accuracy =  .  (11)  142  4.3 實驗結果 本文利用 CiteSeerX 提供的資料，計算每經過一千篇的訓練後，則會增加多少 N-連詞的先驗 規則，因資料經過 Genia Tagger 處理之後會提供資料原始字詞 (Original Word)、字根還原 (Based form)、詞性標記 (POS) 等資訊，則訓練方法給的文字資料為此三種方式，透過運算得 到的結果。  圖 2: 經由訓練，增加的規則數 每經過一千篇的訓練，則 CT 會增加約六萬的 N-連詞規則，經過測試，並沒有發現收斂的 現象，可能是因為文章當中有過多特殊字詞，或者是因為我們設定的 N-連詞太過於長，導致 1CiteSeerx: http://citeseerx.ist.psu.edu/about/site 96  組合過多。  表 11: 專家標註文步的句數 (# of sentence with correct tags)  move-tag B P M R C Total Sentence  # of sentence 27 10 21 60 24  142  在測試階段，為能了解資料經過訓練的篇數是否影響文步標籤的精確率，所以評估每經過 一千篇訓練的 CT 表，預測句子文步的標註是否正確。 首先評估原始資料經過逐篇訓練而得到的 CT 資料表，所預測句子文步的精確率。 由圖 3得知評估的結果，可以觀察句子文步標籤的精確率，發現 CT 資料表每經過一千篇的 訓練，得到的結果逐漸改善。 由於 BF 做出的實驗結果與 OW 相似，所以在此只顯現精確率的結果。 再者，評估文章經過詞性標記與意元集組處理的句子所訓練的 CT 資料表，預測句子文步 的精確率。  圖 3: 關於 OW 標定句子文步資訊  圖 4: 關於 BPC 標定句子文步資訊  
Automatic mispronunciation detection plays a crucial role in a computer assisted pronunciation training (CAPT) system. The main purpose of mispronunciation detection is to judge whether the pronunciations of a non-native speaker are correct or not. In general, the process of mispronunciation detection can be divided into two parts: 1) a front-end feature extraction module that generates pronunciation detection features based on an input speech segment and its associated reference acoustic models; and 2) a back-end classification module that determines the correctness of the pronunciation of the speech segment according to the output of a classifier that takes the pronunciation detection features of the segment as the input. The main contributions of this work are three-fold. First, we investigate the use of two state-of-the-art acoustic models, respectively based on deep neural networks (DNN) and convolutional neural networks 103  (CNN), and compare their effectiveness for the extraction of discriminative pronunciation detection features. Second, we experiment with different types of classification methods and propose a novel integration of DNN- and CNN-based decision scores at the back-end. Third, we provide an extensive set of empirical evaluations on the aforementioned two modules and associated methods based on a recently compiled corpus for learning Mandarin Chinese as the second language. The experimental results reveal the performance utility of our approach in relation to several existing baselines. Keywords：Mispronunciation detection, Automatic Speech Recognition, Deep Neural Networks, Convolutional Neural Networks 一、 緒論 現今全球化的時代裡，精通兩種或兩種以上的語言不僅是優勢更是必要的能力。在 十幾年以前，英語還是國際通用的語言；但近年來，由於中國市場的快速發展，全球華 語學習熱潮席捲而來，學習華語的人數預估已經超過一億，在許多非華語語系的亞洲、 歐洲以及美洲國家，華話已經逐漸成為一種必須學習的語言[1][2]。語言學習又分為聽 (listening)、說(speaking)、讀(reading)和寫(writing)等四類學習面向。隨著第二外語學習 者(second language learner)的人數與日俱增，華語師資的需求也越來越大；尤其在語言 學習中，說與寫的對錯往往需要透過專業的語言教師來評斷，但語言教師的人數遠遠不 及華語學習者數量。因此，電腦輔助語言學習(computer assisted language learning, CALL) 的研究領域越來越重要，本篇論文將專注此研究領域有關於電腦輔助發音訓練 (computer assisted pronunciation training, CAPT)－「說」的技術發展與探討。 圖一、自動發音檢測之流程 一 般 而 言 ， 電 腦 輔 助 發 音 訓 練 (CAPT) 包 括 兩 個 部 分 ： 分 別 是 錯 誤 發 音 檢 測 (mispronunciation detection)與錯誤發音診斷(mispronunciation diagnosis)。錯誤發音檢測 104  系統是請學習者讀誦口說教材，針對學習者念誦的錄音，標記學習者的發音是正確發音 (correct pronunciation)或錯誤發音(mispronunciation)，標記的目標可以是音素(phone)層次 或詞(word)層次；錯誤發音診斷是當系統偵測到使用者的發音出現錯誤時給予有幫助的 回饋，假設教材題目為「師範(shi1 fan4)」，但學習者念成「吃範(chi1 fan4)」，系統除了 判斷出學習者有錯誤發音之外，還可以回饋學習者念的「師(shi1)」可能念成「吃(chi1)」。 而本篇論文將聚焦在如何改善錯誤發音檢測之效能。目前，在錯誤發音檢測的評估方式 中，召回率(recall)和精準度(precision)的曲線與接收者操作特徵曲線(receiver operating characteristic curve, ROC)是最常被採用來評估效能之優劣。我們認為相較於正確發音檢 測(correct pronunciation detection)，錯誤發音檢測對於學習者而言是較為重要；所以，本 篇論文後續在召回率和精準度曲線的評估實驗中，我們將集中討論錯誤發音檢測的效能 表現。 自動錯誤發音檢測的研究大部分是基於現有的語音辨識技術而發展，希望能達到像 專業語言教師一樣地給予語言學習者所念誦語句適當的發音評估。在本論文中，我們將 語音辨識模組視為錯誤發音檢測系統的前端(front-end)，而錯誤發音檢測(分類)模組視為 系統的後端(back-end)。前端的語音辨識模組如果能藉由聲學模型的使用，產生音框 (frame)或者段落(segment)層次的事後機率來做為具鑑別性的發音檢測特徵，則後端偵測 錯誤發音時就能基於這些發音檢測特徵來精準地判斷學習者的發音正確與否。因此，語 音辨識模組中聲學模型所產生的回饋將是我們評斷發音好壞與否的重要依據。在語音辨 識研究上，有別於傳統使用梅爾倒頻譜係數(mel-frequency cepstral coefficients, MFCC)之 語音特徵的高斯混合模型-隱藏式馬可夫模型(gaussian mixture model-hidden markov model, GMM-HMM)的聲學模型，近年來由於機器學習演算法[3][4][5]與電腦硬體的進 步，訓練多隱藏層(hidden layers)及大量輸出神經元(neurons)類神經網路的方法也更有效 率在學術界與實務界激起了深層學習(deep learning)的浪潮，顛覆了幾十年來的研究生 態。許多學者與實務家研究將深層類神經網路(deep neural networks, DNN)當作語音辨識 的聲學模型的重要組成，取代傳統 GMM 的角色來計算每個音框所對應 HMM 狀態的觀 測機率(observation probability)或相似度值(likelihood)。雖然 DNN 在語音辨識領域已經 有相當優異的效果，但也有許多研究指出摺積類神經網路(convolutional neuron networks, CNN)在音素辨識[6]以及大詞彙連續語音辨識[7]的任務上的表現更優於 DNN；這可歸 功於 CNN 能從語音特徵中擷取出發音中細微的位移不變(shift invariance)的特性。透過 CNN 來做為發音檢測特徵的擷取模組，期望能夠從不同國家的華語學習者之發音訊號 中求取出對發音檢測有幫助、具鑑別性的發音檢測特徵(能提供更具鑑別力的事後機率 來幫助錯誤發音檢測)，提升自動檢測錯誤發音的能力。本篇論文對於錯誤發音檢測研 究有三項主要貢獻：首先，我們比較並結合當前基於深層類神經網路(DNN)與摺積類神 經網路(CNN)之先進的聲學模型以產生更具鑑別性發音檢測特徵；再者，我們比較並結 合不同分類方法，以期能達到更佳的發音檢測表現；最後，針對錯誤發音檢測之構成模 組，進行一系列廣泛且深入的實驗分析與討論。 本篇論文的安排如下：第二小節將介紹錯誤發音檢測相關研究的發展近況；第三小 節則是介紹錯誤發音檢測前端模組的聲學模型，分別有 GMM、DNN 與 CNN 三種模型 與 HMM 的結合；第四小節介紹三種錯誤發音檢測的方法，分別是發音優劣程度 (goodness of pronunciation, GOP)、支持向量機(support vector machine, SVM)與邏輯迴歸 (logistic regression, LR)；第五小節則是分析不同聲學模型(DNN-HMM 和 CNN-HMM)在 不同分類器(GOP、SVM 和 LR)中的表現，與將兩種聲學模型經過分類器 LR 所產生的 發音檢測分數值作線性組合後的結果，以及基於 CNN 聲學模型在不同分類器所產生的 105  輸出發音檢測分數對應之排序取調和平均做為結合後的分類結果；最後，在第六小節， 我們提出結論與一些未來可能的研究方向。 二、 相關研究 在大多數的錯誤發音檢測研究中幾乎都是以自動語音辨識為前端，而將後端視為分 類問題[8]。例如，Franco 等人[9]使用母語者的 HMM 之對數相似度值(log-likelihood)與 非母語者的 HMM 之對數相似度值計算比值，稱為對數相似度比值(log-likelihood ratio, LLR)，該論文的實驗顯示使用對數相似度比值(LLR)對於錯誤發音檢測之表現勝過直接 使用對數相似度值。Witt 等人[10]提出 GOP 作為錯誤發音檢測之評估方式，該方法基 於聲學模型所產生的事後機率(posterior probability)對音素層次的發音計算評估分數，並 訂定門檻值(threshold)來區分正確發音與錯誤發音；陸續也有其它研究是基於 GOP 的方 法進行改良[11][12]。另一方面，Huang 等人[8]將鑑別式訓練應用在 GOP 估測，以最小 大化 F 度量(F-measure)為目標作鑑別式訓練。Ito 等人[13]使用決策樹(decision tree)的方 法並針對不同錯誤發音的情況定義各自的門檻值來進行錯誤發音檢測；該論文的實驗證 明其效果勝過所有發音共用相同的門檻值。Truong 等人[14]比較決策樹與線性鑑別分析 (linear discriminant analysis, LDA)用於荷蘭語學習者的錯誤發音檢測任務。廣義上來看， GOP 也屬於一種二元分類的方法，但 GOP 只有考慮到目標(正確)音素與它的混淆音素 的對數相似度值。有鑒於此，Wei 等人[15]使用目標音素與其它所有音素的對數相似度 值做為輸入分類器的發音檢測特徵，並將 SVM 做為分類器來辨認音素特徵對應的輸出 為正確發音或錯誤發音標記。但除了每一個音素的對數相似度值來作為發音檢測特徵， Hu 等人[16]不只使用[15]提出的發音檢測特徵，還額外地將目標音素與其它音素的對數 相似度比值加入成為額外輸入的發音檢測特徵，並使用特殊結構的邏輯迴歸來進行錯誤 發音檢測，該結構透過共享隱藏層來解決部分音素資料稀疏(data sparse)的問題。不同於 [16]的貢獻，我們認為藉由良好的聲學模型產生之事後機率而得的具鑑別性發音檢測特 徵，應有助於錯誤發音檢測的效果；因此，本論文將聚焦於前端聲學模型的比較與融合。 上述的方法皆是運用聲學模型所擷取的發音檢測特徵進行錯誤發音的檢測，除了將 音素或語句分類為正確發音與錯誤發音外，也有研究著重在評斷語句的發音品質。 Neumeyer 等人[17]使用 HMM 計算出對數相似度值與強制對位(forced alignment)後的音 素發音持續時間(duration)資訊，並據此對非母語學習者語句層次的發音品質進行評估。 Chen 等人[18][19][20]提出詞層次的發音品質評估，共分成 5 個等級來區分發音的品質， 並使用資訊檢索的排序學習法(learning to rank)來結合不同發音檢測特徵用於發音品質 評估；其中，在[20]比較各類發音檢測特徵的影響力與 4 種音素層次轉換到詞層次的發 音檢測特徵轉換方法。 而在聲學模型方面，與傳統 GMM-HMM 相比，DNN-HMM 在語音辨識準確率上已 被證實能有顯著的效能提升[21][22]，這主要可能歸功於 DNN 能夠模擬任意的函數，能 替語音訊號所內含的複雜對應關係建立模型，表達能力比 GMM 更強。優良的事後機率 蘊藏豐富的發音鑑別性資訊，使得錯誤發音檢測的效果更好，有許多 DNN-HMM 應用 在 CAPT 的效果已被驗證勝過傳統的 GMM-HMM[2][16][23]，因此聲學模型在計算事 後機率的任務中扮演著非常關鍵的角色[24]，而基於深層類神經網路的聲學模型計算而 得對發音檢測有幫助的事後機率可使 GOP 與其它分類器達到最佳的檢測效果。相較於 DNN，CNN 被視為是另一種更有效率的深層類神經網路，可用於擷取語音訊號中的頻 106  譜變化的位移不變性並且能針對頻譜的相關性建立模型[6][7]。CNN 與 DNN 不同在於： 神經元間的連接不是全連接的(fully-connected)以及同一層的某些神經元間會共享連接 的權重(weight sharing)。Sainath 等人[7]提出 CNN 作為聲學模型更勝於 DNN 的原因是 因為他們認為 DNN 有兩項缺點。首先，DNN 的架構中沒有明確地處理語音訊號中的不 變特徵的功能，例如不同語者說話方式不同，在頻譜上會有細微的位移。DNN 需要運 用各種語者調適(speaker adaptation)技術來降低特徵的變化，DNN 同時需要巨大的網路 規 模 及 大 量 的 訓 練 樣 本 (training sample) 來 達 到 這 件 事 ； 但 CNN 能 透 過 摺 積 核 (conventional filter)沿著頻譜的時間與頻率掃描，以較少的參數數量捕捉到頻譜平移的不 變性。其次，DNN 忽略了輸入的拓撲(topological)結構，它的輸入特徵可以以任何順序 輸入網路，而不影響最後的效能[21]；然而語音訊號所對應的頻譜內容著實含有豐富的 關聯性，而能夠善用頻譜的局部相關性而建立模型的 CNN 在許多任務上的效果都明顯 優於 DNN[25][26][27][28]。因此，本論文將融合兩者的優點，並探討兩種類神經網路所 訓練的聲學模型(DNN-HMM 與 CNN-HMM)對於錯誤發音檢測的效果。 三、 聲學模型 3.1 深層類神經網路 傳統語音辨識系統透過 HMM 來處理語音訊號在時間上的變異，並使用生成模型 GMM 來建立聲學模型，但是使用高斯混合模型的問題在於如何選出最佳的混合高斯函 數的數量，反而導致 GMM 受到侷限。而近年來，在語音辨識的領域中，取代以往的生 成模型(generative model)，透過可視為鑑別式模型(discriminant model)的類神經網路[29] 來估測音素層次的 HMM 狀態之事後機率的研究越來越多。 圖二、摺積類神經網路之示意圖 DNN 是一種前饋式(feed-forward)的類神經網路，它的輸入層與輸出層之間包含一 層以上的隱藏層[30]，每一個隱藏層的神經元通常使用邏輯函數(logistic function)將輸入 映射到上一層，邏輯函數通常使用 sigmoid 函數。假設輸入層表示為第0層，輸出層表 107  示為第 層，表示有 + 1層的深層類神經網路，此前饋運算可以表示為：  ℓ＝ ( ℓ)＝ (Wℓ ℓ−1 + ℓ) , ( ℓ = 0, 1, 2, 3, … , )  (1)  (  )  
Traditionally, cerebellar model articulation controller (CMAC) is used in motor control, inverted pendulum robot, and nonlinear channel equalization. In this study, we investigate the capability of CMAC for speech enhancement. We construct a CMAC-based supervised speech enhancement system, which includes offline and online phases. In the offline phase, a paired noisy-clean speech dataset is prepared and used to train the parameters in a CMAC model. In the online phase, the trained CMAC model transforms the input noisy speech signals to enhanced speech signals with reduced noise components. To test the CMAC-based speech enhancement system, this study adopted three speech objective evaluation metrics, including perceptual evaluation of speech quality (PESQ), segmental signal-to-noise ratio (SSNR) and speech distortion index (SDI). A well-known traditional speech enhancement approach, minimum mean-square-error (MMSE) algorithm, was also tested performance for comparison. Experimental results demonstrated that CMAC provides superior performances to the MMSE method for all of the three objective evaluation metrics. 關鍵詞：小腦模型控制器，語音增強，最小均方誤差 Keywords: CMAC, Speech Enhancement, MMSE 123  一、簡介 語音訊號會由於背景雜音造成語音品質降低，語音增強系統(Speech Enhancement System) 主要目的是減少雜音成分，從而提高訊雜比(SNR)。從吵雜語音中估計出乾淨語音是許 多實際應用中非常重要的語音技術，如自動語音識別(Automatic Speech Recognition, ASR)和助聽器(Hearing Aids) [1, 2]等應用。語音增強算法大致分為兩類，即非監督 (Unsupervised)和監督(Supervised)算法，非監督語音增強算法優點在於需要很少甚至不 需要事先準備數據，一個好的非監督語音增強算法是利用頻譜恢復 [3]，頻譜恢復方法 的目標是在頻域中估計出增益函數，以用來降低雜音，頻譜恢復的方法包括譜減法 (Spectral Subtraction, SS) [4]和溫尼濾波器(Wiener Filtering) [5]，與他們的各種延伸 [6-9]。 此外，另一些頻譜恢復的方法是推導出語音訊號和帶雜音訊號的概率模型(Probabilistic Models)，成功的例子包括最小均方誤差(MMSE)頻譜估計 [10-14]、最大事後頻譜振幅 (Maximum A Posteriori Spectral Amplitude, MAPA)估計器 [15-18]和最大可能頻譜振幅 (Maximum Likelihood Spectral Amplitude, MLSA)估計器 [19, 20]等。目的是用雜訊追蹤 法(Noise Tracking)估計出雜訊的功率頻譜，常見的雜訊追蹤法如語音活動檢測(Voice Activity Detection, VAD)、最小統計法(Minimum Statistic, MS) [21, 22]等。得到雜訊功率 頻譜後，即可得到事前訊雜比(a priori SNR)與事後訊雜比(a posteriori SNR)，根據這兩 種訊雜比可以算出增益函數(Gain Function)，利用此增益函數做語音增強，即可估計出 乾淨語音訊號頻譜。而監督語音增強算法需要事先混合雜音和乾淨語料，以便處理在線 (Online)語音增強，成功的例子包括 Deep Neural Network(DNN) [23]、Deep Denoising Autoencoder(DDAE) [24]、Sparse Coding [25]及 Nonnegative Matrix Factorization(NMF) [26]語音增強算法等。本文提出的 CMAC 語音增強是採用監督算法。 近 年 來 在 語 音 增 強 系 統 (Speech Enhancement System) 上 有 許 多 機 器 學 習 (Machine Learning)方法，如: DNN、Sparse Coding 及 NMF 等。本論文則使用 CMAC，近年 CMAC 較常應用在馬達控制 [27]、倒單擺機器人 [28]、MIMO [29]控制等，而在訊號處理方面， 非線性信道均衡(Nonlinear Channel Equalization)以及雜訊消除(Noise Cancellation)系統 上均有良好的效果 [30]，我們則研究此方法在語音增強系統(Speech Enhancement System)上的效果。由於在降噪的過程中可能會造成語音訊號失真，這會嚴重降低語音 的品質，因此我們使用 SDI 評估方法來決定 CMAC 參數的調整，最後使用 SSNR 與 PESQ 評估語音訊雜比與語音品質。 小腦模型控制器(CMAC)被列為是非完全連接感知機(non-fully connected perceptron-like) 聯想記憶網路(associative memory network)重疊接受域(receptive-field) [31]。它可以解決 規模快速增長(fast size-growing)的問題，還有現有神經網路學習上的困難。傳統的 CMAC 使用局部性(local)固定二進制接受域(receptive-field)基礎函數，缺點是輸出中每 個量化的狀態不變，不保留衍生的信息。學習時 CMAC 的輸入為帶雜訊語音，輸出為 增強後乾淨語音，我們會記錄學習完成後的 CMAC 內的所有參數，在測試時直接使用 這個 CMAC Model 亦可以把雜音消除。 本論文第二章介紹 CMAC 主架構，第三章介紹 CMAC 參數的自適應學習算法，第四章 介紹實驗與評估方法，音檔處理過程以及 CMAC 消除雜音步驟，再探討 CMAC 各參數 的設置會造成什麼影響，第五章結論。 124  二、CMAC 結構 CMAC 架構圖示於圖一(A)，是由一個輸入空間(Input space)，聯想記憶空間(Association memory space)，接受域空間(Receptive-field space)，權重儲存空間(Weight memory space)， 輸出空間(Output space)組成，圖一(B)是由一個由二維情況下的圖解法。  x1 x2 ⋮ ⋮ xN  ⋮ bj ⋮  j ⋮  y Output  Input  Weight memory Receptive-field Association memory (A) Variable x2  Layer4 Layer3 Layer2 Layer1  h f d b g e c a  
Named entity recognition (NER) is of vital importance in information extraction and natural language processing. Current NER models are trained mainly on journalistic documents such as news articles. Since they have not been trained to deal with informal documents, the performance drops on Web documents, which may lack sentence structure and contain colloquial expression. Therefore, the State-of-the-art NER systems do not work well on Web 148  documents. When users want to recognize named entity from Web documents, they certainly have to retrain the new model. Retraining a new model is labor intensive and time consuming. The preparatory work includes preparing a large set of training data, labeling named entity, selecting an appropriate segmentation, symbols unification, normalization, designing feature, preparing dictionary, and so on. Besides, users need to repeat the previous work for different languages or different recognition types. In this research, we propose a NER model generation tool for effective Web entity extraction. We propose a semi-supervised learning approach for NER model training via automatic labeling and tri-training, which makes use of unlabeled data and structured resources containing known named entities. Experiments confirmed that the use of this tool can be applied in different languages for various types of named entities. In the task of Chinese organization name extraction, the generated model can achieve 86.1% F1 score on the 38,692 sentences with 16,241 distinct names, while the performance for Japanese organization name, English organization name, Chinese location name extraction, Chinese address recognition and English address recognition can be reached 80.3%, 83.2%, 84.5%, 97.2% and 94.8% F1-measure, respectively. 關鍵詞：命名實體辨識，協同訓練，Tri-Training Keywords: Named Entity Recognition, Co-Training, Tri-Training. 一、 緒論 命名實體辨識是自然語言處理的一項重要基礎工作，其辨識正確率對後續的語意分析 （Semantic Analysis）、機器翻譯（Machine Translation）等自然語言處理議題具重大的 影響。在大量文字資料中，常有人名、地名、組織名等有意義的專有名稱出現，然而因 應社會需要及科技發展，這些不斷被創造的詞彙，難以被單一詞庫所收藏，因此需有命 名實體辨識以便擴充詞庫。不同類型的命名實體出現於語句中的位置、規則或詞性皆不 相同，因此需要的特徵值也都不同。以中文組織名稱辨識為例，目前許多關於組織名稱 辨認的研究，主要是從新聞或一些較正式的文章中訓練組織名稱擷取模型[7] [11] [13]， 但是網路上商家組織名稱傾向較不正式的命名方式，例如：彼得公雞地中海餐廳、造紙 龍手創館等，而新聞等較正式的體裁則容易出現公司行號與正規的組織名稱，如：伊甸 基金會、國立中央大學、高鐵公司等，且網路上發表於論壇或社群媒體的文章語句結構 與用字遣詞皆與正式文章不同，因此辨識效果不佳。如表一以及表二所示，我們利用 2,000 筆已知地址為查詢關鍵字，於 Google 搜尋結果片段（Search Snippets）中包含關 鍵字的句子為測試資料，再使用 Stanford NER1 (Named Entity Recognizer) 來做組織名 稱辨識實驗，F1 效果只能達到 54.3%。另外，我們也利用 200 筆中文地點名稱為查詢 關鍵字，利用 Google search snippets 包含關鍵字的句子為測試資料，同樣利用 Stanford NER 來做地點名稱辨識實驗，F1 效果僅達 20.1%。顯示現有的公開 NER 工具對於 Web 上非正式文章的命名實體辨識效果有限，並導致後續的相關研究效能有限。 命名實體辨識可視為序列標記（Sequence Labeling）的問題，故通常使用 Conditional Random Field（CRF）來解決此問題，CRF 為一機率架構的無向圖（Undirected Graphical） 模型，常用於標注序列資料。我們利用開放的 CRF++[3]程式進行實驗，為了使 CRF 標 記能有好的準確率，我們必須處理原始大量文字資料，包含人工收集答案、標記答案等， 同時為了提升模組辨識效果也必須要為資料做適當切割、選擇斷詞工具、統一符號、數 
164  frequency in corpus-level. Thus, we propose a PMI-β priors methods on BTM. Our PMI-β priors method can adjust the co-occurrence score to prevent the common words problem. Next, we will describe the detail of our method of PMI-β priors. However, just consider the frequency of bi-term in corpus-level will generate the topics which contain too many common words. To solve this problem, we consider the Pointwise Mutual Information (PMI) [9]. Since the PMI score not only considers the co-occurrence frequency of the two words, but also normalizes by the single word frequency. Thus, we want to apply PMI score in the original BTM. A suitable way to apply PMI scores is modifying the priors in the BTM. The reason is that the priors modifying will not increase the complexity in the generation model and very intuitive. Clearly, there are two kinds of priors in BTM which are β-prior and β-priors. The β-prior is a corpus-topic bias without the data. While the β-priors are topic-word biases without the data. Applying the PMI score to the β-priors is the only one choice because we can adjust the degree of the word co-occurrence by modifying the distributions in the β-priors. For example, we assume that a topic contains three words “pen”, “apple” and “banana”. In the symmetric priors, we set <0.1, 0.1, 0.1> which means no bias of these three words, while we can apply <0.1, 0.5, 0.5> to enhance the word co-occurrence of “apple” and “banana”. Thus the topic will prefer to put the “apple” and “banana” together in the topic sampling step. Table 1 shows the clustering results on the Twitter2011 dataset, when we set the number of topic to 50. As expected, BTM is better than Mixture of unigram and LDA got the worst result when we adopt the symmetric priors <0.1>. When apply the PMI-β priors, we get the better result than BTM with symmetric priors. Otherwise, our baseline method, PCA-β, is better than the original LDA because the PCA-β prior can make up the lack of the global word co-occurrence information in the original LDA.  Table 1. The Clustering Results on Twitter2011 dataset  Model 
With the increased popularity of mobile devices, local search has become a new popular service. Therefore, we need a powerful POI (Points of Interest) database to support local search. In recent years, the web has become the largest data source of POIs. With the prevalence of Internet, people will share their travel experience and information of POIs that they had been visited on social network, their blogs, and even check-in post. Besides, many companies and organizations publish their business on their own websites, resulting a large number of POIs. 180  In this paper, we propose a POI database construction system from the immense data of the Web. Our system consists of two parts: the query-based crawler, and the POI extraction system. The goal of query-based crawler is to collect address-bearing pages (ABP) from the web as address is a good indicator of POIs. The second part is POI extraction system. We use CRF (Conditional Random Field) to train a Chinese postal address recognition model and a Chinese organization recognition model. After the extraction of addresses and POI names from ABP with these two CRF models, we then leant a model to pair an address and a POI name as a POI. Finally, we extract POI associated information for each POI to construct a complete POI data. 關鍵詞：電子地圖、網路爬蟲、資訊擷取、POI 資料庫 Keywords: electronic map, web crawler, information extraction, POI database. 一、 緒論 電子地圖不僅是數位化後的地圖，因為不受限於有限的空間，可以根據瀏覽者的需求， 整合其背後資料利用圖層疊加的特性對社會經濟資料進行標記與分析，所以產生了相當 多新穎的服務，如買屋租屋搜尋、景點搜尋等等。另者，由於近年行動裝置的進步與普 及，連帶使得行動定位與目的地導航成為一項新興的熱門服務，現今的電子地圖大多整 合了以上的功能，提供了完整的適地性服務(Location-based Service)，使得地圖搜尋成為 日常生活不可或缺的功能。 雖然電子地圖能夠提供給我們諸多的便利，但除了基本的地理資訊外，電子地圖還必須 要仰賴其系統後方豐富且充沛的資料庫，才能更加突顯其功效。多數地理資料庫都是依 靠人工編輯，但是要將所有的 POI 都使用人工的方式加入資料庫是一件耗時費力的事 情，因此也限制了現今地點資料庫的地點數量與內容。然而在網際網路盛行的現今，雖 然政府工業局或商業司有企業登記資料，但企業登記名稱與店家名稱往往不一致，例如 嘟嘟房停車實由中興電工經營，因此即使有政府開放資料，商家 POI 資料仍然不夠完 整，但是除了政府機構的網站，POI 亦經常伴隨其描述出現在其他網頁中，如連鎖商店 的網頁、部落格的餐廳介紹及景點介紹，甚至於社群網站的打卡資訊等，這些網頁中或 多或少都包含了 POI 的描述訊息，因此若能有效率地找到上述這些含有 POI 資料的網 頁，並由程式自動將其擷取出可用的 POI 資料，便可有效地擴展資料庫的地點數量與 內容。 根據 W3C 的定義，一個 POI 會包含許多資訊，像是名稱、位置、電話以及相關資訊等 等，其中位置用於定位標記到地圖上，可用地址或經緯座標表示。由於地址的識別率較 高相對其他 POI 資訊更容易擷取，因此本篇論文中，我們提出一個 POI 資料庫的建置 系統，以地址擷取做為 POI 辨識策略，並且從包含地址的網頁中擷取與地址相對應的 POI 名稱和相關資訊，用來建立一個 POI 資料庫，提供 POI 搜尋服務，POI 資料範例如 圖 一。 181  圖 一、POI 範例 圖 二、網頁中的 POI 相關資訊與雜訊 本系統包含三個模組。第一模組是網頁的爬取(Crawler)，我們首先以地址為關鍵字串來 蒐集包含地址的網頁(Address-bearing Pages, ABP)，我們引入 Chang 等人在 2012 年[5] 和 Lin 等人在 2014 年[10]所提出的兩種模型來從 ABP 中擷取出地址；第二個模組則是 POI 擷取模組，我們使用 Huang 等人在 2015 年[8]提出的中文組織名稱辨識模型來擷取 ABP 當中的 POI 名稱。最後我們會將辨識出的 POI 名稱以及地址組成許多筆 POI，並 透過 POI 配對驗證模組將正確的 POI 資料放入資料庫當中。 另外，因為大多數使用者是由關鍵字或是類別反查商店在地圖上的位置，因此用以描述 地址的相關資訊是否足夠，會大幅影響查詢系統的檢索效能，為此我們提出了 POI 相 關資訊擷取模組，為每個 POI 擷取相關描述來解決此問題。如圖 二所示，網頁中包含 許多地址的描述，但同時仍有許多與地址不相關的內容。在本篇論文中我們將應用中文 組織名稱辨識模組來加強這類型網頁的地址相關資訊擷取。 本論文共分成五個章節，第一章為緒論，說明研究的動機與目的並簡單的介紹本篇論文； 第二章為相關研究，介紹和本論文相關的研究；第三章為系統架構與研究方法，詳述如 何從網路中找尋 ABP，並由程式自動擷取出 POI；第四章為實驗，評估系統的效能及 POI 資料的正確性；第五章為結論，總結本論文的貢獻。 182  二、 相關研究 近幾年來，由於網路上巨量資料的累積與行動裝置的普及，地理資訊檢索(Geographic Information Retrieval)以及區域搜尋開始受到重視。國際間地理資訊檢索領域的研究以 ACM SIGSpatial workshop on GIR 較負盛名，自 2004 年起收錄相關領域的研究報告， 相關研究主題包括了地理資訊系統的發展模式、地理數據庫的存取與網路內容與多媒體 的分析、基於文字與地理資訊系統整合的方法(如資訊擷取、自然語言處理、空間資料 的索引與搜尋等)、以及地理術語的識別與時空(spatio-temporal)的概念。 另外則是從 2008 開始與 WWW 同時舉辦的 Workshop on Location and the Web(LocWeb)， 後續也在 CHI、IoT、CIKM 等會議舉行，某種層次來看，LocWeb 與 Web 的關係更為 緊密。而國內研討會則以台灣地理資訊學會舉辦的研討會為主，主題包含了地理空間數 據可視化、地理資訊系統技術發展與整合應用、開放資料與群眾外包(Crowdsourcing)、 防救災與資通技術整合、以及自然環境資源管理與環境監測相關研究。 相對於學界的小數量蒐集、特定專業性的問題探討，業界對於地理資訊與跨領域整合的 潛在商 機與經濟效應上更為積極。例如 Google 在地圖、街景上的投資，同時持續「免 費」開放使用其服務，吸引了全球使用者的力量「零成本」貢獻大量的使用者標記，累 積了目前任一個國家無能與之匹敵的大數據資料。無論是在地圖、地理數據、網頁文字、 圖片及使用者查詢詞紀錄，都讓其他 LBS 應用服務難望其項背。而其他商業巨擘如 Bing Maps、Yahoo! Maps、Apple Maps、Facebook 的地理資料庫所擁有的數據亦不容小覷， 甚至是全球性非營利組織的地理位置資訊，如：OpenStreetMap1、Wikimapia2等也都具 備了數千萬的 POI 資訊。 Ahlers 與 Boll 在基於地點的網頁搜尋研究中[1]，提出了一個從網頁中擷取地點的系統 架構，主要分為 crawling、斷詞與索引網頁，以及搜尋與排序等三個子系統，其中他們 所採用的 crawling 策略又可分為兩種：以地點字典為主和以關鍵字為主的方法[2]，透 過自適應化(adaptive)的學習與預測可能包含地點的網頁來有效提升整體召回率(recall)， 該研究主要針對德國多個城市進行網頁爬取與索引。 在本篇論文中，我們從網際網路中找出包含地址網頁，並且利用命名實體辨識(Named Entity Recognition, NER)從網頁中擷取地址以及 POI 名稱並且將其配對，在得到 POI， 再為其找出相關的描述，使得該筆 POI 資料能夠在資訊檢索系統中被檢索。因此本研 究的主要技術可以分為如何有效地爬取目標網頁、命名實體辨識、地址和 POI 名稱的 配對以及相關資訊擷取。 我們採用的中文地址擷取方法是 Chang 等人於 2012 年所提出的模型[5]，使用機器學習 法中序列標記的 CRF 做為其訓練及測試模型，配合台灣地址的特性建立了 17 種地址特 徵並且使用 Start/End 標記法，接著再配合極大分子序列演算法 (Maximal Scoring Subsequences)，其準確率約在 94%至 99%之間。 而中文組織名稱擷取模組的建置則是使用 Huang 等人於 2015 年提出的方法[8]，同樣是 使用 CRF 做為其訓練及測試的模型，利用組織名稱中常出現的詞彙(e.g., 店、公司)以 及組織名稱前後常出現的詞彙等總共建立了 18 種特徵，並且使用 Self-Testing 以及 Tri-Training 等方法再更進一步地提升準確率，最終其準確率可以在非結構化的網頁中 達到 86.13%。 
Association analysis has attracted considerable attention recently in many research fields, mining data relations and rules from huge volume of data especially. This study aims at mining issues of public concern and analyzing its relations from massive of unstructured data. The main resource of this study is environmental related documents from PTT bulletin board system. A model is constructed via the collected environmental documents for predictions of issues of public concerns and possible future directions. The experimental results show that mining information from documents of PTT bulletin board system can effectively understand the public concerns and predict possible future directions. The reports from the prediction system may be used as a reference for environmental authorities. The prediction model we propose not only precisely masters of opinions from public to improve the administrative quality of environmental authorities, but also strengthens the content of press release to cover and answer the significant important issues of public concerns. The prediction system can be also applied to different applications, such as market investigation and opinion analysis. 關鍵詞：關聯分析，意見探勘，議題偵測 Keywords: Association Analysis, Opinion Mining, Topic Detection 196  一、緒論 近年來社群網路的使用者來越多，根據 eMarketer 調查指出，2015 年全球使用社群網 站的人數預估將突破 21 億 8000 萬人1，不同種類的討論話題都可能會出現在各大社群 網站中。網路將人與人之間的距離拉近，不同來源的資訊也隨著網路的便利性以及社群 網站的發達，快速地將資訊傳播開來。而網路上的資訊來源不只來自新聞媒體，民眾個 人經驗、小道消息更是有別於新聞媒體儘量保持客觀的態度，主觀的陳述與透過社群網 路的公開討論，使得訊息的面向更加豐富多元，因此，社群網路使用者討論資料已經成 為文字探勘與議題分析之重要素材來源。 批踢踢實業坊(PTT)是一個電子佈告欄系統(Bulletin Board System, BBS)，於 1995 年創立，目前在批踢踢實業坊與其分站批踢踢兔註冊總人數約 125 萬人，兩站尖峰時段 超過 15 萬名使用者同時上線，擁有超過 2 萬個不同主題的看板，每日約 4 萬篇新文章 被發表，是非常好的文字探勘與議題偵測素材來源。 文字探勘是近年來隨著人工智慧和自然語言處理技術發展的一門新興技術。主要從 大量文字資料中自動化辨識與挖掘有用的資訊，萃取出隱含的或過去不為人知，但可信 與有效的訊息。並且依據使用者文字表達特徵，在一群未經處理的資料中找到使用者可 能感興趣的資訊。其中關聯法則分析就是文字探勘中一種重要的分析模型，利用關聯分 析探勘出有用的資訊，可做為政府決策的參考依據[1] [2]。 政府單位發布政策資訊給民眾，常使用新聞稿的形式向民眾宣達，因此，如何加強 政策溝通對政府機關十分重要。一般而言，公部門擬稿人是憑藉個人經驗與蒐集過去歷 史資訊來撰寫文稿。然而，擬稿人可能會受限於個人迷思或因特定領域知識不足，對於 議題焦點之掌握程度，有參差不齊的現象。現今社群網路的出現，使得任何人都可以透 過社群網路取得資訊，並且透過巨量網民的討論資訊，發掘目前大眾所關注的議題與輿 情焦點。透過對巨量社群網路資料進行文字探勘即可以達到上述的目標。建立議題預測 模型，協助公部門擬稿人撰寫新聞稿之方向；運用巨量資料，分析民眾關注之議題，作 為政策研擬與新聞發布後之輿情蒐集，可以即時回應或加強政策溝通，應是可行之有效 方式。 本研究架構共分為五章，包含「緒論」、「文獻探討」、「實驗資料」、「預測模型」與 「結論」，內容分別說明如下： 第一章「緒論」說明研究背景與動機以及論文架構。 第二章「文獻探討」則回顧 整理過去文字探勘方法與應用、關聯分析演算法理論及我國政府對於文字探勘與巨量資 料分析之策略。第三章「實驗資料」說明資料取得來源與統計資訊。第四章「預測模型」 介紹本研究預測模型設計與參數設定，以及系統分析流程。第五章「實驗結果與分析」 探討與分析實驗結果。第六章「結論」總結所有分析資訊、研究貢獻以及未來可能研究 方向。 二、文獻探討 文字探勘（Text Mining）主要是針對半結構化（Semi-structured）或非結構化（Unstructured） 儲存格式的文件資料進行探勘，這些非結構化資料隱藏著許多重要的資訊，是近年來重 要的研究領域之一[3]。透過分析文本中的文字特徵，從中萃取出隱含性資訊，轉換成 
 SKCC 和 代汉语语法信息词  GKB  的词 对 ，以语料 依托并结合 代汉语词  义词词林 和 代  汉语搭配词词 等词 资源进行 代汉语语义词 的多义词词 校 工作  首  先 SKCC 和 GKB 的词 对 入手 ，设 了候选拟修改多义词的 取算法 ，对  取出来的 1605 个多义词进行了义 补录 合并 删除，补充释义，修改翻译  和示例等工作， 时针对 类特殊的 食物+作物类 多义词建立了义 的树形  结构以满足 粒度的词义消歧任 要求  方便 续的修改工作 ，本文开发了  SKCC 和 GKB 之间的词 映射 ，在 的基础 进行了多义词映射工作  关键词 义 区分 代汉语语义词 多义词映射 多义词修改 New editing and checking work of the Semantic Knowledge base of Contemporary Chinese (SKCC) Abstract: This paper is rooted in the two principles and methods that should be followed by sense discrimination for Chinese language processing: Completeness and discreteness. Built on the comparison of Semantic Knowledge-base of Contemporary Chinese (SKCC) and Grammatical Knowledge base of Contemporary Chinese (GKB), supported by large scale corpus, we conducted our new editing and checking works. Firstly, we designed a novel multi-sense lexicon candidate abstraction algorithm based on lexicon comparison between SKCC and GKB. For all 1605 candidate multi-sense lexicon, we conducted editing work on the senses, explanation, and its translation Then, we built a tree structure to process a special food and plant lexicon. Thirdly, a mapping platform between SKCC and GKB has been built to help us built mapping relationships between multi-sense lexical between SKCC and GKB. Finally, we finished mapping work for all multi-sense lexicon in SKCC.  Key words: Distinguish word sense; SKCC; Multi-sense word mapping; Multi-sense word editing  、引言  代汉语语义词  Semantic Knowledge-base of Contemporary Chinese，以  简称 SKCC 是 个 面向汉英机器翻译的大规模汉语语义知识 ，目的是在语  法分析的基础 ，给自然语言处理 供更 全面 深入的语义信息 [1]作  家 科 技 进 等奖 获得 目 “ 综 合 型 语言知 识 Comprehensive Language  206  Knowledge Base，以 简称 CLKB ”的 部 分， SKCC 被广泛 用于 算词 汇语义学的基础研究和 用研究之中，例如魏雪 袁 林 2014 以 SKCC 依托建立了 词语义 类组合模式 [2]，张仰森 2012 利用 SKCC 进行了词汇语 义相似度的 算[3]等 来 算词汇学特别是词义划分理论领域取得了较大 的进展，吴云芳 士汶 2006 出 了 实际操作性的面向汉语信息处理的 词语义 区分原则和方法 [4]，而 SKCC 自 2003 第 版发布以来 直没 进行 大规模更 因 ， 必要结合自 SKCC 发布以来语义词 编纂和词义划分理 论的 果，对 SKCC 的多义词词 进行 修 ，使 更好地 自然语言处理服  本文首先建立了 SKCC 和 GKB 的多义词映射 台， 据映射的结果 确定拟进行  修改的多义词词条，然 结合 SKCC 代汉语语法信息词 Grammatical  Knowledge-base of Contemporary Chinese，以 简称 GKB [5] 义词词林  [6] 代汉语搭配词 词 [7]和 代汉语 词 [8]等多部词 资 源，以 2000  1-3 的 人民日报 语料 和 京大学 CCL 语料 语料资源进行 SKCC  多义词词 校 和 修 工作 工作包括对  1356 个 SKCC 多义词词条的校 和  1508 个多义词的映射，经过校 和 修 工作 的 SKCC 在多义词部分规模和  质量方面 了较大 高，可以 算语义分析和词义消歧等任 供更好的支  持  SKCC 改进的理论分析  义 划分是 SKCC 编制的 心问 ，也是词义消歧等自然语言处理任 的基础 性工作 Palmer 2001 [9]指出自然语言处理技术的瓶颈之 在 于 算机 确 辨析词义 吴云芳 士汶 2006 [4] 出了作 自然语言处理任 的多义词 词义划分的 个原则 (1) 般限定 词类 的多义词而 包括跨词类的多义 象 (2) 将 形异义词和词语的 义 在 个 面 考察而 追求 形和多 义的 格区分 (3) 消歧处理的对象 要是词汇义而 是在词语或固定组合中出 的语素义 基于 个原则 ， 出了 “完备性”和“离散性”的要求，并 出 基于语料 实证进 行词语义 粒度划分的准则 “完备性”的要求 据词 语的义 区分 ，操作者 以 利对语料中的每 个目标词 标注出义 SKCC 中部分词条 符合 “完备性”的要求，例如 SKCC 对“大小”区 分了 个义 大小 (1) 量 属性，物体的 大小 (2) 身份，大人和小孩 但 遇到 人民日报 语料 中 列词组时 ， 述 2 个义 显然无法 之对 大小领 /大小领 /没大小 就需要添 个义 表述关系类的“大小”，释义 “表示关系的尊卑或长幼”  207  “离散性”是指意义分析系统中 义 的内涵 重合 ，SKCC 中 部 分词义 之 间存在着义 重合甚 蕴含的 象，例如 SKCC 中对“驾驶”区分了四个义  驾驶 (1) 操纵 载工  (2) 操纵车 船等 载 工 行驶  (3) 操作 他工 作业  (4) 操纵航空器  四个义 中 ，义 2 3 4 仅释义 以包含在义 中 体 体论元角色和语义类 也完全  
Word Complexity Measure (WCM, Stoel-Gammon, 2010) is a system of phonological assessment for children’s speech productions, a method that focuses on the complexity rather than accuracy. With its flexible parameter program, the assessment can be adjusted to the phonological properties of different languages. In the current study, the WCM was used to assess speech production of three Mandarin-learning children from birth to three years old. In addition to the original parameters in Stoel-Gammon (2010), the Chinese version of WCM made some adjustments, including incorporating productions of fricatives, affricates, /ʐ/, /y/, and the late acquired vowels and consonants, to examine the complexity of speech productions. Major findings in the developmental changes of the first 3 years are: 1) the complexity of the intelligible words increased, with individual differences in the stability of changes; 2) the complexity of the unintelligible syllables also elaborated; 3) the percentage of simple words/syllables decreased in both intelligible and unintelligible productions. Keywords: Production complexity, Mandarin-learning children, Phonological development, Word complexity measure 1. Introduction Word Complexity Measure (WCM, Stoel-Gammon, 2010) is a measurement for developmental phonology and disorder. WCM focuses on the complexity and is based on point-giving process. Comparing with Percentage of Consonant Correct (PCC, Shriberg & Kwiatkowski, 1982) and the measurement of whole-word productions (Ingram, 2002), WCM demonstrates advantages in describing the development of speech production because the parameter in WCM was designed to mirror the properties of child phonology. The PCC also calculates points in children’s speech for measuring intelligibility. However, in PCC only the accurate speech sounds are given points in the process, and those unintelligible utterances are not scored because they are not real words. Namely, the PCC examines and quantifies the accuracy of the sounds that is articulated correctly. However, children produce not only the intelligible words but many non-word sounds, especially in the early ages. By means of the WCM, those unintelligible utterances can also be scored and quantified. Children with 1,2 Department of Foreign Languages and Literature, National Cheng Kung University 233  small vocabulary can also be inspected. For example, children with language disorders can also be examined with this measurement. This may provide a useful tool for clinical assessment. Moreover, WCM provides phonological development scales with which phonological changes can be tracked. In addition, children’s forms can be compared with target forms with WCM measures. Namely, both independent and relational analysis can be done with WCM. The whole-word production (Ingram, 2002) proposed four measures to estimate speech production of children: 1) the phonological mean length of utterance (PMLU); 2) the proportion of whole-word proximity; 3) the proportion of whole-word correctness; 4) the proportion of whole-word variability. The core lies in the PMLU, which gives points to each word based on two factors: 1) the number of segments in a word; 2) the number of correct consonants. The former factor demonstrates the independent analysis, and the later the relational analysis. In this aspect, the measurement of whole-word production is close to the WCM. However, the WCM can further provide a more comprehensive picture by accounting for both qualitative and quantitative nature in phonological development (Stoel-Gammon, 2010). Furthermore, the prevailing advantage of the WCM is the flexible parameters that can be adjusted to target languages. This measurement was initially designed for English-speaking children, while in the present some of the parameters of WCM were adjusted to observe Chinese phonological system. The recorded speech productions were divided into two groups: intelligible words and unintelligible utterances. Each group was scored separately to prevent improper comparison of speech sounds in non-words and real words. Based on the phonological parameters, each sample was awarded a ‘complexity score’ and was calculated to get a ratio which mirrored the nature of error and accuracy. In general cases, first words emerge at the age of 12-15 months (Stoel-Gammon, 1989). The longitudinal speech productions analyzed in the present study started from about 2 months of age to better observe the transition from pre-speech vocalization to real-word productions. 2. Methodology 2.1 Participants Data of 3 typically developing boys (Child A, B, and C) were analyzed from 2 to 36 months of age. This longitudinal data is part of a larger scale of longitudinal observation of phonetic development in Mandarin-learning children. 2.2 Procedure A wireless AKG microphone system was linked to a SONY DAT recorder with a signal-to-noise ratio above 91 dB for audio recording. The mini-microphone was pinned on infants’ shirt, or placed close to infants’ mouth. Each recording session lasts for approximately 50 minutes. The caregiver and an experimenter were presented in each of the recording session. Spontaneous infant vocalizations were elicited in natural interaction. The sample words were collected based on natural conversation among the observers, participants, and the parents, and the picture-naming task (after approximately 18 months of age). Twelve recordings for each participant were analyzed in the study, and nearly 50 speech samples 234  were included in each recording. Each speech sample was awarded a score. Higher scores denote the presence of complex or later acquired phonological parameter. The adjusted parameters in the present study are listed below: Word patterns (1) Productions with more than two syllables receive 1 point. Syllable structures (1) Productions with a word–final consonant receive 1 point. (2) Productions with a triphthong receive 1 point for each triphthong. Sound classes (1) Productions with a velar consonant receive 1 point for each velar. (2) Productions with a rhotic vowel /ɚ/ sound receive 1 point for each /ɚ/. (3) Productions with a fricative or affricate receive 1 point for each fricative and affricate. (4) Productions with a /ʐ/ sound receive 1 point for each /ʐ/. (5) Productions with a /y/ receive 1 point for each /y/. (6) Productions with any of the late acquired sounds /ɨ/, /ɪaʊ/, /ɪo/, /ɪaŋ/, /tɕʰ /, /tɕ/, /ts/, or /ɕ/ receive 1 point for each of the late acquired sounds. In this version of the WCM, the complexity indexes are modified. In word patterns, instead of words used in English, utterances (non-words) or phrases (intelligible meaningful units) were the units for analysis in Chinese version. In rule 1 in the current study, the distinction between a phrase and a fragment were made first. A phrase may include more than two words with a complete meaning (for example ‘ping guo’ apple in Chinese), while a fragment is a unfinished phrase that expresses incomplete meaning due to the sound quality or noise interruption. A fragment may contain only one word or more. Either a phrase or a fragment is counted as one unit. In syllable structures, the consonant cluster in the original WCM in rule 2 is replaced by triphthongs. In sound classes, the syllabic liquid sound in the original WCM is deleted, and the only rhotic vowel in Chinese /ɚ/ is counted in rule 2. The voiced fricative /ʐ/ and the rounded high front vowel /y/ were added in rules 4 and 5 respectively. In rule 6, the late acquired sounds /ɨ/, /ɪaʊ/, /ɪo/, /ɪaŋ/, /tɕʰ /, /tɕ/, /ts/, and /ɕ/ were extracted from those acquired in the later stages of the longitudinal observation, according to the order of emergence and stabilization of vowels and consonants from birth to 36 months of age. This parameter acts as an indicator of phonological development. The higher scores reflect more advanced level of phonological development. Furthermore, the collected data were organized into two sets. The identifiable words and unintelligible utterances were scored separately. The speech children made were grouped together as a phrase or a fragment according to the contexts and then were given 235  points through the parameters. Each unit was scored a total point to reflect the quantified level. Furthermore, the points given by each parameter can also show the qualitative level of development. Table 1 presents the process of scoring for intelligible words. Table 1. The 9 parameters and scoring 子母聲國 音 音 調 字  group Phrase Fragment > 2 syl Final C Triphthong Velar Fr, affri z r rho V y Late acquired sound Total points  5 23 4 掉 1 p 1 0 2 0 1 0 0 0 1 5 5 16 4 到 1 21 29 5 水 1 8 12 8 了 1 The speech children made were classified as either a phrase or a fragment according to the contexts and then were given points according to the parameters. Table 1 presents the process of scoring for intelligible words. The corresponding target form was separately scored. The same process was also applied to the unintelligible utterances. The WCM assesses developmental phonology through the independent analysis and the relational analysis. The independent analysis records children’s productions to track their phonological development, as shown in Table 2. The WCM score presents the complexity of speech production to show the developmental level of different ages. Hence, the independent analysis can demonstrate the long-term phonological development of a child. Both quantitative and qualitative information can be revealed through the independent analysis.  Table 2. The independent analysis: children’s phonological development  WCM  WCM range  Words with 0 point  Child (months;days) A(15;09)  Sample words 1  child 0  child 0  Words scored 0/sample words (%) 1/1 (100%)  A(18;22)  
 In this paper, we introduce the concept of a novel application, called Knowledge Learning from User Search, aiming at identifying timely new knowledge triples from user search log. In the literature, the need of knowledge enrichment has been recognized as the key to the success of knowledge-based search. However, previous work of automatic knowledge extraction, such as Google Knowledge Vault, attempt to identify the unannotated knowledge triples from the full web-scale content in the offline execution. In our study, we show that most people demand a specific knowledge, such as the marriage between Brad Pitt and Angelina Jolie, soon after the information is announced. Moreover, the number of queries of such knowledge dramatically declines after a few days, meaning that the most people cannot obtain the precise knowledge from the execution of the offline knowledge enrichment. To remedy this, we propose the SCKE framework to extract new knowledge triples which can be executed in the online scenario. We model the ’Query-Click Page’ bipartite graph to extract the query correlation and to identify cohesive pairwise entities, finally statistically identifying the confident relation between entities. Our experimental studies show that new triples can also be identified in the very beginning after the event happens, enabling the capability to provide the up-to-date knowledge summary for most user queries. Introduction  The technology of Knowledge Bases, abbreviated as KB, is recently highlighted by 248  Internet giants such as Google, Yahoo and so on. For example, the Knowledge Graph project 1 , announced by Google at 2012, attempts to integrate the semantic information from KB into the search engine, enabling the capability of QuestionAnswering for specific queries. Currently, when we issue ’Avatar Director’, the exact answer ’James Cameron’ will be revealed as the conspicuous block in the search result. As the evolution of the interactive interface moving toward small screens (such as smart phones or wearable devices), the search content with lots of relevant URLs is no longer considered as an effective manner. It is believed that the QAbased search engine is the key ingredient of the next-generation information technology. However, the public large-scale knowledge bases, such as crowd-based Dbpedia [1], Freebase [3], NELL [4], and YAGO [26], have been reported to encounter the progressively slow growth of content [27]. The bottleneck implies that the human editing is no longer the effective manner for knowledge maintenance. Since the size of knowledge in current KBs still deviates far from completion, Google recently devotes to develop a systematic solution, called Knowledge Vault [9] (called KV for short), for the purpose of knowledge enrichment. As the full scan of Google-indexed pages, the KV framework is able to annotate new relation between two known entities such as people or movies. The design of KV is based on the fact that some relations, such as nationality of people, should be innate so that such relations are likely to be discovered after exhaustive search on the whole web. After the content match of entities, corresponding to Zappa and Rose in this case, the ’parent’ relation can be further identified by the technology of text understanding. As their evaluation, the knowledge size is finally enlarged by 100 times as compared to the current knowledge base. Knowledge Vault highlights the necessity of knowledge enrichment. Unfortunately, the strategy of exhaustive scan of the whole web is not scalable to capture the daily updated knowledge, which is believed to be of interest to most 
According to the investigation report of the Department of Health, Executive Yuan, R.O.C. in 2007, it is estimated that about 7.3% of Taiwan's population suffer from the major depressive disorder. How to identify patients with depression tendency is one of important health issues. Thus, this project tries to develop a novel technique to automatically identify the depression tendency of bloggers using their blog posts. 263  With the fast growth of social networks, bloggers usually write daily posts with their emotion and events happened in work, home, or life. Although there are lots of research works about emotion analysis and classification, to our knowledge, there is no work focusing on prediction of blogger’s depression tendency based on emotion analysis. In this project, we try to analyze key factors affecting major depressive disorder, such as negative event, negative emotion, symptom and negative thought, and then use these four factors to assist bloggers to predict depression tendency. Therefore, we focus on the investigation of the following two research issues (1) analysis of relevant factors of depression on blog posts written by patients with the major depressive disorder, (2) development of event-emotion-driven depression tendency prediction model. 關鍵詞：憂鬱傾向，事件，負面情緒，症狀，負面想法 Keywords: Depression Tendency, Event, Negative Emotion, Symptom, Negative Thought. 一、緒論 雖然台灣近年經濟起飛，生活富裕，但民眾似乎愈活愈不快樂，台灣民眾的生活快 樂指數在東亞七個國家敬陪末座 [24]。近年來自殺的報導逐年增加，甚至許多年輕學 子的自殺事件也時有所聞，這些自殺事件大部分是因為生活中遭遇巨大壓力，或是已經 患有憂鬱病症一段時間，最終因強烈的負面情緒引發自殺負面想法而造成悲劇。憂鬱症 已列為聯合國世界衛生組織視為新世紀三大疾病，與癌症、愛滋病一起蠶食鯨吞著人民 的身心健康。根據聯合國世界衛生組織估計，全球目前有二到四億人口正為憂鬱症所苦， 估計在亞洲至少約有五千萬的憂鬱症患者，且人數不斷上升。2020年，憂鬱症將與心臟 病，成為影響人類生活甚巨的前二大疾病。根據統計，台灣地區2007年統計結果，憂鬱 症盛行率約8.9％，換言之，超過兩百萬人罹患憂鬱症。40%的憂鬱症患者會有輕生或自 殺的念頭，10~15%的患者因自殺而死亡。因此，憂鬱症的防治變得日益重要，所以有 效的找出有憂鬱傾向的民眾已經是一項不容忽視的醫療衛生議題。 網際網路的快速發展造就社群網路的興起，許多社群網路服務網站提供人們彼此更 快速方便的聯絡溝通模式，像是部落格(Blog)和近年火紅的微網誌(Micro Blog)。痞客邦 (PIXNET)、隨意窩(Xuite)、yam天空部落等都是國內知名的部落格服務網站。另外臉書 (Facebook) 、推特(Twitter) 、噗浪(Plurk) 等都是非常熱門的國際微網誌服務網站。部 落格提供網路使用者隨意撰寫文章紀錄生活中遭遇的點點滴滴，並抒發心情感受，有驚、 有怒、有喜樂也有悲傷。根據我們對大量的部落格和論壇文章觀察，發現許多文章內容 出現事件、負面情緒、症狀及負面想法等相關詞彙，如圖一所示，作者面對 ”重考” 事件，心理產生多種負面情緒如 ”大哭”、”恐慌”、”焦慮”、”沮喪”等，結果也出現一些 生理症狀 ”睡不好”，甚至負面想法 ”撐不下去了”。經由長期追蹤文章負面情緒詞的類 型與出現頻率，可以了解部落格作者所發生的心理問題。所以本研究首先嘗試利用部落 格文章內容來分析作者的負面情緒，然後偵測部落格作者是否有強烈情緒不穩或憂鬱傾 向，或甚至產生自殺企圖。我們期待開發有效的創新技術，從部落格文章判斷有憂鬱傾 向的作者，進而協助這些作者預防或治療憂鬱症。本研究特別關注兩項重要研究議題， 並開發相關處理技術：(1)憂鬱症患者網誌文章的憂鬱傾向與相關因素分析，(2)發展部 264  落格作者的憂鬱傾向預測模型。因此我們提出事件情緒驅動的憂鬱傾向預測模型 (Event-Emotion-driven Depression Tendency Prediction Model)，藉由負面情緒、事件、症 狀和負面想法特徵的分析，然後判別出部落格作者的憂鬱傾向。 圖一、憂鬱症患者發表網誌文章 (藍色矩形實線代表事件；紅色矩形虛線代表負面情緒； 紫色橢圓實線代表症狀；綠色橢圓虛線代表負面想法) 二、 文獻探討 （一） 憂鬱症簡介與臨床診斷技術 憂鬱症通常是指重性憂鬱障礙(major depressive disorder)，在醫學上被視作一種精神 疾病。憂鬱症患者的典型症狀是心情低落，通常會沉浸在憂鬱的情緒狀態中，對一些有 興趣的事物皆感無趣，絕望或是認為人生沒有價值，更甚者會有自殺念頭。在生理上也 會出現症狀，例如失眠、沒有食慾造成體重下降、疲勞、無精打采沒有活力或是出現腸 胃問題等。 憂鬱症的病因在醫學上目前尚未確認，從心理學的研究認為一個人的人格發展在許 多方面促使了憂鬱症的發作，很多學派支持這個論點，例如精神分析學、存在主義心理 學等[10, 12]。而社會學的研究觀點則前瞻地指出身處的環境、人際關係與遭遇的生活 265  事件是罹患憂鬱症的重要原因[26]，例如家庭功能受損或是處於惡劣工作環境中。本研 究認為生物、心理、社會這三個因素都是重要影響因素，另外對於影響部落格作者負面 情緒的負面生活事件將深入探究。 現今醫學的憂鬱症診斷標準主要是根據美國精神疾病協會(American Psychiatric Association, APA) 的精神疾病診斷與統計手冊第四版修訂版(DSM-IV-TR) (2000)[1]和 世界衛生組織(World Health Organization, WHO) 的國際疾病與相關健康問題統計分類 (ICD-10) (2007) [17]，另外董氏基金會[25]也有提供憂鬱症的自我診斷評量表讓一般民 眾自我篩檢。下面簡單說明 DSM-IV-TR 有關憂鬱症的九項診斷標準： 1. 憂鬱情緒：快樂不起來、煩躁和鬱悶。 2. 興趣與喜樂減少：提不起興趣。 3. 無法專注：無法決斷、矛盾猶豫、無法專心。 4. 體重和食慾失常：體重下降或增加、食慾下降或增加 5. 失眠(或嗜睡)：難入睡或整天想睡。 6. 精神運動性遲滯(或激動)：思考動作變緩慢、腦筋變鈍。 7. 疲累失去活力：整天想躺床、體力變差。 8. 無價值感或罪惡感：覺得活著沒意思、自責難過，都是負面想法。 9. 自殺意圖：反覆想到死亡，甚至有自殺意念、企圖或計畫。 （二） 部落格情緒分析 近幾年來部落格的服務快速崛起，許多學者發現情緒是部落格網誌文章中一個重要 的成分，而國內台大資工系陳信希教授最近幾年也有不少的研究關注在部落格的情緒分 析上[11, 22, 23]。此外，近幾年以情緒偵測和分類為主的相關研究[11, 20]快速增加，而 部落格網誌內容主題分析也逐漸地變成熱門的研究對象[6]。許多研究[3, 11, 19, 20, 21] 皆針對部落格網誌文章的內容主題與情緒，提出各式各樣的方法進行情緒辨識與分類。 Leshed and Kaye[9]針對 LiveJournal8.com 網站部落格使用者如何詮釋他們的心情做了一 個全面性的調查，該網站提供了 132 種心情選項當作部落格使用者情緒標記。Hsu 和 Lin[3]也提供了一個 SVM 分類器對大量的文字列表做心情的關聯分析。例如，”電腦” (Computer)這個詞彙相當有可能被歸類到”煩悶”的部落格條目。Yang 等人[19, 20, 21] 利用 SVM 和 CRF 對部落格網誌文章作情緒的分類，他們以句為單位和以文章為單位 進行大規模效能評估。這些情緒偵測和分類的相關研究觸發本研究對於情緒研究的延伸 應用，我們嘗試利用負面情緒特徵提出創新的部落格作者的憂鬱傾向預測研究。 （三） 部落格事件擷取 本研究是以紀錄部落格作者個人日常生活上引發負面情緒的負面事情作為事件的 定義，大概以家庭、感情、學業、工作四種類型為主的生活事件。過去有關事件擷取的 相 關 研究 主要來自於兩個研究領域，一個是主題偵測與追蹤(Topic Detection and Tracking, TDT)，另一個是自然語言處理。 Chen 等人 (2008)提出的 TSCAN [23]和 Kumaran 等人[7]提出的 NED 皆是 TDT 相 關的研究，在此他們對於 Topic 的定義是一個有重大影響和意義的事件或活動。大部份 有關事件的相關研究主要都是以新聞熱門事件為主[8]。Teng and Chen [22]提出了利用 Temporal Collocation 的方法對部落格網誌文章做事件抽取，但是他們著重在熱門主題事 件抽取，相對來說本研究則深入探究更瑣碎的生活事件抽取為主，技術上應該比較困 難。 這幾年在自然語言處理領域中，有需多相關事件擷取的研究。2005 年 Pustejovsky 266  [14, 15]為了探究事件和時間的關係提出 TimeML (Time Markup Language)概念。接著 2007 年，Mani (2007)利用 TimeML 的時間標註結構，標註新聞文章的事件、時間和彼 此關係。Pustejovsky 開始進行 Textual Inference Tasks [4, 5, 16] 的研究，首先他以 TimeML 為基礎建構 Event Structure Lexicon (ESL)，針對 Event Implicature 和 Entailed Subevent 進行推論。Palmer 等[13]作了不少由動詞出發的事件表達，分析，與預測研究。 他們對於 Event Relation 的偵測，主要是運用學術上廣泛使用的詞典資源，如：Wordnet 和 Framenet，來訓練包含語法與語意資訊的特定領域動詞詞彙網 Verbnet。 三、研究方法 為了從部落格作者的網誌文章瞭解和預測部落格作者的憂鬱傾向，本研究搜集了大 量憂鬱症患者的網誌文章或 BBS 論壇文章，藉由深入觀察事件、負面情緒、症狀、負 面想法等四項重要特徵對於憂鬱症的複雜影響與關聯，我們首先嘗試提出創新的分析和 預測技術： (一) 提供憂鬱症患者網誌文章的憂鬱傾向與相關因素分析，(二) 建構一個 事 件 情 緒 驅 動 的 憂 鬱 傾 向 預 測 模 型 (Event-Emotion-driven Depression Tendency Prediction Model)。這些分析報告和創新技術應該能夠有效地提早判斷具有憂鬱傾向的 部落格作者，建議他們盡速尋求專業醫療的協助。下面我們將詳細說明本研究研究及發 展的主要分析方法與預測技術。 （一） 憂鬱症患者網誌文章的憂鬱傾向與相關因素分析 1. 初步的觀察和分析 根據我們對憂鬱症患者網誌文章的觀察，當憂鬱症患者在撰寫文章時經常會出現 各種負面情緒字眼例如”大哭”、”恐慌”、”焦慮”、”沮喪” (圖一)，而這些負面情緒大部 分由一般生活上的事件所引起，如”重考”。比較嚴重的憂鬱症患者在撰寫文章時，除了 會出現比較強烈的負面情緒字眼，甚至會出現身體症狀和引發心理嚴重的負面想法，例 如”想死”和”自殺”。 2. 主要的分析方法： 藉由大量憂鬱症患者網誌文章觀察，我們提出以事件、負面情緒、症狀、負面想 法四項重要特徵為主的憂鬱傾向創新分析方法，然後我們根據兩個醫學分析模型來探討 我們提出的分析方法的有效性和優點。 (1) 生物心理社會分析模型 1977 年學者 Engel[2]提出生物、心理、社會三合一分析模型(Biopsychosocial Model, BPS Model)的新醫學概念，對於病患面對疾病的分析因素包含了生物面(Biological)、心 理面(Psychological)和社會面(Social)的三方因素。其中生物面即是身體症狀(Symptom)， 心理面則涵蓋了情緒(Emotion)、想法(Thought)與行為(Behavior)，社會面即是病患面對 的環境因素，換個角度講，也就是病患所發生的生活事件(Event)。透過進一步的分析比 較後，我們可以將 BPS 模型的三個主要因素對應到我們提出的創新憂鬱症分析方法的 四項因素，如表一所示，這樣的相似對應關係顯示我們的分析方法應該全面性地涵蓋憂 鬱傾向的相關重要因素。 表一、BPS model 與本研究分析方法的對應關係 267  Analysis Model BPS model Correspondence Our analysis method  Biological Symptom Symptom  Analysis Factors Psychological Emotion Thought Behavior Negative Negative Thought Emotion  Social Event Negative Event  (2) DSM-IV-TR 憂鬱症臨床判別標準 現今的醫學尚未提供生物檢測方法以直接確診憂鬱症患者。以重度憂鬱症而言， 精神科醫師目前主要根據美國精神疾病協會(American Psychiatric Association, APA) 的 精神疾病診斷與統計手冊第四版修訂版(DSM-IV-TR)的九項判別標準對病患做篩檢和 診斷。如表二所示，我們提出的憂鬱傾向分析方法的負面情緒因素，可以相對應 DSM-IV-TR 的前三項判別標準(項次 1、2、3)。憂鬱症患者的網誌文章也常出現負面情 緒伴隨著負面的生理症狀，譬如說哭泣、頭痛、失眠、食慾下降等等。這些症狀剛好符 合表 1 中 DSM-IV-TR 的 4、5、6、7 項判別標準。另外憂鬱症患者的網誌文章也常出現 負面想法，例如自殺、跳樓、自殘、燒炭等等。這些負面想法也符合表二中 DSM-IV-TR 的第 8、9 兩項判別標準。根據這三項因素的對應關係，可以清楚顯示我們的分析方法 應該有效地涵蓋憂鬱症臨床診斷判別標準。值得注意的是我們提出的事件因素並未出現 在 DSM-IV-TR 的九項判別標準，本研究提出這項創新因素，並深入探究是否可以有效 地協助分析憂鬱傾向。  表二、DSM-IV-TR 重度憂鬱症九項判別標準與本研究提出的憂鬱因素對應關係  判別標準 說明  憂鬱傾向 因素  
Pitch contours are important for synthesizing highly natural speech signal. In this paper, we study a new pitch-contour generation method. The method proposed is to combine ANN prediction module with global-variance matching (GVM) and real contour selection (RCS) modules. Here, a syllable pitch contour is first analyzed and then transformed via discrete cosine transform (DCT) to a DCT-coefficient vector. Each sequence of DCT vectors analyzed from a training sentence plus contextual parameters are then used to train the ANN weights and GVM parameters. In pitch-contour generation experiments, we measure variance-ratio (VR) values for objective evaluations. The modules, GVM and RCS, are shown to be helpful to promote VR values. In addition, in subjective evaluation, the pitch-contour generation method, ANN + GVM, is shown to be more natural than the method, ANN only. Also, the method, ANN + GVM + RCS, is shown to be better than ANN + GVM. Keywords: speech synthesis, pitch contour, discrete cosine transform, artificial neural network, global variance, contour selection.  277  一、緒論 一個合成語音信號的自然度主要是由韻律參數(如基週軌跡、音長、音量等)所決定，其 中基週軌跡對於自然度之提升更顯得重要，因此，過去已有許多不同的音節基週軌跡產 生方法被先前的研究者所提出[1, 2, 3, 4, 5, 6]。目前，隱藏式馬可夫模型(hidden Markov model, HMM) 雖 然 已 被 許 多 人 採 用 於 作 語 音 合 成 的 研 究 [7, 8] ， 然 而 MSD-HMM (multi-space probability distribution HMM)產生出的基週軌跡並不十分地令人滿意，這種 情形已有不少人注意到[3, 6]。 我們覺得基週軌跡之產生，並不需要和頻譜係數之產生綁在同一種機制(即 HMM)裡， 並且我們想要進一步提升所產生出的基週軌跡的自然度，因此在本論文中，我們嘗試研 究、提出一種把類神經網路(artificial neural network, ANN)預測[1, 2]、全域變異數匹配 (global-variance matching, GVM)與真實軌跡挑選(real contour selection, RCS)三者作結合 的方法，希望用以提升合成語音的自然度。 過去 Toda 與 Tokuda 提出 GVM 之作法[9]，來對 HMM 產生的頻譜係數作調整，以減緩 發生頻譜過度平滑(spectral over-smoothing)的現象，而藉以提升合成語的音質。在此， 我們發現到 ANN 產生的表示基週軌跡的 DCT (discrete cosine transform)係數，也同樣會 發生過度平滑(over smoothing)的現象，因此我們覺得對 ANN 產生的基週軌跡 DCT 係 數，作 GVM 匹配將有助於提升 ANN 基週軌跡的自然度。此外，我們也受到另一個觀 念的啟發，就是語音轉換(voice conversion)領域中前人提出的，以挑選目標語者音框的 真實頻譜係數來取代轉換出的頻譜係數，如此用以改進轉換出語音的音質[10]。因此， 我們認為把 ANN 產生並且經過 GVM 匹配的 DCT 係數向量 X 作為參考，而據以選出 一個最靠近 X 的真實基週軌跡 DCT 係數向量 Y，然後把 Y 拿去取代 X，如此將可更進 一步提升所產生的基週軌跡的自然度。關於 RCS 的實作，我們可依據各個音節的語境 資料來作語境的分類，然後把屬於不同語境分類的各個真實基週軌跡 DCT 向量，分別 放入不同的收集區(pool)裡。 整體來說，我們系統在訓練階段的處理流程如圖一所示。首先對每個錄音語句的的各個 音節作基週軌跡分析；接著，把各個音節的基週軌跡轉換成固定維度的 DCT 係數向量； 然後拿各個訓練語句的 DCT 向量序列及各音節對應的語境資料，去訓練 ANN 為基礎 的基週軌跡產生模型。除了訓練 ANN 模型之外，我們也對各個訓練語句的 DCT 向量 序列作分析，以求得 GVM 匹配所需的參數。此外，我們依據各個音節的語境分類，把 它的基週軌跡 DCT 向量放入對應的收集區裡。 另一方面，產生基週軌跡的整體流程如圖二所示。首先輸入一個文句，接著經由搜尋詞 典來確認各個中文字的音節發音與音調；依據查詢出的一序列音節發音與音調，就可為 各音節準備它對應的語境參數，然後將各音節的語境參數輸入 ANN 模型，去預測該音 節的基週軌跡(即 DCT 係數)；對於 ANN 預測出的基週軌跡，接著使用訓練階段儲存的 全域變異數(GV)參數去對 DCT 係數進行 GVM 匹配；之後，依據 GVM 匹配調整過的 基週軌跡，我們從訓練階段建立的、且和目前音節具有相同語境類型之真實基週軌跡收 集區中，去找出最接近 GVM 匹配過之基週軌跡的一個真實基週軌跡。 278  start  Training sentences  Detect pitch contour  Transform to DCT coeff. (each syllable’s contour)  Context parameters  Train ANN GV analysis  ANN weights GVM parameters  Collect real contours end  real-contour pools (each context type)  圖一、基週軌跡模型之參數訓練的主流程  start  Dictionary  Input a written sentence  ANN weights  Determine contextual parameters (for each syllable) ANN prediction  GVM param. Real contour pools  GV matching Real contour selection end  Genertd. pitch contour (each syllable)  圖二、基週軌跡產生階段之主流程  279  二、模型參數訓練 如圖一所示，我們需要訓練 ANN 模型的權重，分析出 GVM 匹配所需的參數，及分別 儲存不同語境類型的真實基週軌跡(即 DCT 係數向量)。 （一） 、語句錄音與基週軌跡偵測 在此研究中，我們邀請了一位男性語者於錄音室中錄製了 810 句語句，而總音節數為 7,161 個音節。在錄音之後，先以 HTK (HMM toolkit) 軟體進行自動標音，再使用 WaveSurfer 軟體來對各音節的時間邊界作人工微調。 關於音節基週軌跡之偵測，我們使用 HTS (HMM-based speech synthesis system)軟體內含 的 SPTK (Speech Signal Processing Toolkit)模組[8]來進行，並且設定音檔的取樣率設為 22,050 Hz，而音框位移則設為 110 個樣本點。在自動偵測基週軌跡之後，我們發現有 許多音框所偵測出的基頻值是錯誤的，例如一個有聲(voiced)音框的基頻值可能被偵測 為 0，即誤判為無聲(unvoiced)，或是被偵測成真實頻率的一半或兩倍的情形。因此我們 撰寫了一個工具程式，來對偵測錯誤的基週軌跡作半自動或是手動的更正處理。  （二） 、離散餘弦轉換  由於一個語句中各音節的基週軌跡長度不一，長度可能介於 30 至 80 個音框之間，為了 把基週軌跡表示成固定維度數的資料，我們選擇以離散餘弦轉換(DCT)之係數來表示各 音節的基週軌跡。至於維度數量之選擇，在比較過多種維度數之 DCT 轉換與反轉換回 來的基週軌跡曲線後，我們決定將維度數設為 24。一個原始的基週軌跡、和 DCT 反轉 換回來之曲線例子如圖三所示，我們覺得 16 階 DCT 反轉換所得之曲線，仍不夠忠實於 原始曲線。  詳細來說，本研究裡使用的是 DCT-I 之離散餘弦轉換[11]，其正向轉換之公式為：   c(m) = x(0) + (−1)m ⋅ x(N −1) + 2 ⋅  N −2 k =1  x(k  )  ⋅  cos(  m⋅k ⋅π N −1  ),  (1)  m = 0,1,..., 23  其中 x(k)表示一個音節基週軌跡的第 k 個音框的基頻值(以 Hz 為單位)，c(m)表示 DCT 轉換後的第 m 階係數，而 N 則是該音節的音框數。  對於公式(1)，其對應的 DCT 反轉換公式為：   x(k )  =  
In this study, we establish a method to create speech and text synchronized audiobooks with “speech recognition” and “cloud text-to-speech” technology. The user can prepare his own arbitrary articles to create the learning materials for "Shadowing technique" with this method. Besides, the materials are made by "word-level" speech and text synchronized audiobooks. These audiobooks are created by "timed-text" files, and the files are produced from the user's articles and corresponding speech files. By synchronization for speech and text technology, named "CGUAlign", user can easily make the "Timed-text" files. CGUAlign, uses Python to wrap the well-known speech recognition technology─HTK(Hidden Markov Model Toolkit). Just providing text file and the corresponding speech file, obtained from cloud text-to-speech technology, CGUAlign can create the timed-text file to achieve the synchronization of speech and text. Subsequently, we also build a simple website created with JavaScript. This website can use the timed-text file as CALL(Computer-assisted Language Learning) purposes. Using the website, user can browse the synchronized audiobooks to easily do Shadowing technique. Finally this website also provides dictionary function to achieve the goal of CALL. 關鍵字：語音辨識、文字轉語音、雲端語音合成、隱藏式馬可夫模型工具程式庫、電腦 輔助語言學習、音文同步 Keywords: Speech Recognition 、 Text-to-speech 、 HTK 、 Computer-assisted Language Leanring、Speech-text Syncronization 一、緒論 隨著地球村的趨勢來臨，「語言學習」是現今社會普羅大眾所需要面臨的一項課題， 也是一種趨勢，因此培養良好的多國語言能力，已成為當今社會不可或缺的目標。 針對於台灣人而言，英語學習的需求更是顯得比其他語言來得更為重要，事實上我 們知道，「語言學習」並非只是如同一般課程的學習，又分為「聽」、「說」、「讀」、 「寫」，其需要經過「自我內化」、「練習」、「演繹」等過程才能根深蒂固的記憶在 我們腦海中，而在舊有的自我學習中，又缺乏獨特的語言學習環境，缺乏練習的對 象，如果要請他人來指導教學，往往又所費不貲，而數位化雲端學習在當今的世代 是一個熱門的趨勢，如「視訊教學」、「線上學習」，這些都是網路普及與資訊發展 下的重要產物，若我們可以利用適度的電腦回饋結合數位化學習，也許能為更多使 用者造就一個新形態的自我學習方式。 本研究設計一個方便處理有聲書音文同步的技術，利用雲端的文字轉語音 (Text-to-speech)技術，結合語音辨識(Speech Recognition)技術，讓使用者能夠使用 自行準備的文本來製作自己的跟述練習的學習素材，製作達到詞層級(Word-level) 的音文同步有聲書，其不僅可以提供音文同步的電子書供使用者閱讀文章，也可以 讓使用者藉由朗誦文章的方式，並透過跟述練習的實作和即時翻譯的效果，以達到 290  自我內化學習及增進語言能力。 二、相關研究 (一) 跟述練習(Shadowing Technique) Shadowing technique 是一種語言學習技巧，一般我們稱之為跟述練習或者是影子練 習，與目前台灣所常見的講述式教學法不同，它比較相似於所謂的聽說式教學法， 但其又與聽說式教學法有一點點不同，跟述練習與聽說教學法較為不同的地方在於 聽說教學是教師以自身的演繹口說內容來促使學生反覆練習語言內容而跟述練習 比較傾向於學習者自主訓練的方式。 語言學習者跟述的學習對象不一定為真人，也可能僅是一個語音或影像檔，在跟述 的過程中，跟述者以自我所能的發音技巧以及閱讀能力去盡可能地模仿所要學習的 語言對象或者是影音內容，這種學習方式有如鸚鵡學舌，是一種反覆練習以及自我 內化的過程，在其他的研究中[1]我們也可以發現到利用這樣的語言學習技巧是一 種快速內化方式去學習一種語言的方法。 (二) Google Translate 現在 Google 在許多方面廣泛地被使用，不只是在搜尋引擎的功能上，許多人在遇 到語言上問題的時候，往往會藉由 Google 所提供的翻譯功能─ Google Translate 幫忙。 Google Translate 所提供的翻譯功能非常強大，提供近百種的語言相互的翻 譯，而且在取得此功能的便利性上，也是無與倫比，據 Google 統計，至 2015 年 6 月 Google Translate 每天需要處理超過 1000 億筆字詞。 圖一是 Google Translate 的使用介面，其提供的即時翻譯功能，讓使用者可以在左 邊的輸入欄位輸入文字，翻譯結果會即時在右邊的結果框顯示，將滑鼠鼠標移到翻 譯結果文字上可以看到其對應的原文，圖一範例為將中文翻譯成英文的範例。 圖一、Google Translate 網頁介面 291  除此之外，Google Translate 也提供朗讀的功能，即文字轉語音(Text-to-speech)的人 工語音合成朗讀的功能，另外也提供查詢文字拼音的功能，即能夠提供非拼音語言 的羅馬拼音查詢。 Google Translate 所提供的三種功能─翻譯、朗讀、拼音已經很適合做初步的語言 學習，但是其頂多只能製作出句層級(Sentence-level)的效果，其句層級是指在音文 同步播放時當下語音內容是以句子為單位的顯示於畫面中。由而本研究則是進一步 利用這三種功能，利用拼音和人工語音合成的功能能夠製作出 Google Translate 所 無法達到的詞層級(Word-level)的音文同步有聲書，其詞層級是指在音文同步播放 時當下的語音內容除了以句子為單位的顯示於畫面中並附加句子中每個詞的顯示 效果，而詞層級的音文同步有聲書能夠讓學習者更容易的耳聽、眼看來做跟述技巧 的練習。 (三) HTK (Hidden Markov Model Toolkit)[2] HTK 的全名為 Hidden Markov Model Toolkit，是一套應用於語音訓練與辨識的免 費軟體。HTK 於 1989 年開始由英國劍橋大學工程系 (Cambridge University Engineering Department, CUED)的機器智能實驗室 (Machine Intelligence Lab，或是 大眾較為熟悉的 Speech Vision and Robotics Group)進行開發，該團隊利用隱藏式馬 可夫模型 (HMM)建造出一套 HMM-based 的語音辨識系統。1999 年十一月，微軟 購入擁有此軟體的 Entropic 公司，並於翌年將 HTK 定位為免費軟體，期望 HTK 作為語音辨識的共同平台，便能豐富 HTK 的功能性，以及提升語音辨識等相關技 術。為了達到這個目標，HTK 建置官方網站，以提供開放的完整功能原始碼及說 明書。 由於語音辨識的原理包含相當高深的數學，相對地使得程式碼也不易撰寫，造成進 入門檻高，複雜度不易掌控的情況產生。但自從 HTK 在 2000 年定位成開放原始碼 的免費軟體後，大幅降低了進入門檻，並加速提昇語音技術的發展，綜觀目前國內 外語音技術相關的實驗工具和系統開發，絕大部分都以 HTK 為主流；由此可知， HTK 在語音技術的研究領域占了不可或缺的地位。 292  (四) CGUAlign[3] CguAlign 是模仿[4]以 Perl 包裝 HTK 的方法，CguAlign 改用 Python 將 HTK 包裝、 運用的一套技術，為本實驗室一個方便處理音文同步有聲書的技術，原本是為了將 從 Youtube 上所取得的句層級 Timed-text sbv 檔，以程式自動切音取代傳統人工手 動的方式切音成詞層級 Timed-text 檔的方法，此方法除了可以減少人力資源，還 能夠大幅減少人工手動切音所浪費的時間。只需輸入文字檔以及聲音檔，經過音文 對齊的處理，即可得到帶有時間點的 Timed-text 文字檔案。但此技術無法處理過 長的聲音檔，因此希望站在雲端語音合成技術上，將以"句"為層級的 TTS 改良，使 其能夠達到"字"的層級。圖二為 CGUAlign 之流程圖。 圖二、CGUAlign 流程圖 293  圖二.a、切到句的 sbv 檔 圖二.b、trs 檔 圖二.c、lrc 檔 294  三、研究方法 本章節內容旨在介紹整體的研究方法，全章分為三節， (一) 雲端語音合成(Text-to-speech,TTS) (二) CGUAlign 語音辨識-ForceAlignment (三) 網站呈現(Website presentation) 圖三為系統整體流程圖，原始文字檔經由(一)會得到句層級的帶有時間點的文字的 檔案，再經由(二)會得到詞層級的帶有時間點的文字的檔案，最後經由(三)能夠以 音文同步有聲書的方式瀏覽此帶有時間點的文字的檔案，並以此做一個電腦輔助語 言學習的動作。 圖三、系統流程圖 (一) 雲端語音合成(Text-to-speech,TTS) 本節將說明如何將純文本經由文字的預先處理，透過 Google Translate 的雲端文字 轉語音(Text-to-speech,TTS)的服務，取得 TTS 的語音檔，並將此語音檔和文字檔結 合產生句層級的 Timed-text 檔案以供下一階段 CGUAlign 使用。 295  1. 文字切割  圖四、雲端語音合成流程圖  因 Google Translate TTS 無法直接輸入長度大於 100 的字串，因此需要先做文字分 割，將其長度降低於小於 100，並稱此為句層級的純文字檔，基本的切割方法只先 按照標點符號作切割。 Step1:按照標點符號做切割例如: "句號"(“.”,”。”) 、"問號"(“?”,”？”) 、" 驚嘆號"(“!”,”！”) 、 "破折號"(“-”,”─”) 、"冒號"(“:”,”：”) 、"逗號"(“,”,”，”)。 Step2:若最終字串長度還是有超過 100 的，則會從超過 100 的字串以中間的"空白" 切割。  2. 連結 Google 發出請求  此節將討論如何藉由 Google Translate 的 TTS 服務將純文字轉成 TTS 的語音檔案， 利用 Python 的 Standard Library─”urllib.request"和"urllib.parse"，傳送 HTTP GET Request 至 Google Translate 的 URL，其 URL 為： http://translate.google.com/translate_tts 其 URL 的 parameters 如表一。  296  parameters tl q total idx textlen  表一、Google Translate TTS Parameters 意義 Target Language，目標語言，表示要文字 TTS 的語言種類。 Query，欲 TTS 的文字。 Total number of text segments，文章分段的個數。 Index of text segments，文章分段的指標。 String length in this segment，此 Query 的字串長度。  
This study is focus on speech emotion recognition through machine learning method. We add two nonlinear dynamical features: Shannon entropy and curvature index, of each frame other than the traditional features such as pitch, formant, energy, MFCCs. After feature extraction, Fisher discriminant ratio and Genetic algorithm were applied in order to reduce the number of features. We use SVM classifier and cross validation method to discriminate seven emotions in Berlin emotion database. The analyzed results after adding of the nonlinear features show that the emotion recognition rates were 88.89% and 86.21% for male and female, respectively. 關鍵詞：情緒辨識、非線性特徵、支持向量機 Keywords: Speech emotion recognition, non-linear features, support vector machine 306  一、 緒論 在人工智慧、機器學習與網路資訊的快速發展下，在不同領域都已經有許多事情 可以由機器取代，如會議安排、語言學習、語音服務、新聞播報、汽車駕駛等等， 但如果僅僅只是由機器單方面提供制式化的回應服務，或許不是那麼適當，因此 讓機器偵測得人類所要表達的情緒訊息，接著給予最適當的回應是一項重要的機 制。這不僅僅可以增進人機互動的樂趣，也可在一般客服機器提供客觀資訊外， 給予適切地問候話語；在智慧家庭與照護系統方面，若可得知使用者當下情緒而 做出反應，如切換音樂、燈光控制等等，可以提升人機互動的成效；其他像是娛 樂產品的介面也是可以應用的主題。目前在機器與人的互動上，基本上可利用視 覺與聽覺兩種人類感官，本研究著重於聽覺之語音情緒辨識系統，期望藉由語音 訊號來分辨使用者目前的情緒，進而提升溝通效果。 對於情緒的描述方式大致可分為離散與維度兩種形式，前者即為日常生活所使用 之詞彙，如開心、生氣、悲傷等，在如此大量之情感詞彙中，一般認為能夠為人 類與具有社會性之哺乳動物所共有情感稱為基本情感，不同學者對於基本情感的 定義也不相同，其中以 Ekman 提出之六大基本情感較為廣泛被使用，當然亦有 許多依此發展或其他理論而形成的基本情緒，如下表一[1]；後者則將情感狀態 描述於激活度-效價情感空間(arousal-valence emotional space)或是激勵-效價-控 制空間(activation - valence -dominance space)中，其中每一個維度對應著心理學的 屬性[2、3]。基本上，透過聲音來傳遞情緒上大致可分為兩個方向，一為透過語 意，即由字面上的意思；另外是藉由語調來傳遞情緒。而在本研究中則採用了離 散情緒分類及透過語調來擷取特徵，進而作情緒分類判斷。 過去文獻中，Moataz El Ayadi 等人[4]提供不同語料庫收集方式之資訊及許多語音 訊號特徵之計算方式與分類方法；Siqing Wu 等[5]利用調變頻譜特徵(MSFs)與不 同特徵組合進行情緒分類，其最佳準確率 91.6%為 MSFs 與聲韻(prosodic)特徵的 組合法；Patricia Henríquez[6]等利用非線性動態特徵進行語音情緒辨識研究，準 確率最高可達 80.75%；Ali Shahzadi 等[7]以聲韻特徵、頻譜特徵與非線性動態特 徵依不同組合進行研究，其準確率最高為男性 85.9%，女性為 82.72%。本研究 的目標是透過分析語音來辨識情緒，以過去學者之研究為基礎，利用語音訊號擷 取特徵量，再以挑選後的特徵量作為支持向量機(support vector machine, SVM)中 的訓練資料，藉此訓練出分類模型，結果證明在一般常見語音特徵如音高(pitch)、 能量(energy)、共振峰(formant)、梅爾倒頻譜係數(Mel-scale Frequency Cepstral Coefficients, MFCC)，額外加入了夏農熵(Shannon entropy)和曲率指標兩項非線性 特徵有提升語音情緒辨識之效用。 307  學者  表 一、基本情感之定義 基本情感  Arnold  Anger, aversion, courage, dejection, desire, despair, dear, hate, hope, love, sadness  Ekman, Friesen, Ellsworth Anger, disgust, fear, joy, sadness, surprise  Fridja Gray Izard James McDougall Mower Oatley, Johnson-Laird  Desire, happiness, interest, surprise, wonder, sorrow Desire, happiness, interest, surprise, wonder, sorrow Anger, contempt, disgust, distress, fear, guilt, interest, joy, shame, surprise Fear, grief, love, rage Fear, disgust, elation, fear, subjection, tender-emotion, wonder Pain, pleasure Anger, disgust, anxiety, happiness, sadness Panksepp  Panksepp Plutchik Tomkins Watson Weiner, Graham  Anger, disgust, anxiety, happiness, sadness Acceptance, anger, anticipation, disgust, joy, fear, sadness, surprise Tomkins Anger, interest, contempt, disgust, distress, fear, joy, shame, surprise Fear, love rage Happiness, sadness  308  二、 研究方法  (一) 實驗資料庫  本研究的資料來自於德國柏林語音情緒資料庫(Berlin emotion database)[8]，其中 包含了生氣(anger)、無聊(boredom)、厭惡(disgust)、害怕(fear)、開心(joy)、中性 (neutral)和傷心(sadness)共七種情緒，由十位專業演員(五男、五女)各別演示上述 七種情緒對應的句子所組成，共有 535 句語音訊號。  (二) 特徵擷取  將語音訊號進行音框(frame)的切割，通常視窗長度為 20~40ms，用來計算特徵參 數，而為了讓特徵變化有延續性，會將部分視窗重疊(overlap)，本研究所使用之 視窗長度為 32ms，重疊部分為 16ms。擷取的特徵分為兩部分，一為傳統使用之 聲韻和頻譜特徵，另一部分則是非線性動態特徵 Shannon entropy 和 curvature index。  1. 聲韻特徵  在聲韻特徵中，收集了音高、能量、過零率(zero crossing rate,ZCR)、TEO(Teager energy operator)等常見語音分析特徵。音高擷取方式是使用 ACF(auto-correlation function)，但為了避免 ACF 的值介於一個不定的區間，將其正規化至 1 與-1 之  間後，再搭配音量閾值判斷音高，即得NACF(τ)  =  ∑  2 ∑ s(i)s(i+τ) 。過零率即為訊 s2(i)+ ∑ s2(i+τ)  號過零點的次數，一般而言其值在有語音的時候會比安靜或環境雜訊較大時低，  因此本研究採用此方法搭配音量來判斷 voice activity ratio，voice activity ratio 即  為一段訊號內有語音與無語音的比例(如下圖一)。TEO 則是在還原聲音經過氣管 及人的腔體作用後所產生的語音訊號，TEO(si) = si2 − si−1si+1，上述公式內之 s 即為一個音框內的原始訊號，i 表示第 i 點訊號。  圖 一、 Voice activity detection，綠色線為起始位置，紅色線為結束  309  2. 頻譜特徵 頻域所使用之特徵，第一項為梅爾倒頻譜係數，配合人耳聽覺對不同頻率有不同 的敏感度的特性，提出了這項係數；本研究所使用之 pre-emphasis 之高通濾波器 參數為 0.9，共取 13 個梅爾倒頻譜係數。共振峰是將時域訊號轉為頻域後，取其 包絡線(envelope)後可得到一條較為平滑的頻譜曲線，其中有若干個高點，這些 高點表示能量集中的位置，也就是共振峰，可描述人類聲道中的共振情形(如下 圖二)。本研究利用快速傅立葉轉換(FFT)及 linear predictive coding(LPC)方式取得 第 1 到第 3 個共振峰(F1~F3)的頻率值及其頻寬。  圖 二、Formant 結果  3. 非線性動態特徵 夏農熵在資訊理論中扮演了很重要的角色，除了可用來作為資訊量的量測外，同 時也是對某個系統之不確定性或混亂程度的度量方法，若熵值越高則系統的不確 定性(uncertainty)越高，反之亦然。隨機變數 的夏農熵可定義為 H( ) = − ∑ ( ) log ( ) ， ∈ 其中 ( ) = { = }, ∈ 。使用不同基底會有一轉換常數的差異。  曲率指標[9]是一動態系統的指標，曲率指標之定義如下，對於 維空間曲線  (t) ∈ ℝ 可得 − 1個高維度曲率κi, 1 ≤ i ≤ − 1，則曲率指標為  ∫ ( )𝑑𝑡  Κ = lim 0  , 1 ≤ ≤ − 1。  →∞  由上式可知，曲率指標是藉由動態平均的方式來描述，其功用在於系統出現結構  變化時，可以在指標上出現相應變化，是以吾人預期，當不同情緒變化表現在語  音訊號時，其對應的曲率指標也會有所不同。計算曲率指標前，需要運用相空間  重構的技術將語音訊號重構到高維度空間上，本研究中重構維度 = 3，且只有  K1在特徵挑選過程中被選中。  310  圖 三、 Curvature index 計算結果 4. 統計值 計算語音訊號每個音框的上述特徵值後進行統計，其統計量包含最小值(min)、 最大值(max)、最大與最小值的差(range)、平均(mean)、中位數(median)、切尾均 值(trimmed mean)之 10%與 25%、第 1、5、10、25、75、90、95、99 的百分位 數(percentile)、四分差(interquartile range)、平均差(average deviation)、標準差 (standard deviation)、偏態(skewness)和峰度(kurtosis)共 20 項。另外也計算相鄰兩 音框之一階與二階倒數之統計量，以表示兩音框間的變化程度，最後將所有統計 量當作語音訊號之特徵進行挑選與分類。  (三) 特徵挑選 特徵選取的目標是要從原有的特徵集合中挑選出鑑別能力較好的特徵，使其辨識 率能夠達到最高值，不但能夠簡化分類器的計算，並可藉此了解分類問題關係。 特徵挑選時使用了 10 折交叉驗證(10-fold cross validation)，避免對單一資料形成 over-fitting。  本研究利用了費雪鑑別比(Fisher discriminate ratio, FDR)與基因演算法(genetic algorithm, GA)進行特徵挑選。依據費雪判別分析的概念，分屬二個類別的特徵 其組內差距越小，組間差距越大，可獲得越好的分類效果。多組類別之 FDR 計 算方式如下[10]  2 FDR(u) = C(C − 1) ∑ c1  ∑ c2  (µc1,µ − µc2,µ )2 σc21 + σc22  ,  
We describe the design, development, and API for two discourse parsers for Rhetorical Structure Theory. The two parsers use the same underlying framework, but one uses features that rely on dependency syntax, produced by a fast shift-reduce parser, whereas the other uses a richer feature space, including both constituent- and dependency-syntax and coreference information, produced by the Stanford CoreNLP toolkit. Both parsers obtain state-of-the-art performance, and use a very simple API consisting of, minimally, two lines of Scala code. We accompany this code with a visualization library that runs the two parsers in parallel, and displays the two generated discourse trees side by side, which provides an intuitive way of comparing the two parsers. 
We present a toolkit for coreference resolution error analysis. It implements a recently proposed analysis framework and contains rich components for analyzing and visualizing recall and precision errors. 
 tail1  We present hyp, an open-source toolkit for the representation, manipulation, and optimization of weighted directed hypergraphs. hyp provides compose, project, invert functionality, k-best path algorithms, the inside and outside algorithms, and more. Finite-state machines are modeled as a special case of directed hypergraphs. hyp consists of a C++ API, as well as a command line tool, and is available for download at github.com/sdl-research/hyp. 
Educational research has demonstrated that asking students to respond to reﬂection prompts can increase interaction between instructors and students, which in turn can improve both teaching and learning especially in large classrooms. However, administering an instructor’s prompts, collecting the students’ responses, and summarizing these responses for both instructors and students is challenging and expensive. To address these challenges, we have developed an application called CourseMIRROR (Mobile Insitu Reﬂections and Review with Optimized Rubrics). CourseMIRROR uses a mobile interface to administer prompts and collect reﬂective responses for a set of instructorassigned course lectures. After collection, CourseMIRROR automatically summarizes the reﬂections with an extractive phrase summarization method, using a clustering algorithm to rank extracted phrases by student coverage. Finally, CourseMIRROR presents the phrase summary to both instructors and students to help them understand the difﬁculties and misunderstandings encountered. 
The RExtractor system is an information extractor that processes input documents by natural language processing tools and consequently queries the parsed sentences to extract a knowledge base of entities and their relations. The extraction queries are designed manually using a tool that enables natural graphical representation of queries over dependency trees. A workﬂow of the system is designed to be language and domain independent. We demonstrate RExtractor on Czech and English legal documents. 
In this demonstration, we will present our online parser1 that allows users to submit any sentence and obtain an analysis following the specification of AMR (Banarescu et al., 2014) to a large extent. This AMR analysis is generated by a small set of rules that convert a native Logical Form analysis provided by a preexisting parser (see Vanderwende, 2015) into the AMR format. While we demonstrate the performance of our AMR parser on data sets annotated by the LDC, we will focus attention in the demo on the following two areas: 1) we will make available AMR annotations for the data sets that were used to develop our parser, to serve as a supplement to the LDC data sets, and 2) we will demonstrate AMR parsers for German, French, Spanish and Japanese that make use of the same small set of LF-to-AMR conversion rules. 
We showcase ICE, an Integrated Customization Environment for Information Extraction. ICE is an easy tool for non-NLP experts to rapidly build customized IE systems for a new domain. 
Abstract Meaning Representation (AMR), an annotation scheme for natural language semantics, has drawn attention for its simplicity and representational power. Because AMR annotations are not designed for human readability, we present AMRICA, a visual aid for exploration of AMR annotations. AMRICA can visualize an AMR or the difference between two AMRs to help users diagnose interannotator disagreement or errors from an AMR parser. AMRICA can also automatically align and visualize the AMRs of a sentence and its translation in a parallel text. We believe AMRICA will simplify and streamline exploratory research on cross-lingual AMR corpora. 
This paper describes Ckylark, a PCFG-LA style phrase structure parser that is more robust than other parsers in the genre. PCFG-LA parsers are known to achieve highly competitive performance, but sometimes the parsing process fails completely, and no parses can be generated. Ckylark introduces three new techniques that prevent possible causes for parsing failure: outputting intermediate results when coarse-to-ﬁne analysis fails, smoothing lexicon probabilities, and scaling probabilities to avoid underﬂow. An experiment shows that this allows millions of sentences can be parsed without any failures, in contrast to other publicly available PCFG-LA parsers. Ckylark is implemented in C++, and is available opensource under the LGPL license.1 
Entity Linking (EL) systems’ performance is uneven across corpora or depending on entity types. To help overcome this issue, we propose an EL workflow that combines the outputs of several open source EL systems, and selects annotations via weighted voting. The results are displayed on a UI that allows the users to navigate the corpus and to evaluate annotation quality based on several metrics. 
We present a syntactic analysis query toolkit geared speciﬁcally towards massive dependency parsebanks and morphologically rich languages. The query language allows arbitrary tree queries, including negated branches, and is suitable for querying analyses with rich morphological annotation. Treebanks of over a million words can be comfortably queried on a low-end netbook, and a parsebank with over 100M words on a single consumer-grade server. We also introduce a web-based interface for interactive querying. All contributions are available under open licenses.  Operator < > <@L <@R >@L >@R ! &| + ->  Meaning governed by governs governed by on the left governed by on the right has dependent on the left has dependent on the right negation and / or match if both sets not empty universal quantiﬁcation  Table 1: Query language operators.  2 Demonstration outline  
“Deep-syntactic” dependency structures bridge the gap between the surface-syntactic structures as produced by state-of-the-art dependency parsers and semantic logical forms in that they abstract away from surfacesyntactic idiosyncrasies, but still keep the linguistic structure of a sentence. They have thus a great potential for such downstream applications as machine translation and summarization. In this demo paper, we propose an online version of a deep-syntactic parser that outputs deep-syntactic structures from plain sentences and visualizes them using the Brat tool. Along with the deep-syntactic structures, the user can also inspect the visual presentation of the surface-syntactic structures that serve as input to the deep-syntactic parser and that are produced by the joint tagger and syntactic transition-based parser ran in the pipeline before deep-syntactic parsing takes place. 
Developing machine learning algorithms for natural language processing (NLP) applications is inherently an iterative process, involving a continuous reﬁnement of the choice of model, engineering of features, selection of inference algorithms, search for the right hyperparameters, and error analysis. Existing probabilistic program languages (PPLs) only provide partial solutions; most of them do not support commonly used models such as matrix factorization or neural networks, and do not facilitate interactive and iterative programming that is crucial for rapid development of these models. In this demo we introduce WOLFE, a stack designed to facilitate the development of NLP applications: (1) the WOLFE language allows the user to concisely deﬁne complex models, enabling easy modiﬁcation and extension, (2) the WOLFE interpreter transforms declarative machine learning code into automatically differentiable terms or, where applicable, into factor graphs that allow for complex models to be applied to real-world applications, and (3) the WOLFE IDE provides a number of different visual and interactive elements, allowing intuitive exploration and editing of the data representations, the underlying graphical models, and the execution of the inference algorithms. 
For the task of question answering (QA) over Freebase on the WEBQUESTIONS dataset (Berant et al., 2013), we found that 85% of all questions (in the training set) can be directly answered via a single binary relation. Thus we turned this task into slot-ﬁlling for <question topic, relation, answer> tuples: predicting relations to get answers given a question’s topic. We design efﬁcient data structures to identify question topics organically from 46 million Freebase topic names, without employing any NLP processing tools. Then we present a lean QA system that runs in real time (in ofﬂine batch testing it answered two thousand questions in 51 seconds on a laptop). The system also achieved 7.8% better F1 score (harmonic mean of average precision and recall) than the previous state of the art. 
Sociolinguists are regularly faced with the task of measuring phonetic features from speech, which involves manually transcribing audio recordings – a major bottleneck to analyzing large collections of data. We harness automatic speech recognition to build an online end-to-end web application where users upload untranscribed speech collections and receive formant measurements of the vowels in their data. We demonstrate this tool by using it to automatically analyze President Barack Obama’s vowel pronunciations. 
We present an open source, freely available Java implementation of Align, Disambiguate, and Walk (ADW), a state-of-the-art approach for measuring semantic similarity based on the Personalized PageRank algorithm. A pair of linguistic items, such as phrases or sentences, are ﬁrst disambiguated using an alignment-based disambiguation technique and then modeled using random walks on the WordNet graph. ADW provides three main advantages: (1) it is applicable to all types of linguistic items, from word senses to texts; (2) it is all-in-one, i.e., it does not need any additional resource, training or tuning; and (3) it has proven to be highly reliable at different lexical levels and multiple evaluation benchmarks. We are releasing the source code at https://github.com/pilehvar/adw/. We also provide at http://lcl.uniroma1.it/adw/ a Web interface and a Java API that can be seamlessly integrated into other NLP systems requiring semantic similarity measurement. 
We present Brahmi-Net - an online system for transliteration and script conversion for all major Indian language pairs (306 pairs). The system covers 13 Indo-Aryan languages, 4 Dravidian languages and English. For training the transliteration systems, we mined parallel transliteration corpora from parallel translation corpora using an unsupervised method and trained statistical transliteration systems using the mined corpora. Languages which do not have parallel corpora are supported by transliteration through a bridge language. Our script conversion system supports conversion between all Brahmi-derived scripts as well as ITRANS romanization scheme. For this, we leverage co-ordinated Unicode ranges between Indic scripts and use an extended ITRANS encoding for transliterating between English and Indic scripts. The system also provides top-k transliterations and simultaneous transliteration into multiple output languages. We provide a Python as well as REST API to access these services. The API and the mined transliteration corpus are made available for research use under an open source license. 
Natural language processing research increasingly relies on the output of a variety of syntactic and semantic analytics. Yet integrating output from multiple analytics into a single framework can be time consuming and slow research progress. We present a CONCRETE Chinese NLP Pipeline: an NLP stack built using a series of open source systems integrated based on the CONCRETE data schema. Our pipeline includes data ingest, word segmentation, part of speech tagging, parsing, named entity recognition, relation extraction and cross document coreference resolution. Additionally, we integrate a tool for visualizing these annotations as well as allowing for the manual annotation of new data. We release our pipeline to the research community to facilitate work on Chinese language tasks that require rich linguistic annotations. 
We present an interactive web-based writing assistance system that is based on recent advances in crosslingual compositional distributed semantics. Given queries in Japanese or English, our system can retrieve semantically related sentences from high quality English corpora. By employing crosslingually constrained vector space models to represent phrases, our system naturally sidesteps several difﬁculties that would arise from direct word-to-text matching, and is able to provide novel functionality like the visualization of semantic relationships between phrases interlingually and intralingually. 
We have developed the TextEvaluator system for providing text complexity and Common Core-aligned readability information. Detailed text complexity information is provided by eight component scores, presented in such a way as to aid in the user’s understanding of the overall readability metric, which is provided as a holistic score on a scale of 100 to 2000. The user may select a targeted US grade level and receive additional analysis relative to it. This and other capabilities are accessible via a feature-rich front-end, located at http://texteval-pilot.ets.org/TextEvaluator/. 
 writing. For example, NetSpeak (www.netspeak.  This paper describes WriteAhead2, an interactive writing environment that provides lexical and grammatical suggestions for second language learners, and helps them write ﬂuently and avoid common writing errors. The method involves learning phrase tem-  org) uses Google Web 1T Corpus to retrieve common phrases relevant to a user query, while Marking Mate (www.readingandwritingtools.com) accepts an user essay and offers a grade with corrective feedback. However, learner writers sorely need concrete writing suggestions, right in the context of  plates from dictionary examples, and extracting grammar patterns with example phrases from an academic corpus. At run-time, as the user types word after word, the actions trigger a list after list of suggestions. Each successive list contains grammar patterns and examples, most relevant to the half-baked sentence. WriteAhead2 facilitates steady, timely,  writing. Learners could be more effectively assisted, if CALL tools provide such suggestions as learners write away. Consider an online writer who is composing a sentence starting with ”We propose a method ...” The best way the system can help is probably not just dictionary-lookup, but rather in-page sugges-  and spot-on interactions between learner writers and relevant information for effective assisted writing. Preliminary experiments show that WriteAhead2 has the potential to induce better writing and improve writing skills.  tions tailor-made for this very context of continuing the unﬁnished sentence. Furthermore, ﬁxed-length ngrams such as (1) method for automatic evaluation and (2) method for determining the is not good enough, or general enough, for all writers address-  
This paper presents a multi-strategy and multisource question answering (QA) system that can use multiple strategies to both answer natural language (NL) questions and respond to keywords. We use multiple information sources including curated knowledge base, raw text, auto-generated triples, and NL processing results. We develop open semantic answer type detector for answer merging and improve previous developed single QA modules such as knowledge base based QA, information retrieval based QA. 
We introduce an interactive interface that aims to help English as a Second Language (ESL) students overcome language related hindrances while reading a text. The interface allows the user to ﬁnd supplementary information on selected difﬁcult words. The interface is empowered by our lexical substitution engine that provides context-based synonyms for difﬁcult words. We also provide a practical solution for a real-world usage scenario. We demonstrate using the lexical substitution engine – as a browser extension that can annotate and disambiguate difﬁcult words on any webpage. 
We aim to improve speech retrieval performance by augmenting traditional N-gram language models with different types of topic context. We present a latent topic model framework that treats documents as arising from an underlying topic sequence combined with a cache-based repetition model. We analyze our proposed model both for its ability to capture word repetition via the cache and for its suitability as a language model for speech recognition and retrieval. We show this model, augmented with the cache, captures intuitive repetition behavior across languages and exhibits lower perplexity than regular LDA on held out data in multiple languages. Lastly, we show that our joint model improves speech retrieval performance beyond N-grams or latent topics alone, when applied to a term detection task in all languages considered. 
In this paper, we propose a method to ﬁnd the safest path between two locations, based on the geographical model of crime intensities. We consider the police records and news articles for ﬁnding crime density of different areas of the city. It is essential to consider news articles as there is a signiﬁcant delay in updating police crime records. We address this problem by updating the crime intensities based on current news feeds. Based on the updated crime intensities, we identify the safest path. It is this real time updation of crime intensities which makes our model way better than the models that are presently in use. Our model would also inform the user of crime sprees in a particular area thereby ensuring that user avoids these crime hot spots. Keywords: Crime detection, Hotspot identiﬁcation, Safest Path, Topic Modeling, Latent Dirichlet Allocation, Latent Semantic Analysis, Natural Language Processing. 
Our thesis proposal aims at integrating word similarity measures in pattern ranking for relation extraction bootstrapping algorithms. We note that although many contributions have been done on pattern ranking schemas, few explored the use of word-level semantic similarity. Our hypothesis is that word similarity would allow better pattern comparison and better pattern ranking, resulting in less semantic drift commonly problematic in bootstrapping algorithms. In this paper, as a ﬁrst step into this research, we explore different pattern representations, various existing pattern ranking approaches and some word similarity measures. We also present a methodology and evaluation approach to test our hypothesis. 
We propose a way to automatically improve the annotation of verbal complex predicates in PropBank which until now has been treating language mostly in a compositional manner. In order to minimize the manual re-annotation effort, we build on the recently introduced concept of aliasing complex predicates to existing PropBank rolesets which encompass the same meaning and argument structure. We suggest to ﬁnd aliases automatically by applying a multilingual distributional model that uses the translations of simple and complex predicates as features. Furthermore, we set up an annotation effort to obtain a frequency balanced, realistic test set for this task. Our method reaches an accuracy of 44% on this test set and 72% for the more frequent test items in a lenient evaluation, which is not far from the upper bounds from human annotation. 
Information Extraction (IE) has become an indispensable tool in our quest to handle the data deluge of the information age. IE can broadly be classiﬁed into Named-entity Recognition (NER) and Relation Extraction (RE). In this thesis, we view the task of IE as ﬁnding patterns in unstructured data, which can either take the form of features and/or be speciﬁed by constraints. In NER, we study the categorization of complex relational1 features and outline methods to learn feature combinations through induction. We demonstrate the efﬁcacy of induction techniques in learning : i) rules for the identiﬁcation of named entities in text – the novelty is the application of induction techniques to learn in a very expressive declarative rule language ii) a richer sequence labeling model – enabling optimal learning of discriminative features. In RE, our investigations are in the paradigm of distant supervision, which facilitates the creation of large albeit noisy training data. We devise an inference framework in which constraints can be easily speciﬁed in learning relation extractors. In addition, we reformulate the learning objective in a max-margin framework. To the best of our knowledge, our formulation is the ﬁrst to optimize multi-variate non-linear performance measures such as Fβ for a latent variable structure prediction task. 
Most of the work in sentiment analysis and opinion mining focuses on extracting explicit sentiments. Opinions may be expressed implicitly via inference rules over explicit sentiments. In this thesis, we incorporate the inference rules as constraints in joint prediction models, to develop an entity/event-level sentiment analysis system which aims at detecting both explicit and implicit sentiments expressed among entities and events in the text, especially focusing on but not limited to sentiments toward events that positively or negatively affect entities (+/-effect events). 
This paper describes work in progress to use clustering to create a lexicon of words that engage in the lexico-semantic relationship known as grading. While other resources like thesauri and taxonomies exist detailing relationships such as synonymy, antonymy, and hyponymy, we do not know of any thorough resource for grading. This work focuses on identifying the words that may participate in this relationship, paving the way for the creation of a true grading lexicon later. 
The well related tasks of evaluating the Semantic Textual Similarity and Semantic Relatedness have been under a special attention in NLP community. Many different approaches have been proposed, implemented and evaluated at different levels, such as lexical similarity, word/string/POS tags overlapping, semantic modeling (LSA, LDA), etc. However, at the level of syntactic structure, it is not clear how signiﬁcant it contributes to the overall accuracy. In this paper, we make a preliminary evaluation of the impact of the syntactic structure in the tasks by running and analyzing the results from several experiments regarding to how syntactic structure contributes to solving these tasks. 
Traditional approaches to Sentiment Analysis (SA) rely on large annotated data sets or wide-coverage sentiment lexica, and as such often perform poorly on under-resourced languages. This paper presents empirical evidence of an efﬁcient SA approach using freely available machine translation (MT) systems to translate Arabic tweets to English, which we then label for sentiment using a state-of-theart English SA system. We show that this approach signiﬁcantly outperforms a number of standard approaches on a gold-standard heldout data set, and performs equally well compared to more cost-intense methods with 76% accuracy. This conﬁrms MT-based SA as a cheap and effective alternative to building a fully ﬂedged SA system when dealing with under-resourced languages. Keywords: Sentiment Analysis, Arabic, Twitter, Machine Translation 
In this thesis proposal we present a novel semantic embedding method, which aims at consistently performing semantic clustering at sentence level. Taking into account special aspects of Vector Space Models (VSMs), we propose to learn reproducing kernels in classiﬁcation tasks. By this way, capturing spectral features from data is possible. These features make it theoretically plausible to model semantic similarity criteria in Hilbert spaces, i.e. the embedding spaces. We could improve the semantic assessment over embeddings, which are criterion-derived representations from traditional semantic vectors. The learned kernel could be easily transferred to clustering methods, where the Multi-Class Imbalance Problem is considered (e.g. semantic clustering of deﬁnitions of terms). 
This thesis proposal sheds light on the role of interactive machine learning and implicit user feedback for manual annotation tasks and semantic writing aid applications. First we focus on the cost-effective annotation of training data using an interactive machine learning approach by conducting an experiment for sequence tagging of German named entity recognition. To show the effectiveness of the approach, we further carry out a sequence tagging task on Amharic part-of-speech and are able to signiﬁcantly reduce time used for annotation. The second research direction is to systematically integrate different NLP resources for our new semantic writing aid tool using again an interactive machine learning approach to provide contextual paraphrase suggestions. We develop a baseline system where three lexical resources are combined to provide paraphrasing in context and show that combining resources is a promising direction. 
Community question answering (CQA) websites contain millions of question and answer (QnA) pairs that represent real users’ interests. Traditional methods for relation extraction from natural language text operate over individual sentences. However answer text is sometimes hard to understand without knowing the question, e.g., it may not name the subject or relation of the question. This work presents a novel model for relation extraction from CQA data, which uses discourse of QnA pairs to predict relations between entities mentioned in question and answer sentences. Experiments on 2 publicly available datasets demonstrate that the model can extract from ∼20% to ∼40% additional relation triples, not extracted by existing sentence-based models. 
Parallel corpora are constructed by taking a document authored in one language and translating it into another language. However, the information about the authored and translated sides of the corpus is usually not preserved. When available, this information can be used to improve statistical machine translation. Existing statistical methods for translation direction detection have low accuracy when applied to the realistic out-of-domain setting, especially when the input texts are short. Our contributions in this work are threefold: 1) We develop a multi-corpus parallel dataset with translation direction labels at the sentence level, 2) we perform a comparative evaluation of previously introduced features for translation direction detection in a cross-domain setting and 3) we generalize a previously introduced type of features to outperform the best previously proposed features in detecting translation direction and achieve 0.80 precision with 0.85 recall. 
Discourse markers (DMs) are ubiquitous cohesive devices used to connect what is said or written. However, across languages there is divergence in their usage, placement, and frequency, which is considered to be a major problem for machine translation (MT). This paper presents an overview of a proposed thesis, exploring the difﬁculties around DMs in MT, with a focus on Chinese and English. The thesis will examine two main areas: modelling cohesive devices within sentences and modelling discourse relations (DRs) across sentences. Initial experiments have shown promising results for building a prediction model that uses linguistically inspired features to help improve word alignments with respect to the implicit use of cohesive devices, which in turn leads to improved hierarchical phrasebased MT. 
Evaluating the quality of language output tasks such as Machine Translation (MT) and Automatic Summarisation (AS) is a challenging topic in Natural Language Processing (NLP). Recently, techniques focusing only on the use of outputs of the systems and source information have been investigated. In MT, this is referred to as Quality Estimation (QE), an approach that uses machine learning techniques to predict the quality of unseen data, generalising from a few labelled data points. Traditional QE research addresses sentencelevel QE evaluation and prediction, disregarding document-level information. Documentlevel QE requires a different set up from sentence-level, which makes the study of appropriate quality scores, features and models necessary. Our aim is to explore documentlevel QE of MT, focusing on discourse information. However, the ﬁndings of this research can improve other NLP tasks, such as AS. 
Document classiﬁcation and topic models are useful tools for managing and understanding large corpora. Topic models are used to uncover underlying semantic and structure of document collections. Categorizing large collection of documents requires hand-labeled training data, which is time consuming and needs human expertise. We believe engaging user in the process of document labeling helps reduce annotation time and address user needs. We present an interactive tool for document labeling. We use topic models to help users in this procedure. Our preliminary results show that users can more eﬀectively and eﬃciently apply labels to documents using topic model information.  We present a user interactive tool for document labeling that uses topic models to help users assign appropriate labels to documents (Section 2). In Section 3, we describe our user interface and experiments on Congressional Bills data set. We also explain an evaluation metric to assess the quality of assigned document labels. In preliminary results, we show that annotators can more quickly label a document collection given a topic modeling overview. While engaging user in the process of content-analysis has been studied before(as we discuss in Section 4), in Section 4 we describe how our new framework allows for more ﬂexibility and interactivity. Finally, in Section 5, we discuss the limitation of our framework and how we plan to extend it in future. 2 Interactive Document Labeling  
Machine learning has been popularly used in numerous natural language processing tasks. However, most machine learning models are built using a single dataset. This is often referred to as one-shot learning. Although this one-shot learning paradigm is very useful, it will never make an NLP system understand the natural language because it does not accumulate knowledge learned in the past and make use of the knowledge in future learning and problem solving. In this thesis proposal, I ﬁrst present a survey of lifelong machine learning (LML). I then narrow down to one speciﬁc NLP task, i.e., topic modeling. I propose several approaches to apply lifelong learning idea in topic modeling. Such capability is essential to make an NLP system versatile and holistic. 
This paper suggests an architectural approach of representing knowledge graph for complex question-answering. There are four kinds of entity relations added to our knowledge graph: syntactic dependencies, semantic role labels, named entities, and coreference links, which can be effectively applied to answer complex questions. As a proof of concept, we demonstrate how our knowledge graph can be used to solve complex questions such as arithmetics. Our experiment shows a promising result on solving arithmetic questions, achieving the 3folds cross-validation score of 71.75%. 
This paper presents a machine learning system that uses dependency-based features and lexical features for recognizing textual entailment. The proposed system evaluates the feature values automatically. The performance of the proposed system is evaluated by conducting experiments on RTE1, RTE2 and RTE3 datasets. Further, a comparative study of the current system with other ML-based systems for RTE to check the performance of the proposed system is also presented. The dependency-based heuristics and lexical features from the current system have resulted in significant improvement in accuracy over existing state-of-art ML-based solutions for RTE. 
The aim of this thesis proposal is to perform bilingual lexicon extraction for cases in which small parallel corpora are available and it is not easy to obtain monolingual corpus for at least one of the languages. Moreover, the languages are typologically distant and there is no bilingual seed lexicon available. We focus on the language pair Spanish-Nahuatl, we propose to work with morpheme based representations in order to reduce the sparseness and to facilitate the task of ﬁnding lexical correspondences between a highly agglutinative language and a fusional one. We take into account contextual information but instead of using a precompiled seed dictionary, we use the distribution and dispersion of the positions of the morphological units as cues to compare the contextual vectors and obtaining the translation candidates. 
This thesis explores the computational structure of morphological paradigms from the perspective of unsupervised learning. Three topics are studied: (i) stem identiﬁcation, (ii) paradigmatic similarity, and (iii) paradigm induction. All the three topics progress in terms of the scope of data in question. The ﬁrst and second topics explore structure when morphological paradigms are given, ﬁrst within a paradigm and then across paradigms. The third topic asks where morphological paradigms come from in the ﬁrst place, and explores strategies of paradigm induction from child-directed speech. This research is of interest to linguists and natural language processing researchers, for both theoretical questions and applied areas. 
English, like many languages, uses a wide variety of ways to talk about the future, which makes the automatic identiﬁcation of future reference a challenge. In this research we extend Latent Dirichlet allocation (LDA) for use in the identiﬁcation of future-referring sentences. Building off a set of hand-designed rules, we trained a ADAGRAD classiﬁer to be able to automatically detect sentences referring to the future. Uni-bi-trigram and syntactic rule mixed feature was found to provide the highest accuracy. Latent Dirichlet Allocation (LDA) indicated the existence of four major categories of future orientation. Lastly, the results of these analyses were found to correlate with a range of behavioral measures, offering evidence in support of the psychological reality of the categories. 
We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument ﬁllers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles deﬁned in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages. 
We present a joint model for predicate argument alignment. We leverage multiple sources of semantic information, including temporal ordering constraints between events. These are combined in a max-margin framework to ﬁnd a globally consistent view of entities and events across multiple documents, which leads to improvements over a very strong local baseline. 
Most recent unsupervised methods in vector space semantics for assessing thematic ﬁt (e.g. Erk, 2007; Baroni and Lenci, 2010; Sayeed and Demberg, 2014) create prototypical roleﬁllers without performing word sense disambiguation. This leads to a kind of sparsity problem: candidate role-ﬁllers for different senses of the verb end up being measured by the same “yardstick”, the single prototypical role-ﬁller. In this work, we use three different feature spaces to construct robust unsupervised models of distributional semantics. We show that correlation with human judgements on thematic ﬁt estimates can be improved consistently by clustering typical role-ﬁllers and then calculating similarities of candidate roleﬁllers with these cluster centroids. The suggested methods can be used in any vector space model that constructs a prototype vector from a non-trivial set of typical vectors. 
Vector Space Models (VSMs) of Semantics are useful tools for exploring the semantics of single words, and the composition of words to make phrasal meaning. While many methods can estimate the meaning (i.e. vector) of a phrase, few do so in an interpretable way. We introduce a new method (CNNSE) that allows word and phrase vectors to adapt to the notion of composition. Our method learns a VSM that is both tailored to support a chosen semantic composition operation, and whose resulting features have an intuitive interpretation. Interpretability allows for the exploration of phrasal semantics, which we leverage to analyze performance on a behavioral task. 
Incremental parsers have potential advantages for applications like language modeling for machine translation and speech recognition. We describe a new algorithm for incremental transition-based Combinatory Categorial Grammar parsing. As English CCGbank derivations are mostly right branching and non-incremental, we design our algorithm based on the dependencies resolved rather than the derivation. We introduce two new actions in the shift-reduce paradigm based on the idea of ‘revealing’ (Pareschi and Steedman, 1987) the required information during parsing. On the standard CCGbank test data, our algorithm achieved improvements of 0.88% in labeled and 2.0% in unlabeled F-score over a greedy non-incremental shift-reduce parser. 
Parsing full-ﬂedged predicate-argument structures in a deep syntax framework requires graphs to be predicted. Using the DeepBank (Flickinger et al., 2012) and the PredicateArgument Structure treebank (Miyao and Tsujii, 2005) as a test ﬁeld, we show how transition-based parsers, extended to handle connected graphs, beneﬁt from the use of topologically different syntactic features such as dependencies, tree fragments, spines or syntactic paths, bringing a much needed context to the parsing models, improving notably over long distance dependencies and elided coordinate structures. By conﬁrming this positive impact on an accurate 2nd-order graphbased parser (Martins and Almeida, 2014), we establish a new state-of-the-art on these data sets. 
Text documents of varying nature (e.g., summary documents written by analysts or published, scientiﬁc papers) often cite others as a means of providing evidence to support a claim, attributing credit, or referring the reader to related work. We address the problem of predicting a document’s cited sources by introducing a novel, discriminative approach which combines a content-based generative model (LDA) with author-based features. Further, our classiﬁer is able to learn the importance and quality of each topic within our corpus – which can be useful beyond this task – and preliminary results suggest its metric is competitive with other standard metrics (Topic Coherence). Our ﬂagship system, Logit-Expanded, provides state-of-the-art performance on the largest corpus ever used for this task. 
In this paper, we apply a weakly-supervised learning approach for slot tagging using conditional random ﬁelds by exploiting web search click logs. We extend the constrained lattice training of Ta¨ckstro¨m et al. (2013) to non-linear conditional random ﬁelds in which latent variables mediate between observations and labels. When combined with a novel initialization scheme that leverages unlabeled data, we show that our method gives signiﬁcant improvement over strong supervised and weakly-supervised baselines. 
Character n-grams have been identiﬁed as the most successful feature in both singledomain and cross-domain Authorship Attribution (AA), but the reasons for their discriminative value were not fully understood. We identify subgroups of character n-grams that correspond to linguistic aspects commonly claimed to be covered by these features: morphosyntax, thematic content and style. We evaluate the predictiveness of each of these groups in two AA settings: a single domain setting and a cross-domain setting where multiple topics are present. We demonstrate that character ngrams that capture information about afﬁxes and punctuation account for almost all of the power of character n-grams as features. Our study contributes new insights into the use of n-grams for future AA work and other classiﬁcation tasks. 
Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classiﬁcation. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-ofword conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods. 
Syntactic linearization algorithms take a bag of input words and a set of optional constraints, and construct an output sentence and its syntactic derivation simultaneously. The search problem is NP-hard, and the current best results are achieved by bottom-up bestﬁrst search. One drawback of the method is low efﬁciency; and there is no theoretical guarantee that a full sentence can be found within bounded time. We propose an alternative algorithm that constructs output structures from left to right using beam-search. The algorithm is based on incremental parsing algorithms. We extend the transition system so that word ordering is performed in addition to syntactic parsing, resulting in a linearization system that runs in guaranteed quadratic time. In standard evaluations, our system runs an order of magnitude faster than a state-of-the-art baseline using best-ﬁrst search, with improved accuracies. 
We present a statistical framework to extract information-rich citation sentences that summarise the main contributions of a scientiﬁc paper. In a ﬁrst stage, we automatically discover salient keywords from a paper’s citation summary, keywords that characterise its main contributions. In a second stage, exploiting the results of the ﬁrst stage, we identify citation sentences that best capture the paper’s main contributions. Experimental results show that our approach using methods rooted in quantitative statistics and information theory outperforms the current state-of-the-art systems in scientiﬁc paper summarisation. 
Automatic headline generation is a sub-task of document summarization with many reported applications. In this study we present a sequence-prediction technique for learning how editors title their news stories. The introduced technique models the problem as a discrete optimization task in a feature-rich space. In this space the global optimum can be found in polynomial time by means of dynamic programming. We train and test our model on an extensive corpus of ﬁnancial news, and compare it against a number of baselines by using standard metrics from the document summarization domain, as well as some new ones proposed in this work. We also assess the readability and informativeness of the generated titles through human evaluation. The obtained results are very appealing and substantiate the soundness of the approach. 
We present a novel method for aligning a sequence of instructions to a video of someone carrying out a task. In particular, we focus on the cooking domain, where the instructions correspond to the recipe. Our technique relies on an HMM to align the recipe steps to the (automatically generated) speech transcript. We then reﬁne this alignment using a state-of-the-art visual food detector, based on a deep convolutional neural network. We show that our technique outperforms simpler techniques based on keyword spotting. It also enables interesting applications, such as automatically illustrating recipes with keyframes, and searching within a video for events of interest. 
We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning. 
We address the problem of automatically aligning natural language sentences with corresponding video segments without any direct supervision. Most existing algorithms for integrating language with videos rely on handaligned parallel data, where each natural language sentence is manually aligned with its corresponding image or video segment. Recently, fully unsupervised alignment of text with video has been shown to be feasible using hierarchical generative models. In contrast to the previous generative models, we propose three latent-variable discriminative models for the unsupervised alignment task. The proposed discriminative models are capable of incorporating domain knowledge, by adding diverse and overlapping features. The results show that discriminative models outperform the generative models in terms of alignment accuracy.  Verb  Nouns  Label the bottle  Add 500 mL of DI water to the labeled bottle  Add 500 mL of DI water to the labeled bottle Transfer 1 mL of MgSO4 to the 50 mL Falcon tube  Video Objects Touched by Hands  Alignment of Video Segments with Text Sentences  Figure 1: The proposed discriminative learning algorithm aligns protocol sentences to corresponding video frames. We incorporate features that can learn the co-occurrences of nouns and verbs in the sentences with the objects in the video.  
Content analysis, a widely-applied social science research method, is increasingly being supplemented by topic modeling. However, while the discourse on content analysis centers heavily on reproducibility, computer scientists often focus more on scalability and less on coding reliability, leading to growing skepticism on the usefulness of topic models for automated content analysis. In response, we introduce TopicCheck, an interactive tool for assessing topic model stability. Our contributions are threefold. First, from established guidelines on reproducible content analysis, we distill a set of design requirements on how to computationally assess the stability of an automated coding process. Second, we devise an interactive alignment algorithm for matching latent topics from multiple models, and enable sensitivity evaluation across a large number of models. Finally, we demonstrate that our tool enables social scientists to gain novel insights into three active research questions. 
Inferring latent attributes of online users has many applications in public health, politics, and marketing. Most existing approaches rely on supervised learning algorithms, which require manual data annotation and therefore are costly to develop and adapt over time. In this paper, we propose a lightly supervised approach based on label regularization to infer the age, ethnicity, and political orientation of Twitter users. Our approach learns from a heterogeneous collection of soft constraints derived from Census demographics, trends in baby names, and Twitter accounts that are emblematic of class labels. To counteract the imprecision of such constraints, we compare several constraint selection algorithms that optimize classiﬁcation accuracy on a tuning set. We ﬁnd that using no user-annotated data, our approach is within 2% of a fully supervised baseline for three of four tasks. Using a small set of labeled data for tuning further improves accuracy on all tasks. 
We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines. 
A portmanteau is a type of compound word that fuses the sounds and meanings of two component words; for example, “frenemy” (friend + enemy) or “smog” (smoke + fog). We develop a system, including a novel multitape FST, that takes an input of two words and outputs possible portmanteaux. Our system is trained on a list of known portmanteaux and their component words, and achieves 45% exact matches in cross-validated experiments.  W1 afﬂuence anecdote chill ﬂavor guess jogging sheep spanish zeitgeist  W2 inﬂuenza data relax favorite estimate juggling people english ghost  PM afﬂuenza anecdata chillax ﬂavorite guesstimate joggling sheeple spanglish zeitghost  Table 1: Valid component words and portmanteaux.  
This work improves monolingual sentence alignment for text simpliﬁcation, speciﬁcally for text in standard and simple Wikipedia. We introduce a method that improves over past efforts by using a greedy (vs. ordered) search over the document and a word-level semantic similarity score based on Wiktionary (vs. WordNet) that also accounts for structural similarity through syntactic dependencies. Experiments show improved performance on a hand-aligned set, with the largest gain coming from structural similarity. Resulting datasets of manually and automatically aligned sentence pairs are made available. 
We present an intuitive and effective method for inducing style scores on words and phrases. We exploit signal in a phrase’s rate of occurrence across stylistically contrasting corpora, making our method simple to implement and efﬁcient to scale. We show strong results both intrinsically, by correlation with human judgements, and extrinsically, in applications to genre analysis and paraphrasing. 
Research on entity linking has considered a broad range of text, including newswire, blogs and web documents in multiple languages. However, the problem of entity linking for spoken language remains unexplored. Spoken language obtained from automatic speech recognition systems poses different types of challenges for entity linking; transcription errors can distort the context, and named entities tend to have high error rates. We propose features to mitigate these errors and evaluate the impact of ASR errors on entity linking using a new corpus of entity linked broadcast news transcripts. 
Monolingual alignment models have been shown to boost the performance of question answering systems by ”bridging the lexical chasm” between questions and answers. The main limitation of these approaches is that they require semistructured training data in the form of question-answer pairs, which is difﬁcult to obtain in specialized domains or lowresource languages. We propose two inexpensive methods for training alignment models solely using free text, by generating artiﬁcial question-answer pairs from discourse structures. Our approach is driven by two representations of discourse: a shallow sequential representation, and a deep one based on Rhetorical Structure Theory. We evaluate the proposed model on two corpora from different genres and domains: one from Yahoo! Answers and one from the biology domain, and two types of non-factoid questions: manner and reason. We show that these alignment models trained directly from discourse structures imposed on free text improve performance considerably over an information retrieval baseline and a neural network language model trained on the same data. 
The task of Named Entity Disambiguation is to map entity mentions in the document to their correct entries in some knowledge base. We present a novel graph-based disambiguation approach based on Personalized PageRank (PPR) that combines local and global evidence for disambiguation and effectively ﬁlters out noise introduced by incorrect candidates. Experiments show that our method outperforms state-of-the-art approaches by achieving 91.7% in micro- and 89.9% in macroaccuracy on a dataset of 27.8K named entity mentions. 
Several techniques have recently been proposed for training “self-normalized” discriminative models. These attempt to ﬁnd parameter settings for which unnormalized model scores approximate the true label probability. However, the theoretical properties of such techniques (and of self-normalization generally) have not been investigated. This paper examines the conditions under which we can expect self-normalization to work. We characterize a general class of distributions that admit self-normalization, and prove generalization bounds for procedures that minimize empirical normalizer variance. Motivated by these results, we describe a novel variant of an established procedure for training self-normalized models. The new procedure avoids computing normalizers for most training examples, and decreases training time by as much as factor of ten while preserving model quality. 
Word embeddings have been found useful for many NLP tasks, including part-of-speech tagging, named entity recognition, and parsing. Adding multilingual context when learning embeddings can improve their quality, for example via canonical correlation analysis (CCA) on embeddings from two languages. In this paper, we extend this idea to learn deep non-linear transformations of word embeddings of the two languages, using the recently proposed deep canonical correlation analysis. The resulting embeddings, when evaluated on multiple word and bigram similarity tasks, consistently improve over monolingual embeddings and over embeddings transformed with linear CCA. 
We present a discriminative model for detecting disﬂuencies in spoken language transcripts. Structurally, our model is a semiMarkov conditional random ﬁeld with features targeting characteristics unique to speech repairs. This gives a signiﬁcant performance improvement over standard chain-structured CRFs that have been employed in past work. We then incorporate prosodic features over silences and relative word duration into our semi-CRF model, resulting in further performance gains; moreover, these features are not easily replaced by discrete prosodic indicators such as ToBI breaks. Our ﬁnal system, the semi-CRF with prosodic information, achieves an F-score of 85.4, which is 1.3 F1 better than the best prior reported F-score on this dataset. 
This paper presents a novel technique for empty category (EC) detection using distributed word representations. A joint model is learned from the labeled data to map both the distributed representations of the contexts of ECs and EC types to a low dimensional space. In the testing phase, the context of possible EC positions will be projected into the same space for empty category detection. Experiments on Chinese Treebank prove the effectiveness of the proposed method. We improve the precision by about 6 points on a subset of Chinese Treebank, which is a new state-ofthe-art performance on CTB. 
A large part of human communication involves referring to entities in the world and often these entities are objects that are visually present for the interlocutors. A system that aims to resolve such references needs to tackle a complex task: objects and their visual features need to be determined, the referring expressions must be recognised, and extra-linguistic information such as eye gaze or pointing gestures need to be incorporated. Systems that can make use of such information sources exist, but have so far only been tested under very constrained settings, such as WOz interactions. In this paper, we apply to a more complex domain a reference resolution model that works incrementally (i.e., word by word), grounds words with visually present properties of objects (such as shape and size), and can incorporate extra-linguistic information. We ﬁnd that the model works well compared to previous work on the same data, despite using fewer features. We conclude that the model shows potential for use in a realtime interactive dialogue system. 
Marketing materials such as ﬂyers and other infographics are a vast online resource. In a number of industries, such as the commercial real estate industry, they are in fact the only authoritative source of information. Companies attempting to organize commercial real estate inventories spend a signiﬁcant amount of resources on manual data entry of this information. In this work, we propose a method for extracting structured data from free-form commercial real estate ﬂyers in PDF and HTML formats. We modeled the problem as text categorization and Named Entity Recognition (NER) tasks and applied a supervised machine learning approach (Support Vector Machines). Our dataset consists of more than 2,200 commercial real estate ﬂyers and associated manually entered structured data, which was used to automatically create training datasets. Traditionally, text categorization and NER approaches are based on textual information only. However, information in visually rich formats such as PDF and HTML is often conveyed by a combination of textual and visual features. Large fonts, visually salient colors, and positioning often indicate the most relevant pieces of information. We applied novel features based on visual characteristics in addition to traditional text features and show that performance improved signiﬁcantly for both the text categorization and NER tasks. 
We propose a method for simultaneously translating from a single source language to multiple target languages T1, T2, etc. The motivation behind this method is that if we only have a weak language model for T1 and translations in T1 and T2 are associated, we can use the information from a strong language model over T2 to disambiguate the translations in T1, providing better translation results. As a speciﬁc framework to realize multi-target translation, we expand the formalism of synchronous context-free grammars to handle multiple targets, and describe methods for rule extraction, scoring, pruning, and search with these models. Experiments ﬁnd that multi-target translation with a strong language model in a similar second target language can provide gains of up to 0.8-1.5 BLEU points.1 
This paper describes a joint model of word segmentation and phonological alternations, which takes unsegmented utterances as input and infers word segmentations and underlying phonological representations. The model is a Maximum Entropy or log-linear model, which can express a probabilistic version of Optimality Theory (OT; Prince and Smolensky (2004)), a standard phonological framework. The features in our model are inspired by OT’s Markedness and Faithfulness constraints. Following the OT principle that such features indicate “violations”, we require their weights to be non-positive. We apply our model to a modiﬁed version of the Buckeye corpus (Pitt et al., 2007) in which the only phonological alternations are deletions of word-ﬁnal /d/ and /t/ segments. The model sets a new state-ofthe-art for this corpus for word segmentation, identiﬁcation of underlying forms, and identiﬁcation of /d/ and /t/ deletions. We also show that the OT-inspired sign constraints on feature weights are crucial for accurate identiﬁcation of deleted /d/s; without them our model posits approximately 10 times more deleted underlying /d/s than appear in the manually annotated data. 
For phylogenetic inference, linguistic typology is a promising alternative to lexical evidence because it allows us to compare an arbitrary pair of languages. A challenging problem with typology-based phylogenetic inference is that the changes of typological features over time are less intuitive than those of lexical features. In this paper, we work on reconstructing typologically natural ancestors To do this, we leverage dependencies among typological features. We ﬁrst represent each language by continuous latent components that capture feature dependencies. We then combine them with a typology evaluator that distinguishes typologically natural languages from other possible combinations of features. We perform phylogenetic inference in the continuous space and use the evaluator to ensure the typological naturalness of inferred ancestors. We show that the proposed method reconstructs known language families more accurately than baseline methods. Lastly, assuming the monogenesis hypothesis, we attempt to reconstruct a common ancestor of the world’s languages. 
A weakly-supervised method is applied to anonymized queries to extract lexical interpretations of compound noun phrases (e.g., “fortune 500 companies”). The interpretations explain the subsuming role (“listed in”) that modiﬁers (fortune 500) play relative to heads (companies) within the noun phrases. Experimental results over evaluation sets of noun phrases from multiple sources demonstrate that interpretations extracted from queries have encouraging coverage and precision. The top interpretation extracted is deemed relevant for more than 70% of the noun phrases. 
We present an approach to speech recognition that uses only a neural network to map acoustic input to characters, a character-level language model, and a beam search decoding procedure. This approach eliminates much of the complex infrastructure of modern speech recognition systems, making it possible to directly train a speech recognizer using errors generated by spoken language understanding tasks. The system naturally handles out of vocabulary words and spoken word fragments. We demonstrate our approach using the challenging Switchboard telephone conversation transcription task, achieving a word error rate competitive with existing baseline systems. To our knowledge, this is the ﬁrst entirely neural-network-based system to achieve strong speech transcription results on a conversational speech task. We analyze qualitative differences between transcriptions produced by our lexicon-free approach and transcriptions produced by a standard speech recognition system. Finally, we evaluate the impact of large context neural network character language models as compared to standard n-gram models within our framework. 
 The advent of social media has brought Internet memes, a unique social phenomenon, to the front stage of the Web. Embodied in the form of images with text descriptions, little do we know about the “language of memes”. In this paper, we statistically study the correlations among popular memes and their wordings, and generate meme descriptions from raw images. To do this, we take a multimodal approach—we propose a robust nonparanormal model to learn the stochastic dependencies among the image, the candidate descriptions, and the popular votes. In experiments, we show that combining text and vision helps identifying popular meme descriptions; that our nonparanormal model is able to learn dense and continuous vision features jointly with sparse and discrete text features in a principled manner, outperforming various competitive baselines; that our system can generate meme descriptions using a simple pipeline. 
We present a two-stage framework to parse a sentence into its Abstract Meaning Representation (AMR). We ﬁrst use a dependency parser to generate a dependency tree for the sentence. In the second stage, we design a novel transition-based algorithm that transforms the dependency tree to an AMR graph. There are several advantages with this approach. First, the dependency parser can be trained on a training set much larger than the training set for the tree-to-graph algorithm, resulting in a more accurate AMR parser overall. Our parser yields an improvement of 5% absolute in F-measure over the best previous result. Second, the actions that we design are linguistically intuitive and capture the regularities in the mapping between the dependency structure and the AMR of a sentence. Third, our parser runs in nearly linear time in practice in spite of a worst-case complexity of O(n2). 
Most modern statistical machine translation systems are based on linear statistical models. One extremely effective method for estimating the model parameters is minimum error rate training (MERT), which is an efﬁcient form of line optimisation adapted to the highly nonlinear objective functions used in machine translation. We describe a polynomial-time generalisation of line optimisation that computes the error surface over a plane embedded in parameter space. The description of this algorithm relies on convex geometry, which is the mathematics of polytopes and their faces. Using this geometric representation of MERT we investigate whether the optimisation of linear models is tractable in general. Previous work on ﬁnding optimal solutions in MERT (Galley and Quirk, 2011) established a worstcase complexity that was exponential in the number of sentences, in contrast we show that exponential dependence in the worst-case complexity is mainly in the number of features. Although our work is framed with respect to MERT, the convex geometric description is also applicable to other error-based training methods for linear models. We believe our analysis has important ramiﬁcations because it suggests that the current trend in building statistical machine translation systems by introducing a very large number of sparse features is inherently not robust. 
Abstract structures from which the generation naturally starts often do not contain any functional nodes, while surface-syntactic structures or a chain of tokens in a linearized tree contain all of them. Therefore, data-driven linguistic generation needs to be able to cope with the projection between non-isomorphic structures that differ in their topology and number of nodes. So far, such a projection has been a challenge in data-driven generation and was largely avoided. We present a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classiﬁer based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 
This work focuses on the insensitivity of existing word alignment models to domain differences, which often yields suboptimal results on large heterogeneous data. A novel latent domain word alignment model is proposed, which induces domain-conditioned lexical and alignment statistics. We propose to train the model on a heterogeneous corpus under partial supervision, using a small number of seed samples from different domains. The seed samples allow estimating sharper, domain-conditioned word alignment statistics for sentence pairs. Our experiments show that the derived domain-conditioned statistics, once combined together, produce notable improvements both in word alignment accuracy and in translation accuracy of their resulting SMT systems. 
People vary widely in their temporal orientation—how often they emphasize the past, present, and future—and this affects their ﬁnances, health, and happiness. Traditionally, temporal orientation has been assessed by self-report questionnaires. In this paper, we develop a novel behavior-based assessment using human language on Facebook. We ﬁrst create a past, present, and future message classiﬁer, engineering features and evaluating a variety of classiﬁcation techniques. Our message classiﬁer achieves an accuracy of 71.8%, compared with 52.8% from the most frequent class and 58.6% from a model based entirely on time expression features. We quantify a users’ overall temporal orientation based on their distribution of messages and validate it against known human correlates: conscientiousness, age, and gender. We then explore social scientiﬁc questions, ﬁnding novel associations with the factors openness to experience, satisfaction with life, depression, IQ, and one’s number of friends. Further, demonstrating how one can track orientation over time, we ﬁnd differences in future orientation around birthdays. 
Recent years have seen increased interest in text normalization in social media, as the informal writing styles found in Twitter and other social media data often cause problems for NLP applications. Unfortunately, most current approaches narrowly regard the normalization task as a “one size ﬁts all” task of replacing non-standard words with their standard counterparts. In this work we build a taxonomy of normalization edits and present a study of normalization to examine its effect on three different downstream applications (dependency parsing, named entity recognition, and text-to-speech synthesis). The results suggest that how the normalization task should be viewed is highly dependent on the targeted application. The results also show that normalization must be thought of as more than word replacement in order to produce results comparable to those seen on clean text. 
More and more of the information available on the web is dialogic, and a signiﬁcant portion of it takes place in online forum conversations about current social and political topics. We aim to develop tools to summarize what these conversations are about. What are the CENTRAL PROPOSITIONS associated with different stances on an issue; what are the abstract objects under discussion that are central to a speaker’s argument? How can we recognize that two CENTRAL PROPOSITIONS realize the same FACET of the argument? We hypothesize that the CENTRAL PROPOSITIONS are exactly those arguments that people ﬁnd most salient, and use human summarization as a probe for discovering them. We describe our corpus of human summaries of opinionated dialogs, then show how we can identify similar repeated arguments, and group them into FACETS across many discussions of a topic. We deﬁne a new task, ARGUMENT FACET SIMILARITY (AFS), and show that we can predict AFS with a .54 correlation score, versus an ngram system baseline of .39 and a semantic textual similarity system baseline of .45. 
We present a simple and yet effective approach that can incorporate rationales elicited from annotators into the training of any offthe-shelf classiﬁer. We show that our simple approach is effective for multinomial na¨ıve Bayes, logistic regression, and support vector machines. We additionally present an active learning method tailored speciﬁcally for the learning with rationales framework. 
This paper presents a framework to infer spatial knowledge from verbal semantic role representations. First, we generate potential spatial knowledge deterministically. Second, we determine whether it can be inferred and a degree of certainty. Inferences capture that something is located or is not located somewhere, and temporally anchor this information. An annotation effort shows that inferences are ubiquitous and intuitive to humans.  S  NP  VP  NNP John  AUX was THEME  VBN incarcerated  VP  LOCATION  PP  at Shawshank prison  Figure 1: Semantic roles (solid arrows) and additional spatial knowledge (discontinuous arrow).  
Tree trimming is the problem of extracting an optimal subtree from an input tree, and sentence extraction and sentence compression methods can be formulated and solved as tree trimming problems. Previous approaches require integer linear programming (ILP) solvers to obtain exact solutions. The problem of this approach is that ILP solvers are black-boxes and have no theoretical guarantee as to their computation complexity. We propose a dynamic programming (DP) algorithm for tree trimming problems whose running time is O(N L log N ), where N is the number of tree nodes and L is the length limit. Our algorithm exploits the zero-suppressed binary decision diagram (ZDD), a data structure that represents a family of sets as a directed acyclic graph, to represent the set of subtrees in a compact form; the structure of ZDD permits the application of DP to obtain exact solutions, and our algorithm is applicable to different tree trimming problems. Moreover, experiments show that our algorithm is faster than state-of-the-art ILP solvers, and that it scales well to handle large summarization problems. 
Context representations are a key element in distributional models of word meaning. In contrast to typical representations based on neighboring words, a recently proposed approach suggests to represent a context of a target word by a substitute vector, comprising the potential ﬁllers for the target word slot in that context. In this work we ﬁrst propose a variant of substitute vectors, which we ﬁnd particularly suitable for measuring context similarity. Then, we propose a novel model for representing word meaning in context based on this context representation. Our model outperforms state-of-the-art results on lexical substitution tasks in an unsupervised setting. 
Gradable terms such as brief, lengthy and extended illustrate varying degrees of a scale and can therefore participate in comparative constructs. Knowing the set of words that can be compared on the same scale and the associated ordering between them (brief < lengthy < extended) is very useful for a variety of lexical semantic tasks. Current techniques to derive such an ordering rely on WordNet to determine which words belong on the same scale and are limited to adjectives. Here we describe an extension to recent work: we investigate a fully automated pipeline to extract gradable terms from a corpus, group them into clusters reﬂecting the same scale and establish an ordering among them. This methodology reduces the amount of required handcrafted knowledge, and can infer gradability of words independent of their part of speech. Our approach infers an ordering for adjectives with comparable performance to previous work, but also for adverbs with an accuracy of 71%. We ﬁnd that the technique is useful for inferring such rankings among words across different domains, and present an example using biomedical text. 
We take a novel approach to zero pronoun resolution in Chinese: our model explicitly tracks the ﬂow of focus in a discourse. Our approach, which generalizes to deictic references, is not reliant on the presence of overt noun phrase antecedents to resolve to, and allows us to address the large percentage of “non-anaphoric” pronouns ﬁltered out in other approaches. We furthermore train our model using readily available parallel Chinese/English corpora, allowing for training without hand-annotated data. Our results demonstrate improvements on two test sets, as well as the usefulness of linguistically motivated features.  
We present a new approach to harvesting a large-scale, high quality image-caption corpus that makes a better use of already existing web data with no additional human efforts. The key idea is to focus on De´ja` Image-Captions: naturally existing image descriptions that are repeated almost verbatim – by more than one individual for different images. The resulting corpus provides association structure between 4 million images with 180K unique captions, capturing a rich spectrum of everyday narratives including ﬁgurative and pragmatic language. Exploring the use of the new corpus, we also present new conceptual tasks of visually situated paraphrasing, creative image captioning, and creative visual paraphrasing. 
We present a comparative investigation of word representations for part-of-speech (POS) and morphological tagging, focusing on scenarios with considerable differences between training and test data where a robust approach is necessary. Instead of adapting the model towards a speciﬁc domain we aim to build a robust model across domains. We developed a test suite for robust tagging consisting of six languages and different domains. We ﬁnd that representations similar to Brown clusters perform best for POS tagging and that word representations based on linguistic morphological analyzers perform best for morphological tagging. 
In spite of the apparent irregularity of the English spelling system, Chomsky and Halle (1968) characterize it as “near optimal”. We investigate this assertion using computational techniques and resources. We design an algorithm to generate word spellings that maximize both phonemic transparency and morphological consistency. Experimental results demonstrate that the constructed system is much closer to optimality than the traditional English orthography. 
Analyzing public opinions towards products, services and social events is an important but challenging task. An accurate sentiment analyzer should take both lexicon-level information and corpus-level information into account. It also needs to exploit the domainspeciﬁc knowledge and utilize the common knowledge shared across domains. In addition, we want the algorithm being able to deal with missing labels and learning from incomplete sentiment lexicons. This paper presents a LCCT (Lexicon-based and Corpus-based, Co-Training) model for semi-supervised sentiment classiﬁcation. The proposed method combines the idea of lexicon-based learning and corpus-based learning in a uniﬁed cotraining framework. It is capable of incorporating both domain-speciﬁc and domainindependent knowledge. Extensive experiments show that it achieves very competitive classiﬁcation accuracy, even with a small portion of labeled data. Comparing to state-ofthe-art sentiment classiﬁcation methods, the LCCT approach exhibits signiﬁcantly better performances on a variety of datasets in both English and Chinese. 
Multiview LSA (MVLSA) is a generalization of Latent Semantic Analysis (LSA) that supports the fusion of arbitrary views of data and relies on Generalized Canonical Correlation Analysis (GCCA). We present an algorithm for fast approximate computation of GCCA, which when coupled with methods for handling missing values, is general enough to approximate some recent algorithms for inducing vector representations of words. Experiments across a comprehensive collection of test-sets show our approach to be competitive with the state of the art. 
The semantic representation of individual word senses and concepts is of fundamental importance to several applications in Natural Language Processing. To date, concept modeling techniques have in the main based their representation either on lexicographic resources, such as WordNet, or on encyclopedic resources, such as Wikipedia. We propose a vector representation technique that combines the complementary knowledge of both these types of resource. Thanks to its use of explicit semantics combined with a novel cluster-based dimensionality reduction and an effective weighting scheme, our representation attains state-of-the-art performance on multiple datasets in two standard benchmarks: word similarity and sense clustering. We are releasing our vector representations at http://lcl.uniroma1.it/nasari/. 
We present a novel evaluation method for grammatical error correction that addresses problems with previous approaches and scores systems in terms of improvement on the original text. Our method evaluates corrections at the token level using a globally optimal alignment between the source, a system hypothesis, and a reference. Unlike the M2 Scorer, our method provides scores for both detection and correction and is sensitive to different types of edit operations. 
Research on ranked retrieval of spoken content has assumed the existence of some automated (word or phonetic) transcription. Recently, however, methods have been demonstrated for matching spoken terms to spoken content without the need for language-tuned transcription. This paper describes the ﬁrst application of such techniques to ranked retrieval, evaluated using a newly created test collection. Both the queries and the collection to be searched are based on Gujarati produced naturally by native speakers; relevance assessment was performed by other native speakers of Gujarati. Ranked retrieval is based on fast acoustic matching that identiﬁes a deeply nested set of matching speech regions, coupled with ways of combining evidence from those matching regions. Results indicate that the resulting ranked lists may be useful for some practical similarity-based ranking tasks. 
Linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical, phonological, morphological, and syntactic) from a “donor” language to a “recipient” language as a result of contacts between communities speaking different languages. Borrowed words are found in all languages, and—in contrast to cognate relationships—borrowing relationships may exist across unrelated languages (for example, about 40% of Swahili’s vocabulary is borrowed from Arabic). In this paper, we develop a model of morpho-phonological transformations across languages with features based on universal constraints from Optimality Theory (OT). Compared to several standard— but linguistically naïve—baselines, our OTinspired model obtains good performance with only a few dozen training examples, making this a cost-effective strategy for sharing lexical information across languages. 
We present Model Invertibility Regularization (MIR), a method that jointly trains two directional sequence alignment models, one in each direction, and takes into account the invertibility of the alignment task. By coupling the two models through their parameters (as opposed to through their inferences, as in Liang et al.’s Alignment by Agreement (ABA), and Ganchev et al.’s Posterior Regularization (PostCAT)), our method seamlessly extends to all IBMstyle word alignment models as well as to alignment without parallel data. Our proposed algorithm is mathematically sound and inherits convergence guarantees from EM. We evaluate MIR on two tasks: (1) On word alignment, applying MIR on fertility based models we attain higher F-scores than ABA and PostCAT. (2) On Japanese-to-English backtransliteration without parallel data, applied to the decipherment model of Ravi and Knight, MIR learns sparser models that close the gap in whole-name error rate by 33% relative to a model trained on parallel data, and further, beats a previous approach by Mylonakis et al. 
A key challenge of designing coherent semantic ontology for spoken language understanding is to consider inter-slot relations. In practice, however, it is difﬁcult for domain experts and professional annotators to deﬁne a coherent slot set, while considering various lexical, syntactic, and semantic dependencies. In this paper, we exploit the typed syntactic dependency theory for unsupervised induction and ﬁlling of semantics slots in spoken dialogue systems. More speciﬁcally, we build two knowledge graphs: a slot-based semantic graph, and a word-based lexical graph. To jointly consider word-to-word, word-toslot, and slot-to-slot relations, we use a random walk inference algorithm to combine the two knowledge graphs, guided by dependency grammars. The experiments show that considering inter-slot relations is crucial for generating a more coherent and compete slot set, resulting in a better spoken language understanding model, while enhancing the interpretability of semantic slots. 
This study tackles the problem of paraphrase acquisition: achieving high coverage as well as accuracy. Our method ﬁrst induces paraphrase patterns from given seed paraphrases, exploiting the generality of paraphrases exhibited by pairs of lexical variants, e.g., “amendment” and “amending,” in a fully empirical way. It then searches monolingual corpora for new paraphrases that match the patterns. This can extract paraphrases comprising words that are completely different from those of the given seeds. In experiments, our method expanded seed sets by factors of 42 to 206, gaining 84% to 208% more coverage than a previous method that generalizes only identical word forms. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrases retained reasonable quality, given substantially high-quality seeds. 
We introduce a distantly supervised event extraction approach that extracts complex event templates from microblogs. We show that this near real-time data source is more challenging than news because it contains information that is both approximate (e.g., with values that are close but different from the gold truth) and ambiguous (due to the brevity of the texts), impacting both the evaluation and extraction methods. For the former, we propose a novel, “soft”, F1 metric that incorporates similarity between extracted ﬁllers and the gold truth, giving partial credit to different but similar values. With respect to extraction methodology, we propose two extensions to the distant supervision paradigm: to address approximate information, we allow positive training examples to be generated from information that is similar but not identical to gold values; to address ambiguity, we aggregate contexts across tweets discussing the same event. We evaluate our contributions on the complex domain of earthquakes, with events with up to 20 arguments. Our results indicate that, despite their simplicity, our contributions yield a statistically-signiﬁcant improvement of 33% (relative) over a strong distantly-supervised system. The dataset containing the knowledge base, relevant tweets and manual annotations is publicly available. 
We present a self-training approach to unsupervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms. Our approach, called ‘iterated reranking’ (IR), starts with dependency trees generated by an unsupervised parser, and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees. Our system achieves 1.8% accuracy higher than the stateof-the-part parser of Spitkovsky et al. (2013) on the WSJ corpus. 
We deﬁne a restricted class of non-projective trees that 1) covers many natural language sentences; and 2) can be parsed exactly with a generalization of the popular arc-eager system for projective trees (Nivre, 2003). Crucially, this generalization only adds constant overhead in run-time and space keeping the parser’s total run-time linear in the worst case. In empirical experiments, our proposed transition-based parser is more accurate on average than both the arc-eager system or the swap-based system, an unconstrained nonprojective transition system with a worst-case quadratic runtime (Nivre, 2009). 
 1800 1750 1700 1650 1600 1550 1500  Representation learning is the dominant technique for unsupervised domain adaptation, but existing approaches have two major weaknesses. First, they often require the speciﬁcation of “pivot features” that generalize across domains, which are selected by taskspeciﬁc heuristics. We show that a novel but simple feature embedding approach provides better performance, by exploiting the feature template structure common in NLP problems. Second, unsupervised domain adaptation is typically treated as a task of moving from a single source to a single target domain. In reality, test data may be diverse, relating to the training data in some ways but not others. We propose an alternative formulation, in which each instance has a vector of domain attributes, can be used to learn distill the domain-invariant properties of each feature.1 
Words are polysemous. However, most approaches to representation learning for lexical semantics assign a single vector to every surface word type. Meanwhile, lexical ontologies such as WordNet provide a source of complementary knowledge to distributional information, including a word sense inventory. In this paper we propose two novel and general approaches for generating sense-speciﬁc word embeddings that are grounded in an ontology. The ﬁrst applies graph smoothing as a postprocessing step to tease the vectors of different senses apart, and is applicable to any vector space model. The second adapts predictive maximum likelihood models that learn word embeddings with latent variables representing senses grounded in an speciﬁed ontology. Empirical results on lexical semantic tasks show that our approaches effectively captures information from both the ontology and distributional statistics. Moreover, in most cases our sense-speciﬁc models outperform other models we compare against. 
Sentiment analysis has undergone a shift from document-level analysis, where labels expresses the sentiment of a whole document or whole sentence, to subsentential approaches, which assess the contribution of individual phrases, in particular including the composition of sentiment terms and phrases such as negators and intensiﬁers. Starting from a small sentiment treebank modeled after the Stanford Sentiment Treebank of Socher et al. (2013), we investigate suitable methods to perform compositional sentiment classiﬁcation for German in a data-scarce setting, harnessing cross-lingual methods as well as existing general-domain lexical resources. 
Crowdsourcing makes it possible to create translations at much lower cost than hiring professional translators. However, it is still expensive to obtain the millions of translations that are needed to train statistical machine translation systems. We propose two mechanisms to reduce the cost of crowdsourcing while maintaining high translation quality. First, we develop a method to reduce redundant translations. We train a linear model to evaluate the translation quality on a sentenceby-sentence basis, and ﬁt a threshold between acceptable and unacceptable translations. Unlike past work, which always paid for a ﬁxed number of translations for each source sentence and then chose the best from them, we can stop earlier and pay less when we receive a translation that is good enough. Second, we introduce a method to reduce the pool of translators by quickly identifying bad translators after they have translated only a few sentences. This also allows us to rank translators, so that we re-hire only good translators to reduce cost. 
We investigate the problem of predicting the quality of automatic speech recognition (ASR) output under the following rigid constraints: i) reference transcriptions are not available, ii) conﬁdence information about the system that produced the transcriptions is not accessible, and iii) training and test data come from multiple domains. To cope with these constraints (typical of the constantly increasing amount of automatic transcriptions that can be found on the Web), we propose a domain-adaptive approach based on multitask learning. Different algorithms and strategies are evaluated with English data coming from four domains, showing that the proposed approach can cope with the limitations of previously proposed single task learning methods. 
This paper studies how to incorporate the external word correlation knowledge to improve the coherence of topic modeling. Existing topic models assume words are generated independently and lack the mechanism to utilize the rich similarity relationships among words to learn coherent topics. To solve this problem, we build a Markov Random Field (MRF) regularized Latent Dirichlet Allocation (LDA) model, which deﬁnes a MRF on the latent topic layer of LDA to encourage words labeled as similar to share the same topic label. Under our model, the topic assignment of each word is not independent, but rather affected by the topic labels of its correlated words. Similar words have better chance to be put into the same topic due to the regularization of MRF, hence the coherence of topics can be boosted. In addition, our model can accommodate the subtlety that whether two words are similar depends on which topic they appear in, which allows word with multiple senses to be put into different topics properly. We derive a variational inference method to infer the posterior probabilities and learn model parameters and present techniques to deal with the hardto-compute partition function in MRF. Experiments on two datasets demonstrate the effectiveness of our model. 
Named entity recognition (NER) systems trained on newswire perform very badly when tested on Twitter. Signals that were reliable in copy-edited text disappear almost entirely in Twitter’s informal chatter, requiring the construction of specialized models. Using wellunderstood techniques, we set out to improve Twitter NER performance when given a small set of annotated training tweets. To leverage unlabeled tweets, we build Brown clusters and word vectors, enabling generalizations across distributionally similar words. To leverage annotated newswire data, we employ an importance weighting scheme. Taken all together, we establish a new state-of-the-art on two common test sets. Though it is wellknown that word representations are useful for NER, supporting experiments have thus far focused on newswire data. We emphasize the effectiveness of representations on Twitter NER, and demonstrate that their inclusion can improve performance by up to 20 F1. 
Topic models provide insights into document collections, and their supervised extensions also capture associated document-level metadata such as sentiment. However, inferring such models from data is often slow and cannot scale to big data. We build upon the “anchor” method for learning topic models to capture the relationship between metadata and latent topics by extending the vector-space representation of word-cooccurrence to include metadataspeciﬁc dimensions. These additional dimensions reveal new anchor words that reﬂect speciﬁc combinations of metadata and topic. We show that these new latent representations predict sentiment as accurately as supervised topic models, and we ﬁnd these representations more quickly without sacriﬁcing interpretability. Topic models were introduced in an unsupervised setting (Blei et al., 2003), aiding in the discovery of topical structure in text: large corpora can be distilled into human-interpretable themes that facilitate quick understanding. In addition to illuminating document collections for humans, topic models have increasingly been used for automatic downstream applications such as sentiment analysis (Titov and McDonald, 2008; Paul and Girju, 2010; Nguyen et al., 2013). Unfortunately, the structure discovered by unsupervised topic models does not necessarily constitute the best set of features for tasks such as sentiment analysis. Consider a topic model trained on Amazon product reviews. A topic model might discover a topic about vampire romance. However, we often want to  go deeper, discovering facets of a topic that reﬂect topic-speciﬁc sentiment, e.g., “buffy” and “spike” for positive sentiment vs. “twilight” and “cullen” for negative sentiment. Techniques for discovering such associations, called supervised topic models (Section 2), both produce interpretable topics and predict metadata values. While unsupervised topic models now have scalable inference strategies (Hoffman et al., 2013; Zhai et al., 2012), supervised topic model inference has not received as much attention and often scales poorly. The anchor algorithm is a fast, scalable unsupervised approach for ﬁnding “anchor words”—precise words with unique co-occurrence patterns that can deﬁne the topics of a collection of documents. We augment the anchor algorithm to ﬁnd supervised sentiment-speciﬁc anchor words (Section 3). Our algorithm is faster and just as effective as traditional schemes for supervised topic modeling (Section 4). 
When text is translated from one language into another, sentiment is preserved to varying degrees. In this paper, we use Arabic social media posts as stand-in for source language text, and determine loss in sentiment predictability when they are translated into English, manually and automatically. As benchmarks, we use manually and automatically determined sentiment labels of the Arabic texts. We show that sentiment analysis of English translations of Arabic texts produces competitive results, w.r.t. Arabic sentiment analysis. We discover that even though translation signiﬁcantly reduces the human ability to recover sentiment, automatic sentiment systems are still able to capture sentiment information from the translations. 
Some state-of-the-art summarization systems use integer linear programming (ILP) based methods that aim to maximize the important concepts covered in the summary. These concepts are often obtained by selecting bigrams from the documents. In this paper, we improve such bigram based ILP summarization methods from different aspects. First we use syntactic information to select more important bigrams. Second, to estimate the importance of the bigrams, in addition to the internal features based on the test documents (e.g., document frequency, bigram positions), we propose to extract features by leveraging multiple external resources (such as word embedding from additional corpus, Wikipedia, Dbpedia, WordNet, SentiWordNet). The bigram weights are then trained discriminatively in a joint learning model that predicts the bigram weights and selects the summary sentences in the ILP framework at the same time. We demonstrate that our system consistently outperforms the prior ILP method on different TAC data sets, and performs competitively compared to other previously reported best results. We also conducted various analyses to show the contributions of different components. 
We present a new algorithm for transforming dependency parse trees into phrase-structure parse trees. We cast the problem as structured prediction and learn a statistical model. Our algorithm is faster than traditional phrasestructure parsing and achieves 90.4% English parsing accuracy and 82.4% Chinese parsing accuracy, near to the state of the art on both benchmarks. 
Discourse relation classiﬁcation is an important component for automatic discourse parsing and natural language understanding. The performance bottleneck of a discourse parser comes from implicit discourse relations, whose discourse connectives are not overtly present. Explicit discourse connectives can potentially be exploited to collect more training data to collect more data and boost the performance. However, using them indiscriminately has been shown to hurt the performance because not all discourse connectives can be dropped arbitrarily. Based on this insight, we investigate the interaction between discourse connectives and the discourse relations and propose the criteria for selecting the discourse connectives that can be dropped independently of the context without changing the interpretation of the discourse. Extra training data collected only by the freely omissible connectives improve the performance of the system without additional features. 
Coreference resolution is a key problem in natural language understanding that still escapes reliable solutions. One fundamental difﬁculty has been that of resolving instances involving pronouns since they often require deep language understanding and use of background knowledge. In this paper we propose an algorithmic solution that involves a new representation for the knowledge required to address hard coreference problems, along with a constrained optimization framework that uses this knowledge in coreference decision making. Our representation, Predicate Schemas, is instantiated with knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that signiﬁcantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets. 
This paper presents an in-depth investigation on integrating neural language models in translation systems. Scaling neural language models is a difﬁcult task, but crucial for real-world applications. This paper evaluates the impact on end-to-end MT quality of both new and existing scaling techniques. We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios. We also focus on scalable training algorithms and investigate noise contrastive estimation and diagonal contexts as sources for further speed improvements. We explore the tradeoffs between neural models and back-off ngram models and ﬁnd that neural models make strong candidates for natural language applications in memory constrained environments, yet still lag behind traditional models in raw translation quality. We conclude with a set of recommendations one should follow to build a scalable neural language model for MT. 
The Bechdel test is a sequence of three questions designed to assess the presence of women in movies. Many believe that because women are seldom represented in ﬁlm as strong leaders and thinkers, viewers associate weaker stereotypes with women. In this paper, we present a computational approach to automate the task of ﬁnding whether a movie passes or fails the Bechdel test. This allows us to study the key differences in language use and in the importance of roles of women in movies that pass the test versus the movies that fail the test. Our experiments conﬁrm that in movies that fail the test, women are in fact portrayed as less-central and less-important characters. 
Dialogue systems that support users in complex problem solving must interpret user utterances within the context of a dynamically changing, user-created problem solving artifact. This paper presents a novel approach to semantic grounding of noun phrases within tutorial dialogue for computer programming. Our approach performs joint segmentation and labeling of the noun phrases to link them to attributes of entities within the problem-solving environment. Evaluation results on a corpus of tutorial dialogue for Java programming demonstrate that a Conditional Random Field model performs well, achieving an accuracy of 89.3% for linking semantic segments to the correct entity attributes. This work is a step toward enabling dialogue systems to support users in increasingly complex problem-solving tasks. 
We describe how a question-answering system can learn about its domain from conversational dialogs. Our system learns to relate concepts in science questions to propositions in a fact corpus, stores new concepts and relations in a knowledge graph (KG), and uses the graph to solve questions. We are the ﬁrst to acquire knowledge for question-answering from open, natural language dialogs without a ﬁxed ontology or domain model that predetermines what users can say. Our relation-based strategies complete more successful dialogs than a query expansion baseline, our taskdriven relations are more effective for solving science questions than relations from general knowledge sources, and our method is practical enough to generalize to other domains. 
Automatic analysis of impaired speech for screening or diagnosis is a growing research ﬁeld; however there are still many barriers to a fully automated approach. When automatic speech recognition is used to obtain the speech transcripts, sentence boundaries must be inserted before most measures of syntactic complexity can be computed. In this paper, we consider how language impairments can affect segmentation methods, and compare the results of computing syntactic complexity metrics on automatically and manually segmented transcripts. We ﬁnd that the important boundary indicators and the resulting segmentation accuracy can vary depending on the type of impairment observed, but that results on patient data are generally similar to control data. We also ﬁnd that a number of syntactic complexity metrics are robust to the types of segmentation errors that are typically made. 
Semantic grammars can be applied both as a language model for a speech recognizer and for semantic parsing, e.g. in order to map the output of a speech recognizer into formal meaning representations. Semantic speech recognition grammars are, however, typically created manually or learned in a supervised fashion, requiring extensive manual effort in both cases. Aiming to reduce this effort, in this paper we investigate the induction of semantic speech recognition grammars under weak supervision. We present empirical results, indicating that the induced grammars support semantic parsing of speech with a rather low loss in performance when compared to parsing of input without recognition errors. Further, we show improved parsing performance compared to applying n-gram models as language models and demonstrate how our semantic speech recognition grammars can be enhanced by weights based on occurrence frequencies, yielding an improvement in parsing performance over applying unweighted grammars. 
In modern practice, labeling a dataset often involves aggregating annotator judgments obtained from crowdsourcing. State-of-theart aggregation is performed via inference on probabilistic models, some of which are dataaware, meaning that they leverage features of the data (e.g., words in a document) in addition to annotator judgments. Previous work largely prefers discriminatively trained conditional models. This paper demonstrates that a data-aware crowdsourcing model incorporating a generative multinomial data model enjoys a strong competitive advantage over its discriminative log-linear counterpart in the typical crowdsourcing setting. That is, the generative approach is better except when the annotators are highly accurate in which case simple majority vote is often sufﬁcient. Additionally, we present a novel mean-ﬁeld variational inference algorithm for the generative model that signiﬁcantly improves on the previously reported state-of-the-art for that model. We validate our conclusions on six text classiﬁcation datasets with both human-generated and synthetic annotations. 
We describe a novel max-margin learning approach to optimize non-linear performance measures for distantly-supervised relation extraction models. Our approach can be generally used to learn latent variable models under multivariate non-linear performance measures, such as Fβ-score. Our approach interleaves Concave-Convex Procedure (CCCP) for populating latent variables with dual decomposition to factorize the original hard problem into smaller independent sub-problems. The experimental results demonstrate that our learning algorithm is more effective than the ones commonly used in the literature for distant supervision of information extraction models. On several data conditions, we show that our method outperforms the baseline and results in up to 8.5% improvement in the F1-score. 
We present a new deep learning architecture Bi-CNN-MI for paraphrase identiﬁcation (PI). Based on the insight that PI requires comparing two sentences on multiple levels of granularity, we learn multigranular sentence representations using convolutional neural network (CNN) and model interaction features at each level. These features are then the input to a logistic classiﬁer for PI. All parameters of the model (for embeddings, convolution and classiﬁcation) are directly optimized for PI. To address the lack of training data, we pretrain the network in a novel way using a language modeling task. Results on the MSRP corpus surpass that of previous NN competitors. 
Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufﬁcient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also beneﬁting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classiﬁcation (for query classiﬁcation) and information retrieval (ranking for web search), and demonstrates signiﬁcant gains over strong baselines in a comprehensive set of domain adaptation. 
We approach the task of morphological inﬂection generation as discriminative string transduction. Our supervised system learns to generate word-forms from lemmas accompanied by morphological tags, and reﬁnes them by referring to the other forms within a paradigm. Results of experiments on six diverse languages with varying amounts of training data demonstrate that our approach improves the state of the art in terms of predicting inﬂected word-forms.  Figure 1: A partial inﬂection table for the German verb atmen “to breathe” in Wiktionary.  
We present penalized expectation propagation (PEP), a novel algorithm for approximate inference in graphical models. Expectation propagation is a variant of loopy belief propagation that keeps messages tractable by projecting them back into a given family of functions. Our extension, PEP, uses a structuredsparsity penalty to encourage simple messages, thus balancing speed and accuracy. We speciﬁcally show how to instantiate PEP in the case of string-valued random variables, where we adaptively approximate ﬁnite-state distributions by variable-order n-gram models. On phonological inference problems, we obtain substantial speedup over previous related algorithms with no signiﬁcant loss in accuracy. 
Machine transliteration is often referred to as phonetic translation. We show that transliterations incorporate information from both spelling and pronunciation, and propose an effective model for joint transliteration generation from both representations. We further generalize this model to include transliterations from other languages, and enhance it with reranking and lexicon features. We demonstrate signiﬁcant improvements in transliteration accuracy on several datasets. 
It is well known that prosodic information is used by infants in early language acquisition. In particular, prosodic boundaries have been shown to help infants with sentence and wordlevel segmentation. In this study, we extend an unsupervised method for word segmentation to include information about prosodic boundaries. The boundary information used was either derived from oracle data (handannotated), or extracted automatically with a system that employs only acoustic cues for boundary detection. The approach was tested on two different languages, English and Japanese, and the results show that boundary information helps word segmentation in both cases. The performance gain obtained for two typologically distinct languages shows the robustness of prosodic information for word segmentation. Furthermore, the improvements are not limited to the use of oracle information, similar performances being obtained also with automatically extracted boundaries. 
We introduce the challenge of detecting semantically compatible words, that is, words that can potentially refer to the same thing (cat and hindrance are compatible, cat and dog are not), arguing for its central role in many semantic tasks. We present a publicly available data-set of human compatibility ratings, and a neural-network model that takes distributional embeddings of words as input and learns alternative embeddings that perform the compatibility detection task quite well. 
Distributional representations of words have been recently used in supervised settings for recognizing lexical inference relations between word pairs, such as hypernymy and entailment. We investigate a collection of these state-of-the-art methods, and show that they do not actually learn a relation between two words. Instead, they learn an independent property of a single word in the pair: whether that word is a “prototypical hypernym”. 
This paper presents the ﬁrst attempt to use word embeddings to predict the compositionality of multiword expressions. We consider both single- and multi-prototype word embeddings. Experimental results show that, in combination with a back-off method based on string similarity, word embeddings outperform a method using count-based distributional similarity. Our best results are competitive with, or superior to, state-of-the-art methods over three standard compositionality datasets, which include two types of multiword expressions and two languages. 
This paper proposes a novel approach to train word embeddings to capture antonyms. Word embeddings have shown to capture synonyms and analogies. Such word embeddings, however, cannot capture antonyms since they depend on the distributional hypothesis. Our approach utilizes supervised synonym and antonym information from thesauri, as well as distributional information from large-scale unlabelled text data. The evaluation results on the GRE antonym question task show that our model outperforms the state-of-the-art systems and it can answer the antonym questions in the F-score of 89%. 
Vectorial representations of words derived from large current events datasets have been shown to perform well on word similarity tasks. This paper shows vectorial representations derived from substantially smaller explanatory text datasets such as English Wikipedia and Simple English Wikipedia preserve enough lexical semantic information to make these kinds of category judgments with equal or better accuracy. 
This paper addresses the problem of morphological modeling in statistical speech-tospeech translation for English to Iraqi Arabic. An analysis of user data from a real-time MT-based dialog system showed that generating correct verbal inﬂections is a key problem for this language pair. We approach this problem by enriching the training data with morphological information derived from sourceside dependency parses. We analyze the performance of several parsers as well as the effect on different types of translation models. Our method achieves an improvement of more than a full BLEU point and a signiﬁcant increase in verbal inﬂection accuracy; at the same time, it is computationally inexpensive and does not rely on target-language linguistic tools. 
This paper gives a detailed experiment feedback of different approaches to adapt a statistical machine translation system towards a targeted translation project, using only small amounts of parallel in-domain data. The experiments were performed by professional translators under realistic conditions of work using a computer assisted translation tool. We analyze the inﬂuence of these adaptations on the translator productivity and on the overall post-editing effort. We show that signiﬁcant improvements can be obtained by using the presented adaptation techniques. 
Word embedding has been found to be highly powerful to translate words from one language to another by a simple linear transform. However, we found some inconsistence among the objective functions of the embedding and the transform learning, as well as the distance measurement. This paper proposes a solution which normalizes the word vectors on a hypersphere and constrains the linear transform as an orthogonal transform. The experimental results conﬁrmed that the proposed solution can offer better performance on a word similarity task and an English-toSpanish word translation task. 
We propose the use of neural networks to model source-side preordering for faster and better statistical machine translation. The neural network trains a logistic regression model to predict whether two sibling nodes of the source-side parse tree should be swapped in order to obtain a more monotonic parallel corpus, based on samples extracted from the word-aligned parallel corpus. For multiple language pairs and domains, we show that this yields the best reordering performance against other state-of-the-art techniques, resulting in improved translation quality and very fast decoding. 
We present APRO, a new method for machine translation tuning that can handle large feature sets. As opposed to other popular methods (e.g., MERT, MIRA, PRO), which involve randomness and require multiple runs to obtain a reliable result, APRO gives the same result on any run, given initial feature weights. APRO follows the pairwise ranking approach of PRO (Hopkins and May, 2011), but instead of ranking a small sampled subset of pairs from the kbest list, APRO efﬁciently ranks all pairs. By obviating the need for manually determined sampling settings, we obtain more reliable results. APRO converges more quickly than PRO and gives similar or better translation results. 
Supervised morphological paradigm learning by identifying and aligning the longest common subsequence found in inﬂection tables has recently been proposed as a simple yet competitive way to induce morphological patterns. We combine this non-probabilistic strategy of inﬂection table generalization with a discriminative classiﬁer to permit the reconstruction of complete inﬂection tables of unseen words. Our system learns morphological paradigms from labeled examples of inﬂection patterns (inﬂection tables) and then produces inﬂection tables from unseen lemmas or base forms. We evaluate the approach on datasets covering 11 different languages and show that this approach results in consistently higher accuracies vis-a`-vis other methods on the same task, thus indicating that the general method is a viable approach to quickly creating highaccuracy morphological resources. 
We present the ﬁrst dynamic programming (DP) algorithm for shift-reduce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars. To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a ﬁxed tag sequence. Experiments on both English and Chinese treebanks show that our DP parser signiﬁcantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers.  shift-reduce constituency parser which always ﬁnishes in same number of steps, eliminating the complicated asynchronicity issue in previous work (Zhu et al., 2013; Wang and Xue, 2014), and then develop dynamic programming on top of that. Secondly, to alleviate the error propagation from POS tagging, we also extends the algorithm to take a tagging sausage lattice as input, which is a compromise between pipeline and joint approaches (Hatori et al., 2011; Li et al., 2011; Wang and Xue, 2014). Our DP parser achieves state-of-the-art performances on both Chinese and English treebanks (at 90.8% on PTB and 83.9% on CTB, the latter being the highest in literature).  
Transcribing documents from the printing press era, a challenge in its own right, is more complicated when documents interleave multiple languages—a common feature of 16th century texts. Additionally, many of these documents precede consistent orthographic conventions, making the task even harder. We extend the state-of-the-art historical OCR model of Berg-Kirkpatrick et al. (2013) to handle word-level code-switching between multiple languages. Further, we enable our system to handle spelling variability, including now-obsolete shorthand systems used by printers. Our results show average relative character error reductions of 14% across a variety of historical texts. 
Citation sentences (citances) to a reference article have been extensively studied for summarization tasks. However, citances might not accurately represent the content of the cited article, as they often fail to capture the context of the reported ﬁndings and can be affected by epistemic value drift. Following the intuition behind the TAC (Text Analysis Conference) 2014 Biomedical Summarization track, we propose a system that identiﬁes text spans in the reference article that are related to a given citance. We refer to this problem as citance-reference spans matching. We approach the problem as a retrieval task; in this paper, we detail a comparison of different citance reformulation methods and their combinations. While our results show improvement over the baseline (up to 25.9%), their absolute magnitude implies that there is ample room for future improvement. 
A major opportunity for NLP to have a realworld impact is in helping educators score student writing, particularly content-based writing (i.e., the task of automated short answer scoring). A major challenge in this enterprise is that scored responses to a particular question (i.e., labeled data) are valuable for modeling but limited in quantity. Additional information from the scoring guidelines for humans, such as exemplars for each score level and descriptions of key concepts, can also be used. Here, we explore methods for integrating scoring guidelines and labeled responses, and we ﬁnd that stacked generalization (Wolpert, 1992) improves performance, especially for small training sets. 
Existing timeline generation systems for complex events consider only information from traditional media, ignoring the rich social context provided by user-generated content that reveals representative public interests or insightful opinions. We instead aim to generate socially-informed timelines that contain both news article summaries and selected user comments. We present an optimization framework designed to balance topical cohesion between the article and comment summaries along with their informativeness and coverage of the event. Automatic evaluations on real-world datasets that cover four complex events show that our system produces more informative timelines than state-of-theart systems. In human evaluation, the associated comment summaries are furthermore rated more insightful than editor’s picks and comments ranked highly by users.  March 16th, 2014 - Crimeans vote in a referendum to rejoin Russia or return to its status under the 1992 constitution. March 17th, 2014 - The Crimean parliament ofﬁcially declared independence and requested full accession to the Russian Federation. - Obama declared sanctions on Russian ofﬁcials considered responsible for the crisis. - The leader of the pro-Russian organization “Youth Unity” was arrested. March 18th, 2014 - President Obama warned Vladimir Putin that further provocations by Russia could isolate and diminish its inﬂuence. - One pro-Russian soldier was killed in the Simferopol incident. … summaries for other dates …  * Comment A: The “Crimean Parliament”, headed by an ethnic Russian separatist who was elected leader of parliament AFTER pro-Russian armed forces occupied the parliamentary chambers, has voted for Crimea to be annexed into Russia… * Comment B: Does the West and US have a policy at all? The Obama administration has warned of “increasingly harsh sanctions”, but it is unlikely that Europe will comply… * Comment C: Sanctions are effective and if done in unison with the EU…  Figure 1: A snippet of the event timeline on Ukraine Crisis is displayed on the left. On the right, we display a set of representative comments addressing the article summary of March 17th. Comment A (underlined) brings a perspective on “Crimean parliament passes declaration of independence” (the article sentence is also underlined on the left). Comments B and C focus on Obama’s sanctions on Ukrainian and Russian ofﬁcials. Sentences linked by edges belong to the same event thread, which is centered on the entities with the same color.  
In this paper we study the task of movie script summarization, which we argue could enhance script browsing, give readers a rough idea of the script’s plotline, and speed up reading time. We formalize the process of generating a shorter version of a screenplay as the task of ﬁnding an optimal chain of scenes. We develop a graph-based model that selects a chain by jointly optimizing its logical progression, diversity, and importance. Human evaluation based on a question-answering task shows that our model produces summaries which are more informative compared to competitive baselines. 
We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-tograph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-totext generator. The framework is data-driven, trainable, and not speciﬁcally designed for a particular domain. Experiments on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization 
Previous work on text coherence was primarily based on matching multiple mentions of the same entity in diﬀerent parts of the text; therefore, it misses the contribution from semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft). In this paper, we capture such semantic relatedness by leveraging world knowledge (e.g., Gates is the person who created Microsoft), and use two existing evaluation frameworks. First, in the unsupervised framework, we introduce semantic relatedness as an enrichment to the original graph-based model of Guinaudeau and Strube (2013). In addition, we incorporate semantic relatedness as additional features into the popular entity-based model of Barzilay and Lapata (2008). Across both frameworks, our enriched model with semantic relatedness outperforms the original methods, especially on short documents. 
Recent work has successfully leveraged the semantic information extracted from lexical knowledge bases such as WordNet and FrameNet to improve English event coreference resolvers. The lack of comparable resources in other languages, however, has made the design of high-performance non-English event coreference resolvers, particularly those employing unsupervised models, very difficult. We propose a generative model for the under-studied task of Chinese event coreference resolution that rivals its supervised counterparts in performance when evaluated on the ACE 2005 corpus. 
Coreference is a core nlp problem. However, newswire data, the primary source of existing coreference data, lack the richness necessary to truly solve coreference. We present a new domain with denser references—quiz bowl questions—that is challenging and enjoyable to humans, and we use the quiz bowl community to develop a new coreference dataset, together with an annotation framework that can tag any text data with coreferences and named entities. We also successfully integrate active learning into this annotation pipeline to collect documents maximally useful to coreference models. State-of-the-art coreference systems underperform a simple classiﬁer on our new dataset, motivating non-newswire data for future coreference research. 
Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data. Unfortunately, these methods share a shortcoming with all other distantly supervised approaches: they cannot learn to extract target relations without existing data in the knowledge base, and likewise, these models are inaccurate for relations with sparse data. Rule-based extractors, on the other hand, can be easily extended to novel relations and improved for existing but inaccurate relations, through ﬁrst-order formulae that capture auxiliary domain knowledge. However, usually a large set of such formulae is necessary to achieve generalization. In this paper, we introduce a paradigm for learning low-dimensional embeddings of entity-pairs and relations that combine the advantages of matrix factorization with ﬁrst-order logic domain knowledge. We introduce simple approaches for estimating such embeddings, as well as a novel training algorithm to jointly optimize over factual and ﬁrst-order logic information. Our results show that this method is able to learn accurate extractors with little or no distant supervision alignments, while at the same time generalizing to textual patterns that do not appear in the formulae. 
Most successful Entity Linking (EL) methods aim to link mentions to their referent entities in a structured Knowledge Base (KB) by comparing their respective contexts, often using similarity measures. While the KB structure is given, current methods have suffered from impoverished information representations on the mention side. In this paper, we demonstrate the effectiveness of Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to select high quality sets of entity “collaborators” to feed a simple similarity measure (Jaccard) to link entity mentions. Experimental results show that AMR captures contextual properties discriminative enough to make linking decisions, without the need for EL training data, and that system with AMR parsing output outperforms hand labeled traditional semantic roles as context representation for EL. Finally, we show promising preliminary results for using AMR to select sets of “coherent” entity mentions for collective entity linking 1. 
This paper describes IDEST, a new method for learning paraphrases of event patterns. It is based on a new neural network architecture that only relies on the weak supervision signal that comes from the news published on the same day and mention the same real-world entities. It can generalize across extractions from different dates to produce a robust paraphrase model for event patterns that can also capture meaningful representations for rare patterns. We compare it with two state-of-the-art systems and show that it can attain comparable quality when trained on a small dataset. Its generalization capabilities also allow it to leverage much more data, leading to substantial quality improvements. 
Extensive lexical knowledge is necessary for temporal analysis and planning tasks. We address in this paper a lexical setting that allows for the straightforward incorporation of rich features and structural constraints. We explore a lexical event ordering task, namely determining the likely temporal order of events based solely on the identity of their predicates and arguments. We propose an “edgefactored” model for the task that decomposes over the edges of the event graph. We learn it using the structured perceptron. As lexical tasks require large amounts of text, we do not attempt manual annotation and instead use the textual order of events in a domain where this order is aligned with their temporal order, namely cooking recipes. 
Current approaches to cross-lingual information retrieval (CLIR) rely on standard retrieval models into which query translations by statistical machine translation (SMT) are integrated at varying degree. In this paper, we present an attempt to turn this situation on its head: Instead of the retrieval aspect, we emphasize the translation component in CLIR. We perform search by using an SMT decoder in forced decoding mode to produce a bag-ofwords representation of the target documents to be ranked. The SMT model is extended by retrieval-speciﬁc features that are optimized jointly with standard translation features for a ranking objective. We ﬁnd signiﬁcant gains over the state-of-the-art in a large-scale evaluation on cross-lingual search in the domains patents and Wikipedia. 
Evaluation of segment-level machine translation metrics is currently hampered by: (1) low inter-annotator agreement levels in human assessments; (2) lack of an effective mechanism for evaluation of translations of equal quality; and (3) lack of methods of signiﬁcance testing improvements over a baseline. In this paper, we provide solutions to each of these challenges and outline a new human evaluation methodology aimed speciﬁcally at assessment of segment-level metrics. We replicate the human evaluation component of WMT-13 and reveal that the current state-of-the-art performance of segment-level metrics is better than previously believed. Three segment-level metrics — METEOR, NLEPOR and SENTBLEUMOSES — are found to correlate with human assessment at a level not signiﬁcantly outperformed by any other metric in both the individual language pair assessment for Spanish-toEnglish and the aggregated set of 9 language pairs. 
We present our work on leveraging multilingual parallel corpora of small sizes for Statistical Machine Translation between Japanese and Hindi using multiple pivot languages. In our setting, the source and target part of the corpus remains the same, but we show that using several different pivot to extract phrase pairs from these source and target parts lead to large BLEU improvements. We focus on a variety of ways to exploit phrase tables generated using multiple pivots to support a direct source-target phrase table. Our main method uses the Multiple Decoding Paths (MDP) feature of Moses, which we empirically verify as the best compared to the other methods we used. We compare and contrast our various results to show that one can overcome the limitations of small corpora by using as many pivot languages as possible in a multilingual setting. Most importantly, we show that such pivoting aids in learning of additional phrase pairs which are not learned when the direct sourcetarget corpus is small. We obtained improvements of up to 3 BLEU points using multiple pivots for Japanese to Hindi translation compared to when only one pivot is used. To the best of our knowledge, this work is also the ﬁrst of its kind to attempt the simultaneous utilization of 7 pivot languages at decoding time. 
The rapid growth of information sources brings a unique challenge to biographical information extraction: how to ﬁnd speciﬁc facts without having to read all the words. An effective solution is to follow the human scanning strategy which keeps a speciﬁc keyword in mind and searches within a speciﬁc scope. In this paper, we mimic a scanning process to extract biographical facts. We use event and relation triggers as keywords, identify their scopes and apply type constraints to extract answers within the scope of a trigger. Experiments demonstrate that our approach outperforms state-of-the-art methods up to 26% absolute gain in F-score without using any syntactic analysis or external knowledge bases. 
Two goals are targeted by computer philology for ancient manuscript corpora: ﬁrstly, making an edition, that is roughly speaking one text version representing the whole corpus, which contains variety induced through copy errors and other processes and secondly, producing a stemma. A stemma is a graphbased visualization of the copy history with manuscripts as nodes and copy events as edges. Its root, the so-called archetype, is the supposed original text or urtext from which all subsequent copies are made. Our main contribution is to present one of the ﬁrst computational approaches to automatic archetype reconstruction and to introduce the ﬁrst textbased evaluation for automatically produced archetypes. We compare a philologically generated archetype with one generated by bioinformatic software. 
Bootstrapped classiﬁers iteratively generalize from a few seed examples or prototypes to other examples of target labels. However, sparseness of language and limited supervision make the task difﬁcult. We address this problem by using distributed vector representations of words to aid the generalization. We use the word vectors to expand entity sets used for training classiﬁers in a bootstrapped pattern-based entity extraction system. Our experiments show that the classiﬁers trained with the expanded sets perform better on entity extraction from four online forums, with 30% F1 improvement on one forum. The results suggest that distributed representations can provide good directions for generalization in a bootstrapping system. 
We present a multi-task learning approach that jointly trains three word alignment models over disjoint bitexts of three languages: source, target and pivot. Our approach builds upon model triangulation, following Wang et al., which approximates a source-target model by combining source-pivot and pivot-target models. We develop a MAP-EM algorithm that uses triangulation as a prior, and show how to extend it to a multi-task setting. On a low-resource Czech-English corpus, using French as the pivot, our multi-task learning approach more than doubles the gains in both Fand Bleu scores compared to the interpolation approach of Wang et al. Further experiments reveal that the choice of pivot language does not signiﬁcantly aﬀect performance. 
In this paper, we describe the problem of cognate identiﬁcation in NLP. We introduce the idea of gap-weighted subsequences for discriminating cognates from non-cognates. We also propose a scheme to integrate phonetic features into the feature vectors for cognate identiﬁcation. We show that subsequence based features perform better than state-ofthe-art classiﬁer for the purpose of cognate identiﬁcation. The contribution of this paper is the use of subsequence features for cognate identiﬁcation. 
In this paper, we investigate the challenging task of understanding short text (STU task) by jointly considering topic modeling and knowledge incorporation. Knowledge incorporation can solve the content sparsity problem effectively for topic modeling. Speciﬁcally, the phrase topic model is proposed to leverage the auto-mined knowledge, i.e., the phrases, to guide the generative process of short text. Experimental results illustrate the effectiveness of the mechanism that utilizes knowledge to improve topic modeling’s performance. 
An acid test for any new Word Sense Disambiguation (WSD) algorithm is its performance against the Most Frequent Sense (MFS). The ﬁeld of WSD has found the MFS baseline very hard to beat. Clearly, if WSD researchers had access to MFS values, their striving to better this heuristic will push the WSD frontier. However, getting MFS values requires sense annotated corpus in enormous amounts, which is out of bounds for most languages, even if their WordNets are available. In this paper, we propose an unsupervised method for MFS detection from the untagged corpora, which exploits word embeddings. We compare the word embedding of a word with all its sense embeddings and obtain the predominant sense with the highest similarity. We observe significant performance gain for Hindi WSD over the WordNet First Sense (WFS) baseline. As for English, the SemCor baseline is bettered for those words whose frequency is greater than 2. Our approach is language and domain independent. 
We present a novel approach for relation classiﬁcation, using a recursive neural network (RNN), based on the shortest path between two entities in a dependency graph. Previous works on RNN are based on constituencybased parsing because phrasal nodes in a parse tree can capture compositionality in a sentence. Compared with constituency-based parse trees, dependency graphs can represent relations more compactly. This is particularly important in sentences with distant entities, where the parse tree spans words that are not relevant to the relation. In such cases RNN cannot be trained effectively in a timely manner. However, due to the lack of phrasal nodes in dependency graphs, application of RNN is not straightforward. In order to tackle this problem, we utilize dependency constituent units called chains. Our experiments on two relation classiﬁcation datasets show that Chain based RNN provides a shallower network, which performs considerably faster and achieves better classiﬁcation results. 
LR parsing is a popular parsing strategy for variants of Context-Free Grammar (CFG). It has also been used for mildly context-sensitive formalisms, such as Tree-Adjoining Grammar. In this paper, we present the ﬁrst LRstyle parsing algorithm for Linear ContextFree Rewriting Systems (LCFRS), a mildly context-sensitive extension of CFG which has received considerable attention in the last years. 
We present a simple, yet effective approach to adapt part-of-speech (POS) taggers to new domains. Our approach only requires a dictionary and large amounts of unlabeled target data. The idea is to use the dictionary to mine the unlabeled target data for unambiguous word sequences, thus effectively collecting labeled target data. We add the mined instances to available labeled newswire data to train a POS tagger for the target domain. The induced models signiﬁcantly improve tagging accuracy on held-out test sets across three domains (Twitter, spoken language, and search queries). We also present results for Dutch, Spanish and Portuguese Twitter data, and provide two novel manually-annotated test sets. 
Multi-document Summarization (MDS) is of great value to many real world applications. Many scoring models are proposed to select appropriate sentences from documents to form the summary, in which the clustering-based methods are popular. In this work, we propose a uniﬁed sentence scoring model which measures representativeness and diversity at the same time. Experimental results on DUC04 demonstrate that our MDS method outperforms the DUC04 best method and the existing clustering-based methods, and it yields close results compared to the state-of-the-art generic MDS methods. Advantages of the proposed MDS method are two-fold: (1) The density peaks clustering algorithm is ﬁrstly adopted, which is effective and fast. (2) No external resources such as Wordnet and Wikipedia or complex language parsing algorithms is used, making reproduction and deployment very easy in real environment. 
This paper reports on our research to generate multilingual semantic lexical resources and develop multilingual semantic annotation software, which assigns each word in running text to a semantic category based on a lexical semantic classification scheme. Such tools have an important role in developing intelligent multilingual NLP, text mining and ICT systems. In this work, we aim to extend an existing English semantic annotation tool to cover a range of languages, namely Italian, Chinese and Brazilian Portuguese, by bootstrapping new semantic lexical resources via automatically translating existing English semantic lexicons into these languages. We used a set of bilingual dictionaries and word lists for this purpose. In our experiment, with minor manual improvement of the automatically generated semantic lexicons, the prototype tools based on the new lexicons achieved an average lexical coverage of 79.86% and an average annotation precision of 71.42% (if only precise annotations are considered) or 84.64% (if partially correct annotations are included) on the three languages. Our experiment demonstrates that it is feasible to rapidly develop prototype semantic annotation tools for new languages by automatically bootstrapping new semantic lexicons based on existing ones.  
Sparse representations of text such as bag-ofwords models or extended explicit semantic analysis (ESA) representations are commonly used in many NLP applications. However, for short texts, the similarity between two such sparse vectors is not accurate due to the small term overlap. While there have been multiple proposals for dense representations of words, measuring similarity between short texts (sentences, snippets, paragraphs) requires combining these token level similarities. In this paper, we propose to combine ESA representations and word2vec representations as a way to generate denser representations and, consequently, a better similarity measure between short texts. We study three densiﬁcation mechanisms that involve aligning sparse representation via many-to-many, many-to-one, and oneto-one mappings. We then show the effectiveness of these mechanisms on measuring similarity between short texts. 
In September 2014, Twitter users unequivocally reacted to the Ray Rice assault scandal by unleashing personal stories of domestic abuse via the hashtags #WhyIStayed or #WhyILeft. We explore at a macro-level ﬁrsthand accounts of domestic abuse from a substantial, balanced corpus of tweeted instances designated with these tags. To seek insights into the reasons victims give for staying in vs. leaving abusive relationships, we analyze the corpus using linguistically motivated methods. We also report on an annotation study for corpus assessment. We perform classiﬁcation, contributing a classiﬁer that discriminates between the two hashtags exceptionally well at 82% accuracy with a substantial error reduction over its baseline. 
Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study. 
In this paper we present our work on recognizing high level social constructs such as Leadership and Status from textual conversation using an approach that makes use of the background knowledge about social hierarchy and integrates statistical methods and symbolic logic based methods. We use a stratiﬁed approach in which we ﬁrst detect lower level language constructs such as politeness, command and agreement that help us to infer intermediate constructs such as deference, closeness and authority that are observed between the parties engaged in conversation. These intermediate constructs in turn are used to determine the social constructs Leadership and Status. We have implemented this system successfully in both English and Korean languages and achieved considerable accuracy. 
We present two simple modiﬁcations to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 
This work is an attempt to automatically obtain numerical attributes of physical objects. We propose representing each physical object as a feature vector and representing sizes as linear functions of feature vectors. We train the function in the framework of the combined regression and ranking with many types of fragmentary clues including absolute clues (e.g., A is 30cm long) and relative clues (e.g., A is larger than B). 
Unsupervised word embeddings have been shown to be valuable as features in supervised learning problems; however, their role in unsupervised problems has been less thoroughly explored. In this paper, we show that embeddings can likewise add value to the problem of unsupervised POS induction. In two representative models of POS induction, we replace multinomial distributions over the vocabulary with multivariate Gaussian distributions over word embeddings and observe consistent improvements in eight languages. We also analyze the eﬀect of various choices while inducing word embeddings on “downstream” POS induction results. 
Integer Linear Programming (ILP) based summarization methods have been widely adopted recently because of their state-of-the-art performance. This paper proposes two new modiﬁcations in this framework for update summarization. Our key idea is to use discriminative models with a set of features to measure both the salience and the novelty of words and sentences. First, these features are used in a supervised model to predict the weights of the concepts used in the ILP model. Second, we generate preliminary sentence candidates in the ILP model and then rerank them using sentence level features. We evaluate our method on different TAC update summarization data sets, and the results show that our system performs competitively compared to the best TAC systems based on the ROUGE evaluation metric. 
This paper presents an annotation scheme for adding entity and event target annotations to the MPQA corpus, a rich span-annotated opinion corpus. The new corpus promises to be a valuable new resource for developing systems for entity/event-level sentiment analysis. Such systems, in turn, would be valuable in NLP applications such as Automatic Question Answering. We introduce the idea of entity and event targets (eTargets), describe the annotation scheme, and present the results of an agreement study. 
Online shopping caters the needs of millions of users on a daily basis. To build an accurate system that can retrieve relevant products for a query like “MB252 with travel bags” one requires product and query categorization mechanisms, which classify the text as Home&Garden>Kitchen&Dining>Kitchen Appliances>Blenders. One of the biggest challenges in e-Commerce is that providers like Amazon, e-Bay, Google, Yahoo! and Walmart organize products into different product taxonomies making it hard and time-consuming for sellers to categorize goods for each shopping platform. To address this challenge, we propose an automatic product categorization mechanism, which for a given product title assigns the correct product category from a taxonomy. We conducted an empirical evaluation on 445, 408 product titles and used a rich product taxonomy of 319 categories organized into 6 levels. We compared performance against multiple algorithms and found that the best performing system reaches .88 f-score. 
Voice conversion is the task of transforming a source speaker’s voice so that it sounds like a target speaker’s voice. We present a GPUfriendly local regression model for voice conversion that is capable of converting speech in real-time and achieves state-of-the-art accuracy on this task. Our model uses a new approximation for computing local regression coefﬁcients that is explicitly designed to preserve memory locality. As a result, our inference procedure is amenable to efﬁcient implementation on the GPU. Our approach is more than 10X faster than a highly optimized CPUbased implementation, and is able to convert speech 2.7X faster than real-time. 
Response-based learning allows to adapt a statistical machine translation (SMT) system to an extrinsic task by extracting supervision signals from task-speciﬁc feedback. In this paper, we elicit response signals for SMT adaptation by executing semantic parses of translated queries against the Freebase database. The challenge of our work lies in scaling semantic parsers to the lexical diversity of opendomain databases. We ﬁnd that parser performance on incorrect English sentences, which is standardly ignored in parser evaluation, is key in model selection. In our experiments, the biggest improvements in F1-score for returning the correct answer from a semantic parse for a translated query are achieved by selecting a parser that is carefully enhanced by paraphrases and synonyms. 
Developing a system that can automatically respond to a user’s utterance has recently become a topic of research in natural language processing. However, most works on the topic take into account only a single preceding utterance to generate a response. Recent works demonstrate that the application of statistical machine translation (SMT) techniques towards monolingual dialogue setting, in which a response is treated as a translation of a stimulus, has a great potential, and we exploit the approach to tackle the context-dependent response generation task. We attempt to extract relevant and signiﬁcant information from the wider contextual scope of the conversation, and incorporate it into the SMT techniques. We also discuss the advantages and limitations of this approach through our experimental results. 
Open domain relation extraction systems identify relation and argument phrases in a sentence without relying on any underlying schema. However, current state-of-the-art relation extraction systems are available only for English because of their heavy reliance on linguistic tools such as part-of-speech taggers and dependency parsers. We present a cross-lingual annotation projection method for language independent relation extraction. We evaluate our method on a manually annotated test set and present results on three typologically different languages. We release these manual annotations and extracted relations in ten languages from Wikipedia. 
Natural language processing (NLP) annotation projects employ guidelines to maximize inter-annotator agreement (IAA), and models are estimated assuming that there is one single ground truth. However, not all disagreement is noise, and in fact some of it may contain valuable linguistic information. We integrate such information in the training of a cost-sensitive dependency parser. We introduce ﬁve different factorizations of IAA and the corresponding loss functions, and evaluate these across six different languages. We obtain robust improvements across the board using a factorization that considers dependency labels and directionality. The best method-dataset combination reaches an average overall error reduction of 6.4% in labeled attachment score. 
Research on automatically geolocating social media users has conventionally been based on the text content of posts from a given user or the social network of the user, with very little crossover between the two, and no benchmarking of the two approaches over comparable datasets. We bring the two threads of research together in ﬁrst proposing a text-based method based on adaptive grids, followed by a hybrid network- and text-based method. Evaluating over three Twitter datasets, we show that the empirical difference between textand network-based methods is not great, and that hybridisation of the two is superior to the component methods, especially in contexts where the user graph is not well connected. We achieve state-of-the-art results on all three datasets.  of automatically geolocating (predicting lat/long coordinates) of users based on their publicly available posts, metadata and social network information. These approaches are built on the premise that a user’s location is evident from their posts, or through location homophily in their social network. Our contributions in this paper are: a) the demonstration that network-based methods are generally superior to text-based user geolocation methods due to their robustness; b) the proposal of a hybrid classiﬁcation method that backs-off from network- to text-based predictions for disconnected users, which we show to achieve state-of-the-art accuracy over all Twitter datasets we experiment with; and c) empirical evidence to suggest that text-based geolocation methods are largely competitive with network-based methods.  
This work, concerning paraphrase identiﬁcation task, on one hand contributes to expanding deep learning embeddings to include continuous and discontinuous linguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN to learn the discriminative weights of words and phrases speciﬁc to paraphrase task, so that a weighted sum of embeddings can represent sentences more effectively. Based on these two innovations we get competitive state-of-the-art performance on paraphrase identiﬁcation. 
Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overﬁtting. We demonstrate that our model attains state-of-the-art results on ACE and ERE ﬁne-grained relation extraction. 
We present a new context-aware method for lexical simpliﬁcation that uses two free language resources and real web frequencies. We compare it with the state-of-the-art method for lexical simpliﬁcation in Spanish and the established simpliﬁcation baseline, that is, the most frequent synonym. Our method improves upon the other methods in the detection of complex words, in meaning preservation, and in simplicity. Although we use Spanish, the method can be extended to other languages since it does not require alignment of parallel corpora. 
We introduce a simple wrapper method that uses off-the-shelf word embedding algorithms to learn task-speciﬁc bilingual word embeddings. We use a small dictionary of easily-obtainable task-speciﬁc word equivalence classes to produce mixed context-target pairs that we use to train off-the-shelf embedding models. Our model has the advantage that it (a) is independent of the choice of embedding algorithm, (b) does not require parallel data, and (c) can be adapted to speciﬁc tasks by re-deﬁning the equivalence classes. We show how our method outperforms off-the-shelf bilingual embeddings on the task of unsupervised cross-language partof-speech (POS) tagging, as well as on the task of semi-supervised cross-language super sense (SuS) tagging. 
We present the ﬁrst truly streaming cross document coreference resolution (CDC) system. Processing inﬁnite streams of mentions forces us to use a constant amount of memory and so we maintain a representative, ﬁxed sized sample at all times. For the sample to be representative it should represent a large number of entities whilst taking into account both temporal recency and distant references. We introduce new sampling techniques that take into account a notion of streaming discourse (current mentions depend on previous mentions). Using the proposed sampling techniques we are able to get a CEAFe score within 5% of a non-streaming system while using only 30% of the memory.  or store a sample. Compression is more computationally expensive as it involves merging/forgetting mention components (for example: components of a vector) whereas sampling decides to store or forget whole mentions. We investigate sampling techniques due to their computational efﬁciency. We explore which mentions should be stored while performing streaming CDC. A sample should represent a diverse set of entities while taking into account both temporal recency and distant mentions. We show that using a notion of streaming discourse, where what is currently being mentioned depends on what was previously mentioned signiﬁcantly improves performance on a new CDC annotated Twitter corpus.  
We present a large-scale Native Language Identiﬁcation (NLI) experiment on new data, with a focus on cross-corpus evaluation to identify corpus- and genre-independent language transfer features. We test a new corpus and show it is comparable to other NLI corpora and suitable for this task. Cross-corpus evaluation on two large corpora achieves good accuracy and evidences the existence of reliable language transfer features, but lower performance also suggests that NLI models are not completely portable across corpora. Finally, we present a brief case study of features distinguishing Japanese learners’ English writing, demonstrating the presence of cross-corpus and cross-genre language transfer features that are highly applicable to SLA and ESL research. 
Speech transcripts often only capture semantic content, omitting disﬂuencies that can be useful for analyzing social dynamics of a discussion. This work describes steps in building a model that can recover a large fraction of locations where disﬂuencies were present, by transforming carefully annotated text to match the standard transcription style, introducing a two-stage model for handling different types of disﬂuencies, and applying semi-supervised learning. Experiments show improvement in disﬂuency detection on Supreme Court oral arguments, nearly 23% improvement in F1. 
Semantic parsing has made signiﬁcant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the ﬁrst linear-time incremental shift-reduce-style semantic parsing algorithm which is more efﬁcient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GEOQUERY, JOBS and ATIS domains. 
We present a framework for using continuousspace vector representations of word meaning to derive new vectors representing the meaning of senses listed in a semantic network. It is a post-processing approach that can be applied to several types of word vector representations. It uses two ideas: ﬁrst, that vectors for polysemous words can be decomposed into a convex combination of sense vectors; secondly, that the vector for a sense is kept similar to those of its neighbors in the network. This leads to a constrained optimization problem, and we present an approximation for the case when the distance function is the squared Euclidean. We applied this algorithm on a Swedish semantic network, and we evaluate the quality of the resulting sense representations extrinsically by showing that they give large improvements when used in a classiﬁer that creates lexical units for FrameNet frames. 
 Random walks over large knowledge bases like WordNet have been successfully used in word similarity, relatedness and disambiguation tasks. Unfortunately, those algorithms are relatively slow for large repositories, with signiﬁcant memory footprints. In this paper we present a novel algorithm which encodes the structure of a knowledge base in a continuous vector space, combining random walks and neural net language models in order to produce novel word representations. Evaluation in word relatedness and similarity datasets yields equal or better results than those of a random walk algorithm, using a dense representation (300 dimensions instead of 117K). Furthermore, the word representations are complementary to those of the random walk algorithm and to corpus-based continuous representations, improving the stateof-the-art in the similarity dataset. Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations. 
We present a data-driven technique for acquiring domain-level importance of verbs from the analysis of abstract/article pairs of world news articles. We show that existing lexical resources capture some the semantic characteristics for important words in the domain. We develop a novel characterization of the association between verbs and personal story narratives, which is descriptive of verbs avoided in summaries for this domain. 
Lemmatization for the Sumerian language, compared to the modern languages, is much more challenging due to that it is a long dead language, highly skilled language experts are extremely scarce and more and more Sumerian texts are coming out. This paper describes how our unsupervised Sumerian named-entity recognition (NER) system helps to improve the lemmatization of the Cuneiform Digital Library Initiative (CDLI), a specialist database of cuneiform texts, from the Ur III period. Experiments show that a promising improvement in personal name annotation in such texts and a substantial reduction in expert annotation effort can be achieved by leveraging our system with minimal seed annotation. 
Our research aims to extract information about medication use from veterinary discussion forums. We introduce the task of categorizing information about medication use to determine whether a doctor has prescribed medication, changed protocols, observed effects, or stopped use of a medication. First, we create a medication detector for informal veterinary texts and show that features derived from the Web can be very powerful. Second, we create classiﬁers to categorize each medication mention with respect to six categories. We demonstrate that this task beneﬁts from a rich linguistic feature set, domain-speciﬁc semantic features produced by a weakly supervised semantic tagger, and balanced self-training. 
This paper presents CROWN, an automatically constructed extension of WordNet that augments its taxonomy with novel lemmas from Wiktionary. CROWN ﬁlls the important gap in WordNet’s lexicon for slang, technical, and rare lemmas, and more than doubles its current size. In two evaluations, we demonstrate that the construction procedure is accurate and has a signiﬁcant impact on a WordNet-based algorithm encountering novel lemmas. 
Cross-lingual text classiﬁcation is a major challenge in natural language processing, since often training data is available in only one language (target language), but not available for the language of the document we want to classify (source language). Here, we propose a method that only requires a bilingual dictionary to bridge the language gap. Our proposed probabilistic model allows us to estimate translation probabilities that are conditioned on the whole source document. The assumption of our probabilistic model is that each document can be characterized by a distribution over topics that help to solve the translation ambiguity of single words. Using the derived translation probabilities, we then calculate the expected word frequency of each word type in the target language. Finally, these expected word frequencies can be used to classify the source text with any classiﬁer that was trained using only target language documents. Our experiments conﬁrm the usefulness of our proposed method. 
The subconscious inﬂuence of framing on perceptions of political issues is well-document in political science and communication research. A related line of work suggests that drawing attention to framing may help reduce such framing effects by enabling frame reﬂection, critical examination of the framing underlying an issue. However, deﬁnite guidance on how to identify framing does not exist. This paper presents a technique for identifying frame-invoking language. The paper ﬁrst describes a human subjects pilot study that explores how individuals identify framing and informs the design of our technique. The paper then describes our data collection and annotation approach. Results show that the best performing classiﬁers achieve performance comparable to that of human annotators, and they indicate which aspects of language most pertain to framing. Both technical and theoretical implications are discussed. 
While the effect of various lexical, syntactic, semantic and stylistic features have been addressed in persuasive language from a computational point of view, the persuasive effect of phonetics has received little attention. By modeling a notion of euphony and analyzing four datasets comprising persuasive and nonpersuasive sentences in different domains (political speeches, movie quotes, slogans and tweets), we explore the impact of sounds on different forms of persuasiveness. We conduct a series of analyses and prediction experiments within and across datasets. Our results highlight the positive role of phonetic devices on persuasion. 
 Input video:  Solving the visual symbol grounding problem has long been a goal of artiﬁcial intelligence. The ﬁeld appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a uniﬁed deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation. 
 Given a (static) scene, a human can effortlessly describe what is going on (who is doing what to whom, how, and why). The process requires knowledge about the world, how it is perceived, and described. In this paper we study the problem of interpreting and verbalizing visual information using abstract scenes created from collections of clip art images. We propose a model inspired by machine translation operating over a large parallel corpus of visual relations and linguistic descriptions. We demonstrate that this approach produces human-like scene descriptions which are both ﬂuent and relevant, outperforming a number of competitive alternatives based on templates, sentence-based retrieval, and a multimodal neural language model. 
This work presents a ﬂexible and efﬁcient discriminative training approach for statistical machine translation. We propose to use the RPROP algorithm for optimizing a maximum expected BLEU objective and experimentally compare it to several other updating schemes. It proves to be more efﬁcient and effective than the previously proposed growth transformation technique and also yields better results than stochastic gradient descent and AdaGrad. We also report strong empirical results on two large scale tasks, namely BOLT Chinese→English and WMT German→English, where our ﬁnal systems outperform results reported by Setiawan and Zhou (2013) and on matrix.statmt.org. On the WMT task, discriminative training is performed on the full training data of 4M sentence pairs, which is unsurpassed in the literature. 
Translation models often fail to generate good translations for infrequent words or phrases. Previous work attacked this problem by inducing new translation rules from monolingual data with a semi-supervised algorithm. However, this approach does not scale very well since it is very computationally expensive to generate new translation rules for only a few thousand sentences. We propose a much faster and simpler method that directly hallucinates translation rules for infrequent phrases based on phrases with similar continuous representations for which a translation is known. To speed up the retrieval of similar phrases, we investigate approximated nearest neighbor search with redundant bit vectors which we ﬁnd to be three times faster and signiﬁcantly more accurate than locality sensitive hashing. Our approach of learning new translation rules improves a phrase-based baseline by up to 1.6 BLEU on Arabic-English translation, it is three-orders of magnitudes faster than existing semi-supervised methods and 0.5 BLEU more accurate. 
This paper introduces a task of identifying and semantically classifying lexical expressions in running text. We investigate the online reviews genre, adding semantic supersense annotations to a 55,000 word English corpus that was previously annotated for multiword expressions. The noun and verb supersenses apply to full lexical expressions, whether single- or multiword. We then present a sequence tagging model that jointly infers lexical expressions and their supersenses. Results show that even with our relatively small training corpus in a noisy domain, the joint task can be performed to attain 70% class labeling F1. 
This paper proposes a novel approach to sentiment analysis that leverages work in sociology on symbolic interactionism. The proposed approach uses Affect Control Theory (ACT) to analyze readers’ sentiment towards factual (objective) content and towards its entities (subject and object). ACT is a theory of affective reasoning that uses empirically derived equations to predict the sentiments and emotions that arise from events. This theory relies on several large lexicons of words with affective ratings in a three-dimensional space of evaluation, potency, and activity (EPA). The equations and lexicons of ACT were evaluated on a newly collected news-headlines corpus. ACT lexicon was expanded using a label propagation algorithm, resulting in 86,604 new words. The predicted emotions for each news headline was then computed using the augmented lexicon and ACT equations. The results had a precision of 82%, 79%, and 68% towards the event, the subject, and object, respectively. These results are signiﬁcantly higher than those of standard sentiment analysis techniques. 
Most of the current approaches to sentiment analysis of product reviews are dependent on lexical sentiment information and proceed in a bottom-up way, adding new layers of features to lexical data. In this paper, we maintain that a typical product review is not a bag of sentiments, but a narrative with an underlying structure and reoccurring patterns, which allows us to predict its sentiments knowing only its general polarity and discourse cues that occur in it. We hypothesize that knowing only the review’s score and its discourse patterns would allow us to accurately predict the sentiments of its individual sentences. The experiments we conducted prove this hypothesis and show a substantial improvement over the lexical baseline. 
User-generated passwords tend to be memorable, but not secure. A random, computergenerated 60-bit string is much more secure. However, users cannot memorize random 60bit strings. In this paper, we investigate methods for converting arbitrary bit strings into English word sequences (both prose and poetry), and we study their memorability and other properties. 
Categories such as ANIMAL or FURNITURE are acquired at an early age and play an important role in processing, organizing, and conveying world knowledge. Theories of categorization largely agree that categories are characterized by features such as function or appearance and that feature and category acquisition go hand-in-hand, however previous work has considered these problems in isolation. We present the ﬁrst model that jointly learns categories and their features. The set of features is shared across categories, and strength of association is inferred in a Bayesian framework. We approximate the learning environment with natural language text which allows us to evaluate performance on a large scale. Compared to highly engineered pattern-based approaches, our model is cognitively motivated, knowledge-lean, and learns categories and features which are perceived by humans as more meaningful. 
If speakers use language rationally, they should structure their messages to achieve approximately uniform information density (UID), in order to optimize transmission via a noisy channel. Previous work identiﬁed a consistent increase in linguistic information across sentences in text as a signature of the UID hypothesis. This increase was derived from a predicted increase in context, but the context itself was not quantiﬁed. We use microblog texts from Twitter, tied to a single shared event (the baseball World Series), to quantify both linguistic and non-linguistic context. By tracking changes in contextual information, we predict and identify gradual and rapid changes in information content in response to in-game events. These ﬁndings lend further support to the UID hypothesis and highlights the importance of nonlinguistic common ground for language production and processing. 
Previous work has debated whether humans make use of hierarchic syntax when processing language (Frank and Bod, 2011; Fossum and Levy, 2012). This paper uses an eye-tracking corpus to demonstrate that hierarchic syntax signiﬁcantly improves reading time prediction over a strong n-gram baseline. This study shows that an interpolated 5-gram baseline can be made stronger by combining n-gram statistics over entire eye-tracking regions rather than simply using the last n-gram in each region, but basic hierarchic syntactic measures are still able to achieve signiﬁcant improvements over this improved baseline. 
Vector space word representations are learned from distributional information of words in large corpora. Although such statistics are semantically informative, they disregard the valuable information that is contained in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a method for reﬁning vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our reﬁnement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 
We present an unsupervised model for inducing signed social networks from the content exchanged across network edges. Inference in this model solves three problems simultaneously: (1) identifying the sign of each edge; (2) characterizing the distribution over content for each edge type; (3) estimating weights for triadic features that map to theoretical models such as structural balance. We apply this model to the problem of inducing the social function of address terms, such as Madame, comrade, and dude. On a dataset of movie scripts, our system obtains a coherent clustering of address terms, while at the same time making intuitively plausible judgments of the formality of social relations in each ﬁlm. As an additional contribution, we provide a bootstrapping technique for identifying and tagging address terms in dialogue.1 
We present a language agnostic, unsupervised method for inducing morphological transformations between words. The method relies on certain regularities manifest in highdimensional vector spaces. We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers. We evaluate this method across six different languages and nine datasets, and show signiﬁcant improvements across all languages. 
 In this paper, we present a machine learn-  ing approach for identifying shallow dis-  course relations in news wire text. Our  approach has 2 phases. The arguments  detection phase will identify arguments  and explicit connectives by using the  Conditional  Random  Fields  (CRFs) learning algorithm with a set of  features such as words, parts of speech  (POS) and features extracted from the  parsing tree of sentences. The second  phase, the sense classification phase, will  classify arguments and explicit connec-  tives into one of fifteen types of senses  by using the SMO classifier with a sim-  ple feature set. The performance of sys-  tem was evaluated three different data  sets given by the CoNLL 2015 Shared  Task. The parser of our system was  ranked 4 of 16 participating systems on  F-measure when evaluating on the blind  data set (strict matching).  
We present a system that implements an end-to-end discourse parser. The system uses a pipeline architecture with seven stages: preprocessing, recognizing explicit connectives, identifying argument positions, identifying and labeling arguments, classifying explicit and implicit connectives, and identifying attribution structures. The discourse structure of a document is inferred based on these components. For NLP analysis, we use Illinois NLP software1 and the Stanford Parser. We use lexical and semantic features based on function words, sentiment lexicons, brown clusters, and polarity features. Our system achieves an F1 score of 0.2492 in overall performance on the development set and 0.1798 on the blind test set. 
We present a theoretical analysis of online parameter tuning in statistical machine translation (SMT) from a coactive learning view. This perspective allows us to give regret and generalization bounds for latent perceptron algorithms that are common in SMT, but fall outside of the standard convex optimization scenario. Coactive learning also introduces the concept of weak feedback, which we apply in a proofof-concept experiment to SMT, showing that learning from feedback that consists of slight improvements over predictions leads to convergence in regret and translation error rate. This suggests that coactive learning might be a viable framework for interactive machine translation. Furthermore, we ﬁnd that surrogate translations replacing references that are unreachable in the decoder search space can be interpreted as weak feedback and lead to convergence in learning, if they admit an underlying linear model. 
In coreference resolution, a fair amount of research treats mention detection as a preprocessed step and focuses on developing algorithms for clustering coreferred mentions. However, there are signiﬁcant gaps between the performance on gold mentions and the performance on the real problem, when mentions are predicted from raw text via an imperfect Mention Detection (MD) module. Motivated by the goal of reducing such gaps, we develop an ILP-based joint coreference resolution and mention head formulation that is shown to yield signiﬁcant improvements on coreference from raw text, outperforming existing state-ofart systems on both the ACE-2004 and the CoNLL-2012 datasets. At the same time, our joint approach is shown to improve mention detection by close to 15% F1. One key insight underlying our approach is that identifying and co-referring mention heads is not only sufﬁcient but is more robust than working with complete mentions. 
Combinatory Categorial Grammar (CCG) is a lexicalized grammar formalism in which words are associated with categories that specify the syntactic conﬁgurations in which they may occur. We present a novel parsing model with the capacity to capture the associative adjacent-category relationships intrinsic to CCG by parameterizing the relationships between each constituent label and the preterminal categories directly to its left and right, biasing the model toward constituent categories that can combine with their contexts. This builds on the intuitions of Klein and Manning’s (2002) “constituentcontext” model, which demonstrated the value of modeling context, but has the advantage of being able to exploit the properties of CCG. Our experiments show that our model outperforms a baseline in which this context information is not captured. 
This paper presents a synchronous-graphgrammar-based approach for string-toAMR parsing. We apply Markov Chain Monte Carlo (MCMC) algorithms to learn Synchronous Hyperedge Replacement Grammar (SHRG) rules from a forest that represents likely derivations consistent with a ﬁxed string-to-graph alignment. We make an analogy of string-toAMR parsing to the task of phrase-based machine translation and come up with an efﬁcient algorithm to learn graph grammars from string-graph pairs. We propose an effective approximation strategy to resolve the complexity issue of graph compositions. We also show some useful strategies to overcome existing problems in an SHRG-based parser and present preliminary results of a graph-grammar-based approach. 
In this paper, we present a hybrid approach for performing token and sentence levels Dialect Identiﬁcation in Arabic. Speciﬁcally we try to identify whether each token in a given sentence belongs to Modern Standard Arabic (MSA), Egyptian Dialectal Arabic (EDA) or some other class and whether the whole sentence is mostly EDA or MSA. The token level component relies on a Conditional Random Field (CRF) classiﬁer that uses decisions from several underlying components such as language models, a named entity recognizer and and a morphological analyzer to label each word in the sentence. The sentence level component uses a classiﬁer ensemble system that relies on two independent underlying classiﬁers that model different aspects of the language. Using a featureselection heuristic, we select the best set of features for each of these two classiﬁers. We then train another classiﬁer that uses the class labels and the conﬁdence scores generated by each of the two underlying classiﬁers to decide upon the ﬁnal class for each sentence. The token level component yields a new state of the art F-score of 90.6% (compared to previous state of the art of 86.8%) and the sentence level component yields an accuracy of 90.8% (compared to 86.6% obtained by the best state of the art system). 
Supervised machine learning classiﬁcation algorithms assume both train and test data are sampled from the same domain or distribution. However, performance of the algorithms degrade for test data from different domain. Such cross domain classiﬁcation is arduous as features in the test domain may be different and absence of labeled data could further exacerbate the problem. This paper proposes an algorithm to adapt classiﬁcation model by iteratively learning domain speciﬁc features from the unlabeled test data. Moreover, this adaptation transpires in a similarity aware manner by integrating similarity between domains in the adaptation setting. Cross-domain classiﬁcation experiments on different datasets, including a real world dataset, demonstrate efﬁcacy of the proposed algorithm over state-of-theart. 
We study the impact of source length and verbosity of the tuning dataset on the performance of parameter optimizers such as MERT and PRO for statistical machine translation. In particular, we test whether the verbosity of the resulting translations can be modiﬁed by varying the length or the verbosity of the tuning sentences. We ﬁnd that MERT learns the tuning set verbosity very well, while PRO is sensitive to both the verbosity and the length of the source sentences in the tuning set; yet, overall PRO learns best from highverbosity tuning datasets. Given these dependencies, and potentially some other such as amount of reordering, number of unknown words, syntactic complexity, and evaluation measure, to mention just a few, we argue for the need of controlled evaluation scenarios, so that the selection of tuning set and optimization strategy does not overshadow scientiﬁc advances in modeling or decoding. In the mean time, until we develop such controlled scenarios, we recommend using PRO with a large verbosity tuning set, which, in our experiments, yields highest BLEU across datasets and language pairs. 
Cross-lingual dependency parsing aims to train a dependency parser for an annotation-scarce target language by exploiting annotated training data from an annotation-rich source language, which is of great importance in the ﬁeld of natural language processing. In this paper, we propose to address cross-lingual dependency parsing by inducing latent crosslingual data representations via matrix completion and annotation projections on a large amount of unlabeled parallel sentences. To evaluate the proposed learning technique, we conduct experiments on a set of cross-lingual dependency parsing tasks with nine different languages. The experimental results demonstrate the efﬁcacy of the proposed learning method for cross-lingual dependency parsing. 
This work examines the impact of crosslinguistic transfer on grammatical errors in English as Second Language (ESL) texts. Using a computational framework that formalizes the theory of Contrastive Analysis (CA), we demonstrate that language speciﬁc error distributions in ESL writing can be predicted from the typological properties of the native language and their relation to the typology of English. Our typology driven model enables to obtain accurate estimates of such distributions without access to any ESL data for the target languages. Furthermore, we present a strategy for adjusting our method to low-resource languages that lack typological documentation using a bootstrapping approach which approximates native language typology from ESL texts. Finally, we show that our framework is instrumental for linguistic inquiry seeking to identify ﬁrst language factors that contribute to a wide range of difﬁculties in second language acquisition. 
Most computational sociolinguistics studies have focused on phonological and lexical variation. We present the ﬁrst large-scale study of syntactic variation among demographic groups (age and gender) across several languages. We harvest data from online user-review sites and parse it with universal dependencies. We show that several age and gender-speciﬁc variations hold across languages, for example that women are more likely to use VP conjunctions. 
Cross-lingual transfer has been shown to produce good results for dependency parsing of resource-poor languages. Although this avoids the need for a target language treebank, most approaches have still used large parallel corpora. However, parallel data is scarce for low-resource languages, and we report a new method that does not need parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We show empirical improvements over a baseline delexicalised parser on both the CoNLL and Universal Dependency Treebank datasets. We analyse the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 
Two questions asking the same thing could be too different in terms of vocabulary and syntactic structure, which makes identifying their semantic equivalence challenging. This study aims to detect semantically equivalent questions in online user forums. We perform an extensive number of experiments using data from two different Stack Exchange forums. We compare standard machine learning methods such as Support Vector Machines (SVM) with a convolutional neural network (CNN). The proposed CNN generates distributed vector representations for pairs of questions and scores them using a similarity metric. We evaluate in-domain word embeddings versus the ones trained with Wikipedia, estimate the impact of the training set size, and evaluate some aspects of domain adaptation. Our experimental results show that the convolutional neural network with in-domain word embeddings achieves high performance even with limited training data. 
Although entity linking is a widely researched topic, the same cannot be said for entity linking geared for languages other than English. Several limitations including syntactic features and the relative lack of resources prevent typical approaches to entity linking to be used as eﬀectively for other languages in general. We describe an entity linking system that leverage semantic relations between entities within an existing knowledge base to learn and perform entity linking using a minimal environment consisting of a part-of-speech tagger. We measure the performance of our system against Korean Wikipedia abstract snippets, using the Korean DBpedia knowledge base for training. Based on these results, we argue both the feasibility of our system and the possibility of extending to other domains and languages in general. 
We propose a discriminatively trained recurrent neural network (RNN) that predicts the actions for a fast and accurate shift-reduce dependency parser. The RNN uses its output-dependent model structure to compute hidden vectors that encode the preceding partial parse, and uses them to estimate probabilities of parser actions. Unlike a similar previous generative model (Henderson and Titov, 2010), the RNN is trained discriminatively to optimize a fast beam search. This beam search prunes after each shift action, so we add a correctness probability to each shift action and train this score to discriminate between correct and incorrect sequences of parser actions. We also speed up parsing time by caching computations for frequent feature combinations, including during training, giving us both faster training and a form of backoff smoothing. The resulting parser is over 35 times faster than its generative counterpart with nearly the same accuracy, producing state-of-art dependency parsing results while requiring minimal feature engineering. 
Scarcity of annotated corpora for many languages is a bottleneck for training ﬁnegrained sentiment analysis models that can tag aspects and subjective phrases. We propose to exploit statistical machine translation to alleviate the need for training data by projecting annotated data in a source language to a target language such that a supervised ﬁne-grained sentiment analysis system can be trained. To avoid a negative inﬂuence of poor-quality translations, we propose a ﬁltering approach based on machine translation quality estimation measures to select only high-quality sentence pairs for projection. We evaluate on the language pair German/English on a corpus of product reviews annotated for both languages and compare to in-target-language training. Projection without any ﬁltering leads to 23 % F1 in the task of detecting aspect phrases, compared to 41 % F1 for in-target-language training. Our approach obtains up to 47 % F1. Further, we show that the detection of subjective phrases is competitive to in-target-language training without ﬁltering. 
We present labeled morphological segmentation—an alternative view of morphological processing that uniﬁes several tasks. We introduce a new hierarchy of morphotactic tagsets and CHIPMUNK, a discriminative morphological segmentation system that, contrary to previous work, explicitly models morphotactics. We show improved performance on three tasks for all six languages: (i) morphological segmentation, (ii) stemming and (iii) morphological tag classiﬁcation. For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline. 
Massive knowledge resources, such as Wikidata, can provide valuable information for lexical inference, especially for proper-names. Prior resource-based approaches typically select the subset of each resource’s relations which are relevant for a particular given task. The selection process is done manually, limiting these approaches to smaller resources such as WordNet, which lacks coverage of propernames and recent terminology. This paper presents a supervised framework for automatically selecting an optimized subset of resource relations for a given target inference task. Our approach enables the use of large-scale knowledge resources, thus providing a rich source of high-precision inferences over proper-names.1 
This paper describes a set of methods to link entities across images and text. As a corpus, we used a data set of images, where each image is commented by a short caption and where the regions in the images are manually segmented and labeled with a category. We extracted the entity mentions from the captions and we computed a semantic similarity between the mentions and the region labels. We also measured the statistical associations between these mentions and the labels and we combined them with the semantic similarity to produce mappings in the form of pairs consisting of a region label and a caption entity. In a second step, we used the syntactic relationships between the mentions and the spatial relationships between the regions to rerank the lists of candidate mappings. To evaluate our methods, we annotated a test set of 200 images, where we manually linked the image regions to their corresponding mentions in the captions. Eventually, we could match objects in pictures to their correct mentions for nearly 89 percent of the segments, when such a matching exists. 
Corpus labeling projects frequently use low-cost workers from microtask marketplaces; however, these workers are often inexperienced or have misaligned incentives. Crowdsourcing models must be robust to the resulting systematic and nonsystematic inaccuracies. We introduce a novel crowdsourcing model that adapts the discrete supervised topic model sLDA to handle multiple corrupt, usually conﬂicting (hence “confused”) supervision signals. Our model achieves signiﬁcant gains over previous work in the accuracy of deduced ground truth. 
We propose MVCNN, a convolution neural network (CNN) architecture for sentence classiﬁcation. It (i) combines diverse versions of pretrained word embeddings and (ii) extracts features of multigranular phrases with variable-size convolution ﬁlters. We also show that pretraining MVCNN is critical for good performance. MVCNN achieves state-of-the-art performance on four tasks: on small-scale binary, small-scale multi-class and largescale Twitter sentiment prediction and on subjectivity classiﬁcation. 
We present an approach for opinion role induction for verbal predicates. Our model rests on the assumption that opinion verbs can be divided into three different types where each type is associated with a characteristic mapping between semantic roles and opinion holders and targets. In several experiments, we demonstrate the relevance of those three categories for the task. We show that verbs can easily be categorized with semi-supervised graphbased clustering and some appropriate similarity metric. The seeds are obtained through linguistic diagnostics. We evaluate our approach against a new manuallycompiled opinion role lexicon and perform in-context classiﬁcation. 
Typically, visually-grounded language learning systems only accept feature data about objects in the environment that are explicitly mentioned, whether through annotation labels or direct reference through natural language. We show that when objects are described ambiguously using natural language, a system can use a combination of the pragmatic principles of Contrast and Conventionality, and multiple-instance learning to learn from ambiguous examples in an online fashion. Applying child language learning strategies to visual learning enables more effective learning in real-time environments, which can lead to enhanced teaching interactions with robots or grounded systems in multi-object environments. 
Software system development is guided by the evolution of requirements. In this paper, we address the task of requirements traceability, which is concerned with providing bi-directional traceability between various requirements, enabling users to ﬁnd the origin of each requirement and track every change made to it. We propose a knowledge-rich approach to the task, where we extend a supervised baseline system with (1) additional training instances derived from human-provided annotator rationales; and (2) additional features derived from a hand-built ontology. Experiments demonstrate that our approach yields a relative error reduction of 11.1–19.7%. 
 One of the most common features across all known languages is their variability in word order. We show that differences in the prenominal and postnominal placement of adjectives in the noun phrase across ﬁve main Romance languages is not only subject to heaviness effects, as previously reported, but also to subtler structural interactions among dependencies that are better explained as effects of the principle of dependency length minimisation. These effects are almost purely structural and show lexical conditioning only in highly frequent collocations. 
We present a novel word level vector representation based on symmetric patterns (SPs). For this aim we automatically acquire SPs (e.g., “X and Y”) from a large corpus of plain text, and generate vectors where each coordinate represents the cooccurrence in SPs of the represented word with another word of the vocabulary. Our representation has three advantages over existing alternatives: First, being based on symmetric word relationships, it is highly suitable for word similarity prediction. Particularly, on the SimLex999 word similarity dataset, our model achieves a Spearman’s ρ score of 0.517, compared to 0.462 of the state-of-the-art word2vec model. Interestingly, our model performs exceptionally well on verbs, outperforming stateof-the-art baselines by 20.2–41.5%. Second, pattern features can be adapted to the needs of a target NLP application. For example, we show that we can easily control whether the embeddings derived from SPs deem antonym pairs (e.g. (big,small)) as similar or dissimilar, an important distinction for tasks such as word classiﬁcation and sentiment analysis. Finally, we show that a simple combination of the word similarity scores generated by our method and by word2vec results in a superior predictive power over that of each individual model, scoring as high as 0.563 in Spearman’s ρ on SimLex999. This emphasizes the differences between the signals captured by each of the models. 
We present a novel learning method for word embeddings designed for relation classiﬁcation. Our word embeddings are trained by predicting words between noun pairs using lexical relation-speciﬁc features on a large unlabeled corpus. This allows us to explicitly incorporate relationspeciﬁc information into the word embeddings. The learned word embeddings are then used to construct feature vectors for a relation classiﬁcation model. On a wellestablished semantic relation classiﬁcation task, our method signiﬁcantly outperforms a baseline based on a previously introduced word embedding method, and compares favorably to previous state-of-the-art models that use syntactic information or manually constructed external resources. 
As documents tend to contain temporal information, extracting such information is attracting much research interests recently. In this paper, we propose a hybrid method that combines machine-learning models and hand-crafted rules for the task of extracting temporal information from unstructured Korean texts. We address Korean-speciﬁc research issues and propose a new probabilistic model to generate complementary features. The performance of our approach is demonstrated by experiments on the TempEval-2 dataset, and the Korean TimeBank dataset which we built for this study. 
We present a transition-based arc-eager model to parse spinal trees, a dependencybased representation that includes phrasestructure information in the form of constituent spines assigned to tokens. As a main advantage, the arc-eager model can use a rich set of features combining dependency and constituent information, while parsing in linear time. We describe a set of conditions for the arc-eager system to produce valid spinal structures. In experiments using beam search we show that the model obtains a good trade-off between speed and accuracy, and yields state of the art performance for both dependency and constituent parsing measures. 
We propose a method that learns a crosslingual projection of word representations from one language into another. Our method utilizes translatable context pairs as bonus terms of the objective function. In the experiments, our method outperformed existing methods in three language pairs, (English, Spanish), (Japanese, Chinese) and (English, Japanese), without using any additional supervisions. 
Neural language models (NLMs) have been able to improve machine translation (MT) thanks to their ability to generalize well to long contexts. Despite recent successes of deep neural networks in speech and vision, the general practice in MT is to incorporate NLMs with only one or two hidden layers and there have not been clear results on whether having more layers helps. In this paper, we demonstrate that deep NLMs with three or four layers outperform those with fewer layers in terms of both the perplexity and the translation quality. We combine various techniques to successfully train deep NLMs that jointly condition on both the source and target contexts. When reranking nbest lists of a strong web-forum baseline, our deep models yield an average boost of 0.5 TER / 0.5 BLEU points compared to using a shallow NLM. Additionally, we adapt our models to a new sms-chat domain and obtain a similar gain of 1.0 TER / 0.5 BLEU points.1 
The emergence of user forums in electronic news media has given rise to the proliferation of opinion manipulation trolls. Finding such trolls automatically is a hard task, as there is no easy way to recognize or even to deﬁne what they are; this also makes it hard to get training and testing data. We solve this issue pragmatically: we assume that a user who is called a troll by several people is likely to be one. We experiment with different variations of this deﬁnition, and in each case we show that we can train a classiﬁer to distinguish a likely troll from a non-troll with very high accuracy, 82–95%, thanks to our rich feature set. 
Using automatic measures such as labeled and unlabeled attachment scores is common practice in dependency parser evaluation. In this paper, we examine whether these measures correlate with human judgments of overall parse quality. We ask linguists with experience in dependency annotation to judge system outputs. We measure the correlation between their judgments and a range of parse evaluation metrics across ﬁve languages. The humanmetric correlation is lower for dependency parsing than for other NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 
Text classiﬁcation tasks suffer from curse of dimensionality due to large feature space. Short text data further exacerbates the problem due to their sparse and noisy nature. Feature selection thus becomes an important step in improving the classiﬁcation performance. In this paper, we propose a novel feature selection method using Wavelet Packet Transform. Wavelet Packet Transform (WPT) has been used widely in various ﬁelds due to its efﬁciency in encoding transient signals. We demonstrate how short text classiﬁcation task can be beneﬁted by feature selection using WPT due to their sparse nature. Our technique chooses the most discriminating features by computing inter-class distances in the transformed space. We experimented extensively with several short text datasets. Compared to well known techniques our approach reduces the feature space size and improves the overall classiﬁcation performance signiﬁcantly in all the datasets. 
We present a compositional distributional semantic model which is an implementation of the tensor-based framework of Coecke et al. (2011). It is an extended skipgram model (Mikolov et al., 2013) which we apply to adjective-noun combinations, learning nouns as vectors and adjectives as matrices. We also propose a novel measure of adjective similarity, and show that adjective matrix representations lead to improved performance in adjective and adjective-noun similarity tasks, as well as in the detection of semantically anomalous adjective-noun pairs. 
Model selection (picking, for example, the feature set and the regularization strength) is crucial for building high-accuracy NLP models. In supervised learning, we can estimate the accuracy of a model on a subset of the labeled data and choose the model with the highest accuracy. In contrast, here we focus on type-supervised learning, which uses constraints over the possible labels for word types for supervision, and labeled data is either not available or very small. For the setting where no labeled data is available, we perform a comparative study of previously proposed and one novel model selection criterion on type-supervised POS-tagging in nine languages. For the setting where a small labeled set is available, we show that the set should be used for semi-supervised learning rather than for model selection only – using it for model selection reduces the error by less than 5%, whereas using it for semi-supervised learning reduces the error by 44%. 
Supervised word sense disambiguation (WSD) systems are usually the best performing systems when evaluated on standard benchmarks. However, these systems need annotated training data to function properly. While there are some publicly available open source WSD systems, very few large annotated datasets are available to the research community. The two main goals of this paper are to extract and annotate a large number of samples and release them for public use, and also to evaluate this dataset against some word sense disambiguation and induction tasks. We show that the open source IMS WSD system trained on our dataset achieves stateof-the-art results in standard disambiguation tasks and a recent word sense induction task, outperforming several task submissions and strong baselines. 
It is well-known that readers are less likely to ﬁxate their gaze on closed class syntactic categories such as prepositions and pronouns. This paper investigates to what extent the syntactic category of a word in context can be predicted from gaze features obtained using eye-tracking equipment. If syntax can be reliably predicted from eye movements of readers, it can speed up linguistic annotation substantially, since reading is considerably faster than doing linguistic annotation by hand. Our results show that gaze features do discriminate between most pairs of syntactic categories, and we show how we can use this to annotate words with part of speech across domains, when tag dictionaries enable us to narrow down the set of potential categories. 
Among the more recent applications for natural language processing algorithms has been the analysis of spoken language data for diagnostic and remedial purposes, fueled by the demand for simple, objective, and unobtrusive screening tools for neurological disorders such as dementia. The automated analysis of narrative retellings in particular shows potential as a component of such a screening tool since the ability to produce accurate and meaningful narratives is noticeably impaired in individuals with dementia and its frequent precursor, mild cognitive impairment, as well as other neurodegenerative and neurodevelopmental disorders. In this article, we present a method for extracting narrative recall scores automatically and highly accurately from a word-level alignment between a retelling and the source narrative. We propose improvements to existing machine translation-based systems for word alignment, including a novel method of word alignment relying on random walks on a graph that achieves alignment accuracy superior to that of standard expectation maximization-based techniques for word alignment in a fraction of the time required for expectation maximization. In addition, the narrative recall score features extracted from these high-quality word alignments yield diagnostic classiﬁcation accuracy comparable to that achieved using manually assigned scores and signiﬁcantly higher than that achieved with summary-level text similarity metrics used in other areas of NLP. These methods can be trivially adapted to spontaneous language samples elicited with non-linguistic stimuli, thereby demonstrating the ﬂexibility and generalizability of these methods. 1. Introduction Interest in applying natural language processing (NLP) technology to medical information has increased in recent years. Much of this work has been focused on information retrieval and extraction from clinical notes, electronic medical records, and biomedical academic literature, but there has been some work in directly analyzing the spoken language of individuals elicited during the administration of diagnostic instruments in clinical settings. Analyzing spoken language data can reveal information not only ∗ Rochester Institute of Technology, College of Liberal Arts, 92 Lomb Memorial Dr., Rochester, NY 14623. E-mail: emilypx@rit.edu. ∗∗ Google, Inc., 1001 SW Fifth Avenue, Suite 1100, Portland OR 97204. E-mail: roarkbr@gmail.com. Submission received: 30 December 2013; revised submission received: 21 January 2015; accepted for publication: 4 May 2015. doi:10.1162/COLI a 00232 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 4  about impairments in language but also about a patient’s neurological status with respect to other cognitive processes such as memory and executive function, which are often impaired in individuals with neurodevelopmental disorders, such as autism and language impairment, and neurodegenerative conditions, particularly dementia. Many widely used instruments for diagnosing certain neurological disorders include a task in which the person must produce an uninterrupted stream of spontaneous spoken language in response to a stimulus. A person might be asked, for instance, to retell a brief narrative or to describe the events depicted in a drawing. Much of the previous work in applying NLP techniques to such clinically elicited spoken language data has relied on parsing and language modeling to enable the automatic extraction of linguistic features, such as syntactic complexity and measures of vocabulary use and diversity, which can then be used as markers for various neurological impairments (Solorio and Liu 2008; Gabani et al. 2009; Roark et al. 2011; de la Rosa et al. 2013; Fraser et al. 2014). In this article, we instead use NLP techniques to analyze the content, rather than the linguistic characteristics, of weakly structured spoken language data elicited using neuropsychological assessment instruments. We will show that the content of such spoken responses contains information that can be used for accurate screening for neurodegenerative disorders. The features we explore are grounded in the idea that individuals recalling the same narrative are likely to use the same sorts of words and semantic concepts. In other words, a retelling of a narrative will be faithful to the source narrative and similar to other retellings. This similarity can be measured with techniques such as latent semantic analysis (LSA) cosine distance or the summary-level statistics that are widely used in evaluation of machine translation or automatic summarization, such as BLEU, Meteor, or ROUGE. Perhaps not surprisingly, however, previous work in using this type of spoken language data suggests that people with neurological impairments tend to include irrelevant or off-topic information and to exclude important pieces of information, or story elements, in their retellings that are usually included by neurotypical individuals (Hier, Hagenlocker, and Shindler 1985; Ulatowska et al. 1988; Chenery and Murdoch 1994; Chapman et al. 1995; Ehrlich, Obler, and Clark 1997; Vuorinen, Laine, and Rinne 2000; Creamer and Schmitter-Edgecombe 2010). Thus, it is often not the quantity of correctly recalled information but the quality of that information that reveals the most about a person’s diagnostic status. Summary statistics like LSA cosine distance and BLEU, which are measures of the overall degree of similarity between two texts, fail to capture these sorts of patterns. The work discussed here is an attempt to reveal these patterns and to leverage them for diagnostic classiﬁcation of individuals with neurodegenerative conditions, including mild cognitive impairment and dementia of the Alzheimer’s type. Our method for extracting the elements used in a retelling of a narrative relies on establishing a word alignment between a retelling and a source narrative. Given the correspondences between the words used in a retelling and the words used in the source narrative, we can determine with relative ease the identities of the story elements of the source narrative that were used in the retelling. These word alignments are much like those used to build machine translation models. The amount of data required to generate accurate word alignment models for machine translation, however, far exceeds the amount of monolingual source-to-retelling parallel data available to train word alignment models for our task. We therefore combine several approaches for producing reliable word alignments that exploit the peculiarities of our training data, including an entirely novel alignment approach relying on random walks on graphs. 550  Prud’hommeaux and Roark Graph-Based Word Alignment for Clinical Language Evaluation  Table 1 Abbreviations used in this article.  AER AUC BDAE CDR MCI MMSE WLM  alignment error rate area under the (receiver operating characteristic) curve Boston Diagnostic Aphasia Exam Clinical Dementia Rating mild cognitive impairment Mini-Mental State Exam Wechsler Logical Memory narrative recall subtest  In this article, we demonstrate that this approach to word alignment is as accurate as and more efﬁcient than standard hidden Markov model (HMM)-based alignment (derived using the Berkeley aligner [Liang, Taskar, and Klein 2006]) for this particular data. In addition, we show that the presence or absence of speciﬁc story elements in a narrative retelling, extracted automatically from these task-speciﬁc word alignments, predicts diagnostic group membership more reliably than not only other dementia screening tools but also the lexical and semantic overlap measures widely used in NLP to evaluate pairwise language sample similarity. Finally, we apply our techniques to a picture description task that lacks an existing scoring mechanism, highlighting the generalizability and adaptability of these techniques. The importance of accurate screening tools for neurodegenerative disorders cannot be overstated given the increased prevalence of these disorders currently being observed worldwide. In the industrialized world, for the ﬁrst time in recorded history, the population over 60 years of age outnumbers the population under 15 years of age, and it is expected to be double that of children by 2050 (United Nations 2002). As the elderly population grows and as researchers ﬁnd new ways to slow or halt the progression of dementia, the demand for objective, simple, and noninvasive screening tools for dementia and related disorders will grow. Although we will not discuss the application of our methods to the narratives of children, the need for simple screening protocols for neurodevelopmental disorders such as autism and language impairment is equally urgent. The results presented here indicate that the path toward these goals might include automated spoken language analysis. 2. Background 2.1 Mild Cognitive Impairment Because of the variety of intact cognitive functions required to generate a narrative, the inability to coherently produce or recall a narrative is associated with many different disorders, including not only neurodegenerative conditions related to dementia, but also autism (Tager-Flusberg 1995; Diehl, Bennetto, and Young 2006), language impairment (Norbury and Bishop 2003; Bishop and Donlan 2005), attention deﬁcit disorder (Tannock, Purvis, and Schachar 1993), and schizophrenia (Lysaker et al. 2003). The bulk of the research presented here, however, focuses on the utility of a particular narrative recall task, the Wechsler Logical Memory subtest of the Wechsler Memory Scale (Wechsler 1997), for diagnosing mild cognitive impairment (MCI). (This and other abbreviations are listed in Table 1.) 551  Computational Linguistics  Volume 41, Number 4  MCI is the stage of cognitive decline between the sort of decline expected in typical aging and the decline associated with dementia or Alzheimer’s disease (Petersen et al. 1999; Ritchie and Touchon 2000; Petersen 2011). MCI is characterized by subtle deﬁcits in functions of memory and cognition that are clinically signiﬁcant but do not prevent carrying out the activities of daily life. This intermediary phase of decline has been identiﬁed and named numerous times: mild cognitive decline, mild neurocognitive decline, very mild dementia, isolated memory impairment, questionable dementia, and incipient dementia. Although there continues to be disagreement about the diagnostic validity of the designation (Ritchie and Touchon 2000; Ritchie, Artero, and Touchon 2001), a number of recent studies have found evidence that seniors with some subtypes of MCI are signiﬁcantly more likely to develop dementia than the population as a whole (Busse et al. 2006; Manly et al. 2008; Plassman et al. 2008). Early detection can beneﬁt both patients and researchers investigating treatments for halting or slowing the progression of dementia, but identifying MCI can be problematic, as most dementia screening instruments, such as the Mini-Mental State Exam (MMSE) (Folstein, Folstein, and McHugh 1975), lack sufﬁcient sensitivity to the very subtle cognitive deﬁcits that characterize the disorder (Morris et al. 2001; Ravaglia et al. 2005; Hoops et al. 2009). Diagnosis of MCI currently requires both a lengthy neuropsychological evaluation of the patient and an interview with a family member or close associate, both of which should be repeated at regular intervals in order to have a baseline for future comparison. One goal of the work presented here is to determine whether an analysis of spoken language responses to a narrative recall task, the Wechsler Logical Memory subtest, can be used as a more efﬁcient and less intrusive screening tool for MCI.  2.2 Wechsler Logical Memory Subtest In the Wechsler Logical Memory (WLM) narrative recall subtest of the Wechsler Memory Scale, the individual listens to a brief narrative and must verbally retell the narrative to the examiner once immediately upon hearing the story and again after a delay of 20 to 30 minutes. The examiner scores each retelling according to how many story elements the patient uses in the retelling. The standard scoring procedure, described in more detail in Section 3.2, results in a single summary score for each retelling, immediate and delayed, corresponding to the total number of story elements recalled in that retelling. The Anna Thompson narrative, shown in Figure 1 (later in this article), has been used as the primary WLM narrative for over 70 years and has been found to be sensitive to dementia and related conditions, particularly in combination with tests of verbal ﬂuency and memory. Multiple studies have demonstrated a signiﬁcant difference in performance on the WLM between individuals with MCI and typically aging controls under the standard scoring procedure (Storandt and Hill 1989; Petersen et al. 1999; Wang and Zhou 2002; Nordlund et al. 2005). Further studies have shown that performance on the WLM can help predict whether MCI will progress into Alzheimer’s disease (Morris et al. 2001; Artero et al. 2003; Tierney et al. 2005). The WLM can also serve as a cognitive indicator of physiological characteristics associated with Alzheimer’s disease. WLM scores in the impaired range are associated with the presence of changes in Pittsburgh compound B and cerebrospinal ﬂuid amyloid beta protein, two biomarkers of Alzheimer’s disease (Galvin et al. 2010). Poor performance on the WLM and other narrative memory tests has also been strongly correlated with increased density 552  Prud’hommeaux and Roark Graph-Based Word Alignment for Clinical Language Evaluation of Alzheimer related lesions detected in postmortem neuropathological studies, even in the absence of previously reported or detected dementia (Schmitt et al. 2000; Bennett et al. 2006; Price et al. 2009). We note that clinicians do not use the WLM as a diagnostic test by itself for MCI or any other type of dementia. The WLM summary score is just one of a large number of instrumentally derived scores of memory and cognitive function that, in combination with one another and with a clinician’s expert observations and examination, can indicate the presence of a dementia, aphasia, or other neurological disorder. 2.3 Previous Work Much of the previous work in applying automated analysis of unannotated transcripts of narratives for diagnostic purposes has focused not on evaluating properties speciﬁc to narratives but rather on using narratives as a data source from which to extract speech and language features. Solorio and Liu (2008) were able to distinguish the narratives of a small set of children with speciﬁc language impairment (SLI) from those of typically developing children using perplexity scores derived from part-ofspeech language models. In a follow-up study on a larger group of children, Gabani et al. (2009) again used part-of-speech language models in an attempt to characterize the agrammaticality that is associated with language impairment. Two part-of-speech language models were trained for that experiment: one on the language of children with SLI and one on the language of typically developing children. The perplexity of each child’s utterances was calculated according to each of the models. In addition, the authors extracted a number of other structural linguistic features including mean length of utterance, total words used in the narrative, and measures of accurate subject–verb agreement. These scores collectively performed well in distinguishing children with language impairment, achieving an F1 measure of just over 70% when used within a support vector machine (SVM) for classiﬁcation. In a continuation of this work, de la Rosa et al. (2013) explored complex language-model-based lexical and syntactic features to more accurately characterize the language used in narratives by children with language impairment. Roark et al. (2011) extracted a subset of the features used by Gabani et al. (2009), along with a much larger set of language complexity features derived from syntactic parse trees for utterances from narratives produced by elderly individuals for the diagnosis of MCI. These features included simple measures, such as words per clause, and more complex measures of tree depth, embedding, and branching, such as Frazier and Yngve scores. Selecting a subset of these features for classiﬁcation with an SVM yielded a classiﬁcation accuracy of 0.73, as measured by the area under the receiver operating characteristic curve (AUC). A similar approach was followed by Fraser et al. (2014) to distinguish different types of primary progressive aphasia, a group of subtypes of dementia distinct from Alzheimer’s disease and MCI, in a small group of elderly individuals. The authors considered almost 60 linguistic features, including some of those explored by Roark et al. (2011) as well as numerous others relating to part-of-speech frequencies and ratios. Using a variety of classiﬁers and feature combinations for three different two-way classiﬁcation tasks, the authors achieved classiﬁcation accuracies ranging between 0.71 and 1.0. An alternative to analyzing narratives in terms of syntactic and lexical features is to evaluate the content of the narrative retellings themselves in terms of their ﬁdelity to the source narrative. Hakkani-Tur, Vergyri, and Tur (2010) developed a method of 553  Computational Linguistics  Volume 41, Number 4  automatically evaluating an audio recording of a picture description task, in which the patient looks at a picture and narrates the events occurring in the picture, similar to the task we will be analyzing in Section 8. After using automatic speech recognition (ASR) to transcribe the recording, the authors measured unigram overlap between the ASR output transcript and a predeﬁned list of key semantic concepts. This unigram overlap measure correlated highly with manually assigned counts of these semantic concepts. The authors did not investigate whether the scores, derived either manually or automatically, were associated with any particular diagnostic group or disorder. Dunn et al. (2002) were among the ﬁrst to apply automated methods speciﬁcally to scoring the WLM subtest and determining the relationship between these scores and measures of cognitive function. The authors used Latent Semantic Analysis (LSA) to measure the semantic distance from a retelling to the source narrative. The LSA scores correlated very highly with the scores assigned by examiners under the standard scoring guidelines and with independent measures of cognitive functioning. In subsequent work comparing individuals with and without an English-speaking background (Lautenschlager et al. 2006), the authors proposed that LSA-based scoring of the WLM as a cognitive measure is less biased against people with different linguistic and cultural backgrounds than other widely used cognitive measures. This work demonstrates not only that accurate automated scoring of narrative recall tasks is possible but also that the objectivity offered by automated measures has speciﬁc beneﬁts for tests like the WLM, which are often administered by practitioners working in a community setting and serving a diverse population. We will compare the utility of this approach with our alignment-based approach subsequently in the article. More recently, Lehr et al. (2013) used a supervised method for scoring the responses to the WLM, transcribed both manually and via ASR, using conditional random ﬁelds. This technique resulted in slightly higher scoring and classiﬁcation accuracy than the unsupervised method described here. An unsupervised variant of their algorithm, which relied on the methods described in this article to provide training data to the conditional random ﬁeld, yielded about half of the scoring gains and nearly all of the classiﬁcation gains of what we report here. A hybrid method that used the methods in this article to derive features was the best performing system in that paper. Hence the methods described here are important components to that approach. We also note, however, that the supervised classiﬁer-based approach to scoring retellings requires a signiﬁcant amount of hand-labeled training data, thus rendering the technique impractical for application to a new narrative or to any picture description task. The importance of this distinction will become clear in Section 8, in which the approach outlined here is applied to a new data set lacking an existing scoring mechanism or a linguistic reference against which the responses can be scored. In this article, we will be discussing the application of our methods to manually generated transcripts of retellings and picture descriptions produced by adults with and without neurodegenerative disorders. We note, however, that the same techniques have been applied to narratives transcribed using ASR output (Lehr et al. 2012, 2013) with little degradation in accuracy, given sufﬁcient adaptation of the acoustic and language models to the WLM retelling domain. In addition, we have applied alignment-based scoring to the narratives of children with neurodevelopmental disorders, including autism and language impairment (Prud’hommeaux and Rouhizadeh 2012), with similarly strong diagnostic classiﬁcation accuracy, further demonstrating the applicability of these methods to a variety of input formats, elicitation techniques, and diagnostic goals. 554  Prud’hommeaux and Roark Graph-Based Word Alignment for Clinical Language Evaluation 3. Data 3.1 Experimental Participants The participants for this study were drawn from an ongoing study of brain aging at the Layton Aging and Alzheimer’s Disease Center at the Oregon Health and Science University. Seventy-two of these participants had received a diagnosis of MCI, and 163 individuals served as typically aging controls. Demographic information about the experimental participants is shown in Table 2. There were no signiﬁcant differences in age and years of education between the two groups. The Layton Center data included retellings for individuals who were not eligible for the present study because of their age or diagnosis. Transcriptions of 48 retellings produced by these ineligible participants were used to train and tune the word alignment model but were not used to evaluate the word alignment, scoring, or classiﬁcation accuracy. We diagnose MCI using the Clinical Dementia Rating (CDR) scale (Morris 1993), following earlier work on MCI (Petersen et al. 1999; Morris et al. 2001), as well as the work of Shankle et al. (2005) and Roark et al. (2011), who have previously attempted diagnostic classiﬁcation using neuropsychological instrument subtest responses. The CDR is a numerical dementia staging scale that indicates the presence of dementia and its level of severity. The CDR score is derived from measures of cognitive function in six domains: Memory; Orientation; Judgment and Problem Solving; Community Affairs; Home and Hobbies; and Personal Care. These measures are determined during an extensive semi-structured interview with the patient and a close family member or caregiver. A CDR of 0 indicates the absence of dementia, and a CDR of 0.5 corresponds to a diagnosis of MCI (Ritchie and Touchon 2000). This measure has high expert interrater reliability (Morris 1993) and is assigned without any information derived from the WLM subtest. 3.2 Wechsler Logical Memory The WLM test, discussed in detail in Section 2.2, is a subtest of the Wechsler Memory Scale (Wechsler 1997), a neuropsychological instrument used to evaluate memory function in adults. Under standard administration of the WLM, the examiner reads a brief narrative to the participant, excerpts of which are shown in Figure 1. The participant then retells the narrative to the examiner twice: once immediately upon hearing the narrative and a second time after 20 to 30 minutes. Two retellings from one of the participants in our study are shown in Figures 2 and 3. (There are currently two narrative  Table 2 Layton Center participant demographic data. Neither age nor years of education were signiﬁcantly different between groups.  Age (years) Education (years) Diagnosis n Mean Std Mean Std  MCI  72 88.7 6.0 14.9  2.6  Non-MCI 163 87.3 4.6 15.1  2.5  555  Computational Linguistics  Volume 41, Number 4  Anna / Thompson / of South / Boston / employed / as a cook / in a school / cafeteria / reported / at the police / station / that she had been [...] robbed of / ﬁfty-six dollars. / She had four / small children / the rent was due / and they hadn’t eaten / for two days. / The police / touched by the woman’s story / took up a collection / for her. Figure 1 Excerpts of WLM narrative with slashes indicating the boundaries between story elements. Twenty-two of the 25 story elements are shown here.  Ann Taylor worked in Boston as a cook. And she was robbed of sixty-seven dollars. Is that right? And she had four children and reported at the some kind of station. The fellow was sympathetic and made a collection for her so that she can feed the children. Figure 2 WLM retelling by a participant before MCI diagnosis (score = 12).  She was robbed. And she had a couple children to feed. She had no food for them. And people made a collection for her and to pay for her, for the food for the children. Figure 3 WLM retelling by same participant as in Figure 2 after MCI diagnosis (score = 5).  retelling subtests that can be administered as part of the Wechsler Memory Scale, but the Anna Thompson narrative used in the present study is the more widely used and has appeared in every version of the Wechsler Memory Scale with only minor modiﬁcations since the instrument was ﬁrst released 70 years ago.) Following the published scoring guidelines, the examiner scores the participant’s response by counting how many of the 25 story elements are recalled in the retelling without regard to their ordering or relative importance in the story. We refer to this as the summary score. The boundaries between story elements are indicated with slashes in Figure 1. The retelling in Figure 2, produced by a participant without MCI, received a summary score of 12 for the 12 story elements recalled: Anna, Boston, employed, as a cook, and robbed of, she had four, small children, reported, station, touched by the woman’s story, took up a collection, and for her. The retelling in Figure 3, produced by the same participant after receiving a diagnosis of MCI two years later, earns a summary score of 5 for the 5 elements recalled: robbed, children, had not eaten, touched by the woman’s story, and took up a collection. Note that some of the story elements in these retellings were not recalled verbatim. The scoresheet provided with the exam indicates the lexical substitutions and degree of paraphrasing that are permitted, such as Ann or Annie for Anna, or any indication that the story evoked sympathy for touched by the woman’s story. Although the scoring guidelines have an air of arbitrariness in that paraphrasing is only sometimes permitted, they do allow the test to be scored with high inter-rater reliability (Mitchell 1987). Recall that each participant produces two retellings for the WLM: an immediate retelling and a delayed retelling. Each participant’s two retellings were transcribed at the utterance level. The transcripts were downcased, and all pause-ﬁllers, incomplete words, and punctuation were removed. The transcribed retellings were scored manually according to the published scoring guidelines, as described earlier in this section. 556  Prud’hommeaux and Roark Graph-Based Word Alignment for Clinical Language Evaluation 4. Diagnostic Classiﬁcation Framework 4.1 Classiﬁer The goal of the work presented here is to demonstrate the utility of a variety of features derived from the WLM retellings for diagnostic classiﬁcation of individuals with MCI. To perform this classiﬁcation, we use LibSVM (Chang and Lin 2011), as implemented within the Waikato Environment for Knowledge Analysis (Weka) API (Hall et al. 2009), to train SVM classiﬁers, using a radial basis function kernel and default parameter settings. We evaluate classiﬁcation via receiver operating characteristic (ROC) curves, which have long been widely used to evaluate diagnostic tests (Zweig and Campbell 1993; Faraggi and Reiser 2002; Fan, Upadhye, and Worster 2006) and are also increasingly used in machine learning to evaluate classiﬁers in ranking scenarios (Cortes, Mohri, and Rastogi 2007; Ridgway et al. 2014). Analysis of ROC curves allows for classiﬁer evaluation without selecting a speciﬁc, potentially arbitrary, operating point. To use standard clinical terminology, ROC curves track the tradeoff between sensitivity and speciﬁcity. Sensitivity (true positive rate) is what is commonly called recall in computational linguistics and related ﬁelds—that is, the percentage of items in the positive class that were correctly classiﬁed as positives. Speciﬁcity (true negative rate) is the percentage of items in the negative class that were correctly classiﬁed as negatives, which is equal to one minus the false positive rate. If the threshold is set so that nothing scores above threshold, the sensitivity (true positive rate, recall) is 0.0 and speciﬁcity (true negative rate) is 1.0. If the threshold is set so that everything scores above threshold, sensitivity is 1.0 and speciﬁcity is 0.0. As we sweep across intervening threshold settings, the ROC curve plots sensitivity versus one minus speciﬁcity, true positive rate versus false positive rate, providing insight into the precision/recall tradeoff at all possible operating points. Each point (tp, fp) in the curve has the true positive rate as the ﬁrst dimension and false positive rate as the second dimension. Hence each curve starts at the origin (0, 0), the point corresponding to a threshold where nothing scores above threshold, and ends at (1, 1), the point where everything scores above threshold. ROC curves can be characterized by the area underneath them (“area under curve” or AUC). A perfect classiﬁer, with all positive items ranked above all negative items, has an ROC curve that starts at point (0, 0), goes straight up to (1, 0)—the point where true positive is 1.0 and false positive is 0.0 (since it is a perfect classiﬁer)—before continuing straight over to the ﬁnal point (1, 1). The area under this curve is 1.0, hence a perfect classiﬁer has an AUC of 1.0. A random classiﬁer, whose ROC curve is a straight diagonal line from the origin to (1, 1), has an AUC of 0.5. The AUC is equivalent to the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example, and is, in fact, equivalent to the Wilcoxon-Mann-Whitney statistic (Hanley and McNeil 1982). This statistic allows for classiﬁer comparison without the need of pre-specifying arbitrary thresholds. For tasks like clinical screening, different tradeoffs between sensitivity and speciﬁcity may apply, depending on the scenario. See Fan, Upadhye, and Worster (2006) for a useful discussion of clinical use of ROC curves and the AUC score. In that paper, the authors note that there are multiple scales for interpreting the value of AUC, but that a rule-of-thumb is that AUC ≤ 0.75 is generally not clinically useful. For the present article, however, AUC mainly provides us the means for evaluating the relative quality of different classiﬁers. One key issue for this sort of analysis is the estimation of the AUC for a particular classiﬁer. Leave-pair-out cross-validation—proposed by Cortes, Mohri, and Rastogi 557  Computational Linguistics  Volume 41, Number 4  (2007) and extensively validated in Pahikkala et al. (2008) and Airolaa et al. (2011)— is a method for providing an unbiased estimate of the AUC, and the one we use in this article. In the leave-pair-out technique, every pairing between a negative example (i.e., a participant without MCI) and a positive example (i.e., a participant with MCI) is tested using a classiﬁer trained on all of the remaining examples. The results of each positive/negative pair can be used to calculate the Wilcoxon-Mann-Whitney statistic as follows. Let s(e) be the score of some example e; let P be the set of positive examples and N the set of negative examples; and let [s(p) > s(n)] be 1 if true and 0 if false. Then:  AUC(s, P, n)  =  
Metaphor enriches our communication with a more diverse imagery and provides an important mechanism for reasoning about concepts. At the same time, it is also a very common linguistic device that has long become a part of our everyday language. Metaphors arise through systematic associations between distinct, and seemingly unrelated, concepts. For example, when we say “The wheels of Stalin’s regime were well-oiled and already turning,” we view a political system in terms of a mechanism, it can function, break, have wheels, and so forth. The existence of this association allows us to transfer knowledge and inferences from the domain of mechanisms to that of political systems. As a result, we reason about political systems in terms of mechanisms and discuss them using the mechanism terminology, giving rise to a variety of metaphorical expressions. This view of metaphor is widely known as Conceptual Metaphor Theory (CMT). It was proposed by Lakoff and Johnson (1980), who claimed that metaphor is not merely a property of language, but rather a cognitive mechanism that structures our conceptual system in a certain way. Lakoff and Johnson explained metaphor through a presence of a mapping between two domains of experience: the target (e.g., politics) and the source (e.g., mechanism). Metaphor is thus not limited to meaning extensions of individual ∗ International Computer Science Institute, 1947 Center St. Ste. 600, Berkeley, CA 94704, USA. E-mail: katia@icsi.berkeley.edu. Submission received: 9 October 2013; revised version received: 4 March 2015; accepted for publication: 10 June 2015. doi:10.1162/COLI a 00233 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 4  words, but rather involves a complex cross-domain knowledge projection process. Let us consider a few more examples. (1) “President Obama is rebuilding the campaign machinery that vaulted him into ofﬁce” (New York Times 2011) (2) 20 steps towards a modern, working democracy (3) Time to mend our foreign policy. (4) “She knows the nuts and bolts, and it’s not the nuts and bolts inside legislation, it’s the nuts and bolts of raising money, preparing the party for elections, a political consultant kind of politics.” (Bzdek 2008) These examples demonstrate how multiple properties and inferences from the domain of mechanisms are systematically projected onto our knowledge about politics. Lakoff and Johnson coined the term conceptual metaphor to describe such mappings from the source domain to the target. The view of an inter-conceptual mapping as a basis of metaphor was echoed by other prominent theories in the ﬁeld. These include, most notably, the comparison view, formulated in the Structure–Mapping Theory of Gentner (1983), and the interaction view (Black 1962; Hesse 1966). However, the principles of CMT have inspired and inﬂuenced much of the computational work on metaphor, thus becoming more central to this paper. Conceptual metaphor manifests itself in language in the form of linguistic metaphor, or metaphorical expressions. These in turn include lexical metaphor, that is, single-word meaning extensions (as in Examples (2) and (3)); multi-word metaphorical expressions (e.g., “the government turned a blind eye to corruption”); or extended metaphor, that spans longer discourse fragments. Manifestations of metaphor are frequent in language, appearing on average in every third sentence of general-domain text, according to corpus studies (Cameron 2003; Martin 2006; Shutova and Teufel 2010; Steen et al. 2010). This makes metaphor an important subject of linguistic research and makes its accurate processing essential for a range of practical NLP applications. These include, for example, (1) machine translation (MT): Because a large number of metaphorical expressions are culture-speciﬁc, they represent a considerable challenge for MT (e.g., the English metaphor “to shoot down someone’s arguments” cannot be literally translated into German as “Argumente abschießen” and metaphor interpretation is required); (2) opinion mining: Metaphorical expressions tend to contain a strong emotional component—for example, compare the metaphorical expression “Government loosened stranglehold on business” and its literal counterpart Government deregulated business (Narayanan 1999); (3) information retrieval (IR): Non-literal language without appropriate disambiguation may lead to false positives in information retrieval (e.g., documents describing “old school gentlemen” should not be returned for the query school [Korkontzelos et al. 2013]); and many others. Because metaphor interpretation requires complex analogical comparisons and the projection of inference structures across domains, the task of automatic metaphor processing is challenging. For many years, computational work on metaphor evolved around the use of hand-coded knowledge and rules to model metaphorical associations, making the systems hard to scale. Recent years have seen a growing interest in statistical modeling of metaphor (Mason 2004; Gedigian et al. 2006; Shutova 2010; Shutova, Sun, and Korhonen 2010; Turney et al. 2011; Dunn 2013a; Heintz et al. 2013; Hovy et al. 2013; Li, Zhu, and Wang 2013; Mohler et al. 2013; Shutova and Sun 2013;  580  Shutova  Design and Evaluation of Metaphor Processing Systems  Strzalkowski et al. 2013; Tsvetkov, Mukomel, and Gershman 2013), with many new techniques opening routes for improving system accuracy and robustness. A wide range of methods have been proposed and investigated by the community, including supervised (Gedigian et al. 2006; Dunn 2013a; Hovy et al. 2013; Mohler et al. 2013; Tsvetkov, Mukomel, and Gershman 2013) and unsupervised (Heintz et al. 2013; Shutova and Sun 2013) learning, distributional approaches (Shutova 2010, 2013; Shutova, Van de Cruys, and Korhonen 2012), lexical resource-based methods (Krishnakumaran and Zhu 2007; Wilks et al. 2013), psycholinguistic features (Turney et al. 2011; Gandy et al. 2013; Neuman et al. 2013; Strzalkowski et al. 2013), and Web search (Veale and Hao 2008; Bollegala and Shutova 2013; Li, Zhu, and Wang 2013). Although individual approaches tackling individual aspects of metaphor have met with success, the insights gained from these experiments are still difﬁcult to integrate into a single computational metaphor modeling landscape, because of the lack of a uniﬁed task deﬁnition, a shared data set, and well-deﬁned evaluation standards. This hampers our progress as a community in this area. In this paper we take a step towards closing this gap: We review the recent work on computational modeling of metaphor, the tasks addressed, the system features proposed, and the evaluations conducted, and analyze the relevance of different linguistic aspects of metaphor for system performance and applicability, with the aim of identifying the desired properties of metaphor processing systems and a set of requirements for their evaluation. 2. Considerations in the Design of a Metaphor Processing System When designing a metaphor processing system, one faces a number of choices. Some stem from the linguistic and cognitive properties of metaphor, others concern the applicability and usefulness of the system in the wider NLP context. In this section, we analyze individual aspects of metaphor and their relevance to computational modeling, as well as their interplay in the design of a real-world system. 2.1 Linguistic Considerations and Levels of Analysis Linguistic considerations that inform the design of metaphor processing systems concern primarily the choice of the level (or levels) of analysis. The levels of metaphor analysis include (1) linguistic metaphor (or metaphorical expressions), (2) conceptual metaphor, (3) extended metaphor, and (4) metaphorical inference. Let us consider an example of manifestations of the conceptual metaphor EUROPEAN INTEGRATION as a TRAIN JOURNEY, popular in the early 1990s, at various levels. r Conceptual: EUROPEAN INTEGRATION as a TRAIN JOURNEY r Linguistic: The coupling of the carriages may not be reliably secure, but the pan-European express is in motion. r Extended metaphor: “There is a fear that the European train will thunder forward, laden with its customary cargo of gravy, towards a destination neither wished for nor understood by electorates. But the train can be stopped.” (Margaret Thatcher, Sunday Times, 20 Sept 1992) r Metaphorical inference: The fact that expensive tracks have to be laid for the train to move forward means that someone has to fund the process of European integration.  581  Computational Linguistics  Volume 41, Number 4  2.1.1 Linguistic metaphor. Linguistic metaphor, or metaphorical expressions, concern the surface realization of metaphorical mechanisms, and have been unsurprisingly central to metaphor processing research to date (Birke and Sarkar 2006; Gedigian et al. 2006; Krishnakumaran and Zhu 2007; Shutova, Sun, and Korhonen 2010; Turney et al. 2011; Dunn 2013a; Gandy et al. 2013; Heintz et al. 2013; Hovy et al. 2013; Neuman et al. 2013; Shutova 2013; Strzalkowski et al. 2013; Tsvetkov, Mukomel, and Gershman 2013). Metaphorical expressions represent the way in which text-processing systems encounter metaphor, and there is little doubt that any real-world metaphor processing system, whatever the approach or the application, ultimately needs to be able to identify and interpret them. When focusing on linguistic metaphor, one needs to further take into account the level of conventionality of the expressions; how different syntactic constructions are used to convey metaphorical meanings and at what level metaphor annotation needs to be done (word, relation, or sentence level). Thus the following considerations become important for the task and system design: r Level of conventionality of the metaphors accepted: Metaphor is a productive phenomenon, that is, its novel examples continue to emerge in language. However, a large number of metaphorical expressions become conventionalized over time (e.g., “I cannot grasp his way of thinking”). Although metaphorical in nature, their meanings are deeply entrenched in everyday use, and their comprehension is likened to that of literally used terms (Nunberg 1987). According to Gibbs (1984), metaphorical expressions are spread along a continuum from highly conventional, lexicalized metaphors to entirely novel and creative ones. Gibbs thus suggests that there is no clear demarcation line between literal and metaphorical language, and the distinction between them is rather governed by the level of conventionality of metaphorical expressions. From the usage perspective, metaphoricity may be viewed as a gradient phenomenon rather than a binary one (Dunn 2011), and conventionality becomes an important factor in the design and evaluation of metaphor processing systems. It is not yet clear where on the metaphorical–literal continuum the system should draw the line between what it considers metaphorical and what it considers literal. The answer to this question most likely depends on the NLP application in mind. However, generally speaking, real-world NLP applications are unlikely to be concerned with historical aspects of metaphor, but rather with the identiﬁcation of ﬁgurative language that needs to be interpreted differently from the literal language. We therefore suggest that NLP applications do not necessarily need to address highly conventional and lexicalized metaphors that can be interpreted using standard word sense disambiguation techniques, but rather would beneﬁt from the identiﬁcation of less conventional and more creative language. Much of the metaphor processing work has focused on conventional metaphor, though in principle capable of identifying novel metaphor as well (Shutova, Sun, and Korhonen 2010; Turney et al. 2011; Dunn 2013a; Gandy et al. 2013; Heintz et al. 2013; Neuman et al. 2013; Shutova 2013; Strzalkowski et al. 2013; Tsvetkov, Mukomel, and Gershman 2013), with few approaches modeling only novel metaphor (Desalle, Gaume, and Duvignau 2009) or discriminating between conventional and novel metaphors (Krishnakumaran and Zhu 2007). Other approaches have 582  Shutova  Design and Evaluation of Metaphor Processing Systems  looked at literal versus non-literal distinction deﬁned more broadly (Birke and Sarkar 2006; Li and Sporleder 2009, 2010). r Syntactic constructions covered: Metaphors vary with respect to how they are expressed in language grammatically. The grammatical structure of metaphorical expressions is tightly coupled with the inference processes that produce them and are guided by the semantic frames they evoke (Sullivan 2007, 2013). According to Sullivan, conceptual metaphors are realized in language via mapping semantic roles in the source frame onto roles in the target frame. This suggests that both the source and the target domain impose a set of constraints on the roles that can be mapped, and thus on the syntactic options for expressing the metaphorical meaning. There has not yet been a computational approach investigating the interplay of frame semantics and surface structure of metaphorical language. However, the community has addressed modeling linguistic metaphor in a range of syntactic constructions. Verbal or adjectival metaphors are particularly widely embraced by NLP researchers (most approaches), with a few works focusing on copula constructions (Krishnakumaran and Zhu 2007; Gandy et al. 2013; Li, Zhu, and Wang 2013; Neuman et al. 2013) or other nominal metaphors (Heintz et al. 2013; Hovy et al. 2013; Li, Zhu, and Wang 2013; Mohler et al. 2013; Strzalkowski et al. 2013). A number of approaches to metaphor identiﬁcation have also addressed multiword metaphors (Li and Sporleder 2010; Heintz et al. 2013; Hovy et al. 2013; Mohler et al. 2013). Corpus-linguistic research has shown that verbs and adjectives account for a large proportion of metaphorical expressions observed in the data (Cameron 2003; Shutova and Teufel 2010). However, a recent study (Jamrozik et al. 2013) has also shown that relational words tend to have a higher metaphorical potential. This corresponds to the data on verbal metaphor frequency, and it also suggests that relational nouns are important (e.g., “words are friends of translators”). r Lexical, relation, or sentence level: Finally, one needs to decide if metaphor should be annotated at the word level (i.e., tagging the source domain words alone), relation level (i.e., tagging both source and target words in a particular grammatical relation), or sentence level (i.e., tagging sentences that contain metaphorical language, without explicit annotation of source and target domain words). The word or relation levels provide the most information and have been the focus of the majority of approaches (Gedigian et al. 2006; Shutova, Sun, and Korhonen 2010; Turney et al. 2011; Gandy et al. 2013; Heintz et al. 2013; Hovy et al. 2013; Neuman et al. 2013; Shutova 2013; Shutova and Sun 2013; Wilks et al. 2013). However, some works annotated metaphor at the sentence level (Krishnakumaran and Zhu 2007; Dunn 2013a; Li, Zhu, and Wang 2013; Mohler et al. 2013; Strzalkowski et al. 2013; Tsvetkov, Mukomel, and Gershman 2013). 2.1.2 Conceptual metaphor. Conceptual metaphor represents a cognitive and conceptual mechanism by which humans produce and comprehend metaphorical expressions. Manifestations of conceptual metaphor are ubiquitous in language, communication, and even decision-making (Thibodeau and Boroditsky 2011). Here are a few examples of 583  Computational Linguistics  Volume 41, Number 4  common metaphorical mappings: TIME is MONEY (e.g., “That ﬂat tire cost me an hour”), IDEAS are PHYSICAL OBJECTS (e.g., “I cannot grasp his way of thinking”), EMOTIONS ARE VEHICLES (e.g., “she was transported with pleasure”), FEELINGS ARE LIQUIDS (e.g., “all of this stirred an unfathomable excitement in her”), LIFE IS A JOURNEY (e.g., “He arrived at the end of his life with very little emotional baggage”); ARGUMENT IS A WAR (e.g., “He shot down all of my arguments,” “He attacked every weak point in my argument”). On one hand, few would disagree that the metaphor processing systems capable of understanding and applying conceptual metaphor should be in a better position to accurately handle linguistic metaphors as well. However, a series of questions arise when designing a model of conceptual metaphor. For example, how does one represent conceptual metaphors in the system? What labels does one assign to source and target domains? Is it even possible to name all the conceptual metaphors that humans use and is it necessary to do so? A computational model needs a clear deﬁnition of what constitutes the source and target domains (whether they are manually listed or automatically learned) and the consistency and coverage of source and target domain categories would play a crucial role in how well the model can account for real-world data. Previous research on annotation of conceptual metaphor (Shutova and Teufel 2010) has shown that the annotators tend to disagree on the assignment of source and target domain categories. The most variation stems from the level of generality of the selected categories, indicating that while cross-domain mappings are intuitive to humans (i.e., they can be annotated in arbitrary text in principle), labeling source and target domains consistently appears to be a challenging task. What this suggests is that, although the mechanism of conceptual metaphor may be helpful to the system, the question of how to best represent source and target domains within the system remains open. A predeﬁned set of categories, such as those widely discussed in the linguistic literature on CMT, may not be sufﬁcient or even suitable for a computational model. And despite the validity of the main principles of CMT as a linguistic theory, it is not straightforward to port it to computational modeling of metaphor. A more ﬂexible, and potentially datadriven, representation of source and target domain categories is needed for the latter purpose. A data-driven representation would also be better suited to account for the freedom of interpretation that some metaphors allow, since more ﬂexible structures can be dynamically learned from the data. So far, the community has attempted assigning manually created labels to metaphorical mappings (Mason 2004; Baumer, Tomlinson, and Richland 2009), harvesting ﬁne-grained mappings between individual nouns (Li, Zhu, and Wang 2013), using lexical resources to deﬁne or expand source and target domain categories (Gandy et al. 2013; Mohler et al. 2013), representing source and target concepts as word clusters (Shutova and Sun 2013) or automatically learned topics (Heintz et al. 2013), and learning metaphorical mappings implicitly within the model without explicit labeling (Shutova, Sun, and Korhonen 2010). 2.1.3 Extended metaphor. Extended metaphor refers to the use of metaphor at the discourse level. It manifests itself in discourse via a sequence of metaphorically used language, yielded by the same conceptual metaphor, whereby a continuous scenario from the source domain is metaphorically projected onto the target. For instance, viewing European integration as a train journey led to numerous metaphorical expressions in political discourse. Each of them mapped certain properties of train journeys to political processes—with countries as loosely connected carriages, peoples of different countries as passengers of their respective carriages, expensive tracks that have to be laid for the train to move forward, and the ﬁnal destination not very well understood. This metaphor has been dominating the debate, with the European leaders arguing over its details 584  Shutova  Design and Evaluation of Metaphor Processing Systems  (Beigman Klebanov and Beigman 2010). One can see this metaphor frequently reappear and evolve over time, helping politicians defend their agendas and, not least, shedding some clarity on an otherwise uncertain future. Research in linguistics and political science (Musolff 2000; Lakoff 2008; Lakoff and Wehling 2012) suggests that the use of a particular metaphor often guides the speakers’ argumentation strategy throughout a piece of discourse, as well as participants’ behavior in a dialogue. Beigman Klebanov and Beigman (2010) investigated extended metaphor within a game-theoretic framework, demonstrating that maintaining the metaphorical frame in a debate is rationalizable in terms of the gains the participants may get from doing so and their potential losses from swerving away from the metaphor. Their approach reverse-engineers the motivations behind the use of extended metaphor within a formal framework. Beigman Klebanov and Beigman’s work was an important advance, enhancing our understanding of the inner workings of extended metaphor, motivation behind its use, and its effects on social dynamics. However, a computational method for identiﬁcation and interpretation of extended metaphor in real-world discourse is yet to be proposed. A discourse-level metaphor processing system would need to identify a chain of metaphorical expressions in a text, which indicates a systematic association of the text topic with a particular domain. These chains would then demonstrate how continuous scenarios can be transferred across domains. Recovering this information from the data would allow us to better understand the structure behind metaphorical associations, as well as the inferential process by which knowledge is projected across domains. This system would also ﬁnd application in social science, where metaphorical framing is widely studied as an indicator of the underlying cultural and moral models (Lakoff and Wehling 2012). 2.1.4 Metaphorical inference. When projecting knowledge from one domain to another, a set of complex inferences take place. Metaphorical inferences are grounded in the source domain and result in the production of surface structures we observe in language as metaphorical expressions. Metaphorical mappings are thus realized via projecting inference structures from the source domain onto the target. For example, when European integration is metaphorically viewed as a train journey, our knowledge of typical events and their consequences from the domain of train journey are projected onto our reasoning about the process of European integration. For instance, if we know that expensive tracks need to be laid before a train can move forward, we can infer that someone also needs to fund the process of European integration. Interestingly, in the presence of a conceptual metaphor such inference can take place even without any linguistic metaphor referring to the tracks being present, but rather on the basis of our common sense knowledge about the functioning of trains. Besides allowing us to derive new information about the target domain, projecting the inferential structures from the source domain also invokes an emotional response coming from the source domain—for example, an unknown destination or a great expense make one feel uneasy when referred to both literally and metaphorically. Such a transfer of inferential processes and emotional content is believed by some to be one of the central purposes of metaphor (Kovecses 2005; Feldman 2006; Thibodeau and Boroditsky 2011). Psychologists Thibodeau and Boroditsky (2011) investigated how metaphor and metaphorical inference affect decision-making. Their hypothesis was that metaphors in language “instantiate frame-consistent knowledge structures [from the source domain] and invite structurally consistent inferences [about the target domain].” They used two groups of subjects, who were presented with two different texts about crime. In the ﬁrst text crime was metaphorically portrayed as a VIRUS and 585  Computational Linguistics  Volume 41, Number 4  in the second as a BEAST. The two groups were then asked a set of questions on how to tackle crime in the city. It turned out that, whereas the ﬁrst group tended to opt for preventive measures in tackling crime (e.g., stronger social policies), the second group converged on punishment- or restraint-oriented measures. According to Thibodeau and Boroditsky, their results demonstrate that metaphors have profound inﬂuence on how we conceptualize and act with respect to societal issues. Interestingly, the participants would explain their decisions via arguments unrelated to the metaphor, showing that the effect of metaphorical inference in guiding human reasoning is rather covert. Being able to reproduce these inferences is likely to make automatic metaphor understanding better informed and hence more accurate. It may also provide a mechanism of representing source–target domain mappings, that themselves are generalizations over a set of inferences transferred from one domain to another. And ﬁnally, these inferences provide a platform for metaphor interpretation, namely, deriving the meaning of a metaphorical expression and the additional connotations it introduces (that are likely to originate from the source domain). There is a consensus among cognitive linguists that it is metaphorical inference that provides for the very texture behind the use of metaphor (Hobbs 1981; Carbonell 1982; Rohrer 1997; Turner and Fauconnier 2003; Feldman 2006). Although uncovering this texture is certainly one of the main objectives of computational metaphor understanding, it is at the same time a very challenging undertaking. Reproducing metaphorical inferences would require the ability to learn vast amounts of world knowledge from the data, as well as performing complex crossdomain comparisons. And despite being a very promising route, it has not yet been attempted in NLP. 2.2 Applicability Another set of considerations in the design of metaphor processing systems stems from the needs of real-world NLP. The high frequency of metaphorical language in textual data makes accurate metaphor processing desired for a number of NLP applications. Thus, the format of metaphor understanding that metaphor processing systems provide should ideally be informed by the requirements of external NLP and a number of considerations arise in that respect: r Metaphor processing typically involves two tasks: metaphor identiﬁcation (distinguishing between literal and metaphorical language in text) and metaphor interpretation (identifying the intended meaning of the metaphor). Both of these provide useful information for language understanding and need to be addressed, either independently or as a single process. r A metaphor processing system should provide a representation of metaphor interpretation that can be easily integrated with other NLP systems. This criterion places constraints on how the metaphor processing task should be deﬁned. The most universally applicable metaphor interpretation would be in the text-to-text form. This means that a metaphor processing system would take raw text as input and provide textual output, in which metaphors are interpreted. r In order to be useful for real-world NLP, the system needs to be capable of processing real-world data, and thus operate on unrestricted, continuous text. Rather than only dealing with individual carefully selected clear-cut 586  Shutova  Design and Evaluation of Metaphor Processing Systems  examples, the system should be fully implemented and tested on free, naturally occurring text. r To enable wide applicability, the system needs to be open-domain—that is, operate in all domains, genres, and topics. Therefore, ideally it should not rely on any domain-speciﬁc information or focus on individual types of instances (e.g., a limited set of hand-chosen source-target domain mappings). r To be easily adaptable to new domains, the system should not rely on task-speciﬁc hand-coded knowledge. This means it needs to be either data-driven and be able to automatically acquire the knowledge it needs from text corpora, or rely only on large-scale, general-domain lexical resources (that are already in existence and do not need to be created in a costly manner). However, it would be an advantage if no such resource is required and the system can dynamically induce meanings in context. r To be robust, the system needs to be able to deal with metaphors represented by all word classes and syntactic constructions. Many existing models are designed with speciﬁc kinds of metaphorical expressions in mind, for instance nominal metaphors in copula constructions or verbal metaphors in verb–object relations. To be applicable to support real-world NLP applications, these models need to be extended beyond those speciﬁc word classes and syntactic constructions, and be able to process any kind of metaphorical language. When designing a metaphor processing task, methodology, and evaluation strategy, one thus needs to keep these criteria in mind. Although modeling all of the phenomena described in this section within a single system is by no means a requirement, it is critically important to be aware of all the guises that metaphor may take, both conceptually and empirically. 3. Metaphor Annotation and Resources 3.1 Corpora Metaphor annotation studies have typically focused on one (or both) of the following tasks: (1) identiﬁcation of metaphorical senses in text (i.e., distinguishing between literal and non-literal meanings), and (2) assignment of the corresponding source– target domain mappings. The majority of corpus-linguistic studies were concerned with metaphorical expressions and mappings within a limited domain—for example, WAR, BUSINESS, FOOD, or PLANT metaphors (Santa Ana 1999; Izwaini 2003; Koller 2004; Skorczynska Sznajder and Pique-Angordans 2004; Chung, Ahrens, and Huang 2005; Hardie et al. 2007; Gong, Ahrens, and Huang 2008; Lu and Ahrens 2008; Low et al. 2010), in a particular genre or type of discourse (Charteris-Black 2000; Cameron 2003; Izwaini 2003; Koller 2004; Skorczynska Sznajder and Pique-Angordans 2004; Martin 2006; Hardie et al. 2007; Lu and Ahrens 2008; Beigman Klebanov and Flor 2013), or in individual examples in isolation from wider context (Wikberg 2006; Lo¨ nnekerRodman 2008). In addition, these approaches often focused on a small predeﬁned set of source and target domains. Another vein of corpus-based research concerned crosslinguistic differences in the use of metaphor, also in a speciﬁc domain—for example, 587  Computational Linguistics  Volume 41, Number 4  ﬁnancial discourse (Charteris-Black and Ennis 2001), metaphors describing FEELINGS (Stefanowitsch 2004; Diaz-Vera and Caballero 2013), or metaphorical expressions referring to body parts (Deignan and Potter 2004). Three recent studies are notable in that they moved away from investigating particular domains to a more general study of how metaphor behaves in unrestricted continuous text. Wallington et al. (2003), Shutova and Teufel (2010), and Steen et al. (2010) conducted consecutive metaphor annotation in open-domain texts. Wallington et al. (2003) used two teams of annotators and compared externally prescribed deﬁnitions of metaphor with intuitive internal ones. Team A was asked to annotate “interesting stretches,” whereby a phrase was considered interesting if (1) its signiﬁcance in the document was non-physical, (2) it could have a physical signiﬁcance in another context with a similar syntactic frame, and (3) this physical signiﬁcance was related to the abstract one. Team B had to annotate phrases according to their own intuitive deﬁnition of metaphor. Apart from metaphorical expressions, the respective source–target domain mappings were also to be annotated. For this latter task, the annotators were given a set of mappings from the Master Metaphor List and were asked to assign the most suitable ones. However, the authors do not report the level of interannotator agreement, nor the coverage of the mappings in the Master Metaphor List on their data. The fact that the method is limited to a set of mappings exempliﬁed in the Master Metaphor List suggests that it may not scale well to real-world data, because the predeﬁned inventory of mappings is unlikely to be sufﬁcient to cover the majority of metaphorical expressions in arbitrary text. Steen and his colleagues (Pragglejaz Group 2007; Steen et al. 2010) proposed a metaphor identiﬁcation procedure (MIP). In the framework of this procedure, the sense of every word in the text is considered as a potential metaphor. Every word is then tagged as literal or metaphorical, based on whether is has a “more basic, contemporary meaning” in other contexts than the current one. The summary of their annotation procedure is presented is Figure 1. In a sense, such annotation can be viewed as a form of word sense disambiguation with an emphasis on metaphoricity. Steen and colleagues ran a reliability study involving near-native speaker annotators (strongly relying on dictionary deﬁnitions) and report an interannotator agreement of 0.85 in terms of Fleiss’ kappa. MIP laid the basis for the creation of the VU Amsterdam Metaphor Corpus1 (Steen et al. 2010). This corpus is a subset of BNC Baby2 annotated for linguistic metaphor. Its size is 200,000 words and it comprises four genres: news text, academic text, ﬁction, and conversations. The corpus has already found application in computational metaphor processing research (Dunn 2013b; Niculae and Yaneva 2013), as well as inspiring metaphor annotation efforts in other languages (Badryzlova et al. 2013). The study of Shutova and Teufel (2010) was concerned with annotation of both metaphorical expressions and metaphorical mappings in continuous text. Their annotation procedure is based on MIP, modifying and extending it to the identiﬁcation of conceptual metaphors along with the linguistic ones. Following MIP, the annotators were asked to identify the more basic sense of the word, and then label the context in which the word occurs in the basic sense as the source domain, and the current context as the target. They were provided with a list of suggested common source  
In this article, we investigate aspects of sentential meaning that are not expressed in local predicate–argument structures. In particular, we examine instances of semantic arguments that are only inferable from discourse context. The goal of this work is to automatically acquire and process such instances, which we also refer to as implicit arguments, to improve computational models of language. As contributions towards this goal, we establish an effective framework for the difﬁcult task of inducing implicit arguments and their antecedents in discourse and empirically demonstrate the importance of modeling this phenomenon in discourse-level tasks. Our framework builds upon a novel projection approach that allows for the accurate detection of implicit arguments by aligning and comparing predicate–argument structures across pairs of comparable texts. As part of this framework, we develop a graph-based model for predicate alignment that signiﬁcantly outperforms previous approaches. Based on such alignments, we show that implicit argument instances can be automatically induced and applied to improve a current model of linking implicit arguments in discourse. We further validate that decisions on argument realization, although being a subtle phenomenon most of the time, can considerably affect the perceived coherence of a text. Our experiments reveal that previous models of coherence are not able to predict this impact. Consequently, we develop a novel coherence model, which learns to accurately predict argument realization based on automatically aligned pairs of implicit and explicit arguments. 1. Introduction The goal of semantic parsing is to automatically process natural language text and map the underlying meaning of text to appropriate representations. Semantic role labeling ∗ School of Informatics, University of Edinburgh, EH8 9AB Edinburgh, United Kingdom. E-mail: mroth@inf.ed.ac.uk. ∗∗ Department of Computational Linguistics, Heidelberg University, 69120 Heidelberg, Germany. E-mail: frank@cl.uni-heidelberg.de. Submission received: 5 June 2014; revised version received: 13 April 2015; accepted for publication: 28 August 2015. doi:10.1162/COLI a 00236 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 4  induces shallow semantic representations, so-called predicate–argument structures, by processing sentences and mapping them to predicates and associated arguments. Arguments of these structures can, however, be non-local in natural language text, as shown in Example 1. Example 1 (a) El Salvador is the only Latin American country which has troops in Iraq. (b) Nicaragua withdrew its troops last month.1 Applying a semantic role labeling system on sentence (1b) produces a representation that consists of the predicate withdraw, a temporal modiﬁer (last month) and two associated arguments: the entity withdrawing (Nicaragua) and the thing being withdrawn (its troops). From the previous sentence (1a), we can additionally infer a third argument: namely, the source from which Nicaragua withdrew its troops (Iraq). By leaving this piece of information implicit, the text fragment in sentence (1b) illustrates a typical case of non-local, or implicit, role realization (Gerber and Chai 2012). In this article, we view implicit arguments as a discourse-level phenomenon and treat corresponding instances as implicit references to discourse entities. Taking this perspective, we build upon previous work on discourse analysis. Following Sidner (1979) and Joshi and Kuhn (1979), utterances in discourse typically focus on a set of salient entities, which are also called the foci or centers. Using the notion of centers, Grosz, Joshi, and Weinstein (1995) deﬁned the Centering framework, which relates the salience of an entity in discourse to linguistic factors such as choice of referring expression and syntactic form.2 Both extremes of salience, that is, contexts of referential continuity and irrelevance, can also be reﬂected by the non-realization of an entity (Brown 1983). Speciﬁc instances of this phenomenon, so-called zero anaphora, have been well-studied in pro-drop languages such as Japanese (Kameyama 1985), Turkish (Turan 1995), and Italian (Di Eugenio 1990). For English, only a few studies exist that explicitly investigated the effect of non-realizations on coherence. Existing work suggests, however, that indirect references and non-realizations are important for modeling and measuring coherence (Poesio et al. 2004; Karamanis et al. 2009), respectively, and that such phenomena need to be taken into consideration to explain local coherence where adjacent sentences are neither connected by discourse relations nor in terms of coreference (Louis and Nenkova 2010). In this work, we propose a new model to predict whether realizing an argument contributes to local coherence in a given position in discourse. Example 1 illustrated a text fragment, in which argument realization is necessary in the ﬁrst sentence but redundant in the second. That is, mentioning Iraq in the second sentence is not necessary (for a human being) to understand the meaning of the text. In contrast, making both references explicit, as shown in Example 2, would be redundant and could lead to the perception that the text is merely a concatenation of two independent sentences — rather than a set of adjacent sentences that form a meaningful, or coherent, discourse.  
Anna Korhonen∗ University of Cambridge We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantiﬁes similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reﬂect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables ﬁne-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-ofthe-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures. 1. Introduction There is very little similar about coffee and cups. Coffee refers to a plant, which is a living organism or a hot brown (liquid) drink. In contrast, a cup is a man-made solid of broadly well-deﬁned shape and size with a speciﬁc function relating to the consumption of liquids. Perhaps the only clear trait these concepts have in common is that they are concrete entities. Nevertheless, in what is currently the most popular evaluation gold standard for semantic similarity, WordSim(WS)-353 (Finkelstein et al. 2001), coffee and ∗ Computer Laboratory University of Cambridge, UK. E-mail: {felix.hill, anna.korhonen}@ cl.cam.ac.uk. ∗∗ Technion, Israel Institute of Technology, Haifa, Israel. E-mail: roiri@ie.technion.ac.il. Submission received: 25 July 2014; revised submission received: 10 June 2015; accepted for publication: 31 August 2015. doi:10.1162/COLI a 00237 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 4  cup are rated as more “similar” than pairs such as car and train, which share numerous common properties (function, material, dynamic behavior, wheels, windows, etc.). Such anomalies also exist in other gold standards such as the MEN data set (Bruni et al. 2012a). As a consequence, these evaluations effectively penalize models for learning the evident truth that coffee and cup are dissimilar. Although clearly different, coffee and cup are very much related. The psychological literature refers to the conceptual relationship between these concepts as association, although it has been given a range of names including relatedness (Budanitsky and Hirst 2006; Agirre et al. 2009), topical similarity (Hatzivassiloglou et al. 2001), and domain similarity (Turney 2012). Association contrasts with similarity, the relation connecting cup and mug (Tversky 1977). At its strongest, the similarity relation is exempliﬁed by pairs of synonyms; words with identical referents. Computational models that effectively capture similarity as distinct from association have numerous applications. Such models are used for the automatic generation of dictionaries, thesauri, ontologies, and language correction tools (Biemann 2005; Cimiano, Hotho, and Staab 2005; Li et al. 2006). Machine translation systems, which aim to deﬁne mappings between fragments of different languages whose meaning is similar, but not necessarily associated, are another established application (He et al. 2008; Marton, Callison-Burch, and Resnik 2009). Moreover, since, as we establish, similarity is a cognitively complex operation that can require rich, structured conceptual knowledge to compute accurately, similarity estimation constitutes an effective proxy evaluation for general-purpose representation-learning models whose ultimate application is variable or unknown (Collobert and Weston 2008; Baroni and Lenci 2010). As we show in Section 2, the predominant gold standards for semantic evaluation in NLP do not measure the ability of models to reﬂect similarity. In particular, in both WS353 and MEN, pairs of words with associated meaning, such as coffee and cup (rating = 6.810), telephone and communication (7.510), or movie and theater (7.710), receive a high rating regardless of whether or not their constituents are similar. Thus, the utility of such resources to the development and application of similarity models is limited, a problem exacerbated by the fact that many researchers appear unaware of what their evaluation resources actually measure.1 Although certain smaller gold standards—those of Rubenstein and Goodenough (1965) (RG) and Agirre et al. (2009) (WS-Sim)—do focus clearly on similarity, these resources suffer from other important limitations. For instance, as we show, and as is also the case for WS-353 and MEN, state-of-the-art models have reached the average performance of a human annotator on these evaluations. It is common practice in NLP to deﬁne the upper limit for automated performance on an evaluation as the average human performance or inter-annotator agreement (Yong and Foo 1999; Cunningham 2005; Resnik and Lin 2010). Based on this established principle and the current evaluations, it would therefore be reasonable to conclude that the problem of representation learning, at least for similarity modeling, is approaching resolution. However, circumstantial evidence suggests that distributional models are far from perfect. For instance, we are some way from automatically generated dictionaries, thesauri, or ontologies that can be used with the same conﬁdence as their manually created equivalents. 
Although Prolog was intended to be used in building natural language processing (NLP) applications, the language has often been neglected in many modern NLP introductory courses and texts. Because of the popularity of machine learning and statistical approaches to language processing problems over the past decades, researchers and practitioners tend to use other modern programming languages such as C++ and Java for developing NLP applications. However, with a growing interest in semantic processing and knowledge base construction in recent years, researchers have found Prolog to be an indispensable tool for tasks such as searching databases and performing symbolic reasoning. We have already seen recent successful deployment of Prolog-based NLP systems in industry. An example would be the use of Prolog in IBM’s DeepQA project to express pattern-matching rules.1 Pierre M. Nugues’ comprehensive textbook Language Processing with Perl and Prolog provides a timely, in-depth introduction to the ﬁeld of NLP using Prolog and, to a lesser extent, Perl. This book is suitable for anyone wanting to enter NLP through learning to prototype NLP modules in Prolog. I approve of how Nugues has structured his textbook. It starts from ﬁrst principles, moves on to concepts such as words, syntax, and semantics, and concludes with a discussion on the more advanced and open topics of discourse and dialogues. Chapter 1 gives an overview of the ﬁeld of natural language processing. Chapters 2 and 3 then provide a technical discussion of corpus processing and data representations. Chapter 4 focuses on information theory and machine learning. The important concepts of entropy and perplexity are introduced ﬁrst. They are followed by a discussion on the learning of a decision tree model. One minor quibble is that general machine learning concepts such as supervised and unsupervised learning are presented under the subsection “Machine Learning” within “Entropy and Decision Trees.” A brief introduction to “Linear Models” is given before moving on to regression and classiﬁcation models, including perceptrons, support vector machines, and logistic regression. I liked the way that such concepts were elucidated—I thought the use of automatic language detection as an example to illustrate how the different models worked was a nice 
Deep Learning waves have lapped at the shores of computational linguistics for several years now, but 2015 seems like the year when the full force of the tsunami hit the major Natural Language Processing (NLP) conferences. However, some pundits are predicting that the ﬁnal damage will be even worse. Accompanying ICML 2015 in Lille, France, there was another, almost as big, event: the 2015 Deep Learning Workshop. The workshop ended with a panel discussion, and at it, Neil Lawrence said, “NLP is kind of like a rabbit in the headlights of the Deep Learning machine, waiting to be ﬂattened.” Now that is a remark that the computational linguistics community has to take seriously! Is it the end of the road for us? Where are these predictions of steamrollering coming from? At the June 2015 opening of the Facebook AI Research Lab in Paris, its director Yann LeCun said: “The next big step for Deep Learning is natural language understanding, which aims to give machines the power to understand not just individual words but entire sentences and paragraphs.”1 In a November 2014 Reddit AMA (Ask Me Anything), Geoff Hinton said, “I think that the most exciting areas over the next ﬁve years will be really understanding text and videos. I will be disappointed if in ﬁve years’ time we do not have something that can watch a YouTube video and tell a story about what happened. In a few years time we will put [Deep Learning] on a chip that ﬁts into someone’s ear and have an English-decoding chip that’s just like a real Babel ﬁsh.”2 And Yoshua Bengio, the third giant of modern Deep Learning, has also increasingly oriented his group’s research toward language, including recent exciting new developments in neural machine translation systems. It’s not just Deep Learning researchers. When leading machine learning researcher Michael Jordan was asked at a September 2014 AMA, “If you got a billion dollars to spend on a huge research project that you get to lead, what would you like to do?”, he answered: “I’d use the billion dollars to build a NASA-size program focusing on natural language processing, in all of its glory (semantics, pragmatics, etc.).” He went on: “Intellectually I think that NLP is fascinating, allowing us to focus on highly structured inference problems, on issues that go to the core of ‘what is thought’ but remain eminently practical, and on a technology ∗ Departments of Computer Science and Linguistics, Stanford University, Stanford CA 94305-9020, U.S.A. E-mail: manning@cs.stanford.edu. 
I want to thank the ACL for giving me the Lifetime Achievement Award of 2015. It is the appreciation of not only my work, but also of the work that my fellow researchers, my colleagues, and my students have done through all these years. It is an honor for all of us. As a veteran of NLP research, I am fortunate to witness and be a part of its long yet inspiring journey in China. So today, to everyone here, my friends, colleagues, and students, either well-known scientists or young researchers: I’d like to share my experience and thoughts with you. 1. Early Machine Translation in China The history of machine translation (MT) in China dates back to 1956. At that time the new country had immense construction projects to recover what had been ruined in the war. However, the government precisely recognized the signiﬁcance of machine translation, and started to explore this area, as the fourth country following the United States, the United Kingdom, and the Soviet Union. In 1959, Russian–Chinese machine translation was demonstrated on a Type-104 general-purpose computer made in China. This ﬁrst MT system had a dictionary of 2,030 entries, and 29 groups of rules for lexical analysis. Programmed by machine instructions, the system was able to translate nine different types of sentences. It used punched tape as the input, and the output was a special kind of code for Chinese characters, since there was no Chinese character output device at the time. As the pioneer in Chinese MT, the system touched the issues of word sense disambiguation, word reordering, and proposed the idea of predicate-focused sentence analysis and pivot language for multilingual translation. In the same year, machine translation research at the Harbin Institute of Technology (HIT) was started by Prof. Zhen Wang (and later Prof. Kaizhu Wang), focusing on the Russian–Chinese MT group. The pursuit for MT has never halted after these forerunners. doi:10.1162/COLI a 00240 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 4  2. The CEMT Series In 1960, I was admitted to HIT. Five years later, I graduated and became a faculty member in the computer department of HIT, which was probably the ﬁrst computer discipline among Chinese universities. I started my research, however, not from machine translation but from information retrieval (IR). I was fully occupied by how to effectively store books and documents on computers, and then retrieve them quickly and accurately. The start of my research in MT was incidentally caused by IR problems. At that time, Ming Zhou was my Ph.D. student. He is now the principal researcher of Natural Language Computing at Microsoft Research Asia (MSRA), and many of you may be acquainted with him. In 1985, at the beginning of his graduate study, he was aiming to address the topic of word extraction for Chinese documents to boost IR performance. For an exhaustive survey, Ming went to Beijing from Harbin alone, and buried himself at the National Library for over a month. He came back disappointed, ﬁnding that the related work was some language-dependent solutions for English. Actually, many research directions encountered this problem at that time. That’s why Ming and I decided to develop an MT system through which we could ﬁrst translate Chinese materials into English, so as to take advantage of the solutions proposed for English, and ﬁnally translate the results back into Chinese, if necessary. In those years, the translation from Chinese to other foreign languages was less studied in China. Everything was hard in the beginning. We had to build everything from scratch, such as collecting and inputting each entry of the translation dictionary. Fortunately, we were not alone. I came to know many peer scholars, including Prof. Weitian Wu, Zhiwei Feng, Prof. Zhendong Dong, Prof. Shiwen Yu, and Prof. Changning Huang, as well as Dr. Zhaoxiong Chen. Although we didn’t work together, we could always learn from each other and inspire each other in MT research. After three years’ effort, we accomplished a rule-based MT system named CEMT-I (Li et al. 1988). It ran on an IBM PC XT1 and was capable of translating eight kinds of Chinese sentence patterns with fewer than one thousand rules. It had a dictionary of 30,000 Chinese-English entries. Simple or even crude as it now seems, it really encouraged every member of our team. After that, we developed CEMT-II (Zhou et al. 1990) and CEMT-III (Zhao, Li, and Zhang 1995) successively. The CEMT series seemed to have a special kind of magic. Almost all the students who participated in these projects devoted themselves to machine translation in their following careers, including Ming Zhou, Min Zhang, and Tiejun Zhao. 3. DEAR and BT863 Inspired by the success of the CEMT series, we also developed a computer-aided translation system called “DEAR.” DEAR was put to market via a software chain store. Although it did not sell much, it was our ﬁrst effort to commercialize the MT technology. I still remember how excited I was when I saw DEAR placed on the shelves for the ﬁrst time. Today, it still reminds me that research work cannot just stay in the lab. Also in the 1980s, China’s NLP ﬁeld was marked by a milestone event: the establishment of the Chinese Information Processing Society of China (CIPS). From then on, NLP researchers throughout the country have been connected and the academic exchange has been facilitated at the national scale. It was far beyond my imagination then that,  
Charles University in Prague Aravind Joshi University of Pennsylvania Jane Robinson, a pioneering computational linguist, made major contributions to machine translation, natural language, and speech systems research programs at the RAND Corporation, IBM, and in the AI Center at SRI International. She served as ACL president in 1982. Jane became a computational linguist accidentally. She had a Ph.D. in history from UCLA, but could not obtain a faculty position in that ﬁeld because those were reserved for men. Instead, she took positions teaching English, ﬁrst at UCLA and then at California State College, Los Angeles. While at LA State, where she was tasked with teaching engineers how to write, Jane noticed an announcement for a talk on Chomsky’s transformational grammar. She went to the talk thinking this work on grammar might help her teach better. Although its subject matter did not match her expectations, the talk marked a turning point in her career. In the late 1950s, Jane became a consultant to the RAND Corporation group working on machine translation under Dave Hays (ACL president, 1964). From the beginning, Jane was concerned with identifying connections between different traditions in formal grammars and their corresponding detailed linguistic realizations. Her 1965 International Conference on Computational Linguistics (COLING) paper, “Endocentric constructions and the Cocke parsing logic” (Robinson 1965), is a beautiful example of connecting speciﬁc linguistic phenomena to parsing strategies in a way that preserves the nature of the linguistic phenomena, endocentric constructions. While at RAND, Jane became colleague and friend to many in the machine translation and emerging computational linguistics world, including Susumo Kuno (ACL president 1967), Martin Kay (ACL president, 1969), Joyce Friedman (ACL president, 1971), and Karen Sparck Jones (ACL president, 1994). In the late 1960s, Jane moved to the Automata Theory and Computability Group at the IBM Thomas J. Watson Research Center in Yorktown Heights, NY. She used her knowledge of formal work on grammars and parsing to draw correspondences between Dependency Grammars and Phrase Structure Grammars. Although Jane came ∗ E-mail: grosz@eecs.harvard.edu. doi:10.1162/COLI a 00235 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 4 
Jonathan Chevelu∗ IRISA, University of Rennes 1 Arnaud Delhay∗ IRISA, University of Rennes 1 Linguistic corpus design is a critical concern for building rich annotated corpora useful in different domains of applications. For example, speech technologies such as ASR (Automatic Speech Recognition) or TTS (Text-to-Speech) need a huge amount of speech data to train datadriven models or to produce synthetic speech. Collecting data is always related to costs (recording speech, verifying annotations, etc.), and as a rule of thumb, the more data you gather, the more costly your application will be. Within this context, we present in this article solutions to reduce the amount of linguistic text content while maintaining a sufﬁcient level of linguistic richness required by a model or an application. This problem can be formalized as a Set Covering Problem (SCP) and we evaluate two algorithmic heuristics applied to design large text corpora in English and French for covering phonological information or POS labels. The ﬁrst considered algorithm is a standard greedy solution with an agglomerative/spitting strategy and we propose a second algorithm based on Lagrangian relaxation. The latter approach provides a lower bound to the cost of each covering solution. This lower bound can be used as a metric to evaluate the quality of a reduced corpus whatever the algorithm applied. Experiments show that a suboptimal algorithm like a greedy algorithm achieves good results; the cost of its solutions is not so far from the lower bound (about 4.35% for 3-phoneme coverings). Usually, constraints in SCP are binary; we proposed here a generalization where the constraints on each covering feature can be multi-valued. ∗ IRISA, University of Rennes 1, 6 rue de Kerampont, Lannion 22300, France. E-mail: {nelly.barbot, olivier.bo¨effard, jonathan.chevelu, arnaud.delhay}@irisa.fr. This work is partially supported by the French National Research Agency (ANR) in the framework of the project Phorevox. Submission received: 7 May 2013; revised submission received: 20 November 2014; accepted for publication: 18 March 2015. doi:10.1162/COLI a 00225 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 3  1. Introduction In automatic speech and language processing, many technologies make extensive use of written or read text sets. These linguistic corpora are a necessity to train models or to extract rules, and the quality of the results strongly depends on a corpus’ content. Often, the reference corpus should provide a maximum diversity of content. For example, in Tian, Nurminen, and Kiss (2005), and Tian and Nurminen (2009), it turns out that maximizing the text coverage of the learning corpus improves an automatic syllabiﬁcation based on a neural network. Similarly, a high quality speech synthesis system based on the selection of speech units requires a rich corpus in terms of diphones, diphones in context, triphones, and prosodic markers. In particular, Bunnell (2010) shows the importance of a good coverage of diphones and triphones for the intelligibility of a voice produced by a unit selection speech synthesis system. To cover the best attributes needed for a task, several strategies are then possible. A ﬁrst method—very simple—is to collect text randomly, but it soon becomes expensive because of the natural distribution of linguistic events following the Zipf’s law. Very few events are extremely frequent and many events are very rare. This problem is often made difﬁcult by the fact that many technologies require several variants of the same event (as in a Text-to-Speech [TTS] system using several acoustical versions of the same phonological unit). Usually, a large volume of data needs to be collected. However, and depending on the applications, building such corpora is often achieved under a constraint of parsimony. As an example, for a TTS system, a high-quality synthetic voice generally needs a huge number of speech recordings. But minimizing the duration of a recording is also a critical point to ensure uniform quality of the voice, to reduce the drudgery of the recording, to reduce the ﬁnancial cost, or to follow a technical constraint on the amount of collected data for embedded systems. Moreover, a reduced set tends to limit the need of human implication for checking the data (transcription and annotation). Similarly, in the natural language processing ﬁeld (NLP), the adaptation of a generic model to a speciﬁc domain often requires new annotated data that illustrate its speciﬁcities (as in Candito, Anguiano, and Seddah 2011). However, the creation cost of such data highly depends on the kind of labels used to adapt the model. In particular, the annotation in syntax trees is really more expensive than in Part-of-Speech (POS) tags. Then, it could be more efﬁcient to annotate a compact corpus that reﬂects the phenomena variability than a corpus with a natural distribution of events, which implies many redundancies (see Neubig and Mori 2010). In a machine learning framework, the active learning strategy can be used as an alternative that reduces the manual data annotation effort to design the training corpus without diminishing the quality of the model to train (see Settles 2010 or Schein, Sandler, and Ungar 2004). It consists of building the corpus iteratively by choosing an item according to an external source of information (a user or an experimental measure). This approach has been applied in NLP, speech recognition, and spoken language understanding (see for instance Tomanek and Olsson 2009 and Gotab, Be´chet, and Damnati 2009). A second alternative, when no direct quality measure is available, consists of covering a large set of attributes that may impact the ﬁnal quality (after annotation or recording). This kind of approach might also be preferred when the ﬁnal corpus is built in one batch (for instance, because of out-sourcing or annotator/performer consistency constraints). A method could be an automatic extraction from a huge text corpus of a minimal sized subset that covers the identiﬁed attributes. This  356  Barbot et al.  Large Linguistic Corpus Reduction  problem is a generalization of the Set-Covering Problem (SCP), which is an NPhard problem, as shown in Karp (1972). It is then necessary to use heuristics or sub-optimal algorithms for a reasonable computation time. Moreover, Raz and Safra (1997) and Alon, Moshkovitz, and Safra (2006) have shown that the SCP cannot be polynomially approximated with ratio c × ln(n) unless P = NP, when c is a constant, and n refers to the size of the universe to cover. That means that one cannot be certain to obtain a result under this ratio with any polynomial algorithm. However, the latter complexity results are given for any kind of distribution in the mono-representation case. One can ask if good multi-represented coverages can be achieved efﬁciently on data following Zipf’s law, which is usual in the domain of NLP. Within the ﬁeld of speech processing, the most frequently used strategy is a greedy method based on an agglomeration policy. This iterative algorithm selects the sentence with the highest score at each iteration. The score reﬂects the contribution of the sentence to the covering under construction. In Gauvain, Lamel, and Eske´nazi (1990), this methodology has been applied to build a database of read speech from a text corpus for the evaluation of speech recognition systems using hierarchically organized covering attributes. Van Santen and Buchsbaum (1997) have tested different variants of greedy selection of texts by varying the units to cover (diphones, duration, etc.) and the “scores” for a sentence depending on the considered applications. In Tian, Nurminen, and Kiss (2005), the learning corpus for an automatic system of syllabiﬁcation is designed using a greedy approach with the Levenshtein distance as a score function in order to maximize its text diversity. In Franc¸ois and Boe¨ffard (2001), the methodology gives a priority to the rarest categories of allophones. The latter methodology has been implemented for the deﬁnition of the multi-speaker corpus Neologos in Krstulovic´ et al. (2006). In the article of Krul et al. (2006), the authors constructed a corpus where the distribution of diphonemes/triphonemes matches a uniform distribution. A greedy algorithm is led by a score function based on the Kullback-Liebler divergence. A similar method is used in Krul et al. (2007) to design a reduced database in accordance with a speciﬁc domain distribution. Kawai et al. (2000) propose a pair exchange mechanism that Rojc and Kacˇicˇ (2000) apply after a ﬁrst reverse greedy algorithm—also called spitting greedy—deleting the useless sentences. In Cadic, Boidin, and d’Alessandro (2010), the covering of “sandwich” units (deﬁned to be more adapted to corpus-based speech synthesis) is carried out by generating new sentences in a semi-automatic way. Candidates are generated using ﬁnite state transducers. The sentences are ordered according to a greedy criterion (their sandwiches richness) and presented to a human evaluator. This collection of artiﬁcial and rich sentences enables an effective reduction of the size of the covering but requires expensive human intervention to obtain semantically correct sentences that will be therefore easier to record. The results of these previously cited studies are difﬁcult to compare because of the different initial corpora and covering constraints (partial or full covering) and evaluation criteria (the number of gathered sentences, the Kullback divergence, etc.). In Zhang and Nakamura (2008), a priority policy for the rare units is added into an agglomerative greedy algorithm in order to get a covering of triphoneme classes from a large text corpus in Chinese language. The results show that this priority policy driven by the score function and the phonetic content of the sentences reduces the covering size compared with a standard agglomerative greedy algorithm. Similarly, in Franc¸ois and Boe¨ffard (2002), several combinations of greedy algorithms (agglomeration, spitting, pair exchange, or priority to rare units) were applied 357  Computational Linguistics  Volume 41, Number 3  to the construction of a corpus for speech synthesis in French containing at least three representatives of the most frequent diphones. Based on this work, the best strategy would be the application of an agglomerative greedy followed by a spitting greedy algorithm. During the agglomeration phase, the score of a sentence corresponds to the number of its unit instances that remain to be covered normalized by its length. During the spitting phase, at each iteration, the longest redundant sentence is removed from the covering. This algorithm is called the Agglomeration and Spitting Algorithm (ASA). As an alternative to a greedy algorithm, which is sub-optimal, solving the SCP using Lagrangian relaxation principles can provide an exact solution for problems of reasonable size. However, for speech processing, the SCP has several millions of sentences with tens of thousands of covering features. Considering these practical constraints, Chevelu et al. (2007) adapted a Lagrangian relaxation based algorithm proposed by Caprara, Fischetti, and Toth (1999). In the context of Italian railways, Caprara, Fischetti, and Toth proposed heuristics to solve scheduling problems and won a competition, called Faster, organized by the Italian Operational Research Society in 1994, ahead of other Lagrangian relaxation heuristics–based algorithms, like Ceria, Nobili, and Sassano (1998). In Chevelu et al. (2007, 2008), the algorithm takes into account the constraints of multi-representation. A minimal number of representatives for the same unit may be required. The proposed algorithm, called LamSCP — Lagrangian-based Algorithm for Multi-represented SCP — is applied to extract coverings of diphonemes with a mono- or a 5-representation and coverings of triphonemes with mono-representation constraints. These results are compared with the greedy strategy ASA and are about 5% to 10% better. Besides, the LamSCP provides a lower bound for the cost of the optimal covering and allows for evaluating the quality of the results. In Barbot, Boe¨ffard, and Delhay (2012), phonological content of diphoneme coverings is studied regarding many parameters. These coverings are obtained by different algorithms (LamSCP, ASA, greedy based on the Kullback divergence) and some of the coverings are randomly completed to reach a given size (from 20,000 to 30,000 phones). It turns out that the coverings obtained using LamSCP and ASA provide a good representation of short units and the representation of long units mainly depends on the length of the corpus. In this article, we present in more detail the LamSCP algorithm and its score functions and heuristics that take into account multi-representation constraints. We deepen the study about the performance of LamSCP for the construction of a phonologically rich corpus according to the size of the search space. We evaluate LamSCP and ASA algorithms on a corpus of sentences in English for a covering of multi-represented diphones, where the minimal number of required unit representatives varies from one to ﬁve. We also compare them in the case of very constrained triphoneme coverings in English and French, which represent about 12 times more units to cover. Additionally, both algorithms are tested to provide multi-represented coverings of POS tags in order to assess their ability to deal with different kinds of linguistic data. A particular effort has been made on methodology to obtain comparable measures, to study the stability of both algorithms, and to establish conﬁdence intervals for each solution. This article is organized as follows. In Section 2, the SCP framework and the associated notations are introduced. The ASA algorithm is described in Section 3 and the LamSCP is detailed in Section 4. The experimental methodology is presented in Section 5 and results are discussed in Section 6. Before concluding in Section 8, we present experiments in the context of TTS where we evaluate on that task the beneﬁts of a reduction in section 7. 358  Barbot et al.  Large Linguistic Corpus Reduction  2. The Set-Covering Problem  Before describing the SCP-solving algorithms proposed in this article, we introduce in this section some notations and the Lagrangian properties used by LamSCP. Let us consider a corpus A composed of n sentences s1, . . . , sn. According to the target applications, these sentences are annotated with respect to phonological, acoustic, prosodic attributes, and so forth. Each sentence is then associated with a family of units of different types. The set of units present in A is denoted U = {u1, . . . , um} and A can be represented by a matrix A = (aij), where aij is the number of instances of unit ui in the sentence sj. Therefore, the j th column of A corresponds to sentence sj in A. To simplify the writing, we deﬁne the sets M = {1, . . . , m} and N = {1, . . . , n}. For a given vector of integers B = (b1, . . . , bm)T, a reduction X of A, also called covering of U , is deﬁned as a subset of A which contains, for every i ∈ M, at least bi instances of ui. It can be described by a vector X = (x1, . . . , xn)T where xj = 1 if sj belongs to X and xj = 0 otherwise. In other words, a covering is a solution X ∈ {0, 1}n of the following system  ∑  ∀i ∈ M, aijxj ≥ bi  (1)  j∈N  that is, AX ≥ B where B is called the constraint vector. Our aim is to optimize a covering according to a cost function minimization crite- rion. The covering cost is given by summing the costs of the sentences that compose the covering. The optimization problem can be formulated as the following SCP:  X∗ = arg min CX  (2)  X∈{0,1}n  AX≥B  where C = (c1, . . . , cn) is the cost vector and cj the cost of the sentence sj. Because of the objective to minimize the total length of the covering, we have chosen to deﬁne the cost of a sentence as one of its length features. According to the considered application, the sentence cost can be deﬁned as its number of phones (one of our objectives is to design a phonetically rich script with a minimal speech recording duration), or its number of words, part-of-speech tags, breath groups, and so on. In Caprara, Fischetti, and Toth (1999), Caprara, Toth, and Fischetti (2000), and Ceria, Nobili, and Sassano (1998), the studied crew scheduling problem is a particular case of Equation (2) where A is a binary matrix and B = 1Rm (i.e., with mono-representation constraints). In order to ensure that Equation (1) admits a solution, we assume that, for each i ∈ M, the minimal number bi of ui instances required in the covering is not greater than the number (A1Rn )i of ui instances in A, that is A1Rn ≥ B. Under this assumption, A is the maximal size solution of Equation (1), represented by X = 1Rn. In the case where bi is greater than the number of ui instances in A, bi is set to (A1Rn )i. To drive the SCP algorithms during the sentence selection phase, the covering capacity µj of sentence sj is deﬁned as the number of its unit instances required in the covering in view of the constraint vector:  ∑{}  µj = min aij, bi  (3)  i∈M  359  Computational Linguistics  Volume 41, Number 3  Let us notice that µj does not consider the excess unit instances: For example, if sj contains aij = 10 instances of ui and at least bi = 3 instances of ui are required, the contribution of ui to µj derivation only takes into account three instances of ui. 3. Greedy Algorithm ASA  In this section, the two main steps that compose the algorithm ASA are brieﬂy described. First, an agglomerative greedy procedure is applied to A so as to derive a covering. Next, a spitting greedy procedure reduces this covering in order to approach the optimal solution of Equation (2).  3.1 Agglomerative Greedy Strategy  The greedy strategy builds a sub-optimal solution to the SCP Equation (2) in an iterative way. At each iteration, the lowest cost sentence is chosen from A. If several sentences correspond to the lowest cost, the one coming ﬁrst (i.e., the one with the lowest index) is chosen. Initially, the set of selected sentences X is empty, the matrix A˜ associated with the candidate sentences is assigned to A, the current covering capacity of sj is given by µ˜ j = µj, and the current constraint vector B˜ = B. The cost of sentence sj is deﬁned by  {  σj =  cj/µ˜ j if µ˜ j ̸= 0 ∞ otherwise  (4)  Indeed, if µ˜ j = 0, it turns out that sj does not cover any unit missing in the solution X  under construction and its inﬁnite cost σj avoids its selection.  At each content of s,  iteration, the B˜ is updated  steolemctaexd{sB˜en−teAn˜∆ce,  s is}added to X 0Rm where the  . Taking jth entry  into account of ∆ equals  the 1 if  sj = s and 0 otherwise. Next, the associated column of s in A˜ is set to 0Rm . For each  sentence sj with a non-zero µ˜ j feature, µ˜ j is then updated using A˜ and B˜ in Equation (3).  At last, the agglomerative greedy algorithm is stopped as soon as all the constraints  are satisﬁed, that is, B˜ = 0Rm .  3.2 Spitting Greedy Strategy  The spitting greedy strategy also consists in building iteratively a sub-optimal solution Y to Equation (2) by reducing the size of a covering. The initial covering Y is set to the solution X derived by the agglomerative phase described earlier. At each iteration, the set of the redundant sentences of Y is calculated and the costliest one (according to the cost function C) is removed from Y. An element s of Y is said to be redundant if for each ui ∈ U, its number of instances into Y, denoted mi(Y ), and into Y \ {s}, denoted mi(Y \ {s}), check min {mi(Y ), bi} = min {mi(Y \ {s}), bi}. In other words, s is a redundant element of the covering Y if Y \ {s} is also a covering solution of Equation (1). The spitting greedy algorithm stops when the redundant sentence set is empty.  4. Lagrangian Relaxation Based–Algorithm  This section describes the main phases of the algorithm called LamSCP. This algorithm takes advantage of the Lagrangian relaxation properties reviewed herein in order to approach the optimal solution of Equation (2) as close as possible. Strongly inspired by  360  Barbot et al.  Large Linguistic Corpus Reduction  Caprara, Fischetti, and Toth (1999), but generalized to the multi-representation problem, this algorithm provides a lower bound of the optimal solution cost. Having such information is very useful for assessing the achievements of the SCP algorithms.  4.1 Lagrangian Relaxation Principles Let us brieﬂy recall the main principles of Lagrangian relaxation on which LamSCP is based to solve Equation (2) (see Fisher [1981] for more details on Lagrangian relaxation). First, the Lagrangian function associated with Equation (2) is deﬁned by  L(X, Λ) = CX + ΛT(B − AX) = ΛTB + C(Λ)X  (5)  where Λ ∈ (R+)m, X ∈ {0, 1}n, and C(Λ) = C − ΛTA. The coordinates of Λ =  (λ1, . . . , λm)T are called Lagrangian multipliers and can be interpreted as a weighting  of constraints (1). The jth entry of C(Λ), called Lagrangian cost cj(Λ) of sentence sj, takes  into account its cost cj every covering X and  and the every Λ  ∈ad(eRq+u)amcy, tohfeiLtsacgormanpgoiasintifounntcotioadndsraetsissﬁEeqsuLa(tXio, Λn)(2≤).  For CX.  Thus, the dual Lagrangian function deﬁned by  L(Λ) = min L(X, Λ)  (6)  X∈{0,1}n  presents the following fundamental property: For every Λ ∈ Rm+ and every covering X, we have L(Λ) ≤ CX. Hence, L(Λ) is a lower bound of the minimal covering cost, CX∗, but does not necessarily correspond to the cost of a covering. In order to compute L(Λ), an acceptable solution for the vector X minimizing L(X, Λ) is X(Λ) = (x1(Λ), . . . , xn(Λ))T where  {  xj(Λ) =  
Raymond T. Ng† University of British Columbia Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts. Rhetorical analysis seeks to uncover this coherence structure. In this article, we present CODRA— a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse. CODRA comprises a discourse segmenter and a discourse parser. First, the discourse segmenter, which is based on a binary classiﬁer, identiﬁes the elementary discourse units in a given text. Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing. We present two approaches to combine these two stages of parsing effectively. By conducting a series of empirical evaluations over two different data sets, we demonstrate that CODRA signiﬁcantly outperforms the state-of-the-art, often by a wide margin. We also show that a reranking of the k-best parse hypotheses generated by CODRA can potentially improve the accuracy even further. 1. Introduction A well-written text is not merely a sequence of independent and isolated sentences, but instead a sequence of structured and related sentences, where the meaning of a sentence relates to the previous and the following ones. In other words, a well-written ∗ Arabic Language Technologies, Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar. E-mail: sjoty@qf.org.qa. ∗∗ Computer Science Department, University of British Columbia, Vancouver, BC, Canada, V6T 1Z4. E-mail: carenini@cs.ubc.ca. † Computer Science Department, University of British Columbia, Vancouver, BC, Canada, V6T 1Z4. E-mail: rng@cs.ubc.ca. Submission received: 11 May 2014; revised version received: 29 January 2015; accepted for publication: 18 March 2015. doi:10.1162/COLI a 00226 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law.  Computational Linguistics  Volume 41, Number 3  text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneﬁcial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daume´ and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzma´n et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is driven by sentence semantics. Webber (2004) and Danlos (2009) extend sentence grammar to formalize discourse structure. Rhetorical Structure Theory (RST), proposed by Mann and Thompson (1988), is perhaps the most inﬂuential theory of discourse in computational linguistics. Although it was initially intended to be used in text generation, later it became popular as a framework for parsing the structure of a text (Taboada and Mann 2006). RST represents texts by labeled hierarchical structures, called Discourse Trees (DTs). For example, consider the DT shown in Figure 1 for the following text: But he added: “Some people use the purchasers’ index as a leading indicator, some use it as a coincident indicator. But the thing it’s supposed to measure—manufacturing strength—it missed altogether last month.” The leaves of a DT correspond to contiguous atomic text spans, called elementary discourse units (EDUs; six in the example). EDUs are clause-like units that serve as building blocks. Adjacent EDUs are connected by coherence relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by internal nodes), which in turn are also subject to this relation linking. Discourse units linked by a rhetorical relation are  Attribution  But he added: (1)  Contrast  Contrast  Same-Unit  "Some people use the purchasers’ index as a leading indicator, (2)  some use it as a coincident indicator. (3)  Elaboration  But the thing it’s supposed to measure (4)  -- manufacturing strength -- (5)  it missed altogether last month." <P> (6)  Figure 1 Discourse tree for two sentences in RST–DT. Each sentence contains three EDUs. Horizontal lines indicate text segments; satellites are connected to their nuclei by curved arrows and two nuclei are connected with straight lines.  386  Joty, Carenini, and Ng  CODRA  further distinguished based on their relative importance in the text: nuclei are the core parts of the relation and satellites are peripheral or supportive ones. For example, in Figure 1, Elaboration is a relation between a nucleus (EDU 4) and a satellite (EDU 5), and Contrast is a relation between two nuclei (EDUs 2 and 3). Carlson, Marcu, and Okurowski (2002) constructed the ﬁrst large RST-annotated corpus (RST–DT) on Wall Street Journal articles from the Penn Treebank. Whereas Mann and Thompson (1988) had suggested about 25 relations, the RST–DT uses 53 mono-nuclear and 25 multi-nuclear relations. The relations are grouped into 16 coarse-grained categories; see Carlson and Marcu (2001) for a detailed description of the relations. Conventionally, rhetorical analysis in RST involves two subtasks: discourse segmentation is the task of breaking the text into a sequence of EDUs, and discourse parsing is the task of linking the discourse units (EDUs and larger units) into a labeled tree. In this article, we use the terms discourse parsing and rhetorical parsing interchangeably. While recent advances in automatic discourse segmentation have attained high accuracies (an F-score of 90.5% reported by Fisher and Roark [2007]), discourse parsing still poses signiﬁcant challenges (Feng and Hirst 2012) and the performance of the existing discourse parsers (Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) is still considerably inferior compared with the human gold standard. Thus, the impact of rhetorical structure in downstream NLP applications is still very limited. The work we present in this article aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing discourse parsers. First, existing discourse parsers typically model the structure and the labels of a DT separately, and also do not take into account the sequential dependencies between the DT constituents. However, for several NLP tasks, it has recently been shown that joint models typically outperform independent or pipeline models (Murphy 2012, page 687). This is also supported in a recent study by Feng and Hirst (2012), in which the performance of a greedy bottom–up discourse parser improved when sequential dependencies were considered by using gold annotations for the neighboring (i.e., previous and next) discourse units as contextual features in the parsing model. To address this limitation of existing parsers, as the ﬁrst contribution, we propose a novel discourse parser based on probabilistic discriminative parsing models, expressed as Conditional Random Fields (CRFs) (Sutton, McCallum, and Rohanimanesh 2007), to infer the probability of all possible DT constituents. The CRF models effectively represent the structure and the label of a DT constituent jointly, and, whenever possible, capture the sequential dependencies. Second, existing discourse parsers typically apply greedy and sub-optimal parsing algorithms to build a DT. To cope with this limitation, we use the inferred (posterior) probabilities from our CRF parsing models in a probabilistic CKY-like bottom–up parsing algorithm (Jurafsky and Martin 2008), which is non-greedy and optimal. Furthermore, a simple modiﬁcation of this parsing algorithm allows us to generate k-best (i.e., the k highest probability) parse hypotheses for each input text that could then be used in a reranker to improve over the initial ranking using additional (global) features of the discourse tree as evidence, a strategy that has been successfully explored in syntactic parsing (Charniak and Johnson 2005; Collins and Koo 2005). Third, most of the existing discourse parsers do not discriminate between intrasentential parsing (i.e., building the DTs for the individual sentences) and multisentential parsing (i.e., building the DT for the whole document). However, we argue that distinguishing between these two parsing conditions can result in more effective parsing. Two separate parsing models could exploit the fact that rhetorical relations 387  Computational Linguistics  Volume 41, Number 3  are distributed differently intra-sententially versus multi-sententially. Also, they could independently choose their own informative feature sets. As another key contribution of our work, we devise two different parsing components: one for intra-sentential parsing, the other for multi-sentential parsing. This provides for scalable, modular, and ﬂexible solutions that can exploit the strong correlation observed between the text structure (i.e., sentence boundaries) and the structure of the discourse tree. In order to develop a complete and robust discourse parser, we combine our intrasentential and multi-sentential parsing components in two different ways. Because most sentences have a well-formed discourse sub-tree in the full DT (e.g., the second sentence in Figure 1), our ﬁrst approach constructs a DT for every sentence using our intrasentential parser, and then runs the multi-sentential parser on the resulting sentencelevel DTs to build a complete DT for the whole document. However, this approach would fail in those cases where discourse structures violate sentence boundaries, also called “leaky” boundaries (Vliet and Redeker 2011). For example, consider the ﬁrst sentence in Figure 1. It does not have a well-formed discourse sub-tree because the unit containing EDUs 2 and 3 merges with the next sentence and only then is the resulting unit merged with EDU 1. Our second approach, in order to deal with these leaky cases, builds sentence-level sub-trees by applying the intra-sentential parser on a sliding window covering two adjacent sentences and by then consolidating the results produced by overlapping windows. After that, the multi-sentential parser takes all these sentence-level sub-trees and builds a full DT for the whole document. Our discourse parser assumes that the input text has already been segmented into elementary discourse units. As an additional contribution, we propose a novel discriminative approach to discourse segmentation that not only achieves state-of-theart performance, but also reduces time and space complexities by using fewer features. Notice that the combination of our segmenter with our parser forms a COmplete probabilistic Discriminative framework for Rhetorical Analysis (CODRA). Whereas previous systems have been tested on only one corpus, we evaluate our framework on texts from two very different genres: news articles and instructional howto manuals. The results demonstrate that our approach to discourse parsing provides consistent and statistically signiﬁcant improvements over previous methods both at the sentence level and at the document level. The performance of our ﬁnal system compares very favorably to the performance of state-of-the-art discourse parsers. Finally, the oracle accuracy computed based on the k-best parse hypotheses generated by our parser demonstrates that a reranker could potentially improve the accuracy further. After discussing related work in Section 2, we present our rhetorical analysis framework in Section 3. In Section 4, we describe our discourse parser. Then, in Section 5 we present our discourse segmenter. The experiments and analysis of results are presented in Section 6. Finally, we summarize our contributions with future directions in Section 7. 2. Related Work Rhetorical analysis has a long history—dating back to Mann and Thompson (1988), when RST was initially proposed as a useful linguistic method for describing natural texts, to more recent attempts to automatically extract the rhetorical structure of a given text (Hernault et al. 2010). In this section, we provide a brief overview of the computational approaches that follow RST as the theory of discourse, and that are related to our work; see the survey by Stede (2011) for a broader overview that also includes other theories of discourse. 388  Joty, Carenini, and Ng  CODRA  2.1 Unsupervised and Rule-Based Approaches Although the most effective approaches to rhetorical analysis to date rely on supervised machine learning methods trained on human-annotated data, unsupervised methods have also been proposed, as they do not require human-annotated data and can be more easily applied to new domains. Often, discourse connectives like but, because, and although convey clear information on the kind of relation linking the two text segments. In his early work, Marcu (2000a) presented a shallow rule-based approach relying on discourse connectives (or cues) and surface patterns. He used hand-coded rules, derived from an extensive corpus study, to break the text into EDUs and to build DTs for sentences ﬁrst, then for paragraphs, and so on. Despite the fact that this work pioneered the ﬁeld of rhetorical analysis, it has many limitations. First, identifying discourse connectives is a difﬁcult task on its own, because (depending on the usage), the same phrase may or may not signal a discourse relation (Pitler and Nenkova 2009). For example, but can either signal a Contrast discourse relation or can simply perform non-discourse acts. Second, discourse segmentation using only discourse connectives fails to attain high accuracy (Soricut and Marcu 2003). Third, DT structures do not always correspond to paragraph structures; for example, Sporleder and Lapata (2004) report that more than 20% of the paragraphs in the RST–DT corpus (Carlson, Marcu, and Okurowski 2002) do not correspond to a discourse unit in the DT. Fourth, discourse cues are sometimes ambiguous; for example, but can signal Contrast, Antithesis and Concession, and so on. Finally, a more serious problem with the rule-based approach is that often rhetorical relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu and Echihabi (2002) found that only 61 out of 238 Contrast relations and 79 out of 307 Cause–Explanation relations were explicitly signaled by cue phrases. In the British National Corpus, Sporleder and Lascarides (2008) report that half of the sentences lack a discourse cue. Other studies (Schauer and Hahn 2001; Stede 2004; Taboada 2006; Subba and Di-Eugenio 2009) report even higher ﬁgures: About 60% of discourse relations are not explicitly signaled. Therefore, rather than relying on hand-coded rules based on discourse cues and surface patterns, recent approaches use machine learning techniques with a large set of informative features. While some rhetorical relations need to be explicitly signaled by discourse cues (e.g., Concession) and some do not (e.g., Background), there is a large middle ground of relations that may be signaled or not. For these “middle ground” relations, can we exploit features present in the signaled cases to automatically identify relations when they are not explicitly signaled? The idea is to use unambiguous discourse cues (e.g., although for Contrast, for example for Elaboration) to automatically label a large corpus with rhetorical relations that could then be used to train a supervised model.1 A series of previous studies have explored this idea. Marcu and Echihabi (2002) ﬁrst attempted to identify four broad classes of relations: Contrast, Elaboration, Condition, and Cause–Explanation–Evidence. They used a naive Bayes classiﬁer based on word pairs (w1, w2), where w1 occurs in the left segment, and w2 occurs in the right segment. Sporleder and Lascarides (2005) included other features (e.g., words and their stems, Part-of-Speech [POS] tags, positions, segment lengths) in a boosting-based classiﬁer (i.e., BoosTexter [Schapire and Singer 2000]) to further improve relation classiﬁcation accuracy. However, these studies evaluated classiﬁcation performance on the instances  
Jean-Philippe Me´tivier∗ Universite´ de Caen Basse-Normandie, GREYC, CNRS, UMR 6072 Agreement measures have been widely used in computational linguistics for more than 15 years to check the reliability of annotation processes. Although considerable effort has been made concerning categorization, fewer studies address unitizing, and when both paradigms are combined even fewer methods are available and discussed. The aim of this article is threefold. First, we advocate that to deal with unitizing, alignment and agreement measures should be considered as a uniﬁed process, because a relevant measure should rely on an alignment of the units from different annotators, and this alignment should be computed according to the principles of the measure. Second, we propose the new versatile measure γ, which fulﬁlls this requirement and copes with both paradigms, and we introduce its implementation. Third, we show that this new method performs as well as, or even better than, other more specialized methods devoted to categorization or segmentation, while combining the two paradigms at the same time. 1. Introduction A growing body of work in computational linguistics (CL hereafter) or natural language processing manifests an interest in corpus studies, and requires reference annotations for system evaluation or machine learning purposes. The question is how to ensure that an annotation can be considered, if not as the “truth,” than at least as a suitable ∗ Normandie University, France; UNICAEN, GREYC, F-14032 Caen, France; CNRS, UMR 6072, F-14032 Caen, France. E-mail: {yann.mathet, antoine.widlocher, jean-philippe.metivier}@unicaen.fr Submission received: 15 July 2013; revised version received: 5 October 2014; accepted for publication: 12 January 2015. doi:10.1162/COLI a 00227 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 3  reference. For some simple and systematic tasks, domain experts may be able to annotate texts with almost total conﬁdence, but this is generally not the case when no expert is available, or when the tasks become harder. The very notion of “truth” may even be utopian when the annotation process includes a certain degree of interpretation, and we should in such cases look for a consensus, also called the “gold standard,” rather than for the “truth.” For these reasons, a classic strategy for building annotated corpora with sufﬁcient conﬁdence is to give the same annotation task to several annotators, and to analyze to what extent they agree in order to assess the reliability of their annotations. This is the aim of inter-annotator agreement measures. It is important to point out that most of these measures do not evaluate the distance from annotations to the “truth,” but rather the distance across annotators. Of course, the hope is that the annotators will agree as far as possible, and it is usually considered that a good inter-annotator agreement ensures the constancy and the reproducibility of the annotations: When agreement is high, then the task is consistent and correctly deﬁned, and the annotators can be expected to agree on another part of the corpus, or at another time, and their annotations therefore constitute a consensual reference (even if, as shown for example by Reidsma and Carletta [2008], such an agreement is not necessarily informative for machine learning purposes). Moreover, once several annotators reach good agreement on a given part of a corpus, then each of them can annotate alone other parts of the corpus with great conﬁdence in the reproducibility (see the preface to Gwet [2012, page 6] for illuminating considerations). Consequently, inter-annotator agreement measurement is an important point for all annotation efforts because it is often considered that a given agreement value provided by a given method validates or invalidates the consistency of an annotation effort. How to measure agreement, and how we deﬁne a good measure, is another part of the problem. There is no universal answer, because how to measure depends on the nature of the task, hence on the kind of annotations. Admittedly, much work has already been done for some kinds of annotation efforts, namely, when annotators have to choose a category for previously identiﬁed entities. This approach, which we will call pure categorization, has led to several well-known and widely discussed coefﬁcients such as κ, π, or α, since the 1950s. Some more recent efforts have been made in the domain of unitizing, following Krippendorff’s terminology (Krippendorff 2013), where annotators have to identify by themselves what the elements to be annotated in a text are, and where they are located. Studies are scarce, however, as Krippendorff pointed out: “Measuring the reliability of unitizing has been largely ignored in favor of coding predeﬁned units” (Krippendorff 2013, page 310). This scarcity concerns either segmentation, where annotators simply have to mark boundaries in texts to separate contiguous segments, or more generally unitizing, where gaps may exist between units. Moreover, some even more complex conﬁgurations may occur (overlapping or embedding units), which are more rarely taken into account. And when categorization meets unitizing, as is the case in CL in such ﬁelds as, for example, NAMED ENTITY RECOGNITION1 or DISCOURSE FRAMING, very few methods are proposed and discussed. That is the main problem we focus on in this article and to which γ provides solutions.  
This article presents a mathematical and empirical veriﬁcation of computational constancy measures for natural language text. A constancy measure characterizes a given text by having an invariant value for any size larger than a certain amount. The study of such measures has a 70-year history dating back to Yule’s K, with the original intended application of author identiﬁcation. We examine various measures proposed since Yule and reconsider reports made so far, thus overviewing the study of constancy measures. We then explain how K is essentially equivalent to an approximation of the second-order Re´nyi entropy, thus indicating its signiﬁcation within language science. We then empirically examine constancy measure candidates within this new, broader context. The approximated higher-order entropy exhibits stable convergence across different languages and kinds of text. We also show, however, that it cannot identify authors, contrary to Yule’s intention. Lastly, we apply K to two unknown scripts, the Voynich manuscript and Rongorongo, and show how the results support previous hypotheses about these scripts. 1. Introduction A constancy measure for a natural language text is deﬁned, in this article, as a computational measure that converges to a value for a certain amount of text and remains invariant for any larger size. Because such a measure exhibits the same value for any size of text larger than a certain amount, its value could be considered as a text characteristic. The concept of such a text constancy measure was introduced by Yule (1944) in the form of his measure K. Since Yule, there has been a continuous quest for such measures, and various formulae have been proposed. They can be broadly categorized into three types, namely, those measuring (1) repetitiveness, (2) power law character, and (3) complexity. ∗ Kyushu University, 744 Motooka Nishiku, Fukuoka City, Fukuoka, Japan. E-mail: kumiko@ait.kyushu-u.ac.jp. ∗∗ JST-PRESTO, 4-1-8 Honcho, Kawaguchi, Saitama 332-0012, Japan. † Gunosy Inc., 6-10-1 Roppongi, Minato-ku, Tokyo, Japan. Submission received: 11 July 2013; revised version received: 17 February 2015; accepted for publication: 18 March 2015. doi:10.1162/COLI a 00228 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 3  Yule’s original intention for K’s utility lay in author identiﬁcation, assuming that it would differ for texts written by different authors. State-of-the-art multivariate machine learning techniques are powerful, however, for solving such language engineering tasks, in which Yule’s K is used only as one variable among many, as reported in Stamatatos, Fakotakis, and Kokkinakis (2001) and Stein, Lipka, and Prettenhofer (2010). We believe that constancy measures today, however, have greater importance in understanding the mathematical nature of language. Although mathematical models of language have been studied in the computational linguistics milieu, via Markov models (Manning and Schuetze 1999), Zipf’s law and its modiﬁcations (Mandelbrot 1953; Zipf 1965; Bell, Cleary, and Witten 1990), and Pitman-Yor models (Teh 2006) more recently, the true mathematical model of linguistic processes is ultimately unknown. Therefore, the convergence of a constancy measure must be examined through empirical veriﬁcation. Because some constancy measures have a mathematical theory of convergence for a known process, discrepancies in the behavior of real linguistic data from such a theory would shed light on the nature of linguistic processes and give hints towards improving the mathematical models. Furthermore, as one application, a convergent measure would allow for comparison of different texts through a common, stable norm, provided that the measure converges for a sufﬁciently small amount of text. One of our goals is to discover a non-trivial measure with a certain convergence speed that distinguishes the different natures of texts. The objective of this article is thus to provide a potential explanation of what the study of constancy measures over 70 years has been about, by answering the three following questions mathematically and empirically: Question 1 Does a measure exhibit constancy? Question 2 If so, how fast is the convergence speed? Question 3 How discriminatory is the measure? We seek answers by ﬁrst showing the meaning of Yule’s K in relation to the Re´nyi higher-order entropy, and by then empirically examining constancy across large-scale texts of different kinds. We ﬁnally provide an application by considering the natures of two unknown scripts, the Voynich manuscript and Rongorongo, in order to show the possible utility of a constancy measure. The most important and closest previous work was reported in Tweedie and Baayen (1998), the ﬁrst paper to have examined the empirical behavior of constancy measures on real texts. The authors used English literary texts to test constancy measure candidates proposed prior to their work. Today, the coverage and abundance of language corpora allow us to conduct a larger-scale investigation across multiple languages. Recently, Golcher (2007) tested his measure V (discussed later in this paper) with IndoEuropean languages and also programming language sources. Our papers (Kimura and Tanaka-Ishii 2011, 2014) also precede this one, presenting results preliminary to this article but with only part of our data, and neither of those provides mathematical analysis with respect to the Re´nyi entropy. Compared with these previous reports, our contribution here can be summarized as follows: r Our work elucidates the mathematical relation of Yule’s K to Re´nyi’s higher-order entropy and explains why K converges. r Our work vastly extends the corpora used for empirical examination in terms of both size and language. 482  Tanaka-Ishii and Aihara  Computational Constancy Measures of Texts  r Our work compares the convergent values for these corpora. r Our work also presents results for unknown language data, speciﬁcally from the Voynich manuscript and Rongorongo. We start by summarizing the potential constancy measures proposed so far. 2. Constancy Measures The measures proposed so far can broadly be categorized into three types, calculating the repetitiveness, power-law distribution, or complexity of text. This section mathematically analyzes these measures and summarizes them. 2.1 Measures Based on Repetitiveness The study of text constancy started with proposals for simple text measures of vocabulary repetitiveness. The representative example is Yule’s K (Yule 1944), while Golcher recently proposed V as another candidate (Golcher 2007). 2.1.1 Yule’s K. To the best of our knowledge, the oldest mention of constancy values was made by Yule with his notion of K (Yule 1944). Let N be the total number of words in a text, V(N) be the number of distinct words, V(m, N) be the number of words appearing m times in the text, and mmax be the largest frequency of a word. Yule’s K is then deﬁned as follows, through the ﬁrst and second moments of the vocabulary population distribution of V(m, N), where S1 = N = m mV(m, N), and S2 = m m2V(m, N) (Yule 1944; Herdan 1964):  K  =  C S2  − S21  S1  mmax  =C  −  
CSIRO Jiri Baum∗∗ Sabik Software Solutions In recent years, many studies have been published on data collected from social media, especially microblogs such as Twitter. However, rather few of these studies have considered evaluation methodologies that take into account the statistically dependent nature of such data, which breaks the theoretical conditions for using cross-validation. Despite concerns raised in the past about using cross-validation for data of similar characteristics, such as time series, some of these studies evaluate their work using standard k-fold cross-validation. Through experiments on Twitter data collected during a two-year period that includes disastrous events, we show that by ignoring the statistical dependence of the text messages published in social media, standard cross-validation can result in misleading conclusions in a machine learning task. We explore alternative evaluation methods that explicitly deal with statistical dependence in text. Our work also raises concerns for any other data for which similar conditions might hold. 1. Introduction With the emergence of popular social media services such as Twitter and Facebook, many studies in the area of Natural Language Processing (NLP) have been published that analyze the text data from these services for a variety of applications, such as opinion mining, sentiment analysis, event detection, or crisis management (Culotta 2010; Sriram et al. 2010; Yin et al. 2012). Many of these studies have primarily relied on building classiﬁcation models for different learning tasks, such as text classiﬁcation or Named Entity Recognition. The effectiveness of these models is often evaluated using cross-validation techniques. Cross-validation, ﬁrst introduced by Geisser (1974), has been acclaimed as the most popular evaluation method for estimating prediction errors in regression and ∗ CSIRO, Sydney, New South Wales, Australia. E-mail: {sarvnaz.karimi, jie.jin}@csiro.au. ∗∗ Sabik Software Solutions Pty Ltd, Sydney, New South Wales, Australia. E-mail: jiri@baum.com.au. Submission received: 4 October 2013; revised submission received: 9 September 2014; accepted for publication: 10 November 2014. doi:10.1162/COLI a 00230 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 3  classiﬁcation problems. In that method, the data D are randomly partitioned into k non-overlapping subsets (folds) Dk of approximately equal size. The validation step is repeated k times, using a different Dv = Dk as the validation data and Dt = D \ Dk as the training data each time. The ﬁnal evaluation is the average over the k validation steps. Cross-validation is found to have a lower variance than a single hold-out set validation and thus it is commonly used on both moderate and large amounts of data without introducing efﬁciency concerns (Arlot and Celisse 2010). Compared with other choices of k, 10-fold cross-validation has been accepted as the most reliable method, which gives a highly accurate estimate of the generalization error of a given model for a variety of algorithms and applications. Despite its wide applications, debates on the appropriateness of cross-validation have been raised in a number of areas, particularly in time series analysis (Bergmeir and Ben´ıtez 2012) and chemical engineering (Sheridan 2013). A fundamental assumption of cross-validation is that the data need to be independent and identically distributed (i.i.d.) between folds (Arlot and Celisse 2010). Therefore, if the data points used for training and validation are not independent, cross-validation is no longer valid and would usually overestimate the validity of the model. For time series forecasting, because the data are comprised of correlated observations and might be generated by a process that evolves over time, the training set and the validation set are not independent if randomly chosen for cross-validation. Researchers since the early 1990s have used modiﬁed variants of cross-validation to compensate for time dependence within times series (Chu and Marron 1991; Bergmeir and Ben´ıtez 2012). In the area of chemical engineering, the work of (Sheridan 2013) investigates the dependence in chemical data and observes that the existence of similar compounds or molecules across the data set leads to overoptimistic results using standard k-fold cross-validation. We take such observations from the time series and chemical domains as a warning to investigate the data dependence in computational linguistics. We argue that even when the data appear to be independently generated and when there is no reason to believe that temporal dependencies are present, unexpected statistical dependencies may be induced through an incorrect application of cross-validation. Once there is a chance of having similar or otherwise dependent data points, random split of the data without taking this factor into account would cause incorrect or at least unreliable evaluation, which may lead to invalid or at least unjustiﬁed conclusions. Although similar concerns have been raised by a prior study (Lerman et al. 2008) that cross-validation might not be suitable for measuring the accuracy of public opinion forecasting, there is a lack of systematic analysis on how potential data dependency might invalidate cross-validation and what alternative evaluation methods exist. With the aim of gaining further insight into this issue, we perform a detailed empirical study based on text classiﬁcation in Twitter, and show that inappropriate choice of cross-validation techniques could potentially lead to misleading conclusions. This concern could apply more generally to other data types of similar nature. We also explore several evaluation methods, mostly borrowed from research on time series, that are better suited for statistically dependent data and that could be adapted by researchers working in the NLP area.  2. Unexpected Statistical Dependence in Microblog Data We argue that microblog data, such as Twitter messages, known as tweets, are statistically dependent in nature. It has been demonstrated that there is redundancy in 540  Karimi, Yin, and Baum  Evaluation Methods for Statistically Dependent Text  Twitter language (Zanzotto, Pennacchiotti, and Tsioutsiouliklis 2011), which in turn can translate to evidence against statistical independence of microblog posts in given periods of time or occurrences of the same event. In summary, statistical dependence in tweets can arise for the following reasons: Events The events of interest, or events being discussed by the microbloggers at large, may be temporally limited within certain speciﬁc time horizons; Textual Links Hashtags and other idiomatic expressions may be invented, gain and lose popularity, fall out of use, or be reused with a different meaning. In addition, particular microbloggers may actively share information on certain types of events or express their opinions on certain topics in similar contexts; Twinning There may be “twinning” of tweets (and therefore data points) because of various forms of retweeting, where different microbloggers post substantially the same tweets, in response to one another. Many existing studies that use microblog data (e.g., for tweet classiﬁcation)—especially those related to detecting or monitoring events—have adopted k-fold cross-validation to evaluate the effectiveness of the learned classiﬁcation models (e.g., Culotta 2010; Sriram et al. 2010; Jiang et al. 2011; Uysal and Croft 2011; Takemura and Tajima 2012; Kumar et al. 2014). However, they overlook the possible statistical dependence among microblog data in terms of content (e.g., sharing hashtags in Twitter), relevance to current events, and time of publishing that could potentially impact their evaluation results. In Table 1 we use examples of these studies to illustrate how potential statistical dependence of microblog data is omitted, which might have impacted the validity of the results in these studies. The potential source of the ignored statistical dependence shown in the last column is categorized into Events, Textual Links, and Twinning (described above), based on the list of features that the authors have used in their machine learning approaches. For example, if one is interested in detecting speciﬁc events (e.g., diseases [Culotta 2010] or disasters [Verma et al. 2011; Kumar, Jiang, and Fang 2014]), using tweets from the middle of an event to train for and evaluate the detection of the onset of that same event is clearly invalid. In addition, certain users (e.g., authorities that announce disease outbreaks such as @ECDC Outbreaks, or car accidents such as @emergencyAUS) may always post in the same way [Verma et al. 2011; Kumar, Jiang, and Fang 2014], making tweets share similar contexts; or microbloggers always use the same hashtags to indicate similar topics (Jiang et al. 2011). More generally, if substantially verbatim “twin” copies of tweets ﬁnd their way into both the training and validation data sets (e.g., [Uysal and Croft 2011; Verma et al. 2011; Kumar, Jiang, and Fang 2014]), model validity will be overestimated. We note that, as we do not have access to the data sets used in these studies, we cannot verify the level of inﬂuence that the data dependence and their choice of evaluation have on their reported results. However, we raise a warning that there might be an overlooked unexpected dependence inﬂuencing their results. 3. Validation for Statistically Dependent Data Where the data is not independent, dependence-aware validation needs to be used. We describe four dependence-aware validation methods that take data dependence into consideration in the evaluation. In the following discussion, we refer to the data set used in the experiments as D = {d1, d2, · · · , dn}. The training data is referred to as Dt, and testing, evaluation or validation data as Dv, where both Dt and Dv are subsets of D. 541  Computational Linguistics  Volume 41, Number 3  Table 1 Examples of studies using k-fold cross-validation for evaluating classiﬁcation methods on Twitter data.  Reference  Study  Data  Dependence Concern  Sriram et al. (2010) Culotta (2010) Verma et al. (2011) Jiang et al. (2011) Uysal and Croft (2011) Takemura and Tajima (2012) Kumar, Jiang, and Fang (2014)  Classiﬁcation of tweets into ﬁve categories of events, news, deals, opinions, and private messages Classiﬁcation of tweets using regression models into ﬂu or not ﬂu Classiﬁcation of tweets to speciﬁc disaster events, and identiﬁcation of tweets that contained situational awareness content Sentiment classiﬁcation of tweets; hashtags were mentioned as one of the features, retweets were considered to share the same sentiment Personalized tweet ranking using retweet behavior. Decision tree classiﬁers were used based on features from the tweet content, user behavior, and tweet author Classiﬁcation of tweets into three categories based on whether they should be read now, later, or outdated Classiﬁcation of tweets into two categories of road hazard and non-hazard  5,407 tweets, no mention of removing retweets  Events, Textual Links, Twinning  500,000 tweets from a Events, Textual Links,  10-week period  Twinning  1,965 tweets collected from speciﬁc disasters in the U.S. by keyword search  Twinning, Textual Links  Tweets found using keyword search, without removing retweets  Twinning, Textual Links  24,200 tweets, from which 2,547 were retweeted by the seed users  Twinning, Textual Links  9,890 tweets from a ﬁxed period of time, annotated for time-(in)dependency  Events, Textual Links, Twinning  30,876 tweets, retweets were not removed  Events, Textual Links, Twinning  542  Karimi, Yin, and Baum  Evaluation Methods for Statistically Dependent Text  Border-Split Cross-Validation. This method, proposed by Chu and Marron (1991), is a modiﬁcation of k-fold cross-validation for time series data. Data are partitioned into the folds in time sequence rather than randomly; then in each validation step, data points within a time distance h of any point in the validation data set are excluded from the training data. That is, for each validation step k, the validation data is Dv = Dk, there is a border data set Db = {db ∈ D \ Dv : (∃dv∈Dv) |tdb − tdv | ≤ h}, which is disregarded, and the training data is Dt = D \ (Dk ∪ Db). This method assumes that the data dependence is conﬁned to some known radius h, with data points beyond that radius being independent.  Time-Split Validation. In time-split validation (Sheridan 2013), a particular time t∗ is  chosen; data points prior to this time are allocated to the training data set and data  points after {dv ∈ D : tdv  t∗ >  are t∗}.  used Note  as validation data; that is, Dt = {dt ∈ D : tdt ≤ that this is not a cross-validation method, as  t∗} and Dv = no “crossing”  takes place. The motivation is to emulate prospective prediction: A model is built using  only the information available up to time t∗ and evaluated on data that are collected  after that time.  Time-Border-Split Validation. This is a combination of border-split and time-split validation. Both a time t∗ and a radius h are chosen; data points prior to time t∗ − h are allocated to the training data set and data points after time t∗ are used as validation data; that is, Dt = {dt ∈ D : tdt ≤ t∗ − h} and Dv = {dv ∈ D : tdv > t∗}. The remaining data points are not used. The motivation is to combine the conservative aspects of both time-split and border-split, emulating prospective prediction more carefully.  Neighbor-Split Validation. Proposed by (Sheridan 2013), this method assumes the existence of some similarity metric for the data points. The number of neighbors for each data point is calculated, using a threshold on the similarity metric. A desired fraction of data points with the fewest neighbors are then allocated to the validation data Dv and the rest to the training data Dt. The motivation of this approach is to deliberately reduce the similarity between training and validation data. It is inspired by leave-class-out validation or cross-validation, which assumes a pre-existing classiﬁcation (rather than similarity metric) and allocates data points to the validation data Dv or cross-validation folds Dk, according to this classiﬁcation. The advantage of neighborsplit over leave-class-out is that the size of the validation data is a parameter that can be chosen, rather than being dependent on the (potentially unbalanced) sizes of the classes in the classiﬁcation.  4. A Case Study on Tweet Classiﬁcation  We focus on two tweet classiﬁcation tasks in our case study. The ﬁrst is a binary classiﬁcation, where tweets are classiﬁed into disaster-related or not; the second is a disaster type classiﬁcation, where tweets are predicted to one of the following six classes: nondisaster, storm, earthquake, ﬂooding, ﬁre, and other disasters. In our experiments, we use LibSVM (Chang and Lin 2011) to build discriminative classiﬁers for our classiﬁcation tasks. Tweets are short texts currently limited up to 140 characters. Often, microbloggers use tweets in reply to others by using mentions (which are Twitter usernames and are preceded by @), or use hashtags such as #CycloneEvan to make the grouping of similar messages easier, or to increase the visibility of their posts to others interested in the  543  Computational Linguistics  Volume 41, Number 3  same topic. Use of links to Web pages, mostly full stories of what is brieﬂy reported in the tweet, is also popular. Selecting features for a text classiﬁer that is built on Twitter data can therefore beneﬁt from both conventional textual features, such as n-grams, and Twitter-speciﬁc features, such as hashtags and mentions. In our experiments, we investigate the effect of the following features and their combinations on classiﬁcation of tweets for disasters: (1) n-grams: Unigrams and bigrams of tweet text at the word level, excluding any hashtag or mention in the text. To ﬁnd n-grams we pre-process tweets to remove stopwords; (2) Hashtag: Two different features are explored. First, a binary feature of the hashtags in the tweets, which indicates whether a hashtag exists in the tweet or not; second, the total number of hashtags in a tweet; (3) Mention: Two types of features (binary and mention count) are explored, exactly the same as hashtags explained above; and (4) Link: A binary feature that speciﬁes whether or not a tweet contains any link to a Web page. 4.1 Data and Annotation We randomly sampled a total of 7,500 English tweets published in two years from December 2010 to December 2012 from a system (Yin et al. 2012) that stores billions of tweets from the Twitter streaming API. Explicit retweets were excluded. This set contained a number of disasters such as earthquakes in Christchurch, New Zealand 2011; York ﬂoods, England 2012; and Hurricane Sandy, United States 2012. For a machine learner such as a classiﬁer to work, we need to present it with a representative set of labeled training data. We therefore annotated our tweet data manually to identify disaster tweets and their types. We annotated the data set based on two main questions: Is this tweet talking about a speciﬁc disaster? What type of disaster is it talking about? Types of disasters were deﬁned as earthquake, ﬁre, ﬂooding, storm, other, and non-disaster. Annotations were done by three annotators for each tweet, who were hired through the crowd-sourcing service Crowdﬂower. After taking majority votes where at least two out of the three annotators agreed on both questions, we ended up with a set of 6,580 annotated tweets, of which 2,897 tweets were identiﬁed as disaster-related and 3,683 as non-disaster. In disaster tweets, 37% were annotated with earthquake, followed by ﬁre, ﬂooding, and storm constituting 23%, 21%, and 12%, respectively. 4.2 Experimental Set-up For our tweet classiﬁcation tasks, we set up experiments to compare ﬁve validation methods—standard 10-fold cross-validation, border-split cross-validation, neighborsplit validation, time-split validation, and time-border-split validation—for identifying tweets that are relevant to a disaster, and whether or not we can broadly identify the type of disaster. We evaluate classiﬁcation effectiveness using the accuracy metric, which is the percentage of correctly classiﬁed tweets. We used the following settings for our evaluation schemes: r k-fold cross-validation: We used k = 10 folds. r Border-split cross-validation: We used k = 10 folds for border-split and a radius h = 20 days. We assume that for most events, the social media activity on the topic dies after three weeks of their occurrence. 544  Karimi, Yin, and Baum  Evaluation Methods for Statistically Dependent Text  r Neighbor-split validation: We used cosine similarity to ﬁnd a subset of the data that has the least number of neighbors in the data set. We weighted hashtags double and disregarded mentions in calculating the similarity. Neighborhood was decided using a minimum threshold of 0.25 on the resulting cosine similarity. The size of the test data set was the same as in time-split (below) but the size of the training data set was 5,868. r Time-split validation: We chose the cut-off time t∗ so that 90% of the data was used as training (5,922 tweets) and 10% as validation (658 tweets). r Time-border-split validation: We used the same t∗ and h as time-split and border-split. This resulted in a training data set containing 87.1% of the data and a validation data set identical to the one in time-split at 10% of the data (658 tweets). In removing the border, 2.9% of the data were discarded. The size of the training set was 5,750 tweets.  4.3 Results We run two sets of experiments: Discriminating disaster tweets from non-disaster (Disaster or Not), and classifying tweets into the six classes of earthquake, ﬁre, ﬂooding, storm, other, and non-disaster (Disaster Type). Table 2 compares the classiﬁcation results for SVM on a range of feature combinations using ﬁve different validation methods. We aim to show that inappropriate choice of cross-validation could possibly lead to misleading conclusions, including overestimated classiﬁcation accuracies, and suboptimal feature sets that yield the highest accuracies. k-fold Cross-Validation. The ﬁrst set of experiments was conducted using standard 10-fold cross-validation. For discriminating disaster tweets from non-disaster, SVM achieved a maximum of 92.8% accuracy when unigrams and hashtags were used. A similar result of 92.7% accuracy was recorded for classifying tweets to their disaster types. Unsurprisingly, having hashtags as additional features was the most  Table 2 Comparison of different evaluation methods for tweet classiﬁcation with different feature combinations. Standard deviations are given in parentheses when applicable.  Features  10-Fold CV Border CV Neighbor Time Time-Border  Disaster or Not  Unigram  86.2 (±1.9) 76.3 (±14.4) 89.8  78.5  78.9  Unigram+Hashtag  92.8 (±0.9) 81.8 (±14.6) 89.7  80.1  77.5  Unigram+Hashtag Count  87.9 (±1.5) 79.3 (±12.3) 85.1  82.0  81.6  Unigram+Mention  86.6 (±1.7) 74.8 (±19.1) 89.7  78.8  79.0  Unigram+Hashtag Count+Mention Count+Link 88.0 (±1.4) 78.9 (±12.6) 85.2  82.0  81.4  Unigram+Bigram  86.5 (±1.3) 74.4 (±16.7) 89.8  78.5  78.6  Unigram+Bigram+Hashtag  92.3 (±1.0) 79.5 (±16.6) 90.0  77.3  73.7  Disaster Type  Unigram  83.4 (±1.8) 66.8 (±20.4) 85.4  68.9  69.0  Unigram+Hashtag  92.7 (±1.1) 79.4 (±17.7) 89.4  77.8  74.5  Unigram+Hashtag Count  84.3 (±1.5) 67.1 (±20.9) 81.4  68.6  67.5  Unigram+Mention  83.9 (±1.6) 66.9 (±20.3) 85.5  69.1  68.2  Unigram+Hashtag Count+Mention Count+Link 84.4 (±1.6) 66.7 (±20.9) 81.4  68.0  67.0  Unigram+Bigram  83.0 (±1.7) 63.8 (±24.9) 88.5  66.7  65.8  Unigram+Bigram+Hashtag  91.1 (±1.0) 73.5 (±21.2) 88.3  72.0  69.4  545  Computational Linguistics  Volume 41, Number 3  helpful. As a side note, the standard deviations of 10-fold cross-validation are quite small; although it is well-known that these are not an unbiased estimate of the true variance (see, for instance, Bengio and Grandvalet [2004]), it nevertheless seems to suggest a degree of conﬁdence. If we would assume that tweets are statistically independent, we could conclude that the SVM classiﬁer using unigrams plus hashtags is the best performer for classifying disaster tweets with a high accuracy of over 92%. This is what most previous studies have done. However, this result is overoptimistic. For example, during the cross-validation, tweets, with the same hashtags are distributed over all folds, making it easier for the classiﬁer to associate labels with known hashtags. Border-Split Cross-Validation. In contrast to 10-fold cross-validation, border-split cross-validation gives a much lower performance score across all of the results. The standard deviations are also much larger. Neighbor-split Validation. The neighbour-split results are very different. Unlike previous work (Sheridan 2013), we ﬁnd that neighbor-split judges the effectiveness of the models quite highly, near the scores from 10-fold cross-validation but ranking the feature combinations differently. We believe the high scores are because it tends to pick non-disaster tweets into the validation set, as those tweets have fewest neighbors. This makes it easier to classify all validation tweets into the majority class (i.e., non-disasters). Time-split and Time-border-split Validation. Neither of these validations leads to results similar to 10-fold cross-validation; they are substantially lower, in one case by over 20 percentage points (bottom row of table). Numerically, they are similar to the border-split cross-validation results, but again with a different ranking of the feature combinations. Time-split and time-border-split methods gave similar results to each other, with different rankings but only small numerical differences. Coincidentally, in our data set, the time t∗ fell largely between events of interest, so the elimination of the border resulted in only a small correction, confounded with the effect of the small reduction in training set size.  4.4 Discussion The validation methods provide very different results, with a number of the differences around 20 percentage points. In addition, they disagree on the ranking of the feature combinations, which may be the more important problem in many studies. This is particularly visible with the Disaster or Not classiﬁcation; the best combination of features (bold in Table 2) is different, depending on the validation method used. A study based on 10-fold cross-validation would suggest Unigram+Hashtag for this problem, which is the second-worst combination of features, according to time-border-split validation. Further, the conﬁdence intervals, if used, would suggest conﬁdence in this choice. Given our experiments, we believe that the standard k-fold cross-validation substantially overestimates the performance of the models in our case study. Time-split and time-border-split validation are more likely to represent accurate evaluation when temporally dependent data is involved, as they simulate true prospective prediction. However, they both have the downside that they only rely on one pair of training and validation data sets. This means that they will have a larger variance than a  546  Karimi, Yin, and Baum  Evaluation Methods for Statistically Dependent Text  cross-validation method, and do not give any measure of conﬁdence in their own results. Border-split cross-validation may also be acceptable, provided that its assumptions are satisﬁed—primarily, that the data points are independent beyond a certain radius. Depending on a particular task, this may be easy to decide (e.g., separate events) or hard (e.g., a number of overlapping events with no clear time difference). We also ﬁnd that neighbor-split overestimates rather than underestimates the performance of the models relative to time-split and time-border-split. This represents a hazard to its use: The neighborhood measure may interact in unexpected ways with other features of the data, rendering the validation unpredictable. Ideally, one would use multiple large test data sets, all collected during separate, non-overlapping periods of time, after the training data set. This would be the most reliable measure of the performance of the models, but also the most expensive in terms of required data collection and annotation. Failing that, we recommend time-split or time-border-split validation when temporal statistical dependence cannot be ruled out. 5. Conclusions We used a common task in NLP, text classiﬁcation, on a relatively recent but widely used data source, Twitter streams, to show that blindly following the same evaluation method for tasks of similar nature could lead to invalid conclusions. In particular, we investigated the most common evaluation method for machine learning applications, standard 10-fold cross-validation, and compared it with other validation methods that take the statistical dependence in the data into account, including time-split, bordersplit, neighbor-split, and a combination of time- and border-split validation. We showed how cross-validation can overestimate the effectiveness of a tweet classiﬁcation application. We argued that text in microblogs or other similar text from social media (e.g., Web forums or even online news) can be statistically dependent for speciﬁc studies, such as those looking at events. Researchers therefore need to be careful in choosing evaluation methodologies based on the nature of the data at hand to avoid bias in their results.  
Alexander Fraser† LMU Munich Philipp Koehn‡ University of Edinburgh Hinrich Schu¨ tze§ LMU Munich In this article, we present a novel machine translation model, the Operation Sequence Model (OSM), which combines the beneﬁts of phrase-based and N-gram-based statistical machine translation (SMT) and remedies their drawbacks. The model represents the translation process as a linear sequence of operations. The sequence includes not only translation operations but also reordering operations. As in N-gram-based SMT, the model is: (i) based on minimal translation units, (ii) takes both source and target information into account, (iii) does not make a phrasal independence assumption, and (iv) avoids the spurious phrasal segmentation problem. As in phrase-based SMT, the model (i) has the ability to memorize lexical reordering triggers, (ii) builds the search graph dynamically, and (iii) decodes with large translation units during search. The unique properties of the model are (i) its strong coupling of reordering and translation where translation and reordering decisions are conditioned on n previous translation and reordering decisions, and (ii) the ability to model local and long-range reorderings consistently. Using BLEU as a metric of translation accuracy, we found that our system performs signiﬁcantly ∗ Qatar Computing Research Institute, Qatar Foundation. E-mail: ndurrani@qf.org.qa. ∗∗ CIS, Ludwig Maximilian University Munich. E-mail: schmid@cis.uni-muenchen.de. † CIS, Ludwig Maximilian University Munich. E-mail: fraser@cis.uni-muenchen.de. ‡ University of Edinburgh, Edinburgh. E-mail: pkoehn@inf.ed.ac.uk@inf.ed.ac.uk. § CIS, Ludwig Maximilian University Munich. E-mail: hs2014@cislmu.org. Some of the research presented here was carried out while the authors were at the University of Stuttgart and the University of Edinburgh. Submission received: 5 October 2013; revised version received: 23 October 2014; Accepted for publication: 25 November 2014. doi:10.1162/COLI a 00218 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 2  better than state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems (Ncode) on standard translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM. 1. Introduction Statistical Machine Translation (SMT) advanced near the beginning of the century from word-based models (Brown et al. 1993) towards more advanced models that take contextual information into account. Phrase-based (Koehn, Och, and Marcu 2003; Och and Ney 2004) and N-gram-based (Casacuberta and Vidal 2004; Marin˜ o et al. 2006) models are two instances of such frameworks. Although the two models have some common properties, they are substantially different. The present work is a step towards combining the beneﬁts and remedying the ﬂaws of these two frameworks. Phrase-based systems have a simple but effective mechanism that learns larger chunks of translation called bilingual phrases.1 Memorizing larger units enables the phrase-based model to learn local dependencies such as short-distance reorderings, idiomatic collocations, and insertions and deletions that are internal to the phrase pair. The model, however, has the following drawbacks: (i) it makes independence assumptions over phrases, ignoring the contextual information outside of phrases, (ii) the reordering model has difﬁculties in dealing with long-range reorderings, (iii) problems in both search and modeling require the use of a hard reordering limit, and (iv) it has the spurious phrasal segmentation problem, which allows multiple derivations of a bilingual sentence pair that have the same word alignment but different model scores. N-gram-based models are Markov models over sequences of tuples that are generated monotonically. Tuples are minimal translation units (MTUs) composed of source and target cepts.2 The N-gram-based model has the following drawbacks: (i) only precalculated orderings are hypothesized during decoding, (ii) it cannot memorize and use lexical reordering triggers, (iii) it cannot perform long distance reorderings, and (iv) using tuples presents a more difﬁcult search problem than in phrase-based SMT. The Operation Sequence Model. In this article we present a novel model that tightly integrates translation and reordering into a single generative process. Our model explains the translation process as a linear sequence of operations that generates a source and target sentence in parallel, in a target left-to-right order. Possible operations are (i) generation of a sequence of source and target words, (ii) insertion of gaps as explicit target positions for reordering operations, and (iii) forward and backward jump operations that do the actual reordering. The probability of a sequence of operations is deﬁned according to an N-gram model, that is, the probability of an operation depends on the n − 1 preceding operations. Because the translation (lexical generation) and reordering operations are coupled in a single generative story, the reordering decisions may depend on preceding translation decisions and translation decisions may depend  
Giorgio Satta† University of Padua The weak equivalence of Combinatory Categorial Grammar (CCG) and Tree-Adjoining Grammar (TAG) is a central result of the literature on mildly context-sensitive grammar formalisms. However, the categorial formalism for which this equivalence has been established differs signiﬁcantly from the versions of CCG that are in use today. In particular, it allows restriction of combinatory rules on a per grammar basis, whereas modern CCG assumes a universal set of rules, isolating all cross-linguistic variation in the lexicon. In this article we investigate the formal signiﬁcance of this difference. Our main result is that lexicalized versions of the classical CCG formalism are strictly less powerful than TAG. 1. Introduction Since the late 1970s, several grammar formalisms have been proposed that extend the power of context-free grammars in restricted ways. The two most prominent members of this class of “mildly context-sensitive” formalisms (a term coined by Joshi 1985) are Tree-Adjoining Grammar (TAG; Joshi and Schabes 1997) and Combinatory Categorial Grammar (CCG; Steedman 2000; Steedman and Baldridge 2011). Both formalisms have been applied to a broad range of linguistic phenomena, and are being widely used in computational linguistics and natural language processing. In a seminal paper, Vijay-Shanker and Weir (1994) showed that TAG, CCG, and two other mildly context-sensitive formalisms—Head Grammar (Pollard 1984) and Linear Indexed Grammar (Gazdar 1987)—all characterize the same class of string languages. However, when citing this result it is sometimes overlooked that the result applies to a version of CCG that is quite different from the versions that are in practical use today. ∗ Department of Computer and Information Science, Linko¨ ping University, 581 83 Linko¨ ping, Sweden. E-mail: marco.kuhlmann@liu.se. ∗∗ Department of Linguistics, Karl-Liebknecht-Str. 24–25, University of Potsdam, 14476 Potsdam, Germany. E-mail: koller@ling.uni-potsdam.de. † Department of Information Engineering, University of Padua, via Gradenigo 6/A, 35131 Padova, Italy. E-mail: satta@dei.unipd.it. Submission received: 4 December 2013; revised submission received: 26 July 2014; accepted for publication: 25 November 2014. doi:10.1162/COLI a 00219 © 2015 Association for Computational Linguistics  / /  Computational Linguistics  Volume 41, Number 2  The goal of this article is to contribute to a better understanding of the signiﬁcance of this difference. The difference between “classical” CCG as formalized by Vijay-Shanker and Weir (1994) and the modern perspective may be illustrated with the combinatory rule of backward-crossed composition. The general form of this rule looks as follows:  Backward-crossed composition, general form: Y/Z X Y ⇒ X/Z  (<B× )  This rule is frequently used in the analysis of heavy NP shift, as in the sentence Kahn blocked skillfully a powerful shot by Rivaldo (example from Baldridge 2002). However, backward crossed composition cannot be universally active in English as this would cause the grammar to also accept strings such as *a powerful by Rivaldo shot, which is witnessed by the derivation in Figure 1. To solve this problem, in the CCG formalism of Vijay-Shanker and Weir (1994), one may restrict backward-crossed composition to instances where X and Y are both verbal categories—that is, functions into the category S of sentences (cf. Steedman 2000, Section 4.2.2). With this restriction the unwanted derivation in Figure 1 can be blocked, and a powerful shot by Rivaldo is still accepted as grammatical. Other syntactic phenomena require other grammar-speciﬁc restrictions, including the complete ban of certain combinatory rules (cf. Steedman 2000, Section 4.2.1). Over the past 20 years, CCG has evolved to put more emphasis on supporting fully lexicalized grammars (Baldridge and Kruijff 2003; Steedman and Baldridge 2011), in which as much grammatical information as possible is pushed into the lexicon. This follows the tradition of other frameworks such as Lexicalized Tree-Adjoining Grammar (LTAG) and Head-Driven Phrase Structure Grammar (HPSG). Grammar-speciﬁc rule restrictions are not connected to individual lexicon entries, and are therefore avoided. Instead, recent versions of CCG have introduced a new, lexicalized control mechanism in the form of modalities or slash types. The basic idea here is that combinatory rules only apply if the slashes in their input categories have the right types. For instance, the modern version of backward-crossed composition (Steedman and Baldridge 2011) takes the following form, where × is a slash type that allows crossed but not harmonic composition:  Backward-crossed composition, modalized form: Y/×Z X ×Y ⇒ X/×Z  (<B× )  Figure 1 Overgeneration caused by unrestricted backward crossed composition. 188  Kuhlmann, Koller, and Satta  Lexicalization and Generative Power in CCG  The derivation in Figure 1 can now be blocked by assigning the slash in the category of powerful a type that is incompatible with ×, and thus cannot feed into type-aware backward-crossed composition as an input category. This “modalization” of grammatical composition, which imports some of the central ideas from the type-logical tradition of categorial grammar (Moortgat 2011) into the CCG framework, is attractive because it isolates the control over the applicability of combinatory rules in the lexicon. Quoting Steedman and Baldridge (2011, p. 186):  Without typed slashes, language-speciﬁc restrictions or even bans on some combinatory rules are necessary in order to block certain ungrammatical word orders. With them, the combinatory rules are truly universal: the grammar of every language utilizes exactly the same set of rules, without modiﬁcation, thereby leaving all cross-linguistic variation in the lexicon. As such, CCG is a fully lexicalized grammar formalism.  The stated goal is thus to express all linguistically relevant restrictions on the use of combinatory rules in terms of lexically speciﬁed, typed slashes. But to what extent can this goal actually be achieved? So far, there have been only partial answers—even when, as in the formalism of Vijay-Shanker and Weir (1994), we restrict ourselves to the rules of composition, but exclude other rules such as type-raising and substitution. Baldridge and Kruijff (2003) note that the machinery of their multi-modal formalism can be simulated by rule restrictions, which shows that this version of lexicalized CCG is at most as expressive as the classical formalism. At the other end of the spectrum, we have shown in previous work that a “pure” form of lexicalized CCG with neither rule restrictions nor slash types is strictly less expressive (Kuhlmann, Koller, and Satta 2010). The general question of whether “classical” CCG (rule restrictions) and “modern” CCG (slash types instead of rule restrictions) are weakly equivalent has remained open. In this article, we answer this question. Our results are summarized in Figure 2. After setting the stage (Section 2), we ﬁrst pinpoint the exact type of rule restrictions that make the classical CCG formalism weakly equivalent to TAG (Section 3). We do so by focusing on a class of grammars that we call preﬁx-closed. Unlike the “pure” grammars that we studied in earlier work, preﬁx-closed grammars do permit rule restrictions (under certain conditions that seem to be satisﬁed by linguistic grammars). We show that the generative power of preﬁx-closed CCG depends on its ability to express target restrictions, which are exactly the functions-into type of restrictions that are needed to block the derivation in Figure 1. We prove that the full class of preﬁx-closed CCGs is weakly equivalent to TAG, and the subclass that cannot express target restrictions is strictly less powerful. This result signiﬁcantly sharpens the picture of the generative capacity of CCG. In a further step (Section 4), we then prove that for at least one popular incarnation of modern, lexicalized CCG, slash types are strictly less expressive than rule  Weakly equivalent to TAG  Strictly less powerful than TAG  VW-CCG  Preﬁx-closed VW-CCG  Preﬁx-closed VW-CCG  with target restrictions (Theorem 2) without target restrictions (Theorem 3)  O-CCG (Theorem 4)  Figure 2 Summary of the results in this article. VW-CCG is the formalism of Vijay-Shanker and Weir (1994).  189  Computational Linguistics  Volume 41, Number 2  restrictions. More speciﬁcally, we look at a variant of CCG consisting of the composition rules implemented in OpenCCG (White 2013), the most widely used development platform for CCG grammars. We show that this formalism is (almost) preﬁx-closed and cannot express target restrictions, which enables us to apply our generative capacity result from the ﬁrst step. The same result holds for (the composition-only fragment of) the formalism of Baldridge and Kruijff (2003). Thus we ﬁnd that, at least with existing means, the weak equivalence result of Vijay-Shanker and Weir cannot be obtained for lexicalized CCG. We conclude the article by discussing the implications of our results (Section 5). 2. Background In this section we provide the technical background of the article. We introduce the basic architecture of CCG, present the formalism of Vijay-Shanker and Weir (1994), and set the points of reference for our results about generative capacity. 2.1 Basic Architecture of CCG The two central components of CCG are a lexicon that associates words with categories, and rules that specify how categories can be combined. Taken together, these components give rise to derivations, such as the one shown in Figure 3. Lexicon. A category is a syntactic type that identiﬁes a constituent as either “complete” or “incomplete.” The categories of complete constituents are taken to be primitives; the categories of incomplete constituents are modeled as (curried) functions that specify the type and the direction of their arguments. Every category is projected from the lexicon, which is a ﬁnite collection of word–category pairs. To give examples (taken from Steedman 2012), an English intransitive verb such as walks has a category that identiﬁes it as a function seeking a (subject) noun phrase to its left (indicated by a backward slash) and returning a complete sentence; and a transitive verb such as admires has a category that identiﬁes it as a function seeking an (object) noun phrase to its right (indicated by a forward slash) and returning a constituent that acts as an intransitive verb. We denote lexical assignment using the “colon equals” operator: walks := S NP admires := (S NP)/NP Formally, the set C(A) of categories is built over a ﬁnite set A of atomic categories, the primitive syntactic types. It is the smallest set such that 1. if A ∈ A then A ∈ C(A);  /  /  Harry ..... NP  admires ..  Louise ..  / /  (S NP)/NP NP > S NP < S  Figure 3 A sample derivation (adapted from Steedman 2012).  190  Kuhlmann, Koller, and Satta  Lexicalization and Generative Power in CCG  /  2. if X, Y ∈ C(A) then X/Y ∈ C(A) and X Y ∈ C(A).  We use the letters A, B, C to denote atomic categories, the letters X, Y, Z to denote arbitrary categories, and the symbol | to denote slashes (forward or backward). We treat slashes as left-associative operators and omit unnecessary parentheses. By this convention, every category X can be written in the form  X = A|mXm · · · |1X1 where m ≥ 0, A is an atomic category that we call the target of X, and the |iXi are slash–category pairs that we call the arguments of X. Intuitively, the target of X is the atomic category that is obtained after X has been applied to all of its arguments. We use the Greek letters α, β, γ to denote (potentially empty) sequences of arguments. The number m is called the arity of the category X.  Rules. Categories can combine under a number of rules; this gives rise to derivations such as the one shown in Figure 3. Each rule speciﬁes an operation that assembles two input categories into an output category. The two most basic rules of CCG are the directed versions of function application:  X/Y Y ⇒ X  (forward application)  (>)  Y XY ⇒ X  (backward application)  (<)  /  Formally, a rule is a syntactic object in which the letters X, Y, Z act as variables for categories. A rule instance is obtained by substituting concrete categories for all variables in the rule. For example, the derivation in Figure 3 contains the following instances of function application. We denote rule instances by using a triple arrow instead of the double arrow in our notation for rules.  /  /  /  (S NP)/NP NP  S NP  and  NP S NP  S  Application rules give rise to derivations equivalent to those of context-free grammar. Indeed, versions of categorial grammar where application is the only mode of combination, such as AB-grammar (Ajdukiewicz 1935; Bar-Hillel, Gaifman, and Shamir 1960), can only generate context-free languages. CCG can be more powerful because it also includes other rules, derived from the combinators of combinatory logic (Curry, Feys, and Craig 1958). In this article, as in most of the formal work on CCG, we restrict our attention to the rules of (generalized) composition, which are based on the B combinator.1 The general form of composition rules is shown in Figure 4. In each rule, the two input categories are distinguished into one primary (shaded) and one secondary input category. The number n of outermost arguments of the secondary input category is called the degree of the rule.2 In particular, for n = 0 we obtain the rules of function  
Ido Dagan† Bar-Ilan University Jacob Goldberger‡ Bar-Ilan University Entailment rules between predicates are fundamental to many semantic-inference applications. Consequently, learning such rules has been an active ﬁeld of research in recent years. Methods for learning entailment rules between predicates that take into account dependencies between different rules (e.g., entailment is a transitive relation) have been shown to improve rule quality, but suffer from scalability issues, that is, the number of predicates handled is often quite small. In this article, we present methods for learning transitive graphs that contain tens of thousands of nodes, where nodes represent predicates and edges correspond to entailment rules (termed entailment graphs). Our methods are able to scale to a large number of predicates by exploiting structural properties of entailment graphs such as the fact that they exhibit a “tree-like” property. We apply our methods on two data sets and demonstrate that our methods ﬁnd high-quality solutions faster than methods proposed in the past, and moreover our methods for the ﬁrst time scale to large graphs containing 20,000 nodes and more than 100,000 edges. 1. Introduction Performing textual inference is at the heart of many semantic inference applications, such as Question Answering (QA) and Information Extraction (IE). A prominent generic ∗ Computer Science Department, Stanford University, Stanford, CA 94305. Web: http://www.stanford.edu/∼joberant. ∗∗ Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, 6997801, Israel. Web: http://www.tau.ac.il/∼nogaa. † Computer Science Department, Bar-Ilan University, Ramat-Gan, 52900, Israel. Web: http://www.cs.biu.ac.il/∼dagan. ‡ Engineering Department, Bar-Ilan University, Ramat-Gan, 52900, Israel. Web: http://www.eng.biu.ac.il/goldbej. Submission received: 23 November 2013; revised version received: 26 September 2014; accepted for publication: 25 November 2014. doi:10.1162/COLI a 00220 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 2  paradigm for textual inference is Textual Entailment (Dagan et al. 2013). In textual entailment, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text. For example, the text Cyprus was invaded by the Ottoman Empire in 1571 implies the hypothesis The Ottomans attacked Cyprus. Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule speciﬁes the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules x ⇒ y and y ⇒ z imply the rule x ⇒ z. For example, from the rules X reduce nausea ⇒ X help with nausea and X help with nausea ⇒ X associated with nausea we can infer the rule X reduce nausea ⇒ X associated with nausea (Figure 1). Accordingly, Berant, Dagan, and Goldberger (2012) proposed taking advantage of transitivity to improve learning of entailment rules. They modeled learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules that respect transitivity. To solve this optimization problem, they formulated it as an Integer Linear Program (ILP) and used an off-the-shelf ILP solver to ﬁnd an exact solution. Indeed, they showed that applying global transitivity constraints results in more accurate graphs (known as entailment graphs) than methods that ignore the property of transitivity. Although using an off-the-shelf ILP solver is straightforward, ﬁnding the optimal set of edges respecting transitivity is NP-hard, and practically, transitivity constraints impose substantial restrictions on the scalability of the methods. In fact, in some cases ﬁnding the optimal set of edges for entailment graphs with even ∼50 nodes was quite slow. In this article, we develop algorithms for learning entailment graphs that take advantage of structural properties such as transitivity, but substantially reduce the computational cost of inference, thus allowing us to solve the aforementioned optimization problem on very large graphs.  X-related-to-nausea  X-associated-with-nausea  X-prevent-nausea  X-help-with-nausea  X-reduce-nausea X-treat-nausea Figure 1 A fragment of an entailment graph about the concept nausea from the data set used by Berant, Dagan, and Goldberger. Edges that can be inferred by transitivity are omitted for clarity. 222  Berant et al.  Efﬁcient Global Learning of Entailment Graphs  Our method contains two main steps. The ﬁrst step is based on a sparsity assumption that even if an entailment graph contains many predicates, most of them do not entail one another, and thus we can decompose the graph into smaller components that can be solved more efﬁciently. For example, the predicates X parent of Y, X child of Y and X relative of Y are independent from the predicates X works with Y, X boss of Y, and X manages Y, and thus we can consider each one of these two sets separately. We prove that ﬁnding the optimal solution for each of the smaller components results in a global optimal solution for our optimization problem. The second step proposes a polynomial heuristic approximation algorithm for ﬁnding a transitive set of edges in each one of the smaller components. It is based on a novel modeling assumption that entailment graphs exhibit a “tree-like” property, which we term forest reducible. For example, the graph in Figure 1 is not a tree, because the predicates X related to nausea and X associated with nausea form a cycle. However, these two predicates are synonymous, and if they were merged into a single node, then the graph would become a tree. We propose a simple iterative approximation algorithm, where in each iteration a single node is deleted from the graph and then inserted back in a way that improves the objective function value. We prove that if we impose a constraint that entailment graphs must be forest reducible, then each iteration can be performed in linear time. This results in an algorithm that can scale to entailment graphs containing tens of thousands of nodes. We apply our algorithm on two data sets. The ﬁrst data set includes medium-sized entailment graphs where predicates are typed, that is, the arguments are restricted to belong to a particular semantic type (for instance, Xperson parent of Yperson). We show that using our algorithm, we can substantially improve runtime, suffering from only a slight reduction in the quality of learned entailment graphs. The second data set includes a much larger graph containing 20,000 untyped nodes, where applying state-of-the-art methods that use an ILP solver is completely impossible. We run our algorithm on this data set and demonstrate that we can learn knowledge bases with more than 100,000 entailment rules at a higher precision compared with local learning methods. The article is organized as follows. In Section 2 we survey prior work on the learning of entailment rules. We ﬁrst focus on local methods (Section 2.1), that is, methods that handle each pair of predicates independently of other predicates, and then describe global methods (Section 2.2), that is, methods that take into account multiple predicates simultaneously. Section 3 is the core of the article and describes our main algorithmic contributions. After formalizing entailment rule learning as a graph optimization problem, we present the two steps of our algorithm. Section 3.1 describes the ﬁrst step, in which a large entailment graph is decomposed into smaller components. Section 3.2 describes the second step, in which we develop an efﬁcient heuristic approximation based on the assumption that entailment graphs are forest reducible. Section 4 describes experiments on the ﬁrst data set containing medium-sized entailment graphs with typed predicates. Section 5 presents an empirical evaluation on a large graph containing 20,000 untyped predicates. We also perform a qualitative analysis in Section 5.4 to further elucidate the behavior of our algorithm. Section 6 concludes the article. This article is based on previous work (Berant, Dagan, and Goldberger 2011; Berant et al. 2012), but expands over it in multiple directions. Empirically, we present results on a novel data set (Section 5) that is by orders of magnitude larger than in the past. Algorithmically, we present the Tree-Node-And-Component-Fix algorithm, which is an extension of the Tree-Node-Fix algorithm presented in Berant et al. (2012) and achieves best results in our experimental evaluation. Theoretically, we provide an NP-hardness  223  Computational Linguistics  Volume 41, Number 2  proof for the Max-Trans-Forest optimization problem presented in Berant et al. (2012) and an ILP formulation for it. Last, we perform in Section 5.4 an extensive qualitative analysis of the graphs learned by our local and global algorithms from which we draw conclusions for future research directions. 2. Background In this section we describe prior work relevant for entailment rule learning. First, we describe local methods that estimate entailment for a pair of predicates, focusing on methods that we employ in this article. Then, we describe global methods that perform inference over a larger set of predicates. Speciﬁcally, we provide details on the method and optimization problem developed by Berant, Dagan, and Goldberger (2012) for which we propose scalable inference algorithms in this article. Last, we survey some other related work in NLP that uses global inference. 2.1 Local Learning In local learning, given a pair of predicates (i, j) we would like to determine whether i ⇒ j. The main sources of information utilized in the past for local learning were (1) lexicographic resources, (2) co-occurrence, and (3) distributional similarity. We brieﬂy describe the ﬁrst two and then expand more on distributional similarity, which is the most commonly used source of information. Lexicographic resources are manually built knowledge bases from which semantic information may be extracted. For example, the hyponymy, toponymy, and synonymy relations in WordNet (Fellbaum 1998) can be used to detect entailment between nouns and verbs. Although WordNet is the most popular lexicographic resource, other resources such as CatVar, Nomlex, and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004) used pattern-based methods to generate the commonly used VerbOcean resource. Both lexicographic as well as pattern-based methods suffer from limited coverage. Distributional similarity is therefore used to automatically construct broad-scale resources. Distributional similarity methods are based on the “distributional hypothesis” (Harris 1954) that semantically similar predicates occur with similar arguments. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al. 2004; Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010), which differ in terms of the speciﬁcs of the ways in which predicates are represented, the features that are extracted, and the function used to compute feature vector similarity. Next, we elaborate on the methods that we use in this article. Lin and Pantel (2001) proposed an algorithm for learning paraphrase relations between binary predicates, that is, predicates with two variables such as X treat Y. For each binary predicate, Lin and Pantel compute two sets of features Fx and Fy, 224  Berant et al.  Efﬁcient Global Learning of Entailment Graphs  which are the words that instantiate the arguments X and Y, respectively, in a large  corpus. Given a predicate u and its feature set for the X variable Fx, every feature fx ∈ Fx  is weighted by pointwise mutual information between the predicate and the feature:  w(  fx )  =  log  Pr( fx|u) Pr( fx )  ,  where  the  probabilities  are  computed  using  maximum  likelihood  over the corpus. Given two predicates u and v, the Lin measure (Lin 1998) is computed  for the variable X in the following manner:  Linx(u, v) =  f ∈Fux∩Fvx [wux ( f ) + wvx( f )] f ∈Fux wux ( f ) + f ∈Fvx wvx( f )  (1)  The measure is computed analogously for the variable Y and the ﬁnal distributional similarity score, termed DIRT, is the geometric average of the scores for the two variables: If DIRT(u, v) is high, this means that the templates u and v share many “informative” arguments and so it is possible that u ⇒ v. Szpektor and Dagan (2008) suggested two modiﬁcations to DIRT. First, they looked at unary predicates, that is, predicates with a single variable such as X treat. Secondly, they computed a directional score that is more suited for capturing entailment relations compared to the symmetric Lin score. They proposed that if for two unary predicates u ⇒ v, then relatively many of the features of u should be covered by the features of v. This is captured by the asymmetric Cover measure (Weeds and Weir 2003):  Cover(u, v) =  f ∈Fu∩Fv wu( f ) f ∈Fu wu( f )  (2)  The ﬁnal directional score, termed BInc (Balanced Inclusion), is the geometric average of the Lin measure and the Cover measure. Both Lin and Pantel as well as Szpektor and Dagan compute a similarity score using a single argument. However, it is clear that although this alleviates sparsity problems, considering pairs of arguments jointly provides more information. Yates and Etzioni (2009), Schoenmackers et al. (2010), and even earlier, Szpektor et al. (2004), presented methods that compute semantic similarity based on pairs of arguments. A problem common to all local methods presented above is predicate ambiguity— predicates may have different meanings and different entailment relations in different contexts. Some works resolved the problem of ambiguity by representing predicates with argument variables that are typed (Pantel et al. 2007; Schoenmackers et al. 2010). For example, argument variables in the work of Schoenmackers et al. were restricted to belong to one of 156 types, such as country or profession. A different solution that has attracted substantial attention recently is to represent the various contexts in which a predicate can appear in a low-dimensional latent space (for example, using Latent Dirichlet Allocation [Blei, Ng, and Jordan 2003]) and infer entailment relations between predicates based on the contexts in which they appear (Ritter, Mausam, and Etzioni 2010; O´ Se´aghdha 2010; Dinu and Lapata 2010; Melamud et al. 2013). In the experiments presented in this article we will use the representation of Schoenmackers et al. in one experiment, and ignore the problem of predicate ambiguity in the other. 225  Computational Linguistics  Volume 41, Number 2  2.2 Global Learning The idea of global learning is that, by jointly learning semantic relations between a large number of natural language phrases, one can use the dependencies between the relations to improve accuracy. A natural way to model that is with a graph where nodes are phrases and edges represent semantic similarity. Snow, Jurafsky, and Ng (2006) presented one of the early examples of global learning in the context of learning noun taxonomies. In their work, they enforced a transitivity constraint over the taxonomy using a greedy inference procedure and demonstrated that this improves the quality of the taxonomy. Transitivity constraints were also enforced by Yates and Etzioni (2009), who proposed a clustering algorithm for learning undirected synonymy relations. Nakashole, Weikum, and Suchanek (2012) learned a taxonomy of binary predicates and also enforced transitivity with a greedy algorithm. Berant, Dagan, and Goldberger (2012) developed a global method for learning entailment relations between predicates. In this article we present scalable approximation algorithms for the optimization problem they propose, and so we provide further detail on their method. The input to their algorithm is a large corpus and a lexicographic resource, such as WordNet, and the output is a set of entailment rules that respect the constraint of transitivity. The algorithm is composed of two main steps. In the ﬁrst step, a set of predicates is extracted from the corpus and a local entailment classiﬁer is trained based on examples automatically generated from the lexicographic resource. At this point, we can derive for each pair of predicates (i, j) a score wij ∈ R that estimates whether i ⇒ j. In the second step of their algorithm, they construct a graph, where predicates are nodes and edges represent entailment rules. Using the local scores they look for the set of edges E that maximizes the objective function (i,j)∈E wij under the constraint that edges respect transitivity. They show that this optimization problem is NP-hard and ﬁnd an exact solution using an ILP solver over small graphs. They also avoid problems of predicate ambiguity by partially contextualizing the predicates. In this article, we present efﬁcient and scalable heuristic approximation algorithms for the optimization problem they propose. In recent years, there has been substantial work on approximation algorithms for global inference problems. Do and Roth (2010) suggested a method for the related task of learning taxonomic relations between terms. Given a pair of terms, they construct a small graph that contains the two terms and a few other related terms, and then impose constraints on the graph structure. They construct these small graphs because their work is geared towards scenarios where relations are determined on-the-ﬂy for a given pair of terms and no global knowledge base is ever explicitly constructed. Because they independently construct a graph for each pair of terms, their method easily produces solutions where global constraints, such as transitivity, are violated. Another approximation method that violates transitivity constraints is LP relaxation (Martins, Smith, and Xing 2009). In LP relaxation, binary variables are replaced by continuous variables, transforming the problem from an ILP to a Linear Program (LP), which is polynomial. An LP solver is then applied, and variables that are assigned a fractional value are rounded to their nearest integer, so many violations of transitivity may occur. The solution when applying LP relaxation is not a transitive graph; we will show in Section 4 that our approximation method is substantially faster. Global inference has gained popularity in recent years in NLP, and a common approximation method that has been extensively utilized is dual decomposition (Sontag, Globerson, and Jaakkola 2011). Dual decomposition has been successfully applied in  226  Berant et al.  Efﬁcient Global Learning of Entailment Graphs  tasks such as Information Extraction (Reichart and Barzilay 2012), Machine Translation (Chang and Collins 2011), Parsing (Rush et al. 2012), and Named Entity Recognition (Wang, Che, and Manning 2013). To the best of our knowledge, it has not yet been applied for the task of learning entailment relations. The graph decomposition method we present in Section 3.1 can be viewed as an ideal case of dual decomposition, where we can decompose the problem into disjoint components in a way that we do not need to ensure consistency of the results obtained on each component separately. 3. Efﬁcient Inference Our goal is to learn a large knowledge base of entailment rules between natural language predicates. Following Berant, Dagan, and Goldberger (2012), we formulate this task as a graph-learning problem, where, given the nodes of an entailment graph, we would like to ﬁnd the best set of edges that respect a global transitivity constraint. Our main modeling assumption is that textual entailment is a transitive relation. Berant, Dagan, and Goldberger (2012) have demonstrated that this modeling assumption holds for focused entailment graphs, that is, graphs in which predicates are disambiguated by one of their arguments. For example, a graph that focuses on the concept nausea might contain a en entailment rule between predicates such as X prevents nausea ⇒ X affects nausea, where the predicate X prevent Y is disambiguated by the instantiation of the argument nausea. In this work, we will examine this modeling assumption for more general graphs. In Section 4 we show that transitivity holds in typed entailment graphs, that is, graphs where each predicate speciﬁes the semantic type of its arguments (for example, Xdrug prevent Ysymptom). In Section 5.4, we examine the assumption of transitivity when predicates do not carry any typing information (for example, X prevent Y); and we observe that in this setting the assumption of transitivity is often violated because of the predicate ambiguity. Nevertheless, we show that even in this set-up we can empirically take advantage of the transitivity assumption. Let V be a set of predicates, which are the nodes of the entailment graph, and let w : V × V → R be an entailment weighting function. Given a pair of predicates (i, j), a positive wij indicates local tendency to decide that i entails j, whereas a negative wij indicates local tendency to decide that i does not entail j. We want to ﬁnd a global entailment transitive graph that is most consistent with the local cues. Formally, our goal is to ﬁnd the directed graph G = (V, E) that maximizes the sum of edge weights (i,j)∈E wij, under the constraint that the graph is transitive—that is, for every triple of nodes (i, j, k), if (i, j) ∈ E and ( j, k) ∈ E, then (i, k) ∈ E. Berant, Dagan, and Goldberger (2012) proved that this optimization problem (which we term Max-Trans-Graph) is NP-hard, and provided an ILP formulation for it. Let xij be an indicator for whether i ⇒ j, then x = {xij : i = j} are the variables of the following ILP:  max x  wijxij  (3)  i=j  s.t.  ∀i, j, k ∈ V  xij + xjk − xik ≤ 1  ∀i, j ∈ V  xij ∈ {0, 1}  227  Computational Linguistics  Volume 41, Number 2  The objective function is the sum of weights over the edges of G, and the constraint xij + xjk − xik ≤ 1 on the binary variables enforces transitivity (i.e., xij = xjk = 1 implies that xik = 1). The weighting function w is trained separately using supervised learning methods and we describe the details of training for each one of our experiments in Sections 4 and 5. There is a simple probabilistic modeling that motivates the score (3) that we optimize. Assume that for each pair of nodes (i, j), we are given a probability pij(1) = p(xij = 1) that i ⇒ j (the probability that i j is denoted by pij(0) = 1 − pij(1)). Assuming a uniform probability over graphs, the posterior probability (which can also be viewed as the likelihood) of a graph, represented by an edge-set x, is:  p(x) ∝ pij(xij)  (4)  i=j  It can be easily veriﬁed that:1  log  p(xij )  =  log  pij (1 ) pij (0 )  xij  +  log  pij (0 )  (5)  Hence,  log p(x) = log p(xij) = wijxij + const  (6)  i=j  i=j  such that ‘const’ is a scalar that does not depend on x and  wij  =  log  pij (1 ) pij (0 )  (7)  The optimal entailment graph, therefore, is arg maxx p(x) = arg maxx i=j wijxij, where the maximization is over all the transitive graphs. Hence the most likely transitive entailment graph is obtained as the solution of the ILP maximization problem (3). Berant, Dagan, and Goldberger solved this optimization problem with an ILP solver, but because ILP is NP-hard, this does not scale well; the number of variables is O(|V|2) and the number of constraints is O(|V|3). Thus, even a graph with 80 nodes (predicates) has more than half a million constraints. In this section, we describe a heuristic algorithm that empirically provides high-quality solutions for this optimization problem in graphs with tens of thousands of nodes. Our algorithm contains two main steps. The ﬁrst step (Section 3.1) is based on a structural assumption that entailment graphs are relatively sparse—that is, most predicates do not entail one another. This allows us to decompose the graph into smaller components in a way that guarantees that an exact solution for each one of the components results in an exact solution for Max-Trans-Graph. However, often even after decomposition, components are too large and ﬁnding an exact solution is still intractable. The second step (Section 3.2) proposes a heuristic algorithm for ﬁnding a good solution for each one of the components. This step is based on an observation that  
Shujie Liu† Microsoft Research Ming Zhou† Microsoft Research Ke Xu∗ Beihang University We present a statistical parsing framework for sentence-level sentiment classiﬁcation in this article. Unlike previous works that use syntactic parsing results for sentiment analysis, we develop a statistical parser to directly analyze the sentiment structure of a sentence. We show that complicated phenomena in sentiment analysis (e.g., negation, intensiﬁcation, and contrast) can be handled the same way as simple and straightforward sentiment expressions in a uniﬁed and probabilistic way. We formulate the sentiment grammar upon Context-Free Grammars (CFGs), and provide a formal description of the sentiment parsing framework. We develop the parsing model to obtain possible sentiment parse trees for a sentence, from which the polarity model is proposed to derive the sentiment strength and polarity, and the ranking model is dedicated to selecting the best sentiment tree. We train the parser directly from examples of sentences annotated only with sentiment polarity labels but without any syntactic annotations or polarity annotations of constituents within sentences. Therefore we can obtain training data easily. In particular, we train a sentiment parser, s.parser, from a large amount of review sentences with users’ ratings as rough sentiment polarity labels. Extensive experiments on existing benchmark data sets show signiﬁcant improvements over baseline sentiment classiﬁcation approaches. ∗ State Key Laboratory of Software Development Environment, Beihang University, XueYuan Road No.37, HaiDian District, Beijing, P.R. China 100191. E-mail: donglixp@gmail.com; kexu@nlsde.buaa.edu.cn. ∗∗ Contribution during internship at Microsoft Research. † Natural Language Computing Group, Microsoft Research Asia, Building 2, No. 5 Danling Street, Haidian District, Beijing, P.R. China 100080. E-mail: {fuwei, shujliu, mingzhou}@microsoft.com. ‡ Corresponding author. Submission received: 10 December 2013; revised version received: 26 July 2014; accepted for publication: 28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 2  1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classiﬁcation, which identiﬁes sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classiﬁcation. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identiﬁcation as a special text classiﬁcation task and focuses on building classiﬁers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classiﬁcation approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created manually by experts. It has been widely recognized that sentiment expressions are colloquial and evolve over time very frequently. Taking tweets from Twitter1 and movie reviews on IMDb2 as examples, people use very casual language as well as informal and new vocabulary to comment on general topics and movies. In practice, it is not feasible to create and maintain sentiment lexicons to capture sentiment expressions with high coverage. On the other hand, the learning-based approach relies on large annotated samples to overcome the vocabulary coverage and deals with variations of words in sentences. Human ratings in reviews (Maas et al. 2011) and emoticons in tweets (Davidov, Tsur, and Rappoport 2010; Zhao et al. 2012) are extensively used to collect a large number of training corpora to train the sentiment classiﬁer. However, it is usually not easy to design effective features to build the classiﬁer. Among others, unigrams have been reported as the most effective features (Pang, Lee, and Vaithyanathan 2002) in sentiment classiﬁcation. Handling complicated expressions delivering people’s opinions is one of the most challenging problems in sentiment analysis. Compositionalities such as negation, intensiﬁcation, contrast, and their combinations are typical cases. We show some concrete examples here: (1) The movie is not good. [negation] (2) The movie is very good. [intensiﬁcation] (3) The movie is not funny at all. [negation + intensiﬁcation] (4) The movie is just so so, but i still like it. [contrast] (5) The movie is not very good, but i still like it. [negation + intensiﬁcation + contrast] The negation expressions, intensiﬁcation modiﬁers, and the contrastive conjunction can change the polarity (Examples (1), (3), (4), (5)), strength (Examples (2), (3), (5)), or both (Examples (3), (5)) of the sentiment of the sentences. We do not need any detailed explanations here as they can be commonly found and easily understood in people’s  
This paper is concerned with automatic generation of all possible questions from a topic of interest. Speciﬁcally, we consider that each topic is associated with a body of texts containing useful information about the topic. Then, questions are generated by exploiting the named entity information and the predicate argument structures of the sentences present in the body of texts. The importance of the generated questions is measured using Latent Dirichlet Allocation by identifying the subtopics (which are closely related to the original topic) in the given body of texts and applying the Extended String Subsequence Kernel to calculate their similarity with the questions. We also propose the use of syntactic tree kernels for the automatic judgment of the syntactic correctness of the questions. The questions are ranked by considering both their importance (in the context of the given body of texts) and syntactic correctness. To the best of our knowledge, no previous study has accomplished this task in our setting. A series of experiments demonstrate that the proposed topic-to-question generation approach can signiﬁcantly outperform the state-of-the-art results. 1. Introduction We live in an information age where all kinds of information is easily accessible through the Internet. The increasing demand for access to different types of information available online have interested researchers in a broad range of Information Retrieval–related areas, such as question answering, topic detection and tracking, summarization, multimedia retrieval, chemical and biological informatics, text structuring, and text mining. Although search engines do a remarkable job in searching through a heap of information, they have certain limitations, as they cannot satisfy the end users’ information need to have more direct access to relevant documents. For example, if we ask for the impact of the current global ﬁnancial crisis in different parts of the world, we can expect to sift through thousands of results for the answer. This fact can be more understandable by the following scenario. When a user enters a query, they are served with a ranked list of relevant documents by the standard document retrieval systems (i.e., search engines), ∗ University of Lethbridge, 4401 University Drive West, Lethbridge, Alberta, T1K 3M4, Canada. E-mail: chali@cs.uleth.ca. ∗∗ Philips Research North America, 345 Scarborough Rd, Briarcliff Manor, New York, 10510, USA. E-mail: sadid.hasan@philips.com. Submission received: 19 May, 2013; revised submission received: 26 May, 2014; accepted for publication: 22 June, 2014. doi:10.1162/COLI a 00206 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 1  and their search task is usually not over (Chali, Joty, and Hasan 2009). The next step for the user is to look into the documents themselves and search for the precise piece of information they were looking for. This method is time-consuming, and a correct answer could easily be missed by either an incorrect query resulting in missing documents or by careless reading. This is why Question Answering (QA) has received immense attention from the information retrieval, information extraction, machine learning, and natural language processing communities in the last 15 years (Hirschman and Gaizauskas 2001; Strzalkowski and Harabagiu 2008; Kotov and Zhai 2010). The main goal of QA systems is to retrieve relevant answers to natural language questions from a collection of documents rather than using keyword matching techniques to extract documents. Automated QA research focuses on how to respond with exact answers to a wide variety of questions, including: factoid, list, deﬁnition, how, why, hypothetical, semantically constrained, and crosslingual questions (Simmons 1965; Kupiec 1993; Voorhees 1999; Hirschman and Gaizauskas 2001; Greenwood 2005; Wang 2006; Moldovan, Clark, and Bowden 2007). One of the main requirements of a QA system is that it must receive a well-formed question as input in order to come up with the best possible correct answer as output. Available studies revealed that humans are not very skilled in asking good questions about a topic of their interest. They are forgetful in nature; this often restricts them to properly express whatever that is peeking in their mind. Therefore, they would beneﬁt from automated Question Generation (QG) systems that can assist in meeting their inquiry needs (Lauer, Peacock, and Graesser 1992; Graesser et al. 2001; Rus and Graesser 2009; Ali, Chali, and Hasan 2010; Kotov and Zhai 2010; Olney, Graesser, and Person 2012). Another beneﬁt of QG is that it can be a good tool to help improve the quality of the QA systems (Graesser et al. 2001; Rus and Graesser 2009). These beneﬁts of a QG system motivate us to address the important problem of topic-to-question generation, where the main goal is to generate all possible questions about a given topic. For example, given the topic Apple Inc. Logos, we would like to generate questions such as What is Apple Inc.?, Where is Apple Inc. located?, Who designed Apple’s Logo?, and so forth. The problem of topic-to-question generation can be viewed as a generalization of the problem of answering complex questions. Complex questions are essentially broader information requests about a certain topic, whose answers could be obtained from pieces of information scattered in multiple documents. For example, consider the complex question:1 Describe steps taken and worldwide reaction prior to the introduction of the Euro on January 1, 1999. Include predictions and expectations reported in the press. This question is requesting an elaboration about the topic “Introduction of the Euro,” which can be answered by following complex procedures such as question decomposition or inferencing and synthesizing information from multiple documents (e.g., multi-document summarization). Answering complex questions is not easy as it is not always understandable to which direction one should move to search for the answer to a complex question. This situation arises because of the wider focus of the topic that is inherent in the complex question in consideration. For example, a complex question like Describe the tsunami disaster in Japan has a wider focus without a single or well-deﬁned information need. To narrow down the focus, this question can be decomposed into a series of simple questions such as How many people were killed in the tsunami?, How many people became homeless?, Which cities were mostly damaged?, and so on.  
Wenjie Li† The Hong Kong Polytechnic University Xiaohua Liu∗∗ Microsoft Research Ming Zhou∗∗ Microsoft Research In this article we address the task of cross-lingual sentiment lexicon learning, which aims to automatically generate sentiment lexicons for the target languages with available English sentiment lexicons. We formalize the task as a learning problem on a bilingual word graph, in which the intra-language relations among the words in the same language and the interlanguage relations among the words between different languages are properly represented. With the words in the English sentiment lexicon as seeds, we propose a bilingual word graph label propagation approach to induce sentiment polarities of the unlabeled words in the target language. Particularly, we show that both synonym and antonym word relations can be used to build the intra-language relation, and that the word alignment information derived from bilingual parallel sentences can be effectively leveraged to build the inter-language relation. The evaluation of Chinese sentiment lexicon learning shows that the proposed approach outperforms existing approaches in both precision and recall. Experiments conducted on the NTCIR data set further demonstrate the effectiveness of the learned sentiment lexicon in sentence-level sentiment classiﬁcation. ∗ Contribution during internship at Microsoft Research (Beijing). ∗∗ Furu Wei, Xiaohua Liu, and Ming Zhou are from Microsoft Research, Beijing, China. E-mail: {fuwei, xiaoliu, mingzhou}@microsoft.com. † Dehong Gao and Wenjie Li are from the Department of Computing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu@163.com, cswjli@comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 1  1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classiﬁcation (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predeﬁned sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Okumura 2005; Rao and Ravichandran 2009). However, current work mainly focuses on English sentiment lexicon generation or expansion, while sentiment lexicon learning for other languages has not been well studied. In this article, we address the issue of cross-lingual sentiment lexicon learning, which aims to generate sentiment lexicons for a non-English language (hereafter referred to as “the target language”) with the help of the available English sentiment lexicons. The underlying motivation of this task is to leverage the existing English sentiment lexicons and substantial linguistic resources to label the sentiment polarities of the words in the target language. To this end, we need an approach to transferring the sentiment information from English words to the words in the target language. The few existing approaches ﬁrst build word relations between English and the target language. Then, based on the word relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language. In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the literature. One is based on translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). The two approaches in Duh, Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language. To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. Speciﬁcally, we model this task with a bilingual word graph, which is composed of two intra-language subgraphs and an interlanguage subgraph. The intra-language subgraphs are used to model the semantic relations among the words in the same languages. When building them, we incorporate both synonym and antonym word relations in a novel manner, represented by positive and negative sign weights in the subgraphs, respectively. These two intra-language subgraphs are then connected by the inter-language subgraph. We propose Bilingual word graph Label Propagation (BLP), which simultaneously takes the inter-language relations and the intra-language relations into account in an iterative way. Moreover, we leverage the word alignment information derived from a parallel corpus to build the inter-language relations. We connect two words from different languages that are aligned to each other in a parallel sentence pair. Taking advantage of a large parallel corpus, this approach signiﬁcantly improves the coverage of the generated sentiment lexicon. The experimental results on Chinese sentiment lexicon learning show the effectiveness of the proposed approach in terms of both precision and recall. We further  22  Gao et al.  Cross-lingual Sentiment Lexicon Learning  evaluate the impact of the learned sentiment lexicon on sentence-level sentiment classiﬁcation. When using words in the learned sentiment lexicon as features for sentiment classiﬁcation of the target language, the sentiment classiﬁcation can achieve a high performance. We make the following contributions in this article. 1. We present a generic approach to automatically learning sentiment lexicons for the target language with the available sentiment lexicon in English, and we formalize the cross-lingual sentiment learning task on a bilingual word graph. 2. We build a bilingual word graph by using synonym and antonym word relations and propose a bilingual word graph label propagation approach, which effectively leverages the inter-language relations and both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning. 3. We leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve signiﬁcant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predeﬁned sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. For example, the conjunction word and tends to link two words with the same polarity, whereas the conjunction word but is likely to link two words with opposite polarities. Their approach only considers adjectives, not nouns or verbs, and it is unable to extract adjectives that are not conjoined by conjunctions. Riloff et al. (2003) deﬁne several pattern templates and extract sentiment words by two bootstrapping approaches. Turney and Littman (2003) calculate the pointwise mutual information (PMI) of a given word with positive and negative sets of sentiment words. The sentiment polarity of the word is determined by average PMI values of the positive and negative sets. To obtain PMI, they provide queries (consisting of the given word and the sentiment word) to the search engine. The number of hits and the position (if the given word is near the sentiment word) are used to estimate the association of the given word to the sentiment word. Hu and Liu (2004) research sentiment word learning on customer reviews and they assume that the sentiment words tend to be correlated with product features. The frequent nouns and noun phrases are treated as product features. Then they extract the adjective words  23  Computational Linguistics  Volume 41, Number 1  as sentiment words from those sentences that contain one or more product features. This approach may work on a product review corpus, where one product feature may frequently appear. But for other corpora, like news articles, this approach may not be effective. Qiu et al. (2011) combine sentiment lexicon learning and opinion target extraction. A double propagation approach is proposed to learn sentiment words and to extract opinion targets simultaneously, based on eight manually deﬁned rules. The semantic-based approaches determine the sentiment polarity of a given word according to the word semantic relation, like the synonyms of sentiment seed words. The word semantic relation is usually obtained from dictionaries, for example, WordNet.1 Kim and Hovy (2004) assume that the synonyms of a positive (negative) word are positive (negative) and its antonyms are negative (positive). Initializing with a set of sentiment words, they expand sentiment lexicons based on these two kinds of word relations. Kamps et al. (2004) build a synonym graph according to the synonym relation (synset) derived from WordNet. The sentiment polarity of a word is calculated by the shortest path to two sentiment words good and bad. However, the shortest path cannot precisely describe the sentiment orientation, considering there are only ﬁve steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the polarity orientations of the words in WordNet. They select seven positive words and seven negative words and expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classiﬁer to predict the sentiment polarities of all the words in WordNet and use the glosses (textual deﬁnitions of the words in WordNet) as the features of classiﬁcation. The sentiment lexicon generated is the well-known SentiWordNet.2 2.2 Cross-Lingual Sentiment Lexicon Learning The work on cross-lingual sentiment lexicon learning is still at an early stage and can be categorized into two types, according to how they bridge the words in two languages. Mihalcea et al. (2007) generate sentiment lexicon for Romanian by directly translating the English sentiment words into Romanian through bilingual English–Romanian dictionaries. When confronting multiword translations, they translate the multiwords word by word. Then the validated translations must occur at least three times on the Web. The approach proposed by Hassan et al. (2011) learns sentiment words based on English WordNet and WordNets in the target languages (e.g., Hindi and Arabic). Crosslingual dictionaries are used to connect the words in two languages and the polarity of a given word is determined by the average hitting time from the word to the English sentiment word set. These approaches connect words in two languages based on crosslingual dictionaries. The main concern of these approaches is the effect of morphological inﬂection (i.e., a word may be mapped to multiple words in cross-lingual dictionaries).  
In parsing with Tree Adjoining Grammar (TAG), independent derivations have been shown by Schabes and Shieber (1994) to be essential for correctly supporting syntactic analysis, semantic interpretation, and statistical language modeling. However, the parsing algorithm they propose is not directly applicable to Feature-Based TAGs (FB-TAG). We provide a recognition algorithm for FB-TAG that supports both dependent and independent derivations. The resulting algorithm combines the beneﬁts of independent derivations with those of Feature-Based grammars. In particular, we show that it accounts for a range of interactions between dependent vs. independent derivation on the one hand, and syntactic constraints, linear ordering, and scopal vs. nonscopal semantic dependencies on the other hand. 1. Introduction A Tree Adjoining Grammar (TAG; Joshi and Schabes 1997) consists of a set of elementary trees and two combining operations, substitution and adjunction. Consequently, a TAG derivation can be described by a tree (called a derivation tree) specifying which elementary TAG trees were combined using which operations to yield that derivation. In this tree, each vertex is labeled with a tree name and each edge with a description of the operation (node address and operation type) used to combine the trees labeling its end vertices. As we shall see in Section 3.2, in TAG, each derivation tree speciﬁes a unique parse tree, also called derived tree. In previous work, it has been argued that TAG derivation trees provide a good approximation of semantic dependencies between the words of a sentence (Kroch 1989; Rambow, Vijay-Shanker, and Weir 1995; Candito and Kahane 1998; Kallmeyer and Kuhlmann 2012). As shown by Schabes and Shieber (1994), however, there are several possible ways of deﬁning TAG derivation trees, depending on how multiple adjunction of several auxiliary trees at the same tree node is handled. The standard notion of derivation proposed by Vijay-Shanker (1987) forbids multiple adjunction, thus enforcing dependent derivations. In contrast, the extended notion of derivation proposed by Schabes ∗ UMR 7503, Campus Scientiﬁque, BP 239, F-54506 Vandoeuvre-le`s-Nancy Cedex, France. E-mail:{claire.gardent,shashi.narayan}@loria.fr. Submission received: 2 August 2013; revised version received: 16 June 2014; accepted for publication: 21 September 2014. doi:10.1162/COLI a 00217 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 1  N pepper αpepper  N Adj N∗ roasted βroasted  N Adj N∗ red βred  αpepper 0 βred 0 βroasted Standard (dependent) Derivation  αpepper 00 βred βroasted Extended (independent) Derivation  Figure 1 An example TAG with the alternative TAG derivations for the phrase roasted red pepper. αpepper, βred, and βroasted are the elementary trees for pepper (initial tree), red (auxiliary tree), and roasted (auxiliary tree), respectively.  and Shieber (1992, 1994) allows multiple adjunction at a single node, thereby yielding so-called independent derivations (i.e., derivations where the relation between the adjoining trees is left unspeciﬁed). The difference between the two types of derivations is illustrated in Figure 1. While in the standard (dependent) derivation, one adjective tree is adjoined to the other adjective tree, which itself is adjoined to the noun tree for pepper; in the extended (independent) derivation, both adjective trees adjoin to the noun tree. Schabes and Shieber (1994) argue that allowing both for dependent and independent derivations better reﬂects linguistic dependencies. Making use of the distinction introduced in TAG between predicative and modiﬁer auxiliary trees (Schabes and Shieber (1994), Section 3.1), they deﬁne a parsing algorithm that assigns dependent derivations to predicative auxiliary trees but independent derivations to multiple modiﬁer auxiliary trees adjoining to the same node. In case both predicative and modiﬁer auxiliary trees adjoin to the same node, their parsing algorithm ensures that predicative trees appear above the modiﬁer trees in the derived tree. This parsing algorithm is deﬁned for featureless variants of TAG. In contrast, in implemented TAGs (e.g., XTAG [The XTAG Research Group 2001], SemXTAG [Gardent 2008], or XXTAG1 [Alahverdzhieva 2008]) feature structures and feature uniﬁcation are central. They are used to minimize the size of the grammar; to model linguistic phenomena such as verb/subject agreement; and to encode a uniﬁcation-based syntax/semantics interface (e.g., Gardent and Kallmeyer 2003). In this article, we extend Schabes and Shieber’s proposal to Feature-Based TAG (FB-TAG); and we show that the resulting parsing algorithm naturally accounts for the interplay of dependent vs. independent derivation structures with syntactic constraints, linear ordering, and scopal vs. nonscopal semantic dependencies. The article is organized as follows. In Section 2, we recap the motivations for independent derivations put forward by Schabes and Shieber (1994) and we brieﬂy discuss the interactions that may arise between dependent and independent derivations. Section 3 summarizes their approach. In Section 4, we present the intuitions and motivations underlying our proposal and we highlight the differences with Schabes and Shieber’s approach. Section 5 presents our proposal. Section 6 concludes.  2. Why Are Independent Derivations Desirable?  We start by summarizing Schabes and Shieber’s motivations for independent derivations. We then discuss the interactions between dependent and independent derivations.  
Modeling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists. The categorical model of Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) provides a solution by unifying a categorial grammar and a distributional model of meaning. It takes into account syntactic relations during semantic vector composition operations. But the setting is abstract: It has not been evaluated on empirical data and applied to any language tasks. We generate concrete models for this setting by developing algorithms to construct tensors and linear maps and instantiate the abstract parameters using empirical data. We then evaluate our concrete models against several experiments, both existing and new, based on measuring how well models align with human judgments in a paraphrase detection task. Our results show the implementation of this general abstract framework to perform on par with or outperform other leading models in these experiments.1 1. Introduction The distributional approach to the semantic modeling of natural language, inspired by the notion—presented by Firth (1957) and Harris (1968)—that the meaning of a word is tightly related to its context of use, has grown in popularity as a method of semantic representation. It draws from the frequent use of vector-based document models in information retrieval, modeling the meaning of words as vectors based on the distribution of co-occurring terms within the context of a word. Using various vector similarity metrics as a measure of semantic similarity, these distributional semantic models (DSMs) are used for a variety of NLP tasks, from ∗ DeepMind Technologies Ltd, 5 New Street Square, London EC4A 3 TW. E-mail: etg@google.com. ∗∗ School of Electronic Engineering and Computer Science, Queen Mary University of London, Mile End Road, London E1 4NS, United Kingdom. E-mail: mehrnoosh.sadrzadeh@qmul.ac.uk. † The work described in this article was performed while the authors were at the University of Oxford. 1 Support from EPSRC grant EP/J002607/1 is acknowledged. Submission received: 26 September 2012; revised submission received: 31 October 2013; accepted for publication: 5 April 2014. doi:10.1162/COLI a 00209 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 1  automated thesaurus building (Grefenstette 1994; Curran 2004) to automated essay marking (Landauer and Dumais 1997). The broader connection to information retrieval and its applications is also discussed by Manning, Raghavan, and Schu¨ tze (2011). The success of DSMs in essentially word-based tasks such as thesaurus extraction and construction (Grefenstette 1994; Curran 2004) invites an investigation into how DSMs can be applied to NLP and information retrieval (IR) tasks revolving around larger units of text, using semantic representations for phrases, sentences, or documents, constructed from lemma vectors. However, the problem of compositionality in DSMs—of how to go from word to sentence and beyond—has proved to be non-trivial. A new framework, which we refer to as DisCoCat, initially presented in Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) reconciles distributional approaches to natural language semantics with the structured, logical nature of formal semantic models. This framework is abstract; its theoretical predictions have not been evaluated on real data, and its applications to empirical natural language processing tasks have not been studied. This article is the journal version of Grefenstette and Sadrzadeh (2011a, 2011b), which ﬁll this gap in the DisCoCat literature; in it, we develop a concrete model and an unsupervised learning algorithm to instantiate the abstract vectors, linear maps, and vector spaces of the theoretical framework; we develop a series of empirical natural language processing experiments and data sets and implement our algorithm on large scale real data; we analyze the outputs of the algorithm in terms of linear algebraic equations; and we evaluate the model on these experiments and compare the results with other competing unsupervised models. Furthermore, we provide a linear algebraic analysis of the algorithm of Grefenstette and Sadrzadeh (2011a) and present an in-depth study of the better performance of the method of Grefenstette and Sadrzadeh (2011b). We begin in Section 2 by presenting the background to the task of developing compositional distributional models. We brieﬂy introduce two approaches to semantic modeling: formal semantic models and distributional semantic models. We discuss their differences, and relative advantages and disadvantages. We then present and critique various approaches to bridging the gap between these models, and their limitations. In Section 3, we summarize the categorical compositional distributional framework of Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) and provide the theoretical background necessary to understand it; we also sketch the road map of the literature leading to the development of this setting and outline the contributions of this paper to the ﬁeld. In Section 4, we present the details of an implementation of this framework, and introduce learning algorithms used to build semantic representations in this implementation. In Section 5, we present a series of experiments designed to evaluate this implementation against other unsupervised distributional compositional models. Finally, in Section 6 we discuss these results, and posit future directions for this research area. 2. Background Compositional formal semantic models represent words as parts of logical expressions, and compose them according to grammatical structure. They stem from classical ideas in logic and philosophy of language, mainly Frege’s principle that the meaning of a sentence is a function of the meaning of its parts (Frege 1892). These models relate to well-known and robust logical formalisms, hence offering a scalable theory of meaning that can be used to reason about language using logical tools of proof and inference. Distributional models are a more recent approach to semantic modeling, representing 72  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  the meaning of words as vectors learned empirically from corpora. They have found their way into real-world applications such as thesaurus extraction (Grefenstette 1994; Curran 2004) or automated essay marking (Landauer and Dumais 1997), and have connections to semantically motivated information retrieval (Manning, Raghavan, and Schu¨ tze 2011). This two-sortedness of deﬁning properties of meaning: “logical form” versus “contextual use,” has left the quest for “what is the foundational structure of meaning?”—a question initially the concern of solely linguists and philosophers of language—even more of a challenge. In this section, we present a short overview of the background to the work developed in this article by brieﬂy describing formal and distributional approaches to natural language semantics, and providing a non-exhaustive list of some approaches to compositional distributional semantics. For a more complete review of the topic, we encourage the reader to consult Turney (2012) or Clark (2013).  2.1 Montague Semantics  Formal semantic models provide methods for translating sentences of natural language into logical formulae, which can then be fed to computer-aided automation tools to reason about them (Alshawi 1992). To compute the meaning of a sentence consisting of n words, meanings of these words must interact with one another. In formal semantics, this further interaction is represented as a function derived from the grammatical structure of the sentence. Such models consist of a pairing of syntactic analysis rules (in the form of a grammar) with semantic interpretation rules, as exempliﬁed by the simple model presented on the left of Figure 1. The semantic representations of words are lambda expressions over parts of logical formulae, which can be combined with one another to form well-formed logical expressions. The function | − | : L → M maps elements of the lexicon L to their interpretation (e.g., predicates, relations, domain objects) in the logical model M used. Nouns are typically just logical atoms, whereas adjectives and verbs and other relational words are interpreted as predicates and relations. The parse of a sentence such as “cats like milk,” represented here as a binarized parse tree, is used to produce its semantic interpretation by substituting semantic representations for their grammatical constituents and applying β-reduction where needed. Such a derivation is shown on the right of Figure 1. What makes this class of models attractive is that it reduces language meaning to logical expressions, a subject well studied by philosophers of language, logicians, and linguists. Its properties are well known, and it becomes simple to evaluate the meaning of a sentence if given a logical model and domain, as well as verify whether or not one sentence entails another according to the rules of logical consequence and deduction.  |like|(|cats|, |milk|)  Syntactic Analysis Semantic Interpretation  S → NP VP  |VP|(|NP|)  NP → cats, milk, etc. |cats|, |milk|, . . .  VP → Vt NP  |Vt|(|NP|)  ⇒ |cats|  λx.|like|(x, |milk|)  Vt → like, hug, etc. λyx.|like|(x, y), . . .  λyx.|like|(x, y) |milk|  Figure 1 A simple model of formal semantics.  73  Computational Linguistics  Volume 41, Number 1  However, such logical analysis says nothing about the closeness in meaning or topic of expressions beyond their truth-conditions and which models satisfy these truth conditions. Hence, formal semantic approaches to modeling language meaning do not perform well on language tasks where the notion of similarity is not strictly based on truth conditions, such as document retrieval, topic classiﬁcation, and so forth. Furthermore, an underlying domain of objects and a valuation function must be provided, as with any logic, leaving open the question of how we might learn the meaning of language using such a model, rather than just use it.  2.2 Distributional Semantics  A popular way of representing the meaning of words in lexical semantics is as distributions in a high-dimensional vector space. This approach is based on the distributional hypothesis of Harris (1968), who postulated that the meaning of a word was dictated by the context of its use. The more famous dictum stating this hypothesis is the statement of Firth (1957) that “You shall know a word by the company it keeps.” This view of semantics has furthermore been associated (Grefenstette 2009; Turney and Pantel 2010) with earlier work in philosophy of language by Wittgenstein (presented in Wittgenstein 1953), who stated that language meaning was equivalent to its real world use. Practically speaking, the meaning of a word can be learned from a corpus by looking at what other words occur with it within a certain context, and the resulting distribution can be represented as a vector in a semantic vector space. This vectorial representation is convenient because vectors are a familiar structure with a rich set of ways of computing vector distance, allowing us to experiment with different word similarity metrics. The geometric nature of this representation entails that we can not only compare individual words’ meanings with various levels of granularity (e.g., we might, for example, be able to show that cats are closer to kittens than to dogs, but that all three are mutually closer than cats and steam engines), but also apply methods frequently called upon in IR tasks, such as those described by Manning, Raghavan, and Schu¨ tze (2011), to group concepts by topic, sentiment, and so on. The distribution underlying word meaning is a vector in a vector space, the basis vectors of which are dictated by the context. In simple models, the basis vectors will be annotated with words from the lexicon. Traditionally, the vector spaces used in such models are Hilbert spaces (i.e., vector spaces with orthogonal bases, such that the inner product of any one basis vector with another [other than itself] is zero). The semantic vector for any word can be represented as the weighted sum of the basis vectors:  −−−−−−−→ some word  =  ci−→ni  i  where {−→ni }i is the basis is the weight associated  owfitthhebvaescistovrescptoarce−→ntih. e  meaning  of  the  word  lives  in,  and  ci  ∈  R  The word ni  construction of associated with  tbhaesivsevcteocrtofror−→nia,  word is done by how many times  counting, for each lexicon it occurs in the context of  each occurrence of the word for which we are constructing the vector. This count is then  typically adjusted according to a weighting scheme (e.g., TF-IDF). The “context” of a  word can be something as simple as the other words occurring in the same sentence as  74  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  the word or within k words of it, or something more complex, such as using dependency relations (Pado´ and Lapata 2007) or other syntactic features. Commonly, the similarity of two semantic vectors is computed by taking their cosine measure, which is the sum of the product of the basis weights of the vectors:  cosine(−→a ,  −→ b)  =  i cai cbi i (cai )2 i (cbi )2  where  cai  and  cbi  are  the  basis  weights  for  −→a  and  −→ b,  respectively.  However,  other  options may be a better ﬁt for certain implementations, typically dependent on the  weighting scheme.  Readers interested in learning more about these aspects of distributional lexical  semantics are invited to consult Curran (2004), which contains an extensive overview  of implementation options for distributional models of word meaning.  2.3 Compositionality and Vector Space Models  In the previous overview of distributional semantic models of lexical semantics, we have seen that DSMs are a rich and tractable way of learning word meaning from a corpus, and obtaining a measure of semantic similarity of words or groups of words. However, it should be fairly obvious that the same method cannot be applied to sentences, whereby the meaning of a sentence would be given by the distribution of other sentences with which it occurs. First and foremost, a sentence typically occurs only once in a corpus, and hence substantial and informative distributions cannot be created in this manner. More importantly, human ability to understand new sentences is a compositional mechanism: We understand sentences we have never seen before because we can generate sentence meaning from the words used, and how they are put into relation. To go from word vectors to sentence vectors, we must provide a composition operation allowing us to construct a sentence vector from a collection of word vectors. In this section, we will discuss several approaches to solving this problem, their advantages, and their limitations.  2.3.1 Additive Models. The simplest composition operation that comes to mind is straightforward vector addition, such that:  −→ ab  =  −→a  +  −→ b  Conceptually speaking, if we view word vectors as semantic information distributed across a set of properties associated with basis vectors, using vector addition as a semantic composition operation states that the information of a set of lemmas in a sentence is simply the sum of the information of the individual lemmas. Although crude, this approach is computationally cheap, and appears sufﬁcient for certain NLP tasks: Landauer and Dumais (1997) show it to be sufﬁcient for automated essay marking tasks, and Grefenstette (2009) shows it to perform better than a collection of other simple similarity metrics for summarization, sentence paraphrase, and document paraphrase detection tasks. vectHoroawdedvietiro, nthiesrecoamremtwuotaptirvinec, itphaelroefbojerect,i−Jo−onh−s−nt−od−ar−ad−nd−kit−iw−vie−n→me o=d−Jeol−sh→onf+cod−m−rap−n→oksi+tiow−n−:i−n→ﬁers=t,  75  Computational Linguistics  Volume 41, Number 1  −−−−−−−−−−−−→ Wine drank John, and thus vector addition ignores syntactic structure completely; and second, vector addition sums the information contained in the vectors, effectively jumbling the meaning of words together as sentence length grows. The ﬁrst objection is problematic, as the syntactic insensitivity of additive models leads them to equate the representation of sentences with patently different meanings. Mitchell and Lapata (2008) propose to add some degree of syntactic sensitivity—namely, accounting for word order—by weighting word vectors according to their order of appearance in a sentence as follows:  −→ ab  =  α−→a  +  −→ βb  where α, β ∈ R. Consequently, −J−o−h−n−−d−−r−−a−−n−−k−−w−−−i−n→−e−=−−α→· not have the same representation as Wine drank John  J−=o−h→αn·+w−−βi−n→·e−d+−ra−βn→k· −d+−ra−γn→·k−w−+i−n→eγ  w· −Joo−uh→lnd.  The question of how to obtain weights and whether they are only used to reﬂect  word order or can be extended to cover more subtle syntactic information is open,  but it is not immediately clear how such weights may be obtained empirically and  whether this mode of composition scales well with sentence length and increase in  syntactic complexity. Guevara (2010) suggests using machine-learning methods such  as partial least squares regression to determine the weights empirically, but states that  this approach enjoys little success beyond minor composition such as adjective-noun  or noun-verb composition, and that there is a dearth of metrics by which to evaluate  such machine learning–based systems, stunting their growth and development.  The second objection states that vector addition leads to increase in ambiguity as  we construct sentences, rather than decrease in ambiguity as we would expect from  giving words a context; and for this reason Mitchell and Lapata (2008) suggest replacing  additive models with multiplicative models as discussed in Section 2.3.2, or combining  them with multiplicative models to form mixture models as discussed in Section 2.3.3.  2.3.2 Multiplicative Models. The multiplicative model of Mitchell and Lapata (2008) is  an attempt to solve the ambiguity problem discussed in Section 2.3.1 and provide  implicit disambiguation during composition. The composition operation proposed is  the component-wise multiplication ( ) of two vectors: Vectors are expressed as the  weighted sum of their basis vectors, and  −p→osed b=  vector is the product i ci −→ni , we have  of  the  weights  the weight of of the original  tvheectboarss;isfovre−→cato=rs  of i  tchi−→nei c, oamnd-  −→ ab  =  −→a  −→ b  =  cici −→ni  i  Such multiplicative models are shown by Mitchell and Lapata (2008) to perform better at verb disambiguation tasks than additive models for noun-verb composition, against a baseline set by the original verb vectors. The experiment they use to show this will also serve to evaluate our own models, and form the basis for further experiments, as discussed in Section 5. This approach to compositionality still suffers from two conceptual problems: First, component-wise multiplication is still commutative and hence word order is not accounted for; second, rather than “diluting” information during large compositions  76  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  and creating ambiguity, it may remove too much through the “ﬁltering” effect of  component-wise multiplication.  The ﬁrst problem is more difﬁcult to deal with for multiplicative models than for  aadredictoivmemmuotdaetilvs,ebaencaduhseenbcoethαs−→caalarβm−→bul=tipαli−→cbationβ−→aandancodmtphouns ewnto-rwdisoermdeurltciapnlincoattiobne  taken into account using scalar weights.  To illustrate how the second problem entails that multiplicative models do not scale  well with sentence length, we look at the structure of component-wise multiplication  again: −→a  −→ b=  i cici −→ni . For any i, if ci = 0 or ci = 0, then cici = 0, and therefore  for any composition, the number of non-zero basis weights of the produced vector is  less than or equal to the number of non-zero basis weights of the original vectors: At  each composition step information is ﬁltered out (or preserved, but never increased).  Hence, as the number of vectors to be composed grows, the number of non-zero  bTfoahrseiarsenfwyoeriei,g,−→ahfiotsrisoafonrytthhceoopgmoropndoauslicttotiovn−ae−1co−.tf.−o.−rtah−is−e→ta1fyotsrhmethne−a−a1−s1−.−a.−.−m..−−.ae−a−i −i.−o..−−r..−—→.a→anmn==or−→ae−→10r.eIa.tl.ifs.otilcloa−→awlilys—t.hd.ae.tcpreu−→aanrse,elsiyf. multiplicative models alone are not apt as a single mode of composition beyond noun-  verb (or adjective-noun) composition operations.  One solution to this second problem not discussed by Mitchell and Lapata (2008)  would be to such that −→a  in−→troduce b= i  some smoothing factor s (ci + s)(ci + s)−→ni , ensuring  ∈ R+ for point-wise multiplication that information is never completely  ﬁltered out. Seeing how the problem of syntactic insensitivity still stands in the way of  full-blown compositionality for multiplicative models, we leave it to those interested in  salvaging purely multiplicative models to determine whether some suitable value of s  can be determined.  2.3.3 Mixture Models. The problems faced by multiplicative models presented in Section 2.3.2 are acknowledged in passing by Mitchell and Lapata (2008), who propose mixing additive and multiplicative models in the hope of leveraging the advantage of each while doing away with their pitfalls. This is simply expressed as the weighted sum of additive and multiplicative models:  −→ ab  =  α−→a  +  β−→b  +  γ(−→a  −→ b)  where α, β, and γ are predetermined scalar weights. The problems for these models are threefold. First, the question of how scalar weights are to be obtained still needs to be determined. Mitchell and Lapata (2008) concede that one advantage of purely multiplicative models over weighted additive or mixture models is that the lack of scalar weights removes the need to optimize the scalar weights for particular tasks (at the cost of not accounting for syntactic structure), and avoids the methodological concerns accompanying this requirement. Second, the question of how well this process scales from noun-verb composition to more syntactically rich expressions must be addressed. Using scalar weights to account for word order seems ad hoc and superﬁcial, as there is more to syntactic structure than the mere ordering of words. Therefore, an account of how to build sentence vectors for sentences such as The dog bit the man and The man was bitten by the dog in order to give both sentences the same (or a similar) representation would need to give a richer role to scalar weights than mere token order. Perhaps speciﬁc  77  Computational Linguistics  Volume 41, Number 1  weights could be given to particular syntactic classes (such as nouns) to introduce a more complex syntactic element into vector composition, but it is clear that this alone is not a solution, as the weight for nouns dog and man would be the same, allowing for the−s−a−m−e−−c−o−m−m−−u−ta−t−i→ve de−g−e−n−e−r−ac−y−−o−b−se−r−v−e→d in non-weighted additive models, in which the dog bit the man = the man bit the dog. Introducing a mixture of weighting systems accounting for both word order and syntactic roles may be a solution, but it is not only ad hoc but also arguably only partially reﬂects the syntactic structure of the sentence. The third problem is that Mitchell and Lapata (2008) show that in practice, although mixture models perform better at verb disambiguation tasks than additive models and weighted additive models, they perform equivalently to purely multiplicative models with the added burden of requiring parametric optimization of the scalar weights. Therefore, whereas mixture models aim to take the best of additive and multiplicative models while avoiding their problems, they are only partly successful in achieving the latter goal, and demonstrably do little better in achieving the former.  2.3.4 Tensor-Based Models. From Sections 2.3.1–2.3.3 we observe that the need for incor-  porating syntactic information into DSMs to achieve true compositionality is pressing,  if only to develop a non-commutative composition operation that can take into account  word order without the need for adhoc weighting schemes, and hopefully richer syn-  tactic information as well.  An early proposal by Smolensky and colleagues (Smolensky 1990; Smolensky and  Legendre 2006) to use linear algebraic tensors as a composition operation solves the  problem of ﬁnding non-commutative vector composition operators. The composition of  two vectors is their tensor product, sometimes called the plied to vectors rather than vector spaces. For −→a ∈ V =  kronecker i ci−→ni , and  product −→  when  a−→p-  b ∈ W = j cj nj ,  we have:  −→ ab  =  −→a  ⊗  −→ b  =  ci  cj  −→ni  ⊗  −→ nj  ij  The composition operation takes the original vectors and maps them to a vector in a larger vector space V ⊗ W, which is the tensor space of the original vectors’ spaces. Here the second instance of ⊗ is not a recursive application of the kronecker product, but rather the pairing of basis elements of V and W to form a basis element of V ⊗ W. The shared notation and occasional conﬂation of kronecker and tensor products may seem confusing, but is fairly standard in multilinear algebra. The advantage of this approach is twofold: First, vectors for different words need not live in the same spaces but can be composed nonetheless. This allows us to represent vectors for different word classes (topics, syntactic roles, etc.) in different spaces with different bases, which was not possible under additive or multiplicative models. Second, because the product vector lives in a larger space, we obtain the intuitive notion that the information of the whole is richer and more complex than the information of the parts.  Dimensionality Problems. However, as observed and commented upon by Smolensky himself, this increase in dimensionality brings two rather large problems for tensor based models. The ﬁrst is computational: The size of the product vector space is the product of the size of the original vector spaces. If we assume that all words live in the  78  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  same space N of dimensionality dim(N), then the dimensionality of an n-word sentence vector is dim(N)n. If we have as many basis vectors for our word semantic space as there are lexemes in our vocabulary (e.g., approximately 170k in English2), then the size of our sentence vectors quickly reaches magnitudes for which vector comparison (or even storage) are computationally intractable.3 Even if, as most DSM implementations do, we restrict the basis vectors of word semantic spaces to the k (e.g., k = 2,000) most frequent words in a corpus, the sentence vector size still grows exponentially with sentence length, and the implementation problems remain. The second problem is mathematical: Sentences of different length live in different spaces, and if we assign different vector spaces to different word types (e.g., syntactic classes), then sentences of different syntactic structure live in different vector spaces, and hence cannot be compared directly using inner product or cosine measures, leaving us with no obvious mode of semantic comparison for sentence vectors. If any model wishes to use tensor products in composition operations, it must ﬁnd some way of reducing the dimensionality of product vectors to some common vector space so that they may be directly compared. One notable method by which these dimensionality problems can be solved in general are the holographic reduced representations proposed by Plate (1991). The product vector of two vectors is projected into a space of smaller dimensionality by circular convolution to produce a trace vector. The circular correlation of the trace vector and one of the original vectors produces a noisy version of the other original vector. The noisy vector can be used to recover the clean original vector by comparing it with a predeﬁned set of candidates (e.g., the set of word vectors if our original vectors are word meanings). Traces can be summed to form new traces effectively containing several vector pairs from which original vectors can be recovered. Using this encoding/decoding mechanism, the tensor product of sets of vectors can be encoded in a space of smaller dimensionality, and then recovered for computation without ever having to fully represent or store the full tensor product, as discussed by Widdows (2008). There are problems with this approach that make it unsuitable for our purposes, some of which are discussed in Mitchell and Lapata (2010). First, there is a limit to the information that can be stored in traces, which is independent of the size of the vectors stored, but is a logarithmic function of their number. As we wish to be able to store information for sentences of variable word length without having to directly represent the tensored sentence vector, setting an upper bound to the number of vectors that can be composed in this manner limits the length of the sentences we can represent compositionally using this method. Second, and perhaps more importantly, there are restrictions on the nature of the vectors that can be encoded in such a way: The vectors must be independently distributed such that the mean Euclidean length of each vector is 1. Such conditions are unlikely to be met in word semantic vectors obtained from a corpus; and as the failure to do so affects the system’s ability to recover clean vectors, holographic reduced representations are not prima facie usable for compositional DSMs, although it is important to note that Widdows (2008) considers possible application areas where they may be of use, although once again these mostly involve noun-verb and adjectivenoun compositionality rather than full blown sentence vector construction. We retain 2 Source: http://www.oxforddictionaries.com/page/howmanywords. 3 At four bytes per integer, and one integer per basis vector weight, the vector for John loves Mary would require roughly (170, 000 · 4)3 ≈ 280 petabytes of storage, which is over ten times the data Google processes on a daily basis according to Dean and Ghemawat (2008). 79  Computational Linguistics  Volume 41, Number 1  from Plate (1991) the importance of ﬁnding methods by which to project the tensored sentence vectors into a common space for direct comparison, as will be discussed further in Section 3.  Syntactic Expressivity. An additional problem of a more conceptual nature is that using  the tensor product as a composition operation simply preserves word order. As we  discussed in Section 2.3.3, this is not enough on its own to model sentence meaning. We  need to have some means by which to incorporate syntactic analysis into composition  operations.  Early work on including syntactic sensitivity into DSMs by Grefenstette (1992)  suggests using crude syntactic relations to determine the frame in which the distri-  butions for word vectors are collected from the corpus, thereby embedding syntac-  tic information into the word vectors. This idea was already present in the work of  Smolensky, who used sums of pairs of vector representations and their roles, obtained  by taking their tensor products, to obtain a vector representation for a compound. The  application of these ideas to DSMs was studied by Clark and Pulman (2007), who  suggest instantiating the roles to dependency relations and using the distributional  representations of words as the vectors. For example, in the sentence Simon loves red  wine, Simon is the subject of loves, wine is its object, and red is an adjective describing  wine. Hence, from the dependency tree with loves as root node, its subject and object as  children, and their adjectival descriptors (if any) as their children, we read the following  structure:  −−−→ loves  ⊗  −−→ subj  ⊗  −−−−→ Simon  ⊗  −→ obj  ⊗  −−−→ wine  ⊗  −→ adj  ⊗  −→ red.  Using  the  equality  relation  for  inner products of tensor products:  −→a  ⊗  −→ b  |  −→c  ⊗  −→ d  =  −→a | −→c  ×  −→ b  |  −→ d  We can therefore express inner-products of sentence vectors efﬁciently without ever having to actually represent the tensored sentence vector:  −−−−−−−−−−−−→ Simon loves red wine  |  −−−−−−−−−−−−−−→ Mary likes delicious rose´  =  −−→ loves  ⊗  −→ subj  ⊗  −−→ Simon  ⊗  −→ obj  ⊗  −−→ wine  ⊗  −→ adj  ⊗  −→ red  |  −→ likes  ⊗  −→ subj  ⊗  −−→ Mary  ⊗  −→ obj  ⊗  −→ rose´  ⊗  −→ adj  ⊗  −−−−→ delicious  =  −−→ loves  |  −→ likes  ×  −→ subj  |  −→ subj  ×  −−→ Simon  |  −−→ Mary  ×  −→ obj  |  −→ obj  ×  −−→ wine  |  −→ rose´  ×  −→ adj  |  −→ adj  ×  −→ red  |  −−−−→ delicious  =  −−→ loves  |  −→ likes  ×  −−→ Simon  |  −−→ Mary  ×  −−→ wine  |  −→ rose´  ×  −→ red  |  −−−−→ delicious  This example shows that this formalism allows for sentence comparison of sentences with identical dependency trees to be broken down to term-to-term comparison without the need for the tensor products to ever be computed or stored, reducing computation to inner product calculations. However, although matching terms with identical syntactic roles in the sentence works well in the given example, this model suffers from the same problems as the original tensor-based compositionality of Smolensky (1990) in that, by the authors’ own admission, sentences of different syntactic structure live in spaces of different dimensionality and thus cannot be directly compared. Hence we cannot use this to measure the similarity between even small variations in sentence structure, such as the pair “Simon likes red wine” and “Simon likes wine.”  2.3.5 SVS Models. The idea of including syntactic relations to other lemmas in word representations discussed in Section 2.3.4 is applied differently in the structured vector  80  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  space model presented by Erk and Pado´ (2008). They propose to represent word meanings not as simple vectors, but as triplets:  w = (v, R, R−1)  where v is the word vector, constructed as in any other DSM, R and R−1 are selectional preferences, and take the form of R → D maps where R is the set of dependency relations, and D is the set of word vectors. Selectional preferences are used to encode the lemmas that w is typically the parent of in the dependency trees of the corpus in the case of R, and typically the child of in the case of R−1. Composition takes the form of vector updates according to the following protocol. Let a = (va, Ra, Ra−1) and b = (vb, Rb, Rb−1) be two words being composed, and let r be the dependency relation linking a to b. The vector update procedure is as follows:  a = (va b = (vb  Rb−1(r), Ra − {r}, Ra−1 ) Ra(r), Rb, Rb−1 − {r})  where a , b are the updated word meanings, and is whichever vector composition (addition, component-wise multiplication) we wish to use. The word vectors in the triplets are effectively ﬁltered by combination with the lemma which the word they are being composed with expects to bear relation r to, and this relation between the composed words a and b is considered to be used and hence removed from the domain of the selectional preference functions used in composition. This mechanism is therefore a more sophisticated version of the compositional disambiguation mechanism discussed by Mitchell and Lapata (2008) in that the combination of words ﬁlters the meaning of the original vectors that may be ambiguous (e.g., if we have one vector for bank); but contrary to Mitchell and Lapata (2008), the information of the original vectors is modiﬁed but essentially preserved, allowing for further combination with other terms, rather than directly producing a joint vector for the composed words. The added fact that R and R−1 are partial functions associated with speciﬁc lemmas forces grammaticality during composition, since if a holds a dependency relation r to b which it never expects to hold (for example a verb having as its subject another verb, rather than the reverse), then Ra and Rb−1 are undeﬁned for r and the update fails. However, there are some problems with this approach if our goal is true compositionality. First, this model does not allow some of the “repeated compositionality” we need because of the update of R and R−1. For example, we expect that an adjective composed with a noun produces something like a noun in order to be further composed with a verb or even another adjective. However, here, because the relation adj would be removed from Rb−1 for some noun b composed with an adjective a, this new representation b would not have the properties of a noun in that it would no longer expect composition with an adjective, rendering representations of simple expressions like “the new red car” impossible. Of course, we could remove the update of the selectional preference functions from the compositional mechanism, but then we would lose this attractive feature of grammaticality enforcement through the partial functionality of R and R−1. Second, this model does little more than represent the implicit disambiguation that is expected during composition, rather than actually provide a full blown compositional model. The inability of this system to provide a novel mechanism by which to obtain  81  Computational Linguistics  Volume 41, Number 1  a joint vector for two composed lemmas—thereby building towards sentence vectors— entails that this system provides no means by which to obtain semantic representations of larger syntactic structures that can be compared by inner product or cosine measure as is done with any other DSM. Of course, this model could be combined with the compositional models presented in Sections 2.3.1–2.3.3 to produce sentence vectors, but whereas some syntactic sensitivity would have been obtained, the word ordering and other problems of the aforementioned models would still remain, and little progress would have been made towards true compositionality. We retain from this attempt to introduce compositionality in DSMs that including information obtained from syntactic dependency relations is important for proper disambiguation, and that having some mechanism by which the grammaticality of the expression being composed is a precondition for its composition is a desirable feature for any compositional mechanism.  2.4 Matrix-Based Compositionality  The ﬁnal class of approaches to vector composition we wish to discuss are three matrix-based models.  Generic Additive Model. The ﬁrst is the Generic Additive Model of Zanzotto et al. (2010). This is a generalization of the weighted additive model presented in Section 2.3.1. In this model, rather than multiplying lexical vectors by ﬁxed parameters α and β before adding them to form the representation of their combination, they are instead the arguments of matrix multiplication by square matrices A and B:  −→ ab  =  A−→a  +  −→ Bb  Here, A and B represent the added information provided by putting two words into  relation.  o(−→fat,hT−→ebhc,eo−→cmn)ubwminheaertiircoean−→laocfoa−→nnatdean−→nbtdoa−→fbreA. lTeahxniiscdalelBasreinmsinalegnatsriycnsevtdeemcbtoytrhsle,irnaeenbadyr  regression over triplets −→c is the expected output requires the provision of  labeled data for linear regression to be performed. Zanzotto et al. (2010) suggest several  sources for this labeled data, such as dictionary deﬁnitions and word etymologies.  This approach is richer than the weighted additive models because the matrices  act as linear maps on the vectors they take as “arguments,” and thus can encode more  subtle syntactic or semantic relations. However, this model treats all word combinations  as the same operation—for example, treating the combination of an adjective with its  argument and a verb with its subject as the same sort of composition. Because of the  diverse ways there are of training such supervised models, we leave it to those who  wish to further develop this speciﬁc line of research to perform such evaluations.  Adjective Matrices. The second approach is the matrix-composition model of Baroni and Zamparelli (2010), which they develop only for the case of adjective-noun composition, although their approach can seamlessly be used for any other predicate-argument composition. Contrary to most of the earlier approaches proposed, which aim to combine two lexical vectors to form a lexical vector for their combination, Baroni and Zamparelli suggest giving different semantic representations to different types, or more speciﬁcally to adjectives and nouns.  82  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  In this model, nouns are lexical vectors, as with other models. However, embracing  a view of adjectives that is more in line with formal semantics than with distributional  semantics, they model adjectives as linear maps taking lexical vectors as input and pro-  ducing lexical vectors as output. Such linear maps can be encoded as square matrices,  and applied to their matrix encoding the  arguments adjective’s  bliynemaratmriaxpm, aunltdip−nli−ocu→antiobne.tCheonlecxriectaellys,elmetaMntaidcjecvtievectboer  the for  a noun; their combination is simply  −−−−−−−−−→ adjective noun  =  Madjective  ×  n−−ou→n  Similarly to the Generic Additive by linear regression over a set of pairs semantic vectors for the arguments of  t(Mh−ne−oou→dande,lj−→,ecctht)ievwemhienartearitxchoefropvrueecsa,tocahrnsda−nd−−o→cju→encitsaivtrheeetihsseelemlaearxnnicetaidcl  vector corresponding to the expected output of the composition of the adjective with  that noun.  This may, at ﬁrst blush, also appear to be a supervised training method for learn-  ing adjective matrices from “labeled data,” seeing how the expected output vectors  are needed. However, Baroni and Zamparelli (2010) work around this constraint by  automatically producing the labeled data from the corpus by treating the adjective-  noun compound as a single token, and learning its vector using the same distributional  learning methods they used to learn the vectors for nouns. This same approach can be  extended to other unary relations without change and, using the general framework  of the current article, an extension of it to binary predicates has been presented in  Grefenstette et al. (2013), using multistep regression. For a direct comparison of the  results of this approach with some of the results of the current article, we refer the  reader to Grefenstette et al. (2013).  Recursive Matrix-Vector Model. The third approach is the recently developed Recursive Matrix-Vector Model (MV-RNN) of Socher et al. (2012), which claims the two matrixbpaasireidngmoofdaellsexdiecsaclrsibemedanhteircevaescstoprec−→aialwciatshesa.nInopMeVra-tRioNnNm, awtroirxdAs .arWeirtehpinretsheinstemdoadsela, tg(w−→icvo,eFCnoip)rthseotref,atpethiaaoecrnhsvseenoocotnfonari-ttssceeorcnmmhteipilnndocarneleenninno(t−→dt−a→hcee, iAifnso)prtahmrnoedodtfru(e−→acbeeb,diisBnb)pay.rrioazdpeupdcltyeridenegb, ytthhpeeesorefpmoerramantitiniocgnretmhpearetfrsoiexlnlootwaf toiinongne child to the vector of the other, and vice versa, and then projecting both of the products back into the same vector space as the child vectors using a projection matrix W, which must also be learned:  −→c = W ×  B × −→−→a A× b  Second, the matrix C is calculated by projecting the pairing of matrices A and B back into the same space, using a projection matrix WM, which must also be learned:  C = WM ×  A B  83  Computational Linguistics  Volume 41, Number 1  The pairing (−→c , C) obtained through these operations forms the semantic representation of the phrase falling under the scope of the segment of the parse tree below that node. This approach to compositionality yields good results in the experiments described in Socher et al. (2012). It furthermore has appealing characteristics, such as treating relational words differently through their operation matrices, and allowing for recursive composition, as the output of each composition operation is of the same type of object as its inputs. This approach is highly general and has excellent coverage of different syntactic types, while leaving much room for parametric optimization. The principal mathematical difference with the compositional framework presented subsequently is that composition in MV-RNN is always a binary operation; for example, to compose a transitive verb with its subject and object one would ﬁrst need to compose it with its object, and then compose the output of that operation with the subject. The framework we discuss in this article allows for the construction of larger representations for relations of larger arities, permitting the simultaneous composition of a verb with its subject and object. Whether or not this theoretical difference leads to signiﬁcant differences in composition quality requires joint evaluation. Additionally, the description of MV-RNN models in Socher et al. (2012) speciﬁes the need for a source of learning error during training, which is easy to measure in the case of label prediction experiments such as sentiment prediction, but non-trivial in the case of paraphrase detection where no objective label exists. A direct comparison to MV-RNN methods within the context of experiments similar to those presented in this article has been produced by Blacoe and Lapata (2012), showing that simple operations perform on par with the earlier complex deep learning architectures produced by Socher and colleagues; we leave direct comparisons to future work. Early work has shown that the addition of a hidden layer with non-linearities to these simple models will improve the results. 2.5 Some Other Approaches to Distributional Semantics Domains and Functions. In recent work, Turney (2012) suggests modeling word representations not as a single semantic vector, but as a pair of vectors: one containing the information of the word relative to its domain (the other words that are ontologically similar), and another containing information relating to its function. The former vector is learned by looking at what nouns a word co-occurs with, and the latter is learned by looking at what verb-based patterns the word occurs in. Similarity between sets of words is not determined by a single similarity function, but rather through a combination of comparisons of the domain components of words’ representations with the function components of the words’ representations. Such combinations are designed on a task-speciﬁc basis. Although Turney’s work does not directly deal with vector composition of the sort we explore in this article, Turney shows that similarity measures can be designed for tasks similar to those presented here. The particular limitation of his approach, which Turney discusses, is that similarity measures must be speciﬁed for each task, whereas most of the compositional models described herein produce representations that can be compared in a task-independent manner (e.g., through cosine similarity). Nonetheless, this approach is innovative, and will merit further attention in future work in this area. Language as Algebra. A theoretical model of meaning as context has been proposed in Clarke (2009, 2012). In that model, the meaning of any string of words is a vector built  84  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  from the occurrence of the string in a corpus. This is the most natural extension of distributional models from words to strings of words: in that model, one builds vectors for strings of words in exactly the same way as one does for words. The main problem, however, is that of data sparsity for the occurrences of strings of words. Words do appear repeatedly in a document, but strings of words, especially for longer strings, rarely do so; for instance, it hardly happens that an exact same sentence appears more than once in a document. To overcome this problem, the model is based on the hypothetical concept of an inﬁnite corpus, an assumption that prevents it from being applied to real-world corpora and experimented within natural language processing tasks. On the positive side, the model provides a theoretical study of the abstract properties of a general bilinear associative composition operator; in particular, it is shown that this operator encompasses other composition operators, such as addition, multiplication, and even tensor product. 3. DisCoCat In Section 2, we discussed lexical DSMs and the problems faced by attempts to provide a vector composition operation that would allow us to form distributional sentence representations as a function of word meaning. In this section, we will present an existing formalism aimed at solving this compositionality problem, as well as the mathematical background required to understand it and further extensions, building on the features and failures of previously discussed attempts at syntactically sensitive compositionality. Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) propose adapting a category theoretic model, inspired by the categorical compositional vector space model of quantum protocols (Abramsky and Coecke 2004), to the task of compositionality of semantic vectors. Syntactic analysis in the form of pregroup grammars—a categorial grammar—is given categorical semantics in order to be represented as a compact closed category P (a concept explained subsequently), the objects of which are syntactic types and the morphisms of which are the reductions forming the basis of syntactic analysis. This syntactic category is then mapped onto the semantic compact closed category FVect of ﬁnite dimensional vector spaces and linear maps. The mapping is done in the product category FVect × P via the following procedure. Each syntactic type is interpreted as a vector space in which semantic vectors for words with that particular syntactic type live; the reductions between the syntactic types are interpreted as linear maps between the interpreted vector spaces of the syntactic types. The key feature of category theory exploited here is its ability to relate in a canonical way different mathematical formalisms that have similar structures, even if the original formalisms belong in different branches of mathematics. In this context, it has enabled us to relate syntactic types and reductions to vector spaces and linear maps and obtain a mechanism by which syntactic analysis guides semantic composition operations. This pairing of syntactic analysis and semantic composition ensures both that grammaticality restrictions are in place as in the model of Erk and Pado´ (2008) and syntactically driven semantic composition in the form of inner-products provide the implicit disambiguation features of the compositional models of Erk and Pado´ (2008) and Mitchell and Lapata (2008). The composition mechanism also involves the projection of tensored vectors into a common semantic space without the need for full representation of the tensored vectors in a manner similar to Plate (1991), without restriction to the nature of the vector spaces it can be applied to. This avoids the problems faced by other tensor-based composition mechanisms such as Smolensky (1990) and Clark and Pulman (2007). 85  Computational Linguistics  Volume 41, Number 1  The word vectors can be speciﬁed model-theoretically and the sentence space can be deﬁned over Boolean values to obtain grammatically driven truth-theoretic semantics in the style of Montague (1974), as proposed by Clark, Coecke, and Sadrzadeh (2008). Some logical operators can be emulated in this setting, such as using swap matrices for negation as shown by Coecke, Sadrzadeh, and Clark (2010). Alternatively, corpus-based variations on this formalism have been proposed by Grefenstette et al. (2011) to obtain a non-truth theoretic semantic model of sentence meaning for which logical operations have yet to be deﬁned. Before explaining how this formalism works, in Section 3.3, we will introduce the notions of pregroup grammars in Section 3.1, and the required basics of category theory in Section 3.2. 3.1 Pregroup Grammars Presented by Lambek (1999, 2008) as a successor to his syntactic calculus (Lambek 1958), pregroup grammars are a class of categorial type grammars with pregroup algebras as semantics. Pregroups are particularly interesting within the context of this work because of their well-studied algebraic structure, which can trivially be mapped onto the structure of the category of vector spaces, as will be discussed subsequently. Logically speaking, a pregroup is a non-commutative form of Linear Logic (Girard 1987) in which the tensor and its dual par coincide; this logic is sometimes referred to as Bi-Compact Linear Logic (Lambek 1999). The formalism works alongside the general guidelines of other categorial grammars, for instance, those of the combinatory categorial grammar (CCG) designed by Steedman (2001) and Steedman and Baldridge (2011). They consist of atomic grammatical types that combine to form compound types. A series of CCGlike application rules allow for type-reductions, forming the basis of syntactic analysis. As our ﬁrst step, we show how this syntactic analysis formalism works by presenting an introduction to pregroup algebras. Pregroups. A pregroup is an algebraic structure of the form (P, ≤, ·, 1, (−)l, (−)r). Its elements are deﬁned as follows: r P is a set {a, b, c, . . .}. r The relation ≤ is a partial ordering on P. r The binary operation · is an associative, non-commutative monoid multiplication with the type − · − : P × P → P, such that if a, b ∈ P then a · b ∈ P. In other words, P is closed under this operation. r 1 ∈ P is the unit, satisfying a · 1 = a = 1 · a for all a ∈ P. r (−)l and (−)r are maps with types (−)l : P → P and (−)r : P → P such that for any a ∈ P, we have that al, ar ∈ P. The images of these maps are referred to as the left and the right adjoints. These are unique and satisfy the following conditions: – Reversal: if a ≤ b then bl ≤ al (and similarly for ar, br). – Ordering: a · ar ≤ 1 ≤ ar · a and al · a ≤ 1 ≤ a · al. – Cancellation: alr = a = arl. – Self-adjointness of identity: 1r = 1 = 1l. – Self-adjointness of multiplication: (a · b)r = br · ar.  86  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  As a notational simpliﬁcation we write ab for a · b, and if abcd ≤ cd we write abcd → cd and call this a reduction, omitting the identity wherever it might appear. Monoid multiplication is associative, so parentheses may be added or removed for notational clarity without changing the meaning of the expression as long as they are not directly under the scope of an adjoint operator. An example reduction in pregroup might be: aarbclc → bclc → b We note here that the reduction order is not always unique, as we could have reduced as follows: aarbclc → aarb → b. As a further notational simpliﬁcation, if there exists a chain of reductions a → . . . → b we may simply write a → b (in virtue of the transitivity of partial ordering relations). Hence in our given example, we can express both reduction paths as aarbclc → b. Pregroups and Syntactic Analysis. Pregroups are used for grammatical analysis by freely generating the set P of a pregroup from the basic syntactic types n, s, . . . , where here n stands for the type of both a noun and a noun phrase and s for that of a sentence. The conﬂation of nouns and noun phrases suggested here is done to keep the work discussed in this article as simple as possible, but we could of course model them as different types in a more sophisticated version of this pregroup grammar. As in any categorial grammar, words of the lexicon are assigned one or more possible types (corresponding to different syntactic roles) in a predeﬁned type dictionary, and the grammaticality of a sentence is veriﬁed by demonstrating the existence of a reduction from the combination of the types of words within the sentence to the sentence type s. For example, having assigned to noun phrases the type n and to sentences the type s, the transitive verbs will have the compound type nrsnl. We can read from the type of a transitive verb that it is the type of a word which “expects” a noun phrase on its left and a noun phrase on its right, in order to produce a sentence. A sample reduction of John loves cake with John and cake being noun phrases of type n and loves being a verb of type nrsnl is as follows: n(nrsnl)n → s We see that the transitive verb has combined with the subject and object to reduce to a sentence. Because the combination of the types of the words in the string John loves cake reduces to s, we say that this string of words is a grammatical sentence. As for more examples, we recall that intransitive verbs can be given the type nrs such that John sleeps would be analyzed in terms of the reduction n(nrs) → s. Adjectives can be given the type nnl such that red round rubber ball would be analyzed by (nnl)(nnl)(nnl)n → n. And so on and so forth for other syntactic classes. Lambek (2008) presents the details of a slightly more complex pregroup grammar with a richer set of types than presented here. This grammar is hand-constructed and iteratively extended by expanding the type assignments as more sophisticated grammatical constructions are discussed. No general mechanism is proposed to cover all such types of assignments for larger fragments (e.g., as seen in empirical data). Pregroup grammars have been proven to be learnable by Be´chet, Foret, and Tellier (2007), who also discuss the difﬁculty of this task and the nontractability of the procedure. Because of these constraints and lack of a workable pregroup parser, the pregroup grammars  87  Computational Linguistics  Volume 41, Number 1  we will use in our categorical formalism are derived from CCG types, as we explain in the following. Pregroup Grammars and Other Categorial Grammars. Pregroup grammars, in contrast with other categorial grammars such as CCG, do not yet have a large set of tools for parsing available. If quick implementation of the formalism described later in this paper is required, it would be useful to be able to leverage the mature state of parsing tools available for other categorial grammars, such as the Clark and Curran (2007) statistical CCG parser, as well as Hockenmaier’s CCG lexicon and treebank (Hockenmaier 2003; Hockenmaier and Steedman 2007). In other words, is there any way we can translate at least some subset of CCG types into pregroup types? There are some theoretical obstacles to consider ﬁrst: Pregroup grammars and CCG are not equivalent. Buszkowski (2001) shows pregroup grammars to be equivalent to context-free grammars, whereas Joshi, Vijay-Shanker, and Weir (1989) show CCG to be weakly equivalent to more expressive mildly context-sensitive grammars. However, if our goal is to exploit the CCG used in Clark and Curran’s parser, or Hockenmaier’s lexicon and treebank, we may be in luck: Fowler and Penn (2010) prove that some CCGs, such as those used in the aforementioned tools, are strongly context-free and thus expressively equivalent to pregroup grammars. In order to be able to apply the parsing tools for CCGs to our setting, we use a translation mechanism from CCG types to pregroup types based on the Lambek-calculus-to-pregroup-grammar translation originally presented in Lambek (1999). In this mechanism, each atomic CCG type X is assigned a unique pregroup type x; for any X/Y in CCG we have xyl in the pregroup grammar; and for any X\Y in CCG we have yrx in pregroup grammar. Therefore, by assigning NP to n and S to s we could, for example, translate the CCG transitive verb type (S\NP)/NP into the pregroup type nrsnl, which corresponds to the pregroup type we used for transitive verbs in Section 3.1. Wherever type replacement (e.g., N → NP) is allowed in CCG we set an ordering relation in the pregroup grammar (e.g., n¯ ≤ n, where n¯ is the pregroup type associated with N). Because forward and backward slash “operators” in CCG are not associative whereas monoid multiplication in pregroups is, it is evident that some information is lost during the translation process. But because the translation we need is one-way, we may ignore this problem and use CCG parsing tools to obtain pregroup parses. Another concern lies with CCG’s crossed composition and substitution rules. The translations of these rules do not in general hold in a pregroup; this is not a surprise as pregroups are a simpliﬁcation of the Lambek Calculus and these rules did not hold in the Lambek Calculus either, as shown in Moortgat (1997), for example. However, for the phenomena modeled in this paper, the CCG rules without the backward cross rules will sufﬁce. In general for the case of English, one can avoid the use of these rules by overloading the lexicon and using additional categories. To deal with languages that have cross dependancies, such as Dutch, various solutions have been suggested (e.g., see Genkin, Francez, and Kaminski 2010; Preller 2010). 3.2 Categories Category theory is a branch of pure mathematics that allows for a general and uniform formulation of various different mathematical formalisms in terms of their main structural properties using a few abstract concepts such as objects, arrows, and combinations and compositions of these. This uniform conceptual language allows for derivation of new properties of existing formalisms and for relating these to properties of other formalisms, if they bear similar categorical representation. In this function, it has been  88  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  at the center of recent work in unifying two orthogonal models of meaning, a qualitative categorial grammar model and a quantitative distributional model (Clark, Coecke, and Sadrzadeh 2008; Coecke, Sadrzadeh, and Clark 2010). Moreover, the unifying categorical structures at work here were inspired by the ones used in the foundations of physics and the modeling of quantum information ﬂow, as presented in Abramsky and Coecke (2004), where they relate the logical structure of quantum protocols to their state-based vector spaces data. The connection between the mathematics used for this branch of physics and those potentially useful for linguistic modeling has also been noted by several sources, such as Widdows (2005), Lambek (2010), and Van Rijsbergen (2004). In this section, we will brieﬂy examine the basics of category theory, monoidal categories, and compact closed categories. The focus will be on deﬁning enough basic concepts to proceed rather than provide a full-blown tutorial on category theory and the modeling of information ﬂow, as several excellent sources already cover both aspects (e.g., Mac Lane 1998; Walters 1991; Coecke and Paquette 2011). A categoriesin-a-nutshell crash course is also provided in Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010). The Basics of Category Theory. A basic category C is deﬁned in terms of the following elements: r A collection of objects ob(C). r A collection of morphisms hom(C). r A morphism composition operation ◦. Each morphism f has a domain dom( f ) ∈ ob(C) and a codomain codom( f ) ∈ ob(C). For dom( f ) = A and codom( f ) = B we abbreviate these deﬁnitions as f : A → B. Despite the notational similarity to function deﬁnitions (and sets and functions being an example of a category), it is important to state that nothing else is presupposed about morphisms, and we should not treat them a priori as functions. The following axioms hold in every category C : r For any f : A → B and g : B → C there exists h : A → C and h = g ◦ f . r For any f : A → B, g : B → C and h : C → D, ◦ satisﬁes (h ◦ g) ◦ f = h ◦ (g ◦ f ). r For every A ∈ ob(C) there is an identity morphism idA : A → A such that for any f : A → B, f ◦ idA = f = idB ◦ f . We can model various mathematical formalisms using these basic concepts, and verify that these axioms hold for them. For example, category Set with sets as objects and functions as morphisms, or category Rel with sets as objects and relations as morphisms, category Pos with posets as objects and order-preserving maps as morphisms, and category Group with groups as objects and group homomorphisms as morphisms, to name a few. The product C × D of two categories C and D is a category with pairs (A, B) as objects, where A ∈ ob(C) and B ∈ ob(D). There exists a morphism (f, g) : (A, B) → (C, D) in C × D if and only if there exists f : A → C ∈ hom(C) and g : B → D ∈ hom(D). Product categories allow us to relate objects and morphisms of one mathematical formalism to those in another, in this example those of C to D.  89  Computational Linguistics  Volume 41, Number 1  Compact Closed Categories. A monoidal category C is a basic category to which we add a monoidal tensor ⊗ such that: r For all A, B ∈ ob(C) there is an object A ⊗ B ∈ ob(C). r For all A, B, C ∈ ob(C), we have (A ⊗ B) ⊗ C = A ⊗ (B ⊗ C). r There exists some I ∈ ob(C) such that for any A ∈ ob(C), we have I ⊗ A = A = A ⊗ I. r For f : A → C and g : B → D in hom(C) there is f ⊗ g : A ⊗ B → C ⊗ D in hom(C). r For f1 : A → C, f2 : B → D, g1 : C → E and g2 : D → F the following equality holds:  (g1 ⊗ g2) ◦ ( f1 ⊗ f2) = (g1 ◦ f1) ⊗ (g2 ◦ f2)  The product category of two monoidal categories has a monodial tensor, deﬁned pointwisely by (a, A) ⊗ (b, B) := (a ⊗ b, A ⊗ B). A compact closed category C is a monoidal category with the following additional axioms:  r Each object A ∈ ob(C) has left and right “adjoint” objects Al and Ar in ob(C). r There exist four structural morphisms for each object A ∈ ob(C):  – –  ηηlArA  : :  I I  → →  A ⊗ Al. Ar ⊗ A.  – –  l Ar A  : :  Al ⊗ A A ⊗ Ar  → →  I. I.  r The previous structural morphisms satisfy the following equalities:  –  (1A ⊗  l A  )  ◦  (ηlA  ⊗  1A )  =  1A.  –  (  r A  ⊗  1A )  ◦  (1A  ⊗  ηrA )  =  1A.  –  (1Ar ⊗  r A  )  ◦  (ηr  ⊗  1Ar  )  =  1Ar  .  –  (  l A  ⊗  1Al  )  ◦  (1Al  ⊗  ηlA )  =  1Al .  Compact closed categories come equipped with complete graphical calculi, surveyed in Selinger (2010). These calculi visualize and simplify the axiomatic reasoning within the category to a great extent. In particular, Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) show how they depict the pregroup grammatical reductions and visualize the ﬂow of information in composing meanings of single words and forming meanings for sentences. Although useful at an abstract level, these calculi do not play the same simplifying role when it comes to the concrete and empirical computations; therefore we will not discuss them in this article. A very relevant example of a compact closed category is a pregroup algebra P. Elements of a pregroup are objects of the category, the partial ordering relation provides the morphisms, 1 is I, and monoidal multiplication is the monoidal tensor. Another very relevant example is the category FVect of ﬁnite dimensional Hilbert spaces and linear maps over R—that is, vector spaces over R with orthogonal bases of ﬁnite dimension, and an inner product operation − | − : A × A → R for every vector space A. The objects of FVect are vector spaces, and the morphisms are linear  90  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  maps between vector spaces. The unit object is R and the monoidal tensor is the linear algebraic tensor product. FVect is degenerate in its adjoints, in that for any vector space A, we have Al = Ar = A∗, where A∗ is the dual space of A. Moreover, by ﬁxing a basis we obtain that A∗ ∼= A. As such, we can effectively do away with adjoints in this category, and “collapse” l, r, ηl, and ηr maps into “adjoint-free” and η maps. In this category, the maps are inner product operations, A : A ⊗ A → R, and the η maps η : R → A ⊗ A generate maximally entangled states, also referred to as Bell-states.  3.3 A Categorical Passage from Grammar to Semantics  In Section 3.2 we showed how a pregroup algebra and vector spaces can be modeled as compact closed categories and how product categories allow us to relate the objects and morphisms of one category to those of another. In this section, we will present how Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) suggest building on this by using categories to relate semantic composition to syntactic analysis in order to achieve syntax-sensitive composition in DSMs.  3.3.1 Syntax Guides Semantics. The product category FVect × P has as object pairs (A, a),  where A is a vector space and a is a pregroup grammatical type, and as morphism pairs  ( f, ≤) where f is a linear map and ≤ a pregroup ordering relation. By the deﬁnition of  product categories, for any two vector space-type pairs (A, a) and (B, b), there exists  a morphism (A, a) → (B, b) only if there is both a linear map from A into B and a  partial ordering a → b. If we view these pairings as the association of syntactic types  with vector spaces containing semantic vectors for words of that type, this restriction  effectively states that a linear map from A to B is only “permitted” in the product  category if a reduces to b.  Both P and FVect being compact closed, it is simple to show that FVect × P is as well,  by considering the pairs of unit objects and structural morphisms from the separate  categories: I is now (R, 1), and the structural morphisms are (  A,  l a  ),  (  A,  r a  ),  (ηA, ηla),  and (ηA, ηra). We are particularly interested in the maps, which are deﬁned as follows  (from the deﬁnition of product categories):  (  A,  l A  )  :  (A  ⊗  A,  ala)  →  (R,  1)  (  A,  r A  )  :  (A  ⊗  A,  aar )  →  (R,  1)  This states that whenever there is a reduction step in the grammatical analysis of a sentence, there is a composition operation in the form of an inner product on the semantic front. Hence, if nouns of type n live in some noun space N and transitive verbs of type nlsnr live in some space N ⊗ S ⊗ N, then there must be some structural morphism of the form:  (  N ⊗ 1S ⊗  N,  rn1s  l n  )  :  (N  ⊗  (N  ⊗  S  ⊗  N)  ⊗  N,  n(nrsnl )n)  →  (S,  s)  We can read from this morphism the functions required to compose a sentence with a noun, a transitive verb, and an object to obtain a vector living in some sentence space S, namely, ( N ⊗ 1S ⊗ N ). The form of a syntactic type is therefore what dictates the structure of the semantic space associated with it. The structural morphisms of the product category guarantee that for every syntactic reduction there is a semantic composition morphism provided by the product category: syntactic analysis guides semantic composition.  91  Computational Linguistics  Volume 41, Number 1  3.3.2 Example. To give an example, we give syntactic type n to nouns, and nrs to intran-  sitive verbs. The grammatical reduction for kittens sleep, namely,  to in  the morphism  r n  ⊗  1s  in  P.  The  syntactic  some vector space N, and the intransitive  types−−d−i→ctate verb sleep in  that N⊗  nnrs → s, c−−or−r−e→sponds the noun kittens lives S. The reduction−−m−−o→r-  ph−i−sm−→gives us the composition morphism ( N ⊗ 1S), which we can apply to kittens  ⊗ sleep.  us  exBpecaanudse−k−itw−te−e→nsca=n  exipcrkieitstesnsa−→nniyavnedc−st−oler−e→pas=the  weighted sum of its basis ij csijleep−→ni ⊗ −→sj ; then we can  vectors, let express the  composition as follows:  −−−−−−−−→  −−−−→ −−−→  kittens sleep = ( N ⊗ 1S)(kittens ⊗ sleep)  ⎛  ⎞  = ( N ⊗ 1S ) ⎝ cki ittens−→ni ⊗  csjkleep−→nj ⊗ −→sk ⎠  i  jk  ⎛  ⎞  = ( N ⊗ 1S ) ⎝ cki ittenscsjkleep−→ni ⊗ −→nj ⊗ −→sk ⎠ ijk  =  cki ittenscsjkleep −→ni | −→nj −→sk  ijk  =  cki ittens csikleep−→sk  ik  In these equations, we have expressed the vectors in their explicit form, we have consolidated the sums by virtue of distributivity of linear algebraic tensor product over −aw→ndiedi=githi−→notjsn,aanwredes0hcaaolvtaheresar),wpapinsliede.dAﬁtnshaeallytree, nswusoeltrehodafvtlheinesesiaemr, pwmlieaﬁphesdavtotehteohbietnavdienicceetdosrascinvocemectpoo−→nrniteh|na−→nttsj l(iva=ses1thiinef sentence space S. Transitive sentences can be dealt with in a similar fashion:  −−−−−−−−−−−−−→ kittens chase mice  = ( N ⊗ 1S ⊗  N  −−−−→ )(kittens  ⊗  −−−→ chase  ⊗  −−→ mice)  ⎛  ⎛  ⎞  ⎞  = ( N ⊗ 1S ⊗ N ) ⎝ cki ittens−→ni ⊗ ⎝ ccjkhlase−→nj ⊗ −→sk ⊗ −→nl ⎠ ⊗  cmmice−n→m⎠  i  jkl  m  ⎛  ⎞  = ( N ⊗ 1S ⊗ N ) ⎝ cki ittensccjkhlasecmmice−→ni ⊗ −→nj ⊗ −→sk ⊗ −→nl ⊗ n−→m⎠ ijklm  =  cki ittensccjkhlasecmmice −→ni | −→nj −→sk −→nl | −n→m  ijklm  =  cki ittensccikhmasecmmice−→sk  ikm  92  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  In −to−−t−h→e kittens  b⊗cootc−mh−hp−ac→osaessie⊗tsio,m−ni−ti→cmies oiimrnpphtohisremtat,nratnntaosmitnievolyet,ec−kta−hist−ateet−,→ntnhsee⊗vtes−er−lne−sne→operedipnsrottdhouebctienptcraoasmnsespdituivateseda.cragWsueemacenandnt  treat the tensor products here as commas separating function arguments, thereby  avoiding the dimensionality problems presented by earlier tensor-based approaches to  compositionality.  3.4 This Article and the DisCoCat Literature As elaborated on in Section 2.3.4, the ﬁrst general setting for pairing meaning vectors with syntactic types was proposed in Clark and Pulman (2007). The setting of a DisCoCat generalized this by making the meaning derivation process rely on a syntactic type system, hence overcoming its central problem whereby the vector representations of strings of words with different grammatical structure lived in different spaces. A preliminary version of a DisCoCat was developed in Clark, Coecke, and Sadrzadeh (2008), a full version was elaborated on in Coecke, Sadrzadeh, and Clark (2010), where, based on the developments of Preller and Sadrzadeh (2010), it was also exempliﬁed how the vector space model may be instantiated in a truth theoretic setting where meanings of words were sets of their denotations and meanings of sentences were their truth values. A nontechnical description of this theoretical setting was presented in Clark (2013), where a plausibility truth-theoretic model for sentence spaces was worked out and exempliﬁed. The work of Grefenstette et al. (2011) focused on a tangential branch and developed a toy example where neither words nor sentence spaces were Boolean. The applicability of the theoretical setting to a real empirical natural language processing task and data from a large scale corpus was demonstrated in Grefenstette and Sadrzadeh (2011a, 2011b). There, we presented a general algorithm to build vector representations for words with simple and complex types and the sentences containing them; then applied the algorithm to a disambiguation task performed on the British National Corpus (BNC). We also investigated the vector representation of transitive verbs and showed how a number of single operations may optimize the performance. We discuss these developments in detail in the following sections.  4. Concrete Semantic Spaces In Section 3.3 we presented a categorical formalism that relates syntactic analysis steps to semantic composition operations. The structure of our syntactic representation dictates the structure of our semantic spaces, but in exchange, we are provided with composition functions by the syntactic analysis, rather than having to stipulate them ad hoc. Whereas the approaches to compositional DSMs presented in Section 2 either failed to take syntax into account during composition, or did so at the cost of not being able to compare sentences of different structure in a common space, this categorical approach projects all sentences into a common sentence space where they can be directly compared. However, this alone does not give us a compositional DSM. As we have seen in the previous examples, the structure of semantic spaces varies with syntactic types. We therefore cannot construct vectors for different syntactic types in the same way, as they live in spaces of different structure and dimensionality. Furthermore, nothing has yet been said about the structure of the sentence space S into which expressions reducing to type s are projected. If we wish to have a compositional DSM  93  Computational Linguistics  Volume 41, Number 1  that leverages all the beneﬁts of lexical DSMs and ports them to sentence-level distributional representations, we must specify a new sort of vector construction procedure. In the original formulation of this formalism by Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010), examples of how such a compositional DSM could be used for logical evaluation are presented, where S is deﬁned as a Boolean space with True and False as basis vectors. However, the word vectors used are handwritten and speciﬁed model-theoretically, as the authors leave it for future research to determine how such vectors might be obtained from a corpus. In this section, we will discuss a new way of constructing vectors for compositional DSMs, and of deﬁning sentence space S, in order to reconcile this powerful categorical formalism with the applicability and ﬂexibility of standard distributional models.  4.1 Deﬁning Sentence Space  Assume the following sentences are all true:  1. The dogs chased the cats. 2. The dogs annoyed the cats. 3. The puppies followed the kittens. 4. The train left the station. 5. The president followed his agenda.  If asked which sentences have similar meaning, we would most likely point to the  pair (1) and (3), and perhaps to a lesser degree (1) and (2), and (2) and (3). Sentences (4)  and (5) obviously speak of a different state of the world as the other sentences.  If we compare these by truth value, we obviously have no means of making such  distinctions. If we compare these by lexical similarity, (1) and (2) seem to be a closer  match than (1) and (3). If we are classifying these sentences by some higher order  relation such as “topic,” (5) might end up closer to (3) than (1). What, then, might cause  us to pair (1) and (3)?  Intuitively, this similarity seems to be because the subjects and objects brought into  relation by similar verbs are themselves similar. Abstracting away from tokens to some  notion of property, we might say that both (1) and (3) express the fact that something  furry and feline and furtive is being pursued by something aggressive (or playful)  and canine. Playing along with the idea that lexical distributional semantics present  concepts (word meanings) as “messy bundles of properties,” it seems only natural to  have the way these properties are acted upon, qualiﬁed, and related as the basis for  sentence-level distributional representations. In this respect, we here suggest that the  sentence space S, instead of qualifying the truth value of a sentence, should express  how the properties of the semantic objects within are qualiﬁed or brought into relation  by verbs, adjectives, and other predicates and relations.  More speciﬁcally, we examine two suggestions for deﬁning the sentence space, namely, SI ∼= N for sentences with intransitive verbs and ST ∼= N ⊗ N for sentences with transitive verbs. These deﬁnitions mean that the sentence space’s dimensions are  commensurate with either those of N, or those of N ⊗ N. These are by no means the  only options, but as we will discuss here, they offer practical beneﬁts.  In hence,  −t→sh1e=ca−→ns1e,  −→os2f  S=I ,−→nt2h,eanbadssiso  elements on. In the  are labeled with unique basis elements of N, case of ST, the basis elements are labeled with  94  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  u−−n−iq−u→e  ordered  pairs  of  elements  from  N,  for  example,  −→s1  =  −−−−→ (n1, n1),  −→s2  =  −−−−→ (n2, n1),  −→s3  =  (n1, n2), and so on, following the same matrix ﬂattening structure used in the proof of  the equal cardinality of N and Q. −−−−→ will use the notations (ni, nj) and  Because −→ni ⊗ −→nj  of the isomorphism between ST and N ⊗ N, we interchangeably, as both constitute appropriate  ways of representing the basis elements of such a space. To propagate this distinction  on the syntactic level, we deﬁne types sI and sT for intransitive and transitive sentences, respectively.  In creating this distinction, we lost one of the most appealing features of the frame-  work of Coecke, Sadrzadeh, and Clark (2010), namely, the result that all sentence vectors  live in the same sentence space. A mathematical solution to this two-space problem was  suggested in Grefenstette et al. (2011), and a variant of the models presented in this article permitting the non-problematic projection into a single sentence space (S ∼= N)  has been presented by Grefenstette et al. (2013). Keeping this separation allows us to  deal with both the transitive and intransitive cases in a simpler manner, and because  the experiments in this article only compare intransitive sentences with intransitive  sentences, and transitive sentences with transitive sentences, we will not address the  issue of uniﬁcation here.  4.2 Noun-Oriented Types  While Lambek’s pregroup types presented in Lambek (2008) include a rich array of basic types and hand-designed compound types in order to capture speciﬁc grammatic properties, for the sake of simplicity we will use a simpler set of grammatical types for experimental purposes, similar to some common types found in the CCG-bank (Steedman 2001). We assign a basic pregroup type n for all nouns, with an associated vector space N for their semantic representations. Furthermore, we will treat noun-phrases as nouns, assigning to them the same pregroup type and semantic space. CCG treats intransitive verbs as functions NP\S that consume a noun phrase and return a sentence, and transitive verbs as functions (NP\S)/NP that consume a noun phrase and return an intransitive verb function, which in turn consumes a noun phrase and returns a sentence. Using our distinction between intransitive sentences, we give intransitive verbs the type nrsI associated with the semantic space N ⊗ SI, and transitive verbs the type nrsTnl associated with the semantic space N ⊗ ST ⊗ N. Adjectives, in CCG, are treated as functions NP/NP, consuming a noun phrase and returning a noun phrase; and hence we give them the type nnl and associated semantic space N ⊗ N. With the provision of a learning procedure for vectors in these semantic spaces, we can use these types to construct sentence vector representations for simple intransitive verb–based and transitive verb–based sentences, with and without adjectives applied to subjects and objects.  4.3 Learning Procedures  To begin, we construct the semantic space N for all nouns in our lexicon (typically limited by the words available in the corpus used). Any distributional semantic model can be used for this stage, such as those presented in Curran (2004), or the lexical semantic models used by Mitchell and Lapata (2008). It seems reasonable to assume that higher quality lexical semantic vectors—as measured by metrics such as the WordSim353 test  95  Computational Linguistics  Volume 41, Number 1  of Finkelstein et al. (2001)—will produce better relational vectors from the procedure  designed subsequently. We will not test the hypothesis here, but note that it is an  underlying assumption in most of the current literature on the subject (Erk and Pado´  2008; Mitchell and Lapata 2008; Baroni and Zamparelli 2010).  Building upon the foundation of the thus constructed noun vectors, we construct  semantic representations for relational words. In pregroup grammars (or other com-  binatorial grammars such as CCG), we can view such words as functions, taking as  arguments those types present as adjoints in the compound pregroup type, and return-  ing a syntactic object whose type is that of the corresponding reduction. For example, an adjective nnl takes a noun or noun phrase n and returns a noun phrase n from the  reduction (nnl)n → n. It can also compose with another adjective to return an adjective (nnl)(nnl) → nnl. We wish for our semantic representations to be viewed in the same  way, such that the composition of an adjective with a noun (1N ⊗ N)((N ⊗ N) ⊗ N) can be viewed as the application of a function f : N → N to its argument of type N.  To learn the representation of such functions, we assume that their meaning can  be characterized by the properties that their arguments hold in the corpus, rather than  just by their context as is the case in lexical distributional semantic models. To give  an example, rather than learning what the adjective angry means by observing that  it co-occurs with words such as ﬁghting, aggressive, or mean, we learn its meaning by  observing that it typically takes as argument words that have the property of being  (i.e., co-occur with words such as) ﬁghting, aggressive, and mean. Whereas in the lexical  semantic case, such associations might only rarely occur in the corpus, in this indirect  method we learn what properties the adjective relates to even if they do not co-occur  with it directly.  In turn, through composition with its argument, we expect the function for such  an adjective to strengthen the properties that characterize it in the object it takes as  argument. If, indeed, angry is characterized by arguments that have high basis weights  for basis elements corresponding to the concepts (or context words) ﬁghting, aggressive,  and mean, and relatively low counts for semantically different concepts such as passive,  peaceful, and loves, then when we apply angry to dog the vector for the compound  angry dog should contain some of the information found in the vector for dog. But this  vector should also have higher values for the basis weights of ﬁghting, aggressive, and  mean, and correspondingly lower values for the basis weights of passive, peaceful, and  loves.  To turn this idea into a concrete algorithm for constructing the semantic repre-  sentation for relations of any arity, as ﬁrst presented in Grefenstette et al. (2011), we  examine how we would deal with this for binary relations such as transitive verbs. If  a transitive verb of semantic type N ⊗ ST ⊗ N is viewed as a function f : N × N → ST that expresses the extent to which the properties of subject and object are brought into  relation by the verb, we learn the meaning of the verb by looking at what properties are  b−→vro∈ugNht⊗inStTo⊗reNla,ticoann  by be  its arguments in a corpus. Recall expressed as the weighted sum of  that the its basis  vector for elements:  a  verb  v,  −→v = cvijk−→ni ⊗ −→sj ⊗ −→nk ijk  W{(−Se−U−taB−k→Jel,  t−Oh−Be−→Jsl )e}tl.oWf evewctioshrstofocraltchuelastuebtjheectbaansids  object of v weightings  in the {cvijk}ijk  corpus to be argv = for v. Exploiting our  earlier deﬁnition of the basis {sj}j of ST, which states that for any value of i and k there  96  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  is some value of j such that sj = (ni, nk), we deﬁne Δijk = 1 if indeed sj = (ni, nk) and 0 otherwise. Using all of this, we deﬁne the calculation of each basis weight cvijk as:  cvijk =  ΔijkcSi UBJl cOk BJl  l  This allows for a full formulation of −→v as follows:  −→v =  ΔijkcSi UBJl cOk BJl −→ni ⊗ −→sj ⊗ −→nk  l ijk  The nested sums here may seem computationally inefﬁcient, seeing how this would involve computing size(argv) × dim(N)2 × dim(S) = size(argv) × dim(N)4 prod- ucts. However, using the decomposition of basis elements of S into pairs of basis elements of N (effectively basis elements of N ⊗ N), we can remove the Δijk term and ignore all values of j where sj = (ni, nk), because the basis weight for this combination of indices would be 0. Hence we simplify:  −→v = l  cSi UBJl  cOk BJl  −→ni  ⊗  −−−−→ (ni, nk )  ⊗  −→nk  ik  This representation is still bloated: We perform fewer calculations, but still obtain a  vector of the  in which all the dim(N)4 values  basis weights are non-zero.  where sj In short,  =the(nvi,enckt)oarrwe 0ei,ghhetnscfeowr −→hveraeroe,nulynddiemr(tNh)is2  learning algorithm, entirely characterized by the values of a dim(N) by dim(N) matrix,  the entries of which are products cSi UBJl cOk BJl where i and k have become row and column indices.  −l→uat,e−→wUa s∈cionNmg ,pthwaicrstitatenexdnpr−→oueusr⊗sido−→wenﬁon∈fitN−→ivon⊗aosNff,SobTleloaawssasf.oslLploeatwcetsh:iseokmroonrpehckicertopNro⊗duNct,  we can of two  formuvectors  −→u ⊗ −→w = cui cwj −→ni ⊗ −→nj ij Equipped with this deﬁnition, we can formulate the compact form of −→v :  −→v =  cSi UBJl cOk BJl −→ni ⊗ −→nk  l ik  =  −−−→ SUBJl  ⊗  −−→ OBJl  l  In short, we are only required to iterate through the corpus once, taking for each  instance of a transitive verb v the kronecker product of its subject and object, and  summing discarded  trheelasteivaecrtoostsheallprinevstiaonucseds eoﬁfnvit.ioItniosfs−→ivm:pTlehetodismeeenthsiaotnnaolitiynfroerdmucattiioonn  was by a  factor of dim(N)2 simply discards all basis elements for which the basis weight was 0 by  default.  97  Computational Linguistics  Volume 41, Number 1  This raises a small problem though: This compact representation can no longer be  −u→vsendoinlotnhgeercommaptcohsittihoonsael  mechanism presented in Section 3.3, that it is required to have according  as to  the its  dimensions of syntactic type.  However, a solution can be devised if we return to the sample calculation shown in  Section 3.3.2 of the composition of a transitive verb with its arguments. The composition  reduces as follows:  ( N ⊗ 1S ⊗  N  −−−→ )(SUBJ  ⊗  −→v  ⊗  −−→ OBJ)  =  cSi UBJcvikmcOmBJ−→sk  ikm  where the verb v is represented in its non-compact form. If we introduce the compactness given to us by our isomorphism ST ∼= N ⊗ N we can express this as  −−−−−−−−→ SUBJ v OBJ  =  cSi UBJcvimcOmBJ−→ni ⊗ −n→m  im  where v is represented in its compact form. Furthermore, by introducing the component wise multiplication operation :  −→u −→v = cui cvi −→ni i  we can show the general form of transitive verb composition with the reduced verb representation to be as follows:  −−−−−−−−→ SUBJ v OBJ  =  cSi UBJcvimcOmBJ−→ni ⊗ −n→m  im  =  cvim−→ni ⊗ n−→m  im  = −→v  −−−→ SUBJ  ⊗  −−→ OBJ  cSi UBJcOmBJ−→ni ⊗ −n→m im  To summarize what we have done with transitive verbs:  1. We have treated them as functions, taking two nouns and returning a sentence in a space ST ∼= N ⊗ N.  2. We have built them by counting what properties of subject and object noun vectors are related by the verb in transitive sentences in a corpus.  3. We have assumed that output properties were a function of input properties  by the use of the Δijk function—for example, the weights associated with  ni from the subject affect the “output”  argument and weight for the  with nk from the object sentence basis element  a−→srjg=um−→nei n⊗t  o−→nnkl.y  4. We have shown that this leads to a compact representation of the verb’s semantic form, and an optimized learning procedure.  5. We have shown that the composition operations of our formalism can be adapted to this compact representation.  98  Grefenstette and Sadrzadeh  Categorical Compositional Distributional Model of Meaning  The compact representation and amended composition operation hinge on the choice of ST ∼= N ⊗ N as output type for N ⊗ N as an input type (the pair of arguments the verb takes), justifying our choice of a transitive sentence space. In the intransitive case, the same phenomenon can be observed, since such a verb takes as argument a vector in N and produces a vector in SI ∼= N. Furthermore, our choice to make all other types dependent on one base type—namely, n (with associated semantic space N)— yields the same property for every relational word we wish to learn: The output type is the same as the concatenated (on the syntactic level) or tensored (on the semantic level) input types. It is this symmetry between input and output types that guarantees that any m-ary relation, expressed in the original formulation as an element of tensor space N ⊗ . . . ⊗ N , has a compact representation in N ⊗ . . . ⊗ N , where the ith basis weight  2m  m  of the reduced representation stands for the degree to which the ith element of the input  vector affects the ith element of the output vector.  This allows the speciﬁcation of the generalized learning algorithm for reduced rep-  resentations, ﬁrst presented in Grefenstette and Sadrzadeh (2011a), which is as follows.  Each relational word P with grammatical type π and m adjoint types α1, α2, · · ·, αm is encoded as an (r × . . . × r) multi-dimensional array with m degrees of freedom (i.e., a rank  m tensor). Because our vector space N has a ﬁxed basis, each such array is represented  in vector form as follows:  −→ P  =  cij···ζ (−→n i ⊗ −→n j ⊗ · · · ⊗ −→n ζ)  ij · · · ζ  m  m  This vector lives in the tensor space N ⊗ N ⊗ · · · ⊗ N. Each cij···ζ is computed according m to the procedure described in Figure 2.  Linear algebraically, this procedure corresponds to computing the following  −→ P  =  −→w1 ⊗ −→w 2 ⊗ · · · ⊗ −→w m k  k  1) Consider a sequence of words containing a relational word P and its arguments w1, w2, · · · , wm, occurring in the same order as described in P’s grammatical type π. Refer to these sequences as P-relations. Suppose there are k of them. a23n)) dSRuewptrmpieohvsaeestwwh1eehivgaehscttwocmζeri−→wgohnltobcfa1i seoiasncvhbeaacsrtiogsruvm−→enceζtno. trMw−→nul.lit,iwpl2yhtahsewseewigehitgch2jtson basis vector −→n j, · · · , c1i × c2j × · · · × cmζ 4) Repeat the above steps for all the k P-relations, and sum the corresponding weights  cij···ζ =  c1i × c2j × · · · × cmζ k  k  Figure 2 Procedure for learning weights for matrices of words P with relational types π of m arguments.  99  Computational Linguistics  Volume 41, Number 1  The general formulation of composing a relational word P with its arguments arg1, . . . , argm is now expressed as  −→ P  −a−r→g1 ⊗ . . . ⊗ −ar−g→m  For example, the computation of furry cats nag angry dogs would correspond to the following operations:  −−−−−−−−−−−−−−−−−−→ furry cats nag angry dogs  =  −n−a→g  −−−→ furry  c−→at ⊗ a−n−−g→ry  −−→ dog  This generalized learning algorithm effectively extends the coverage of our ap- proach to any sentence for which we have a pregroup parse (e.g., as might be obtained by our translation mechanism from CCG). For example, determiners would have a type nnl, allowing us to model them as matrices in N ⊗ N; adverbs would be of type (nrs)r(nrs), and hence be elements of S ⊗ N ⊗ N ⊗ S. We could learn them using the procedure deﬁned earlier, although for words like determiners and conjunctions, it is unclear whether we would want to learn such logical words or design them by hand, as was done in Coecke, Sadrzadeh, and Clark (2010) for negation. Nonetheless, the procedure given here allows us to generalize the work presented in this article to sentences of any length or structure based on the pregroup types of the words they contain.  4.4 Example  To conclude this section with an example taken from Grefenstette and Sadrzadeh (2011a), we demonstrate how the meaning of the word show might be learned from a corpus and then composed. Suppose there are two instances of the verb show in the corpus: s1 = table show result s2 = map show location The vector of show is  −−−→ show  =  −−→ −−−→ table ⊗ result  +  m−−a→p  ⊗  −−−−−→ location  Consider a vector space N with four basis vectors far, room, scientiﬁc, and elect. The TF/IDF-weighted values for vectors of selected nouns (built from the BNC) are as shown in Table 1. Part of the matrix compact representation of show is presented in Table 2.  Table 1 Sample weights for selected noun vectors.  i  −→ni  table map result location master dog  
Liang Huang∗∗ Queens College and Graduate Center, The City University of New York Qun Liu∗† Dublin City University Chinese Academy of Sciences Manually annotated corpora are indispensable resources, yet for many annotation tasks, such as the creation of treebanks, there exist multiple corpora with different and incompatible annotation guidelines. This leads to an inefﬁcient use of human expertise, but it could be remedied by integrating knowledge across corpora with different annotation guidelines. In this article we describe the problem of annotation adaptation and the intrinsic principles of the solutions, and present a series of successively enhanced models that can automatically adapt the divergence between different annotation formats. We evaluate our algorithms on the tasks of Chinese word segmentation and dependency parsing. For word segmentation, where there are no universal segmentation guidelines because of the lack of morphology in Chinese, we perform annotation adaptation from the much larger People’s Daily corpus to the smaller but more popular Penn Chinese Treebank. For dependency parsing, we perform annotation adaptation from the Penn Chinese Treebank to a semantics-oriented Dependency Treebank, which is annotated using signiﬁcantly different annotation guidelines. In both experiments, automatic annotation adaptation brings signiﬁcant improvement, achieving state-of-the-art performance despite the use of purely local features in training. ∗ Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, No. 6 Kexueyuan South Road, Haidian District, P.O. Box 2704, Beijing 100190, China. E-mail: {jiangwenbin, liuqun, lvyajuan}@ict.ac.cn. ∗∗ Department of Computer Science, Queens College / CUNY, 65-30 Kissena Blvd., Queens, NY 11367. E-mail: liang.huang.sh@gmail.com. † Centre for Next Generation Localisation, Faculty of Engineering and Computing, Dublin City University. E-mail: qliu@computing.dcu.ie. Submission received: 24 April 2013; revised version received: 6 March 2014; accepted for publication: 18 April 2014. doi:10.1162/COLI a 00210 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 1  1. Introduction Much of statistical NLP research relies on some sorts of manually annotated corpora to train models, but annotated resources are extremely expensive to build, especially on a large scale. The creation of treebanks is a prime example (Marcus, Santorini, and Marcinkiewicz 1993). However, the linguistic theories motivating these annotation efforts are often heavily debated, and as a result there often exist multiple corpora for the same task with vastly different and incompatible annotation philosophies. For example, there are several treebanks for English, including the Chomskian-style Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), the HPSG LinGo Redwoods Treebank (Oepen et al. 2002), and a smaller dependency treebank (Buchholz and Marsi 2006). From the perspective of resource accumulation, it seems a waste in human efforts.1 A second, related problem is that the raw texts are also drawn from different domains, which for the above example range from ﬁnancial news (Penn Treebank/Wall Street Journal) to transcribed dialog (LinGo). It would be nice if a system could be automatically ported from one set of guidelines and/or domain to another, in order to exploit a much larger data set. The second problem, domain adaptation, is very well studied (e.g., Blitzer, McDonald, & Pereira 2006; Daume´ III 2007). This work focuses on the widely existing and equally important problem, annotation adaptation, in order to adapt the divergence between different annotation guidelines and integrate linguistic knowledge in corpora with incongruent annotation formats. In this article, we describe the problem of annotation adaptation and the intrinsic principles of the solutions, and present a series of successively improved concrete models, the goal being to transfer the annotations of a corpus (source corpus) to the annotation format of another corpus (target corpus). The transfer classiﬁer is the fundamental component for annotation adaptation algorithms. It learns correspondence regularities between annotation guidelines from a parallel annotated corpus, which has two kinds of annotations for the same data. In the simplest model (Model 1), the source classiﬁer trained on the source corpus gives its predications to the transfer classiﬁer trained on the parallel annotated corpus, so as to integrate the knowledge in the two corpora. In a variant of the simplest model (Model 2), the transfer classiﬁer is used to transform the annotations in the source corpus into the annotation format of the target corpus; then the transformed source corpus and the target corpus are merged in order to train a more accurate classiﬁer. Based on the second model, we ﬁnally develop an optimized model (Model 3), where two optimization strategies, iterative training and predict-self re-estimation, are integrated to further improve the efﬁciency of annotation adaptation. We experiment on Chinese word segmentation and dependency parsing to test the efﬁcacy of our methods. For word segmentation, the problem of incompatible annotation guidelines is one of the most glaring: No segmentation guideline has been widely accepted due to the lack of a clear deﬁnition of Chinese word morphology. For dependency parsing there also exist multiple disparate annotation guidelines. For  
This book is not beholden to any particular theoretical program. Rather, it is a survey of the morphological and syntactic means by which different languages express meaning, anchored by clear and effective examples from typologically diverse languages. Eschewing theorizing to stay close to data permits a remarkably wide range of linguistic phenomena to be covered, and it is this that is the book’s greatest strength. However, in a few places, a seemingly arbitrary theoretical perspective is assumed rather more tacitly than one might hope, with few hints as to alternative analyses (e.g., see the following remarks about parts of speech in Chapter 6). Furthermore, a bit more theoretical scaffolding could have made the presentation more succinct in places (e.g., Chapter 7’s excellent discussion of heads, arguments, and adjuncts could have been more precise with a basic logical calculus). Finally, although theoretical squabbles can be off-putting to outsiders, theoretical diversity can have practical beneﬁts, particularly in a ﬁeld as omnivorous as NLP. For example, while theorists might disagree about whether morphophonology is best modeled with systems of rewrite rules (e.g., SPE) or constraint satisfaction (e.g., Optimality Theory) (Chomsky and Halle 1968; Prince and Smolensky 2004), each suggests a distinct computational instantiation with different challenges and opportunities. For such reasons, more discussion of theory would not have been unwelcome. This slight objection aside, the book is an excellent introduction to the diversity of linguistic representations that NLP must eventually contend with. The book is organized into 10 chapters, in roughly two parts (the ﬁrst part, morphology; the second, syntax), spread over 100 numbered topics. Chapter 1 gives an overview of the scope of the book, distinguishing morphology and syntax from bag-of-words models. It lays out the premise that knowledge of linguistic structure can guide engineers in proﬁtable directions by facilitating error doi:10.1162/COLI r 00212 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 1  analysis and feature engineering. The notion of bounded variation is introduced: the idea that while languages exhibit diversity in how they pair sound and meaning, this variation is subject to limits, and that different languages can have similarities due to areal, genetic, and typological relatedness. A brief survey of the genetic taxonomy of the world’s languages is given and the number of speakers they have—as well as the striking difference in distributions of the languages in the NLP literature. Chapters 2 and 3 introduce morphology and morphophonology, focusing on the internal structure of words and how they are realized in text and speech. Simple English examples motivate the discussion, but more exotic nonconcatenative processes in Semitic languages and inﬁxation examples from Lakhota emphasize phenomena that may be unfamiliar to those with experience only with Indo-European languages. The conventional tripartite distinction of roots and derivational and inﬂectional afﬁxes is presented to organize the kinds of meaning/function changes characteristic of morphological processes, although compounding and cliticization—which ﬁt less neatly into this taxonomy—are also discussed. Because syntax and semantics were only brieﬂy mentioned in the Introduction, the extensive forward references to the related material in the later chapters were quite helpful for clarifying terms, making the e-book version particularly convenient. Chapter 4 discusses morphosyntax, reviewing the diverse grammatical functions that different languages encode with morphology. Phenomena covered include functions applying to the verbal domain, including tense, mood, and aspect, negation, evidentiality; the nominal domain, including person, number, and gender, case, deﬁniteness, and possession; and various common agreement processes. Having established how words are constructed from morphemes, Chapters 5–9 focus on how syntax is used to combine words to form an unbounded number of sentences whose meaning is determined compositionally. Chapter 5 introduces the distinction between grammaticality of sentences and how syntactic structure determines their meanings, and Chapter 6 introduces parts of speech as clusters of distributional regularities of words and phrases in grammatical sentences. The fact that discussion of grammaticality proceeds almost exclusively in terms of POS—a familiar construct to anyone working in NLP, but one that looks quite different in many theories of syntax (Steedman 2000; Stabler 1997)—is a shortcoming. Chapter 7, perhaps the strongest in the book, discusses syntax in terms of headed phrases that relate to each other either as arguments (which semantically complete the meaning of a predicate) or adjunction (which introduces additional predicates). Diagnostics for distinguishing heads and dependents as well as arguments and adjuncts are given, together with clear examples of their application, and common mistakes (e.g., using optionality as a test of argumenthood or assuming that only verbs can select arguments) are covered. A particularly useful part of this chapter is a discussion of lexical resources (FrameNet, ProbBank) and how they relate to the concepts being discussed. Chapter 8 discusses argument types and grammatical functions, reviewing not entirely successful attempts to create universal inventories of thematic roles, ultimately demonstrating that syntactic roles are less idiosyncratic (at least within single languages), and capture many generalizations useful for semantic analysis. A discussion of cross-linguistic properties of subjects and the distinction between core and oblique arguments follows. Three important sections discuss the subtle and often confusing distinctions between syntactic and semantic arguments with effective examples. Although most of this chapter focuses on English examples, various morphological strategies for marking grammatical functions is discussed. 154  Book Reviews  Chapter 9 concludes the syntax portion of the book, focusing on the processes that can introduce divergences between syntactic and semantic relationships. Because such divergences underlie many constructions with considerable value in NLP (e.g., wh-questions in English) and directly challenge the simplifying assumption of transparency between syntax and semantics, it is fortunate that this section goes into considerable detail, covering phenomena including passivization, dative shift, expletives, raising, control, and various kinds of long distance movement, as well as a good discussion of phenomena found in other languages, such as causative morphology and discontinuous constituents. Chapter 10 provides a brief appendix summarizing various large-scale computational resources (morphological analyzers, parsers, typological atlases) that encode linguistic knowledge. This book serves as a useful introduction to linguistic phenomena that will help NLP researchers orient themselves with respect to phenomena they will encounter as their applications push into new languages and strive for deeper automated understanding of language. The tension between the science of linguistics and natural language engineering and the resulting missed opportunities has been remarked upon in these pages recently (Wintner 2009), and we should applaud this successful effort to ﬁnd common ground. 
The book consists of an introduction and four chapters outlining the four main steps of Web corpus construction. They include: “Data Collection” (Chapter 2), “Basic Corpus Cleaning” (Chapter 3), “Linguistic Processing” (Chapter 4), and “Corpus Evaluation” (Chapter 5). Chapter 2 provides a very useful outline of the main properties of the Web and the crawling strategies. The chapter starts with an overview of a large-scale study of Web connectivity from Baeza-Yates, Castillo, and Efthimiadis (2007), listing various parameters of connectivity for a range of Top-Level Domains. However, there is little discussion of the implications for the corpus development task; for example, does the difference of the in-degree parameter of the Web pages from Chile and the UK have any implications for the Web corpora crawled from those domains? The chapter then proceeds to another important topic, which concerns the parameters of crawling; for example, the crawl bias and the number of seeds, and their inﬂuence on the ﬁnal corpus. Section 2.4.1 illustrates the problems with the crawl bias by an example of deWac, a large commonly used corpus of German (Baroni et al. 2009). The second most frequent proper name bigram in this corpus is found to be Falun Gong. However, more analysis into the nature of the bias should have been beneﬁcial. It is less likely to be related to the PageRank bias, the main bias discussed in Section 2.4.2. Other most frequent bigrams from deWac are not presented in the book, but it is interesting to note that the fourth place in it is occupied by Hartz IV, and the tenth place by Digital Eyes. This suggests that the bias comes from frequency spikes (i.e, a large number of instances collected from a small number of Web sites). Another shortcoming of this chapter is that nothing is said speciﬁcally about obtaining data from such resources as Twitter or Facebook, which need access via APIs rather than direct crawling. doi:10.1162/COLI r 00214 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 1  Chapter 3 introduces methods for basic cleaning of the corpus content, such as processing of text formats (primarily HTML tags), language identiﬁcation, boilerplate removal, and deduplication. Such low-level tasks are not considered to be glamorous from the view of computational linguistics, but they are extremely important for making Web-derived corpora usable (Baroni et al. 2008). The introduction offered in this chapter is reasonably complete, with good explanations of the sources of problems as well as with suggestions for the tools to be used in each task. An important bit which is missing in this chapter concerns the suggestions for choosing a particular cleaning pipeline. Although the choice indeed depends on the purposes of corpus collection, an indication of which pipeline suits which purpose is desirable. Chapter 4 is devoted to basic steps for linguistic processing of Web corpora, such as tokenization, POS tagging, and lemmatization, as well as orthographic normalization. Even though the processing pipeline is roughly the same for all NLP tasks, it becomes harder for Web corpora because they exhibit greater diversity in comparison with more homogeneous text collections (e.g., WSJ texts). Web texts are also considerably noisier, in the sense of containing nonstandard linguistic expressions, which are likely to be a challenge to the tools trained on more standard texts. The chapter presents some interesting case studies—in particular, the sources of POS tagging errors and nonstandard orthography. Chapter 5 describes ways for evaluating and comparing corpora. It gives examples of checking for word and sentence length and for sentence-level duplication. It also introduces methods for comparing frequency lists. Like other chapters it includes many interesting observations, such as the methods for extrinsic evaluation of corpora. However, the chapter does not address many issues important for corpus evaluation and comparison. Given that the previous chapters introduced a number of pipelines and corpora, this chapter would have been an ideal place to illustrate all the aspects of the pipelines by evaluating them in a consistent way. There are occasional references to this goal, such as the frequency lists of French nouns in Section 5.3.1, but this particular comparison is fairly impressionistic, and it concludes with a declaration of basic similarity of the underlying corpora. Does this mean that the crawling, cleaning, and linguistic processing pipelines do not matter? In any case, not even an impressionistic comparison of the pipelines is performed for other evaluation methods. Some illustrations are also not informative (e.g., Table 5.1.1 shows two frequency lists with the identical ranks for their words, which leads to the trivial rank correlation value of 1). The chapter contains a single paragraph devoted to composition of Web corpora. Given the size of such corpora, their evaluation crucially depends on understanding what has been crawled. The task has been approached by a number of models, such as supervised and semisupervised classiﬁcation, clustering, topic modeling, and so forth, which should have been included in the discussion. The discussion does contain a relevant reference to Mehler, Sharoff, and Santini (2010), which surveys approaches to the genres of the Web, but other aspects of corpus composition need to be addressed, too. Overall, it is very useful to have a book that introduces all the aspects of Web corpus construction in a single volume with coherent presentation. The volume under review does cover the entire range of topics relevant to Web corpus construction and illustrates them via numerous examples. I would recommend it to students just starting their corpus development experiments. As for the drawbacks of the volume, there is a need to improve the structure of argumentation for the next edition. Bits of information are sometimes introduced in an incomplete way and re-introduced again in subsequent sections. For examples, two tools for crawling are discussed towards the end of Section 2.3.3, while more tools are mentioned as the discussion of crawling 162  Book Reviews  strategies progresses. Chapter 1 starts with a fairly random list of non-Web corpora, whereas an overview of the book structure is conﬁned to a short paragraph. Often, frustratingly little information is provided besides an annotated bibliography, rather than a presentation of the relevant methods and issues. In some cases this is accompanied with a statement that “covering this topic is beyond the scope of this volume,” even if the nature of the problem and the solutions could have been easily explained in a one-page summary. Another minor concern is an (understandable) emphasis on the tools and corpora developed by the authors, primarily on their German corpus. I have to admit ambivalence in my ﬁnal verdict: The book is a useful introduction to an important topic, but it deﬁnitely warrants a new edition, which eliminates the shortcomings of the current one.  References Baeza-Yates, Ricardo, Carlos Castillo, and Efthimis N. Efthimiadis. 2007. Characterization of national Web domains. ACM Transactions on Internet Technology (TOIT), 7(2):9. Baroni, Marco, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide Web: A collection of very large linguistically processed Web-crawled corpora. Language Resources and Evaluation, 43(3):209–226. 
Marco Baroni University of Trento Distributional semantics has been extended to phrases and sentences by means of composition operations. We look at how these operations affect similarity measurements, showing that similarity equations of an important class of composition methods can be decomposed into operations performed on the subparts of the input phrases. This establishes a strong link between these models and convolution kernels. 1. Introduction Distributional semantics approximates word meanings with vectors tracking cooccurrence in corpora (Turney and Pantel 2010). Recent work has extended this approach to phrases and sentences through vector composition (Clark 2015). Resulting compositional distributional semantic models (CDSMs) estimate degrees of semantic similarity (or, more generally, relatedness) between two phrases: A good CDSM might tell us that green bird is closer to parrot than to pigeon, useful for tasks such as paraphrasing. We take a mathematical look1 at how the composition operations postulated by CDSMs affect similarity measurements involving the vectors they produce for phrases or sentences. We show that, for an important class of composition methods, encompassing at least those based on linear transformations, the similarity equations can be decomposed into operations performed on the subparts of the input phrases, ∗ Department of Enterprise Engineering, University of Rome “Tor Vergata,” Viale del Politecnico, 1, 00133 Rome, Italy. E-mail: fabio.massimo.zanzotto@uniroma2.it. 
Ariadne M. B. R. Carvalho Institute of Computing / Unicamp Fifty years after Damerau set up his statistics for the distribution of errors in typed texts, his ﬁndings are still used in a range of different languages. Because these statistics were derived from texts in English, the question of whether they actually apply to other languages has been raised. We address this issue through the analysis of a set of typed texts in Brazilian Portuguese, deriving statistics tailored to this language. Results show that diacritical marks play a major role, as indicated by the frequency of mistakes involving them, thereby rendering Damerau’s original ﬁndings mostly unﬁt for spelling correction systems, although still holding them useful, should one set aside such marks. Furthermore, a comparison between these results and those published for Spanish show no statistically signiﬁcant differences between both languages—an indication that the distribution of spelling errors depends on the adopted character set rather than the language itself. 1. Introduction Almost 50 years since Damerau’s groundbreaking work was published (Damerau 1964), the ﬁgures he set for the proportion of spelling errors in typed text, along with their classiﬁcation, still remain in use. According to Damerau, over 80% of all misspelled words present a single error which, in turn, falls into one out of four categories, to wit, Insertion (an extra letter is inserted), Omission (one letter is missing), Substitution (one letter is wrong), and Transposition (two adjacent characters are transposed). In fact, these very same ﬁgures lie at the heart of much current mainstream research on related topics, from automatic text spelling correction (e.g., Pirinen and Linde´n 2014) to more advanced information retrieval techniques (e.g., Stein, Hoppe, and Gollub 2012) to optical character recognition techniques (e.g., Reynaert 2011). The reason for such popularity may rest in the simplicity of the approach, whereby numbers can be assigned ∗ EACH–USP, Arlindo Be´ttio, 1000. 03828-000. Sa˜o Paulo, SP – Brazil. E-mail: norton@usp.br. Submission received: 30 January 2014; revised submission received: 21 July 2014; accepted for publication: 18 September 2014. doi:10.1162/COLI a 00216 © 2015 Association for Computational Linguistics  Computational Linguistics  Volume 41, Number 1  to the probability that a certain type of spelling error might take place, simply because that seems to be the frequency with which people make that kind of mistake. Surprisingly enough, even though Damerau derived his statistics uniquely from texts in English, his ﬁndings are applied, with very little to no adaptation at all, to research in a range of different languages, such as Basque (e.g., Aduriz et al. 1997), Persian (e.g., Miangah 2014), and Arabic (e.g., Alkanhal et al. 2012). The question then becomes how appropriate these ﬁgures are when applied to languages other than English. In fact, some researchers have already noticed this potential ﬂaw, and tried to adapt Damerau’s ﬁndings to their own language, usually by modifying DamerauLevenstein edit distance (Wagner and Fischer 1974) to match some language-speciﬁc data, but still without verifying the appropriateness of Damerau’s statistics in the target language (e.g., Rytting et al. 2011; Richter, Strana´k, and Rosen 2012), or by taking into account some feature of that language, such as the presence of diacritics, for example (e.g., Andrade et al. 2012). In this article, we move a step further by analyzing a set of typed texts from two different corpora in Brazilian Portuguese—a language spoken by over 190 million people1—and deriving statistics tailored to this language. We then compare our statistics with results obtained for Spanish (Bustamante, Arnaiz, and Gine´s 2006). As we will show, the behavior demonstrated by native speakers of these languages follow a very similar pattern, straying from Damerau’s original ﬁndings while, at the same time, holding them still useful. As a limitation, in this research we only account for non-word errors, that is, errors that, once made, result in some non-existing word, and which may be detected by a regular dictionary look-up (Deorowicz and Ciura 2005). As an indication of the impact of these results, we added a new module (Gimenes, Roman, and Carvalho 2014)2 to OpenOfﬁce Writer3—a freely available word processor—so as to reorder its suggestions list according to the statistics presented in this article. Within Writer, when typing Pao, instead of Pa˜o (‘bread’ in Portuguese), the correct form comes in ﬁfth place in the suggestion list, with Poca, Poca´, Poc¸a, and Pro´c¸a coming in the ﬁrst four positions. With this module, it was observed that, for the ﬁrst testing corpus, in 27.34% of the cases there was an improvement over Writer’s ranking (i.e., the correct form was ranked higher in the list), whereas in 5.84% the new list was actually worse, and in 66.82% no changes were observed. In the second corpus, an improvement was observed in 19.90% of the cases, in 9.00% ﬁgures were worse, with 71.10% remaining unchanged. Some 10–21% increase in accuracy is not something to be neglected, especially at the price of changing weights in an edit distance scheme. 2. Related Work One of the ﬁrst efforts to derive statistics similar to those by Damerau in a language other than English was made by Medeiros (1995), who analyzed a set of corpora in European Portuguese. In his work, Medeiros found that 80.1% of all spelling errors would ﬁt into one of Damerau’s categories. He also noticed that around 20% of all linguistic errors would be related to the use of diacritics, meaning that they should be taken into account during string edit distance or probability calculations. By  
{``}Personality{''} is a psychological concept describing the individual's characteristic patterns of thought, emotion, and behavior. In the context of Big Data and granular analytics, it is highly important to measure the individual's personality dimensions as these may be used for various practical applications. However, personality has been traditionally studied by questionnaires and other forms of low tech methodologies. The availability of textual data and the development of powerful NLP technologies, invite the challenge of automatically measuring personality dimensions for various applications from granular analytics of customers to the forensic identification of potential offenders. While there are emerging attempts to address this challenge, these attempts almost exclusively focus on one theoretical model of personality and on classification tasks limited when tagged data are not available.The major aim of the tutorial is to provide NLP researchers with an introduction to personality theories that may empower their scope of research. In addition, two secondary aims are to survey some recent directions in computational personality and to point to future directions in which the field may be developed (e.g. Textual Entailment for Personality Analytics).
The rise of Big Data analytics over unstructured text has led to renewed interest in information extraction (IE). These applications need effective IE as a first step towards solving end-to-end real world problems (e.g. biology, medicine, finance, media and entertainment, etc). Much recent NLP research has focused on addressing specific IE problems using a pipeline of multiple machine learning techniques. This approach requires an analyst with the expertise to answer questions such as: {``}What ML techniques should I combine to solve this problem?{''}; {``}What features will be useful for the composite pipeline?{''}; and {``}Why is my model giving the wrong answer on this document?{''}. The need for this expertise creates problems in real world applications. It is very difficult in practice to find an analyst who both understands the real world problem and has deep knowledge of applied machine learning. As a result, the real impact by current IE research does not match up to the abundant opportunities available.In this tutorial, we introduce the concept of transparent machine learning. A transparent ML technique is one that:- produces models that a typical real world use can read and understand;- uses algorithms that a typical real world user can understand; and- allows a real world user to adapt models to new domains.The tutorial is aimed at IE researchers in both the academic and industry communities who are interested in developing and applying transparent ML.
The identification of textual items, or documents, that best match a user{'}s information need, as expressed in search queries, forms the core functionality of information retrieval systems. Well-known challenges are associated with understanding the intent behind user queries; and, more importantly, with matching inherently-ambiguous queries to documents that may employ lexically different phrases to convey the same meaning. The conversion of semi-structured content from Wikipedia and other resources into structured data produces knowledge potentially more suitable to database-style queries and, ideally, to use in information retrieval. In parallel, the availability of textual documents on the Web enables an aggressive push towards the automatic acquisition of various types of knowledge from text. Methods developed under the umbrella of open-domain information extraction acquire open-domain classes of instances and relations from Web text. The methods operate over unstructured or semi-structured text available within collections of Web documents, or over relatively more intriguing streams of anonymized search queries. Some of the methods import the automatically-extracted data into human-generated resources, or otherwise exploit existing human-generated resources. In both cases, the goal is to expand the coverage of the initial resources, thus providing information about more of the topics that people in general, and Web search users in particular, may be interested in.
Every non-trivial text describes interactions and relations between people, institutions, activities, events and so on. What we know about the world consists in large part of such relations, and that knowledge contributes to the understanding of what texts refer to. Newly found relations can in turn become part of this knowledge that is stored for future use.To grasp a text{'}s semantic content, an automatic system must be able to recognize relations in texts and reason about them. This may be done by applying and updating previously acquired knowledge. We focus here in particular on semantic relations which describe the interactions among nouns and compact noun phrases, and we present such relations from both a theoretical and a practical perspective. The theoretical exploration sketches the historical path which has brought us to the contemporary view and interpretation of semantic relations. We discuss a wide range of relation inventories proposed by linguists and by language processing people. Such inventories vary by domain, granularity and suitability for downstream applications.On the practical side, we investigate the recognition and acquisition of relations from texts. In a look at supervised learning methods, we present available datasets, the variety of features which can describe relation instances, and learning algorithms found appropriate for the task. Next, we present weakly supervised and unsupervised learning methods of acquiring relations from large corpora with little or no previously annotated data. We show how enduring the bootstrapping algorithm based on seed examples or patterns has proved to be, and how it has been adapted to tackle Web-scale text collections. We also show a few machine learning techniques which can perform fast and reliable relation extraction by taking advantage of data redundancy and variability.
Analyzing social media texts is a complex problem that becomes difficult to address using traditional Natural Language Processing (NLP) methods. Our tutorial focuses on presenting new methods for NLP tasks and applications that work on noisy and informal texts, such as the ones from social media.Automatic processing of large collections of social media texts is important because they contain a lot of useful information, due to the in-creasing popularity of all types of social media. Use of social media and messaging apps grew 203 percent year-on-year in 2013, with overall app use rising 115 percent over the same period, as reported by Statista, citing data from Flurry Analytics. This growth means that 1.61 billion people are now active in social media around the world and this is expected to advance to 2 billion users in 2016, led by India. The research shows that consumers are now spending daily 5.6 hours on digital media including social media and mo-bile internet usage.At the heart of this interest is the ability for users to create and share content via a variety of platforms such as blogs, micro-blogs, collaborative wikis, multimedia sharing sites, social net-working sites. The unprecedented volume and variety of user-generated content, as well as the user interaction network constitute new opportunities for understanding social behavior and building socially intelligent systems. Therefore it is important to investigate methods for knowledge extraction from social media data. Furthermore, we can use this information to detect and retrieve more related content about events, such as photos and video clips that have caption texts.
This tutorial will give participants a solid understanding of the linguistic features of multiword expressions (MWEs), focusing on the semantics of such expressions and their importance for natural language processing and language technology, with particular attention to the way that FrameNet (framenet.icsi.berkeley.edu) handles this wide spread phenomenon. Our target audience includes researchers and practitioners of language technology, not necessarily experts in MWEs or knowledgeable about FrameNet, who are interested in NLP tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication.NLP research has been interested in automatic processing of multiword expressions, with reports on and tasks relating to such efforts presented at workshops and conferences for at least ten years (e.g. ACL 2003, LREC 2008, COLING 2010, EACL 2014). Overcoming the challenge of automatically processing MWEs remains elusive in part because of the difficulty in recognizing, acquiring, and interpreting such forms.Indeed the phenomenon manifests in a range of linguistic forms (as Sag et al. (2001), among many others, have documented), including: noun + noun compounds (e.g. fish knife, health hazard etc.); adjective + noun compounds (e.g. political agenda, national interest, etc.); particle verbs (shut up, take out, etc.); prepositional verbs (e.g. look into, talk into, etc.); VP idioms, such as kick the bucket, and pull someone{'}s leg, along with less obviously idiomatic forms like answer the door, mention someone{'}s name, etc.; expressions that have their own mini-grammars, such as names with honorifics and terms of address (e.g. Rabbi Lord Jonathan Sacks), kinship terms (e.g. second cousin once removed), and time expressions (e.g. January 9, 2015); support verb constructions (e.g. verbs: take a bath, make a promise, etc; and prepositions: in doubt, under review, etc.). Linguists address issues of polysemy, compositionality, idiomaticity, and continuity for each type included here.While native speakers use these forms with ease, the treatment and interpretation of MWEs in computational systems requires considerable effort due to the very issues that concern linguists.
Computational linguistics has witnessed a surge of interest in approaches to emotion and affect analysis, tackling problems that extend beyond sentiment analysis in depth and complexity. This area involves basic emotions (such as joy, sadness, and fear) as well as any of the hundreds of other emotions humans are capable of (such as optimism, frustration, and guilt), expanding into affective conditions, experiences, and activities. Leveraging linguistic data for computational affect and emotion inference enables opportunities to address a range of affect-related tasks, problems, and non-invasive applications that capture aspects essential to the human condition and individuals{'} cognitive processes. These efforts enable and facilitate human-centered computing experiences, as demonstrated by applications across clinical, socio-political, artistic, educational, and commercial domains. Efforts to computationally detect, characterize, and generate emotions or affect-related phenomena respond equally to technological needs for personalized, micro-level analytics and broad-coverage, macro-level inference, and they have involved both small and massive amounts of data.While this is an exciting area with numerous opportunities for members of the ACL community, a major obstacle is its intersection with other investigatory traditions, necessitating knowledge transfer. This tutorial comprehensively integrates relevant concepts and frameworks from linguistics, cognitive science, affective computing, and computational linguistics in order to equip researchers and practitioners with the adequate background and knowledge to work effectively on problems and tasks either directly involving, or benefiting from having an understanding of, affect and emotion analysis.There is a substantial body of work in traditional sentiment analysis focusing on positive and negative sentiment. This tutorial covers approaches and features that migrate well to affect analysis. We also discuss key differences from sentiment analysis, and their implications for analyzing affect and emotion.The tutorial begins with an introduction that highlights opportunities, key terminology, and interesting tasks and challenges (1). The body of the tutorial covers characteristics of emotive language use with emphasis on relevance for computational analysis (2); linguistic data{---}from conceptual analysis frameworks via useful existing resources to important annotation topics (3); computational approaches for lexical semantic emotion analysis (4); computational approaches for emotion and affect analysis in text (5); visualization methods (6); and a survey of application areas with affect-related problems (7). The tutorial concludes with an outline of future directions and a discussion with participants about the areas relevant to their respective tasks of interest (8).Besides attending the tutorial, tutorial participants receive electronic copies of tutorial slides, a complete reference list, as well as a categorized annotated bibliography that concentrates on seminal works, recent important publications, and other products and resources for researchers and developers.
In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-ofwords and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations. 1  State 1: The old bridge You are standing very close to the bridge’s eastern foundation. If you go east you will be back on solid ground ... The bridge sways in the wind. Command: Go east State 2: Ruined gatehouse The old gatehouse is near collapse. Part of its northern wall has already fallen down ... East of the gatehouse leads out to a small open area surrounded by the remains of the castle. There is also a standing archway offering passage to a path along the old southern inner wall. Exits: Standing archway, castle corner, Bridge over the abyss Figure 1: Sample gameplay from a Fantasy World. The player with the quest of ﬁnding a secret tomb, is currently located on an old bridge. She then chooses an action to go east that brings her to a ruined gatehouse (State 2).  
Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany). In contrast, it is difﬁcult to extract the values of more speciﬁc attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants). In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes. We show that a standard supervised regression model is in fact sufﬁcient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to “objectify” distributional representations for entities, anchoring them more ﬁrmly in the external world in measurable ways. 
In this paper, we introduce an approach to automatically map a standard distributional semantic space onto a set-theoretic model. We predict that there is a functional relationship between distributional information and vectorial concept representations in which dimensions are predicates and weights are generalised quantiﬁers. In order to test our prediction, we learn a model of such relationship over a publicly available dataset of feature norms annotated with natural language quantiﬁers. Our initial experimental results show that, at least for domain-speciﬁc data, we can indeed map between formalisms, and generate high-quality vector representations which encapsulate set overlap information. We further investigate the generation of natural language quantiﬁers from such vectors. 
Compared to tree grammars, graph grammars have stronger generative capacity over structures. Based on an edge replacement grammar, in this paper we propose to use a synchronous graph-to-string grammar for statistical machine translation. The graph we use is directly converted from a dependency tree by labelling edges. We build our translation model in the log-linear framework with standard features. Large-scale experiments on Chinese–English and German–English tasks show that our model is signiﬁcantly better than the state-of-the-art hierarchical phrase-based (HPB) model and a recently improved dependency tree-to-string model on BLEU, METEOR and TER scores. Experiments also suggest that our model has better capability to perform long-distance reordering and is more suitable for translating long sentences. 
We present a novel approach for unsupervised induction of a Reordering Grammar using a modiﬁed form of permutation trees (Zhang and Gildea, 2007), which we apply to preordering in phrase-based machine translation. Unlike previous approaches, we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora. Furthermore, our model (1) handles non-ITG reordering patterns (up to 5-ary branching), (2) is learned from all derivations by treating not only labeling but also bracketing as latent variable, (3) is entirely unlexicalized at the level of reordering rules, and (4) requires no linguistic annotation. Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality. We report signiﬁcant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese. 
Divergent word order between languages causes delay in simultaneous machine translation. We present a sentence rewriting method that generates more monotonic translations to improve the speedaccuracy tradeoff. We design grammaticality and meaning-preserving syntactic transformation rules that operate on constituent parse trees. We apply the rules to reference translations to make their word order closer to the source language word order. On Japanese-English translation (two languages with substantially different structure), incorporating the rewritten, more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline system that only uses gold reference translations. 
This paper describes an approach to largescale modeling of sentiment analysis for the social sciences. The goal is to model relations between nation states through social media. Many cross-disciplinary applications of NLP involve making predictions (such as predicting political elections), but this paper instead focuses on a model that is applicable to broader analysis. Do citizens express opinions in line with their home country’s formal relations? When opinions diverge over time, what is the cause and can social media serve to detect these changes? We describe several learning algorithms to study how the populace of a country discusses foreign nations on Twitter, ranging from state-of-theart contextual sentiment analysis to some required practical learners that ﬁlter irrelevant tweets. We evaluate on standard sentiment evaluations, but we also show strong correlations with two public opinion polls and current international alliance relationships. We conclude with some political science use cases. 
Text data has recently been used as evidence in estimating the political ideologies of individuals, including political elites and social media users. While inferences about people are often the intrinsic quantity of interest, we draw inspiration from open information extraction to identify a new task: inferring the political import of propositions like OBAMA IS A SOCIALIST. We present several models that exploit the structure that exists between people and the assertions they make to learn latent positions of people and propositions at the same time, and we evaluate them on a novel dataset of propositions judged on a political spectrum. 
In this paper, we present a comprehensive study of the relationship between an individual’s personal traits and his/her brand preferences. In our analysis, we included a large number of character traits such as personality, personal values and individual needs. These trait features were obtained from both a psychometric survey and automated social media analytics. We also included an extensive set of brand names from diverse product categories. From this analysis, we want to shed some light on (1) whether it is possible to use personal traits to infer an individual’s brand preferences (2) whether the trait features automatically inferred from social media are good proxies for the ground truth character traits in brand preference prediction. 
Trending topics in microblogs such as Twitter are valuable resources to understand social aspects of real-world events. To enable deep analyses of such trends, semantic annotation is an effective approach; yet the problem of annotating microblog trending topics is largely unexplored by the research community. In this work, we tackle the problem of mapping trending Twitter topics to entities from Wikipedia. We propose a novel model that complements traditional text-based approaches by rewarding entities that exhibit a high temporal correlation with topics during their burst time period. By exploiting temporal information from the Wikipedia edit history and page view logs, we have improved the annotation performance by 17-28%, as compared to the competitive baselines. 
We present a novel framework of system combination for multi-document summarization. For each input set (input), we generate candidate summaries by combining whole sentences from the summaries generated by different systems. We show that the oracle among these candidates is much better than the summaries that we have combined. We then present a supervised model to select among the candidates. The model relies on a rich set of features that capture content importance from different perspectives. Our model performs better than the systems that we combined based on manual and automatic evaluations. We also achieve very competitive performance on six DUC/TAC datasets, comparable to the state-of-the-art on most datasets. 
The task of cross-language document summarization is to create a summary in a target language from documents in a different source language. Previous methods only involve direct extraction of automatically translated sentences from the original documents. Inspired by phrasebased machine translation, we propose a phrase-based model to simultaneously perform sentence scoring, extraction and compression. We design a greedy algorithm to approximately optimize the score function. Experimental results show that our methods outperform the state-of-theart extractive systems while maintaining similar grammatical quality. 
We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern: (1) movement away from evaluation by correlation with human assessment; (2) omission of important components of human assessment from evaluations, in addition to large numbers of metric variants; (3) absence of methods of signiﬁcance testing improvements over a baseline. We outline an evaluation methodology that overcomes all such challenges, providing the ﬁrst method of signiﬁcance testing suitable for evaluation of summarization metrics. Our evaluation reveals for the ﬁrst time which metric variants signiﬁcantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems. 
Social media such as Twitter have become an important method of communication, with potential opportunities for NLG to facilitate the generation of social media content. We focus on the generation of indicative tweets that contain a link to an external web page. While it is natural and tempting to view the linked web page as the source text from which the tweet is generated in an extractive summarization setting, it is unclear to what extent actual indicative tweets behave like extractive summaries. We collect a corpus of indicative tweets with their associated articles and investigate to what extent they can be derived from the articles using extractive methods. We also consider the impact of the formality and genre of the article. Our results demonstrate the limits of viewing indicative tweet generation as extractive summarization, and point to the need for the development of a methodology for tweet generation that is sensitive to genre-speciﬁc issues. 
This paper is concerned with the task of bilingual lexicon induction using imagebased features. By applying features from a convolutional neural network (CNN), we obtain state-of-the-art performance on a standard dataset, obtaining a 79% relative improvement over previous work which uses bags of visual words based on SIFT features. The CNN image-based approach is also compared with state-of-the-art linguistic approaches to bilingual lexicon induction, even outperforming these for one of three language pairs on another standard dataset. Furthermore, we shed new light on the type of visual similarity metric to use for genuine similarity versus relatedness tasks, and experiment with using multiple layers from the same network in an attempt to improve performance. 
Cross-Lingual Learning provides a mechanism to adapt NLP tools available for label rich languages to achieve similar tasks for label-scarce languages. An efficient cross-lingual tool significantly reduces the cost and effort required to manually annotate data. In this paper, we use the Recursive Autoencoder architecture to develop a Cross Lingual Sentiment Analysis (CLSA) tool using sentence aligned corpora between a pair of resource rich (English) and resource poor (Hindi) language. The system is based on the assumption that semantic similarity between different phrases also implies sentiment similarity in majority of sentences. The resulting system is then analyzed on a newly developed Movie Reviews Dataset in Hindi with labels given on a rating scale and compare performance of our system against existing systems. It is shown that our approach significantly outperforms state of the art systems for Sentiment Analysis, especially when labeled data is scarce. 
Opinion summarization is the task of producing the summary of a text, such that the summary also preserves the sentiment of the text. Opinion Summarization is thus a trade-off between summarization and sentiment analysis. The demand of compression may drop sentiment bearing sentences, and the demand of sentiment detection may bring in redundant sentences. We harness the power of submodularity to strike a balance between two conﬂicting requirements. We investigate an incipient class of submodular functions for the problem, and a partial enumeration based greedy algorithm that has performance guarantee of 63%. Our functions generate summaries such that there is good correlation between document sentiment and summary sentiment along with good ROUGE score, which outperforms thestate-of-the-art algorithms. 
In this work, we build an entity/event-level sentiment analysis system, which is able to recognize and infer both explicit and implicit sentiments toward entities and events in the text. We design Probabilistic Soft Logic models that integrate explicit sentiments, inference rules, and +/-effect event information (events that positively or negatively affect entities). The experiments show that the method is able to greatly improve over baseline accuracies in recognizing entity/event-level sentiments. 
A simile is a comparison between two essentially unlike things, such as “Jane swims like a dolphin”. Similes often express a positive or negative sentiment toward something, but recognizing the polarity of a simile can depend heavily on world knowledge. For example, “memory like an elephant” is positive, but “memory like a sieve” is negative. Our research explores methods to recognize the polarity of similes on Twitter. We train classiﬁers using lexical, semantic, and sentiment features, and experiment with both manually and automatically generated training data. Our approach yields good performance at identifying positive and negative similes, and substantially outperforms existing sentiment resources. 
 In this paper we focus on a new problem of event coreference resolution across television news videos. Based on the observation that the contents from multiple data modalities are complementary, we develop a novel approach to jointly encode effective features from both closed captions and video key frames. Experiment results demonstrate that visual features provided 7.2% absolute F-score gain on stateof-the-art text based event extraction and coreference resolution.  
IBM Model 1 is a classical alignment model. Of the ﬁrst generation word-based SMT models, it was the only such model with a concave objective function. For concave optimization problems like IBM Model 1, we have guarantees on the convergence of optimization algorithms such as Expectation Maximization (EM). However, as was pointed out recently, the objective of IBM Model 1 is not strictly concave and there is quite a bit of alignment quality variance within the optimal solution set. In this work we detail a strictly concave version of IBM Model 1 whose EM algorithm is a simple modiﬁcation of the original EM algorithm of Model 1 and does not require the tuning of a learning rate or the insertion of an l2 penalty. Moreover, by addressing Model 1’s shortcomings, we achieve AER and F-Measure improvements over the classical Model 1 by over 30%. 
This paper discusses the use of factorization techniques in distributional semantic models. We focus on a method for redistributing the weight of latent variables, which has previously been shown to improve the performance of distributional semantic models. However, this result has not been replicated and remains poorly understood. We reﬁne the method, and provide additional theoretical justiﬁcation, as well as empirical results that demonstrate the viability of the proposed approach. 
In this paper we explore a POS tagging application of neural architectures that can infer word representations from the raw character stream. It relies on two modelling stages that are jointly learnt: a convolutional network that infers a word representation directly from the character stream, followed by a prediction stage. Models are evaluated on a POS and morphological tagging task for German. Experimental results show that the convolutional network can infer meaningful word representations, while for the prediction stage, a well designed and structured strategy allows the model to outperform stateof-the-art results, without any feature engineering. 
We investigate an extension of continuous online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. 
When building spoken dialogue systems for a new domain, a major bottleneck is developing a spoken language understanding (SLU) module that handles the new domain’s terminology and semantic concepts. We propose a statistical SLU model that generalises to both previously unseen input words and previously unseen output classes by leveraging unlabelled data. After mapping the utterance into a vector space, the model exploits the structure of the output labels by mapping each label to a hyperplane that separates utterances with and without that label. Both these mappings are initialised with unsupervised word embeddings, so they can be computed even for words or concepts which were not in the SLU training data. 
Research on modeling time series text corpora has typically focused on predicting what text will come next, but less well studied is predicting when the next text event will occur. In this paper we address the latter case, framed as modeling continuous inter-arrival times under a logGaussian Cox process, a form of inhomogeneous Poisson process which captures the varying rate at which the tweets arrive over time. In an application to rumour modeling of tweets surrounding the 2014 Ferguson riots, we show how interarrival times between tweets can be accurately predicted, and that incorporating textual features further improves predictions. 
In the last several years, neural network models have signiﬁcantly improved accuracy in a number of NLP tasks. However, one serious drawback that has impeded their adoption in production systems is the slow runtime speed of neural network models compared to alternate models, such as maximum entropy classiﬁers. In Devlin et al. (2014), the authors presented a simple technique for speeding up feed-forward embedding-based neural network models, where the dot product between each word embedding and part of the ﬁrst hidden layer are pre-computed ofﬂine. However, this technique cannot be used for hidden layers beyond the ﬁrst. In this paper, we explore a neural network architecture where the embedding layer feeds into multiple hidden layers that are placed “next to” one another so that each can be pre-computed independently. On a large scale language modeling task, this architecture achieves a 10x speedup at runtime and a signiﬁcant reduction in perplexity when compared to a standard multilayer network. 
A wide range of applications, from social media to scientiﬁc literature analysis, involve graphs in which documents are connected by links. We introduce a topic model for link prediction based on the intuition that linked documents will tend to have similar topic distributions, integrating a max-margin learning criterion and lexical term weights in the loss function. We validate our approach on the tweets from 2,000 Sina Weibo users and evaluate our model’s reconstruction of the social network. 
We study the problem of jointly embedding a knowledge base and a text corpus. The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space. Wang et al. (2014a) rely on Wikipedia anchors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to ﬁt the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of Wang et al. (2014a), which is encouraging as we do not use any anchor information. 
Despite the convexity of structured maxmargin objectives (Taskar et al., 2004; Tsochantaridis et al., 2004), the many ways to optimize them are not equally effective in practice. We compare a range of online optimization methods over a variety of structured NLP tasks (coreference, summarization, parsing, etc) and ﬁnd several broad trends. First, margin methods do tend to outperform both likelihood and the perceptron. Second, for max-margin objectives, primal optimization methods are often more robust and progress faster than dual methods. This advantage is most pronounced for tasks with dense or continuous-valued features. Overall, we argue for a particularly simple online primal subgradient descent method that, despite being rarely mentioned in the literature, is surprisingly effective in relation to its alternatives. 
There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors, in which each dimension corresponds to a context word. In this paper, we initialize an embedding-learning model with distributional vectors. Evaluation on word similarity shows that this initialization signiﬁcantly increases the quality of embeddings for rare words. 
Performing link prediction in Knowledge Bases (KBs) with embedding-based models, like with the model TransE (Bordes et al., 2013) which represents relationships as translations in the embedding space, have shown promising results in recent years. Most of these works are focused on modeling single relationships and hence do not take full advantage of the graph structure of KBs. In this paper, we propose an extension of TransE that learns to explicitly model composition of relationships via the addition of their corresponding translation vectors. We show empirically that this allows to improve performance for predicting single relationships as well as compositions of pairs of them. 
In order to reduce noise in training data, most natural language crowdsourcing annotation tasks gather redundant labels and aggregate them into an integrated label, which is provided to the classiﬁer. However, aggregation discards potentially useful information from linguistically ambiguous instances. For ﬁve natural language tasks, we pass item agreement on to the task classiﬁer via soft labeling and low-agreement ﬁltering of the training dataset. We ﬁnd a statistically signiﬁcant beneﬁt from low item agreement training ﬁltering in four of our ﬁve tasks, and no systematic beneﬁt from soft labeling. 
We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text. Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to speciﬁc queries. These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing. 
Latent Dirichlet allocation (LDA) is a popular topic modeling technique for exploring hidden topics in text corpora. Increasingly, topic modeling needs to scale to larger topic spaces and use richer forms of prior knowledge, such as word correlations or document labels. However, inference is cumbersome for LDA models with prior knowledge. As a result, LDA models that use prior knowledge only work in small-scale scenarios. In this work, we propose a factor graph framework, Sparse Constrained LDA (SC-LDA), for efﬁciently incorporating prior knowledge into LDA. We evaluate SC-LDA’s ability to incorporate word correlation knowledge and document label knowledge on three benchmark datasets. Compared to several baseline methods, SC-LDA achieves comparable performance but is signiﬁcantly faster. 
Path queries on a knowledge graph can be used to answer compositional questions such as “What languages are spoken by people living in Lisbon?”. However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new “compositional” training objective, which dramatically improves all models’ ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results. 
We present a novel method for the crosslingual transfer of dependency parsers. Our goal is to induce a dependency parser in a target language of interest without any direct supervision: instead we assume access to parallel translations between the target and one or more source languages, and to supervised parsers in the source language(s). Our key contributions are to show the utility of dense projected structures when training the target language parser, and to introduce a novel learning algorithm that makes use of dense structures. Results on several languages show an absolute improvement of 5.51% in average dependency accuracy over the state-of-the-art method of (Ma and Xia, 2014). Our average dependency accuracy of 82.18% compares favourably to the accuracy of fully supervised methods. 
Accurate dependency parsing requires large treebanks, which are only available for a few languages. We propose a method that takes advantage of shared structure across languages to build a mature parser using less training data. We propose a model for learning a shared “universal” parser that operates over an interlingual continuous representation of language, along with language-speciﬁc mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations. 
We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages. Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model beneﬁts from incorporating the character-based encodings of words. 
We present an LSTM approach to deletion-based sentence compression where the task is to translate a sentence into a sequence of zeros and ones, corresponding to token deletion decisions. We demonstrate that even the most basic version of the system, which is given no syntactic information (no PoS or NE tags, or dependencies) or desired compression length, performs surprisingly well: around 30% of the compressions from a large test set could be regenerated. We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features. In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness. 
Syntactic language models and N-gram language models have both been used in word ordering. In this paper, we give an empirical comparison between N-gram and syntactic language models on word order task. Our results show that the quality of automatically-parsed training data has a relatively small impact on syntactic models. Both of syntactic and N-gram models can beneﬁt from large-scale raw text. Compared with N-gram models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark. 
We propose a summarization approach for scientiﬁc articles which takes advantage of citation-context and the document discourse model. While citations have been previously used in generating scientiﬁc summaries, they lack the related context from the referenced article and therefore do not accurately reﬂect the article’s content. Our method overcomes the problem of inconsistency between the citation summary and the article’s content by providing context for each citation. We also leverage the inherent scientiﬁc article’s discourse for producing better summaries. We show that our proposed method effectively improves over existing summarization approaches (greater than 30% improvement over the best performing baseline) in terms of ROUGE scores on TAC2014 scientiﬁc summarization dataset. While the dataset we use for evaluation is in the biomedical domain, most of our approaches are general and therefore adaptable to other domains. 
In recent years, the task of recommending hashtags for microblogs has been given increasing attention. Various methods have been proposed to study the problem from different aspects. However, most of the recent studies have not considered the differences in the types or uses of hashtags. In this paper, we introduce a novel nonparametric Bayesian method for this task. Based on the Dirichlet Process Mixture Models (DPMM), we incorporate the type of hashtag as a hidden variable. The results of experiments on the data collected from a real world microblogging service demonstrate that the proposed method outperforms stateof-the-art methods that do not consider these aspects. By taking these aspects into consideration, the relative improvement of the proposed method over the state-of-theart methods is around 12.2% in F1- score. 
This paper proposes a graph-based readability assessment method using word coupling. Compared to the state-of-theart methods such as the readability formulae, the word-based and feature-based methods, our method develops a coupled bag-of-words model which combines the merits of word frequencies and text features. Unlike the general bag-of-words model which assumes words are independent, our model correlates the words based on their similarities on readability. By applying TF-IDF (Term Frequency and Inverse Document Frequency), the coupled TF-IDF matrix is built, and used in the graph-based classiﬁcation framework, which involves graph building, merging and label propagation. Experiments are conducted on both English and Chinese datasets. The results demonstrate both effectiveness and potential of the method. 
Social media represents a rich source of upto-date information about events such as incidents. The sheer amount of available information makes machine learning approaches a necessity for further processing. This learning problem is often concerned with regionally restricted datasets such as data from only one city. Because social media data such as tweets varies considerably across different cities, the training of efﬁcient models requires labeling data from each city of interest, which is costly and time consuming. In this study, we investigate which features are most suitable for training generalizable models, i.e., models that show good performance across different datasets. We reimplemented the most popular features from the state of the art in addition to other novel approaches, and evaluated them on data from ten different cities. We show that many sophisticated features are not necessarily valuable for training a generalized model and are outperformed by classic features such as plain word-n-grams and character-n-grams. 
Engaging in a debate with oneself or others to take decisions is an integral part of our day-today life. A debate on a topic (say, use of performance enhancing drugs) typically proceeds by one party making an assertion/claim (say, PEDs are bad for health) and then providing an evidence to support the claim (say, a 2006 study shows that PEDs have psychiatric side effects). In this work, we propose the task of automatically detecting such evidences from unstructured text that support a given claim. This task has many practical applications in decision support and persuasion enhancement in a wide range of domains. We ﬁrst introduce an extensive benchmark data set tailored for this task, which allows training statistical models and assessing their performance. Then, we suggest a system architecture based on supervised learning to address the evidence detection task. Finally, promising experimental results are reported. 
We use character-based statistical machine translation in order to correct user search queries in the e-commerce domain. The training data is automatically extracted from event logs where users re-issue their search queries with potentially corrected spelling within the same session. We show results on a test set which was annotated by humans and compare against online autocorrection capabilities of three additional web sites. Overall, the methods presented in this paper outperform fully productized spellchecking and autocorrection services in terms of accuracy and F1 score. We also propose novel evaluation steps based on retrieved search results of the corrected queries in terms of quantity and relevance. 
We present a novel ﬁne-tuning algorithm in a deep hybrid architecture for semisupervised text classiﬁcation. During each increment of the online learning process, the ﬁne-tuning algorithm serves as a top-down mechanism for pseudo-jointly modifying model parameters following a bottom-up generative learning pass. The resulting model, trained under what we call the Bottom-Up-Top-Down learning algorithm, is shown to outperform a variety of competitive models and baselines trained across a wide range of splits between supervised and unsupervised training data. 
Sponsored search is at the center of a multibillion dollar market established by search technology. Accurate ad click prediction is a key component for this market to function since the pricing mechanism heavily relies on the estimation of click probabilities. Lexical features derived from the text of both the query and ads play a signiﬁcant role, complementing features based on historical click information. The purpose of this paper is to explore the use of word embedding techniques to generate effective text features that can capture not only lexical similarity between query and ads but also the latent user intents. We identify several potential weaknesses of the plain application of conventional word embedding methodologies for ad click prediction. These observations motivated us to propose a set of novel joint word embedding methods by leveraging implicit click feedback. We verify the effectiveness of these new word embedding models by adding features derived from the new models to the click prediction system of a commercial search engine. Our evaluation results clearly demonstrate the effectiveness of the proposed methods. To the best of our knowledge this work is the ﬁrst successful application of word embedding techniques for the task of click prediction in sponsored search. 
The prevalence of temporal references across all types of natural language utterances makes temporal analysis a key issue in Natural Language Processing. This work adresses three research questions: 1/is temporal expression recognition speciﬁc to a particular domain? 2/if so, can we characterize domain speciﬁcity? and 3/how can subdomain speciﬁcity be integrated in a single tool for uniﬁed temporal expression extraction? Herein, we assess temporal expression recognition from documents written in French covering three domains. We present a new corpus of clinical narratives annotated for temporal expressions, and also use existing corpora in the newswire and historical domains. We show that temporal expressions can be extracted with high performance across domains (best F-measure 0.96 obtained with a CRF model on clinical narratives). We argue that domain adaptation for the extraction of temporal expressions can be done with limited efforts and should cover pre-processing as well as temporal speciﬁc tasks. 
Semi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed relationships while limiting the semantic drift. We research bootstrapping for relationship extraction using word embeddings to ﬁnd similar relationships. Experimental results show that relying on word embeddings achieves a better performance on the task of extracting four types of relationships from a collection of newswire documents when compared with a baseline using TFIDF to ﬁnd similar relationships. 
Scientiﬁc theories and models in Earth science typically involve changing variables and their complex interactions, including correlations, causal relations and chains of positive/negative feedback loops. Variables tend to be complex rather than atomic entities and expressed as noun phrases containing multiple modiﬁers, e.g. oxygen depletion in the upper 500 m of the ocean or timing and magnitude of surface temperature evolution in the Southern Hemisphere in deglacial proxy records. Text mining from Earth science literature is therefore signiﬁcantly different from biomedical text mining and requires different approaches and methods. Our approach aims at automatically locating and extracting variables and their direction of variation: increasing, decreasing or just changing. Variables are initially extracted by matching tree patterns onto the syntax trees of the source texts. Next, variables are generalised in order to enhance their similarity, facilitating hierarchical search and inference. This generalisation is accomplished by progressive pruning of syntax trees using a set of tree transformation operations. Text mining results are presented as a browsable variable hierarchy which allows users to inspect all mentions of a particular variable type in the text as well as any generalisations or specialisations. The approach is demonstrated on a corpus of 10k abstracts of Nature publications in the ﬁeld of Marine science. We discuss experiences with this early prototype and outline a number of possible improvements and directions for future research.  
We consider a novel setting for Named Entity Recognition (NER) where we have access to document-speciﬁc knowledge base tags. These tags consist of a canonical name from a knowledge base (KB) and entity type, but are not aligned to the text. We explore how to use KB tags to create document-speciﬁc gazetteers at inference time to improve NER. We ﬁnd that this kind of supervision helps recognise organisations more than standard widecoverage gazetteers. Moreover, augmenting document-speciﬁc gazetteers with KB information lets users specify fewer tags for the same performance, reducing cost. 
Learning to determine when the timevarying facts of a Knowledge Base (KB) have to be updated is a challenging task. We propose to learn state changing verbs from Wikipedia edit history. When a state-changing event, such as a marriage or death, happens to an entity, the infobox on the entity’s Wikipedia page usually gets updated. At the same time, the article text may be updated with verbs either being added or deleted to reﬂect the changes made to the infobox. We use Wikipedia edit history to distantly supervise a method for automatically learning verbs and state changes. Additionally, our method uses constraints to effectively map verbs to infobox changes. We observe in our experiments that when state-changing verbs are added or deleted from an entity’s Wikipedia page text, we can predict the entity’s infobox updates with 88% precision and 76% recall. One compelling application of our verbs is to incorporate them as triggers in methods for updating existing KBs, which are currently mostly static. 
Because of polysemy, distant labeling for information extraction leads to noisy training data. We describe a procedure for reducing this noise by using label propagation on a graph in which the nodes are entity mentions, and mentions are coupled when they occur in coordinate list structures. We show that this labeling approach leads to good performance even when off-the-shelf classiﬁers are used on the distantly-labeled data. 
Automatic construction of knowledge graphs (KGs) from unstructured text has received considerable attention in recent research, resulting in the construction of several KGs with millions of entities (nodes) and facts (edges) among them. Unfortunately, such KGs tend to be severely sparse in terms of number of facts known for a given entity, i.e., have low knowledge density. For example, the NELL KG consists of only 1.34 facts per entity. Unfortunately, such low knowledge density makes it challenging to use such KGs in real-world applications. In contrast to best-eﬀort extraction paradigms followed in the construction of such KGs, in this paper we argue in favor of ENTIty Centric Expansion (ENTICE), an entity-centric KG population framework, to alleviate the low knowledge density problem in existing KGs. By using ENTICE, we are able to increase NELL’s knowledge density by a factor of 7.7 at 75.5% accuracy. Additionally, we are also able to extend the ontology discovering new relations and entities. 
Temporal taggers are usually developed for a certain language. Besides English, only few languages have been addressed, and only the temporal tagger HeidelTime covers several languages. While this tool was manually extended to these languages, there have been earlier approaches for automatic extensions to a single target language. In this paper, we present an approach to extend HeidelTime to all languages in the world. Our evaluation shows promising results, in particular considering that our approach neither requires language skills nor training data, but results in a baseline tagger for 200+ languages. 
We consider the task of named entity recognition for Chinese social media. The long line of work in Chinese NER has focused on formal domains, and NER for social media has been largely restricted to English. We present a new corpus of Weibo messages annotated for both name and nominal mentions. Additionally, we evaluate three types of neural embeddings for representing Chinese text. Finally, we propose a joint training objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 
This paper presents a framework to model the semantic representation of binary relations produced by open information extraction systems. For each binary relation, we infer a set of preferred types on the two arguments simultaneously, and generate a ranked list of type pairs which we call schemas. All inferred types are drawn from the Freebase type taxonomy, which are human readable. Our system collects 171,168 binary relations from ReVerb, and is able to produce top-ranking relation schemas with a mean reciprocal rank of 0.337. 
This paper studies Cumulative Citation Recommendation (CCR) - given an entity in Knowledge Bases, how to effectively detect its potential citations from volume text streams. Most previous approaches treated all kinds of features indifferently to build a global relevance model, in which the prior knowledge embedded in documents cannot be exploited adequately. To address this problem, we propose a latent document type discriminative model by introducing a latent layer to capture the correlations between documents and their underlying types. The model can better adjust to different types of documents and yield ﬂexible performance when dealing with a broad range of document types. An extensive set of experiments has been conducted on TREC-KBA-2013 dataset, and the results demonstrate that this model can yield a signiﬁcant performance gain in recommendation quality as compared to the state-of-the-art. 
The difﬁcult language in Electronic Health Records (EHRs) presents a challenge to patients’ understanding of their own conditions. One approach to lowering the barrier is to provide tailored patient education based on their own EHR notes. We are developing a system to retrieve EHR note-tailored online consumer oriented health education materials. We explored topic model and key concept identiﬁcation methods to construct queries from the EHR notes. Our experiments show that queries using identiﬁed key concepts with pseudo-relevance feedback signiﬁcantly outperform (over 10-fold improvement) the baseline system of using the full text note. 
 Image-Mediated Learning  We propose an image-mediated learning approach for cross-lingual document retrieval where no or only a few parallel corpora are available. Using the images in image-text documents of each language as the hub, we derive a common semantic subspace bridging two languages by means of generalized canonical correlation analysis. For the purpose of evaluation, we create and release a new document dataset consisting of three types of data (English text, Japanese text, and images). Our approach substantially enhances retrieval accuracy in zero-shot and few-shot scenarios where text-to-text examples are scarce. 
In November 2014, the European Central Bank (ECB) started to directly supervise the largest banks in the Eurozone via the Single Supervisory Mechanism (SSM). While supervisory risk assessments are usually based on quantitative data and surveys, this work explores whether sentiment analysis is capable of measuring a bank’s attitude and opinions towards risk by analyzing text data. For realizing this study, a collection consisting of more than 500 CEO letters and outlook sections extracted from bank annual reports is built up. Based on these data, two distinct experiments are conducted. The evaluations ﬁnd promising opportunities, but also limitations for risk sentiment analysis in banking supervision. At the level of individual banks, predictions are relatively inaccurate. In contrast, the analysis of aggregated ﬁgures revealed strong and signiﬁcant correlations between uncertainty or negativity in textual disclosures and the quantitative risk indicator’s future evolution. Risk sentiment analysis should therefore rather be used for macroprudential analyses than for assessments of individual banks. 
Web reviews have been intensively studied in argumentation-related tasks such as sentiment analysis. However, due to their focus on content-based features, many sentiment analysis approaches are effective only for reviews from those domains they have been speciﬁcally modeled for. This paper puts its focus on domain independence and asks whether a general model can be found for how people argue in web reviews. Our hypothesis is that people express their global sentiment on a topic with similar sequences of local sentiment independent of the domain. We model such sentiment ﬂow robustly under uncertainty through abstraction. To test our hypothesis, we predict global sentiment based on sentiment ﬂow. In systematic experiments, we improve over the domain independence of strong baselines. Our ﬁndings suggest that sentiment ﬂow qualiﬁes as a general model of web review argumentation. 
Open domain targeted sentiment is the joint information extraction task that ﬁnds target mentions together with the sentiment towards each mention from a text corpus. The task is typically modeled as a sequence labeling problem, and solved using state-of-the-art labelers such as CRF. We empirically study the effect of word embeddings and automatic feature combinations on the task by extending a CRF baseline using neural networks, which have demonstrated large potentials for sentiment analysis. Results show that the neural model can give better results by signiﬁcantly increasing the recall. In addition, we propose a novel integration of neural and discrete features, which combines their relative advantages, leading to signiﬁcantly higher results compared to both baselines. 
A fundamental issue in opinion mining is to search a corpus for opinion units, each of which typically comprises the evaluation by an author for a target object from an aspect, such as “This hotel is in a good location”. However, few attempts have been made to address cases where the validity of an evaluation is restricted on a condition in the source text, such as “for traveling with small kids”. In this paper, we propose a method to extract condition-opinion relations from online reviews, which enables ﬁne-grained analysis for the utility of target objects depending the user attribute, purpose, and situation. Our method uses supervised machine learning to identify sequences of words or phrases that comprise conditions for opinions. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally. 
Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classiﬁers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the ﬁrst time. 
This paper introduces the task of questionanswer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure. For example, the verb “introduce” in the previous sentence would be labeled with the questions “What is introduced?”, and “What introduces something?”, each paired with the phrase from the sentence that gives the correct answer. Posing the problem this way allows the questions themselves to deﬁne the set of possible roles, without the need for predeﬁned frame or thematic role ontologies. It also allows for scalable data collection by annotators with very little training and no linguistic expertise. We gather data in two domains, newswire text and Wikipedia articles, and introduce simple classiﬁerbased models for predicting which questions to ask and what their answers should be. Our results show that non-expert annotators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task. 
Target entity disambiguation (TED), the task of identifying target entities of the same domain, has been recognized as a critical step in various important applications. In this paper, we propose a graphbased model called TremenRank to collectively identify target entities in short texts given a name list only. TremenRank propagates trust within the graph, allowing for an arbitrary number of target entities and texts using inverted index technology. Furthermore, we design a multi-layer directed graph to assign different trust levels to short texts for better performance. The experimental results demonstrate that our model outperforms state-of-the-art methods with an average gain of 24.8% in accuracy and 15.2% in the F1-measure on three datasets in different domains. 
Knowledge Base Population (KBP) tasks, such as slot filling, show the particular importance of entity-oriented automatic relevant document acquisition. Rich, diverse and reliable relevant documents satisfy the fundamental requirement that a KBP system explores the nature of an entity. Towards the bottleneck problem between comprehensiveness and definiteness of acquisition, we propose a collaborative archiving method. In particular we introduce topic modeling methodologies into entity biography profiling, so as to build a bridge between fuzzy and exact matching. On one side, we employ the topics in a small-scale high-quality relevant documents (i.e., exact matching results) to summarize the life slices of a target entity (i.e., biography), and on the other side, we use the biography as a reliable reference material to detect new truly relevant documents from a large-scale partially complete pseudo-feedback (i.e., fuzzy matching results). We leverage the archiving method to enhance slot filling systems. Experiments on KBP corpus show significant improvement over stateof-the-art. 
ListNet is a well-known listwise learning to rank model and has gained much attention in recent years. A particular problem of ListNet, however, is the high computation complexity in model training, mainly due to the large number of object permutations involved in computing the gradients. This paper proposes a stochastic ListNet approach which computes the gradient within a bounded permutation subset. It signiﬁcantly reduces the computation complexity of model training and allows extension to Top-k models, which is impossible with the conventional implementation based on full-set permutations. Meanwhile, the new approach utilizes partial ranking information of human labels, which helps improve model quality. Our experiments demonstrated that the stochastic ListNet method indeed leads to better ranking performance and speeds up the model training remarkably. 
Elementary-level science exams pose signiﬁcant knowledge acquisition and reasoning challenges for automatic question answering. We develop a system that reasons with knowledge derived from textbooks, represented in a subset of ﬁrstorder logic. Automatic extraction, while scalable, often results in knowledge that is incomplete and noisy, motivating use of reasoning mechanisms that handle uncertainty. Markov Logic Networks (MLNs) seem a natural model for expressing such knowledge, but the exact way of leveraging MLNs is by no means obvious. We investigate three ways of applying MLNs to our task. First, we simply use the extracted science rules directly as MLN clauses and exploit the structure present in hard constraints to improve tractability. Second, we interpret science rules as describing prototypical entities, resulting in a drastically simpliﬁed but brittle network. Our third approach, called Praline, uses MLNs to align lexical elements as well as deﬁne and control how inference should be performed in this task. Praline demonstrates a 15% accuracy boost and a 10x reduction in runtime as compared to other MLNbased methods, and comparable accuracy to word-based baseline approaches. 
Linking named mentions detected in a source document to an existing knowledge base provides disambiguated entity referents for the mentions. This allows better document analysis, knowledge extraction and knowledge base population. Most of the previous research extensively exploited the linguistic features of the source documents in a supervised or semi-supervised way. These systems therefore cannot be easily applied to a new language or domain. In this paper, we present a novel unsupervised algorithm named Quantiﬁed Collective Validation that avoids excessive linguistic analysis on the source documents and fully leverages the knowledge base structure for the entity linking task. We show our approach achieves stateof-the-art English entity linking performance and demonstrate successful deployment in a new language (Chinese) and two new domains (Biomedical and Earth Science). Experiment datasets and system demonstration are available at http://tw.rpi.edu/web/doc/ hanwang_emnlp_2015 for research purpose. 
Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns between entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allocation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as compared with baselines, our model achieves signiﬁcant and consistent improvements on knowledge base completion and relation extraction from text. The source code of this paper can be obtained from https://github.com/mrlyk423/ relation_extraction. 
This paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class such as “food” or “artist”. The application of entity typing we are interested in is knowledge base completion, speciﬁcally, to learn which classes an entity is a member of. We propose FIGMENT to tackle this problem. FIGMENT is embedding-based and combines (i) a global model that scores based on aggregated contextual information of an entity and (ii) a context model that ﬁrst scores the individual occurrences of an entity and then aggregates the scores. In our evaluation, FIGMENT strongly outperforms an approach to entity typing that relies on relations obtained by an open information extraction system. 
We present KB-UNIFY, a novel approach for integrating the output of different Open Information Extraction systems into a single uniﬁed and fully disambiguated knowledge repository. KB-UNIFY consists of three main steps: (1) disambiguation of relation argument pairs via a sensebased vector representation and a large uniﬁed sense inventory; (2) ranking of semantic relations according to their degree of speciﬁcity; (3) cross-resource relation alignment and merging based on the semantic similarity of domains and ranges. We tested KB-UNIFY on a set of four heterogeneous knowledge bases, obtaining high-quality results. We discuss and provide evaluations at each stage, and release output and evaluation data for the use and scrutiny of the community1. 
Out-of-vocabulary name errors in speech recognition create signiﬁcant problems for downstream language processing, but the fact that they are rare poses challenges for automatic detection, particularly in an open-domain scenario. To address this problem, a multi-task recurrent neural network language model for sentence-level name detection is proposed for use in combination with out-of-vocabulary word detection. The sentence-level model is also effective for leveraging external text data. Experiments show a 26% improvement in name-error detection F-score over a system using n-gram lexical features. 
Distantly supervised approaches have become popular in recent years as they allow training relation extractors without textbound annotation, using instead known relations from a knowledge base and a large textual corpus from an appropriate domain. While state of the art distant supervision approaches use off-theshelf named entity recognition and classiﬁcation (NERC) systems to identify relation arguments, discrepancies in domain or genre between the data used for NERC training and the intended domain for the relation extractor can lead to low performance. This is particularly problematic for “non-standard” named entities such as album which would fall into the MISC category. We propose to ameliorate this issue by jointly training the named entity classiﬁer and the relation extractor using imitation learning which reduces structured prediction learning to classiﬁcation learning. We further experiment with Web features different features and compare against using two off-the-shelf supervised NERC systems, Stanford NER and FIGER, for named entity classiﬁcation. Our experiments show that imitation learning improves average precision by 4 points over an one-stage classiﬁcation model, while removing Web features results in a 6 points reduction. Compared to using FIGER and Stanford NER, average precision is 10 points and 19 points higher with our imitation learning approach. 
Characters are fundamental to literary analysis. Current approaches are heavily reliant on NER to identify characters, causing many to be overlooked. We propose a novel technique for character detection, achieving signiﬁcant improvements over state of the art on multiple datasets. 
This paper introduces a convolutional sentence kernel based on word embeddings. Our kernel overcomes the sparsity issue that arises when classifying short documents or in case of little training data. Experiments on six sentence datasets showed statistically signiﬁcant higher accuracy over the standard linear kernel with ngram features and other proposed models. 
Cooking recipes exist in abundance; but due to their unstructured text format, they are hard to study quantitatively beyond treating them as simple bags of words. In this paper, we propose an ingredientinstruction dependency tree data structure to represent recipes. The proposed representation allows for more reﬁned comparison of recipes and recipe-parts, and is a step towards semantic representation of recipes. Furthermore, we build a parser that maps recipes into the proposed representation. The parser’s edge prediction accuracy of 93.5% improves over a strong baseline of 85.7% (54.5% error reduction). 
Dirichlet process mixture model (DPMM) has great potential for detecting the underlying structure of data. Extensive studies have applied it for text clustering in terms of topics. However, due to the unsupervised nature, the topic clusters are always less satisfactory. Considering that people often have some prior knowledge about which potential topics should exist in given data, we aim to incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded Po´lya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM signiﬁcantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework. 
Recently, neural network based sentence modeling methods have achieved great progress. Among these methods, the recursive neural networks (RecNNs) can effectively model the combination of the words in sentence. However, RecNNs need a given external topological structure, like syntactic tree. In this paper, we propose a gated recursive neural network (GRNN) to model sentences, which employs a full binary tree (FBT) structure to control the combinations in recursive structure. By introducing two kinds of gates, our model can better model the complicated combinations of features. Experiments on three text classiﬁcation datasets show the effectiveness of our model. 
This paper addresses text categorization problem that training data may derive from a different time period from the test data. We present a learning framework which extends a boosting technique to learn accurate model for timeline adaptation. The results showed that the method was comparable to the current state-of-theart biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data. 
Improving the search and browsing experience in PubMed is a key component in helping users detect information of interest. In particular, when exploring a novel ﬁeld, it is important to provide a comprehensive view for a speciﬁc subject. One solution for providing this panoramic picture is to ﬁnd sub-topics from a set of documents. We propose a method that ﬁnds sub-topics that we refer to as themes and computes representative titles based on a set of documents in each theme. The method combines a thematic clustering algorithm and the Pool Adjacent Violators algorithm to induce signiﬁcant themes. Then, for each theme, a title is computed using PubMed document titles and theme-dependent term scores. We tested our system on ﬁve disease sets from OMIM and evaluated the results based on normalized point-wise mutual information and MeSH terms. For both performance measures, the proposed approach outperformed LDA. The quality of theme titles were also evaluated by comparing them with manually created titles. 
Wikipedia is the largest collection of encyclopedic data ever written in the history of humanity. Thanks to its coverage and its availability in machine-readable format, it has become a primary resource for largescale research in historical and cultural studies. In this work, we focus on the subset of pages describing persons, and we investigate the task of recognizing biographical sections from them: given a person’s page, we identify the list of sections where information about her/his life is present. We model this as a sequence classiﬁcation problem, and propose a supervised setting, in which the training data are acquired automatically. Besides, we show that six simple features extracted only from the section titles are very informative and yield good results well above a strong baseline. 
This paper presents a new algorithm to automatically solve algebra word problems. Our algorithm solves a word problem via analyzing a hypothesis space containing all possible equation systems generated by assigning the numbers in the word problem into a set of equation system templates extracted from the training data. To obtain a robust decision surface, we train a log-linear model to make the margin between the correct assignments and the false ones as large as possible. This results in a quadratic programming (QP) problem which can be efﬁciently solved. Experimental results show that our algorithm achieves 79.7% accuracy, about 10% higher than the state-of-the-art baseline (Kushman et al., 2014). 
We present an unsupervised method to ﬁnd lexical variations in Roman Urdu informal text. Our method includes a phonetic algorithm UrduPhone, a featurebased similarity function, and a clustering algorithm Lex-C. UrduPhone encodes roman Urdu strings to their phonetic equivalent representations. This produces an initial grouping of different spelling variations of a word. The similarity function incorporates word features and their context. Lex-C is a variant of k-medoids clustering algorithm that group lexical variations. It incorporates a similarity threshold to balance the number of clusters and their maximum similarity. We test our system on two datasets of SMS and blogs and show an f-measure gain of up to 12% from baseline systems. 
Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 
Multi-label text categorization is a type of text categorization, where each document is assigned to one or more categories. Recently, a series of methods have been developed, which train a classiﬁer for each label, organize the classiﬁers in a partially ordered structure and take predictions produced by the former classiﬁers as the latter classiﬁers’ features. These predictions-asfeatures style methods model high order label dependencies and obtain high performance. Nevertheless, the predictionsas-features methods suffer a drawback. When training a classiﬁer for one label, the predictions-as-features methods can model dependencies between former labels and the current label, but they can’t model dependencies between the current label and the latter labels. To address this problem, we propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classiﬁers for latter labels to the classiﬁer for the current label. We conduct experiments using real-world textual data sets, and these experiments illustrate the predictions-as-features models trained by our algorithm outperform the original models. 
We present a general framework for comparing multiple groups of documents. A bipartite graph model is proposed where document groups are represented as one node set and the comparison criteria are represented as the other node set. Using this model, we present basic algorithms to extract insights into similarities and differences among the document groups. Finally, we demonstrate the versatility of our framework through an analysis of NSF funding programs for basic research. 
Cross-document co-reference resolution (CCR) computes equivalence classes over textual mentions denoting the same entity in a document corpus. Named-entity linking (NEL) disambiguates mentions onto entities present in a knowledge base (KB) or maps them to null if not present in the KB. Traditionally, CCR and NEL have been addressed separately. However, such approaches miss out on the mutual synergies if CCR and NEL were performed jointly. This paper proposes C3EL, an unsupervised framework combining CCR and NEL for jointly tackling both problems. C3EL incorporates results from the CCR stage into NEL, and vice versa: additional global context obtained from CCR improves the feature space and performance of NEL, while NEL in turn provides distant KB features for already disambiguated mentions to improve CCR. The CCR and NEL steps are interleaved in an iterative algorithm that focuses on the highest-conﬁdence still unresolved mentions in each iteration. Experimental results on two different corpora, news-centric and web-centric, demonstrate signiﬁcant gains over state-of-the-art baselines for both CCR and NEL. 
We present a novel model for the task of joint mention extraction and classiﬁcation. Unlike existing approaches, our model is able to effectively capture overlapping mentions with unbounded lengths. The model is highly scalable, with a time complexity that is linear in the number of words in the input sentence and linear in the number of possible mention classes. Our model can be extended to additionally capture mention heads explicitly in a joint manner under the same time complexity. We demonstrate the effectiveness of our model through extensive experiments on standard datasets. 
We propose FINET, a system for detecting the types of named entities in short inputs—such as sentences or tweets—with respect to WordNet’s super ﬁne-grained type system. FINET generates candidate types using a sequence of multiple extractors, ranging from explicitly mentioned types to implicit types, and subsequently selects the most appropriate using ideas from word-sense disambiguation. FINET combats data scarcity and noise from existing systems: It does not rely on supervision in its extractors and generates training data for type selection from WordNet and other resources. FINET supports the most ﬁne-grained type system so far, including types with no annotated training data. Our experiments indicate that FINET outperforms state-of-the-art methods in terms of recall, precision, and granularity of extracted types. 
Extracting named entities in text and linking extracted names to a given knowledge base are fundamental tasks in applications for text understanding. Existing systems typically run a named entity recognition (NER) model to extract entity names ﬁrst, then run an entity linking model to link extracted names to a knowledge base. NER and linking models are usually trained separately, and the mutual dependency between the two tasks is ignored. We propose JERL, Joint Entity Recognition and Linking, to jointly model NER and linking tasks and capture the mutual dependency between them. It allows the information from each task to improve the performance of the other. To the best of our knowledge, JERL is the ﬁrst model to jointly optimize NER and linking tasks together completely. In experiments on the CoNLL’03/AIDA data set, JERL outperforms state-of-art NER and linking systems, and we ﬁnd improvements of 0.4% absolute F1 for NER on CoNLL’03, and 0.36% absolute precision@1 for linking on AIDA. 
We ask how much information a human translator adds to an original text, and we provide a bound. We address this question in the context of bilingual text compression: given a source text, how many bits of additional information are required to specify the target text produced by a human translator? We develop new compression algorithms and establish a benchmark task.  上 个 星 期 的 战 斗 至 少 夺 取12个 人 的 生 命 。 At least 12 people were killed in the battle last week. Last week’s ﬁght took at least 12 lives. The ﬁghting last week killed at least 12. The battle of last week killed at least 12 persons. At least 12 people lost their lives in last week’s ﬁghting. At least 12 persons died in the ﬁghting last week. At least 12 died in the battle last week. At least 12 people were killed in the ﬁghting last week. During last week’s ﬁghting, at least 12 people died. Last week at least twelve people died in the ﬁghting. Last week’s ﬁghting took the lives of twelve people. Figure 1: Eleven human translations of the same source sentence (LDC2002T01).  
This paper proposes a novel hierarchical recurrent neural network language model (HRNNLM) for document modeling. After establishing a RNN to capture the coherence between sentences in a document, HRNNLM integrates it as the sentence history information into the word level RNN to predict the word sequence with cross-sentence contextual information. A two-step training approach is designed, in which sentence-level and word-level language models are approximated for the convergence in a pipeline style. Examined by the standard sentence reordering scenario, HRNNLM is proved for its better accuracy in modeling the sentence coherence. And at the word level, experimental results also indicate a signiﬁcant lower model perplexity, followed by a practical better translation result when applied to a Chinese-English document translation reranking task. 
Neural networks have been shown to improve performance across a range of natural-language tasks. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in hidden layers. We introduce a method for automatically adjusting network size by pruning out hidden units through ∞,1 and 2,1 regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions. 
We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP conﬁgurations can be found by ﬁnite-state composition and dynamic programming. We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multipliers for an adaptively expanding set of variable-length n-gram count features. This is the ﬁrst inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simpliﬁcation, or a bound on string length. Provided that the inference method terminates, it gives a certiﬁcate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation. 
In this paper we present the ﬁrst ever, to the best of our knowledge, discourse parser for multi-party chat dialogues. Discourse in multi-party dialogues dramatically differs from monologues since threaded conversations are commonplace rendering prediction of the discourse structure compelling. Moreover, the fact that our data come from chats renders the use of syntactic and lexical information useless since people take great liberties in expressing themselves lexically and syntactically. We use the dependency parsing paradigm as has been done in the past (Muller et al., 2012; Li et al., 2014). We learn local probability distributions and then use MST for decoding. We achieve 0.680 F1 on unlabelled structures and 0.516 F1 on fully labeled structures which is better than many state of the art systems for monologues, despite the inherent difﬁculties that multi-party chat dialogues have. 
We introduce a new approach to argumentation mining that we applied to a parallel German/English corpus of short texts annotated with argumentation structure. We focus on structure prediction, which we break into a number of subtasks: relation identiﬁcation, central claim identiﬁcation, role classiﬁcation, and function classiﬁcation. Our new model jointly predicts different aspects of the structure by combining the different subtask predictions in the edge weights of an evidence graph; we then apply a standard MST decoding algorithm. This model not only outperforms two reasonable baselines and two datadriven models of global argument structure for the difﬁcult subtask of relation identiﬁcation, but also improves the results for central claim identiﬁcation and function classiﬁcation and it compares favorably to a complex mstparser pipeline. 
Monolingual alignment is the task of pairing semantically similar units from two pieces of text. We report a top-performing supervised aligner that operates on short text snippets. We employ a large feature set to (1) encode similarities among semantic units (words and named entities) in context, and (2) address cooperation and competition for alignment among units in the same snippet. These features are deployed in a two-stage logistic regression framework for alignment. On two benchmark data sets, our aligner achieves F1 scores of 92.1% and 88.5%, with statistically signiﬁcant error reductions of 4.8% and 7.3% over the previous best aligner. It produces top results in extrinsic evaluation as well. 
We present a new method for semantic role labeling in which arguments and semantic roles are jointly embedded in a shared vector space for a given predicate. These embeddings belong to a neural network, whose output represents the potential functions of a graphical model designed for the SRL task. We consider both local and structured learning methods and obtain strong results on standard PropBank and FrameNet corpora with a straightforward product-of-experts model. We further show how the model can learn jointly from PropBank and FrameNet annotations to obtain additional improvements on the smaller FrameNet dataset. 
Relational phrases (e.g., “got married to”) and their hypernyms (e.g., “is a relative of”) are central for many tasks including question answering, open information extraction, paraphrasing, and entailment detection. This has motivated the development of several linguistic resources (e.g. DIRT, PATTY, and WiseNet) which systematically collect and organize relational phrases. These resources have demonstrable practical beneﬁts, but are each limited due to noise, sparsity, or size. We present a new general-purpose method, RELLY, for constructing a large hypernymy graph of relational phrases with high-quality subsumptions using collective probabilistic programming techniques. Our graph induction approach integrates small highprecision knowledge bases together with large automatically curated resources, and reasons collectively to combine these resources into a consistent graph. Using RELLY, we construct a high-coverage, high-precision hypernymy graph consisting of 20K relational phrases and 35K hypernymy links. Our evaluation indicates a hypernymy link precision of 78%, and demonstrates the value of this resource for a document-relevance ranking task. 
We present an unsupervised hard EM approach to automatically mapping instructional recipes to action graphs, which deﬁne what actions should be performed on which objects and in what order. Recovering such structures can be challenging, due to unique properties of procedural language where, for example, verbal arguments are commonly elided when they can be inferred from context and disambiguation often requires world knowledge. Our probabilistic model incorporates aspects of procedural semantics and world knowledge, such as likely locations and selectional preferences for different actions. Experiments with cooking recipes demonstrate the ability to recover high quality action graphs, outperforming a strong sequential baseline by 8 points in F1, while also discovering general-purpose knowledge about cooking. 
Comparison is one of the most important phenomena in language for expressing objective and subjective facts about various entities. Systems that can understand and reason over comparative structure can play a major role in the applications which require deeper understanding of language. In this paper we present a novel semantic framework for representing the meaning of comparative structures in natural language, which models comparisons as predicate-argument pairs interconnected with semantic roles. Our framework supports not only adjectival, but also adverbial, nominal, and verbal comparatives. With this paper, we provide a novel dataset of gold-standard comparison structures annotated according to our semantic framework. 
Sarcasm is generally characterized as a ﬁgure of speech that involves the substitution of a literal by a ﬁgurative meaning, which is usually the opposite of the original literal meaning. We re-frame the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is either literal or sarcastic. We call this the Literal/Sarcastic Sense Disambiguation (LSSD) task. We address two issues: 1) how to collect a set of target words that can have either literal or sarcastic meanings depending on context; and 2) given an utterance and a target word, how to automatically detect whether the target word is used in the literal or the sarcastic sense. For the latter, we investigate several distributional semantics methods and show that a Support Vector Machines (SVM) classiﬁer with a modiﬁed kernel using word embeddings achieves a 7-10% F1 improvement over a strong lexical baseline. 
Taxonomy plays an important role in many applications by organizing domain knowledge into a hierarchy of is-a relations between terms. Previous works on the taxonomic relation identiﬁcation from text corpora lack in two aspects: 1) They do not consider the trustiness of individual source texts, which is important to ﬁlter out incorrect relations from unreliable sources. 2) They also do not consider collective evidence from synonyms and contrastive terms, where synonyms may provide additional supports to taxonomic relations, while contrastive terms may contradict them. In this paper, we present a method of taxonomic relation identiﬁcation that incorporates the trustiness of source texts measured with such techniques as PageRank and knowledge-based trust, and the collective evidence of synonyms and contrastive terms identiﬁed by linguistic pattern matching and machine learning. The experimental results show that the proposed features can consistently improve performance up to 4%-10% of F-measure. 
Logic grid puzzle is a genre of logic puzzles in which we are given (in a natural language) a scenario, the object to be deduced and certain clues. The reader has to ﬁgure out the solution using the clues provided and some generic domain constraints. In this paper, we present a system, LOGICIA, that takes a logic grid puzzle and the set of elements in the puzzle and tries to solve it by translating it to the knowledge representation and reasoning language of Answer Set Programming (ASP) and then using an ASP solver. The translation to ASP involves extraction of entities and their relations from the clues. For that we use a novel learning based approach which uses varied supervision, including the entities present in a clue and the expected representation of a clue in ASP. Our system, LOGICIA, learns to automatically translate a clue with 81.11% accuracy and is able to solve 71% of the problems of a corpus. This is the ﬁrst learning system that can solve logic grid puzzles described in natural language in a fully automated manner. The code and the data will be made publicly available at http://bioai.lab. asu.edu/logicgridpuzzles. 
fast align is a simple, fast, and efﬁcient approach for word alignment based on the IBM model 2. fast align performs well for language pairs with relatively similar word orders; however, it does not perform well for language pairs with drastically different word orders. We propose a segmenting-reversing reordering process to solve this problem by alternately applying fast align and reordering source sentences during training. Experimental results with JapaneseEnglish translation demonstrate that the proposed approach improves the performance of fast align signiﬁcantly without the loss of efﬁciency. Experiments using other languages are also reported. 
We introduce pre-post-editing, possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing. We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens, conﬁrming the promising prospects of the novel translation scenarios offered by our approach. 
Continuous-space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems. A simple, yet effective way to integrate such models in inference is to use them in an N -best rescoring step. In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines. 
In this paper, we propose a paraphrasing model to address the task of system combination for machine translation. We dynamically learn hierarchical paraphrases from target hypotheses and form a synchronous context-free grammar to guide a series of transformations of target hypotheses into fused translations. The model is able to exploit phrasal and structural system-weighted consensus and also to utilize existing information about word ordering present in the target hypotheses. In addition, to consider a diverse set of plausible fused translations, we develop a hybrid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 
We present an incremental adaptation approach for statistical machine translation that maintains a ﬂexible hierarchical domain structure within a single consistent model. Both weights and rules are updated incrementally on a stream of post-edits. Our multi-level domain hierarchy allows the system to adapt simultaneously towards local context at diﬀerent levels of granularity, including genres and individual documents. Our experiments show consistent improvements in translation quality from all components of our approach. 
Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve best results. We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of ﬁve language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data. 
We present novel features designed with a deep neural network for Machine Translation (MT) Quality Estimation (QE). The features are learned with a Continuous Space Language Model to estimate the probabilities of the source and target segments. These new features, along with standard MT system-independent features, are benchmarked on a series of datasets with various quality labels, including postediting effort, human translation edit rate, post-editing time and METEOR. Results show signiﬁcant improvements in prediction over the baseline, as well as over systems trained on state of the art feature sets for all datasets. More notably, the addition of the newly proposed features improves over the best QE systems in WMT12 and WMT14 by a signiﬁcant margin. 
In this paper, we develop a supervised learning technique that improves noisy phrase translation scores obtained by phrase table triangulation. In particular, we extract word translation distributions from small amounts of source-target bilingual data (a dictionary or a parallel corpus) with which we learn to assign better scores to translation candidates obtained by triangulation. Our method is able to gain improvement in translation quality on two tasks: (1) On Malagasy-to-French translation via English, we use only 1k dictionary entries to gain +0.5 Bleu over triangulation. (2) On Spanish-to-French via English we use only 4k sentence pairs to gain +0.7 Bleu over triangulation interpolated with a phrase table extracted from the same 4k sentence pairs. 
This work focuses on the task of ﬁnding latent vector representations of the words in a corpus. In particular, we address the issue of what to do when there are multiple languages in the corpus. Prior work has, among other techniques, used canonical correlation analysis to project pre-trained vectors in two languages into a common space. We propose a simple and scalable method that is inspired by the notion that the learned vector representations should be invariant to translation between languages. We show empirically that our method outperforms prior work on multilingual tasks, matches the performance of prior work on monolingual tasks, and scales linearly with the size of the input data (and thus the number of languages being embedded). 
This paper proposes a method for hierarchical phrase-based stream decoding. A stream decoder is able to take a continuous stream of tokens as input, and segments this stream into word sequences that are translated and output as a stream of target word sequences. Phrase-based stream decoding techniques have been shown to be effective as a means of simultaneous interpretation. In this paper we transfer the essence of this idea into the framework of hierarchical machine translation. The hierarchical decoding framework organizes the decoding process into a chart; this structure is naturally suited to the process of stream decoding, leading to an efﬁcient stream decoding algorithm that searches a restricted subspace containing only relevant hypotheses. Furthermore, the decoder allows more explicit access to the word re-ordering process that is of critical importance in decoding while interpreting. The decoder was evaluated on TED talk data for English-Spanish and English-Chinese. Our results show that like the phrase-based stream decoder, the hierarchical is capable of approaching the performance of the underlying hierarchical phrase-based machine translation decoder, at useful levels of latency. In addition the hierarchical approach appeared to be robust to the difﬁculties presented by the more challenging English-Chinese task. 
In syntax-based machine translation, rule selection is the task of choosing the correct target side of a translation rule among rules with the same source side. We deﬁne a discriminative rule selection model for systems that have syntactic annotation on the target language side (stringto-tree). This is a new and clean way to integrate soft source syntactic constraints into string-to-tree systems as features of the rule selection model. We release our implementation as part of Moses. 
Language use is known to be inﬂuenced by personality traits as well as by sociodemographic characteristics such as age or mother tongue. As a result, it is possible to automatically identify these traits of the author from her texts. It has recently been shown that knowledge of such dimensions can improve performance in NLP tasks such as topic and sentiment modeling. We posit that machine translation is another application that should be personalized. In order to motivate this, we explore whether translation preserves demographic and psychometric traits. We show that, largely, both translation of the source training data into the target language, and the target test data into the source language has a detrimental effect on the accuracy of predicting author traits. We argue that this supports the need for personal and personality-aware machine translation models. 
We introduce Trans-gram, a simple and computationally-efﬁcient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classiﬁcation and word translation tasks. 
Discourse relations can be categorized as continuous or discontinuous in the hypothesis of continuity (Murray, 1997), with continuous relations expressing normal succession of events in discourse such as temporal, spatial or causal. Asr and Demberg (2013) propose a markedness measure to test the prediction that discontinuous relations may have more unambiguous connectives, but restrict the markedness calculation to relations with explicit connectives only. This paper extends their measure to explicit and implicit relations and shows that results from this extension better ﬁt the continuity hypothesis predictions both for the English Penn Discourse (Prasad et al., 2008) and the Chinese Discourse (Zhou and Xue, 2015) Treebanks. 
The widespread use of deception in online sources has motivated the need for methods to automatically proﬁle and identify deceivers. This work explores deception, gender and age detection in short texts using a machine learning approach. First, we collect a new open domain deception dataset also containing demographic data such as gender and age. Second, we extract feature sets including n-grams, shallow and deep syntactic features, semantic features, and syntactic complexity and readability metrics. Third, we build classiﬁers that aim to predict deception, gender, and age. Our ﬁndings show that while deception detection can be performed in short texts even in the absence of a predetermined domain, gender and age prediction in deceptive texts is a challenging task. We further explore the linguistic differences in deceptive content that relate to deceivers gender and age and ﬁnd evidence that both age and gender play an important role in people’s word choices when fabricating lies. 
The phonotactics of a language describes the ways in which the sounds of the language combine to form possible morphemes and words. Humans can learn phonotactic patterns at the level of abstract classes, generalizing across sounds (e.g., “words can end in a voiced stop”). Moreover, they rapidly acquire these generalizations, even before they acquire soundspeciﬁc patterns. We present a probabilistic model intended to capture this earlyabstraction phenomenon. The model represents both abstract and concrete generalizations in its hypothesis space from the outset of learning. This—combined with a parsimony bias in favor of compact descriptions of the input data—leads the model to favor rapid abstraction in a way similar to human learners. 
This paper presents a semantic parsing and reasoning approach to automatically solving math word problems. A new meaning representation language is designed to bridge natural language text and math expressions. A CFG parser is implemented based on 9,600 semi-automatically created grammar rules. We conduct experiments on a test set of over 1,500 number word problems (i.e., verbally expressed number problems) and yield 95.4% precision and 60.2% recall. 
We present a parser for Abstract Meaning Representation (AMR). We treat Englishto-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-speciﬁc language model and add data and features drawn from semantic resources. Our resulting AMR parser significantly improves upon state-of-the-art results. 
According to the principle of compositionality, the meaning of a sentence is computed from the meaning of its parts and the way they are syntactically combined. In practice, however, the syntactic structure is computed by automatic parsers which are far-from-perfect and not tuned to the speciﬁcs of the task. Current recursive neural network (RNN) approaches for computing sentence meaning therefore run into a number of practical difﬁculties, including the need to carefully select a parser appropriate for the task, deciding how and to what extent syntactic context modiﬁes the semantic composition function, as well as on how to transform parse trees to conform to the branching settings (typically, binary branching) of the RNN. This paper introduces a new model, the Forest Convolutional Network, that avoids all of these challenges, by taking a parse forest as input, rather than a single tree, and by allowing arbitrary branching factors. We report improvements over the state-of-the-art in sentiment analysis and question classiﬁcation. 
This paper describes an alignment-based model for interpreting natural language instructions in context. We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment. By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation. To demonstrate the model’s ﬂexibility, we apply it to a diverse set of benchmark tasks. On every task, we outperform strong task-speciﬁc baselines, and achieve several new state-of-the-art results. 
We investigate the need for bigram alignment models and the beneﬁt of supervised alignment techniques in graphemeto-phoneme (G2P) conversion. Moreover, we quantitatively estimate the relationship between alignment quality and overall G2P system performance. We ﬁnd that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task. Moreover, we ﬁnd that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs sufﬁce for them to do so. Finally, we estimate a highly signiﬁcant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature. 
In this paper we propose a framework to improve word segmentation accuracy using input method logs. An input method is software used to type sentences in languages which have far more characters than the number of keys on a keyboard. The main contributions of this paper are: 1) an input method server that proposes word candidates which are not included in the vocabulary, 2) a publicly usable input method that logs user behavior (like typing and selection of word candidates), and 3) a method for improving word segmentation by using these logs. We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain. 
Currently most of state-of-the-art methods for Chinese word segmentation are based on supervised learning, whose features are mostly extracted from a local context. These methods cannot utilize the long distance information which is also crucial for word segmentation. In this paper, we propose a novel neural network model for Chinese word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 
This paper presents a bilingual semisupervised Chinese word segmentation (CWS) method that leverages the natural segmenting information of English sentences. The proposed method involves learning three levels of features, namely, character-level, phrase-level and sentence-level, provided by multiple submodels. We use a sub-model of conditional random ﬁelds (CRF) to learn monolingual grammars, a sub-model based on character-based alignment to obtain explicit segmenting knowledge, and another sub-model based on transliteration similarity to detect out-of-vocabulary (OOV) words. Moreover, we propose a sub-model leveraging neural network to ensure the proper treatment of the semantic gap and a phrase-based translation sub-model to score the translation probability of the Chinese segmentation and its corresponding English sentences. A cascaded log-linear model is employed to combine these features to segment bilingual unlabeled data, the results of which are used to justify the original supervised CWS model. The evaluation shows that our method results in superior results compared with those of the state-of-the-art monolingual and bilingual semi-supervised models that have been reported in the literature. 
In hierarchical phrase-based machine translation, a rule table is automatically learned by heuristically extracting synchronous rules from a parallel corpus. As a result, spuriously many rules are extracted which may be composed of various incorrect rules. The larger rule table incurs more run time for decoding and may result in lower translation quality. To resolve the problems, we propose a hierarchical back-off model for Hiero grammar, an instance of a synchronous context free grammar (SCFG), on the basis of the hierarchical Pitman-Yor process. The model can extract a compact rule and phrase table without resorting to any heuristics by hierarchically backing off to smaller phrases under SCFG. Inference is efﬁciently carried out using two-step synchronous parsing of Xiao et al., (2012) combined with slice sampling. In our experiments, the proposed model achieved higher or at least comparable translation quality against a previous Bayesian model on various language pairs; German/French/Spanish/JapaneseEnglish. When compared against heuristic models, our model achieved comparable translation quality on a full size GermanEnglish language pair in Europarl v7 corpus with signiﬁcantly smaller grammar size; less than 10% of that for heuristic model. 
As conventional word alignment search algorithms usually ignore the consistency constraint in translation rule extraction, improving alignment accuracy does not necessarily increase translation quality. We propose to use coverage, which reﬂects how well extracted phrases can recover the training data, to enable word alignment to model consistency and correlate better with machine translation. This can be done by introducing an objective that maximizes both alignment model score and coverage. We introduce an efﬁcient algorithm to calculate coverage on the ﬂy during search. Experiments show that our consistency-aware search algorithm signiﬁcantly outperforms both generative and discriminative alignment approaches across various languages and translation models. 
Lexical selection is of great importance to statistical machine translation. In this paper, we propose a graph-based framework for collective lexical selection. The framework is established on a translation graph that captures not only local associations between source-side content words and their target translations but also targetside global dependencies in terms of relatedness among target items. We also introduce a random walk style algorithm to collectively identify translations of sourceside content words that are strongly related in translation graph. We validate the effectiveness of our lexical selection framework on Chinese-English translation. Experiment results with large-scale training data show that our approach signiﬁcantly improves lexical selection. 
Learning semantic representations and tree structures of bilingual phrases is beneﬁcial for statistical machine translation. In this paper, we propose a new neural network model called Bilingual Correspondence Recursive Autoencoder (BCorrRAE) to model bilingual phrases in translation. We incorporate word alignments into BCorrRAE to allow it freely access bilingual constraints at different levels. BCorrRAE minimizes a joint objective on the combination of a recursive autoencoder reconstruction error, a structural alignment consistency error and a crosslingual reconstruction error so as to not only generate alignment-consistent phrase structures, but also capture different levels of semantic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 
We present novel models for domain adaptation based on the neural network joint model (NNJM). Our models maximize the cross entropy by regularizing the loss function with respect to in-domain model. Domain adaptation is carried out by assigning higher weight to out-domain sequences that are similar to the in-domain data. In our alternative model we take a more restrictive approach by additionally penalizing sequences similar to the outdomain data. Our models achieve better perplexities than the baseline NNJM models and give improvements of up to 0.5 and 0.6 BLEU points in Arabic-to-English and English-to-German language pairs, on a standard task of translating TED talks. 
The information conveyed by some sentences would be more easily understood by a reader if it were expressed in multiple sentences. We call such sentences content heavy: these are possibly grammatical but difﬁcult to comprehend, cumbersome sentences. In this paper we introduce the task of detecting content-heavy sentences in cross-lingual context. Speciﬁcally we develop methods to identify sentences in Chinese for which English speakers would prefer translations consisting of more than one sentence. We base our analysis and deﬁnitions on evidence from multiple human translations and reader preferences on ﬂow and understandability. We show that machine translation quality when translating content heavy sentences is markedly worse than overall quality and that this type of sentence are fairly common in Chinese news. We demonstrate that sentence length and punctuation usage in Chinese are not sufﬁcient clues for accurately detecting heavy sentences and present a richer classiﬁcation model that accurately identiﬁes these sentences. 
Parameter tuning is a key problem for statistical machine translation (SMT). Most popular parameter tuning algorithms for SMT are agnostic of decoding, resulting in parameters vulnerable to search errors in decoding. The recent research of “search-aware tuning” (Liu and Huang, 2014) addresses this problem by considering the partial derivations in every decoding step so that the promising ones are more likely to survive the inexact decoding beam. We extend this approach from phrase-based translation to syntaxbased translation by generalizing the evaluation metrics for partial translations to handle tree-structured derivations in a way inspired by inside-outside algorithm. Our approach is simple to use and can be applied to most of the conventional parameter tuning methods as a plugin. Extensive experiments on Chinese-to-English translation show signiﬁcant BLEU improvements on MERT, MIRA and PRO. 
In this paper, we address the challenge of creating accurate and robust partof-speech taggers for low-resource languages. We propose a method that leverages existing parallel data between the target language and a large set of resourcerich languages without ancillary resources such as tag dictionaries. Crucially, we use CCA to induce latent word representations that incorporate cross-genre distributional cues, as well as projected tags from a full array of resource-rich languages. We develop a probability-based conﬁdence model to identify words with highly likely tag projections and use these words to train a multi-class SVM using the CCA features. Our method yields average performance of 85% accuracy for languages with almost no resources, outperforming a state-of-the-art partiallyobserved CRF model. 
Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhi’s tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed (Moore, 2014) makes tagging as fast as with Ratnaparkhi’s tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhi’s method results in a much tighter tag dictionary than either Ratnaparkhi’s or our previous method, with accuracy as high as with our previous tag dictionary but much faster tagging—more than 100,000 tokens per second in Perl. 
We present an approach to Arabic automatic diacritization that integrates syntactic analysis with morphological tagging through improving the prediction of case and state features. Our best system increases the accuracy of word diacritization by 2.5% absolute on all words, and 5.2% absolute on nominals over a state-of-theart baseline. Similar increases are shown on the full morphological analysis choice. 
We investigate a combination of a traditional linear sparse feature model and a multi-layer neural network model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods.  ···  ···  (a) discrete linear  (b) continuous NN  (eg. MaltParser) (eg. Chen and Manning (2014))  ···  ···  (c) Turian et al. (2010)  ···  ···  ··· transform  (d) Guo et al. (2014)  
Many NLP systems use dependency parsers as critical components. Jonit learning parsers usually achieve better parsing accuracies than two-stage methods. However, classical joint parsing algorithms signiﬁcantly increase computational complexity, which makes joint learning impractical. In this paper, we proposed an efﬁcient dependency parsing algorithm that is capable of capturing multiple edge-label features, while maintaining low computational complexity. We evaluate our parser on 14 different languages. Our parser consistently obtains more accurate results than three baseline systems and three popular, off-the-shelf parsers. 
We propose online unsupervised domain adaptation (DA), which is performed incrementally as data comes in and is applicable when batch DA is not possible. In a part-of-speech (POS) tagging evaluation, we ﬁnd that online unsupervised DA performs as well as batch DA. 
We describe an approach for machine learning-based empty category detection that is based on the phrase structure analysis of Japanese. The problem is formalized as tree node classiﬁcation, and we ﬁnd that the path feature, the sequence of node labels from the current node to the root, is highly effective. We also ﬁnd that the set of dot products between the word embeddings for a verb and those for case particles can be used as a substitution for case frames. Experiments show that the proposed method outperforms the previous state-of the art method by 68.6% to 73.2% in terms of F-measure. 
We present a new treebank of English and French technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse. 
We present a semi-supervised approach to improve dependency parsing accuracy by using bilexical statistics derived from auto-parsed data. The method is based on estimating the attachment potential of head-modiﬁer words, by taking into account not only the head and modiﬁer words themselves, but also the words surrounding the head and the modiﬁer. When integrating the learned statistics as features in a graph-based parsing model, we observe nice improvements in accuracy when parsing various English datasets. 
We extend and improve upon recent work in structured training for neural network transition-based dependency parsing. We do this by experimenting with novel features, additional transition systems and by testing on a wider array of languages. In particular, we introduce set-valued features to encode the predicted morphological properties and part-ofspeech confusion sets of the words being parsed. We also investigate the use of joint parsing and partof-speech tagging in the neural paradigm. Finally, we conduct a multi-lingual evaluation that demonstrates the robustness of the overall structured neural approach, as well as the beneﬁts of the extensions proposed in this work. Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing, as well as the ease with which new features can be added to neural parsing models. 
Model combination techniques have consistently shown state-of-the-art performance across multiple tasks, including syntactic parsing. However, they dramatically increase runtime and can be difﬁcult to employ in practice. We demonstrate that applying constituency model combination techniques to n-best lists instead of n different parsers results in signiﬁcant parsing accuracy improvements. Parses are weighted by their probabilities and combined using an adapted version of Sagae and Lavie (2006). These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically conﬁrm that six well-known n-best parsers beneﬁt from the proposed methods across six domains. 
We introduce an extension to the bag-ofwords model for learning words representations that take into account both syntactic and semantic properties within language. This is done by employing an attention model that ﬁnds within the contextual words, the words that are relevant for each prediction. The general intuition of our model is that some words are only relevant for predicting local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 
Transition-based dependency parsers usually use transition systems that monotonically extend partial parse states until they identify a complete parse tree. Honnibal et al. (2013) showed that greedy onebest parsing accuracy can be improved by adding additional non-monotonic transitions that permit the parser to “repair” earlier parsing mistakes by “over-writing” earlier parsing decisions. This increases the size of the set of complete parse trees that each partial parse state can derive, enabling such a parser to escape the “garden paths” that can trap monotonic greedy transition-based dependency parsers. We describe a new set of non-monotonic transitions that permits a partial parse state to derive a larger set of completed parse trees than previous work, which allows our parser to escape from a larger set of garden paths. A parser with our new nonmonotonic transition system has 91.85% directed attachment accuracy, an improvement of 0.6% over a comparable parser using the standard monotonic arc-eager transitions. 
The multilingual Paraphrase Database (PPDB) is a freely available automatically created resource of paraphrases in multiple languages. In statistical machine translation, paraphrases can be used to provide translation for out-of-vocabulary (OOV) phrases. In this paper, we show that a graph propagation approach that uses PPDB paraphrases can be used to improve overall translation quality. We provide an extensive comparison with previous work and show that our PPDB-based method improves the BLEU score by up to 1.79 percent points. We show that our approach improves on the state of the art in three different settings: when faced with limited amount of parallel training data; a domain shift between training and test data; and handling a morphologically complex source language. Our PPDB-based method outperforms the use of distributional proﬁles from monolingual source data. 
In hierarchical phrase-based translation, coarse-grained nonterminal Xs may generate inappropriate translations due to the lack of sufﬁcient information for phrasal substitution. In this paper we propose a framework to reﬁne nonterminals in hierarchical translation rules with real-valued semantic representations. The semantic representations are learned via a weighted mean value and a minimum distance method using phrase vector representations obtained from large scale monolingual corpus. Based on the learned semantic vectors, we build a semantic nonterminal reﬁnement model to measure semantic similarities between phrasal substitutions and nonterminal Xs in translation rules. Experiment results on ChineseEnglish translation show that the proposed model signiﬁcantly improves translation quality on NIST test sets. 
We propose a conversion of bilingual sentence pairs and the corresponding word alignments into novel linear sequences. These are joint translation and reordering (JTR) uniquely deﬁned sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modiﬁed Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 
An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a signiﬁcant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.1 
Document level sentiment classiﬁcation remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document. To address this, we introduce a neural network model to learn vector-based document representation in a uniﬁed, bottom-up fashion. The model ﬁrst learns sentence representation with convolutional neural network or long short-term memory. Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classiﬁcation on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classiﬁcation.1 
Joint models of syntactic and semantic parsing have the potential to improve  ARG0  ARG0 ARG0 ARG1  ARG1  ARG1  performance on both tasks—but to date, the best results have been achieved with pipelines. We introduce a joint model us-  He refused to conﬁrm or deny the reports  nsubj  mark  cc conj  det  xcomp  dobj  ing CCG, which is motivated by the close link between CCG syntax and semantics.  Figure 1: Mismatch between syntactic and semantic dependencies.  Semantic roles are recovered by labelling  the deep dependency structures produced by the grammar. Furthermore, because CCG is lexicalized, we show it is possible  He  refused  to  conﬁrm  or  deny  reports  NP (S \NP)/(S \NP) S /S (S \NP)/NP conj (S \NP)/NP NP  ∅  ARG0  ARG1  ARG1  to factor the parsing model over words and  ARG0  ARG1  introduce a new A∗ parsing algorithm— which we demonstrate is faster and more accurate than adaptive supertagging. Our joint model is the ﬁrst to substantially improve both syntactic and semantic accuracy over a comparable pipeline, and also  ARG0 Figure 2: Dependencies produced by a CCG parse. SRL dependencies can be recovered by labelling the edges with a semantic role or ∅. Figure 3 shows a CCG derivation for these dependencies.  achieves state-of-the-art results for a non-  models. For example, in the Figure 1, the se-  ensemble semantic role labelling model.  mantic dependency between He and deny spans  
Semantic parsing maps a sentence in natural language into a structured meaning representation. Previous studies show that semantic parsing with synchronous contextfree grammars (SCFGs) achieves favorable performance over most other alternatives. Motivated by the observation that the performance of semantic parsing with SCFGs is closely tied to the translation rules, this paper explores extending translation rules with high quality and increased coverage in three ways. First, we introduce structure informed non-terminals, better guiding the parsing in favor of well formed structure, instead of using a uninformed non-terminal in SCFGs. Second, we examine the difference between word alignments for semantic parsing and statistical machine translation (SMT) to better adapt word alignment in SMT to semantic parsing. Finally, we address the unknown word translation issue via synthetic translation rules. Evaluation on the standard GeoQuery benchmark dataset shows that our approach achieves the state-of-the-art across various languages, including English, German and Greek. 
This paper introduces GEOS, the ﬁrst automated system to solve unaltered SAT geometry questions by combining text understanding and diagram interpretation. We model the problem of understanding geometry questions as submodular optimization, and identify a formal problem description likely to be compatible with both the question text and diagram. GEOS then feeds the description to a geometric solver that attempts to determine the correct answer. In our experiments, GEOS achieves a 49% score on ofﬁcial SAT questions, and a score of 61% on practice questions.1 Finally, we show that by integrating textual and visual information, GEOS boosts the accuracy of dependency and semantic parsing of the question text. 
Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a uniﬁed fashion across the different ambiguity types. 
We explore some of the practicalities of using random walk inference methods, such as the Path Ranking Algorithm (PRA), for the task of knowledge base completion. We show that the random walk probabilities computed (at great expense) by PRA provide no discernible beneﬁt to performance on this task, so they can safely be dropped. This allows us to deﬁne a simpler algorithm for generating feature matrices from graphs, which we call subgraph feature extraction (SFE). In addition to being conceptually simpler than PRA, SFE is much more efﬁcient, reducing computation by an order of magnitude, and more expressive, allowing for much richer features than paths between two nodes in a graph. We show experimentally that this technique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB. 
Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013). In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations. The proposed model signiﬁcantly improves performance over a model that does not share parameters among textual relations with common sub-structure. 
Authoring a scientiﬁc paper is a complex process involving many decisions. We introduce a probabilistic model of some of the important aspects of that process: that authors have individual preferences, that writing a paper requires trading off among the preferences of authors as well as extrinsic rewards in the form of community response to their papers, that preferences (of individuals and the community) and tradeoffs vary over time. Variants of our model lead to improved predictive accuracy of citations given texts and texts given authors. Further, our model’s posterior suggests an interesting relationship between seniority and author choices. 
We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a ﬁxed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form–function relationship in language, our “composed” word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Beneﬁts over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish). 
Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned against a generic objective that enhances the vectors with syntactic information from the surrounding context. Furthermore, each word is associated with a number of senses, the most plausible of which is selected dynamically during the composition process. We evaluate the produced vectors qualitatively and quantitatively with positive results. At the sentence level, the effectiveness of the framework is demonstrated on the MSRPar task, for which we report results within the state-of-the-art range. 
Online forum discussions proceed differently from face-to-face conversations and any single thread on an online forum contains posts on different subtopics. This work aims to characterize the content of a forum thread as a conversation tree of topics. We present models that jointly perform two tasks: segment a thread into subparts, and assign a topic to each part. Our core idea is a deﬁnition of topic structure using probabilistic grammars. By leveraging the ﬂexibility of two grammar formalisms, Context-Free Grammars and Linear Context-Free Rewriting Systems, our models create desirable structures for forum threads: our topic segmentation is hierarchical, links non-adjacent segments on the same topic, and jointly labels the topic during segmentation. We show that our models outperform a number of tree generation baselines. 
Weak topic correlation across document collections with different numbers of topics in individual collections presents challenges for existing cross-collection topic models. This paper introduces two probabilistic topic models, Correlated LDA (C-LDA) and Correlated HDP (CHDP). These address problems that can arise when analyzing large, asymmetric, and potentially weakly-related collections. Topic correlations in weakly-related collections typically lie in the tail of the topic distribution, where they would be overlooked by models unable to ﬁt large numbers of topics. To efﬁciently model this long tail for large-scale analysis, our models implement a parallel sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et al., 2015). The models are ﬁrst evaluated on synthetic data, generated to simulate various collection-level asymmetries. We then present a case study of modeling over 300k documents in collections of sciences and humanities research from JSTOR. 
The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of lowrank tensors, and pattern weighting, we can efﬁciently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classiﬁcation and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.2% accuracy on the ﬁne-grained sentiment classiﬁcation task.1 
Many models in natural language processing deﬁne probabilistic distributions over linguistic structures. We argue that (1) the quality of a model’s posterior distribution can and should be directly evaluated, as to whether probabilities correspond to empirical frequencies; and (2) NLP uncertainty can be projected not only to pipeline components, but also to exploratory data analysis, telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration, and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create conﬁdence intervals for a political event extraction task.1 
Most existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)based methods. However some models are opaque to probabilistic interpretation, and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information. In addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model. Since generative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models. The model inference reduces to a low rank weighted positive semideﬁnite approximation problem. Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD. In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods. 
Modeling non-stationary time-series data for making predictions is a challenging but important task. One of the key issues is to identify long-term changes accurately in time-varying data. Bayesian Online Change Point Detection (BO-CPD) algorithms efﬁciently detect long-term changes without assuming the Markov property which is vulnerable to local signal noise. We propose a Document based BO-CPD (DBO-CPD) model which automatically detects long-term temporal changes of continuous variables based on a novel dynamic Bayesian analysis which combines a non-parametric regression, the Gaussian Process (GP), with generative models of texts such as news articles and posts on social networks. Since texts often include important clues of signal changes, DBO-CPD enables the accurate prediction of long-term changes accurately. We show that our algorithm outperforms existing BO-CPDs in two real-world datasets: stock prices and movie revenues. 
Recognizing Text Entailment (RTE) plays an important role in NLP applications including question answering, information retrieval, etc. In recent work, some research explore “deep” expressions such as discourse commitments or strict logic for representing the text. However, these expressions suffer from the limitation of inference inconvenience or translation loss. To overcome the limitations, in this paper, we propose to use the predicate-argument structures to represent the discourse commitments extracted from text. At the same time, with the help of the YAGO knowledge, we borrow the distant supervision technique to mine the implicit facts from the text. We also construct a probabilistic network for all the facts and conduct inference to judge the conﬁdence of each fact for RTE. The experimental results show that our proposed method achieves a competitive result compared to the previous work. 
 Traditional approaches to Chinese Semantic Role Labeling (SRL) almost heavily rely on feature engineering. Even worse, the long-range dependencies in a sentence can hardly be modeled by these methods. In this paper, we introduce bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to capture bidirectional and long-range dependencies in a sentence with minimal feature engineering. Experimental results on Chinese Proposition Bank (CPB) show a signiﬁcant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 
In this paper we analyze the performance of different composition models on a large dataset of German compound nouns. Given a vector space model for the German language, we try to reconstruct the observed representation (the corpusestimated vector) of a compound by composing the observed representations of its two immediate constituents. We explore the composition models proposed in the literature and also present a new, simple model that achieves the best performance on our dataset. 
Events are communicated in natural language with varying degrees of certainty. For example, if you are “hoping for a raise,” it may be somewhat less likely than if you are “expecting” one. To study these distinctions, we present scalable, highquality annotation schemes for event detection and ﬁne-grained factuality assessment. We ﬁnd that non-experts, with very little training, can reliably provide judgments about what events are mentioned and the extent to which the author thinks they actually happened. We also show how such data enables the development of regression models for ﬁne-grained scalar factuality predictions that outperform strong baselines. 
We propose a novel method for acquiring entailment pairs of binary patterns on a large-scale. This method exploits the transitivity of entailment and a self-training scheme to improve the performance of an already strong supervised classiﬁer for entailment, and unlike previous methods that exploit transitivity, it works on a largescale. With it we acquired 138.1 million pattern pairs with 70% precision with such non-trivial lexical substitution as “use Y to distribute X”→“X is available on Y” whose extraction is considered difﬁcult. This represents 50.4 million more pattern pairs (a 57.5% increase) than what our supervised baseline extracted at the same precision. 
We consider the problem of embedding knowledge graphs (KGs) into continuous vector spaces. Existing methods can only deal with explicit relationships within each triple, i.e., local connectivity patterns, but cannot handle implicit relationships across different triples, i.e., contextual connectivity patterns. This paper proposes context-dependent KG embedding, a twostage scheme that takes into account both types of connectivity patterns and obtains more accurate embeddings. We evaluate our approach on the tasks of link prediction and triple classiﬁcation, and achieve signiﬁcant and consistent improvements over state-of-the-art methods. 
A prerequisite relation describes a basic relation among concepts in cognition, education and other areas. However, as a semantic relation, it has not been well studied in computational linguistics. We investigate the problem of measuring prerequisite relations among concepts and propose a simple link-based metric, namely reference distance (RefD), that effectively models the relation by measuring how differently two concepts refer to each other. Evaluations on two datasets that include seven domains show that our single metric based method outperforms existing supervised learning based methods. 
Previous studies have shown that health reports in social media, such as DailyStrength and Twitter, have potential for monitoring health conditions (e.g. adverse drug reactions, infectious diseases) in particular communities. However, in order for a machine to understand and make inferences on these health conditions, the ability to recognise when laymen’s terms refer to a particular medical concept (i.e. text normalisation) is required. To achieve this, we propose to adapt an existing phrase-based machine translation (MT) technique and a vector representation of words to map between a social media phrase and a medical concept. We evaluate our proposed approach using a collection of phrases from tweets related to adverse drug reactions. Our experimental results show that the combination of a phrase-based MT technique and the similarity between word vector representations outperforms the baselines that apply only either of them by up to 55%. 
The narrative cloze is an evaluation metric commonly used for work on automatic script induction. While prior work in this area has focused on count-based methods from distributional semantics, such as pointwise mutual information, we argue that the narrative cloze can be productively reframed as a language modeling task. By training a discriminative language model for this task, we attain improvements of up to 27 percent over prior methods on standard narrative cloze metrics. 
Word embeddings encode semantic meanings of words into low-dimension word vectors. In most word embeddings, one cannot interpret the meanings of speciﬁc dimensions of those word vectors. Nonnegative matrix factorization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose online learning of interpretable word embeddings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http: //github.com/skTim/OIWE. 
Machine comprehension of text is the overarching goal of a great deal of research in natural language processing. The Machine Comprehension Test (Richardson et al., 2013) was recently proposed to assess methods on an open-domain, extensible, and easy-to-evaluate task consisting of two datasets. In this paper we develop a lexical matching method that takes into account multiple context windows, question types and coreference resolution. We show that the proposed method outperforms the baseline of Richardson et al. (2013), and despite its relative simplicity, is comparable to recent work using machine learning. We hope that our approach will inform future work on this task. Furthermore, we argue that MC500 is harder than MC160 due to the way question answer pairs were created. 
We propose a grammar induction technique for AMR semantic parsing. While previous grammar induction techniques were designed to re-learn a new parser for each target application, the recently annotated AMR Bank provides a unique opportunity to induce a single model for understanding broad-coverage newswire text and support a wide range of applications. We present a new model that combines CCG parsing to recover compositional aspects of meaning and a factor graph to model non-compositional phenomena, such as anaphoric dependencies. Our approach achieves 66.2 Smatch F1 score on the AMR bank, signiﬁcantly outperforming the previous state of the art. 
Natural language generation (NLG) is a critical component of spoken dialogue and it has a signiﬁcant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems. 
Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and ﬁne-grained models of vector-space representations. Yet while ‘multi-sense’ methods have been proposed and tested on artiﬁcial wordsimilarity tasks, we don’t know if they improve real natural language understanding tasks. In this paper we introduce a multisense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language understanding. We then test the performance of our model on part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identiﬁcation and semantic relatedness, controlling for embedding dimensionality. We ﬁnd that multi-sense embeddings do improve performance on some tasks (part-of-speech tagging, semantic relation identiﬁcation, semantic relatedness) but not on others (named entity recognition, various forms of sentiment analysis). We discuss how these differences may be caused by the different role of word sense information in each of the tasks. The results highlight the importance of testing embedding models in real applications. 
Non-compositionality of multiword expressions is an intriguing problem that can be the source of error in a variety of NLP tasks such as language generation, machine translation and word sense disambiguation. We present methods of non-compositionality detection for English noun compounds using the unsupervised learning of a semantic composition function. Compounds which are not well modeled by the learned semantic composition function are considered noncompositional. We explore a range of distributional vector-space models for semantic composition, empirically evaluate these models, and propose additional methods which improve results further. We show that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex. We show that enforcing sparsity is a useful regularizer in learning complex composition functions. We show further improvements by training a decomposition function in addition to the composition function. Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance. 
This paper presents a novel approach to automatically solving arithmetic word problems. This is the ﬁrst algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predeﬁned templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classiﬁcation problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classiﬁers gain from the use of quantity schemas that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems. 
Two problems arise when using distant supervision for relation extraction. First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data. However, the heuristic alignment can fail, resulting in wrong label problem. In addition, in previous approaches, statistical models have typically been applied to ad hoc features. The noise that originates from the feature extraction process can cause poor performance. In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the ﬁrst problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods. 
We propose CORE, a novel matrix factorization model that leverages contextual information for open relation extraction. Our model is based on factorization machines and integrates facts from various sources, such as knowledge bases or open information extractors, as well as the context in which these facts have been observed. We argue that integrating contextual information—such as metadata about extraction sources, lexical context, or type information—signiﬁcantly improves prediction performance. Open information extractors, for example, may produce extractions that are unspeciﬁc or ambiguous when taken out of context. Our experimental study on a large real-world dataset indicates that CORE has signiﬁcantly better prediction performance than state-ofthe-art approaches when contextual information is available. 
A key challenge in vocabulary acquisition is learning which of the many possible meanings is appropriate for a word. The word generalization problem refers to how children associate a word such as dog with a meaning at the appropriate category level in a taxonomy of objects, such as Dalmatians, dogs, or animals. We present the ﬁrst computational study of word generalization integrated within a word-learning model. The model simulates child and adult patterns of word generalization in a word-learning task. These patterns arise due to the interaction of type and token frequencies in the input data, an inﬂuence often observed in people’s generalization of linguistic categories. 
This study focuses on personality prediction of protagonists in novels based on the Five-Factor Model of personality. We present and publish a novel collaboratively built dataset of ﬁctional character personality and design our task as a text classiﬁcation problem. We incorporate a range of semantic features, including WordNet and VerbNet sense-level information and word vector representations. We evaluate three machine learning models based on the speech, actions and predicatives of the main characters, and show that especially the lexical-semantic features signiﬁcantly outperform the baselines. The most predictive features correspond to reported ﬁndings in personality psychology. 
Expectation-maximization algorithms, such as those implemented in GIZA++ pervade the ﬁeld of unsupervised word alignment. However, these algorithms have a problem of over-ﬁtting, leading to “garbage collector effects,” where rare words tend to be erroneously aligned to untranslated words. This paper proposes a leave-one-out expectationmaximization algorithm for unsupervised word alignment to address this problem. The proposed method excludes information derived from the alignment of a sentence pair from the alignment models used to align it. This prevents erroneous alignments within a sentence pair from supporting themselves. Experimental results on Chinese-English and Japanese-English corpora show that the F1, precision and recall of alignment were consistently increased by 5.0% – 17.2%, and BLEU scores of end-to-end translation were raised by 0.03 – 1.30. The proposed method also outperformed l0-normalized GIZA++ and Kneser-Ney smoothed GIZA++. 
While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be deﬁned between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves signiﬁcant improvements over two state-ofthe-art alignment methods. 
We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach. 
We provide a generalization of discriminative lexicalized shift reduce parsing techniques for phrase structure grammar to a wide range of morphologically rich languages. The model is efﬁcient and outperforms recent strong baselines on almost all languages considered. It takes advantage of a dependency based modelling of morphology and a shallow modelling of constituency boundaries. 
Accurate multilingual transfer parsing typically relies on careful feature engineering. In this paper, we propose a hierarchical tensor-based approach for this task. This approach induces a compact feature representation by combining atomic features. However, unlike traditional tensor models, it enables us to incorporate prior knowledge about desired feature interactions, eliminating invalid feature combinations. To this end, we use a hierarchical structure that uses intermediate embeddings to capture desired feature combinations. Algebraically, this hierarchical tensor is equivalent to the sum of traditional tensors with shared components, and thus can be effectively trained with standard online algorithms. In both unsupervised and semi-supervised transfer scenarios, our hierarchical tensor consistently improves UAS and LAS over state-of-theart multilingual transfer parsers and the traditional tensor model across 10 different languages.1 
We describe an approach to create a diverse set of predictions with spectral learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating multiple spectral models where noise is added to the underlying features in the training set before the estimation of each model. We describe three ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a signiﬁcant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 
 Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However, it is still a challenge problem to sufﬁciently model the complicated syntactic and semantic compositions of the dense features in neural network based methods. In this paper, we propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN). Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing. Speciﬁcally, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model. 
In this paper, a turn-taking phenomenon taxonomy is introduced, organised according to the level of information conveyed. It is aimed to provide a better grasp of the behaviours used by humans while talking to each other, so that they can be methodically replicated in spoken dialogue systems. Five interesting phenomena have been implemented in a simulated environment: the system barge-in with three variants (resulting from either an unclear, an incoherent or a sufﬁcient user message), the feedback and the user barge-in. The experiments reported in the paper illustrate that how such phenomena are implemented is a delicate choice as their impact on the system’s performance is variable. 
This paper focuses on language modeling with adequate robustness to support different domain tasks. To this end, we propose a hierarchical latent word language model (h-LWLM). The proposed model can be regarded as a generalized form of the standard LWLMs. The key advance is introducing a multiple latent variable space with hierarchical structure. The structure can ﬂexibly take account of linguistic phenomena not present in the training data. This paper details the deﬁnition as well as a training method based on layer-wise inference and a practical usage in natural language processing tasks with an approximation technique. Experiments on speech recognition show the effectiveness of hLWLM in out-of domain tasks. 
Speech translation is conventionally carried out by cascading an automatic speech recognition (ASR) and a statistical machine translation (SMT) system. The hypotheses chosen for translation are based on the ASR system’s acoustic and language model scores, and typically optimized for word error rate, ignoring the intended downstream use: automatic translation. In this paper, we present a coarseto-ﬁne model that uses features from the ASR and SMT systems to optimize this coupling. We demonstrate that several standard features utilized by ASR and SMT systems can be used in such a model at the speech-translation interface, and we provide empirical results on the Fisher Spanish-English speech translation corpus. 
This paper proposes a novel approach to generate abstractive summary for multiple documents by extracting semantic information from texts. The concept of Basic Semantic Unit (BSU) is defined to describe the semantics of an event or action. A semantic link network on BSUs is constructed to capture the semantic information of texts. Summary structure is planned with sentences generated based on the semantic link network. Experiments demonstrate that the approach is effective in generating informative, coherent and compact summary. 
This paper demonstrates the effectiveness of a Long Short-Term Memory language model in our initial efforts to generate unconstrained rap lyrics. The goal of this model is to generate lyrics that are similar in style to that of a given rapper, but not identical to existing lyrics: this is the task of ghostwriting. Unlike previous work, which deﬁnes explicit templates for lyric generation, our model deﬁnes its own rhyme scheme, line length, and verse length. Our experiments show that a Long Short-Term Memory language model produces better “ghostwritten” lyrics than a baseline model. 
ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Speciﬁcally, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefﬁcients. 
Automated text summarization is aimed at extracting essential information from original text and presenting it in a minimal, often predeﬁned, number of words. In this paper, we introduce a new approach for unsupervised extractive summarization, based on the Minimum Description Length (MDL) principle, using the Krimp dataset compression algorithm (Vreeken et al., 2011). Our approach represents a text as a transactional dataset, with sentences as transactions, and then describes it by itemsets that stand for frequent sequences of words. The summary is then compiled from sentences that compress (and as such, best describe) the document. The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document. We solve it by a greedy algorithm and present the evaluation results. 
Predicting the success of referring expressions (RE) is vital for real-world applications such as navigation systems. Traditionally, research has focused on studying Referring Expression Generation (REG) in virtual, controlled environments. In this paper, we describe a novel study of spatial references from real scenes rather than virtual. First, we investigate how humans describe objects in open, uncontrolled scenarios and compare our ﬁndings to those reported in virtual environments. We show that REs in real-world scenarios differ signiﬁcantly to those in virtual worlds. Second, we propose a novel approach to quantifying image complexity when complete annotations are not present (e.g. due to poor object recognition capabitlities), and third, we present a model for success prediction of REs for objects in real scenes. Finally, we discuss implications for Natural Language Generation (NLG) systems and future directions. 
Storyline detection from news articles aims at summarizing events described under a certain news topic and revealing how those events evolve over time. It is a difﬁcult task because it requires ﬁrst the detection of events from news articles published in different time periods and then the construction of storylines by linking events into coherent news stories. Moreover, each storyline has different hierarchical structures which are dependent across epochs. Existing approaches often ignore the dependency of hierarchical structures in storyline generation. In this paper, we propose an unsupervised Bayesian model, called dynamic storyline detection model, to extract structured representations and evolution patterns of storylines. The proposed model is evaluated on a large scale news corpus. Experimental results show that our proposed model outperforms several baseline approaches. 
We present an approach for extractive single-document summarization. Our approach is based on a weighted graphical representation of documents obtained by topic modeling. We optimize importance, coherence and non-redundancy simultaneously using ILP. We compare ROUGE scores of our system with state-of-the-art results on scientiﬁc articles from PLOS Medicine and on DUC 2002 data. Human judges evaluate the coherence of summaries generated by our system in comparision to two baselines. Our approach obtains competitive performance. 
We propose to automatically summarize student responses to reﬂection prompts and introduce a novel summarization algorithm that differs from traditional methods in several ways. First, since the linguistic units of student inputs range from single words to multiple sentences, our summaries are created from extracted phrases rather than from sentences. Second, the phrase summarization algorithm ranks the phrases by the number of students who semantically mention a phrase in a summary. Experimental results show that the proposed phrase summarization approach achieves signiﬁcantly better summarization performance on an engineering course corpus in terms of ROUGE scores when compared to other summarization methods, including MEAD, LexRank and MMR. 
The most successful approaches to extractive text summarization seek to maximize bigram coverage subject to a budget constraint. In this work, we propose instead to maximize semantic volume. We embed each sentence in a semantic space and construct a summary by choosing a subset of sentences whose convex hull maximizes volume in that space. We provide a greedy algorithm based on the GramSchmidt process to efﬁciently perform volume maximization. Our method outperforms the state-of-the-art summarization approaches on benchmark datasets. 
Automatic text summarization is widely regarded as the highly difﬁcult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public1. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic. 
While it has been established that transitions between discourse relations are important for coherence, such information has not so far been used to aid in language generation. We introduce an approach to discourse planning for conceptto-text generation systems which simultaneously determines the order of messages and the discourse relations between them. This approach makes it straightforward to use statistical transition models, such as n-gram models of discourse relations learned from an annotated corpus. We show that using such a model significantly improves the quality of the generated text as judged by humans. 
We present experiments with generative models for linearization of unordered labeled syntactic dependency trees (Belz et al., 2011; Rajkumar and White, 2014). Our linearization models are derived from generative models for dependency structure (Eisner, 1996). We present a series of generative dependency models designed to capture successively more information about ordering constraints among sister dependents. We give a dynamic programming algorithm for computing the conditional probability of word orders given tree structures under these models. The models are tested on corpora of 11 languages using test-set likelihood, and human ratings for generated forms are collected for English. Our models beneﬁt from representing local order constraints among sisters and from backing off to less sparse distributions, including distributions not conditioned on the head. 
In this study, we consider a summarization method using the document level similarity based on embeddings, or distributed representations of words, where we assume that an embedding of each word can represent its “meaning.” We formalize our task as the problem of maximizing a submodular function deﬁned by the negative summation of the nearest neighbors’ distances on embedding distributions, each of which represents a set of word embeddings in a document. We proved the submodularity of our objective function and that our problem is asymptotically related to the KL-divergence between the probability density functions that correspond to a document and its summary in a continuous space. An experiment using a real dataset demonstrated that our method performed better than the existing method based on sentence-level similarity. 
We restate the classical logical notion of generation/parsing reversibility in terms of feasible probabilistic sampling, and argue for an implementation based on ﬁnite-state factors. We propose a modular decomposition that reconciles generation accuracy with parsing robustness and allows the introduction of dynamic contextual factors. (Opinion Piece) 
Direct content analysis reveals important details about movies including those of gender representations and potential biases. We investigate the differences between male and female character depictions in movies, based on patterns of language used. Speciﬁcally, we use an automatically generated lexicon of linguistic norms characterizing gender ladenness. We use multivariate analysis to investigate gender depictions and correlate them with elements of movie production. The proposed metric differentiates between male and female utterances and exhibits some interesting interactions with movie genres and the screenplay writer gender. 
The conferences ACL (Association for Computational Linguistics) and EMNLP (Empirical Methods in Natural Language Processing) rank among the premier venues that track the research developments in Natural Language Processing and Computational Linguistics. In this paper, we present a study on the research papers of approximately two decades from these two NLP conferences. We apply keyphrase extraction and corpus analysis tools to the proceedings from these venues and propose probabilistic and vector-based representations to represent the topics published in a venue for a given year. Next, similarity metrics are studied over pairs of venue representations to capture the progress of the two venues with respect to each other and over time. 
Much of what we understand from text is not explicitly stated. Rather, the reader uses his/her knowledge to ﬁll in gaps and create a coherent, mental picture or “scene” depicting what text appears to convey. The scene constitutes an understanding of the text, and can be used to answer questions that go beyond the text. Our goal is to answer elementary science questions, where this requirement is pervasive; A question will often give a partial description of a scene and ask the student about implicit information. We show that by using a simple “knowledge graph” representation of the question, we can leverage several large-scale linguistic resources to provide missing background knowledge, somewhat alleviating the knowledge bottleneck in previous approaches. The coherence of the best resulting scene, built from a question/answer-candidate pair, reﬂects the conﬁdence that the answer candidate is correct, and thus can be used to answer multiple choice questions. Our experiments show that this approach outperforms competitive algorithms on several datasets tested. The signiﬁcance of this work is thus to show that a simple “knowledge graph” representation allows a version of “interpretation as scene construction” to be made viable. 
We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question. WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset. In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset. 
Machine Translation (MT) has advanced in recent years to produce better translations for clients’ speciﬁc domains, and sophisticated tools allow professional translators to obtain translations according to their prior edits. We suggest that MT should be further personalized to the end-user level – the receiver or the author of the text – as done in other applications. As a step in that direction, we propose a method based on a recommender systems approach where the user’s preferred translation is predicted based on preferences of similar users. In our experiments, this method outperforms a set of non-personalized methods, suggesting that user preference information can be employed to provide better-suited translations for each user. 
This paper addresses the question of how language use affects community reaction to comments in online discussion forums, and the relative importance of the message vs. the messenger. A new comment ranking task is proposed based on community annotated karma in Reddit discussions, which controls for topic and timing of comments. Experimental work with discussion threads from six subreddits shows that the importance of different types of language features varies with the community of interest. 
Usernames are ubiquitous on the Internet, and they are often suggestive of user demographics. This work looks at the degree to which gender and language can be inferred from a username alone by making use of unsupervised morphology induction to decompose usernames into sub-units. Experimental results on the two tasks demonstrate the effectiveness of the proposed morphological features compared to a character n-gram baseline. 
 Large-scale Knowledge Bases (such as NELL, Yago, Freebase, etc.) are often sparse, i.e., a large number of valid relations between existing entities are missing. Recent research have addressed this problem by augmenting the KB graph with additional edges mined from a large text corpus while keeping the set of nodes ﬁxed, and then using the Path Ranking Algorithm (PRA) to perform KB inference over this augmented graph. In this paper, we extend this line of work by augmenting the KB graph not only with edges, but also with bridging entities, where both the edges and bridging entities are mined from a 500 million web text corpus. Through experiments on real-world datasets, we demonstrate the value of bridging entities in improving the performance and running time of PRA in the KB inference task. 
We demonstrate the advantage of specializing semantic word embeddings for either similarity or relatedness. We compare two variants of retroﬁtting and a joint-learning approach, and ﬁnd that all three yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classiﬁcation and synonym selection, which rely on either similarity or relatedness but not both. 
Unsupervisedly learned word vectors have proven to provide exceptionally effective features in many NLP tasks. Most common intrinsic evaluations of vector quality measure correlation with similarity judgments. However, these often correlate poorly with how well the learned representations perform as features in downstream evaluation tasks. We present QVEC—a computationally inexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 
We present a higher-order inference system based on a formal compositional semantics and the wide-coverage CCG parser. We develop an improved method to bridge between the parser and semantic composition. The system is evaluated on the FraCaS test suite. In contrast to the widely held view that higher-order logic is unsuitable for efﬁcient logical inferences, the results show that a system based on a reasonably-sized semantic lexicon and a manageable number of non-ﬁrst-order axioms enables efﬁcient logical inferences, including those concerned with generalized quantiﬁers and intensional operators, and outperforms the state-of-the-art ﬁrstorder inference system. 
We present a multilingual corpus of Wikipedia and Twitter texts annotated with FRAMENET 1.5 semantic frames in nine different languages, as well as a novel technique for weakly supervised cross-lingual frame-semantic parsing. Our approach only assumes the existence of linked, comparable source and target language corpora (e.g., Wikipedia) and a bilingual dictionary (e.g., Wiktionary or BABELNET). Our approach uses a truly interlingual representation, enabling us to use the same model across all nine languages. We present average error reductions over running a state-of-the-art parser on word-to-word translations of 46% for target identiﬁcation, 37% for frame identiﬁcation, and 14% for argument identiﬁcation. 
In the last two years, there has been a surge of word embedding algorithms and research on them. However, evaluation has mostly been carried out on a narrow set of tasks, mainly word similarity/relatedness and word relation similarity and on a single language, namely English. We propose an approach to evaluate embeddings on a variety of languages that also yields insights into the structure of the embedding space by investigating how well word embeddings cluster along different syntactic features. We show that all embedding approaches behave similarly in this task, with dependency-based embeddings performing best. This effect is even more pronounced when generating low dimensional embeddings. 
Events and their coreference offer useful semantic and discourse resources. We show that the semantic and discourse aspects of events interact with each other. However, traditional approaches addressed event extraction and event coreference resolution either separately or sequentially, which limits their interactions. This paper proposes a document-level structured learning model that simultaneously identiﬁes event triggers and resolves event coreference. We demonstrate that the joint model outperforms a pipelined model by 6.9 BLANC F1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain. 
When translating between two languages that differ in their degree of morphological synthesis, syntactic structures in one language may be realized as morphological structures in the other, and SMT models need a mechanism to learn such translations. Prior work has used morpheme splitting with ﬂat representations that do not encode the hierarchical structure between morphemes, but this structure is relevant for learning morphosyntactic constraints and selectional preferences. We propose to model syntactic and morphological structure jointly in a dependency translation model, allowing the system to generalize to the level of morphemes. We present a dependency representation of German compounds and particle verbs that results in improvements in translation quality of 1.4–1.8 BLEU in the WMT English–German translation task. 
Recent work in neural machine translation has shown promising performance, but the most eﬀective architectures do not scale naturally to large vocabulary sizes. We propose and compare three variable-length encoding schemes that represent a large vocabulary corpus using a much smaller vocabulary with no loss in information. Common words are unaﬀected by our encoding, but rare words are encoded using a sequence of two pseudo-words. Our method is simple and eﬀective: it requires no complete dictionaries, learning procedures, increased training time, changes to the model, or new parameters. Compared to a baseline that replaces all rare words with an unknown word symbol, our best variable-length encoding strategy improves WMT English-French translation performance by up to 1.7 BLEU. 
The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classiﬁer that takes both the context and target words as input, and can be efﬁciently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 
When applying machine learning to problems in NLP, there are many choices to make about how to represent input texts. They can have a big effect on performance, but they are often uninteresting to researchers or practitioners who simply need a module that performs well. We apply sequential model-based optimization over this space of choices and show that it makes standard linear models competitive with more sophisticated, expensive state-ofthe-art methods based on latent variables or neural networks on various topic classiﬁcation and sentiment analysis problems. Our approach is a ﬁrst step towards black-box NLP systems that work with raw text and do not require manual tuning. 
This paper aims to compare different regularization strategies to address a common phenomenon, severe overﬁtting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, reembedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models. 
Hyper-parameter optimization is an important problem in natural language processing (NLP) and machine learning. Recently, a group of studies has focused on using sequential Bayesian Optimization to solve this problem, which aims to reduce the number of iterations and trials required during the optimization process. In this paper, we explore this problem from a different angle, and propose a multi-stage hyper-parameter optimization that breaks the problem into multiple stages with increasingly amounts of data. Early stage provides fast estimates of good candidates which are used to initialize later stages for better performance and speed. We demonstrate the utility of this new algorithm by evaluating its speed and accuracy against state-of-the-art Bayesian Optimization algorithms on classiﬁcation and prediction tasks. 
Arabic dialect classiﬁcation has been an important and challenging problem for Arabic language processing, especially for social media text analysis and machine translation. In this paper we propose an approach to improving Arabic dialect classiﬁcation with semi-supervised learning: multiple classiﬁers are trained with weakly supervised, strongly supervised, and unsupervised data. Their combination yields signiﬁcant and consistent improvement on two different test sets. The dialect classiﬁcation accuracy is improved by 5% over the strongly supervised classiﬁer and 20% over the weakly supervised classiﬁer. Furthermore, when applying the improved dialect classiﬁer to build a Modern Standard Arabic (MSA) language model (LM), the new model size is reduced by 70% while the English-Arabic translation quality is improved by 0.6 BLEU point. 
Analyzing arguments in user-generated Web discourse has recently gained attention in argumentation mining, an evolving ﬁeld of NLP. Current approaches, which employ fully-supervised machine learning, are usually domain dependent and suffer from the lack of large and diverse annotated corpora. However, annotating arguments in discourse is costly, errorprone, and highly context-dependent. We asked whether leveraging unlabeled data in a semi-supervised manner can boost the performance of argument component identiﬁcation and to which extent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we signiﬁcantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios. 
Twitter is often used in quantitative studies that identify geographically-preferred topics, writing styles, and entities. These studies rely on either GPS coordinates attached to individual messages, or on the user-supplied location ﬁeld in each proﬁle. In this paper, we compare these data acquisition techniques and quantify the biases that they introduce; we also measure their effects on linguistic analysis and textbased geolocation. GPS-tagging and selfreported locations yield measurably different corpora, and these linguistic differences are partially attributable to differences in dataset composition by age and gender. Using a latent variable model to induce age and gender, we show how these demographic variables interact with geography to affect language use. We also show that the accuracy of text-based geolocation varies with population demographics, giving the best results for men above the age of 40. 
We present novel experiments in modeling the rise and fall of story characteristics within narrative, leading up to the Most Reportable Event (MRE), the compelling event that is the nucleus of the story. We construct a corpus of personal narratives from the bulletin board website Reddit, using the organization of Reddit content into topic-speciﬁc communities to automatically identify narratives. Leveraging the structure of Reddit comment threads, we automatically label a large dataset of narratives. We present a change-based model of narrative that tracks changes in formality, affect, and other characteristics over the course of a story, and we use this model in distant supervision and selftraining experiments that achieve signiﬁcant improvements over the baselines at the task of identifying MREs. 
State of the art in opinion mining mainly focuses on positive and negative sentiment summarisation of online customer reviews. We observe that reviewers tend to provide advice, recommendations and tips to the fellow customers on a variety of points of interest. In this work, we target the automatic detection of suggestion expressing sentences in customer reviews. This is a novel problem, and therefore to begin with, requires a well formed problem deﬁnition and benchmark dataset. This work provides a 3- fold contribution, namely, problem deﬁnition, benchmark dataset, and an approach for detection of suggestions for the customers. The problem is framed as a sentence classiﬁcation problem, and a set of linguistically motivated features are proposed. Analysis of the nature of suggestions, and classiﬁcation errors, highlight challenges and research opportunities associated with this problem. 
A microblog repost tree provides strong clues on how an event described therein develops. To help social media users capture the main clues of events on microblogging sites, we propose a novel repost tree summarization framework by effectively differentiating two kinds of messages on repost trees called leaders and followers, which are derived from contentlevel structure information, i.e., contents of messages and the reposting relations. To this end, Conditional Random Fields (CRF) model is used to detect leaders across repost tree paths. We then present a variant of random-walk-based summarization model to rank and select salient messages based on the result of leader detection. To reduce the error propagation cascaded from leader detection, we improve the framework by enhancing the random walk with adjustment steps for sampling from leader probabilities given all the reposting messages. For evaluation, we construct two annotated corpora, one for leader detection, and the other for repost tree summarization. Experimental results conﬁrm the effectiveness of our method. 
 In this work, we improve the performance of intra-sentential zero anaphora resolution in Japanese using a novel method of recognizing subject sharing relations. In Japanese, a large portion of intrasentential zero anaphora can be regarded as subject sharing relations between predicates, that is, the subject of some predicate is also the unrealized subject of other predicates. We develop an accurate recognizer of subject sharing relations for pairs of predicates in a single sentence, and then construct a subject shared predicate network, which is a set of predicates that are linked by the subject sharing relations recognized by our recognizer. We ﬁnally combine our zero anaphora resolution method exploiting the subject shared predicate network and a state-ofthe-art ILP-based zero anaphora resolution method. Our combined method achieved a signiﬁcant improvement over the the ILPbased method alone on intra-sentential zero anaphora resolution in Japanese. To the best of our knowledge, this is the ﬁrst work to explicitly use an independent subject sharing recognizer in zero anaphora resolution.  
For annotation tasks involving independent judgments, probabilistic models have been used to infer ground truth labels from data where a crowd of many annotators labels the same items. Such models have been shown to produce results superior to taking the majority vote, but have not been applied to sequential data. We present two methods to infer ground truth labels from sequential annotations where we assume judgments are not independent, based on the observation that an annotator’s segments all tend to be several utterances long. The data consists of crowd labels for annotation of discourse segment boundaries. The new methods extend Hidden Markov Models to relax the independence assumption. The two methods are distinct, so positive labels proposed by both are taken to be ground truth. In addition, results of the models are checked using metrics that test whether an annotator’s accuracy relative to a given model remains consistent across different conversations. 
This paper presents a detailed comparative framework for assessing the usefulness of unsupervised word representations for identifying so-called implicit discourse relations. Speciﬁcally, we compare standard one-hot word pair representations against low-dimensional ones based on Brown clusters and word embeddings. We also consider various word vector combination schemes for deriving discourse segment representations from word vectors, and compare representations based either on all words or limited to head words. Our main ﬁnding is that denser representations systematically outperform sparser ones and give state-of-the-art performance or above without the need for additional hand-crafted features. 
Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers signiﬁcant improvements over classiﬁcationbased methods. 
Many discourse relations are explicitly marked with discourse connectives, and these examples could potentially serve as a plentiful source of training data for recognizing implicit discourse relations. However, there are important linguistic differences between explicit and implicit discourse relations, which limit the accuracy of such an approach. We account for these differences by applying techniques from domain adaptation, treating implicitly and explicitly-marked discourse relations as separate domains. The distribution of surface features varies across these two domains, so we apply a marginalized denoising autoencoder to induce a dense, domain-general representation. The label distribution is also domain-speciﬁc, so we apply a resampling technique that is similar to instance weighting. In combination with a set of automatically-labeled data, these improvements eliminate more than 80% of the transfer loss incurred by training an implicit discourse relation classiﬁer on explicitly-marked discourse relations. 
While most previous work on Wikiﬁcation has focused on written texts, this paper presents a Wikiﬁcation approach for spoken dialogues. A set of analyzers are proposed to learn dialogue-speciﬁc properties along with domain knowledge of conversations from Wikipedia. Then, the analyzed properties are used as constraints for generating candidates, and the candidates are ranked to ﬁnd the appropriate links. The experimental results show that our proposed approach can signiﬁcantly improve the performances of the task in human-human dialogues. 
Implicit discourse relation recognition remains a serious challenge due to the absence of discourse connectives. In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit discourse relation recognition, which contains only one hidden layer but is effective in relation recognition. The shallow structure alleviates the overﬁtting problem, while the convolution and nonlinear operations help preserve the recognition and generalization ability of our model. Experiments on the benchmark data set show that our model achieves comparable and even better performance when comparing against current state-of-the-art systems. 
This paper presents a study on the role of discourse markers in argumentative discourse. We annotated a German corpus with arguments according to the common claim-premise model of argumentation and performed various statistical analyses regarding the discriminative nature of discourse markers for claims and premises. Our experiments show that particular semantic groups of discourse markers are indicative of either claims or premises and constitute highly predictive features for discriminating between them. 
This paper aims to ﬁnd errors that lead to dialogue breakdowns in chat-oriented dialogue systems. We collected chat dialogue data, annotated them with dialogue breakdown labels, and collected comments describing the error that led to the breakdown. By mining the comments, we ﬁrst identiﬁed error types. Then, we calculated the correlation between an error type and the degree of dialogue breakdown it incurred, quantifying its impact on dialogue breakdown. This is the ﬁrst study to quantitatively analyze error types and their effect in chat-oriented dialogue systems. 
We present a method of using cohesion to improve discourse element identiﬁcation for sentences in student essays. New features for each sentence are derived by considering its relations to global and local cohesion, which are created by means of cohesive resources and subtopic coverage. In our experiments, we obtain signiﬁcant improvements on identifying all discourse elements, especially of +5% F 1 score on thesis and main idea. The analysis shows that global cohesion can better capture thesis statements. 
Domain adaptation is a challenge for supervised NLP systems because of expensive and time-consuming manual annotated resources. We present a novel method to adapt a supervised coreference resolution system trained on newswire to short narrative stories without retraining the system. The idea is to perform inference via an Integer Linear Programming (ILP) formulation with the features of narratives adopted as soft constraints. When testing on the UMIREC1 and N22 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3, our inference substantially outperforms the original inference on the CoNLL 2011 metric. 
We present LEMMING, a modular loglinear model that jointly models lemmatization and tagging and supports the integration of arbitrary global features. It is trainable on corpora annotated with gold standard tags and lemmata and does not rely on morphological dictionaries or analyzers. LEMMING sets the new state of the art in token-based statistical lemmatization on six languages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05 to 1.58. We also give empirical evidence that jointly modeling morphological tags and lemmata is mutually beneﬁcial. 
We describe a simple and efﬁcient algorithm to disambiguate non-functional weighted ﬁnite state transducers (WFSTs), i.e. to generate a new WFST that contains a unique, best-scoring path for each hypothesis in the input labels along with the best output labels. The algorithm uses topological features combined with a tropical sparse tuple vector semiring. We empirically show that our algorithm is more efﬁcient than previous work in a PoStagging disambiguation task. We use our method to rescore very large translation lattices with a bilingual neural network language model, obtaining gains in line with the literature. 
Arabic, Hebrew, and similar languages are typically written without diacritics, leading to ambiguity and posing a major challenge for core language processing tasks like speech recognition. Previous approaches to automatic diacritization employed a variety of machine learning techniques. However, they typically rely on existing tools like morphological analyzers and therefore cannot be easily extended to new genres and languages. We develop a recurrent neural network with long shortterm memory layers for predicting diacritics in Arabic text. Our language-independent approach is trained solely from diacritized text without relying on external tools. We show experimentally that our model can rival state-of-the-art methods that have access to additional resources.  
In this paper, we describe a method based on statistical machine translation (SMT) that is able to restore accents in Hungarian texts with high accuracy. Due to the agglutination in Hungarian, there are always plenty of word forms unknown to a system trained on a ﬁxed vocabulary. In order to be able to handle such words, we integrated a morphological analyzer into the system that can suggest accented word candidates for unknown words. We evaluated the system in different setups, achieving an accuracy above 99% at the highest. 
We propose a novel framework for improving a word segmenter using information acquired from symbol grounding. We generate a term dictionary in three steps: generating a pseudo-stochastically segmented corpus, building a symbol grounding model to enumerate word candidates, and ﬁltering them according to the grounding scores. We applied our method to game records of Japanese chess with commentaries. The experimental results show that the accuracy of a word segmenter can be improved by incorporating the generated dictionary.  comments on them, which were made by Shogi experts. We enumerate substrings (character sequences) in the sentences and match them with Shogi states by a neural network model. The rationale here is that substrings which match with non-language data well tend to be real words. Our method consists of three steps (see Figure 1). First, we segment commentary sentences for a game state in various ways to produce word candidates. Then, we match them with game states of a Shogi playing program. Finally, we compile the symbol grounding results at all states and incorporate them to an automatic WS. To the best of our knowledge, this is the ﬁrst result reporting a performance improvement in an NLP task by symbol grounding.  
Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper, we benchmark recursive neural models against sequential recurrent neural models, enforcing applesto-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classiﬁcation at the sentence level and phrase level; (2) matching questions to answerphrases; (3) discourse parsing; (4) semantic relation extraction. Our goal is to understand better when, and why, recursive models can outperform simpler models. We ﬁnd that recursive models help mainly on tasks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models. 
This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling. Our model leverages either constituency trees or dependency trees of sentences. The tree-based convolution process extracts sentences structural features, which are then aggregated by max pooling. Such architecture allows short propagation paths between the output layer and underlying feature detectors, enabling effective structural feature learning and extraction. We evaluate our models on two tasks: sentiment analysis and question classiﬁcation. In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work. 
Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, it is still a challenge task to model long texts, such as sentences and documents. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classiﬁcation task. 
Deception detection has been receiving an increasing amount of attention from the computational linguistics, speech, and multimodal processing communities. One of the major challenges encountered in this task is the availability of data, and most of the research work to date has been conducted on acted or artiﬁcially collected data. The generated deception models are thus lacking real-world evidence. In this paper, we explore the use of multimodal real-life data for the task of deception detection. We develop a new deception dataset consisting of videos from reallife scenarios, and build deception tools relying on verbal and nonverbal features. We achieve classiﬁcation accuracies in the range of 77-82% when using a model that extracts and fuses features from the linguistic and visual modalities. We show that these results outperform the human capability of identifying deceit. 
In a typical social media content analysis task, the user is interested in analyzing posts of a particular topic. Identifying such posts is often formulated as a classification problem. However, this problem is challenging. One key issue is covariate shift. That is, the training data is not fully representative of the test data. We observed that the covariate shift mainly occurs in the negative data because topics discussed in social media are highly diverse and numerous, but the user-labeled negative training data may cover only a small number of topics. This paper proposes a novel technique to solve the problem. The key novelty of the technique is the transformation of document representation from the traditional ngram feature space to a center-based similarity (CBS) space. In the CBS space, the covariate shift problem is significantly mitigated, which enables us to build much better classifiers. Experiment results show that the proposed approach markedly improves classification. 
With the exponential growth of scholarly data during the past few years, effective methods for topic classiﬁcation are greatly needed. Current approaches usually require large amounts of expensive labeled data in order to make accurate predictions. In this paper, we posit that, in addition to a research article’s textual content, its citation network also contains valuable information. We describe a co-training approach that uses the text and citation information of a research article as two different views to predict the topic of an article. We show that this method improves signiﬁcantly over the individual classiﬁers, while also bringing a substantial reduction in the amount of labeled data required for training accurate classiﬁers. 
Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we ﬁrst identify several semantic structures behind humor and design sets of features for each structure, and next employ a computational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations. 
We compare the multinomial i-vector framework from the speech community with LDA, SAGE, and LSA as feature learners for topic ID on multinomial speech and text data. We also compare the learned representations in their ability to discover topics, quantiﬁed by distributional similarity to gold-standard topics and by human interpretability. We ﬁnd that topic ID and topic discovery are competing objectives. We argue that LSA and i-vectors should be more widely considered by the text processing community as pre-processing steps for downstream tasks, and also speculate about speech processing tasks that could beneﬁt from more interpretable representations like SAGE. 
Efﬁcient computation of n-gram posterior probabilities from lattices has applications in lattice-based minimum Bayes-risk decoding in statistical machine translation and the estimation of expected document frequencies from spoken corpora. In this paper, we present an algorithm for computing the posterior probabilities of all ngrams in a lattice and constructing a minimal deterministic weighted ﬁnite-state automaton associating each n-gram with its posterior for efﬁcient storage and retrieval. Our algorithm builds upon the best known algorithm in literature for computing ngram posteriors from lattices and leverages the following observations to signiﬁcantly improve the time and space requirements: i) the n-grams for which the posteriors will be computed typically comprises all n-grams in the lattice up to a certain length, ii) posterior is equivalent to expected count for an n-gram that do not repeat on any path, iii) there are efﬁcient algorithms for computing n-gram expected counts from lattices. We present experimental results comparing our algorithm with the best known algorithm in literature as well as a baseline algorithm based on weighted ﬁnite-state automata operations. 
This paper describes a novel target-side syntactic language model for phrase-based statistical machine translation, bilingual structured language model. Our approach represents a new way to adapt structured language models (Chelba and Jelinek, 2000) to statistical machine translation, and a ﬁrst attempt to adapt them to phrasebased statistical machine translation. We propose a number of variations of the bilingual structured language model and evaluate them in a series of rescoring experiments. Rescoring of 1000-best translation lists produces statistically signiﬁcant improvements of up to 0.7 BLEU over a strong baseline for Chinese-English, but does not yield improvements for ArabicEnglish. 
Efﬁcient methods for storing and querying language models are critical for scaling to large corpora and high Markov orders. In this paper we propose methods for modeling extremely large corpora without imposing a Markov condition. At its core, our approach uses a succinct index – a compressed sufﬁx tree – which provides near optimal compression while supporting efﬁcient search. We present algorithms for on-the-ﬂy computation of probabilities under a Kneser-Ney language model. Our technique is exact and although slower than leading LM toolkits, it shows promising scaling properties, which we demonstrate through ∞-order modeling over the full Wikipedia collection. 
We present a new unsupervised mechanism, which ranks word n-grams according to their multiwordness. It heavily relies on a new uniqueness measure that computes, based on a distributional thesaurus, how often an n-gram could be replaced in context by a single-worded term. In addition with a downweighting mechanism for incomplete terms this forms a new measure called DRUID. Results show large improvements on two small test sets over competitive baselines. We demonstrate the scalability of the method to large corpora, and the independence of the measure of shallow syntactic ﬁltering. 
Distributed word representations capture relational similarities by means of vector arithmetics, giving high accuracies on analogy detection. We empirically investigate the use of syntactic dependencies on improving Chinese analogy detection based on distributed word representations, showing that a dependency-based embeddings does not perform better than an ngram-based embeddings, but dependency structures can be used to improve analogy detection by ﬁltering candidates. In addition, we show that distributed representations of dependency structure can be used for measuring relational similarities, thereby help analogy mining. 
This paper introduces a novel way to navigate neighborhoods in distributional semantic models. The approach is based on relative neighborhood graphs, which uncover the topological structure of local neighborhoods in semantic space. This has the potential to overcome both the problem with selecting a proper k in k-NN search, and the problem that a ranked list of neighbors may conﬂate several different senses. We provide both qualitative and quantitative results that support the viability of the proposed method. 
Multi-modal semantics has relied on feature norms or raw image data for perceptual input. In this paper we examine grounding semantic representations in raw auditory data, using standard evaluations for multi-modal semantics, including measuring conceptual similarity and relatedness. We also evaluate cross-modal mappings, through a zero-shot learning task mapping between linguistic and auditory modalities. In addition, we evaluate multimodal representations on an unsupervised musical instrument clustering task. To our knowledge, this is the ﬁrst work to combine linguistic and auditory information into multi-modal representations. 
This paper provides the ﬁrst fully automatic approach for classifying clauses with respect to their aspectual properties as habitual, episodic or static. We bring together two strands of previous work, which address only the related tasks of the episodic-habitual and stative-dynamic distinctions, respectively. Our method combines different sources of information found to be useful for these tasks. We are the ﬁrst to exhaustively classify all clauses of a text, achieving up to 80% accuracy (baseline 58%) for the three-way classiﬁcation task, and up to 85% accuracy for related subtasks (baselines 50% and 60%), outperforming previous work. In addition, we provide a new large corpus of Wikipedia texts labeled according to our linguistically motivated guidelines. 
We present a new approach for unsupervised semantic role labeling that leverages distributed representations. We induce embeddings to represent a predicate, its arguments and their complex interdependence. Argument embeddings are learned from surrounding contexts involving the predicate and neighboring arguments, while predicate embeddings are learned from argument contexts. The induced representations are clustered into roles using a linear programming formulation of hierarchical clustering, where we can model task-speciﬁc knowledge. Experiments show improved performance over previous unsupervised semantic role labeling approaches and other distributed word representation models. 
Modeling the entailment relation over sentences is one of the generic problems of natural language understanding. In order to account for this problem, we design a theorem prover for Natural Logic, a logic whose terms resemble natural language expressions. The prover is based on an analytic tableau method and employs syntactically and semantically motivated schematic rules. Pairing the prover with a preprocessor, which generates formulas of Natural Logic from linguistic expressions, results in a proof system for natural language. It is shown that the system obtains a comparable accuracy (≈ 81%) on the unseen SICK data while achieving the stateof-the-art precision (≈ 98%). 
Cross-domain sentiment classification (CSC) aims at learning a sentiment classifier for unlabeled data in the target domain based on the labeled data from a different source domain. Due to the differences of data distribution of two domains in terms of the raw features, the CSC problem is difficult and challenging. Previous researches mainly focused on concepts mining by clustering words across data domains, which ignored the importance of authors’ emotion contained in data, or the different representations of the emotion between domains. In this paper, we propose a novel framework to solve the CSC problem, by modelling the emotion across domains. We first develop a probabilistic model named JEAM to model author’s emotion state when writing. Then, an EM algorithm is introduced to solve the likelihood maximum problem and to obtain the latent emotion distribution of the author. Finally, a supervised learning method is utilized to assign the sentiment polarity to a given online review. Experiments show that our approach is effective and outperforms state-of-the-art approaches. 
This paper presents a new method to identify sentiment of an aspect of an entity. It is an extension of RNN (Recursive Neural Network) that takes both dependency and constituent trees of a sentence into account. Results of an experiment show that our method signiﬁcantly outperforms previous methods. 
This paper introduces ASTD, an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classiﬁed as objective, subjective positive, subjective negative, and subjective mixed. We present the properties and the statistics of the dataset, and run experiments using standard partitioning of the dataset. Our experiments provide benchmark results for 4 way sentiment classiﬁcation on the dataset. 
For ﬁne-grained sentiment analysis, we need to go beyond zero-one polarity and ﬁnd a way to compare adjectives that share a common semantic property. In this paper, we present a semi-supervised approach to assign intensity levels to adjectives, viz. high, medium and low, where adjectives are compared when they belong to the same semantic category. For example, in the semantic category of EXPERTISE, expert, experienced and familiar are respectively of level high, medium and low. We obtain an overall accuracy of 77% for intensity assignment. We show the signiﬁcance of considering intensity information of adjectives in predicting star-rating of reviews. Our intensity based prediction system results in an accuracy of 59% for a 5-star rated movie review corpus.  and low, which share the same semantic property. We have used the semantic frames of FrameNet1.5 (Baker et al., 1998) to obtain these semantic categories. Our approach is based on the idea that the most intense word has higher contextual similarity with high intensity words than with medium or low intensity words. We use the intensity annotated movie review corpus to obtain the most intense word for a semantic category. Then, cosine similarity between word vectors of the most intense word and other words of the category is used to assign intensity levels to those words. Our approach with the used resources is shown in ﬁgure 1.  
Sentiment analysis models often use ratings as labels, assuming that these ratings reﬂect the sentiment of the accompanying text. We investigate (i) whether human readers can infer ratings from review text, (ii) how human performance compares to a regression model, and (iii) whether model performance is affected by the rating “source” (i.e. original author vs. annotator). We collect IMDb movie reviews with author-provided ratings, and have them re-annotated by crowdsourced and trained annotators. Annotators reproduce the original ratings better than a model, but are still far off in more than 5% of the cases. Models trained on annotator-labels outperform those trained on author-labels, questioning the usefulness of author-rated reviews as training data for sentiment analysis. 
We present the Trip-MAML dataset, a Multi-Lingual dataset of hotel reviews that have been manually annotated at the sentence-level with Multi-Aspect sentiment labels. This dataset has been built as an extension of an existent English-only dataset, adding documents written in Italian and Spanish. We detail the dataset construction process, covering the data gathering, selection, and annotation. We present inter-annotator agreement ﬁgures and baseline experimental results, comparing the three languages. Trip-MAML is a multi-lingual dataset for aspect-oriented opinion mining that enables researchers (i) to face the problem on languages other than English and (ii) to the experiment the application of cross-lingual learning methods to the task. 
We present a novel way of extracting features from short texts, based on the activation values of an inner layer of a deep convolutional neural network. We use the extracted features in multimodal sentiment analysis of short video clips representing one sentence each. We use the combined feature vectors of textual, visual, and audio modalities to train a classifier based on multiple kernel learning, which is known to be good at heterogeneous data. We obtain 14% performance improvement over the state of the art and present a parallelizable decision-level data fusion method, which is much faster, though slightly less accurate. 
Sentiment analysis has been a major area of interest, for which the existence of highquality resources is crucial. In Arabic, there is a reasonable number of sentiment lexicons but with major deﬁciencies. The paper presents a large-scale Standard Arabic Sentiment Lexicon (SLSA) that is publicly available for free and avoids the deﬁciencies in the current resources. SLSA has the highest up-to-date reported coverage. The construction of SLSA is based on linking the lexicon of AraMorph with SentiWordNet along with a few heuristics and powerful back-off. SLSA shows a relative improvement of 37.8% over a state-of-theart lexicon when tested for accuracy. It also outperforms it by an absolute 3.5% of F1-score when tested for sentiment analysis. 
For sentiment classiﬁcation, it is often recognized that embedding based on distributional hypothesis is weak in capturing sentiment contrast–contrasting words may have similar local context. Based on broader context, we propose to incorporate Theta Pure Dependence (TPD) into the Paragraph Vector method to reinforce topical and sentimental information. TPD has a theoretical guarantee that the word dependency is pure, i.e., the dependence pattern has the integral meaning whose underlying distribution can not be conditionally factorized. Our method outperforms the state-of-the-art performance on text classiﬁcation tasks. 
 We propose a novel data augmentation approach to enhance computational behavioral analysis using social media text. In particular, we collect a Twitter corpus of the descriptions of annoying behaviors using the #petpeeve hashtags. In the qualitative analysis, we study the language use in these tweets, with a special focus on the ﬁne-grained categories and the geographic variation of the language. In quantitative analysis, we show that lexical and syntactic features are useful for automatic categorization of annoying behaviors, and frame-semantic features further boost the performance; that leveraging large lexical embeddings to create additional training instances signiﬁcantly improves the lexical model; and incorporating frame-semantic embedding achieves the best overall performance. 
We propose a method to detect hidden data in English text. We target a system previously thought secure, which hides messages in tweets. The method brings ideas from image steganalysis into the linguistic domain, including the training of a feature-rich model for detection. To identify Twitter users guilty of steganography, we aggregate evidence; a ﬁrst, in any domain. We test our system on a set of 1M steganographic tweets, and show it to be effective. 
We consider the task of automatically identifying participants’ motivations in the public health campaign Movember and investigate the impact of the different motivations on the amount of campaign donations raised. Our classiﬁcation scheme is based on the Social Identity Model of Collective Action (van Zomeren et al., 2008). We ﬁnd that automatic classiﬁcation based on Movember proﬁles is fairly accurate, while automatic classiﬁcation based on tweets is challenging. Using our classiﬁer, we ﬁnd a strong relation between types of motivations and donations. Our study is a ﬁrst step towards scaling-up collective action research methods. 
Domestic abuse affects people of every race, class, age, and nation. There is signiﬁcant research on the prevalence and effects of domestic abuse; however, such research typically involves population-based surveys that have high ﬁnancial costs. This work provides a qualitative analysis of domestic abuse using data collected from the social and news-aggregation website reddit.com. We develop classiﬁers to detect submissions discussing domestic abuse, achieving accuracies of up to 92%, a substantial error reduction over its baseline. Analysis of the top features used in detecting abuse discourse provides insight into the dynamics of abusive relationships. 
First Story Detection is hard because the most accurate systems become progressively slower with each document processed. We present a novel approach to FSD, which operates in constant time/space and scales to very high volume streams. We show that when computing novelty over a large dataset of tweets, our method performs 192 times faster than a state-of-the-art baseline without sacriﬁcing accuracy. Our method is capable of performing FSD on the full Twitter stream on a single core of modest hardware. 
Social media is a rich source of rumours and corresponding community reactions. Rumours reﬂect different characteristics, some shared and some individual. We formulate the problem of classifying tweet level judgements of rumours as a supervised learning task. Both supervised and unsupervised domain adaptation are considered, in which tweets from a rumour are classiﬁed on the basis of other annotated rumours. We demonstrate how multi-task learning helps achieve good results on rumours from the 2011 England riots. 
In this paper we study the identiﬁcation and veriﬁcation of simple claims about statistical properties, e.g. claims about the population or the inﬂation rate of a country. We show that this problem is similar to extracting numerical information from text and following recent work, instead of annotating data for each property of interest in order to learn supervised models, we develop a distantly supervised baseline approach using a knowledge base and raw text. In experiments on 16 statistical properties about countries from Freebase we show that our approach identiﬁes simple statistical claims about properties with 60% precision, while it is able to verify these claims without requiring any explicit supervision for either tasks. Furthermore, we evaluate our approach as a statistical property extractor and we show it achieves 0.11 mean absolute percentage error. 
QT21 (see http://www.qt21.eu/) is an EU-funded project with several goals related to machine translation. This paper relates to the QT21 goal of "improved evaluation … informed by human translators", using a framework that harmonizes MQM (Multidimensional Quality Metrics) and DQF (Dynamic Quality Framework). The purpose of the paper, which expresses my personal views, is to obtain feedback on three claims I am making about translation quality evaluation of both human and machine translation: (1) Both automatic, holistic reference-based metrics (such as BLEU) and analytic manual metrics of translation quality are needed; (2) one metric is not sufficient for all translation specifications; and (3) widespread use of specifications and the harmonized MQM/DQF framework for developing metrics will have a positive impact beyond the QT21 project. If these three claims turn out to be true, we will see a new era in the relationship between translators and computers. 
This paper presents the results of a preliminary experiment and a main test within the HBB4ALL project that aimed to determine whether automatic interlingual and intralingual subtitling help to better understand news content. Results tend to indicate that the usefulness of automatic subtitling correlates with the participants’ English level, enhancing comprehension only in certain groups. 
This paper gives a brief overview of the EXPloiting Empirical appRoaches to Translation (EXPERT) project, an FP7 Marie Curie Initial Training Network, which is preparing the next generation of world-class researchers in the ﬁeld of hybrid machine translation. The project is employing 15 Marie Curie fellows who are working on 15 individual, but interconnected, projects and is organising local and consortium wide training activities. The project has been running for three years and has already produced high-quality research. This paper presents the most important research achievements of the project. 
If you want to overcome occasional quality control and to establish a coherent quality assurance system for your translations, you need to think holistic in terms of incorporating all possible stakeholders. Furthermore, you have to keep it simple so occasional users do not get frustrated and stop their valuable co-operation. It also might be a good idea to use some features known from social media in order to boost motivation and participation. 
This paper discusses Kamusi Pre:D, a system to improve translation by disambiguating word senses in a source document with reference to a large concept-based lexicon that is aligned by sense across numerous languages. Currently under active development, the program prompts users to select the intended meaning when polysemous terms occur, and gives the user the option to select multiword expressions instead of individual words when the MWE occurs as a lexicalized dictionary entry. The disambiguated text is then automatically matched to sense-specific translation equivalents that have been aligned across languages. Pre:D is intended to integrate with existing translation tools, but greatly improve accuracy by involving human intelligence in vocabulary selection, both through manual document review of ambiguous terms and by reference to the underlying curated multilingual Kamusi dictionary data. Pre:D will aid accurate vocabulary translation among a wide range of language pairs, most currently unserved, and offer significant advantages in time, effort, and quality for multilingual translation projects by disambiguating a document one time for concepts that can be rendered appropriately across numerous languages. 
This document describes the EU FP7 funded FALCON (Federated Active Linguistic data CuratiON) project. 
Consumer-generated reviews (CGR) entail a significant potential business volume in terms of translation and post-editing, however travel review platforms usually rely solely on raw machine translation. As a new digital genre, CGR require specific post-editing guidelines, therefore, this paper focuses on the analysis of a corpus of Spanish machine translation output of hotel reviews in order to identify error patterns and their effects on quality with the aim of designing a post-editing strategy adapted to this particular type of text. 
The University of Bologna/Forlì offers students of the MA in Interpreting a course in Methods and Technologies for Interpreting. A recent addition to the software presented to students is InterpretBank, a CAI tool designed to assist interpreters during the entire workflow of an interpreting assignment. We conducted a pilot study to collect information on the students’ use of CAI tools to look up terminology in the booth. The aim was to verify how such tools can be integrated in the curriculum by identifying potential issues and suggesting solutions. We ran an experiment with 12 MA interpreting students to observe their behaviour during the simultaneous interpreting of terminology-dense texts. Experience seems to play a key role in helping students integrate the tool in their workflow in the booth. Some students, however, tend to excessively rely on the software program, while others see it as a source of distraction and find it hard to focus on the delivery. There is reason to believe the tool will prove a useful addition to the curriculum of trainee interpreters, yet more empirical studies are needed to test and possibly improve the way it can be integrated with current interpreter training approaches. 
In the Skype Translator project, we set ourselves the ambitious goal of enabling successful open-domain conversations between Skype users in different parts of the world, speaking different languages. Building such technology is more than just stitching together the component parts; it also requires work in allowing the parts to talk with one another. In addition to allowing speech communication between users who speak different languages, these technologies also enable Skype communication with another class of users: those who have deafness or hard of hearing. Accommodating these additional users required design changes that benefited all users of Skype Translator. The promise of Skype Translator is not only the breaking down of the language barrier, it is also for breaking down of the hearing barrier. 
This contribution draws on the different models developed to assess and predict technology acceptance (particularly the Unified Theory, UTAUT) and discusses the factors considered and their applicability to CAT tools and professional translators. It further draws on translator studies to discuss how the current research on the translators’ habitus can support and enhance the existing models. The model suggested comprises five categories (performance and effort expectancy, social norms, perceived playfulness and self-determination), whose relevance is tested empirically with a cohort of professional translators. The survey is carried out through a questionnaire where translators working in different language combinations and different institutions and companies, with different status (free-lancers and permanent in-house professionals), report their adherence to specific statements pertaining to the five constructs analyzed. The analysis highlights the importance of one of the two innovative factors contained in this proposal, self-determination, across the professional characteristics of the participants. 
This paper presents an overview of the ALST project, in which speech technologies (speech recognition and speech synthesis) and machine translation were implemented in the voice-over of non-fictional genres and in the audio description of films. The paper presents the project rationale, a brief description of the experiments carried out within the project, as well as its main findings. 
This paper is a follow up to our teaching case study described in ASLIB 2013. The subject of the present paper is how do we integrate the new ISO 25000 series (ISO/IEC 2014) to update the EAGLES 7-steps recipe, which is one of the deliverables of the Evaluation of Natural Language Processing Systems project (EAGLES I and II) based on the ISO 9216 software evaluation series. The present poster paper will focus on the methodology proposed to the students and give some preliminary results in order to give a flavor of the achieved work within only several weeks of our MA course. The main aim of this paper is thus to provide a ready-made methodology to evaluate CAT tools, that can be reused not only in the academic field by contributing to include such knowledge into “basic” translator’s training but also by freelancers willing to evaluate several tools before making their choice. 
We have reached the theoretical limits of what can be achieved through the application of Statistical, Rule based and Transfer based machine translation technology. The limits are those imposed by the Turing architecture which is what we are currently restricted to. The start of the 21st century has seen significant theoretical advances in the domain of human intelligence and its mechanisms and underpinnings. 
Language professionals play an important role in an increasingly multilingual society where people commonly do not sufﬁciently understand all languages used in their environment. While there are many translation environment tools (TEnTs) available to support translators in their tasks, there is evidence that these tools are not used to their full potential. Within the context of a broad research project, SCATE (Smart Computer-Assisted Translation Environment), we investigated the current tools and work practices of language professionals to enable personalization of the user interfaces of translation environments and improve translators workﬂows. We used complementary research methods in our study: a survey among language professionals, semi-structured interviews with ﬁve local companies involved in translation and nine contextual inquiries with both in-house and freelance translators and revisers. Based on the gathered information we identiﬁed eight relevant scales to typify the users and their experience with TEnTs, we created generalized workﬂows and summarized the key insights using two personas. We present a set of recommendations that could positively impact translators workﬂows. These recommendations are in line with, but go beyond state of the art: they are focused on improving efﬁciency, effectiveness and usability of translation environments as well as giving more control to translators. 
The paper introduces a framework to measure products for world-readiness when releasing into a new geography or to set measurable improvement goals. The framework classiﬁes the subjective points of consideration into an objective set of questions grouped into logical verticals to help score a product efﬁciently. 
This workshopoutlines the progress that has been made on the TAUS Dynamic Quality Framework (DQF) in the past year and introduces the TAUS Quality Dashboard where all stakeholders in the global translation services can monitor their performance using industry-shared metrics and benchmark themselves against industry average productivity and quality. The TAUS DQF integration with translation tools via an open API will also be demonstrated. 1. Introduction The diversification in content types and the swift adoption of translation technologies (including machine translation) drives the need for more dynamic and reliable methods and measurements for translation quality evaluation. Industry-shared metrics will lead to more reliable measurements that give all stakeholders in the language service industry useful benchmarks and insights to help them adjust and improve their processes. The industry-shared metrics will turn quality evaluation into business intelligence steering and supporting management decisions. In this workshop, we are going to present the progress that has been made on the TAUS Dynamic Quality Framework (DQF) since the last AsLing workshop one year ago. The workshop will also introduce the TAUS Quality Dashboard, which was released in September 2015. The Dashboard is an industry collaborative platform for the global translation services sector where translator operators and producers will be able to monitor their performance based on a variety of parameters they can select from. We are going to present and demo the integration of DQF in CAT tools as well as the reporting features in the Quality Dashboard and are looking forward to receiving feedback and comments from the participants on the work already done and the future roadmap. 2. The Dynamic Quality Framework The TAUS Dynamic Quality Framework (DQF) was first developed by TAUS in 2011 in close cooperation with many of the TAUS member companies and represents a dynamic approach to quality evaluation. This dynamic evaluation model takes into account the changing landscape accounting for different content types and the adoption of automated translation technologies. The theoretical framework of DQF is built around three evaluation parameters: utility, time and sentiment. The relative weight of these parameters varies in relation to the content type to be translated.The vision behind DQF is to standardize the methods and tools of quality evaluation, aggregate the scores and measurements and make these available through industry-shared metrics. While DQF provides the reference for quality evaluation, the DQF online platform, also known as DQF tools, provides the specific tools needed to carry out quality evaluation in a vendor independent and standardized environment. The DQF tools running on the TAUS website were released in 2014. At the first AsLing conference last year, TAUS presented the results of a survey conducted in the summer of 2014 among translators and academic staff who were conducting quality evaluation tasks for MT output or human translation. All respondents were active 127 Proceedings of the 37th Conference Translating and the Computer, pages 127–136, London, UK, November 26-27, 2015. c 2015 AsLing  users of the TAUS DQF tools and were asked to provide feedback and explain how they did translation Quality Evaluation (QE) and what they expected from the DQF (van der Meer &Görög, 2015). Some of the points raised concerned the lack of transparent evaluation criteria, the difficulty of finding the right metrics, the lack of standardization and the need for different quality levels, not to mention costs and time-to-market. The tools on the TAUS Evaluate platform include a Content Profiling wizard, a tool to carry out MT ranking and comparison, a tool to run post-editing productivity testing and a knowledge base containing best practices and use cases. Quality attributes for MT output are traditionally accuracy and fluency. However, accuracy and fluency can just as easily be adopted to evaluate human translation which can also be checked for types of errors, as the standard approach to quality evaluation currently does. DQF adopts the error typology developed from the existing error-count metrics (see Section 6). 3. From DQF to the Quality Dashboard Collecting quality data through the DQF tools proved to be useful but at the same time this approach still suffered from the limitations of displaying only the data that were related to the submitted projects. If collected data could become shared metrics, measurements would become more reliable and give translation operators and producers (translators) useful benchmarks and insights that help them to adjust and improve processes. This is why a new perspective was taken as to what DQF could achieve. TAUS members and partners started to ask whether there was a way of integrating DQF into the translation workflow and avoid the continuous switching between the normal environment and the DQF tools page. This is why an open API for DQF was developed that connects DQF to the existing translation tools and workflow systems. TAUS provides API specifications and dedicated plugins to allow technology providers and users of translation services to integrate TAUS DQF into their work environment. The data collected through DQF can be displayed on the TAUS Quality Dashboard to allow translators and project, vendor and quality managers track and benchmark the quality, productivity and efficiency of translation. The Quality Dashboard was a natural next step that fits very well with the overall trend in the industry towards open data and metrics. The Quality Dashboard delivers on the DQF vision and provides statistics on translation, benchmarking for translation activity and quality, as well as analysis of translation performance and production.Quality evaluation though the Quality Dashboard becomes business intelligence to help steer and support management decisions. 4. Reporting in the Quality Dashboard The reports in the Quality Dashboard cover the two main areas of Productivity/Efficiency and Quality. [These two areas will be covered in more detail in the following sections]. The Quality Dashboard is a flexible and dynamic tool which offers a number of filters to customize the charts and reports to be displayed.At each level, users can see the overall industry average and the industry average for their specific selection. In addition, users can also benchmark their project(s) against the industry scores. Available filters include language pair, time span, project, technology use (e.g. TM vs. 128  MT), translation process, content type and industry. Reports for quality will include error typology both in terms of number and type of errors. In addition, error review can be customized with penalties and pass/fail rates. There is a development roadmap for all reports to be made available to users and planned until the end of the year. Thanks to all the available filters, reports can be made more or less granular and additional filters can be developed on request from the users. Figure 1 - Time spent per task Figure 1 shows the productivity for each task of a project. In this case, productivity is expressed in total hours spent on translation and review, broken down by review cycle. Time spent per task can be displayed both in aggregated form per project or broken down e.g. per language pair. This allows the identification of possible bottlenecks in the overall workflow. In addition to total number of errors per error category, another report can be generated which provides a more accurate picture of the distribution of errors based on their severity. Figure 2 shows how many errors per category have been labeled as ‘critical’, ‘major’, ‘minor’ or ‘neutral’ at project level, but the same information can also be provided for an individual task. The chart provides the weighted distribution (bars) compared to the absolute count (blue line). Both counts are normalized (e.g. per 1,000 words). 129  Figure 2 - Weighted error distribution Project Managers may be interested to know how many and what kind of errors have been identified by each reviewer, as shown in Figure 3. This can be useful to compare different review styles and better understand the evaluation of e.g. in-country reviewer. Figure 3 - Distribution of errors per task 5. Productivity and Efficiency The Quality Dashboard provides productivity and efficiency metrics across content types, industries, processes used, technologies applied and by language pairs. Productivity is the throughput or speed expressed in the number of words per hour. Productivity tracking is 130  widely used for measuring the throughput of translators or quantifying the quality of MT engines by examining post-editing tasks. It helps evaluating which translation process is more appropriate and assessing the quality of the translation memory or the machine translation system in use. Efficiency is a new score introduced by TAUS (Görög 2015a). The Efficiency Score is a composite indicator of translation productivity based on the words processed per hour and the edit distance. It calculates a weighted score, which gives a much more balanced and realistic insight in the performance of both human and technology resources than the commonly used productivity measurement (Görög 2015b). Using a similar procedure, additional attributes such as quality of the translated segments can be added to the Efficiency Score to reach higher precision. While the productivity score is a good first performance indicator, the TAUS Efficiency Scoregives both translators and managers a more reliable measurement, especially when used in combinationwith the filters for technology, process and content. The Efficiency Score can be an absolute score calculated based on one given project or a relative score thatis calculated using all the relevant data in the DQF database. It can be calculated using the two obligatoryvariables (core variables of words per hour and edit distance) or by adding some optional variables to the calculation to increase precisionand credibility. It can be calculated to measure translator efficiency as well as CAT/TMSor MT engine efficiency. 6. Error Typology A vast majority of providers and buyers of translation services manage their quality program with an error typology template. The LISA QA model and the SAE J2450 are among the two most commonly applied metrics for error category. TAUS has developed a more up-to-date version of these error typologies and made it available under DQF. The DQF error typology approach to quality evaluation involves the use of a list of error categories. The entire text or a sample thereof is evaluated by a qualified linguist who flags errors, applies penalties and establishes whether the content meets a pass threshold. This is a common type of evaluation in the translation sector. Although the error categories might vary, a benchmarking report by TAUS found that there was considerable similarity between the most commonly used typologies by over 20 companies (Language, Terminology, Accuracy and Style) and the types of errors. However, there is less agreement on the penalties to be applied or their severity levels. In 2014, the German Research Center for Artificial Intelligence (DFKI) published the MQM (Multidimensional Quality Metrics) framework as part of the EU-funded QTLaunchPad project based on careful examination and extension of existing quality models (Lommel 2014). MQM is a framework for building task-specific translation metrics. It allows users to create custom metrics that can be used for various assessment purposes. By providing a master vocabulary of error types, users can describe metrics in a fully transparent fashion. MQM has been implemented in a variety of commercial and open-source tools. Under the European funded project QT21, TAUS and DFKI have harmonized the DQF and MQM error typologies1 into one DQF-MQM framework where the high-level branches match the six core DQF issue types (Figure 4). DQF’s analytic method and the MQM hierarchy of translation quality issues have both been modified to share the same basic 
Advances of machine translation technology have in recent years increased its use in various contexts. In the translation industry, work processes involving the use of machine translated texts as a raw translation to be post-edited by a translator are becoming increasingly common. The increasing use of post-editing processes also raises questions related to teaching and training of translators and post-editors, and institutions offering translator training have started to incorporate post-editing in their curricula. This paper describes a machine translation and post-editing course arranged at the University of Helsinki in Fall 2014. From the teacher’s perspective, we discuss experiences of planning and teaching of the post-editing course. The development of the students’ experiences and perception of the course contents, machine translation technology and post-editing are also discussed based on reﬂective essays written by the students after the course. 
This article presents the results of a study designed to evaluate the quality of post-edited wildlife documentary films (in comparison to translated) which are delivered using voiceover and off-screen dubbing. The study proposes a quality assessment at three levels: experts’ assessment, dubbing studio’s assessment and end-users’ assessment. The main contribution of this quality assessment proposal is the inclusion of end-users in the process of assessing the quality of post-edited and translated audiovisual texts. Results show that there is no meaningful difference between the quality of post-edited and translated wildlife documentary films, although translations perform better in certain aspects. 1. Acknowledgements This article is part of the ALST project (reference FFI-2012-31024) led by Anna Matamala and funded by the Spanish "Ministerio de Economía y Competitividad" and it is also partially funded by research group TransMedia Catalonia (2014SGR27) and the grant FI-DGR2013 awarded by AGAUR. This article is part of the research carried out by Carla Ortiz-Boix under the supervision of Dr. Anna Matamala within the PhD in Translation and Intercultural Studies from the Department of Translation, Interpretation & East Asian Studies at Universitat Autònoma de Barcelona. 2. Introduction Quality and quality assessment (QA) have been a central issue in Translation Studies since the beginning of the discipline. Many studies have been carried out in that regard (e.g. Nida, 1964; Reiss et al, 1984; Gambier, 1998; Hansen, 2008; Melby et al, 2014), approaching both quality and QA differently depending on the translation theory (House, 2006). Studies on machine translation (MT) and post-editing (PE) have also addressed quality and QA by developing models and measures to evaluate the quality of the text types (technical and general) in which MT and PE is most frequently applied. Although recent studies (Melero et al, 2006; Bywood et al, 2012; Etchegoyhen et al, 2014; Fernández et al, 2013; Ortiz-Boix and Matamala, forthcoming) have proved that including MT and MT plus PE into the workflow of some audiovisual translation (AVT) modalities, mostly subtitling, would positively impact productivity, research into quality and QA of both MT and PE in AVT is still much needed.  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 16  This article presents an experiment in which the quality of post-edited wildlife documentary excerpts delivered through voice-over (VO) and off-screen dubbing (OD) has been assessed in comparison to the quality of translations of the same wildlife documentary excerpts. This experiment has been carried out because, after research by Ortiz-Boix and Matamala (forthcoming) demonstrated that applying post-editing instead of translation in these transfer modes could be feasible in terms of effort involved, it is yet to be known how this would impact on quality. Our QA proposal takes into account the specificities of the two audiovisual transfer modes involved (VO and OD) and includes a new aspect that has been usually left aside: the involvement of end-users. It also includes a brief quality assessment by the dubbing professionals that recorded the translated and post-edited versions that were used afterwards in the user reception test. In order to contextualize our experiment, Section 3 briefly describes the two audiovisual transfer modes under analysis, and summarizes how post-editing QA, and QA in AVT have been approached so far. Section 4 describes the methodological aspects of our QA test. In Section 5, results are presented, and conclusions and further research are discussed in Section 6. 3. Previous Work This section defines VO and OD, highlighting the specificities of these AVT modalities (3.1). It then summarizes previous work on post-editing QA, with an emphasis on audiovisual translation that has inspired the study (3.2). 3.1. Voice-Over and Off-Screen Dubbing VO is the AVT transfer mode that revoices an audiovisual text in another target language on top of the source language voice, so that both voices are heard simultaneously (Franco et al, 2010). In countries such as Spain, VO is the transfer mode frequently used in factual programs, e.g. documentary films, as it is said to help reproduce the feeling of reality, truth and authenticity given by the original audiovisual product (Franco et al, 2010). In Eastern Europe, however, VO can also be found in fictional TV programs. OD is the transfer mode that revoices off-screen narrations substituting the original voice with a version in the target language (Franco et al, 2010). In other words, when OD is applied, only the target language version is heard, not the original one. OD is used in factual programs and usually combined with VO (OD for off-screen narrators, VO for on-screen interviews). Some of the main features of these transfer modes are the following: 1) Both VO and OD present synchronization constraints. In VO three types of synchrony are observed: kinetic synchrony – the translated text matches the body movements seen on screen–, action synchrony – the translated text matches the actions seen on screen–, and voice-over isochrony – the translated message fits between the beginning and the end of the original speech, leaving some time after the original voice starts and before it ends where only the original can be heard. OD is only endowed with kinetic and action synchronies, as the original voices are not heard in this transfer mode (Orero, 2006; Franco et al, 2010). 2) Different language registers can coexist in audiovisual productions where VO and OD are used: whilst VO is generally used for semi-spontaneous or spontaneous interviews, OD is usually applied to narrators with a planned discourse (Matamala, 2009; Franco et al., 2010). If the original product contains oral features such as fluffs, hesitations and grammatical mistakes, the target language version does not generally reproduce them (Matamala, 2009). In other words, the translation is generally an edited version of the original.  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 17  VO and OD are often used to revoice wildlife documentary films from English into Spanish, the object of our research. This type of non-fictional genre usually includes many terms that might pose additional challenges to the translators (Matamala, 2009). It is also often the case that the source text contains linguistic errors and inconsistencies (Franco et al, 2010), and that a quality written script is not available (Ortiz-Boix, forthcoming). However, translators are expected to deliver a quality written script in the target language so that the recording by voice talents in a dubbing studio can begin. 3.2. Post-Editing Quality Assessment Although research on QA of post-edited text has increased, it is still rather limited. Fiederer and O’Brien (2009), Plitt and Masselot (2010), Carl et al (2011), García (2011), Guerberof (2009, 2012), Melby et al (2014) and Mariana (2014) have dealt with quality in post-editing, to a greater or lesser extent. Up until now, QA has been based mostly on what has been has termed in the QTLauchPad project (Lommel et al, 2014) as either holistic approaches –which assess the quality of the text as a whole – or analytic approaches –which assess the quality by analysing the text in detail according to different sets of specifications. A combination of both can also be found. Holistic approaches: Plitt and Masselot (2010) used the Autodesk translation QA team to assess randomly selected samples of translated and post-edited text using two labels ("average" or "good"), depending on whether they considered the text was fit for publishing. In Carl et al (2011), raters ranked the quality of a list of sentences, either translated or postedited. Fiederer and O’Brien (2009) also assessed the quality of sentences – three translated and three post-edited versions of 30 sentences – according to clarity, accuracy and style on a 4-point scale. Raters were also asked to indicate their favorite option out of the six proposals for each source sentence. Analytic approaches: In García (2011), a rater assessed the quality of a 500-word text by using the Australian National Accreditation Authority for Translators and Interpreter's (NAATI) guidelines. In Guerberof (2009, 2012), three raters blindly assessed translated segments, post-edited segments and segments previously extracted from a translation memory by using the LISA QA model. Mixed approaches: Melby et al (2014), Mariana (2014) and Lommel et al (2014) develop and implement the Multidimensional Quality Metrics (MQM) in their analysis. The model provides a framework for defining metrics and scores that can be used to assess the quality of human translated, post-edited or machine translated texts. It sets error categories, otherwise called issue types, which assess different aspects of quality and problems. MQM is partly based on the translation specifications (Melby, 2014) that define expectations for a particular type of translation; MQM is organized in a hierarchic tree that can include all the necessary issue types for a given text type and a given set of specifications. In the specific field of audiovisual translation, post-editing quality assessment research is still more limited: EU-financed project SUMAT (Etchegoyhen et al, 2014) evaluated the quality of the machine translation output via professional subtitlers who assigned a score to each subtitle. They were asked for general feedback on their experience while post-editing as well as on their perceived quality of the output. Aziz et al (2012) assessed the quality of the machine translated subtitles by post-editing them using the PET tool. The post-edited subtitles were afterwards assessed against translated subtitles using BLEU and TER automatic measures, suggesting there is no meaningful difference in terms of quality between them.  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 18  4. Methodology Our experiment involved one language pair (English into Spanish), and aimed to assess the quality of post-edited wildlife documentaries compared to the quality of human translations. It is built upon the hypothesis that there is no meaningful difference between the quality of postediting and the quality of translation of wildlife documentaries in English delivered through VO and OD in Spanish. The experiment included a three-level quality assessment: (1) quality assessment by experts, with a mixed approach (holistic and analytic); (2) quality assessment by the dubbing studio where the translations and post-editings were recorded, and (3) quality assessment by end-users, who watched both post-edited and translated audiovisual excerpts. The inclusion of end-users in the assessment has been inspired by functionalist approaches to translation and by recent user reception studies in AVT. In the case of wildlife documentaries, we wanted to assess whether both post-edited and translated documentaries fulfilled their function to the same extent, that of informing and entertaining the audience. 4.1. Participants Participants taking part on the first level assessment were six lecturers of MAs on audiovisual translation in universities in Spain who are experts on VO and currently work or have recently worked as professional voice-over translators. The experts' profiles are comparable: all of them have a BA in Translation Studies except for one, who has a BA in German Studies. Furthermore, five of them have either a PhD in Translation or have attended PhD courses on the same field. Previous experience varies among participants: when the experiment was carried out experts 1, 3, and 5 had worked as audiovisual translators between 10 and 16 years and taught for 11, 8, and 5 years respectively, while participants 2, 4, and 6 had between 5 and 8 years of experience as audiovisual translators and taught for the last 4 or 5 years. The number of experts used to rate the documents is higher than in previous studies on QA and post-editing (Guerberof, 2009; García, 2011; or De Sutter et al, 2012) For the second level, only one dubbing studio was used, as only one study was needed to record the materials. Two voice talents, a dubbing director and a sound technician were present during the recording session. In the third level, 56 users with different educational backgrounds took part in the experiment (28 male, 28 female, 23-65 years old, mean age: 39.15). All participants were native speakers of Spanish and 46.43% of the participants were highly proficient in English. Watching habits related to wildlife documentaries do not vary much among participants (96.43% watch a maximum of 3 documentaries on TV every month), but preferences in terms of the audiovisual transfer mode to be used in wildlife documentaries differ: 30.46% prefer subtitling, 44.64% prefer dubbing, and 25% prefer VO. These preferences are correlated with age: participants under 40 prefer subtitled documentaries (50%), whilst participants over 40 prefer voiced-over documentaries (46.3%). 4.2. Materials The materials used for the first level were 6 translations and post-editings of two selfcontained excerpts of a 7-minute wildlife documentary film titled Must Watch: a Lioness Adopts a Baby Antelope that is currently available on Youtube as an independent video (http://www.youtube.com/watch?v=mZw-1BfHFKM). It is part of the episode Odd Couples from the series Unlikely Animal Friends by National Geographic broadcast in 2009. Short excerpts were chosen for practical reasons, despite being aware that this could impact on  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 19  evaluative measures of enjoyment and interest. Additionally, excerpts of a wildlife documentary were chosen since documentaries follow structured conventions and have specific features in terms of terminology (Matamala, 2009). The translations and post-editings (24 in total) were produced by 12 students of an MA on AVT that had had a specific course on VO but no, or almost none, previous experience on post-editing. Hence, they were instructed to correct all the errors and adjust, only if necessary, the text according to the specific constrains of documentary translation. Participants worked in a laboratory environment that recreated current working conditions: they used a .doc document and they were allowed to use any available resources (internet, dictionaries, etc.) To perform both tasks, students were given a maximum of 4 hours, although almost none of them used the entirety of the given time. The audiovisual excerpts were similar in terms of length (first excerpt: 101 seconds, 283 words; second excerpt: 112 seconds, 287 words) and content, and the translations and post-editings contained between 218 and 295 words. They were machine translated through Google Translate, the best free online MT engine to be used to machine translate wildlife documentary scripts according to Ortiz-Boix (forthcoming). For the second level, the best post-editing and the best translation of each excerpt was selected, according to the results of the first-level quality assessment. The recordings of these excerpts were used for the third-level assessment.  4.3. Test Development  Level 1: Experts’ Assessment. Participants carried out the experiment from their  usual place of work. They were given detailed instructions on how to assess the 24 documents  without knowing which of them were translated or post-edited. They were given 20 days to  perform the whole assessment. The experiment was divided into three evaluation rounds:  a)  In round 1, raters were instructed to read each document and grade it according to  their first impression on a 7-point scale (completely unsatisfactory-deficient-fail-pass-good-  very good-excellent). They were just given one day for this task, and the order of the  documents was randomized across participants.  b)  In round 2, raters were asked to correct the documents following a specific  evaluation matrix (see section 4.4.), and grade them after the correction on a 7-point scale.  Afterwards, they had to answer an online questionnaire (see section 4.5.).  c)  In round 3, a final mark between 0 and 10, following Spain’s traditional marking  system, was requested.  There was also a final task in which raters had to guess whether the assessed document was translated or post-edited (post-editing/translation identification task). Level 2: Dubbing Studio Assessment. The scripts and videos were sent to the dubbing studio and a professional recording was requested from them. They were instructed to follow standard procedures. A researcher took observational notes and gathered quantitative and qualitative data on the changes made during the recording session by the dubbing director. Level 3: End-Users’ Assessment. Quality was understood to be based on end-user reception and, following Gambier’s proposal (2009), three aspects were assessed: understanding, enjoyment, and preferences (or response, reaction and repercussion in Gambier’s terms). Participants were invited to a lab environment that recreated the conditions in which documentaries can be watched: they sat in an armchair and watched the documentary excerpts in a 32' flat screen. Taking into account ethical procedures approved by Universitat Autònoma de Barcelona’s ethical committee, participants were administered a pre-task questionnaire (see section 4.6.). They were then shown two of the excerpts without  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 20  knowing whether they were watching a translated or post-edited excerpt. After each viewing, a questionnaire was administered to them to test their comprehension and enjoyment, as well as their preferences (see section 4.6).  4.4. Evaluation Matrix (Level 1) The evaluation matrix applied in the first level is based on MQM because it can be used for both translations and post-editings, and it also allows to select and add only the relevant categories for our text type. Although MQM offers the possibility to include over one hundred issue types, only five categories and eleven subcategories of issue types were selected, as shown on Table 1.  Issue types categories Issue types subcategories  Wrong Translation  Adequacy  Omission Addition  Non-translated words  Register  Style  Inconsistencies  Fluency  Spelling  Typography  Grammar  Others  Variety  Spotting  Voice-over/off-screen dubbing specificities  Action and kinetic synchronies Phonetic transcriptions  VO Isochrony  Design/Layout  Others  Table 1. Evaluation matrix: error typology  The selection was based on previous research on errors produced by MT engines in general texts (Avramidis et al, 2012) and wildlife documentary films (Ortiz-Boix, forthcoming), as well as in post-editings (Guerberof, 2009). As MQM does not contain a domain specific issue type for audiovisual translated texts, a new category was added: VO/DO specificities. It includes the issue types subcategories spotting, action and kinetic synchrony, voice-over isochrony, and incorporation of phonetic transcriptions. Raters were trained on how to apply the evaluation matrix.  4.5. Questionnaire design (Level 1) The questionnaire in level 1 aimed to gather the agreement of the raters with eight statements assessing fluency, grammar, spelling, vocabulary, terminological coherence, voice-over specifications, satisfaction, and success in terms of purpose, using a 7-point Likert scale:   In general, the text was fluent.  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 21   In general, the translation was grammatically correct.  In general, there were no spelling issues.  In general, the vocabulary was appropriate.  In general, the terminology was coherent throughout the text.  In general, the translation met the VO and DO specificities.  In general, the final result was satisfactory; aka the translation met its purpose.  In general, the translation could be sent to the dubbing studio to be recorded. 4.6. Questionnaire design (Level 3) The pre-task questionnaire included five open questions on demographic information (sex, age, highest level of studies achieved, mother tongue, and other spoken languages) as well as seven questions on audiovisual habits. The post-task questionnaire included seven questions on enjoyment. Participants had to report their level of agreement on a 7-point Likert scale on the following statements:  I have followed the excerpt actively.  I have paid more attention to the excerpt than to my own thoughts.  Hearing the Spanish voice on top of the original English version bothered me.  I have enjoyed watching the excerpt. They also had to answer the following questions on a 7-point Likert scale:  Was the excerpt interesting?  Will you look for more information regarding the couple presented on the documentary?  Would you like to watch the whole documentary film? They were also asked 3 questions on perceived quality and comprehension, again on a 7-point Likert scale:  The Spanish narration was completely understandable.  There were expressive problems in the Spanish narration.  There were mistakes in the Spanish narration. Five additional open questions per excerpt were used to test comprehension. Finally, participants were asked which excerpt they preferred. A pilot test was run to validate the questionnaire, which was inspired by Gambier (2009). 4.7. Data and Methods The following data were obtained: Level 1 (experts): 1) 144 documents with corrections (6x24) according to the MQM-based evaluation matrix. 2) The grades for each document in the three scoring rounds. 3) 144 completed questionnaires (6x24 documents) reporting on the participants' views after correcting each document. 4) The results of the post-editing/translation identification task. Level 2 (dubbing studio):  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 22  5) 4 documents with corrections (1x4) made by the dubbing director and their corresponding recordings. 6) Observational data gathered during the recording session.  Level 3 (end-users): 7) 56 completed questionnaires on demographic aspects and audiovisual habits. 8) 112 completed questionnaire responses (14x4) on user enjoyment, comprehension and preferences. In order to analyse the comprehension questionnaire, wrong answers were given 0 points, partially correct answers were assigned 0.5 points and correct answers, 1 point. All data were analysed using the statistical system R-3.1.2, developed by John Chambers and colleagues at Bell Laboratories. In this study, data was analysed according to descriptive statistics.  5. Discussion of Results  Results are presented according to the three levels of assessment. More attention is devoted to levels 2 and 3, as a more detailed analysis of the first level is already presented in Ortiz-Boix and Matamala (forthcoming). 5.1. Quality Assessment by experts1 The quality of both translations and post-editings was rather low and no meaningful differences between post-editings and translations in terms of quality were found, as the difference between the scores for each of the tasks were low. Results are discussed in two different sub-sections: in the holistic approach, the scores given in the evaluation rounds, the questionnaire replies and the identification task results are analysed. The analytic approach discusses the results of the corrections performed by the raters.  5.1.1 Holistic Approach  Results of round 1  indicate  that  experts evaluate  Passes for Round 1 Passes for Round 2  better translations  than post-editings  after reading the Translations  45  41  documents for the  first time: while 45 Post Editings  37  38  out of 72 (62.5%)  translations were  evaluated from Total Possible  72  72  "pass"  to  "excellent", only 37 out of 72 post-  Table 2. Pass marks for round and task  editings (51.39%) were evaluated within this range. However, when documents are rated  again after a thorough correction (round 2), the difference between post-editings and  
In this paper, we report on a post-editing study for general text types from English into Dutch conducted with master's students of translation. We used a fine-grained machine translation (MT) quality assessment method with error weights that correspond to severity levels and are related to cognitive load. Linear mixed effects models are applied to analyze the impact of MT quality on potential post-editing effort indicators. The impact of MT quality is evaluated on three different levels, each with an increasing granularity. We find that MT quality is a significant predictor of all different types of post-editing effort indicators and that different types of MT errors predict different post-editing effort indicators. 1. Introduction In recent years, machine translation (MT) and its subsequent post-editing have become more widely accepted in the translation industry. Especially when it comes to technical texts, machine translation has proven its worth, with companies like Autodesk reporting on productivity increases in comparison with human translation ranging from 20 to 131%, depending on the language combination and translator (Plitt & Masselot, 2010). The main goal of postediting research is no longer finding out whether or not post-editing can be used, but rather finding out when it cannot be used, and how machine translation systems can be improved to better suit post-editors' needs. While post-editing is generally assumed to be faster than human translation, speed is not the only factor that should be taken into account when assessing the post-editing process. More recent studies have looked at ways of determining post-editing effort. This knowledge can be used, on the one hand, to improve the quality of MT systems, and, on the other hand, to reduce post-editors' frustration by only presenting them with a segment containing MT output when the effort required to post-edit that segment is not too high. Krings (2001) mentioned three levels of post-editing effort: temporal effort, or the time needed to post-edit a given text, cognitive effort, or the activation of cognitive processes dur-  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 31  ing post-editing, and technical effort, or the technical operations such as insertions and deletions that are performed during post-editing. According to Krings (2001), post-editing research should concentrate on causes and manifestations of post-editing effort with a focus on cognitive effort: "The type and extent of cognitive processes triggered by the post-editing task must be defined qualitatively and quantitatively, and correlated to the corresponding deficiencies in machine translations as triggering factors" (p. 182). In this paper, we will first discuss some previous work on the effort involved in postediting and the problems that arise when trying to measure cognitive effort in isolation. We then present the results of our study, examining the impact of different types of machine translation errors on post-editing effort indicators with student translators post-editing from English into Dutch. 2. Related research The ultimate goal of post-editing process research is predicting how much effort a post-editor will need to correct a segment before presenting the post-editor with that segment. Depending on the expected effort, a translator can then be given MT output to post-edit whenever the effort to post-edit would be lower than the effort needed when translating that segment from scratch. Two aspects need to be researched in order to reach that ultimate goal: firstly, we need to establish which types of effort we take into account and how we can objectively measure them, and secondly, we need to find ways of predicting effort on the basis of elements contained in either the source text or the MT output. Both aspects will be discussed in the following paragraphs. A number of potential post-editing effort indicators have been introduced in previous research. The distinction between temporal, cognitive and technical effort as proposed by Krings (2001), however, does not seem to be a clear distinction. While temporal effort seems the easiest to measure, as it is simply the time needed to translate a word, segment or text, Koponen et al. (2012) found evidence that post-editing time can also be an indication of cognitive effort. They use a cognitively motivated MT error classification created by Temnikova (2010), but finish their paper with a few remarks on the classification and a suggestion for future work: "A revised set of error categories with more detailed error types (...) is also an interesting direction to help understand the cognitive load in post-editing" (p. 20). Koponen et al. (2012) also looked at a technical effort indicator - keystrokes - and its relationship to cognitive load. However, they found that keystrokes were influenced more by individual differences between participants than by cognitive load. We therefore decided not to include keystrokes as such in our analysis. Related to keystrokes are production units, or sequences of coherent typing activity. Although producing translation output in itself is clearly a technical activity, Lacruz et al. (2012) intuitively felt that an increase in the number of complete editing events (which correspond to the notion of production units) would lead to an increase in cognitive demand as well, making it a cognitive effort indicator in addition to a technical effort indicator. The question remains whether editing events really correspond to cognitive effort. For example, many spelling errors or adjective-noun agreement errors will require quite a few (local) editing events, but are not really difficult to solve. Lacruz et al. (2012) further introduce the average pause ratio (the average time per pause in the segment divided by the average time per word in the segment) as an answer to O'Brien's pause ratio (2006) - the total time in pauses divided by the total editing time. O'Brien (2006) did not find conclusive evidence for a relationship between pauses and cognitive activity. Lacruz et al. (2012) argue that pause ratio is not sensitive enough as a measure for  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 32  cognitive activity, as it does not take average pause length into account. We include both pause measures in our study, to establish whether or not they can both be used, and whether or not they are indicators for different causes of effort. Lacruz et al. (2012) found a relationship between average pause ratio and the number of production units. As production units are delimited by pauses, and the average pause ratio is influenced by the number of pauses, perhaps this finding is related more to intrinsic correlation than to actual impact of cognitive load on pause behavior, although the relationship is most likely more complex. We will look at production units and average pause ratio in isolation to better understand the differences and similarities between both variables. Some of the few effort indicators that seem to be exclusively related to cognitive postediting effort, are the average fixation duration and the number of fixations. Building on the eye-mind hypothesis from Just and Carpenter (1980), a person is cognitively processing what they are looking at. Longer fixations should thus be an indication of more cognitive processing. This assumption was confirmed by Jakobsen and Jensen (2008), who found longer average fixation durations and a higher number of fixations as the complexity of the task increased from reading to translation. Doherty and O'Brien (2009), however, found a higher number of fixations for bad MT output than for good MT output, but they did not find a significant difference between the average fixation durations for both types. We will include both average fixation duration and number of fixations as potential cognitive post-editing effort indicators. From the abovementioned research, it becomes clear that the distinction between the different types of effort indicators is not always easily made. Correlations are identified between different indicators without really knowing whether or not they measure different things. To avoid this circular thinking, we need to find a way of studying the post-editing effort indicators in isolation, by linking them to source text and MT output characteristics rather than to other post-editing effort indicators. O'Brien (2004) has taken a step in this direction by looking at negative translatability indicators (NTIs) in the source texts, or elements that can reasonably be considered to be problematic for MT systems, for example, long noun phrases or gerunds. Although some NTIs indeed seem to have an impact on post-editing effort, there are some NTIs that have no effect, and O'Brien (2004) further found post-editing activity in segments that did not contain NTIs. From these findings, we can derive that NTIs do not conclusively predict post-editing effort, and perhaps another focus is needed. In this paper, we take a look at a fine-grained MT quality assessment and whether or not the average MT error weight of a segment has an impact on the post-editing process. In line with previous research, we take a look at different types of post-editing effort indicators. 3. Methodology 3.1. Participants Participants were ten master's students of translation. All of them were native speakers of Dutch. They had no previous experience post-editing and had passed their final English General Translation exam. They received two gift vouchers of 50 euros each. As we are working with students, it is of course hard to say whether our results will generalize to the professional translation process. However, we have repeated the experiment with professional translators (but the process data has not yet been analyzed), and we found no significant differences in proficiency or attitude towards post-editing between the two groups, so perhaps they are more comparable than often thought.  Proceedings of 4th Workshop on Post-Editing Technology and Practice (WPTP4)  Miami, November 3, 2015 | p. 33  3.2. Text selection The present study is a part of a larger study aimed at comparing the differences between the human translation process and the post-editing process for students and professional translators for general text types. In the present study, the focus will be on the post-editing process of the students only, but the texts have been selected with the larger study in mind. Originally, fifteen different English newspaper articles were selected from newsela.com, a website providing newspaper articles at different levels of complexity, as indicated by a Lexile score. We selected articles with the same level of complexity, i.e., Lexile scores between 1160L and 1190L1, to try to control for textual differences in our studies. Each article was reduced to its first 150-160 words, and then analyzed for additional readability measures and potential translation problems. Texts with on average less than fifteen or more than twenty words per sentence were discarded, as well as texts that contained too many or too few complex compounds, idiomatic expressions, infrequent words or polysemous words. Sentence length ranged from seven to thirty-five words, with an average of eighteen point three, and a median of eighteen words per sentence. The texts were then translated into Dutch by the statistical machine translation system Google Translate. We annotated the MT output for quality, as will be discussed in section 3.4. From the original fifteen texts, the eight texts that were most comparable in difficulty - based on the potential translation problems and MT output quality - were retained. Texts have different subjects and don’t require specialist knowledge to be translated. 3.3. Experimental setup Two sessions were planned for each participant. During the first session, students had to first fill out a survey, and take a LexTALE test (Lemhöfer & Broersma, 2012) to be able to take their English proficiency into account. This was followed by a copytask and a warmup task combining both post-editing and human translation, so the students could get used to the tools and different types of tasks. After the warmup, students post-edited two texts and translated two texts from scratch. During the second session, students again started with a warmup task, followed by two human translation tasks and two post-editing tasks. The order of texts and tasks was balanced in a Latin square design across all participants, to reduce task order effects. The second session ended with a retrospective part, during which students could highlight elements in the text that they found most difficult to translate or post-edit, and another survey to measure how students experienced the experiment and the different tasks. To be able to look at different aspects of post-editing effort, we used a combination of keystroke logging tools and eye tracking. The process itself was registered by the CASMACAT translator's workbench (Alabau et al., 2013), which looks like an actual translation environment to improve ecological validity, yet contains keystroke logging and mouse tracking software for researchers to better be able to observe the translation and post-editing process in detail. The texts were presented to the students one by one, and each text was subdivided in translation segments, corresponding to sentences in the source text. The number of segments in each text ranges from seven to ten. A plugin connects Casmacat to the EyeLink 1000 eyetracker that was used to register the students' eye-movements while translating and post-editing. In addition to these tools, an extra keystroke logging tool, Inputlog (Leijten & Van Waes, 2013) was running in the background. While the CASMACAT software is capable of performing a detailed logging within the CASMACAT interface, it cannot log external applications. Inputlog registers when and which applications other than CASMACAT are 
WIPO has access to a huge amount of patent application texts in different languages, therefore, we have built a machine translation tool (called WIPO translate) trained on big parallel data. We focus on offering quality machine translation to the general public, WIPO translate is fully integrated on our search engine PATENTSCOPE1. We have recently experimented aligning patent full-texts (description and claims) and our tool can now be trained on billions of words. Automatic evaluation metrics show an improvement over publicly available translation sites for the translation of patent texts (e.g. Google Translate, Microsoft translate). We have developed specific user interfaces, which are fully integrated, in our search engine PATENTSCOPE with reasonable translation speed. WIPO translate has now reached maturity in providing translation with competitive scalability, quality and usability. 1. Introduction WIPO has 5 years’ experience in providing quality machine translation on its search engine PATENTSCOPE. Originally trained exclusively on patent titles and abstracts, we have now experimented using descriptions and claims (full text) to train our statistical machine translation tool (called WIPO translate), based on the open source toolkit Moses. Machine translation of patent texts is now integrated in PATENTSCOPE, despite the issues of scalability (translation models trained on billions of words), quality (our automatic evaluation shows an improvement over publicly available translation sites: e.g. Google Translate) and usability (it is fully integrated in our search engine PATENTSCOPE, with a translation speed of less than 2 seconds per sentence). 2. Background The World Intellectual Property Organization (WIPO) provides access to about 50 million patent applications on its search engine PATENTSCOPE. It includes the international Patent Cooperation Treaty (PCT2) applications, but also documents of participating national and regional patent offices. All the PCT applications must be filed in one of the following languages: Arabic, German, English, Spanish, French, Russian, Japanese, Korean, Portuguese or Chinese, then the title and abstract are translated into English and/or French. The description and the claims remain available only in the original filling language. 
Recently, each Intellectual Property (IP) Office has faced common big challenges: promoting the international work sharing in the examination process and developing an environment to access foreign patent documents written in languages other than its native language. As a means of resolving such challenges, the Japan Patent Office (JPO) has actively utilized machine translation. Currently, the JPO has widely provided the foreign general users with the Japanese-English machine translated information on its patent examination results through the “One Portal Dossier” allowing them to retrieve dossier information for applications filed with the IP5 Offices (Japan, the U.S., Europe, China, and the Republic of Korea). Also, the JPO launched a new system in January 2015, utilizing the Chinese-Japanese machine translation dictionary including more than 2 million words. This system enables users to search in the Japanese language more than 12 million machine translated patent and utility model documents of the Chinese and Korean languages. 1. Introduction The number of patent applications filed worldwide greatly increased to 2.57 million in 2013, compared with 1.58 million in 2004. Especially, the number of patent applications filed in China has dramatically increased and grew to 32.1% of the total applications in 2013 from 8.3% in 2004. From a viewpoint of searching patent information, this situation shows that the needs for searching foreign documents including those of the Chinese languages have increased. That is, as the examiners judge the novelty of inventions, inventive step, etc. based on the result of prior art search in the patent examination, it is necessary to search prior art documents including these foreign patent documents precisely as well as efficiently in order to grant a stable right recognized in the world. Actually, as the rate at which patent documents of the Chinese languages are cited as prior art in the examination has gradually accelerated at each Intellectual Property (IP) Office, it has become a challenge common to all IP Offices to improve an access environment such as understanding and searching foreign patent documents written in languages other than its native language. On the other hand, it is not easy to understand and to exhaustively search foreign patent documents around the world. Needless to say, while translation enables us to understand them in our native languages, translating quite a large number of documents only by human has some limitations in terms of costs and resources, as the number of patent applications filed worldwide has increased year by year. Under this situation, the expectation of im-  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 9  provement of the enviorment for searching foreign documents by utilizing machine translation is getting higher. Furthermore, it is hoped that machine translation will be utilized in order to disseminate patent information to other IP Offices. Currently, while intellectual property related activities are getting globalized, applications for a patent of one invention are more often filed in several countries or regions of the world. In case applications for a patent of the same invention are filed in several countries or regions, examiners in each IP Office need to share examination information (dossier information) owned by each IP Office in order to eliminate inefficiencies of conducting duplicate examinations or searches. But if each IP Office provides its examination information in its native language, it is difficult for other IP Offices to utilize such examination information. In order to promote the international work sharing, it is important to disseminate not only information in its native language but also one translated into other languages such as English. In view of this situation where attention is getting focused on utilizing machine translation in the area of patent information, the Japan Patent Office (JPO) has taken measures to actually utilize machine translation. In this paper, I will introduce a search system for the Chinese and Korean documents utilizing machine translation and a service to provide examination information which the JPO has provided. 2. Improvement of the Environment for Searching Patent Documents of the Chinese and Korean languages 2.1. The Chinese and Korean Gazette Translation and Search System (CKGS) As shown in Figure 1, in 1998 the documents to be understood and searched in the Japanese language accounted for 55% of the total patent documents issued in the world. But recently as the number of patent documents of the Chinese language has rapidly increased, documents to be understood only in the Chinese and Korean languages have accounted for 65% of the total. In January 2015, the JPO launched a new service of the “Chinese and Korean Gazette Translation and Search System” (CKGS) which enables users to search in the Japanese language patent and utility model documents of the Chinese and Korean languages in order to enhance the convenience of the environment for searching them. A rule-based translation method is adopted for this system so that important technical terms can be translated and searched properly. The system stores patent and utility model documents machine translated from Chinese and Korean to Japanese, so that Japanese key words can be used for the full-text search of these documents. As of the end of March 2015, the system can be used to search about 12 million Chinese and Korean patent and utility model documents and it is planned to increase the number of such searchable documents gradually in the future. The quality of texts machine translated by the system plays an important role if the CKGS is to be used for practical applications. To this end, the JPO has been conducting projects since Fiscal 2012 to develop a specialized dictionary for patent terms which will be used for the system’s machine translation, as its initiatives to improve machine translation quality (JPO, 2013, 2014a, 2015). Specifically, corresponding Japanese and Chinese sentences are extracted from the patent family of a same invention applied to Japan and China in order to prepare Chinese-Japanese translation corpuses as well as to develop a specialized dictionary. Currently, about 153 million Chinese-Japanese translation corpuses and the specialized user dictionary containing more than 2.2 million words have been developed. The user dictionary is installed to the CKGS to improve the system’s translation quality.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 10  Number of Documents  2,200,000 2,000,000 1,800,000  JP US EP KR CN Other  CN and KR 65%  Other  1,600,000  1,400,000 CN 1,200,000  1,000,000  CN and KR 17%  800,000  600,000  KR  EP  400,000  US  200,000 0  JP 55%  JP 13%  JP  1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 Year  Figure 1. Rapid increase in Chinese patent documents. Note: Patent (incl. utility model) documents issued worldwide are categorized by language and duplicated data are eliminated. Regarding patent documents for the same invention which have been filed and published at multiple Offices, those documents published in Japanese are counted as JP. In case of no Japanese publications, such documents are counted first as US (English) , secondly as EP (English, French, German) ,thirdly as KR (Korean) and last as CN (Chinese), if each language is applicable.  Furthermore, measures are taken to enable the system to handle new technical terms as they appear. Specifically, based on information concerning unknown words which are detected as those not registered in the machine translation dictionary during the system’s machine translation and that concerning incorrect translations and other errors reported by the system’s users, efforts are made to update the dictionary, make additional registrations to the system’s translation memory, and tune the translation engine parameters. With these efforts, the dictionary and the translation engine will be improved so that new technical terms can be handled as they appear. In addition, through the Internet, the CKGS has become available not only to examiners but also to general users. As described above, continued efforts are being made to improve the system’s machine translations so that practical application environment for uses to search Chinese and Korean documents can be provided, and the system’s users appreciate it very much.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 11  SIPO Original Text Data  KIPO  Public Users Patent Attorney/ Enterprise/ Research Institute/ Individual  Chinese and Korean Gazette Translation and Search System Machine-Translation Engine of Chinese/Korean into Japanese  Search Internet  JPO  Dictionary Update  Sentence to be translated Quality Improvement  Search Internet  Examiner Appeal Examiner  Figure 2. Conceptual diagram of Chinese and Korean Gazette Translation and Search System. 2.2. Quality Evaluation of Machine Translations from Chinese to Japanese and Those from Korean to Japanese For realizing machine translations to be widely accepted and used, the quality of such translations plays a very important role. As a prerequisite to deciding whether to introduce such machine translations, the quality of such translations must be evaluated properly. A “Survey on How to Evaluate Quality of Patent Document Machine Translations” was conducted in FY 2013 to investigate how the quality evaluation method of machine translated patent documents should be (JPO, 2014b). Based on the survey’s results, “Quality Evaluation Procedures for Patent Document Machine Translations” was developed in Fiscal 2014 as a guideline to evaluate the quality of machine translation results properly (JPO, 2014c). In order to verify the results of the JPO’s efforts to improve the system’s machine translation quality in accordance with the “Quality Evaluation Procedures for Patent Document Machine Translations,” machine translations provided by the Chinese and Korean Gazette Translation and Search Systemwere evaluated in the end of 2014 with regard to how accurately the system could translate technical terms (or the system’s translation accuracy of technical terms) and how well the system could communicate original contents (or the system’s original contents communication level). With regard to the system’s translation accuracy of technical terms, the system’s machine translations of Chinese documents into Japanese were surveyed. A total of 196 technical terms were selected from various fields, and evaluated and classified into the following 4 grades by human evaluators.  A (Properly Translated Word): Compared with one translated by a human, it is a word translated to a technically same or similar meaning and generally used.  B (Acceptably Translated Word): It is not a translation word generally used as a technical word, but its meaning is almost correct.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 12   C (Mistranslated Word): It is a mistranslated word.  D (Untranslated Word): It is a word not translated or whose translation was erroneously omitted. With regard to the system’s original contents communication level, the system’s machine translations from Chinese and Korean to Japanese were surveyed. 100 machine translated sentences of each of the languages were selected from various fields, and evaluated and classified into the following 5 grades by human evaluators.  5: All contents of its important information are communicated correctly (100%).  4: Almost all contents of its important information are communicated correctly (80% or more).  3: No less than half of the contents of its important information are communicated correctly (50% or more).  2: Several contents of its important information are communicated correctly (20% or more).  1: Its translations cannot be understood, or almost no contents of its important information are communicated correctly (less than 20%). In order to verify how effective it was to install a Chinese-Japanese dictionary developed by JPO to the system, the system’s machine translations from Chinese to Japanese were evaluated before and after that dictionary with approximately 1 million words was installed to it for both surveys on the system’s translation accuracy of technical terms and on the system’s original contents communication level. In addition, for the purpose of comparison, the translated sentence using statistic machine translation (SMT) with 100 million Chinese-Japanese translation corpuses were also evaluated in the same manner. Figure 3 shows the evaluation results of the system’s original contents communication level through machine translations from Chinese and Korean to Japanese. While the system’s average original contents communication level for Korean-Japanese translations is above 4 points, one for Chinese-Japanese translations is merely above 2 points. In addition, in all the technical fields, the system’s original contents communication levels for Korean-Japanese translations are higher than those for Chinese-Japanese translations. It was concluded that this was due to the fact that the grammatical characteristics of Korean were closer to those of Japanese than those of Chinese, and the results indicate that continued efforts must be made to improve the system’s translation accuracy of Chinese documents. Furthermore, the system’s translation accuracy of either Chinese or Korean documents varies from one field to another, and measures must also be taken for individual fields. With regard to this, by the end of FY 2015, the JPO plans to measure the system’s translation accuracy of Chinese documents in individual fields, and focus on developing the Chinese-Japanese dictionary for the technical fields whose translation accuracy is low. Figures 4 and 5 show how accurately the system can translate Chinese documents’ technical terms and how well the system can communicate their original contents, before and after the Chinese-Japanese dictionary developed by JPO was installed, respectively. That dictionary being added to the system’s translation engine, the system’s translation accuracy of technical terms increased by about 4%, and the system’s original contents communication level by about 13%. Specifically, the translation accuracy of technical terms by the system’s machine translation based on a rule-based method is higher than one by the SMT.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 13  5  4  3 Chinese  2  Korean  
When evaluating the suitability of MT for post-editing, there are a lot of variables to consider that could have an impact on its overall effectiveness, including: the languages in question, the documents being translated, the translator workflow, and not least the individual translators themselves. Add to that the various ways we can carry out evaluation - automatic measures, subjective assessments of fluency and adequacy, etc. - and we have got a lot of data on our hands. Despite all of these data points, we are ultimately trying to answer a simple question: is the MT useful and to what extent? In this article, we introduce Iconic Translation Machines and describe a case study on a large-scale post-editing evaluation involving more than 20 translators working on Chinese to English patent translation. We discuss how various evaluations were carried out – from initial MT engine development to translator productivity – and discuss the implications of these findings on the real-world application of MT.  1. Introduction  Iconic Translation Machines (Iconic) is a machine translation software and service provider that specialises in domain-adapted MT. We focus on developing MT engines for specialist content types that are particularly challenging for translation and require more than a pure data-driven approach. Our flagship solution, IPTranslator, was an MT service adapted specifically for patent and intellectual property-related content, and in additional to this, Iconic now develops domain-adapted engines in the areas of finance, life-sciences, and e-commerce.  2. Machine Translation with Subject Matter Expertise  Iconic’s approach to MT extends on the classic data-driven approach of statistical MT by incorporating aspects of syntax-based MT and rule-based MT. These approaches are combined with domain-specific processes that have been developed to address particular stylistic conventions of different content types, such as extremely long sentences with complex alphanumber sequences in patent data. Distinct processes, addressing the language, domain, and style, are combined together in Iconic’s Ensemble ArchitectureTM which selects the most effective combination of processes for a particular input type at runtime.  3. RWS : A Case Study  RWS Group is a world-leading language service provider that specialises in patent translation. Iconic worked with RWS to customise a domain-adapted MT engine for Chinese to English  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 19  translation. The goal of using MT at RWS was to improve the productivity of translators through post-editing and this was taken into account during the development and automatic evaluation of the engines. Iconic used its existing baseline for Chinese—English and supplemented it with additional in-domain data suited to RWS content. This adaptation achieved significant improvements in BLEU and TER scores as shown in Figure 1.  0.8   0.6   0.4   Iconic Baseline   0.2   Iconic Customised    0   BLEU   TER   Figure 1 Improvements over the baseline The TER (translation edit rate) scores give a good indication that the MT output is of sufficient quality to facilitate faster post-editing. In the next section, we describe the evaluations we carried out in order to validate and quantify this in a practical, real-world scenario with professional translators.  3.1. Evaluation Setup: mitigating the variables In order to quantify the increase in productivity of translators post-editing MT output as opposed to translating from scratch, we carried out an evaluation using the TAUS Dynamic Quality Framework (DQF)1. Using this tool, translators post-edit and translate (from scratch) alternating segments in a given test document and the amount of time spent on each segment is measured. The total time spent post-editing vs. translating from scratch is calculated, allowing us to calculate the percentage increase in speed between the two tasks (with the assumption that post-editing will be faster). There are a number of variables in such an evaluation that could have an impact on the veracity of the results. These variables, and the steps we have taken in our evaluation setup in order to mitigate their impact, are show below:  • Translator attitude towards MT: Translators may have a certain bias as relates to MT. We used 24 translators for this evaluation to reduce potential noise from outlying results. • Lack of familiarity with the task: Translators were provided with written instructions, an instructional video, and the opportunity to test the tool prior to beginning the task. • Ability of the translator: Some translators may adapt faster to the task of post-editing than others. We used translators with varying levels of experience to reduce this effect. • Difficulty of the test set: A particularly challenging test set for MT/translation could produce skewed results. We used four different test sets to avoid this. Each translator translated more than 200 segments each and the findings are presented in the next section.  
The generation of precise and comprehensible translations is still a challenge in the patent and scientiﬁc domain. In particular, function words are often poorly translated in standard machine translation systems, particularly across language pairs with greatly diﬀering syntax. In this paper we exploit the target-side structure in tree-totree machine translation to post-edit function words automatically using a tree-based function word language model. We show that a signiﬁcant improvement in human evaluation can be achieved with our proposed method. 
This paper presents a novel hybrid system, which combines rule-based machine translation (RBMT) with phrase-based statistical machine translation (SMT), to translate Chinese patent texts into English. The hybrid architecture is basically guided by the RBMT engine which processes source language parsing and transformation, generating proper syntactic trees for the target language. In the generation stage, the SMT subsystem then provides lexical translation according to the defined structures and generates final translation. According to our empirical evaluation, the hybrid approach outperforms each individual system across a varied set of automatic translation evaluation metrics, verifying the effectiveness of the proposed method. 1. Introduction As one of important applications in Natural Language Processing (NLP), Machine translation (MT) has developed several paradigms in past decades, basically including rule-based MT (RBMT) and statistic-based MT (SMT). Both the two approaches have strengths and weaknesses. RBMT systems tend to produce better translations and deal with long distance dependencies, agreement and constituent reordering in a more principled way (Gorka et al., 2014), since they perform the analysis, transfer and generation steps based on syntactic principles. However, they usually have problems in word translation selection preferences, which usually have negative impacts on the translation quality. Also, in cases in which the input sentence has an unexpected syntactic structure, the parser may fail and the quality of the translation will decrease dramatically.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 52  Contrary to RBMT, SMT models are more robust and usually better in fluent lexical selection since they exploit explicit probabilistic language models trained on very large corpora (Xuan et al., 2012). On the downside, SMT has difficulties in dealing with requirements of linguistic knowledge, such as syntactic functions and long distance word reordering, especially in the translation between distant language pairs such as Japanese and English (Isozaki et al., 2010), which may generate translations with improper even worse structures. While SMT has been recognized as the main stream approach of translation, RBMT has tended to be more effective for limited subject domains than SMT (List, 2012). As a result, hybrid MT (HMT) models have become increasingly popular in recent years, aiming to improve final translation effects and qualities. An typical example that can reflect the the increasing interest in hybrid approaches to MT is the workshop on hybrid approaches to translation (Hytra), which was first held at the EACL2012 conference, since then, it was continuously held in 2013 and 2014, in this year, it took part in conjunction with the ACL2015 conference held in Beijing. It is well known that MT can be applied to various domains. With continued growth in the number of patent applications and the need of exchanging related information, patent domain MT has become one new application of MT, and attracted worldwide attentions of researchers and governments. In this article, we present a novel hybrid translation combination architecture thay takes advantage of RBMT and phrase-based SMT to translate Chinese patent texts into English. As juridical and official documents, Chinese patent documents are usually featured by formal fixed expressions, and much longer sentences with more complex syntactic structures, compared with SMT, rule-based method is more suitable to describe the structures more precisely. Thus, our HMT system is constructed based on the RBMT system (Zhu and Jin, 2012). The main idea is that the RBMT guides main steps in performing source language parsing and transfer, generating proper transferred and reordered syntactic trees for the output, and the SMT system “Moses” (Koehn et al. 2007) then helps the lexical selection by providing more alternative translations according to the trees for target language generation. The final decoding also accounts for fluency by using language models. Since the structures of the translation are already decided by the RBMT subsystem, decoding of SMT will be more fast and efficient in turn. We performed some experiments on the HMT system with several automatic evaluation metrics to test its performance. After comparing the HMT with individual RBMT and SMT systems, as well as Google online translation, the HMT outperformed all individual MT systems and gained much improvements in evaluation metrics, indicating the hybrid approach is indeed beneficial and effective for translation qualities. The rest of the article is organized as follows. Section 2 overviews the related literatures on MT system combination and hybridization. Section 3 presents the individual systems and the architecture of system combination in detail. Section 4 describes the experimental work carried out with the hybrid architecture and discusses the obtained results. Finally, Secetion 6 concludes the work. 2. Related Work This section mainly includes two parts: the first part overviews syntactic reordering in MT and the second part will discuss some previous work on MT system combination. 2.1. Syntactic Reordering In SMT, reordering positons of chunks in source languages to generate proper and acceptable translation has been a hot issue, and syntactic reordering is effcetive in improving the performance of MT. Xia and McCord (2004) proposed an approach for French-English  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 53  translation by automatically extracting rewrite patterns after parsing the source and target sides of the training corpus. Collins et al., (2005) described a method for reordering German clauses in German-English translation. Some lexicalized reordering models(Tillman, 2004; Galley and Manning, 2008; Cherry et al., 2012 ) were employed to predict reordering by taking advantage of lexical information. Different with lexicalized models, a hierarchical phrase-based translation model (Chiang, 2007; Nguyen and Vogel, 2013) based on synchronous grammar was also used in reordering the chunks. For Chinese-English MT, Wang et al., (2007) described a set of syntactic reordering rules that exploited systematic differences between Chinese and English word order and introduced a reordering approach. Zhang et al., (2007) described a sourceside reordering method based on syntactic chunks for phrase-based statistical machine translation. The source language sentences were first shallow parsed. Then, reordering rules were automatically learned from source-side chunks and word alignments. During translation, the rules were used to generate a reordering lattice for each sentence. Cao et al., (2014) proposed a novel lexicalized reordering model which is built directly on synchronous rules. For each target phrase contained in a rule, they calculated its orientation probability conditioned on the rule. Based on a set of dependency-based preordering rules, Cai et al., (2014) presented a dependency-based preordering approach for C-E MT, improved the BLEU score by 1.61 on the NIST 2006 evaluation data. 2.2. MT System Combination System combination has been shown to improve classification performance in various tasks in the field of NLP (Rosti et al., 2007). Frederking and Nirenburg (1994) first applied system combination to MT. They integrated outputs of three different translation system (knowledgebased MT, example-based MT and a lexical transfer system) with Chart Walk Algorithm, then performed post-editing processing on the integrated outputs to generate final translation results. Bangalor et al. (2001) introduced recognizer output voting error(ROVER) (Fiscus, 1997) into MT, using a multiple string alignment (MSA) approach to align the hypotheses together, their experiments proved that integrated output was better than single system translation. Since then, system combination has aroused more attention around the world. Confusion networks is one of the common methods used in combination strategies, which try to combine fragments from a number of different systems and use consensus network decoding to search for the best output from a list of n-best translations (Bangalore et al., 2001; Matusov et al., 2006; Chen et al., 2008; Ayan et al., 2008). In most hybrid systems, the statistical components are usually selected as basic skeletons and in charge of the translation, correspondingly, the companion system provides complementary information. On the other hand, hybrid architectures where the RBMT system leads the translation and the SMT system provides complementary information to adjust the output from the RBMT, has been less explored. Such systems are applied to relative small domains (Simard et al., 2007), the output tends to be grammatical, and the main effect of the combination is an increase in lexical selection quality (Dugast et al., 2007). Following are some typical works led by RBMT in patent domain. Jin (2010) proposed a hybrid approach which combined semantic analysis with rulebased method to translate Chinese patent to English. Alexandru et al., (2011) conducted some experiments on English–French patent domain adaptation of the MT systems used in the PLuTO project, both manual and automatic evaluations showed a slight preference for the hybrid system over the two individual baseline engines. Enache et al., (2012) also presented a system for English-French patent translation on the basis of large scale corpora with statistic method. Sheremetyeva (2013) discussed a Russian-English patent MT system which integrated  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 54  hybrid and rule-based components for several complementary levels of output. There also exists some hybrid systems participating the patent evaluation workshop of the NTCIR conference held in Japan (Isao et al., 2013). Unlike many previous works using SMT system as the basic skeleton and adapting confusion networks, our hybrid system, oriented for patent domain, is constructed based on the RBMT system and does not involve a confusion network. In the RBMT, we build a considerable knowledge base and manually write rules to help the system analyse and reorder the source sentences according to the grammatical expressions in target language, and the SMT is responsible for the target words selection. As a result, the hybrid system can guarantee both proper syntactic structures and lexical selection qualities that are consistent with target language. The hybrid system will be clarified in detail in following sections. 3. System Architecture Before presenting the system combination architecture, we need to first introduce the individual RMBT and SMT. 3.1. RBMT The RBMT engine is based on the traditional translation model which is mainly divided in three steps: (i) analysis of the source language into syntactic-semantic tree structures, (ii) transfer and transformation from source language to target language, and (iii) generation of the target language. It is well known that rule-based approach is featured by the knowledge base and rules which can describe linguistic information. In the system, we have built a considerable knowledge base with more than 50,000 words which cover most patent texts. In the knowledge base, the words are annoated with various syntactic and semantic information. We also manually wrote numerous formal targeted rules to help the engine process the sentences in each step. These rules provide a hierarchical parsing and reordering access to deal with various structures and chunks in source sentences. By using the information both in the knowledge base and the rules, the MT system will finish the processing of three steps. We will describe each of the stages in the following. Analysis of Source Language As mentioned, sentences in patent texts are usually much longer. A sentence (S) ended with a full stop may include several sub-sentences (marked as SS) and chunks separated by punctuations (marked as SST, most are commas, colons and semicolons also included). That is, S = SS1, SS2……SSn. Considering the expression features of patent documents, a parser is specially developed for the patent texts and integrated into the translation engine, aiming to regards a whole long sentence as the basic processing unit. The analysis is conducted in three syntactic levels: first, the sentence is separated into several SS according to the punctuations; then, parsing each SS into chunks served as direct componts of SS, including identifying the subject, predicate verb, object and adverbial etc.; last, further anlyse the chunks into terminals (leaf nodes on syntactic trees). Thus, S is the root of sentence, and it has several SS nodes and separators SST, then each SS node is composed of several chunks such as NP, VP, and adverbial phrases (ADVP) etc, further, chunks are composed of terminal nodes. In the first level, the main purpose is to divide the complex long sentence into several SS mainly by commas. But not all commas can seperate the sentence, because some of them may follow by phrases. In our research, we determine that, for commas following NP and ADVPs, they cannot separate the sentence, and they will be marked as DBT.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 55  In the subsentence level, the system parses the subsentences to get the syntactic components. Parsing rules basically include rules for indentifying predicate verb, ADVP, special NP with long modifiers, and prepositional phrases (PPs) introduced by unique prepositions (such as “把BA” “被BEI” “将JIANG”,etc.)in Chinese. We want to mainly discuss about identification of predicate verbs, which play more important role in parsing. As Chinese lacks necessary lexical changes, when severel verbs appear in the same sentence, it is usually more difficult to identify the proper core verb. During the beginning stage of the identification process, we first write some rules according to the context information to exclude some verbs that cannot be selected as core verb, next, we then design various high and low weights for remaing possible verbs, the weights represent the possibilities that verbs serve as predicate. When matching kinds of rules, the verbs will be added with corresponding weights, as a result, after comparing the weights, the verbs with the highest weight will be selected as final predicate verb. The final stage is chunk-level parsing. In this level, the system will continue to analyse the non-terminal chunks into leaf nodes. Since some chunks may contain complex and nested structures, the system needs to exploit the rules in a circular manner and perform the syntactic analysis hierarchically until each node is parsed. After finishing parsing, the system will generate a syntactic tree for each source sentence, and each node on the tree possesses several marks and symbols representing various syntactic and semantic information. Here is an example sentence in patent texts, including two subsentences, which followed by the syntactic tree. E.g.1: Source sentence: 在上述结构中，单电池由突起部支撑，因此可以提高耐振 动性。 Target sentence: In above structure, the single cell is supported by the protrusions, therefore the vibration resistance can be improved.)  Figure 1. Syntactic Tree of the Example Sentence Fig.2 shows the form of the tree of the example sentence in our MT system. It can be seen from the vertical tree, there exists some orange circles under each subsentences, and each of them represents the direct component chunk of the subsentence, symbols in the angle brackets “< >” indicate syntactic information of the chunks and punctuations, and when click the “+”buttern in front of some circles, the chunks will extend and show the detailed information of each terminal under the chunk.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 56  Figure 2. Tree Structure of the Example Sentence in Our MT System Transfer from Source Language to Target Language When transforming Chinese sentences into English, it is necessary to reorder and transfer chunks and words to guarantee fluent and proper expressions. Corresponding to the analysis phases above, the transformation process also includes three levels in a top-down order: ① transformation of relationships between subsentences, ②structural reordering of chunks in subsentences and ③reordering inside the chunks. Basic operations in the reordering include add or delete nodes, chunks position adjustment etc. In the following, we will introduce the three stages with some rules and examples. Transformation between subsentences mainly refers to transfering the source sentences into expressions commonly used in target language according to the semantic relationships between subsentences. Rule1: {SS1&CHN[在于, 包括]&END%}+ CHN[，]+ SS2SS1+ that + SS2 The rule means that, if the Chinese characters(CHN) such as “在于(lie in), 包括(include)” appear in the end of the first SS1, and followed by the comma and another SS2, then the comma will be replaced by the word “that” when transferred into English. E.g.2: Source sentence:本发明的特征在于，它可以调节输出装置的参数。 Target sentence: The feature of this invention lies in that it can adjust the parameter of the output device. In the example, the sourece sentence includes two subsentences, in which the second one is actually the object of the first one. Considering that, when transformed into English, it is better to transfer the two subsentences into a single sentence by using object clause and replacing the comma with the connecting word “that”. Generally, Chunk-level reordering and transformation inside the chunks play more important roles in generating grammatical target language. Which mainly includes following types: Changing form, tense and voice etc. of core verbs. As for form, for example, if some modal verbs appear before the core verbs, the verbs should be transformed in the form of prototype. As for tense, simple present is considered as default tense in most cases, but if some  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 57  words such as “已, 已经(have already)” appear before the core verbs, then they need to be changed into the perfect tense. As for voice, the default voice is active voice, but verbs should be changed into passive voice if they are followed by words representing passive voice such as the typical preposition “被(BEI)”. On the other hand, for some sentences without subjects, the predicates may also be changed into passive voice, and the objects will serve as subject in English (VP+NP  NP+VP (passive voice)). Transforming the ADVPs introduced by prepositions. Such transformation includes two aspects: (1) reordering positions of adverbials. For those located between subject and predicate verb in Chinese, they need to be reordered to the end of the sentence in English. If some parallel adverbials appeare in the same sentence, it is better to reorder them in reverse order. (2) Transformation of long-distance fixed structures. In some adverbial chunks, “ 当 …… 时 (when……)”and “在……中(in……)”, for example, the left and right boundary words are usu- ally collacations and appear together, which can be directly replaced with corresponding words in English. We have wrote rules to cover the fixed collacations as much as possible. Rule2: NP+ADVP+VPNP+VP+ADVP Rule3: NP+ADVP1+ ADVP2+VPNP+VP+ADVP2+ADVP1 Rule4: (0)CHN[当]+(f){(1)CHN(时)}  DELETE (0)+DELETE(1)+ADD[when] E.g.3: Source sentence: [NP本发明][ADVP1在实验中][ADVP2通过一种有效的方 法][VP提高产品的性能]。 Target sentence: [NP This invention] [VP improves the performance of the products] [ADVP2 by an effective method] [ADVP1 in the experiment]. Reordering special prepositional phrases (PPs) in Chinese. Some prepositions, such as “把(BA)” “将(JIANG)” and “被(BEI)” etc., are unique in Chinese and lack corresponding translation in English. PPs composed of such prepositions and NPs always appear in front of VP, when transfer them into English, these prepositions should be deleted, and NPs behind the prepositions must be reordered to proper positions, usually after the VP. Rule5: NP1 + prep. + NP2 + VP  NP1 +VP + NP2 E.g.4: Source sentence:这些计数器对这些数据输入/输出装置的数量进行计数。 Target sentence: These counters count the number of these data input/output devices.  Before syntactic reordering  After syntactic reordering  NP1 这些计数器(These counters)  NP1 这些计数器(These counters)  PP prep. 对(DUI)  VP 进行计数 (count)  NP2 这些数据输入/输出装置的数 NP2 这些数据输入/输出装置的数量(the  量(the number of these data in- put/output devices) VP 进行计数 (count)  number of these data input/output devices)  Figure 3. Original and Reordered Trees of Example 4  Rule6: NP1 + prep. + NP2 + VP +NP3  NP1 + VP +NP2 +NP3 E.g.5: Source sentence:第二通信模块将第二表示数据发送到计算机系统。 Target sentence: The second communication module sends the second indicating data to the computer system.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 58  Before syntactic reordering  After syntactic reordering  NP1 第二通信模块(The second communi- NP1 第二通信模块(The second communi-  cation module)  cation module)  PP prep. 将(JIANG)  VP 发送(sends)  NP2 第二表示数据(the second in- NP2 第二表示数据(the second indicating  dicating data)  data)  VP 发送到(sends to)  Prep. 到(to)  NP3 计算机系统(the computer system) NP3 计算机系统(the computer system)  Figure 4. Original and Reordered Trees of Example 5  Structural reordering inside chunks, especially in NPs. The most common structure of NPs in Chinese is “modifiers + 的(DE) + head NP”. In which the modifiers can include NP, VP, quantifier phrase (QP), determiner phrase (DP), adjective phrase (ADJP) or even relative clauses. The placement of QP, DP, and ADJP modifiers is somewhat similar to English that these phrases typically occur before the nouns they modify, and they need not reordering. For NP1+DE+NP2, althouth it is analogous to the English possessive structure of “NP1’s NP2” and does not require reordering, the Chinese possessive structure “NP1 DE NP2” can express more sophisticated relationships, additionally, the “NP2 of NP1” expression is more general and can replace“NP1’s NP2” in many cases, except for the case that the NP1 is a pronoun. Thus, the reordering rules will state that and map the following rule. Rule7: NP1+DE+NP2 NP2+DE+NP1 NPs modified by relative clauses (CP), with long distance structures, are quite different with those in English. For such NPs, we apply the rules to reposition the child CP after its sibling head NP under a parent NP. Rule8: CP+DE + NPNP+that+CP E.g.6: Source sentence: 将在性能测试中产生的电能传输给车辆的装置。 Target sentence: The device that transmits the electrical energy produced in the performance test to the vehicle. From the syntactic trees, it can be seen that the CP modifier is reordered to the position just after its sibling head NP, and the whole NP is transformed into a NP modified by an attributive clause. In the transformation, it is necessary to add an additional word “that” between the antecedent and the clause. The example is also a nested chunk with multi-level structures, it clearly outlines the various types needed to be transformed, including ADVP, PP, NP and VP mentioned above. Reordering rules sequentially process the elements in a top-down order, and the rules will be exploited circularly.  Before syntactic reordering  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 59  After syntactic reordering  Figure 5. Original and Reordered Parising Tree of Example 6 Generation of Target Language Generation can be decomposed into two steps. First, word selection. According to the reordered syntactic transformation tree, the MT engine selects target words for each node from the Chinese-English parallel translation dictionary. Second, morphological generation, which consists of generating the target surface forms from their associated morphological information. 3.2. SMT System The phrase-based SMT baseline system Moses is built on the basis of freely available state-ofthe-art tools: the GIZA++ toolkit (Och 2003) to estimate word alignments, the IRST Language Modelling toolkit (IRSTLM) (Federico, et al., 2008) with modified Kneser-Ney smoothing (Chen and Goodman 1999) to guarantee more fluent target language outputs. And in the paper, we use the IRSTLM toolkit to train a 5-gram language model with the patent texts corpus. Last, as decoding is the central stage of SMT, the Moses decoder (Koehn et al. 2007) is employed to find the highest scoring sentence in the target language corresponding to given source sentence. 3.3. Hybrid System Architecture Many previous works use SMT as basic skeleton of the hybrid system. In our work, considering the pros and cons of RBMT and SMT, as well as the special features of patent texts, we try to build a hybrid patent MT system guided by the RBMT. Just as mentioned before, RBMT usually performs better in dealing with long distance structure and reordering. In the system combination, the RBMT is responsible for parsing source language and generate grammatical syntactic reordering lattice for the target language by applying the knowledge base and the rules, the main task of SMT is to generate translation for each node according to the reordered tree determined by the RBMT. While the RBMT guarantees basic proper structures of target languange, the SMT provide more lexical selection, as a result, the hybrid system is supposed to generate more fluent and acceptable translation. In the hybrid system, after word segmentation, the RBMT first analyses Chinese sentences and transform positions of chunks according to the corresponding expressions in English by matching kinds of rules. Next, instead of generating all the translation for the words, the RBMT just genernates partial translation for special words (most are functional words) in the sentences. On the other hand, the RBMT also adds some connecting words such as “that”to make the final translation more fluent. Let’s take a sentence for example.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 60  E.g.7: Source: 权利要求1的滑门机构，其特征在于，所述轴部配置于所述卡的插 入方向上的所述滑门构件的里端。 Transformed: 滑门/机构/ of 权利要求/1/ ，其/特征/在于/ that 所述/轴部/配置于/里 端/of /所述/滑门/构件/on/插入/方向/ of /所述/卡/。 The segmented sentence sequences with partial translation are output results of the RBMT, which will be sent to the SMT as input files. In the SMT stage, the standard statistical decoder Moses is used for monotone decoding to limit reordering, which can not only speed up the decoder, but also increase the translation performance. The system will search proper translation for remaining words in the source sentences, generating final target hypothesis of complete sentences. Sum up, the RBMT is responsible for analyzing and transforming the source sentences, and give translation for partial words in advance, the SMT is mainly responsible for generating translation for most words and chunks. Following is the architecture of the hybrid system.  Figure 6. Architecture of the Hybrid System 4. Experiments In this part, we conducted some evaluations on the hybrid system to test its performances, which were measured by several popular automatic evaluation metrics : WER (Nießen et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006), BLEU (Papineni et al., 2002), NIST (Doddington, 2002), GTM (Melamed et al., 2003), METEOR(Banerjee and Lavie., 2005) and ROUGH (Lin and Och, 2004). All measures were calculated with the Asiya toolkit 1 for MT evaluation (Giménez and Màrquez, 2010). We also compared the scores of the hybrid system with those of RMBT, SMT and Google online translation. 4.1. Experimental Setting We exploited the training set of NTCIR-92 (Sakai and Joho, 2011), including 1 million bilingual Chinese-English patent sentence pairs, to train the Moses decoder. In the develop set of NTCIR- 
This paper demonstrates the effectiveness of cross-lingual patent wikification, which links technical terms in a patent application document to their corresponding Wikipedia articles in different languages. The number of links increases definitely because different language versions of Wikipedia cover different sets of technical terms. We present an experiment of Japanese-to-English cross-lingual anchor text extraction using a dedicated technical term extraction system and a patent parallel corpus. Cross-lingual anchor text extraction retrieves about 10% more technical terms linked to Wikipedia articles than monolingual extraction. We also show that restricting anchor texts to technical terms in a specified Wikipedia category has effect of reducing the number of destination article candidates. 1. Introduction Wikification refers to a task of linking phrases in a text to their corresponding Wikipedia articles. It greatly enhances the understandability of the text; namely, readers of a wikified text can easily figure out the meaning of an unfamiliar phrase by clicking it. Applying wikification to patent application documents is helpful for understanding technical terms in them. With the rapidly increasing quantity and improved quality of Wikipedia articles in technical domains, the effectiveness of patent wikification will be enhanced. A promising extension of wikification is cross-lingual wikification, which links phrases in a text to articles in languages other than that of the text. Wikipedia has more than 35 million articles in more than 290 languages. Since each language version is being edited independently, the quantity and quality of articles are very diverse among languages. While English Wikipedia has nearly five million articles, most other language versions have less than one million articles as of 2015. This indicates that a huge number of entities in the world are explained only in English. Thus, we can enrich wikification results by considering links from other languages to English. Consider a Japanese patent application document that includes the technical term “アーク抑制 (aku yokusei),” which is not explained by any Japanese Wikipedia article but by the English Wikipedia article “Arc suppression.” Cross-lingual wikification links the technical term “アーク抑制 (aku yokusei)” to the English Wikipedia article “Arc suppression.” Such cross-lingual links would be very useful for revealing important entities that could not be found by monolingual wikification. In this paper, we demonstrate the potential effectiveness of cross-lingual patent wikification. The main target is patent application documents in languages other than English (such as Chinese and Japanese), which have been increasing by leaps and bounds. We showed in an experiment that cross-lingual wikification to Japanese patent application documents could significantly increase the number of links by adding English Wikipedia as the linking destination.  Proceedings of 6th Workshp on Patent and Scientific Literature Translation (PSLT6)  Miami, October 30, 2015 | p. 89  2. Related work Wikification consists of two steps: anchor text extraction and disambiguation to Wikipedia articles. We describe studies on the former step that is the main concern of this paper. Anchor texts are phrases that should be linked to Wikipedia articles. One of the most important types of information for anchor text extraction is keyphraseness, namely link probabilities that phrases are used as anchor texts for linking to Wikipedia articles (Mihalcea and Csomai, 2007). Another is relatedness with co-occurring phrases (Milne and Witten, 2008) because a text tends to be linked to articles related to it. Keyword extraction techniques are also applicable to anchor text extraction, because keywords in a text should be anchor texts (Mihalcea and Csomai, 2007). There have been many studies for keyword extraction using syntactic and statistical information (Jacquemin and Bourigault, 2003). With ever more attention being focused on wikification, cross-lingual wikification has been recognized as a new challenging task. Cross-lingual wikification consists of anchor text extraction, translation, and disambiguation. Translation quality severely affects the wikification results with this approach. To our knowledge, translation of technical terms for crosslingual wikification has not been studied. In the previous studies, a large part of anchor texts are named entities, translation of which requires special techniques such as transliteration, name translation mining from comparable corpora, and information extraction-based techniques (McNamee et al., 2011; Cassidy et al., 2012; Miao et al., 2013). 3. Monolingual vs. cross-lingual anchor text extraction In this section, we give a detailed description of our approach to cross-lingual patent wikification. Since our present goal is to estimate increases in the number of links by introducing cross-lingual wikification, we focus mainly on the anchor text extraction problem. What kinds of phrases should be extracted as anchor texts depends on the domain of the text and the application of wikification results. Most anchor texts to be extracted in patent wikification are technical terms, while original wikification (Mihalcea and Csomai, 2007) also extracts named entities. Named entities, such as personal names and place names, seldom occur in patent application documents. In most cases, these are of no interest to readers for understanding what is invented in the patent. We therefore extract technical terms from patent application documents as anchor texts. In order to discriminate technical terms from other kinds of anchor texts, we employ a technical term extraction system. Our anchor text extraction system built for an experiment consists of three parts: technical term extraction and monolingual/cross-lingual anchor text extraction (Fig. 1). 1) Technical term extraction We first extract technical terms from patent application documents as anchor candidates by a technical term extraction system. To extract terms from Japanese patent application documents, we employ termex,1 the automatic domain terminology extraction system developed at Nakagawa Laboratory, University of Tokyo and Mori Laboratory, Yokohama National University. This system is based on occurrence and concatenation frequencies of simple and compound nouns (Nakagawa and Mori, 2003). Termex assigns a score that approximates termhood for each extracted technical term and outputs the ranked list of technical terms for an input document. We assume the whole output of termex as candidates of anchor texts without filtering by the scores. 
In the past few years, developers in companies such as SDL and Microsoft have focused on how to improve the quality of fully automated machine translation (FAMT) by leveraging tools for machine assisted human translation (MAHT). They have also focused on how to improve the quality of MAHT by leveraging FAMT capabilities and on how to leverage interactive MT by leveraging terminologies and dictionaries. This paper describes ways developers and users have found to leverage tools across FAMT, interactive MT, and MAHT to provide increased translation coverage, greater agility, and better quality. It also identifies and describes areas where feedback loops are non-existent or broken, resulting in diverging translations. 
This paper describes the Kidney project, which began as an experiment to determine whether human translation and fully post-edited machine translation are interchangeable and if so which is more efficient. In the experiment, an English-language patent dealing with kidney cells was translated by a professional human translator and by a commercial machine translation system. The raw machine-translation output was then fully post-edited by three other translators. Thus, four translations of the Kidney patent were available. When the four translations were evaluated by professional human translators, it was found that the evaluation results were not sufficiently consistent with each other. That is, the evaluation process was not sufficiently reliable. The focus of the Kidney project then turned to increasing reliability by analyzing evaluations linguistically to decide how to develop a revised evaluation instruments. As of September 2015 the analysis is in progress. When the revised metric is available, translators not previously involved in the project will be trained and will apply the metric to the same four translations to determine whether reliability has increased or decreased. The Kidney project is being conducted within the MQM framework (http://qt21.eu/mqm-definition), which was developed under the leadership of DFKI (http://www.dfki.de/lt/). 1. Credits The Kidney project is a collaborative effort of the Translation Research Group at Brigham Young University (Provo, USA), and the Tradumàtica Group at Universitat Autònoma de Barcelona (Bellaterra, Spain). The main participants are Daryl Hague, Pilar Sanchez-Gijon, Kekoa Riggin, Carla Ortiz, and Alan Melby. We thank DFKI for use of MQM. 2. Some Background on the Kidney Project This paper is an interim report on an on-going project whose focus is to increase the reliability of translation quality evaluation in a particular environment, namely, patent translation for the purpose of filing with a patent office in another country. The project described in this paper is called the Kidney project because it is based on a medical industry patent about kidney cells. However, it is hoped that the results of this project will be applicable to other translation environments, after appropriate adaptation to particular requirements. Logically, any project involving evaluation of translation quality would begin by defining translation quality, although this is seldom done in practice. The quality of a translation, regardless of how it is produced, can be defined as the degree to which it meets agreed on specifications, so long as those specifications take into account the needs of the intended end users. Of course, some would challenge this definition. Various perspectives on translation quality are presented in issue 12 (December 2014) of the journal of the Tradumàtica group (http://revistes.uab.cat/tradumatica/issue/view/5). The Kidney project is based on the MQM framework, which has adopted a specifications-based definition of translation quality compatible with the one in the previous paragraph.  
Proceedings of MT Summit XV, vol. 2: MT Users' Track Hiromi Nakaiwa  Fujitsu Laboratories SunFlare Kansai Gaidai University Crosslanguage NTT Media Intelligence Laboratories  Tokyo University of Information Sciences  Intergroup  Universal Content Nagoya University  Miami, Oct 30 - Nov 3, 2015 | p. 69 
There has always been a high quality requirement from large corporations regarding machine translation due to perceptions and common beliefs which in turn makes it difficult to break into the area without well-maintained engines and processes. This project details from the combination of various internal efforts towards automation in translation of technical communications. Namely working with external language providers/partners, testing the offerings, understanding the nature of our source content and the inclusion of machine translation technologies for rolling out Post-Editing Machine Translation (PEMT). 1. Credits This paper is derived from research on tools, technology and processes since 2012 towards the implementation of Machine Translation (MT) within the Localization workflow of Intel Security. It takes input from machine translation service providers, translation vendors and posteditors, language quality team, localization professionals within the company and the various departments where machine translation helps their productivity and allows them to reach their target customers and a wider audience. 2. Introduction If one was to listen to sales pitches from various MT providers, one could learn a lot. It is noticeable that many claim their system is better in some unique way when compared to other systems or the general knowledge in the area. Too much focus on the individual selling points will distract the receiver from possibly more important pitfalls of the rollout process. To separate the jargon from the relevant, one needs to take a step back and look at content from its conception to its consumption and analyze the supply and demand of it. Quite often, the place to start isn’t on the MT, but the internal content, the tools and the people: the three main pillars we need to “shape” in order to get acceptance for MT and effect whatever systems are needed. We selected 5 different content types; Product Documentation, Knowledge Base, Community generated content, Global Definitions database and Product UI. Our main focus was on Knowledge Base articles and Product Documentation as these were two areas where large corpus existed for MT training and structured authoring teams were in place. Below we will follow the discovery process on our initial testing of content to MT. We will detail some of the tests and the systems that need to be altered and provides some recommendations based on the lessons learned. The main output from this is to be our MT strategy to a PEMT rollout.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 92  3. Source, Target and Speed The first time people start learning about MT there can be a lot to digest. Confidence scores are quoted as unique selling points, BLEU scores are proudly displayed as a metric of quality. MT providers offer pre-ordering, automatic post editing and domain adaptation which they say increases their quality, and there can be decisions to be made around Statistical Machine Translation (SMT), Rules Based Machine Translation (RBMT) or Hybrid methods which are a combination of systems. It can’t be denied that all these things play a part as there are studies that prove this. But how much impact do they really have effecting the ability to roll out high quality MT? Where does attention need to be paid, and what are the priorities? It can be hard to tell at the beginning of an MT program. Having gone through the required ramp up of technical knowledge with the many great online resources and taken advantage of talking to peers in the industry, it comes to a point when actions need to be taken and one must choose a method to proceed with. The focus for our tests was on PEMT and 3 areas stood out for measurement; Productivity, Target Language Quality and Source Language Quality. These were largely driven by internal requirements for Cost, Speed and Quality. It was important we looked at Productivity as demonstrated in Post-Editing studies like Plitt, M., & Masselot, F. (2010) where post-editing substantially increased their productivity when compared to Human Translation (HT). This paper the authors talk about productivity measured in Time, and this seemed a reasonable starting point. We then found a number of tools available to do this such as iOmegaT where timing data is measured in a desktop translation tool. One of the advantages of this tool was the ability to track time per segment, but also revisits to segments, which give a clear picture of how much effort was given to each PostEdited segment. It would be considered normal practice in Translation and Post-Editing for the translator to revisit segments once context became clearer while translating similar segments. Also as iOmegaT is a desktop Computer Aided Translation (CAT) tool, it is closer to the native working environment for translators reducing that variable from the tests. We also mixed the post-editing task by creating a project TM with segments to be fully translated (no MT) and also inserted some previous TM matches for segments to be leveraged/reviewed. The final measurement we wanted from this was how many words were Post-Edited in 1 day relative to how many were translated using the same environment and project. Again, “words per day” is a standard metric in our business for forecasting translation time in projects so it was important to leverage something that is already generally understood. We displayed the throughput relative to the 4 final engines we were testing (4 languages). The final throughputs (Fig. 1) were recorded where the minimum quality bar was met (80% pass mark for LQA).  Figure 1: Throughput per day (words) for Doc and Knowledge Base content We needed to measure quality of target language and for this we already had Language Quality Analysis steps in place. Our LQA score is another measurement that exists in our business day to day, so again it’s something that people in the company already understand. This quality measurement is based on the LISA LQA model and the results are in the form of a chart with an overall score out of 100. While the LISA LQA model will do for general quality assessment we also needed something more specific and repeatable. For this we complimented LQA with an Edit Distance measurement on every segment, giving us some drill-  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 93  down data when investigating problematic segments later on. We displayed this on a graph to demonstrate where most of the effort was for Post-Editing. Figure 2: Edit distance split for Documentation and Knowledge Base content Finally for an end user confidence we did some usability studies on samples of MT where users scored segments on a scale of 1 to 4 where 1 was bad and 4 was good. During our studies we did notice that automated metrics such as BLEU and Edit Distance correlated somewhat with our human usability tests on individual segments but lesser so with a usability test done by a trained linguistic reviewer when looking at the overall project. We put this down to individual strings such as long strings (in excess of 15 words) which caused issues due to writing style.  Figure 3: Usability results for Documentation and Knowledge Base content We have some ability to control standards in our Technical Authoring process, so studies such as Roturier, J. (2004) on Controlled Language (CL) rules effect on MT systems also gave us inspiration for measuring source appropriateness. The idea is that if you have good controlled source authoring style and terminology, then Machine Translation will work better. To understand the nature of this within the company we undertook the task of rewriting some source content to be in a controlled language style. We used this in addition to our normal  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 94  source content to be Machine Translated for benchmarking in the discovery stages of project. The CL rules we used were based on a limited standardized terminology set and some other basic rules such as sentence length. The two types of content we put into a Controlled Language were standard Technical Documentation; Software Doc/Help and Knowledge Base Articles. It should be noted that by creating new source content and style we must expect some impact on the MT statistics as the non-Controlled Language style is used previously to populate the Translation Memories which in turn are used in training the MT engines. Using the Edit-Distance, we counted how many segments did not need editing or needed only a low amount of editing and we could see that there was a higher percentage of 100% Match or Fuzzy Match segments that did not need to be Post-Edited with the content rewritten for Controlled Language. This already showed a clear difference between normal authoring and Controlled Language Authoring with regards to the effectiveness of the MT system when displayed across 4 different MT systems. Fig. 4 pertains to the final 4 engines being tested relative to the throughputs recorded in Fig. 1. Figure 4: Percentage of 100% matches for Content v’s Controlled Language Content It could be said in hindsight that some of our testing was unnecessary. We used a number of other tools that were readily available such as Reading Ease metrics (Flesch-Kincaid and others), measuring the segments and words, average words per segment etc. We wanted full visibility on anything we could measure that may affect output and studied these source metrics on various content types. Despite these measurements not being immediately necessary, they were easy to do, and the lessons learned during this phase do help in the future such as in the ability to notice a high level problem in the authoring process if the numbers move greatly on a particular topic within the Content Management System (CMS).  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 95  Figure 5: Readability Metrics for Documentation/Help and KB Articles  3.1.  Make your Enterprise change  A large company does not change processes at the speed of light, it changes slowly and that occasionally makes you actually wonder if it is changing at all. To be the one who tries to turn the enterprise ship can be a daunting task. At the start of discovery in MT it is important to act as an Influencer. This role is to point out areas that could change and the improvements that could be made, find reasons where MT could help and see where people react. Build up buzz around the topic, prove some results and educate your colleagues. These approaches help show what life might be like in the future with a new practice and may demonstrate the potential value of change. Through this movement, followers join your cause and MT will start to go from a topic of conversation through to being involved in projects. People who believe in you and in what you are doing are allies you need to gather in order to make MT a reality. Every conversation potentially helps the cause as many pre-conceptions can exist due to peoples personal experiences with Google Translate and such. The Enterprise requires due diligence, so every step towards rolling out Post-Editing for productivity should be layered with tests, discussions and some time for people who are not living in the MT or academic world to consume and understand the results. To help in this we have used standard metrics for our organization and we continued this with use of the “Trados Grid” which has an industry understood breakdown of matches in a TM. In Figure 6 we used a GNU license application called KNIME which has some ability for custom workflows of text analytics.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 96  Figure 6: MT Distance matches shown in standard Trados breakdowns using KNIME It can feel like an uphill battle sometimes to get organizations to change. There is so much to prove to ensure the business case. Luckily there are some very useful resources which can help you prove the theories you preach. The MT suppliers often provide excellent information that can be reused and you should ask them for this if in doubt. Research and white papers can be very useful and many of the MT users meet, collaborate and share their experiences at conferences and online. A newcomer to this area could do well to make friends and ask questions as it can be through these connections that you may build confidence in your own ability to make the best steps forward. Not every approach works for every person or company. Compare, focus and learn the appropriate subjects that you need and ultimately help guide your company towards the right path. There has to be a need or a gap that can be filled by using MT technology. The first thing that should be done is to identify what the specific business needs are. Having more than one business need gives you a platform to build a proposition to show value and Return on Investment (ROI) in this area. Business needs for MT in localization are born out of content and publishing. The content gets created and needs to reach an audience. MT can boost the efficiency of that effort through a number of different ways and these are the high level unique selling points for your internal customers that you need to find and understand. Some basic business needs are: • Increase productivity of translation (Plitt, M. and Masselot, F. 2010). • Allow on-demand translation for content that normally does not get translated. • Enable internal users to have access to a larger set of content in their language (Burgett, W., Chang, J., Martin, R. and Yamakawa, Y. 2012). • To speed up a process of collating sentiment analysis from content. • Help understand the “gist” of text not available in your own language. • Enable early versions of localized Documentation or Software. For the purpose of focus in this paper, we are looking at the productivity of translation as it is an area that can show immediate financial savings. Through increasing productivity many lessons will also be learned and skills gathered necessary for many of the other areas of the value proposition while aiming to save money and time.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 97  3.2.  Is MT all that is needed?  This is not only about the MT system as already mentioned earlier in the paper. In some regards it is not even about the MT system itself as the investment in quality of commercial MT systems has been good over recent years, and they continue to get better. The internal workflow is probably the area where most change must happen. This affects a set of items from content writing and curation through how you manage your bilingual content and right down to the end result of publishing and the feedback loop back to the content creation and MT maintenance. Mentioned previously, Controlled Language Source is probably the most important area to start making changes as it can have a massive impact (Roturier, J. 2006) (Doherty, S. 2012) if it is done right. If you are thinking about rolling out MT in your company, you should start here. Who is writing content that will eventually end up being machinetranslated? Is the content good for translation? Does anyone need to change their work practice? After all, in a Globalized company, the source content is most likely only a small percentage of the content distributed to customers around the world. If we can control our language, all the target languages will benefit. Some basic rules on the content creation side can have a great impact, and consequently, effect on throughput and accuracy in the future. We did some after the fact analysis on Distance per segment and noticed some patterns. The basics that seem to make a big difference are:  • Managed and maintained terminology for authoring – reduction of synonyms • Basic style rules – keeping all authoring similar • Reuse of repetitions and phrases in writing • Source content profiling your authoring into groups (Domains) for MT systems.  Translation process is the next area that needs attention. At the end of the day, the translators are your direct link to your market, and to ensure the best language quality possible and the most accurate message possible, they need to be included in your plans. On paper you may have MT systems with high BLEU scores, but does this become good PEMT in the end? The most important factor towards good quality of PEMT is the translator. In our initial PEMT tests we identified a wide discrepancy in results of translator productivity and quality.  Figure 7: Throughput PEMT Spanish with basic Translator Profile info. After analysis it seemed that the one variable in the process was the individual doing the post-editing. We could not effectively baseline results from one group of post-editors to another with this variable so we needed to reduce or eliminate it and started looking at the concept of Translator Profiling. We would like all translators to translate at the same rate and produce the same quality. This just isn’t the case. So there are several parts of a profile that can vary the results, such as individual motivation, or when using freelance or crowdsource  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 98  models where profiling isn’t possible. But as quality is the main requirement for our PEMT there were a number of factors that stood out as a requirement for Translator Profiling for us:  • Experience as a translator is important. • Post-editing experience is less important (but needs to have some. 2 years is good) • Age is not necessarily a factor, but (technical) ability to leverage tools might be • Understanding the content subjects is the most important aspect to reach quality.  What this basically means is that PEMT resources are needed who have spent a good amount of time working on your content so that they understand both your content and your quality expectations. This is evident from Vendor A who has experience on our content and scored high on quality, but lower on throughput. But Vendor B and C did not meet our quality expectations (despite Vendor C having 3 years with our content). The number of years’ experience in post-editing is important to throughput with Vendor B and C retaining very high productivity but Vendor A was slower. The workflow is also an important area to look at. Translation Management Systems (TMS) exist with MT plugged in through APIs. There are other decisions you need to make for your workflow though. Can you trust your translators to not make mistakes such as missing a file for translation? Do you review to ensure no mistakes? Do you allow PEMT segments back into the Translation Memories (TMs) of your main products? Does your TMS edit content before going to the MT system (such as protecting tags or internals)? Do you apply a match penalty on your MT segments and by how much? There is no quick answer for these questions. It is crucial then to understand the nature of the content you want to MT (source) and the nature of the market where you want to publish it (target and quality). Basic localization decisions from normal workflows may need to be rethought when you include MT into the translation strategy.  3.3.  Testing MT  Ultimately you will need to test the MT output. Whether you create the MT systems yourself or use a service, or even outsource completely, it is imperative that you run a test. What you want to achieve from this test is a confidence that the standard of quality is high enough on the output for post-editing to happen with extra efficiency.  • Productivity • Quality (Automatic Estimation) • Quality (Human Evaluation)  For our tests on productivity we decided that time data was the most important. There are other ways to conduct a test, but at the end of the day if a translator takes less time to post-edit than if they were to translate from scratch, then you are on the right path. Time data is difficult to track, but thankfully over the last few years Computer-Assisted Translation (CAT) tools have evolved to start measuring this (Moran, Lewis & Saam 2014). For our tests we used iOmegaT, which is an adaptation of OmegaT, to gain access to the instrumentation and telemetry on the translator’s activities while they post-edit. There are some privacy concerns with this initially; however we found that all translators were happy to be involved once this is part of a test and they had some control over when the feature could be turned off on their console when not in test.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 99  Figure 8: Throughput per type of content showing efficiency compared to others When we say time data, we basically mean the time it takes to post-edit a segment. It should also include subsequent visits to a segment (not just the first attempt) as it can be a common practice for translators to revisit segments after getting a feel of the document they are translating. Some segments will show erroneous measurements, which is explained when a translator takes a break while having a segment open. We allowed an adequate amount of time for any research the translator may have to do, but we did apply a cutoff to reduce the inclusion of these segments in the test. Timing data can be measured in actual time (ms), but what we used is “words per day” throughput as this is something that most people in our company will understand quickly and easily.  Figure 9: Time spent on 100% match segments from MT From time data we can learn a lot if we further refine the application of the time data to other metrics such as the brackets for length of segment, the number of repetitions, the number of segments that do not need editing or the number of segments that need a lot of editing (long time segments). We noted that for some content types 100% matches required excessive time to complete when compared to the time spent in other content types. Upon questioning some of these results, the respondents claimed that some 100% match segments require more time to read and understand before they agree that the MT segment is of correct meaning and  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 100  language quality. These results helped build up a profile of the content for “post-edit ability” and in turn make some recommendations back to the writing teams. In our discovery tests we sampled 5 different content categories that all loosely communicated within the same domain. While the writing styles and lexical complexities may diverge in their own subdomain, the core subjects are the same. Nevertheless this process was worthwhile as we learned as much about our own internal content as we did about the ability for MT to work with it. You could say that this practice taught us a lesson about Content Profiling before pushing a content type through an MT workflow. Moving on to quality, the world of Six Sigma says that “Quality is what your customer wants”. So before we enter into a linguistic quality test we should keep that in mind. It’s not often that a customer will come to you and tell you what they want, so we use trained domain linguists to conduct a linguistic quality analysis as appropriate and added some segment usability scores and automatic/automated algorithms such as General Text Matching (GTM) or Levenshtein distance to the test matrix. Language Quality Assurance (LQA) in its traditional form proved to be sufficient in this case for Quality Evaluation (QE), but more advanced error topologies could also help such as TAUS Dynamic Quality Framework. But with so many factors affecting each segment, you must consider these with a soft focus on the overall quality output as some aspects may need to be prioritized or weighted as having an increased effect on the overall output. What that basically means is that LQA parameters need to be aligned with the quality expectation, and this is hard to manage if you are to baseline quality evaluations against something that may be subjective. In the end, you need a number to go by, but you may actually be more interested in the details of the test (accuracy errors, terminology errors, priority or severity of errors etc.) than the overall score achieved by the text. To balance the linguistic assessment of your MT text with opinions from would be consumers, usability studies can be carried out. We did a number of tests in our company with various native French, Spanish, Chinese and Italian speakers. The test involved going through 100+ segments and scoring them out of 4 (1 for bad and 4 for great). To get a better idea of the diversity in the scoring, we then applied the same scale to LQA and GTM scores (breaking down the percentage brackets into 1 to 4). From the graph below (Figure 3.) we can see the 186 segments in this test more or less correlate to the same sentiment across the 3 types of measurements applied. The results in this case show that the MT for this language is mostly good and there are a minimal amount of truly bad quality segments.  200  100  0  
scores and human evaluations •Differences between system autoscores and PE autoscores •MT evaluations in a production setting •MT evaluations of post-edited files: a case study Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 107  OUR EVALUATION METHODS A TYPICAL EVALUATION PROCESS PER LOCALE AND PER ENGINE  Engine Training  Autoscoring Test Sets (2500 TUs)  Engine Ranking (100 TUs)  Adequacy & Fluency Scoring (100 TUs)  Error Typology Scoring (25 TUs)  Analysis & Report  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 108  OUR EVALUATION METHODS  AUTOMATIC SCORES GENERATED BY WESCORE  80.0  75.0  70.0  65.0  60.0  55.0  50.0  45.0  40.0  35.0  30.0  EnHguinbe1  Proceedings of MT Summit XV, vol. 2: MT Users' Track  ECnogminme12  CEonmgimne23  EnDgIiYn1e4  EnDgIYin2e5  Precision Recall GTM METEOR BLEU Avg. PE Dist. TER Miami, Oct 30 - Nov 3, 2015 | p. 109  OUR EVALUATION METHODS HUMAN EVALUATIONS: ADEQUACY AND FLUENCY SCORING  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 110  OUR EVALUATION METHODS  HUMAN EVALUATION: ERROR TYPOLOGY  25  Errors per 25 Segments  DA  20  ES  FR 15  HI-IN  10  IT  5  JA  NL 0 NO  PT- BR SV  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 111  OUR EVALUATION METHODS  HUMAN EVALUATION: ENGINE RANKING  100  90  80  70  60  50  40  41.5  39  30  20  10  0 de-DE Proceedings of MT Summit XV, vol. 2: MT Users' Track  Engine Ranked Best (out of 100 segments)  66 fr-FR  58.5 51.5  22.5 ja-JP  zh-Hans  HEnugbine1 CEonmginme12 CEonmginme23 DEnIYg1ine4 DEnIYg2ine5 Miami, Oct 30 - Nov 3, 2015 | p. 112  LESSONS LEARNED  • We always perform autoscoring PLUS human scoring for all our MT evaluations. We have internal thresholds that qualify an engine ready for deployment and it’s level of maturity.  • For bake-offs between several engines, we always include engine ranking in addition to our standard scores.  • Productivity tests are valuable during the initial phase of an MT program to build up productivity data for future reference across languages, domains and MT systems.  • Our MT program is now mature and we are able to perform most of our evaluations based on autoscoring PLUS human scoring, and by referencing the productivity data we have collected over a number of years.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 113  NEXT Correlations between automatic scores and human evaluations Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 114  CORRELATIONS CORRELATIONS BETWEEN AUTOMATIC SCORES AND HUMAN EVALUATIONS  Pearson's r 0.50576955 0.50070425 0.49816365 0.49724893 0.49195687 0.47064566 0.38293518 0.31354314 0.2940756 0.28586852 0.28386332 0.26685854 -0.40270902 -0.4788575 -0.5385275 -0.5421933 Proceedings of MT Summit XV, vol. 2: MT Users' Track  Variables Fluency & METEOR Fluency & BLEU Fluency & Recall Fluency & NIST Fluency & GTM Fluency & Precision Adequacy & NIST Adequacy & METEOR Adequacy & Recall Adequacy & GTM Adequacy & BLEU Adequacy & Precision Adequacy & TER Fluency & PE Distance Adequacy & PE Distance Fluency & TER  Strength of Correlation Strong positive relationship Strong positive relationship Strong positive relationship Strong positive relationship Strong positive relationship Strong negative relationship Moderate negative relationship Moderate negative relationship Weak positive relationship Weak positive relationship Weak positive relationship Weak positive relationship Strong negative relationship Strong negative relationship Strong negative relationship Strong negative relationship  Tests (N) 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150 150  Locales 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11  Miami, Oct 30 - Nov 3, 2015 | p. 115  CORRELATIONS THE STRONGEST CORRELATION WAS FOUND BETWEEN FLUENCY AND TER  TER  100  90  80  70  60  50  40  30  20  10  0  1.00  1.50  Proceedings of MT Summit XV, vol. 2: MT Users' Track  2.00  2.50  3.00  3.50  Fluency  4.00  4.50  5.00  Miami, Oct 30 - Nov 3, 2015 | p. 116  CORRELATIONS THE 2ND STRONGEST CORRELATION WAS FOUND BETWEEN ADEQUACY AND PE DISTANCE  PE Distance  100%  90%  80%  70%  60%  50%  40%  30%  20%  10%  0%  1.00  1.50  Proceedings of MT Summit XV, vol. 2: MT Users' Track  2.00  2.50  3.00  3.50  Adequacy  4.00  4.50  5.00  Miami, Oct 30 - Nov 3, 2015 | p. 117  LESSONS LEARNED  • It seems that we cannot rely solely on autoscores as long as the correlation with human judgment is not stronger than the data suggests • TER and PE Distance show the strongest correlation to both Fluency and Adequacy, and therefor seem closer to human judgment than the other scores. • Fluency correlates stronger with system autoscores than Adequacy overall. • PE Distance is the only metric that correlates stronger with Adequacy than Fluency. PE Distance is also the only character-based metric.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 118  NEXT Differences between system autoscores and post-editing autoscores Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 119  SYSTEM VS PE AUTOSCORES ONCORARELVATIEONRS BAETWGEENESY,STETMHSCOERESPOST-EDITING SCORE IS 15 AND 17 POINT HIGHER AND POST-EDITING SCORES FOR PE DISTANCE AND BLEU RESPECTIVELY  Pearson's r 0.832226688 0.832218909  Variables BLEU (System) & BLEU (PE) PE Distance (System) & PE Distance (PE)  Strength of Correlation Very strong positive relationship Very strong positive relationship  Tests (N) 57 57  Locales 9 9  60  50  40  30  33.18  20  10  0 BLEU (System) Proceedings of MT Summit XV, vol. 2: MT Users' Track  50.58  44.32  29.62  BLEU (PE) PE Dist % (System) PE Dist % (PE)  17.40 BLEU DIFF  14.70 PE DIST DIFF %  Miami, Oct 30 - Nov 3, 2015 | p. 120  SYSTEM VS PE AUTOSCORES  CORRELATIONS BETWEEN SYSTEM BLEU AND POST-EDITING BLEU  BLEU (PE)  100  90  80  70  60  50  40  30  20  10  0  0  10  20  30  40  50  60  70  80  90  100  BLEU (System)  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 121  SYSTEM VS PE AUTOSCORES  CORRELATIONS BETWEEN SYSTEM PE DISTANCE AND POST-EDITING PE DISTANCE  PE Distance (PE)  70%  60%  50%  40%  30%  20%  10%  0%  0%  10%  Proceedings of MT Summit XV, vol. 2: MT Users' Track  20%  30%  40%  PE Distance (System)  50%  60%  70%  Miami, Oct 30 - Nov 3, 2015 | p. 122  SYSTEM VS PE AUTOSCORES REAL DATA WHERE WE COMPARE EVALUATION SCORES WITH SCORES FROM A 3-MONTH PILOT  100  90  80  70  60  50  40  30  32.2 35.26  20  10  0  es-ES  PE Distance (%) Pilot1 Eval1  43.69 35.3 fr-FR  46.76 35.08 it-IT  LOOK FOR CONSISTENCY Proceedings of MT Summit XV, vol. 2: MT Users' Track AND BEWARE OF OUTLIERS  34.71 20.46 pt-BR  Miami, Oct 30 - Nov 3, 2015 | p. 123  LESSONS LEARNED • There is a very high correlation between the MT system autoscores generated during the evaluation phase and the autoscores generated from production using the same engines. • However, the post-editing autoscores are considerably better than the MT system autoscores by around15%. • We now differentiate the autoscores in our database as ‘System’ and ‘PE’.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 124  NEXT MT evaluations in a production setting Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 125  PRODUCTION SETTING HOW TO MEASURE POST-EDITING EFFORT  • It is important to monitor the performance of MT and post-editors, especially during the initial launch of a new program • The use of autoscoring to analyze post-project files is a valuable and cost-effective method to measure the post-editing effort • They support rate negotiations and can help us to identify over- or under-editing by post-editors • TER and PE Distance are useful metrics, with different underlying algorithms  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 126  PRODUCTION SETTING HOW TO MEASURE POST-EDITING EFFORT  PE Distance - lower is better! • Measures the number of insertions, deletions, substitutions required to transform MT output to the required quality level • PE Distance values are derived by comparing the post-edited segments with the corresponding machine translation segments • In our analysis the PE distance applies the Levenshtein algorithm and is character-based. This captures morphological post-edits, such as fixing word forms.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 127  PRODUCTION SETTING  HOW TO MEASURE POST-EDITING EFFORT  TER - lower is better! • TER stands for Translation Edit Rate • It is an error metric for machine translation that measures the number of edits required to change a system output into the postedited version • Possible edits include the insertion, deletion, and substitution of single words as well as shifts of word sequences. • Unlike PE Distance, TER is a word-based error metric and therefor does not capture morphological changes during post-editing.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 128  PRODUCTION SETTING  LOOK FOR CONSISTENCY AND BEWARE OF OUTLIERS  50  45  40  35  30  25  20  23  22  15  10  5  0 Resource1  Resource2  de-DE Proceedings of MT Summit XV, vol. 2: MT Users' Track  PE Distance (%)  19  18  Resource1  Resource2  fr-FR  35 18  Resource2  Resource3  it-IT  Miami, Oct 30 - Nov 3, 2015 | p. 129  PRODUCTION SETTING LOOK FOR CONSISTENCY AND BEWARE OF OUTLIERS: POST-PROJECT AUTOSCORES INDICATE UNDEREDITING  50 TER scores for several batches 45  40  43.18  35  38.47  30 30.5 25  20  15  10  34.61  35.1 33.36  38.27  24.28 23.35 24.89 15.28  25.25  25.57  20.73  22.84  18.04  5  0 VJIOPBR1 1 VJIPORB23 VIJPORB32 VJIOPBR1 1 VJIOPBR23 VIJPORB32 VJIPORB45 VJIOPBR1 1 VJIOPBR23 VJIPORB32 VJIOPBR1 1 VJIOPBR23 VJIPORB32 VJIOPBR1 1 VJIPORB23 VIJPORB32  de-DE  es-MX  fr-FR  it-IT  pt-BR  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 130  PRODUCTION SETTING TOOLS TO MEASURE POST-EDITING EFFORT  TOOL iOmegaT MateCat Okapi Post-Edit Compare Qualitivity wescore Proceedings of MT Summit XV, vol. 2: MT Users' Track  INPUT FILES xliff & more xliff xliff sdlxliff sdlxliff tmx  OUTPUT REPORT  PROS  CONS  xml Excel html  Includes productivity data Includes productivity data as a built in feature  Generated in the CAT tool during translation, requires post-editor buy-in Generated in the CAT tool during translation, requires post-editor buy-in  Allows us to measure PE Requires access to predistance post-project and post-edited file sets  html Excel Excel  Allows us to measure PE Requires access to predistance post-project and post-edited file sets  Includes productivity data  Generated in the CAT tool during translation, requires post-editor buy-in  Allows us to measure PE distance post-project  Proprietary tool, Requires access to pre- and post- edited file sets  Miami, Oct 30 - Nov 3, 2015 | p. 131  PRODUCTION SETTING MATECAT IS A FREE ONLINE CAT TOOL WITH EDITING LOG  Proceedings of MT Summit XV, vol. 2: MT Users' Track http://www.matecat.com/support/translation-toolbox/editing-log/  Miami, Oct 30 - Nov 3, 2015 | p. 132  PRODUCTION SETTING USE POST-EDIT COMPARE TO ANALYSE SDLXLIFF FILES  Proceedings of MhTtSutmpmit:X/V,/volw. 2: wMT wUser.s'tTrracak nslationzone.com/openexchange/app/post-editcompare-495.html  Miami, Oct 30 - Nov 3, 2015 | p. 133  PRODUCTION SETTING OKAPI FRAMEWORK TRANSLATION COMPARISON STEP  Proceedings hof MtTtSpum:m/it X/V,wvol.w2: MwT U.seors'pTraeckntag.com/okapi/wiki/index.php?title=Translation_Comparison_Step  Miami, Oct 30 - Nov 3, 2015 | p. 134  PRODUCTION SETTING QUALITIVITY PLUGIN FOR SDL TRADOS STUDIO  Proceedings hof MtTtSpum:m/it X/V,wvol.w2: MwT U.setrsr' Taracnk slationzone.com/openexchange/app/qualitivity-788.html  Miami, Oct 30 - Nov 3, 2015 | p. 135  LESSONS LEARNED  • The use of autoscoring to analyze post-project files is a valuable and costeffective method to measure the post-editing effort.  • A productivity test requires upfront organization and buy-in from translators.  • It is important to find a tool that works with the given file format and workflow.  • Access to pre- and post-edit versions of projects is required. This is a challenge on some accounts.  • Identification and separation of MT segments from fuzzy segments may be required for some tools.  • Look for consistency across languages and resources. Unusually high or  low scores can be a sign of over-editing or under-editing. Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 136  NEXT MT evaluations of postedited files: a case study Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 137  CASE STUDY  TEST PILOT FOR LIGHT AND FULL POST-EDITING • Languages: Chinese (Simplified) and Japanese • The resources are regular translators for this client • In order to have comparable data, the same resource performed both light and full post-editing tasks of 438 segments  Full Postediting  Light Postediting  LQA of PE Kits  Autoscoring PE Kits  Adequacy & Fluency Scoring  Error Typology Scoring  Analysis & Report  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 138  CASE STUDY: HUMAN EVALS  ADEQUACY AND FLUENCY SCORES  5 4.5 4 3.5 3 2.5 2 1.5 
 Proceedings of MT Summit XV, vol. 2: MT Users' Track  www.taus.net  Miami, Oct 30 - Nov 3, 2015 | p. 163  What About Translation Quality  Old School: “One size fits all” Since the 1980’s LISA QA Model, SAE J2450 prescribe today’s quality processes: 1. Static: • One quality fits all purposes, all content, all audiences 2. Subjective: • Evaluations are often subjective and anecdotal 3. Costly: • QE causes friction, delays • QE can cost up to 25% of total translation costs 4. Non-transparent: • Necessity without remedy  Dynamic Quality Framework  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 164  Industry Collaborative Program DQF started in 2011  Participating members  Adobe Appen Autodesk AVB CA Technologies Cisco Crestec Crosslang Dell DFKI eBay EMC Google Hewlett Packard Intel LDS Church Lingo24  Lionbridge Medtronic Microsoft Moravia Nikon Oracle Pactera Pangeanic Paypal Philips PTC Siemens Spil Games Systran VMware Welocalize Yahoo!  Proceedings of MT Summit XV, vol. 2: MT Users' Track This slide may not be used or copied without permission from TAUS  Miami, Oct 30 - Nov 3, 2015 | p. 165  From DQF Tools to Quality Dashboard  DQF Tools Since January 2014 Tools on TAUS web site: to measure: • Productivity • Adequacy • Fluency to review and count: • Translation errors to get: • Stats and reports Used by 100+ members  Quality Dashboard Launched June 2015 DQF integrated in: • CAT Tools • TMS Systems Use of DQF plug-in provides: • Enhanced statistics • Benchmarking Open to everyone  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 166  The Power and Value of the Quality Dashboard  • DQF collects data and generates reports on the Dashboard real-time • Translators, managers, buyers, developers get their own stats, benchmarks and analytics • Not only track and benchmark against your own data, but also against industry averages, between translators, customers, projects, technologies  Quality Evaluation Proceedings of MT Summit XV, vol. 2: MT Users' Track  Business Intelligence Miami, Oct 30 - Nov 3, 2015 | p. 167  What is the average productivity? Home Services Academy Events Blog Review Membership My TAUS  Quality Dashboard Productivity Efficiency Adequacy Fluency Language Time Technology Process Content Industry Project Translator/vendor Customer Statistics Language Time Technology Process Content Industry Translator/vendor Customer Distribution of segments Language Time Content Industry Project  Number of Words per Hour  Productivity Across all languages, industries, technologies, processes, content, translators 350  300  250  287  200  150  100  50  0 Jan-00  August 31, 2015: 566,987,756 words have been measured  i More information > Benchmark Quality Dashboard Content profiling Quality evaluation Adequacy/Fluency Error review MT Ranking Productivity measurement DQF API  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 168  What is the average productivity of MT vs. TM? Home Services Academy Events Blog Review Membership My TAUS  Quality Dashboard  Productivity  Efficiency  Adequacy  Fluency  450  Language Time  400  Technology  Process  350  Content  Industry  300  Project  Translator/vendor  250  Customer  Statistics  200  Language Time  150  Technology Process  100  Content Industry  50  Translator/vendor  0  Customer  Distribution of  segments  Language Time Content  Industry  Project  Chart Title  
This paper will describe a way of assessing the post-editing effort for a specific project, language and engine combination. This serves as a tool for LSPs to estimate the necessary effort on the project and quote accordingly. . 1. Introduction Over the last decade, there has been an upsurge in the use of machine translation, both for the purpose of gisting and for the purpose of post-editing. There are various definitions of postediting: the “term used for the correction of machine translation output by human linguists/editors” (Veale and Way 1997), “…checking, proof-reading and revising translations carried out by any kind of translating automaton” (Gouadec 2007) and “In basic terms, the task of the post-editor is to edit, modify and/or correct pre-translated text that has been processed by a machine translation system from a source language into (a) target language(s).” (Allen 2003) are among them. All definitions center around the notion that a human applies changes to machine translation output in order to create a final translation that reaches a previously agreed quality level. We will refer to this process as PEMT (post-editing machine translation). More and more clients ask their Language Service Providers (LSP) to apply PEMT. Their underlying assumption is that PEMT is faster than conventional translation, as part of the final translation is already there. The demand to use PEMT is mostly combined with a request to reduce the financial rates paid for translation. LSPs have a vested interest in determining if they can afford to comply with such requests. This paper will outline one way of validation. 2. Productivity of PEMT Various studies have shown (Laubli et al 2013, Plitt & Masselo 2010) that PEMT as a process can provide productivity benefits over conventional translation, at least for the conditions examined in those papers. However, the productivity in a specific case depends on a large number of factors. To mention just a few:  The engine quality as such. This depends on the technology that was used, but also on the complexity of the source-target language combination (some combinations are harder than others).  The applicability of this specific engine to this specific project. Was the engine geared towards this particular content in any way, or is it a generic engine? Is the project in question terminology-rich and specific, or very generic?  How much experience with PEMT do the selected vendors have?  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 192   How has the technical preparation of the files been handled? Any wrong segmentation will likely have a detrimental effect on the machine translation quality. As the LSP generally needs to provide a quote to the client before starting the job, and needs to negotiate rates with its own vendors as well, it is vital to know in an early stage whether the productivity increase from using PEMT is sufficient to allow a rate reduction to both the client and the vendors. Doing this fully automated would be ideal, but technology is not quite there yet. A human factor in productivity testing is still needed, but the testing needs to be both cost-effective (the cost of testing should not negate the gain of the project) and timeefficient (the LSP needs to have the results in time to quote to the client within his set time limits). The next sections will discuss SDL’s approach to this dilemma. 3. Approach The recommended process has three stages: assessing the project, creating the test set and running the test. 3.1. Assessing the project The first step, which is still entirely human, consists of an analysis of the project. Aside from the normal steps used for conventional translation, the assessment for PEMT adds a few more questions. The main ones are:  Does the expected gain of this project justify the cost of PEMT testing? Only if so, the next questions come up.  What languages are in the project? Do we need/want to test them individually, or would it be possible to group them – for example, assume that the performance of English>French is a good indicator for English>Italian?  Is the project homogeneous, or does it have flows? For example: a large car manufacturer could offer the user guides for buyers, the marketing brochures for prospective buyers, the repair manuals for the mechanics in the garage and the assembly instructions for the workers in the factory. It seems likely that the linguistic characteristics of these documents will differ, and so will the MTPE productivity. Does the client offer the flows split, or all together? Does it make sense to test them separately? 3.2. Creating the test set Once a decision has been made, a test needs to be set up for the intended language and content type. This test set needs to be representative and varied. The representativity of a test starts from the project sample delivered by the client. If not done before, now it needs to be confirmed with the client that their sample is in no way exceptional. The sample has to mimic the total project in (stylistic and terminological) complexity, content and technical characteristics (markup and segmentation). Ideally it will also be large and consist of several outtakes of the total project. This gives a larger variety in topics and the related terminology. Any non-representative or invalid content needs to be removed from the client sample before taking the next step. In order to select the most representative test set, it is recommended to randomly select segments that have the average segment length of the sample, give or take one or two words. This can be automated, which saves time, and it increases the chance that the linguistic complexity of the test set will mimic the complexity of the sample. A couple of longer and shorter segments can be added to test on the less frequent segments and to add variety to the test. In  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 193  order to test the engine´s coverage of client-specific terminology, it is recommended to select a number of segments from various parts of the project rather than use running text, which will mostly cover only one or two topics and its related terminology. For financial reasons, the smaller the test set can be while still giving meaningful results, the better. The smallest possible number of segments depends on the tool used for the testing and the margin of error this tool gives. While we recommend involving a statistician to assess the minimum for use with a specific tool, around 100 segments seems a good rule of thumb. 3.3. Running the test The next step is to split the test set and have the two parts processed. One part will be done as conventional translation, the other part as PEMT. Both parts have to have the same average segment length to keep the times spent on them comparable. Both parts need to be done by the same resource(s), to ascertain the impact of the PEMT for this resource. In order to increase the predictive value of the test for the project, it is advisable to use (some of) the resources who are likely to be used on the live project in the test. This will also help when it comes to negotiating rates for this particular project – having performed the test, they will have a better idea of its validity and of the MT value. The key factor is the registration of times and actions to obtain meaningful results. Ideally, the interpretation of the test will consist of automated indicators to such a degree, that the test can be validated without having to read source or target language. 4. Tools The more steps in the process can be automated, the cheaper each test becomes. Two steps are candidates for automation: the test bed selection and the hosting and analysis of the test. For the test selection, SDL has created a proprietary tool. It makes an automated selection out of one or more sample file(s), based on characteristics like segment length and linguistic characteristics like question/confirmation etc. Using this tool makes the test bed creation much faster, but as there is a development cost, it is only recommended if enough tests are needed in an LSP to recoup this cost. The second step, running and analyzing the test, has been automated to a large degree in SDL. The ME tool is an online, proprietary tool, which has been used and further customized for a couple of years. In the meantime, similar functionality has been embedded in freeware like the qualitivity Studio plugin and the TAUS DQF framework. For the purpose of this paper, we will focus on the ME tool. 4.1. Characteristics of the ME tool The tool is online, which means that resources can be onboarded by simply registering. This saves the overhead of resources downloading a tool, and prevents most of the complications of local PC setups causing incompatibilities. A test is uploaded as a tmx file. For the part of the test that is conventional translation, the source is copied into the target. For the PEMT part of the test, the MT is copied into the target field. The tool displays the segments to the tester one at a time, only offering the next segment if the user clicks « Done ». It allows users to interrupt the test by clicking a button  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 194  « Continue later ». They can pick up the test at any later time. This keeps the time registration free of disruptions like telephone calls which would otherwise create noise in the results. Besides these buttons, the conventional part of the test only contains source and target fields, where the target field is editable. The PEMT part of the test has an extra field for the MT output, which remains in view for reference. The target field starts containing the MT output and can be edited. The PEMT UI also contains a button « Use MT » to indicate that the MT is correct as is and needs no changes. The button « Done » is only enabled after either selecting this checkbox or making edits, so as to prevent accidentally skipping segments. A screenshot is shown in Figure 1 : Figure 1. Screenshot of PEMT part in ME tool  The number of segments needed in a test depends on the exact tool and setup used. In consultation with a statistician, it was decided that for the ME tool 80 segments is the minimum. The tool allows for larger tests, and enables the comparison of up to 5 different engines using up to 5 resources per test. 4.2. Analysis of the ME results The tool delivers an automated analysis with a number of indicators. These serve to ascertain the validity of the test as well as the actual productivity increase. Among the indicators are: - The translation speed for each resource in both conditions (conventional human transla- tion, henceforth HT, and PEMT). This speed is not relevant for production as the tool is not mimicking the production environment, but extremely high or low numbers can point to a problem in the test. - The actual increase in productivity as a percentage of the original speed for each resource. So if for example the HT speed is 800 words per hour and the PE speed is 880 words per hour, the productivity increase is 10%. - The Levenshtein distance1 per segment and on average for both conditions (human translation and PEMT). 
Speech Translation technology in Skype enables users to have a live translated conversations across language barriers. From data collected from usability studies and thousands of Skype users, we’ve uncovered unique user experience challenges of a translated call that dissuade users from having conversations. This talk summarizes these findings and details how we iterate our designs to maintain a semblance of normalcy in translated conversations.  1. Introduction The advent of deep neural networks in Automatic Speech Recognition (ASR) enabled researchers to reduce the word error rate in recognized speech by a third, and made it feasible to use ASR beyond the limited scope of SMS dictation, personal assistants, voice navigation, and made it applicable to the wider domain of every-day conversational speech. And by chaining together the ASR models with existing text translation, it now became possible to build automatic speech translation software for human conversations with previously unachievable accuracy. This breakthrough in ASR resulted in the Skype Translator project, released in December 2014, enabling users to have automatic translated conversations over Skype. Skype Translator logged over 700 thousand app downloads over 9 months and clocked hundreds of hours in call time. There was clearly a need and interest in automatically-translated speech conversations, both in the personal and business sphere.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 220  Yet, when users and reviewers tried Skype Translator for the first time, feedback for improvement was surprisingly equally concentrated around the experience of using Skype Translator as it was on the quality of translation on Skype Translator. In fact, users were more willing to forgive translation mistakes, acknowledging that speech translation was nascent technology; and were less patient with user experience issues as evidenced by the following excerpts taken from a usability study in February 2015 from first-time Skype Translator users – “It (the call) was very chaotic.” “I zoned out waiting for the translations.” “I tried listening to the voice in the beginning, and when it wasn’t working, I turned to the text.” "A bad translation is a conversation killer" "I know that this is a monumental task and will revolutionize technology… but there isn’t a flow in communication …" "I felt like four people were speaking - two in English and two in Spanish"  Skype Translator Usability Lab, Mountain View – February 3rd 2015 2. User Experience areas of focus Based on our usability studies and data from real-world users using Skype Translator, we identified that user experience of a translated call was a top pain-point for Skype Translator users, and over several design iterations, here are the top aspects of translated call experience we’ve addressed with some success - 2.1. First-Run and Learning Curve Early usage data for the Skype Translator showed that ~40% of users had not made more than two calls on Skype Translator and that most calls on Skype Translator were under several minutes. This can be attributed to several issues such as poor translation quality and connectivity problems; but one underlying issue that emerged was that users didn’t  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 221  know how to conduct a translated call. Having a translated Skype call was dissimilar to a normal Skype call, because included learnt behaviours such as waiting for the translation audio to play, remembering to pause between sentences and avoiding interruptions. This first-run issue was addressed with two UX solutions. Solutions  User Education Video – all first-run users were taken through a two-minute explanation video to walk them through how to conduct a translated call on the first use of Skype Translator  Tooltips – first-run users were given useful tips during their first translated call which provided context for how to have a successful translated call such as reminders to wear a headset. 2.2. Sensory overload After his first call on Skype Translator, one male user study participant sat back and proclaimed - “You have to be a woman to be able to multitask in this thing…" The sentiment he expressed referred to the multitude text and audio output the user receives during a translated call. First, there are four voices in the call – the caller’s, the caller’s translated voice, the callee’s and the callee’s translated voice. This gets cacophonous, especially when sequences of utterances are said in quick succession. Secondly, along with the audio, the user is also reading along to the translated transcript for her utterance and her partner’s utterance in both languages. Many users complained that this was a lot of feedback to follow at once while trying to conduct a normal conversation. Solution  Audio Ducking – A technique used on radio, where if two audio clips are played at the same time, the volume is lowered on the less relevant once. Similarly, for Skype Translator, if translated audio and original audio is played at the same time, audio ducking is used to reduce the volume on the audio in the foreign language. 2.3. Perceived Translation Speed Another frequently heard area of feedback from users was around the slowness of translation. Users felt they had to wait a long time to hear and read their partner’s translated utterance and therefore made the conversation seem stretched out and awkward. To a large extent, this delay is a perceived speed issue, on account of the fact that a user’s translated audio could not be played until the user had completed their utterance, so as to not interrupt the user in the middle of their speech. Solutions  Partial recognition – Partial recognition enabled Skype to return partially understood utterances before the user had finished speaking. These “partials” are displayed in the transcript pane so that the user could follow along minimally to what their partner is saying.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 222   Silence interval – This advanced-user setting let users changed the value of the amount of time Skype would wait before translating their utterance. This allowed for users with a faster cadence of speech to set a low silence interval value that allowed their speech to be translated quicker. 2.4. Misrecognitions and Mistranslations The most frequent problem users encounter during a translated conversation is misrecognitions and mistranslations. Some users see misrecognitions more than others, usually users with regional accents or children because of the lack of training data for these types of speech. We reviewed several unsuccessful approaches to equip users to address misrecognitions and mistranslations. In the first iteration of the design, we tried to get users to cancel out wrong recognitions by clicking on a cancel button which their partner would also be able to see. In another iteration, we attempted to get users to correct the mistranslation by typing in the correct recognition instead by clicking on an Edit button. However, subsequent user studies demonstrated that users were generally unwilling to switch modalities from speaking to typing and clicking. Solutions  Basic user education – During the first-run setup video, users were told to repeat themselves when they were misrecognized or to rephrase their statement.  IM prompt – Skype Translator tracked the confidence scores in the last five user utterances. If Skype Translator saw repeated low-confidence recognitions from users, the user was told that they should use the chat window to type to communicate instead. Therefore users with consistently bad recognitions were prompted towards a workaround. 
ALEXYANISHEVSKY Welocalize October 2015  Miami, Oct 30 - Nov 3, 2015 | p. 224  How Much Cake Is Too Much Cake?  I LOVE CAKE…  TOO MUCH CAKE! ugh!  What is the Tipping Point? Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 225  AGENDA •How Many Engines •How to Split Domains •How to Measure Success •How to Improve Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 226  HOW MANY ENGINES: CRITERIA • Environment: Elegant Deployment? • Cost • How Different are They From Each Other? • Maintenance: Engineering + Linguistic Feedback Implementation  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 227  HOW TO SPLIT DOMAINS: CRITERIA  • Content Owner Feedback • Historical Experience Based On Business Unit or Portfolio • Naming Convention • Style Analysis: Difference in Characteristics Based on Lexical Diversity, Sentence Length + Syntactic Complexity  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 228  HOW TO SPLIT DOMAINS: TOOLS  HOLISTIC APPROACH BASED ON SEVERAL TOOLS: • Build Domain-Specific Language Models + Select TUs for Domain by PPL  • Source Content Profiler – Helps Identify Domain Based on Language Models, as well as Other Stylistic Characteristics  • Style Scorer – Higher Score Indicates Better Match to Style Established by Client’s Documents  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 229  TOOLS: PERPLEXITY EVALUATOR TU LEVEL <tu srclang="EN-US" tuid="75438"> <prop type="x-ppl:train2">208</prop><prop type="xppl:techdoc6">191.025</prop><prop type="x-ppl:support2">325.983</prop><prop type="xppl:sales1">97.0736</prop><prop type="x-ppl:productLoc1">396.398</prop><prop type="xppl:legal1">617.876</prop><tuv xml:lang="EN-US"> <seg>Consistent feature set across multiple platforms (Windows, Mac, iOS, Android).</seg> </tuv> <tuv changedate="20140325T122530Z" changeid="serviceaaa" creationdate="20140325T122530Z" creationid="serviceaaa" lastusagedate="20140325T122530Z" usagecount="0" xml:lang="ES-XL"> <prop type="x-ALS:Context">TEXT</prop> <prop type="x-ALS:Source File">\\DATA\TC\39720\SRC\EN-US\co-02__battle-card_en\co-02__battle-card_en.inx</prop> <seg>Conjunto de características coherente en varias plataformas (Windows, Mac, iOS, Android)</seg> </tuv> </tu>  Proceedings of MT Summit XV, vol. 2: MT Users' Track 
Automatic machine translation systems are seen unable to produce publishable quality translation, so various computer-assisted translation systems that emphasize humanmachine cooperation have been proposed. However, translator collaboration technologies are underdeveloped, an area of great importance for large volume translation tasks. Ideally, all human translation knowledge is shared among translators in order to maximize productivity. In a knowledge engineering manner, our collaborative translation platform collects translation knowledge and actively pushes in real time. The mutual learning between translators and machine simultaneously builds the knowledge base and improves translators’ proficiency. This paper introduces the collaboration strategies used in our platform that not only promote productivity but also ensure the translation quality. Comparative experiments by 36 professional translators prove the effectiveness of our collaboration strategies. A sounding result is that 22 professional translators completed a 97,000 page Chinese-English technical manual translation task within 42 months. 1. Introduction With the advent of big-data era, the amount of technical documents (patents, standards, specifications, manuals) that need translation to different languages explosively increases. Hugevolume technical document translation suddenly became a bottleneck for the globalization of technology. Human translation is inefficient, whereas machine translation (MT) outputs are far from being satisfactory. Recently, the computer-assisted translation (CAT) technology aiming at improving the human translation productivity achieved great progress. The most  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 248  popular two CAT modes are post-editing (PE) and interactive machine translation (IMT). For huge-volume technical document translation, however, the core issues are still unresolved. Besides all the problems in traditional translation tasks, there are three additional challenges particular for huge-volume technical document translation tasks. First, high-volume means that the task requires many professional translators collaborating, so progress management and knowledge sharing technologies play essential roles and can fundamentally affect the overall speed. Second, when there is more than one translator, it is hard to enforce consistent word choices and consistent sentence structure within or across documents. Technical manual normally requires translation of at least publication level, where details like consistent word choice and sentence structure are required. Finally, technical documents require highly specialized knowledge during translation, like technical term knowledge and relevant technical reference knowledge. Without special design, the cost on terminology looking-up by itself will fail our task. Attempting to deal with all these challenges, our collaborative machine translation platform/pipeline incorporates a new thought of the integration of knowledge management and machine translation, which centralizes on a user model. Section 3 and 4 describe the thinking, design and realization of the platform. In the rest of this paper, we select several strategies adapted in our platform tackling two issues: speed and quality. Before starting translation, the high frequency terms and sentences are pre-translated to ensure the accuracy and consistency of important technical concept translations and reduce translation difficulty. During the process of collaborative translation, the reliability of each fragment in the reference translation is color-encoded according to the source of its reference material, so as to help the translators make decisions rapidly. The translations by other translators on the same or similar sentences are pushed in real time, enabling the whole team to share the results. The translators’ progress ranking is displayed, informing them of the team progress knowledge and encouraging them to speed up. Automatic proofreading tool is provided to help translators quickly verify their translation. Synchronous quality checking is adopted to control the translation quality in time. Comparative experiments in section 5 show that these strategies can effectively improve the translation productivity while maintaining high quality. With these strategies, 22 professional translators accomplished a 97,000 page technical manual translation task within 42 months (each translator worked for 19.4 months on average). The quality requirement is higher than publishable level. 2. Related Work More than thirty years ago, Kay (1980) proposed the idea of integrating machine translation and other assistant tools into human translation work (finally published in 1997). And it is predicted that the enhancement of such a system will finally lead people to achieve the goal of machine translation. With the continuous progress of the technologies such as machine translation, information retrieval and knowledge management, human-machine synergetic translation has replaced the traditional human translation mode and evolved several new modes. Translation memory (TM) is the language processing technology which is earliest adopted in the translation process. Up till now, many professional translators still work by retrieving translations of similar fragments in the TM base. With the rapid development of statistical machine translation (SMT) technology, performing post-editing on SMT output becomes a new translation mode. It has been proved that both TM and PE can improve the translation productivity and quality (Mandreoli et al., 2006; Garcia, 2011; Arenas, 2014). Another pilot translation mode is the interactive-predictive machine trans-lation (Barrachina et al., 2009;  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 249  Sanchis-Trilles et al., 2014), in which the human gives the longest correct prefix of the translation and the system accordingly performs new decoding. The above research mainly focuses on how to improve the translation performance of an individual translator. In recent years, how to achieve highly efficient and high quality collaborative translation among multiple translators became a new interest. Some researchers studied the methods of having Internet users to perform crowdsourced translation (Zaidan and Callison-Burch, 2011; Yan et al., 2014), having community members to perform community post-editing on the user generated content (Mitchell et al., 2014), or having monolingual users cooperate to translate (Hu et al., 2010). These studies focus on non-professional translators or even non-bilingual users, and aim at making the quality of translation achieve comprehensible level or specialized level. But publishable-level translation task is still difficult to accomplish. In terms of large-scale collaborative translation among professional translators, the most relevant work is that of Karamanis (2011). The localization practice in two Language Service Providers is thoroughly investigated. The translator team’s activities of manually establishing terminology glossary (Esselink, 2003; Wittner and Goldschmidt, 2007), searching the TM, sending emails and constant messages, and talking with other team members to communicate and share translation results are introduced. In this paper, we further developed these spontaneous and naïve collaboration activities. Automatic analysis tools are used to fully mine the important terms and fragments in the whole translation task, allowing the platform to actively share the translation results and team progress in real time. Besides, the translation quality is controlled more timely through automatic proofreading and synchronous quality checking. These strategies help the translators to better understand the translation task, the team decisions and progresses, so that they can accomplish precise and consistent translation more rapidly. 3. Collaborative Translation Practice 3.1. Project Background In 2010, we started a 97,000 page publication-level Chinese-English technical manual translation project. A project team consisting of a translating group, a quality checking group, a R&D group, and a technology storming group was formed. The members of the translating team are all full-time professional translators. The members of the quality checking team are all full-time professional and experienced translators. They are paid by the amount of translations that meet the quality requirement. At the beginning, the R&D team mines the requirement and configures a series of systems and tools that support the translation. Then they continuously receive feedback from translators during the collaborative translation process, rapidly develop new functions and perform small scale trials. If a new function is satisfactory, then it will be applied in the platform. We made system developers and translators sit next to each other, so that translators can keep communicating with the technicians and the technicians can watch the real translating scenario to improve the platform in time. 3.2. The Collaborative Translation Process High-volume technical document translation is a well-known difficult task. Our approach is to break down large pieces of work into smaller, simplified and more manageable parts. On the basis of the collaborative translation platform, we built a translation pipeline consisting of 3 main stages: pre-translation analysis stage, translation stage and post-translation management stage. Before translation, deep and fragmented analysis is performed. During translation, mul-  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 250  ti-dimension knowledge view, multi-aspect translation collaboration, multi-channel knowledge pushing and multi-layer quality controlling are provided. After translation, finegrained management is performed. In this way, the pipeline decomposes the difficulties in the source texts and refines the translation step by step, thus achieving the effect of mutual knowledge increment between human and machine. The overall collaborative translation process is illustrated in Figure 1.  Figure 1. Overview of the collaborative translation process. In the above figure, during the pre-translation stage, translation unit analysis is to split the source text in the manuals into basic translation units such as paragraphs and sentences. In this project, we take sentences as the basic units. Sentence clustering is to cluster sentences with similar contents. The clustering results are used for extracting translation templates and checking sentence-level consistency. In this project, sentences are clustered with a completelinkage hierarchical clustering algorithm. Cosine distance is used to measure word-level similarity. Version analysis is designed to deal with the frequent changes in the document contents caused by the progress of technologies and the update of products. The differences among different versions are identified to avoid unnecessary repetitive work. Project analysis involves personnel recommendation, cost estimation and progress estimation. During the translation stage, information pushing involves displaying the current translator’s speed, his/her progress on the current document and all translators’ progress ranking. Term view is for listing all the translation units that contain a certain term and their translations. It is designed for integrative viewing of the term translations. Clustering view is for listing all the similar translation units and their translations. It is designed for integrative viewing of the translations of similar units. 4. Platform Architecture The work of this paper is based on a large collaborative translation platform. The platform includes six layers, namely knowledge layer, basic tool layer, interface layer, system layer, application layer and cloud service layer.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 251  (1) The knowledge layer stores and manages the linguistic knowledge for translation such as terminology, bilingual sentences, rules and templates, process knowledge (e.g. translation history, quality checking errors and experience exchanges of translators) and domain knowledge (e.g. relevant technical references and term definitions). (2) The basic tool layer provides the basic component set, including functional tools (such as data storage, network communication and data encryption), language analysis tools (such as lexical analysis, chunk analysis, parsing, text similarity computation and clustering), collaborative translation tools (such as machine translation, translation memory and translator activity recording) and knowledge management tools (such as knowledge collecting, accumulating, main-taining and sharing). (3) The interface layer uniformly packages the tools of the previous layer. Popular network communication interfaces are provided and popular protocols such as HTTP, RESTful, SOAP and CMIS are supported to enable distributed management and concurrent access to the basic tools of the platform. (4) The system layer provides all kinds of assistant systems for translators, including task management system, collaborative translation system, collaborative quality checking system, TM retrieval system, term management system and resource management system. (5) The application layer configures the sys-tems according to the task requirement, and also realizes other applications such as translation data mining and pushing, enterprisecustomized translation project management, translation skill teaching and crowdsourced translation. (6) The cloud service layer makes use of the cloud computing and cloud security technologies to provide cloud-based translation service, online trading service and translator training service, finally achieving the goal of multiple translator collaboration under the cloud environment. It is hard to describe every single technology used in our collaborative translation pipeline in one paper. In the next section, we will introduce several novel strategies for increasing translation productivity in the high-volume technical document translation context. As far as we believe, these strategies can be used in general large-scale translation situations. Of course, these strategies are far from being comprehensive. All the proposed strategies are implemented under the condition of ensuring quality. That is to say, if the translation cannot meet the quality requirement, then it will be returned to the translator for revision before it can be included in the productivity calculation. 5. Productivity Promotion Strategies 5.1. Pre-translating Before starting translation, the technical terms in all the input documents are identified automatically. Since our practice is on a Chinese-English translation task, we trained a Conditional Random Fields (CRFs) model using 2000 manually labeled sentences for each domain to extract Chinese terms. The features are the context (word and part-of-speech) within a 3-word sized window. Experimental results on 568 documents show that the precision of Chinese term recognition is 75.06% and the recall is 79.30%. Then the frequencies of the terms in the whole translation task are counted and the terms are ranked according to the frequency. Table 1 gives some examples.  Proceedings of MT Summit XV, vol. 2: MT Users' Track  Miami, Oct 30 - Nov 3, 2015 | p. 252  Term  Frequency  连接件(connector)  1559  制冷组件(cooling component)  1519  混合装置(mixing equipment)  1330  高压分离器(high pressure separator) 1220  …  …  Table 1: Examples of term analysis result.  The frequencies of the sentences in the whole task are also counted. The high frequency terms and sentences are considered to be important technical concepts and fragments. They are given to human experts to translate. And the corresponding fragments in the source texts are replaced with the decided translations. During the process of collaborative translation, any revision on these translations is prohibited. To verify the influence of pre-translating on productivity, we divided 30 translators into two groups1. Each group has 3 teams, and each team has 5 members. A document of 10,000 characters is offered for translation. The teams in group A evenly split the document into 5 pieces and each member translates 2,000 characters. The high frequency terms/sentences are translated individually and review together after translation. The teams in group B perform pre-translating at first, and then evenly split the document for individual translation and review after translation. The average translation time and reviewing time are compared2. Table 2 shows the results (in minutes).  Group A Group B  Translation Time 241  282  Reviewing Time 182  70  Overall  423  352  Table 2: Comparative result of the pre-translating strategy.  It can be seen that the pre-translating of high frequency terms/sentences increased the translation time of group B, but greatly reduced the reviewing time. Therefore the overall time is less. For large scale translation tasks, pre-translating needs to be done only one time before starting translation, and will consequently save much more time. In terms of quality, pretranslating ensures that the translations of important concepts and fragments are highly consistent.  5.2. Translation Reliability Marking In our human-machine interactive translation interface, a reference translation is provided for translators. Generally, a phrase translation model and a reordering model are both adopted in the phrase-based SMT systems. This brings about a mixture of phrase translation errors and reordering errors in the SMT output as illustrated in Figure 2.  
Patent claim sentences, despite their legal importance in patent documents, still pose difficulties for state-of-the-art statistical machine translation (SMT) systems owing to their extreme lengths and their special sentence structure. This paper describes a method for improving the translation quality of claim sentences, by taking into account the features specific to the claim sublanguage. Our method overcomes the issue of special sentence structure, by transferring the sublanguage-specific sentence structure (SSSS) from the source language to the target language, using a set of synchronous context-free grammar rules. Our method also overcomes the issue of extreme lengths by taking the sentence components to be the processing unit for SMT. The results of an experiment demonstrate that our SSSS transfer method, used in conjunction with pre-ordering, significantly improves the translation quality in terms of BLEU scores by five points, in both English-to-Japanese and Japanese-to-English directions. The experiment also shows that the SSSS transfer method significantly improves structural appropriateness in the translated sentences in both translation directions, which is indicated by substantial gains over 30 points in RIBES scores. 1. Introduction Advances in reordering techniques based on syntactic parsing (Isozaki et al., 2010b; de Gispert et al., 2015), with growing volumes of parallel patent corpora available, have brought significant improvements in the performance of statistical machine translation (SMT) for translating patent documents across distant language pairs (Goto et al., 2012; Goto et al., 2015). However, among various sentences within a patent document, patent claim sentences still pose difficulties for SMT resulting in low translation quality, despite their utmost legal importance. A patent claim sentence is written in a kind of sublanguage (Buchmann et al., 1984; Luckhardt, 1991) in the sense that it has the following two characteristics: (i) comprising a patent claim by itself with an extreme length and (ii) having a typical sentence structure composed of a fixed set of components irrespective of language, such as those illustrated in Figures 1 and 2. The difficulties in patent claim translation lie in these two characteristics. Regarding the first characteristic, the extreme lengths cause syntactic parsers to fail with consequent low  Proceedings of MT Summit XV, vol.1: MT Researchers' Track  Miami, Oct 30 - Nov 3, 2015 | p. 1  reordering accuracy. Regarding the second characteristic, the high regularity of the claim-specific sentence structure cannot be captured and transferred properly by the models trained only on the other parts of patent documents, such as the abstract and background description. This paper presents a method for improving the SMT translation quality of patent claims. We have developed a system that is used as an add-on to state-of-the-art, off-the-shelf SMT systems to deal with the sentence structure specific to the patent claim sublanguage. Our method based on this sublanguage-specific sentence structure (henceforth, SSSS) has two major effects. (1) Pre-ordering and SMT are applied for each sentence component, rather than for the entire long sentence. This in effect shortens the input to pre-ordering and SMT, thus improves translation quality. (2) Claim sentences are translated according to the sentence structure, producing structurally natural translation outputs. We manually extracted a set of language independent claim components. Moreover, using these components, we constructed a set of synchronous rules for English and Japanese to transfer the SSSS in the source language to the target language. The results of an experiment demonstrate these two major effects of our SSSS transfer method. Regarding the first effect, when used in conjunction with pre-ordering, our method improves translation quality by five points in BLEU score (Papineni et al., 2002), in both English-to-Japanese and Japanese-to-English translations. Regarding the second effect, gains in RIBES score (Isozaki et al., 2010a) of over 30 points are obtained, indicating that our SSSS transfer is effective in transferring an input sentence structure to the output sentence.  Components  Example strings  Preamble  An apparatus,  Transitional phrase  comprising:  Element a pencil;  Body  Element an eraser attached to the pencil; and  Element a light attached to the pencil.  Figure 1. Example of an English patent claim (WIPO, 2014)  Components  Example strings  Element 鉛筆と；  Body  Element 鉛筆に取り付けられた消しゴムと；  Element 鉛筆に取り付けられたライトと  Transitional phrase  を備える  Preamble  装置  Figure 2. Japanese patent claim corresponding to Figure 1  2. Transferring Claim-Specific Sentence Structure While patent claims share a common vocabulary and phrases with the rest of the patent document, they are written in a distinctive way that is different from the rest of the patent document, comprising a sublanguage of its own. This writing style of patent claims developed through the history of filing patent applications, and is now described in the literature. According to the WIPO Patent Drafting Manual (WIPO, 2014), the fundamental structure of an English claim is that it is a single sentence consisting of three components:  S → PREA TRAN BODY  Proceedings of MT Summit XV, vol.1: MT Researchers' Track  Miami, Oct 30 - Nov 3, 2015 | p. 2  where S denotes the claim sentence, PREA the preamble, TRAN the transitional phrase and BODY the body. The preamble is an introductory phrase that identifies the category of the invention, the body is the main component of the claim that describes the elements or purposes of the invention, and the transitional phrase is the component that connects the preamble and the body. Figure 1 shows one of the typical structures of English claim sentences, in which the body of the claim comprises claim elements. Each of the elements is a claim component comprising the invention. Figure 2 shows the structure of a Japanese claim sentence corresponding to the English claim sentence shown in Figure 1. Note that the sets of components comprising the claims in the two languages are identical, although the order of components is different in the two languages. Our manual analysis revealed that a claim consists of a fixed set of components and the set is common to the two languages. We also found that there are strict generation rules in each language. For example, the English patent claim sentence in Figure 1 is represented by the set of rules in Figure 3, where ELEM denotes the element component shown in Figure 1. The symbol “+” denotes a non-null list of the preceding components. The corresponding Japanese sentence is represented by another set of rules comprising the same components, as shown in Figure 4. Having observed a strong regularity in the structure of patent claim sentences across languages, we represent the structural transfer in the form of synchronous context-free grammar (SCFG). For example, we derive the SCFG rules in Figure 5 by connecting the corresponding rules in Figures 3 and 4, where the numeric indices indicate correspondences between nonterminals in both constituent trees. We handcrafted a set of SCFG rules for translating patent claim sentences. The details of the process are presented in Section 3.1  S  → PREA TRAN BODY  TRAN → “comprising:”  BODY → ELEM+  Figure 3. Example of generation rules for an English claim sentence  S  → BODY TRAN PREA  TRAN → “備える”  BODY → ELEM+  Figure 4. Example of generation rules for a Japanese claim sentence  S → 〈PREA① TRAN② BODY③, BODY③ TRAN② PREA①〉 BODY → 〈ELEM+, ELEM+〉 TRAN → 〈“comprising:”, “備える”〉 Figure 5. SCFG rules derived from English rules in Figure 3 and Japanese rules in Figure 4  3. Pipeline for Patent Claim Translation While patent claim sentences have a distinctive structure, their components, such as the elements and purposes of the claimed inventions, are described with the same vocabulary and phrases in the other parts of patent documents. We therefore implemented the SSSS transfer as an add-on to off-the-shelf SMT systems. More specifically, given a patent claim sentence in the  Proceedings of MT Summit XV, vol.1: MT Researchers' Track  Miami, Oct 30 - Nov 3, 2015 | p. 3  source language, our method translates it through the following three-step pipeline (see also Figure 6). A button comprising: a plurality of first ribs integrally formed on the surface of the plate-like base portion, each rib radially extending from a center towards the circumference of the plate-like base portion; and an annular portion integrally formed on the surface of the plate-like base portion, to which each center ends of the plurality of first ribs are coupled. (a) Input English sentence [S [PREA A button] [TRAN comprising:] [BODY [ELEM a plurality of first ribs integrally formed on the surface of the plate-like base portion, each rib radially extending from a center towards the circumference of the plate-like base portion;] [ELEM and an annular portion integrally formed on the surface of the plate-like base portion, to which each center ends of the plurality of first ribs are coupled.]]] (b) Synchronously obtained English SSSS [S [BODY [ELEM a plurality of first ribs integrally formed on the surface of the plate-like base portion, each rib radially extending from a center towards the circumference of the plate-like base portion;] [ELEM and an annular portion integrally formed on the surface of the plate-like base portion, to which each center ends of the plurality of first ribs are coupled]] [TRAN を備える] [PREA A button]] (c) Synchronously generated Japanese SSSS [S [BODY [ELEM plate like base portion of circumference towards center from extending plate like base portion of surface on formed integrally first ribs of plurality , each rib radially;] [ELEM and plate like base portion of surface, plurality of first ribs of each center ends coupled are which to on formed integrally annular portion]] [TRAN を備える] [PREA A button]] (d) Each SSSS component pre-ordered [S [BODY [ELEM 前記板状ベース部の前記表面で一体に形成され、各々が前記板状ベース部の中心 から外周に向かって放射状に延在する複数の第１リブと、] [ELEM 前記板状ベース部の前記表面 で一体に形成され、 前記複数の第１リブ各々の中心端が連結された環状部と、]] [TRAN を備え る] [PREA ボタン]] (e) Each SSSS component translated by English-to-Japanese SMT Figure 6. Overview of our translation pipeline  Proceedings of MT Summit XV, vol.1: MT Researchers' Track  Miami, Oct 30 - Nov 3, 2015 | p. 4  1. Step 1. SSSS transfer (Figure 6: (a) → (b), (c)): The given sentence is analyzed using a set of handcrafted SCFG rules. The goal of this step is not to obtain a fine-grained parse tree of the input sentence, but to identify its sublanguagespecific structure, and transfer it to the target language. By the use of the set of SCFG rules, the components in the given sentence are identified, and simultaneously the sentence structure in the target language is generated. 2. Step 2. Pre-ordering (Figure 6: (c) → (d)): The words of each component are reordered so that the order becomes close to that in the target language. This process is performed using a constituent parser. As a result of Step 1, shorter word sequences are the input to this process, resulting in higher parsing and reordering accuracy. 3. Step 3. Translation by SMT (Figure 6: (d) → (e)): Each component is trans- lated by an SMT system, and the translated components joined up to form a sentence, with words conjugated and conjunctions added as necessary. Again, as a result of Step 1, shorter components are input that are easier to translate. The rest of this section elaborates Steps 1 and 2 in turn. 3.1. SSSS Transfer As described in Section 1, one of the major issues in patent claim translation is that, despite the high regularity, the claim-specific sentence structure cannot be captured and transferred properly by models trained only on the other parts of patent documents. This step is introduced to identify the structure of the given patent claim sentence and to generate the structure in the target language simultaneously. This process is performed using a set of handcrafted SCFG rules. We created the rules in the following manner. First, we manually analyzed the English and Japanese claim sentences in our development set (described in Section 4.1) and found that each claim sentence is composed of a fixed set of components and that the set is common to the two languages. The set of components U we have identified is as follows: U ∈ {PREA, TRAN, BODY, ELEM, PURP}, where the first four are explained in the previous section, i.e., preamble, transitional phrase, body and element. PURP denotes the purpose component, which is similar to the element component in the sense that they comprise the body component. We then constructed a set of generation rules for English and Japanese claims using U as a set of non-terminal symbols, and obtained 8 and 16 generation rules respectively. We obtained a larger number of rules for Japanese, because the writing style of Japanese claim sentences is more flexible than that of English claim sentences. Finally, we handcrafted a total of 16 SCFG rules by combining the generation rules for the two languages that have the same set of symbols on both the left- and right-hand sides, respectively. Table 1 shows the entire SCFG rule set for English-to-Japanese translation. Our SCFG rules for Japanese-to-English translation are produced by reversing the above English-to-Japanese generation rules. In the actual implementation of the SCFG rules, we designed each of the rules in the rule set to be deterministic, by using regular expressions for obtaining a unique match for a terminal symbol. For example, to analyze input sentences containing more than one occurrence of the string “comprising:” we prepared a regular expression to match the first occurrence. This heuristic rule correctly matches the claim string in most cases.  Proceedings of MT Summit XV, vol.1: MT Researchers' Track  Miami, Oct 30 - Nov 3, 2015 | p. 5  Table 1. SCFG rule set for English-to-Japanese translation  ID  SCFG rules  R1  S → 〈PREA① TRAN② BODY③, PREA① BODY③ TRAN② PREA①〉  R2  S → 〈PREA① TRAN② BODY③, BODY③ TRAN② PREA①〉  R3  S → 〈PREA① TRAN② BODY③, PREA① BODY③ TRAN② PREA①〉  R4  S → 〈PREA① TRAN② BODY③, BODY③ TRAN② PREA①〉  R5  S → 〈PREA① TRAN② BODY③ TRAN② BODY③,  BODY③ TRAN② BODY③ TRAN② PREA①〉  R6  S → 〈PREA① TRAN② BODY③ TRAN② BODY③,  PREA① BODY③ TRAN② BODY③ TRAN② PREA①〉  R7 BODY → 〈ELEM+, ELEM+〉  R8 BODY → 〈PURP+, PURP+〉  R9 TRAN → 〈“comprising:”, ”備えることを特徴とする”,〉  R10 TRAN → 〈“comprising:”, ”備える”〉  R11 TRAN → 〈“including:”, ”備えることを特徴とする”〉  R12 TRAN → 〈“including:”, ”備える”〉  R13 TRAN → 〈“having:”, ”備えることを特徴とする”〉  R14 TRAN → 〈“having:”, ”備える”〉  R15 TRAN → 〈“wherein:”, ”ことを特徴とする”〉  R16 TRAN → 〈“wherein:”, ”する”〉  3.2. Pre-ordering Another major issue in patent claim translation is that the extreme lengths cause syntactic parsers to fail with consequent low reordering accuracy. To evaluate the effect of introducing our SSSS transfer on the translation quality, we also implemented a pre-ordering tool using state-of-the-art techniques (Isozaki et al., 2010b; Goto et al., 2012; Goto et al., 2015). Our pre-ordering method is based on syntactic parsing. First, the input sentence is parsed into a binary tree structure by using the Berkeley Parser (Petrov et al., 2006). For example, when “He likes apples.” is inputted into our English-to-Japanese translation system, it is parsed as shown in Figure 7. Second, the nodes in the parse tree are reordered using a classifier. For example, according to the classifier's decision, the two children of the “VP” node, i.e., “VBZ” and “NP”, are swapped, whereas the order of the two children of the “S” node, i.e., “NP” and “VP”, is retained. Once such a decision is made for every node with two children (henceforth, binary mode), the word order of the entire sentence becomes very similar to that in Japanese, i.e., “He (kare wa) apples (ringo ga) likes (suki da) . (.)” The pre-ordering model is trained on a given parallel corpus through the following procedure (Section 4.5 of Goto et al., 2015): 1. Parse the source sentences of the parallel corpus.1 2. Perform word alignment on the parallel corpus. 3. Reorder words in each source sentence by swapping some binary nodes so that Kendall's τ over the aligned source and target sentences is maximized. As a  
We introduce a novel method for bilingual phrase representation with Recurrent Neural Networks (RNNs), which transforms a sequence of word feature vectors into a ﬁxed-length phrase vector across two languages. Our method measures the difference between the vectors of source- and target-side phrases, and can be used to predict the semantic equivalence of source and target word sequences in the phrasal translation units used in phrase-based statistical machine translation. Our experiments show that the proposed method is effective in a bilingual phrasal semantic equivalence determination task and a machine translation task. 
In this paper we present a method of detecting zero pronouns in Japanese clauses and identifying their antecedents using aligned sentence pairs from a Japanese-English bilingual corpus and open resource tools. We use syntactic and semantic structures and the alignment of words and phrases in the sentence pairs to automatically detect zero pronouns and determine their antecedents using English translations. We build rules to link antecedents with zero pronouns and create filters to remove problematic sentence pairs. Experimental results confirm the effectiveness of our method. The proposed method allows the construction of an annotated corpus of zero pronoun sentences in which the antecedents of the missing pronouns are flagged. This would be very useful for machine translation (MT), because zero pronoun detection is a vital problem when translating languages which allow zero pronouns. 
This paper proposes an extension of METEOR, a well-known MT evaluation metric, for multiple target languages using an in-house lexical resource called DBnary (an extraction from Wiktionary provided to the community as a Multilingual Lexical Linked Open Data). Today, the use of the synonymy module of METEOR is only exploited when English is the target language (use of WordNet). A synonymy module using DBnary would allow its use for the 21 languages (covered up to now) as target languages. The code of this new instance of METEOR, adapted to several target languages, is provided to the community. We also show that our DBnary augmented METEOR increases the correlation with human judgements on the WMT 2013 and 2014 metrics dataset for English-to-(French, Russian, German, Spanish) language pairs. 1. Introduction Machine translation (MT) is the process of automatically translating a text in a source language into a corresponding text in a target language. In order to evaluate and compare the quality of several MT systems, we need to rate the translation hypothesis produced by each MT system either with the help of human experts (subjective evaluation) or compare it to preexisting human translations (using automatic evaluation metrics, objective evaluation). In practice, subjective evaluation considers various aspects to grade the translation quality, such as adequacy, fluency, and intelligibility. However, subjective evaluation conducted a posteriori often costs too much (in term of human resources) and, thus, objective evaluation metrics (fast and cheap as long as references are available) are often preferred nowadays. One drawback with automatic evaluation metrics is that they compare the MT hypothesis with few (and sometimes only one) reference translations. This is definitely not enough to capture lexical variation in translation. For this reason, metrics which exploit synonymy or stem similarities, such as METEOR (Banerjee and Lavie, 2005), exhibit better correlation with human judgement. METEOR maps words with the same stem or the same synset using lexico-semantic resources. However, so far, the full potential of METEOR is only exploited when English is the target language (use of WordNet).  Proceedings of MT Summit XV, vol.1: MT Researchers' Track  Miami, Oct 30 - Nov 3, 2015 | p. 80  Contribution This paper proposes an extension of METEOR for multiple target languages using a lexical resource called DBnary (Sérasset, 2015). DBnary is an extraction in RDF of the lexical data of multiple editions of Wiktionary. It has several millions of triples describing lexical entries of the extracted languages, and more than 4.6 million translations from 21 languages to more than 1500 target languages. The modified code allowing to call METEOR for new target languages (French, Russian, German, Spanish) is made available to the research community. More target languages (today 21 in total) could be plugged very quickly by interested users using the same lexical resource (DBnary notably includes Bulgarian, Dutch, English, Finnish, French, German, (Modern) Greek, Indonesian, Italian, Japanese, Latin, Lithuanian, Malagasy, Norwegian, Polish, Portuguese, Russian, Serbo-Croat, Spanish, Swedish, Turkish). We also present initial experiments on the WMT 2013 and 2014 metrics dataset and show that our new METEOR slightly increases correlation with human judgments of translation quality, for language pairs with a target language different than English. 2. State of the art Since METEOR was first introduced in 2005, it has been improved and extended to include more features and accommodate more languages for a subset of its features. 2.1. METEOR: the basics Banerjee and Lavie (2005) introduced METEOR to overcome several weakness of BLEU (Papineni, 2002) and NIST (Doddington, 2002) they identified as: the lack of recall, an indirect only measure of level of grammatical wellformedness, the lack of explicit wordmatching between translation and reference, and the use of geometric averaging of n-grams. The goal of METEOR was to aim for better correlation with human judgments of translation quality using not only word-to-word alignment between the translation hypothesis and the reference translation(s). The alignment is incrementally produced by a three-leveled mapping approach between the hypothesis and the reference, using additional resources if needed: exact match of the surface forms of the words, exact match of the computed stems of the words, synonymy overlap through shared WordNet “synset” of the words. The second mapping level uses a stemmer and the third mapping level uses Enlgish WordNet. While no free WordNets are available for languages such as French, Spanish or German, current implementation of METEOR for such languages do not support the third mapping level. 2.2. METEOR: the recent extensions METEOR-NEXT (Denkowski and Lavie, 2010a), was introduced to better correlate with human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic postediting based metric which measures the distance between a MT hypothesis and its post-edited version. The goal was to go beyond a strictly world-level metric with a new aligner supporting phrases (multi-word) matches. Thus, a fourth mapping level was added to implement this new feature using a paraphrase database. For English, the database was developed by Snover (2009a). Later, Denkowski and Lavie (2010b), released paraphrase databases for Czech, German, Spanish and French. In 2014, METEOR Universal was released (Denkowski and Lavie 2014) that enabled the construction of the paraphrase database using only the parallel corpora used to develop the MT system (which was not the case in 2010).  Proceedings of MT Summit XV, vol.1: MT Researchers' Track  Miami, Oct 30 - Nov 3, 2015 | p. 81  In order to prevent synonyms/paraphrases corresponding to different senses to be treated as semantically equivalent, Apidianaki and Marie (2015) proposed METEOR-WSD. The English references are further disambiguated and annotated using Babelfly (Moro et al., 2014) for several language pairs (French, Hindi, German, Czech and Russian to English). For their experiment, Apidianaki and Marie (2015) got a better segment-level Kendall’s τ correlation than METEOR for 4 language pairs when the paraphrase module was activated. 2.3. Lexical resources 2.3.1. WordNet WordNet is a large lexical database for English, developed by linguists of Princeton University (Fellbaum, 1998). Nowadays, it has become an important and a very useful resource for NLP applications, such as machine translation, word sense disambiguation, cross-lingual information retrieval etc. WordNet links nouns, verbs, adjectives and adverbs to sets of cognitive synonyms (called synsets), where each synset represent a specific concept. Synsets are interconnected through conceptual semantic and lexical relations, including synonymy, antonymy, hyponymy etc. Note that words with multiple meanings belong to several synsets, and their senses are arranged by order of frequency. There are different versions of WordNet in languages other than English, such as Arabic WordNet, French WordNet, etc. However, these lexical resources in other languages are not freely available. As already said, METEOR uses WordNet to increase the chance of the MT output words to match the reference words. The latest version of WordNet 3.0, contains in total 117 659 synsets: 82 115 noun synsets, 13 767 verb synsets, 18 156 adjective synsets and 3621 adverb synsets. To Lemmatize forms, METEOR uses the Morphy-7WN1 function included in WordNet. This function uses a two-step process to find lemma of a particular word W. Firstly, Morphy checks for exceptions in a list (containing morphological transformations that are not regular). If W is not in exception list, Morphy uses the rules of detachment for NOUN, VERB and ADJ categories (no rules applied to ADV). After each transformation, WordNet is searched for the resulting string in the syntactic category specified. 2.3.2. DBnary DBnary is a multilingual lexical resource in RDF (Klyne & Carroll, 2004) collected at LIG (Sérasset, 2015). The lexical data is represented using standard vocabularies. The lexicon structure is defined using the LEMON vocabulary (McCrae et al., 2011). Most parts of speech informations are mapped to the Lexinfo or OliA standard vocabularies (Cimiano et al. 2011, Hellman et al. 2015), making it highly reusable in many contexts. It is available either as a set of downloadable files or as Linked Open Data directly accessible to browsers or applications. It may also be queried online using a public SPARQL endpoint. The available lexical data is automatically extracted from 21 different language editions2 of Wiktionary, the dictionary counterpart of Wikipedia. Among available lexical data, one may find 2.9M lexical entries (with parts-of-speech, canonical form for all of them, along with pronunciations when available and inflected forms for some languages). Lexical entries are subdivided into 2.5M lexical senses (with their definitions and some usage example). 
We report on experiments to test the effectiveness of controlled language (CL) rules on texts from Japanese municipal websites. We compiled a set of rules by trial and error, systematically rewriting Japanese source texts and analysing the machine translation (MT) outputs. We then employed native English speakers with little knowledge of Japanese as human evaluators and tested the understandability and accuracy of the English machine translated text. Comparing the results of four MT systems showed that the effectiveness of CL rules varies depending on the particular MT systems. A preliminary selection of optimal rules for each system showed more than 15% increase in MT performance. We also assessed the readability of the Japanese source texts and discuss the way to make them compatible with the quality of the MT outputs. 
The lack of parallel data for many language pairs is an important challenge to statistical machine translation (SMT). One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations especially when a poor morphology language is used as the pivot between rich morphology languages. In this paper, we examine the use of synchronous morphology constraint features to improve the quality of phrase pivot SMT. We compare hand-crafted constraints to those learned from limited parallel data between source and target languages. The learned morphology constraints are based on projected alignments between the source and target phrases in the pivot phrase table. We show positive results on Hebrew-Arabic SMT (pivoting on English). We get 1.5 BLEU points over a phrase pivot baseline and 0.8 BLEU points over a system combination baseline with a direct model built from parallel data. 
 ADAPT Centre, School of Computing, Dublin City University, Ireland  Alfredo Maldonado-Guerra David Lewis ADAPT Centre, Trinity College Dublin, Ireland  maldonaa@tcd.ie dave.lewis@cs.tcd.ie  Abstract Post-editing the output of a statistical machine translation (SMT) system to obtain high-quality translation has become an increasingly common application of SMT, which henceforth we refer to as post-editing-based SMT (PE-SMT). PE-SMT is often deployed as an incrementally retrained system that can learn knowledge from human post-editing outputs as early as possible to augment the SMT models to reduce PE time. In this scenario, the order of input segments plays a very important role in reducing the overall PE time. Under the active learning-based (AL) framework, this paper provides an empirical study of several typical segment prioritization methods, namely the cross entropy difference (CED), n-grams, perplexity (PPL) and translation conﬁdence, and veriﬁes their performance on different data sets and language pairs. Experiments in a simulated setting show that the conﬁdence of translations performs best with decreases of 1.72-4.55 points TER absolute on average compared to the sequential PE-based incrementally retrained SMT. 
 tbystro2@kent.edu  Dept. of Modern & Classical Language Studies, Kent State University, Kent, OH 44242, USA  Abstract Phrase-based machine translation can be configured to produce alignment data that indicates which machine translated target language words correspond to which original source language words. In most prior work that examined the efficacy of post-editing machine translation, posteditors were presented with machine translations (and in most cases the original source language sentences) without also being presented with source-to-target alignment links. We select four news articles, and ask six Russian-English bilinguals and eleven Spanish-English bilinguals to post-edit English machine translation results, in some cases using alignments and in other cases without. We obtain human adequacy judgements of the post-edited sentences, and demonstrate that when machine translation quality is low, post-editing quality is consistently higher, by a statistically significant amount, when bilingual post-editors are presented with alignment data. 
Shallow local multi-bottom up tree transducers (MBOTs) have been successfully used as translation models in several settings because of their ability to model discontinuities. In this contribution, several additional settings are explored and evaluated. The ﬁrst rule extractions for tree-to-tree MBOT with non-minimal rules and for string-to-string MBOT are developed. All existing MBOT systems are systematically evaluated and compared to corresponding baseline systems in three large-scale translation tasks: English-to-German, English-to-Chinese, and English-to-Arabic. Particular emphasis is placed on the use of discontinuous rules. The developed rule extractions and analysis tools will be made publicly available. 
In this paper, we present a way of translating Korean words to Chinese using Chinese character information. A mapping table of Korean and Chinese characters is constructed and used to obtain possible combinations as translation candidates. The candidates are ranked by the combination score which accounts for the possibility of the character combination and context similarity score, which indicates contextual information among words. Parallel resources like Wikipedia aligned data or Wiktionary data are used for preliminary translation and also used during ranking candidates. 
We propose an improved beam search decoding algorithm with constrained softmax operations for neural machine translation (NMT). NMT is a newly emerging approach to predict the best translation by building a neural network instead of a log-linear model. It has achieved comparable translation quality to the existing phrase-based statistical machine translation systems. However, how to perform efﬁcient decoding for NMT is still challenging, especially for commercial systems which provide real-time translation service. Unlike the standard beam search algorithm, we use a priority queue to choose the best hypothesis for the next search, which drastically reduces search space. Another time consuming factor is the softmax operation in the output layer because of the large target vocabulary size. To solve this problem, we introduce a limited word set of translation candidates to greatly reduce the computation complexity. Our experiments show that, under the GPU environment, our method achieves a speed about 3.5 times faster than the well optimized baseline system without sacriﬁcing the translation quality. Our method translates about 117 words per second, beating the real-time translation requirements for practical MT systems. 
Phrase–based machine translation (PBMT) relies upon the phrase-table as the main resource for bilingual knowledge at decoding time. A phrase table in its basic form includes aligned phrases along with four probabilities indicating aspects of the co-occurrence statistics for each phrase pair. In this paper we add a new semantic similarity score as a statistical feature to enrich the phrase table. The new feature is inferred from a bilingual corpus by a neural network (NN), and estimates the semantic similarity of each source and target phrase pair. We observe a signiﬁcant increase in system performance with the addition of the new feature. We evaluated our model on the English–French (En–Fr) and English–Farsi (En–Fa) language pairs. Experimental results show improvements for all translation directions of En↔Fr and En↔Fa. 
The statistical machine translation outputs are not error-free and in a high quality yet. So in the cases that we need high quality translations we definitely need the human intervention. An interactive-predictive machine translation is a framework, which enables the collaboration of the human and the translation system. Here, we address the problem of searching the best suffix to propose to the user in the phrase-based interactive prediction scenario. By adding the jump operation to the common edit distance based search, we try to overcome the lack of some of the reorderings in the search graph which might be desired by the user. The experiments results shows that this method improves the base method by 1.35% in KSMR2, and if we combine the edit error in the proposed method with the translation scores given by the statistical models to select the offered suffix, we could gain the KSMR improvement of about 1.63% compared to the base search method. 1. Introduction Although the significant improvements achieved in the field of statistical machine translation, the current models and therefore the systems which can be built from them are still far from perfect. So, in order to achieve good or even acceptable translations, manual post editing is needed. An alternative to this approach is given by the interactive predictive machine translation paradigm (Barrachina et al., 2009). Under this paradigm, translation is considered as an iterative process where the human translator and the computer collaborates to generate the final translation, and in this way both the human and the computer can help each other. In the interactive predictive MT scenario, the system initially offers a translation for a source sentence. From this translation, the user could mark a prefix as correct and begins to type the rest of the target sentence as it’s desirable for him. By typing the single next character (or maybe the full next word, according to the system settings) the interactive MT system would suggest a new suffix that completes the user’s confirmed prefix with the new character the user has just typed. This procedure continues iteratively until the user accepts the translation to be complete and correct. One of the important issues in the development of interactive predictive MT systems is the strategy to search and offer the best suffix that completes the prefix confirmed by the user. One of the most common ways for phrase-based translation systems is to use the search graph that is obtained once during the translation process, and complete each user’s confirmed pre- 
We show that applying semantic role label constraints to bracketing ITG alignment to train MT systems improves the quality of MT output in comparison to the conventional BITG and GIZA alignments. Moreover, we show that applying soft constraints to SRL-constrained BITG alignment leads to a better translation system compared to using hard constraints which appear too harsh to produce meaningful biparses. We leverage previous work demonstrating that BITG alignments are able to fully cover cross-lingual semantic frame alternations, by using semantic role labeling to further narrow BITG constraints, in a soft fashion that avoids losing relevant portions of the search space. SRL-based evaluation metrics like MEANT have shown that tuning towards preserving the shallow semantic structure across translations, robustly improves translation performance. Our approach brings the same intuition into the training phase. We show that our new alignment outperforms both conventional Moses and BITG alignment baselines in terms of the adequacy-oriented MEANT scores, while still producing comparable results in terms of edit distance metrics. 
Vectorial representations of words have grown remarkably popular in natural language processing and machine translation. The recent surge in deep learning-inspired methods for producing distributed representations has been widely noted even outside these ﬁelds. Existing representations are typically trained on large monolingual corpora using context-based prediction models. In this paper, we propose extending pre-existing word representations by exploiting Wiktionary. This process results in a substantial extension of the original word vector representations, yielding a large multilingual dictionary of word embeddings. We believe that this resource can enable numerous monolingual and cross-lingual applications, as evidenced in a series of monolingual and cross-lingual semantic experiments that we have conducted. 
 October 2015  Literature Lifts Up Computational Linguistics  David K. Elson, Google, delson4@gmail.com Anna Feldman, Montclair State University, feldmana@mail.montclair.edu Anna Kazantseva, National Research Council Canada, anna@anna-kazantseva.com Stan Szpakowicz, University of Ottawa, szpak@eecs.uottawa.ca  Preface to the special issue This collection of papers is a vignette of the ﬁrst three of an ongoing series of workshops on Computational Linguistics for Literature (CLfL), collocated with conferences organized by the Association for Computational Linguistics.1 The aim of the workshops is to create a forum for computational linguists who share a fascination with literature. The workshops have boasted papers on a wide variety of exciting topics such as computational treatment of poetry, automatic identiﬁcation of quotable text, generation of music from literature, and computational models of narratives, to name but a few. This volume contains a small yet representative sample of research which our community carried out between 2012 and 2014. The CLfL workshops cover a somewhat diﬀuse area. Some of the papers look at how computation can answer questions posed by the humanities. Other papers ask how the state of the art in Natural Lan- 1The fourth workshop (https://sites.google.com/site/clﬂ2015/) is already history. LiLT Volume 12, Issue 1, October 2015. Literature Lifts Up Computational Linguistics. Copyright c 2015, CSLI Publications. 
T. S. Eliot{'}s poem The Waste Land is a notoriously challenging example of modernist poetry, mixing the independent viewpoints of over ten distinct characters without any clear demarcation of which voice is speaking when. In this work, we apply unsupervised techniques in computational stylistics to distinguish the particular styles of these voices, offering a computer{'}s perspective on longstanding debates in literary analysis. Our work includes a model for stylistic segmentation that looks for points of maximum stylistic variation, a k-means clustering model for detecting non-contiguous speech from the same voice, and a stylistic profiling approach which makes use of lexical resources built from a much larger collection of literary texts. Evaluating using an expert interpretation, we show clear progress in distinguishing the voices of The Waste Land as compared to appropriate baselines, and we also offer quantitative evidence both for and against that particular interpretation.
How do standards of poetic beauty change as a function of time and expertise? Here we use computational methods to compare the stylistic features of 359 English poems written by 19th century professional poets, Imagist poets, contemporary professional poets, and contemporary amateur poets. Building upon techniques designed to analyze style and sentiment in texts, we examine elements of poetic craft such as imagery, sound devices, emotive language, and diction. We find that contemporary professional poets use significantly more concrete words than 19th century poets, fewer emotional words, and more complex sound devices. These changes are consistent with the tenets of Imagism, an early 20thcentury literary movement. Further analyses show that contemporary amateur poems resemble 19th century professional poems more than contemporary professional poems on several dimensions. The stylistic similarities between contemporary amateur poems and 19th century professional poems suggest that elite standards of poetic beauty in the past {``}trickled down{''} to influence amateur works in the present. Our results highlight the influence of Imagism on the modern aesthetic and reveal the dynamics between {``}high{''} and {``}low{''} art. We suggest that computational linguistics may shed light on the forces and trends that shape poetic style.
Within the field of literary analysis, there are few branches as confusing as that of genre theory. Literary criticism has failed so far to reach a consensus on what makes a genre a genre. In this paper, we examine the degree to which the character structure of a novel is indicative of the genre it belongs to. With the premise that novels are societies in miniature, we build static and dynamic social networks of characters as a strategy to represent the narrative structure of novels in a quantifiable manner. For each of the novels, we compute a vector of literary-motivated features extracted from their network representation. We perform clustering on the vectors and analyze the resulting clusters in terms of genre and authorship.
Since the 18th century, the novel has been one of the defining forms of English writing, a mainstay of popular entertainment and academic criticism. Despite its importance, however, there are few computational studies of the large-scale structure of novels{---}and many popular representations for discourse modeling do not work very well for novelistic texts. This paper describes a high-level representation of plot structure which tracks the frequency of mentions of different characters, topics and emotional words over time. The representation can distinguish with high accuracy between real novels and artificially permuted surrogates; characters are important for eliminating random permutations, while topics are effective at distinguishing beginnings from ends.
Literary works are becoming increasingly available in electronic formats, thus quickly transforming editorial processes and reading habits. In the context of the global enthusiasm for multilingualism, the rapid spread of e-book readers, such as Amazon Kindle R or Kobo Touch R , fosters the development of a new generation of reading tools for bilingual books. In particular, literary works, when available in several languages, offer an attractive perspective for self-development or everyday leisure reading, but also for activities such as language learning, translation or literary studies. An important issue in the automatic processing of multilingual e-books is the alignment between textual units. Alignment could help identify corresponding text units in different languages, which would be particularly beneficial to bilingual readers and translation professionals. Computing automatic alignments for literary works, however, is a task more challenging than in the case of better behaved corpora such as parliamentary proceedings or technical manuals. In this paper, we revisit the problem of computing high-quality. alignment for literary works. We first perform a large-scale evaluation of automatic alignment for literary texts, which provides a fair assessment of the actual difficulty of this task. We then introduce a two-pass approach, based on a maximum entropy model. Experimental results for novels available in English and French or in English and Spanish demonstrate the effectiveness of our method.
This paper focuses on the question of what kind of data needs to be recorded about figurative language, in order to capture the essential meaning of the text and to enable us to re-create a synonymous text, based on that data. A short review of the best known systems of semantic annotation will be presented and their suitability for the task will be analyzed. Also, a method that could be used for representing the meaning of the idioms, metaphors and metonymy in the data model will be considered.
Type theory has played an important role in specifying the formal connection between syntactic structure and semantic interpretation within the history of formal semantics. In recent years rich type theories developed for the semantics of programming languages have become influential in the semantics of natural language. The use of probabilistic reasoning to model human learning and cognition has become an increasingly important part of cognitive science. In this paper we offer a probabilistic formulation of a rich type theory, Type Theory with Records (TTR), and we illustrate how this framework can be used to approach the problem of semantic learning. Our probabilistic version of TTR is intended to provide an interface between the cognitive process of classifying situations according to the types that they instantiate, and the compositional semantics of natural language.
Multilinguality is a key feature of today{'}s Web, and it is this feature that we leverage and exploit in our research work at the Sapienza University of Rome{'}s Linguistic Computing Laboratory, which I am going to overview and showcase in this talk. I will start by presenting BabelNet 3.0, available at http://babelnet.org, a very large multilingual encyclopedic dictionary and semantic network, which covers 271 languages and provides both lexicographic and encyclopedic knowledge for all the open-class parts of speech, thanks to the seamless integration of WordNet, Wikipedia, Wiktionary, OmegaWiki, Wikidata and the Open Multilingual WordNet. Next, I will present Babelfy, available at http://babelfy.org, a unified approach that leverages BabelNet to jointly perform word sense disambiguation and entity linking in arbitrary languages, with performance on both tasks on a par with, or surpassing, those of task-specific state-of-the-art supervised systems. Finally I will describe the Wikipedia Bitaxonomy, available at http://wibitaxonomy.org, a new approach to the construction of a Wikipedia bitaxonomy, that is, the largest and most accurate currently available taxonomy of Wikipedia pages and taxonomy of categories, aligned to each other. I will also give an outline of future work on multilingual resources and processing, including state-of-the-art semantic similarity with sense embeddings.
The creation of high-quality named entity annotated resources is time-consuming and an expensive process. Most of the gold standard corpora are available for English but not for less-resourced languages such as Vietnamese. In Asian languages, this task is remained problematic. This paper focuses on an automatic construction of named entity annotated corpora for Vietnamese-French, a less-resourced pair of languages. We incrementally apply different cross-projection methods using parallel corpora, such as perfect string matching and edit distance similarity. Evaluations on Vietnamese {--}French pair of languages show a good accuracy (F-score of 94.90{\%}) when identifying named entities pairs and building a named entity annotated parallel corpus.
Evaluation approaches for unsupervised rank-based keyword assignment are nearly as numerous as are the existing systems. The prolific production of each newly used metric (or metric twist) seems to stem from general dis-satisfaction with the previous one and the source of that dissatisfaction has not previously been discussed in the literature. The difficulty may stem from a poor specification of the keyword assignment task in view of the rank-based approach. With a more complete specification of this task, we aim to show why the previous evaluation metrics fail to satisfy researchers{'} goals to distinguish and detect good rank-based keyword assignment systems. We put forward a characterisation of an ideal evaluation metric, and discuss the consistency of the evaluation metrics with this ideal, finding that the average standard normalised cumulative gain metric is most consistent with this ideal.
This paper presents an exhaustive study on the generation of graph input to unsupervised graph-based non-contextual single document keyword extraction systems. A concrete hypothesis on concept coordination for documents that are scientific articles is put forward, consistent with two separate graph models : one which is based on word adjacency in the linear text{--}an approach forming the foundation of all previous graph-based keyword extraction methods, and a novel one that is based on word adjacency modulo their modifiers. In doing so, we achieve a best reported NDCG score to date of 0.431 for any system on the same data. In terms of a best parameter f-score, we achieve the highest reported to date (0.714) at a reasonable ranked list cut-off of n = 6, which is also the best reported f-score for any keyword extraction or generation system in the literature on the same data. The best-parameter f-score corresponds to a reduction in error of 12.6{\%} conservatively.
Using and interpreting metadata: We’re seeing a trend where we often have as much or more metadata than actual text to translate. How can this be leveraged to improve results, and make translators’/post‐editors’ lives easier? Quality evaluation, utility prediction: What do we even mean by “utility”? Let’s work together on establishing a standard. Can we get robust, reliable QE from limited (or no) bilingual data? (We’ve got some evidence that the answer is yes…) What can we say about quality vis à vis functionality? Collaboration: We want to help! We’re very interested to see research that’s directly applicable, and we want to find ways to facilitate academic/industry partnerships. We can work with clients to try to make large amounts of domain‐specific (non‐parliamentary!) data available. Please reach out to us so we know what you’re working on. Better yet, let’s work on finding mutually beneficial projects. 
This paper presents the work done to port a deep-transfer rule-based machine translation system to translate from a different source language by maximizing the exploitation of existing resources and by limiting the development work. Speciﬁcally, we report the changes and effort required in each of the system’s modules to obtain an English-Basque translator, ENEUS, starting from the Spanish-Basque Matxin system. We run a human pairwise comparison for the new prototype and two statistical systems and see that ENEUS is preferred in over 30% of the test sentences. 
This paper presents a hybrid machine translation framework based on a preprocessor that translates fragments of the input text by using example-based machine translation techniques. The preprocessor resembles a translation memory with named-entity and chunk generalization, and generates a high quality partial translation that is then completed by the main translation engine, which can be either rule-based (RBMT) or statistical (SMT). Results are reported for both RBMT and SMT hybridization as well as the preprocessor on its own, showing the effectiveness of our approach. 
This paper motivates the need for an homogeneous way of measuring and estimating translation effort (quality) in computeraided translation. It then deﬁnes a general framework for the measurement and estimation of translation effort so that translation technologies can be both optimized and combined in a principled manner. In this way, professional translators will beneﬁt from the seamless integration of all the technologies at their disposal when working on a translation job. 
This paper investigates to what extent the use of paraphrasing in translation memory (TM) matching and retrieval is useful for human translators. Current translation memories lack semantic knowledge like paraphrasing in matching and retrieval. Due to this, paraphrased segments are often not retrieved. Lack of semantic knowledge also results in inappropriate ranking of the retrieved segments. Gupta and Ora˘san (2014) proposed an improved matching algorithm which incorporates paraphrasing. Its automatic evaluation suggested that it could be beneﬁcial to translators. In this paper we perform an extensive human evaluation of the use of paraphrasing in the TM matching and retrieval process. We measure post-editing time, keystrokes, two subjective evaluations, and HTER and HMETEOR to assess the impact on human performance. Our results show that paraphrasing improves TM matching and retrieval, resulting in translation performance increases when translators use paraphrase enhanced TMs.  (or sometimes stem) matching. Most of the commercial systems use edit distance (Levenshtein, 1966) or some variation of it, e.g. the open-source TM OmegaT1 uses word-based edit distance with some extra preprocessing. Although these measures provide a strong baseline, they are not sufﬁcient to capture semantic similarity between the segments as judged by humans. Gupta and Ora˘san (2014) proposed an edit distance measure which incorporates paraphrasing in the process. In the present paper, we perform a human-centred evaluation to investigate the use of paraphrasing in translation memory matching and retrieval. We use the same system as Gupta and Ora˘san (2014) and investigate the following questions: (1) how much of an improvement can paraphrasing provide in terms of retrieval? (2) What is the quality of the retrieved segments and its impact on the work of human translators? These questions are answered using human centred evaluations. To the best of our knowledge, this paper presents the ﬁrst work on assessing the quality of any type of semantically informed TM fuzzy matches based on post-editing time or keystrokes. 2 Related Work  
We propose a novel dependency-based reordering model for hierarchical SMT that predicts the translation order of two types of pairs of constituents of the source tree: head-dependent and dependent-dependent. Our model uses the dependency structure of the source sentence to capture the medium- and long-distance reorderings between these pairs of constituents. We describe our reordering model in detail and then apply it to a language pair in which the languages involved follow different word order patterns, English (SVO) and Farsi (free word order being SOV the most frequent pattern). Our model outperforms a baseline (standard hierarchical SMT) by 0.78 BLEU points absolute, statistically signiﬁcant at p = 0.01. 
The modelling of natural language tasks using data-driven methods is often hindered by the problem of insufﬁcient naturally occurring examples of certain linguistic constructs. The task we address in this paper – quality estimation (QE) of machine translation – suffers from lack of negative examples at training time, i.e., examples of low quality translation. We propose various ways to artiﬁcially generate examples of translations containing errors and evaluate the inﬂuence of these examples on the performance of QE models both at sentence and word levels. 
This paper aims at exploring the potential of a lay community as post-editors. It focusses on 15 members of an online technology support forum, native speakers of the target language (TL) and some knowledge of the source language (SL) translating content that was machine translated from English into German speciﬁc to their own domain. It presents the most predominant errors remaining in the post-edited output and the impact of these on the quality of the post-edited output as measured by domain specialists evaluating adequacy and ﬂuency. This paper further explores examples of these errors and possible solutions to reducing the occurrence of these and maximising the community’s potential. The targeted post-editing quality was “‘good enough”, as determined in the postediting guidelines. The PE results demonstrate that there is still room for improvement in terms of quality. 
Sharon O’Brien ADAPT Centre/SALIS/CTTS Dublin City University Ireland sharon.obrien@dcu.ie  Abstract The increasing use of post-editing in localisation workflows has led to a great deal of research and development in the area, much of it requiring user evaluation. This paper compares some results from a post-editing user interface study carried out using novice and expert translator groups. By comparing rates of productivity, edit distance, engagement with the research, and qualitative findings regarding each group‟s attitude to post-editing, we find that there are trade-offs to be considered when selecting participants for evaluation tasks. Novices may generally be more positive and enthusiastic and will engage considerably with the research while professionals will be more efficient, but their routines and attitudes may prevent full engagement with research objectives. 
Statistical machine translation (SMT) suffers from various problems which are exacerbated where training data is in short supply. In this paper we address the data sparsity problem in the Farsi (Persian) language and introduce a new parallel corpus, TEP++. Compared to previous results the new dataset is more efﬁcient for Farsi SMT engines and yields better output. In our experiments using TEP++ as bilingual training data and BLEU as a metric, we achieved improvements of +11.17 (60%) and +7.76 (63.92%) in the Farsi– English and English–Farsi directions, respectively. Furthermore we describe an engine (SF2FF) to translate between formal and informal Farsi which in terms of syntax and terminology can be seen as different languages. The SF2FF engine also works as an intelligent normalizer for Farsi texts. To demonstrate its use, SF2FF was used to clean the IWSLT–2013 dataset to produce normalized data, which gave improvements in translation quality over FBK’s Farsi engine when used as training data. 
In this paper the author presents methods for dynamic terminology integration in statistical machine translation systems using a source text pre-processing workﬂow. The workﬂow consists of exchangeable components for term identiﬁcation, inﬂected form generation for terms, and term translation candidate ranking. Automatic evaluation for three language pairs shows a translation quality improvement from 0.9 to 3.41 BLEU points over the baseline. Manual evaluation for seven language pairs conﬁrms the positive results; the proportion of correctly translated terms increases from 1.6% to 52.6% over the baseline. 
Multiple references in machine translation evaluation are usually under-explored: they are ignored by alignment-based metrics and treated as bags of n-grams in string matching evaluation metrics, none of which take full advantage of the recurring information in these references. By exploring information on the n-gram distribution and on divergences in multiple references, we propose a method of ngram weighting and implement it to generate new versions of the popular BLEU and NIST metrics. Our metrics are tested in two into-English machine translation datasets. They lead to a signiﬁcant increase in Pearson’s correlation with human ﬂuency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation. 
In this paper we analyse the use of popular automatic machine translation evaluation metrics to provide labels for quality estimation at document and paragraph levels. We highlight crucial limitations of such metrics for this task, mainly the fact that they disregard the discourse structure of the texts. To better understand these limitations, we designed experiments with human annotators and proposed a way of quantifying differences in translation quality that can only be observed when sentences are judged in the context of entire documents or paragraphs. Our results indicate that the use of context can lead to more informative labels for quality annotation beyond sentence level. 
In this paper we present an approach to reduce data sparsity problems when translating from morphologically rich languages into less inﬂected languages by selectively stemming certain word types. We develop and compare three different integration strategies: replacing words with their stemmed form, combined input using alternative lattice paths for the stemmed and surface forms and a novel hidden combination strategy, where we replace the stems in the stemmed phrase table by the observed surface forms in the test data. This allows us to apply advanced models trained on the surface forms of the words. We evaluate our approach by stemming German adjectives in two German→English translation scenarios: a low-resource condition as well as a large-scale state-of-the-art translation system. We are able to improve between 0.2 and 0.4 BLEU points over our baseline and reduce the number of out-of-vocabulary words by up to 16.5%. 
This article presents a method of training maximum-entropy models to perform lexical selection in a rule-based machine translation system. The training method described is unsupervised; that is, it does not require any annotated corpus. The method uses source-language monolingual corpora, the machine translation (MT) system in which the models are integrated, and a statistical target-language model. Using the MT system, the sentences in the sourcelanguage corpus are translated in all possible ways according to the different translation equivalents in the bilingual dictionary of the system. These translations are then scored on the target-language model and the scores are normalised to provide fractional counts for training source-language maximum-entropy lexical-selection models. We show that these models can perform equally well, or better, than using the target-language model directly for lexical selection, at a substantially reduced computational cost.  corpus-based MT requires parallel corpora in the order of tens of millions of words. Although for some language pairs these exist, they only exist for a fraction of the world’s languages. An RBMT system typically consists of an analysis component,1 a transfer component and a generation component. As part of the transfer component it is necessary to make choices regarding words in the source language (SL) which may have more than one translation in the target language (TL). Lexical selection is the task of choosing, for a given SL word, the most adequate translation in the TL among a known set of alternatives. The task is related to the task of word-sense disambiguation (Ide and Ve´ronis, 1998). However, it is different to word-sense disambiguation in that lexical selection is a bilingual problem, not a monolingual problem: its aim is to ﬁnd the most adequate translation, not the most adequate sense. Thus, it is not necessary to choose among a series of ﬁne-grained senses if all these senses result in the same ﬁnal translation; however, it may sometimes be necessary to choose a different translation for the same sense, for example in a collocation. 1.1 Prior work  
The concept of fuzzy matching in translation memories can take place using linguistically aware or unaware methods, or a combination of both. We designed a ﬂexible and time-efﬁcient framework which applies and combines linguistically unaware or aware metrics in the source and target language. We measure the correlation of fuzzy matching metric scores with the evaluation score of the suggested translation to ﬁnd out how well the usefulness of a suggestion can be predicted, and we measure the difference in recall between fuzzy matching metrics by looking at the improvements in mean TER as the match score decreases. We found that combinations of fuzzy matching metrics outperform single metrics and that the best-scoring combination is a non-linear combination of the different metrics we have tested. 
This paper presents experiments on the human ranking task performed during WMT2013. The goal of these experiments is to re-run the human evaluation task with translation studies students and to compare the results with the human rankings performed by the WMT development teams during WMT2013. More speciﬁcally, we test whether we can reproduce, and if yes to what extent, the WMT2013 ranking task and whether specialised knowledge from translation studies inﬂuences the results in terms of intra- and inter-annotator agreement as well as in terms of system ranking. We present two experiments on the English-German WMT2013 machine translation output. Analysis of the data follows the methods described in the ofﬁcial WMT2013 report. The results indicate a higher inter- and intra-annotator agreement, less ties and slight differences in ranking for the translation studies students as compared to the WMT development teams. 
Translation memories (TM) are widely used in the localization industry to improve consistency and speed of human translation. Several approaches have been presented to integrate the bilingual translation units of TMs into statistical machine translation (SMT). We present an extension of these approaches to the integration of partial matches found in a large, monolingual corpus in the target language, using cross-language information retrieval (CLIR) techniques. We use locality-sensitive hashing (LSH) for efﬁcient coarse-grained retrieval of match candidates, which are then ﬁltered by ﬁnegrained fuzzy matching, and ﬁnally used to re-rank the n-best SMT output. We show consistent and signiﬁcant improvements over a state-of-the-art SMT system, across different domains and language pairs on tens of millions of sentences. 
We present a translation system that models the selection of prepositions in a targetside generation component. This novel approach allows the modeling of all subcategorized elements of a verb as either NPs or PPs according to target-side requirements relying on source and target side features. The BLEU scores are encouraging, but fail to surpass the baseline. We additionally evaluate the preposition accuracy for a carefully selected subset and discuss how typical problems of translating prepositions can be modeled with our method. 
This paper presents a method for cleaning and evaluating parallel corpora using word alignments and machine learning algorithms. It is based on the assumption that parallel sentences have many word alignments while non-parallel sentences have few or none. We show that it is possible to build an automatic classifier, which identifies most of non-parallel sentences in a parallel corpus. This method allows us to do (1) automatic quality evaluation of parallel corpus, and (2) automatic parallel corpus cleaning. The method allows us to get cleaner parallel corpora, smaller statistical models, and faster MT training, but this does not always guarantee higher BLEU scores. An open-source implementation of the tool described in this paper is available from https://github.com/tilde-nlp/c-eval. 
The quality and quantity of articles in each Wikipedia language varies greatly. Translating from another Wikipedia is a natural way to add more content, but the translation process is not properly supported in the software used by Wikipedia. Past computer-assisted translation tools built for Wikipedia are not commonly used. We created a tool that adapts to the speciﬁc needs of an open community and to the kind of content in Wikipedia. Qualitative and quantitative data indicates that the new tool helps users translate articles easier and faster. 
This paper describes the challenges of building a Statistical Machine Translation (SMT) system for non-ﬁctional subtitles. Since our experiments focus on a “difﬁcult“ translation direction (i.e. FrenchGerman), we investigate several methods to improve the translation performance. We also compare our in-house SMT systems (including domain adaptation and pre-reordering techniques) to other SMT services and show that prereordering alone signiﬁcantly improves the baseline systems. 
This paper presents a machine translation tool – based on Moses – developed for the International Maritime Organization (IMO) for the automatic translation of documents from Spanish, French, Russian and Arabic to/from English. The main challenge lies in the insufﬁcient size of inhouse corpora (especially for Russian and Arabic). The United Nations (UN) granted IMO the right to use UN resources and we describe experiments and results we obtained with different translation model combination techniques. While BLEU results remain inconclusive for combinations, we also analyze user preferences for certain models (when choosing betweeen IMO only or combined with UN). The combined models are perceived by translators as being much better for general texts while IMO only models seem better for technical texts. 
